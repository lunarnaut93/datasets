{"2019-01-01-677390110": {"title": "Classic Brands Are Feeling The Crunch Of Changing Shopping Habits : NPR", "url": "https://www.npr.org/2019/01/01/677390110/from-campbells-to-kellogg-s-classic-brands-are-feeling-the-crunch", "author": "No author found", "published_date": "2019-01-01", "content": "AUDIE CORNISH, HOST: The world of mainstream consumer brands is in a slow-motion transformation. The companies that make products such as Campbell's Soup, Gillette razors, Crest toothpaste and Dove soap are going through major corporate changes, restructuring and hiring new CEOs. NPR's Alina Selyukh reports these mainstream brands are facing the challenge of adjusting to new shopping habits. ALINA SELYUKH, BYLINE: Think about the last time you went to the supermarket. You probably spent no more than a few seconds choosing from all the different brands of toothpaste or frozen peas or oatmeal. In retail, this is called, dramatically, the first moment of truth. For brands, these few seconds used to be their Holy grail, but in the past decade, shoppers started saying they'd go by other things. JULIET MCFADDEN: Usually by what's cheapest. Like, I'm not a brand person. SELYUKH: That's Juliet McFadden. She's an office manager in Boston, and she's 23. She's just starting to build her finances and lifelong shopping habits. This makes her a huge target for companies like Procter & Gamble, Kraft Heinz or General Mills, but she is not easy to win over. MCFADDEN: I'm not a cereal person. I just usually don't eat breakfast. I don't really drink soda ever. I don't like the yogurts with a ton of sugar in them that are super sweet. Paper towels are expensive. Stuff like that adds up. We have reusable, like, rags that we use and then wash. SELYUKH: McFadden's generation often gets the blame here, the tired trope about millennials killing breakfast cereals or napkins or canned tuna. But really, most Americans could make a similar list. Maybe you choose the store brand of toilet paper, buy a fancier condiment instead of Hellmann's mayo, order eco-friendly diapers on the Internet. Here's David Luttenberger of market research firm Mintel. DAVID LUTTENBERGER: Rather than just relying on brand familiarity, consumers buy today what performs for them. They are much less brand loyal, and they are more driven by performance, by convenience, by price. SELYUKH: At least two major things have changed us as shoppers. During the last recession, Americans warmed up to cheaper off-brand products like generic or store brands, and then they kept buying them even as the economy improved. And, of course, the Internet has completely shaken up our shopping. Think about how people used to learn about new brands. (SOUNDBITE OF AD)UNIDENTIFIED CHILD: (Singing) Oh, I'd love to be an Oscar Meyer wiener. SELYUKH: Only the biggest companies could afford catchy prime time TV ads. (SOUNDBITE OF AD)UNIDENTIFIED CHILD: (Singing) 'Cause if I were an Oscar Meyer wiener, everyone would be in love with me. SELYUKH: And so the boomer generation of shoppers grew up reaching for classic American brands. Now, Campbell's Soup, that symbol of the postwar era of processed foods, is restructuring as Americans are demanding fresher foods with pronounceable ingredients. Kraft Heinz got rid of artificial preservatives and dyes from its mac and cheese. Procter & Gamble lowered the price of Gillette razors for the first time in years to compete with the online startups like Dollar Shave Club. Unilever bought that startup, Dollar Shave Club. The mainstream brands are being squeezed by rivals that are both cheaper and more personalized. AMERICUS REED: They're in a bit of a pickle. SELYUKH: Americus Reed is a marketing professor at the University of Pennsylvania's Wharton School. He says legacy brands have to both stay true for the older, loyal customers but also attract new shoppers. REED: It is a big challenge to reinvent yourself over and over again, right? You look at just, like, in the music industry, very few artists can continue being successful in the sophomore and junior album. You know, you have iconic artists like Madonna who can just reinvent herself every single time and speak to new audiences. SELYUKH: But it is extremely hard to do when you're not Madonna, you're Campbell's Soup. Alina Selyukh, NPR News. AUDIE CORNISH, HOST:  The world of mainstream consumer brands is in a slow-motion transformation. The companies that make products such as Campbell's Soup, Gillette razors, Crest toothpaste and Dove soap are going through major corporate changes, restructuring and hiring new CEOs. NPR's Alina Selyukh reports these mainstream brands are facing the challenge of adjusting to new shopping habits. ALINA SELYUKH, BYLINE: Think about the last time you went to the supermarket. You probably spent no more than a few seconds choosing from all the different brands of toothpaste or frozen peas or oatmeal. In retail, this is called, dramatically, the first moment of truth. For brands, these few seconds used to be their Holy grail, but in the past decade, shoppers started saying they'd go by other things. JULIET MCFADDEN: Usually by what's cheapest. Like, I'm not a brand person. SELYUKH: That's Juliet McFadden. She's an office manager in Boston, and she's 23. She's just starting to build her finances and lifelong shopping habits. This makes her a huge target for companies like Procter & Gamble, Kraft Heinz or General Mills, but she is not easy to win over. MCFADDEN: I'm not a cereal person. I just usually don't eat breakfast. I don't really drink soda ever. I don't like the yogurts with a ton of sugar in them that are super sweet. Paper towels are expensive. Stuff like that adds up. We have reusable, like, rags that we use and then wash. SELYUKH: McFadden's generation often gets the blame here, the tired trope about millennials killing breakfast cereals or napkins or canned tuna. But really, most Americans could make a similar list. Maybe you choose the store brand of toilet paper, buy a fancier condiment instead of Hellmann's mayo, order eco-friendly diapers on the Internet. Here's David Luttenberger of market research firm Mintel. DAVID LUTTENBERGER: Rather than just relying on brand familiarity, consumers buy today what performs for them. They are much less brand loyal, and they are more driven by performance, by convenience, by price. SELYUKH: At least two major things have changed us as shoppers. During the last recession, Americans warmed up to cheaper off-brand products like generic or store brands, and then they kept buying them even as the economy improved. And, of course, the Internet has completely shaken up our shopping. Think about how people used to learn about new brands. (SOUNDBITE OF AD) UNIDENTIFIED CHILD: (Singing) Oh, I'd love to be an Oscar Meyer wiener. SELYUKH: Only the biggest companies could afford catchy prime time TV ads. (SOUNDBITE OF AD) UNIDENTIFIED CHILD: (Singing) 'Cause if I were an Oscar Meyer wiener, everyone would be in love with me. SELYUKH: And so the boomer generation of shoppers grew up reaching for classic American brands. Now, Campbell's Soup, that symbol of the postwar era of processed foods, is restructuring as Americans are demanding fresher foods with pronounceable ingredients. Kraft Heinz got rid of artificial preservatives and dyes from its mac and cheese. Procter & Gamble lowered the price of Gillette razors for the first time in years to compete with the online startups like Dollar Shave Club. Unilever bought that startup, Dollar Shave Club. The mainstream brands are being squeezed by rivals that are both cheaper and more personalized. AMERICUS REED: They're in a bit of a pickle. SELYUKH: Americus Reed is a marketing professor at the University of Pennsylvania's Wharton School. He says legacy brands have to both stay true for the older, loyal customers but also attract new shoppers. REED: It is a big challenge to reinvent yourself over and over again, right? You look at just, like, in the music industry, very few artists can continue being successful in the sophomore and junior album. You know, you have iconic artists like Madonna who can just reinvent herself every single time and speak to new audiences. SELYUKH: But it is extremely hard to do when you're not Madonna, you're Campbell's Soup. Alina Selyukh, NPR News.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-01-681373274": {"title": "To The Dismay Of Free Speech Advocates, Vietnam Rolls Out Controversial Cyber Law : NPR", "url": "https://www.npr.org/2019/01/01/681373274/to-the-dismay-of-free-speech-advocates-vietnam-rolls-out-controversial-cyber-law", "author": "No author found", "published_date": "2019-01-01", "content": "", "section": "Asia", "disclaimer": ""}, "2019-01-01-680542096": {"title": "China To Explore Moon's Far Side With Its Lunar Lander : NPR", "url": "https://www.npr.org/2019/01/01/680542096/chinas-lunar-lander-to-explore-moons-far-side", "author": "No author found", "published_date": "2019-01-01", "content": "RACHEL MARTIN, HOST:  There's a new probe in orbit around the moon. And sometime earlier in this new year, it's expected to land on the Moon's far side, the side we never see from Earth. It is a Chinese probe. And as NPR's Joe Palca explains, although it's essentially a scientific mission, it's also laying the groundwork for sending Chinese astronauts to the moon. JOE PALCA, BYLINE: It only makes sense to send machines to the moon before trying to send humans. JIM HEAD: Before Neil Armstrong set foot on the moon, the United States sent 21 robotic missions to the moon to prepare the way. PALCA: Jim Head is a planetary scientist at Brown University. He says he's convinced the Chinese are doing the same thing. HEAD: The robotic program is providing all the kinds of background that you would need for building space suits and things like that, like lunar rovers. So they're building towards human exploration, for sure. PALCA: That's not to say there won't be interesting science to come from the mission. Briony Horgan is a planetary scientist at Purdue University. BRIONY HORGAN: This mission is really exciting because it's the first time any space agency will have landed on the far side of the moon. PALCA: Horgan says scientists know the far and near sides are very different. HORGAN: The far side is actually much more primitive. It contains really ancient crust that dates back to the very, very early solar system. There's rocks all over the far side that are over 4 billion years old. And now we're really excited to see what those look like up close. PALCA: Now, it may seem odd that, with so much to explore on the far side, all the landing probes to date have gone to the near side. HORGAN: That's mostly because it's a lot easier to communicate with, right? I mean, we can actually see the near side. And so we get direct radio communication with the entire near side of the moon. PALCA: Horgan says the Chinese have solved that problem by adding a third component to the mission. The first component is a lander. The lander carries the second component, a mobile rover. HORGAN: And then the third component is a satellite that's going to stay in orbit above the far side of the moon and act as a relay between the Earth and the far side of the moon. PALCA: It might be handy if U. S. lunar missions could also use the relay satellite. But that's not likely. Official collaboration between NASA and the Chinese space program is essentially prohibited by law. There are also restrictions on exporting U. S. space technology to China. But Brown University's Jim Head says some amount of collaboration just makes sense. HEAD: Why would we send a spacecraft to the same location to do exactly the same thing when we can optimize the amount of scientific return for the United States and for other countries by collaborating as best we can? PALCA: And while China may not be as forthcoming as NASA about what their civilian space program is up to, Head says they're not completely opaque, either. HEAD: If you talk to the right people, they are not holding things back. There are security issues there from their point of view, as well as ours. But nonetheless, they've been very forthcoming with their civilian space program. PALCA: The United States has its own lunar ambitions. The Trump administration has asked NASA to focus on returning humans to the moon. It's just possible there will already be Chinese astronauts there when they arrive. Joe Palca, NPR News. RACHEL MARTIN, HOST:   There's a new probe in orbit around the moon. And sometime earlier in this new year, it's expected to land on the Moon's far side, the side we never see from Earth. It is a Chinese probe. And as NPR's Joe Palca explains, although it's essentially a scientific mission, it's also laying the groundwork for sending Chinese astronauts to the moon. JOE PALCA, BYLINE: It only makes sense to send machines to the moon before trying to send humans. JIM HEAD: Before Neil Armstrong set foot on the moon, the United States sent 21 robotic missions to the moon to prepare the way. PALCA: Jim Head is a planetary scientist at Brown University. He says he's convinced the Chinese are doing the same thing. HEAD: The robotic program is providing all the kinds of background that you would need for building space suits and things like that, like lunar rovers. So they're building towards human exploration, for sure. PALCA: That's not to say there won't be interesting science to come from the mission. Briony Horgan is a planetary scientist at Purdue University. BRIONY HORGAN: This mission is really exciting because it's the first time any space agency will have landed on the far side of the moon. PALCA: Horgan says scientists know the far and near sides are very different. HORGAN: The far side is actually much more primitive. It contains really ancient crust that dates back to the very, very early solar system. There's rocks all over the far side that are over 4 billion years old. And now we're really excited to see what those look like up close. PALCA: Now, it may seem odd that, with so much to explore on the far side, all the landing probes to date have gone to the near side. HORGAN: That's mostly because it's a lot easier to communicate with, right? I mean, we can actually see the near side. And so we get direct radio communication with the entire near side of the moon. PALCA: Horgan says the Chinese have solved that problem by adding a third component to the mission. The first component is a lander. The lander carries the second component, a mobile rover. HORGAN: And then the third component is a satellite that's going to stay in orbit above the far side of the moon and act as a relay between the Earth and the far side of the moon. PALCA: It might be handy if U. S. lunar missions could also use the relay satellite. But that's not likely. Official collaboration between NASA and the Chinese space program is essentially prohibited by law. There are also restrictions on exporting U. S. space technology to China. But Brown University's Jim Head says some amount of collaboration just makes sense. HEAD: Why would we send a spacecraft to the same location to do exactly the same thing when we can optimize the amount of scientific return for the United States and for other countries by collaborating as best we can? PALCA: And while China may not be as forthcoming as NASA about what their civilian space program is up to, Head says they're not completely opaque, either. HEAD: If you talk to the right people, they are not holding things back. There are security issues there from their point of view, as well as ours. But nonetheless, they've been very forthcoming with their civilian space program. PALCA: The United States has its own lunar ambitions. The Trump administration has asked NASA to focus on returning humans to the moon. It's just possible there will already be Chinese astronauts there when they arrive. Joe Palca, NPR News.", "section": "Joe's Big Idea", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-02-681774551": {"title": "China Takes Wind Out Of Apple iPhone Sales : NPR", "url": "https://www.npr.org/2019/01/02/681774551/china-takes-wind-out-of-apple-iphone-sales", "author": "No author found", "published_date": "2019-01-02", "content": "", "section": "Business", "disclaimer": ""}, "2019-01-02-681605918": {"title": "Judge Dismisses San Bernardino Shooting Lawsuit Against Facebook, Google, Twitter : NPR", "url": "https://www.npr.org/2019/01/02/681605918/u-s-judge-dismisses-san-bernardino-shooting-lawsuit-against-facebook-google-twit", "author": "No author found", "published_date": "2019-01-02", "content": "", "section": "National", "disclaimer": ""}, "2019-01-03-678803790": {"title": "Berlin Is A Tech Hub, So Why Are Germany's Internet Speeds So Slow? : NPR", "url": "https://www.npr.org/2019/01/03/678803790/berlin-is-a-tech-hub-so-why-are-germanys-internet-speeds-so-slow", "author": "No author found", "published_date": "2019-01-03", "content": "ARI SHAPIRO, HOST:  Now it's time for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")SHAPIRO: New technologies have already disrupted the lives of taxi drivers, secretaries and business owners. We're going to spend the next few weeks looking at what's ripe for disruption next. We start in Germany. It's Europe's largest economy, and it could lose its edge because of sluggish internet connections. From Berlin, Esme Nicholson reports on the growing frustration with one of the world's biggest telecoms. UNIDENTIFIED PERSON: (Foreign language spoken). ESME NICHOLSON, BYLINE: The flourishing tech scene here in Berlin attracts talent from across the globe. At a startup incubator in the west of the city, an international team has just launched an app called SPRT, which, as the name suggests, connects sports enthusiasts. Amy Cooper, who came here from Britain in June, is editing a promotional video with her colleagues. Cooper, who's 20, says that Berlin's internet speed feels like the dial-up days her parents reminisce about. AMY COOPER: We're working in a co-working office where there's loads of startups. Everything's online. We use it, like, every second of the day, so it's so important for us that it works, and it's reliable. And most of the time, it's not, and that can be really frustrating. NICHOLSON: Claudia Engfeld from the Berlin Chamber of Commerce and Industry says 70 percent of the capital's businesses have complained to them about inadequate broadband. Engfeld says it's not only an issue for new tech companies but also for the city's established engineering firms. CLAUDIA ENGFELD: If you're doing 3D printing in an industrial level, you'll need to be fast. You can't wait two days till your machine has communicated to the printer to do something. This is a disadvantage that Berlin-based entrepreneurs have in comparison to other cities throughout the world. NICHOLSON: Engfeld says connections are so patchy in some parts of the city, companies have had to move premises or ask the staff to work from home. Many CEOs point the finger at Deutsche Telekom, the former state provider which still dominates the domestic market. Instead of installing fiber-optic cables, Deutsche Telekom decided to optimize the old copper telephone-wire system. ENGFELD: It's the technology that's the problem. In Germany, you will find almost everywhere copper cable that's not capable to go faster than 250 Mbit per second. The average reality is about 50 Mbit per second. That's quite poor. NICHOLSON: According to the OECD, less than 2 percent of Germany's broadband connections are carried by pure fiber-optic systems. Deutsche Telekom spokesperson Georg von Wagner insists that most customers don't need anything more. GEORG VON WAGNER: (Through interpreter) Of course, we could have concentrated on fiber-optic cables and given some people in Germany broadband speeds of up to a gigabit. But the rest of the country would only have an average connection of 16 megabits. NICHOLSON: Wagner says Deutsche Telekom compromised by improving its copper wire system, giving the majority of customers access to speeds of up to 50 megabits per second. Tech policy journalist Tomas Rudl says Telekom's fiber-optic strategy, or lack of one, was short-sighted. And He says that the government also has to take its share of the blame. TOMAS RUDL: (Through interpreter) Although Deutsche Telekom was privatized, the state is still one of its major shareholders. So it's within the government's own interest to ensure that its infrastructure policies and funding benefit a major German job provider. NICHOLSON: But it seems the government is catching on. Chancellor Angela Merkel, who only five years ago was mocked for describing the Internet as uncharted territory, has promised to help subsidize the rollout of fiber-optic broadband by 2025. For Amy Cooper at the Berlin startup SPRT, it's too little, too late. COOPER: Should you really have to think about those things in Berlin - you know, in one of the biggest and most important tech cities in Europe? NICHOLSON: And she says when the Internet invariably goes down, it's no use turning your mobile. She says Germany's cellphone system is just as bad. For NPR News, I'm Esme Nicholson in Berlin. (SOUNDBITE OF MINOTAUR SHOCK'S \"MY BURR\") ARI SHAPIRO, HOST:   Now it's time for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") SHAPIRO: New technologies have already disrupted the lives of taxi drivers, secretaries and business owners. We're going to spend the next few weeks looking at what's ripe for disruption next. We start in Germany. It's Europe's largest economy, and it could lose its edge because of sluggish internet connections. From Berlin, Esme Nicholson reports on the growing frustration with one of the world's biggest telecoms. UNIDENTIFIED PERSON: (Foreign language spoken). ESME NICHOLSON, BYLINE: The flourishing tech scene here in Berlin attracts talent from across the globe. At a startup incubator in the west of the city, an international team has just launched an app called SPRT, which, as the name suggests, connects sports enthusiasts. Amy Cooper, who came here from Britain in June, is editing a promotional video with her colleagues. Cooper, who's 20, says that Berlin's internet speed feels like the dial-up days her parents reminisce about. AMY COOPER: We're working in a co-working office where there's loads of startups. Everything's online. We use it, like, every second of the day, so it's so important for us that it works, and it's reliable. And most of the time, it's not, and that can be really frustrating. NICHOLSON: Claudia Engfeld from the Berlin Chamber of Commerce and Industry says 70 percent of the capital's businesses have complained to them about inadequate broadband. Engfeld says it's not only an issue for new tech companies but also for the city's established engineering firms. CLAUDIA ENGFELD: If you're doing 3D printing in an industrial level, you'll need to be fast. You can't wait two days till your machine has communicated to the printer to do something. This is a disadvantage that Berlin-based entrepreneurs have in comparison to other cities throughout the world. NICHOLSON: Engfeld says connections are so patchy in some parts of the city, companies have had to move premises or ask the staff to work from home. Many CEOs point the finger at Deutsche Telekom, the former state provider which still dominates the domestic market. Instead of installing fiber-optic cables, Deutsche Telekom decided to optimize the old copper telephone-wire system. ENGFELD: It's the technology that's the problem. In Germany, you will find almost everywhere copper cable that's not capable to go faster than 250 Mbit per second. The average reality is about 50 Mbit per second. That's quite poor. NICHOLSON: According to the OECD, less than 2 percent of Germany's broadband connections are carried by pure fiber-optic systems. Deutsche Telekom spokesperson Georg von Wagner insists that most customers don't need anything more. GEORG VON WAGNER: (Through interpreter) Of course, we could have concentrated on fiber-optic cables and given some people in Germany broadband speeds of up to a gigabit. But the rest of the country would only have an average connection of 16 megabits. NICHOLSON: Wagner says Deutsche Telekom compromised by improving its copper wire system, giving the majority of customers access to speeds of up to 50 megabits per second. Tech policy journalist Tomas Rudl says Telekom's fiber-optic strategy, or lack of one, was short-sighted. And He says that the government also has to take its share of the blame. TOMAS RUDL: (Through interpreter) Although Deutsche Telekom was privatized, the state is still one of its major shareholders. So it's within the government's own interest to ensure that its infrastructure policies and funding benefit a major German job provider. NICHOLSON: But it seems the government is catching on. Chancellor Angela Merkel, who only five years ago was mocked for describing the Internet as uncharted territory, has promised to help subsidize the rollout of fiber-optic broadband by 2025. For Amy Cooper at the Berlin startup SPRT, it's too little, too late. COOPER: Should you really have to think about those things in Berlin - you know, in one of the biggest and most important tech cities in Europe? NICHOLSON: And she says when the Internet invariably goes down, it's no use turning your mobile. She says Germany's cellphone system is just as bad. For NPR News, I'm Esme Nicholson in Berlin. (SOUNDBITE OF MINOTAUR SHOCK'S \"MY BURR\")", "section": "World", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-04-682247286": {"title": "Astronaut Dials 911 From Space; No First Responders Show Up : NPR", "url": "https://www.npr.org/2019/01/04/682247286/how-to-dial-911-from-space", "author": "No author found", "published_date": "2019-01-04", "content": "", "section": "Strange News", "disclaimer": ""}, "2019-01-04-682164820": {"title": "German Politicians Targeted In Sweeping Hacking Attack : NPR", "url": "https://www.npr.org/2019/01/04/682164820/hackers-attack-hundreds-of-high-profile-german-politicians-journalists", "author": "No author found", "published_date": "2019-01-04", "content": "", "section": "Europe", "disclaimer": ""}, "2019-01-05-682532583": {"title": "Ocean Cleanup Of Plastic Pollution In The Great Pacific Garbage Patch Breaks : NPR", "url": "https://www.npr.org/2019/01/05/682532583/an-engineering-wunderkinds-ocean-plastics-cleanup-device-hits-a-setback", "author": "No author found", "published_date": "2019-01-05", "content": "MICHEL MARTIN, HOST: We've been checking in every so often with Boyan Slat. He's the CEO of The Ocean Cleanup. That's an environmental organization he founded to develop technology to clean up plastic in the ocean. Now, we spoke with Mr. Slat back in September, just after his team launched a floating device designed to clean up that Great Pacific garbage patch. You'll remember that's a small island of trash between California and Hawaii. This is what he told us then. (SOUNDBITE OF ARCHIVED BROADCAST)BOYAN SLAT: Now the real test starts. And we're now, you know, in the next days few - and weeks will really decide whether we can prove the technology because that's really what's required to scale up and rid the oceans of plastic boy. MARTIN: Boyan Slat is with us now via Skype. Welcome back. Thanks so much for talking to us once again. SLAT: My pleasure. Thanks. MARTIN: Could you just remind us of what the goal is here? SLAT: Yes. So halfway between Hawaii in California is this area that's about twice the size of Texas, contains 1. 8 trillion pieces of plastic. And we hope to clean it up, removing about half of this patch every five years. We just launched this first system really to prove the technology, and if that works well, we hope to scale. MARTIN: So could you just describe the device itself for people who haven't had the opportunity to see it yet? It's like, what is it? It's like a boom, like a big rope or what is it? SLAT: So the device is a 2,000-feet-long floating barrier that's in a U-shape. And underneath there is a 10-feet-deep screen that's designed to capture the plastic that's not exactly at the surface. The plastic gets drawn towards the center like a funnel. And that way we first concentrate the plastic before we take it out. MARTIN: So we've been hearing there have been some troubles. What's been going on the past couple of weeks? SLAT: The pulmonary results have been that, on one hand, we have been able to see that the system is indeed propelled by the wind, that it can catch and concentrate plastic. But so far, we've seen two main issues that we hope to resolve in the coming months. The plastic occasionally drifts out of the system. And just last week, we noticed that a 60-feet-long end section of the cleanup system has separated from the rest of the system. So therefore we decided to bring back the system. It's on its way to Hawaii now for both repairs and upgrades. MARTIN: Do you have any sense of what is working as you hoped, and do you have any sense of what the problem is? SLAT: I think we are relatively close to getting it working. We have been able to already catch and concentrate plastic with the system. It's just that it's - sometimes the plastic is also escaping again. So likely what we have to do is we have to speed up the system so that it constantly moves faster than the plastic. And with regards to this material failure, likely we have to locally reinforce the system a bit. But I'm confident that the team will be able to design the appropriate solutions for this, and they'll have the system back in the patch in a few months from now. MARTIN: All right. Well, we'll check back in with you then. That's Boyan Slat, founder and CEO of The Ocean Cleanup. Thank you so much for talking to us once again. SLAT: My pleasure. Thank you. MICHEL MARTIN, HOST:  We've been checking in every so often with Boyan Slat. He's the CEO of The Ocean Cleanup. That's an environmental organization he founded to develop technology to clean up plastic in the ocean. Now, we spoke with Mr. Slat back in September, just after his team launched a floating device designed to clean up that Great Pacific garbage patch. You'll remember that's a small island of trash between California and Hawaii. This is what he told us then. (SOUNDBITE OF ARCHIVED BROADCAST) BOYAN SLAT: Now the real test starts. And we're now, you know, in the next days few - and weeks will really decide whether we can prove the technology because that's really what's required to scale up and rid the oceans of plastic boy. MARTIN: Boyan Slat is with us now via Skype. Welcome back. Thanks so much for talking to us once again. SLAT: My pleasure. Thanks. MARTIN: Could you just remind us of what the goal is here? SLAT: Yes. So halfway between Hawaii in California is this area that's about twice the size of Texas, contains 1. 8 trillion pieces of plastic. And we hope to clean it up, removing about half of this patch every five years. We just launched this first system really to prove the technology, and if that works well, we hope to scale. MARTIN: So could you just describe the device itself for people who haven't had the opportunity to see it yet? It's like, what is it? It's like a boom, like a big rope or what is it? SLAT: So the device is a 2,000-feet-long floating barrier that's in a U-shape. And underneath there is a 10-feet-deep screen that's designed to capture the plastic that's not exactly at the surface. The plastic gets drawn towards the center like a funnel. And that way we first concentrate the plastic before we take it out. MARTIN: So we've been hearing there have been some troubles. What's been going on the past couple of weeks? SLAT: The pulmonary results have been that, on one hand, we have been able to see that the system is indeed propelled by the wind, that it can catch and concentrate plastic. But so far, we've seen two main issues that we hope to resolve in the coming months. The plastic occasionally drifts out of the system. And just last week, we noticed that a 60-feet-long end section of the cleanup system has separated from the rest of the system. So therefore we decided to bring back the system. It's on its way to Hawaii now for both repairs and upgrades. MARTIN: Do you have any sense of what is working as you hoped, and do you have any sense of what the problem is? SLAT: I think we are relatively close to getting it working. We have been able to already catch and concentrate plastic with the system. It's just that it's - sometimes the plastic is also escaping again. So likely what we have to do is we have to speed up the system so that it constantly moves faster than the plastic. And with regards to this material failure, likely we have to locally reinforce the system a bit. But I'm confident that the team will be able to design the appropriate solutions for this, and they'll have the system back in the patch in a few months from now. MARTIN: All right. Well, we'll check back in with you then. That's Boyan Slat, founder and CEO of The Ocean Cleanup. Thank you so much for talking to us once again. SLAT: My pleasure. Thank you.", "section": "Environment", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-06-682714973": {"title": "Troll Watch: Online Harassment Toward Women : NPR", "url": "https://www.npr.org/2019/01/06/682714973/troll-watch-online-harassment-toward-women", "author": "No author found", "published_date": "2019-01-06", "content": "MICHEL MARTIN, HOST: And now we'll return to our Troll Watch series. (SOUNDBITE OF MUSIC)MARTIN: That's where we bring you stories of cybersecurity attacks, bots and, of course, Internet trolls. This week, we're going to hear about a report from the international human rights organization Amnesty International that focused on online abuse directed at women. Their report, called \"Troll Patrol,\" analyzed millions of tweets sent to women last year, and it found what many people already suspected - that women and particularly women of color are targeted for abuse on Twitter. Tanya O'Carroll is one of the report's authors, and she directs Amnesty Tech. That's an arm of the organization focused on the intersection of technology and human rights. And I started our conversation by asking her what made the organization decide to look at the environment for women on Twitter. TANYA O'CARROLL: Well, I think that just the space that we've been operating in as a human rights organization has changed so much in the past 10 years. So in the last couple of years, we've been documenting everything from the use of surveillance, spyware attacks against human rights defenders. And, on the other hand, it's stuff like manipulation of social media. So we tracked in Mexico, for example, the use of trolls to undermine and discredit very prominent critical voices of journalists and human rights defenders. And so it's about trying to upscale ourselves in order to really understand the way that state control and repression actually takes place now that the Internet is here. MARTIN: So tell us about some of the findings from the report that stood out for you. I just want to mention here that the model estimated that of the 14. 5 million tweets mentioning women, a million of them were abusive or, as you describe them, problematic. But you also said that some of the findings on race really stood out. Tell me a little bit more about that. O-CARROLL: Yeah. So we found that women of color were targeted more. So they were, in general, 34 percent more likely to receive abuse than white women, and black women particularly. So I think the most striking finding really is this - 84 percent more likely to receive abuse than white women. Asian women - 70 percent more likely to receive racist or ethnic slurs. I think this is not shocking to a lot of people in the sense that it's what we've been anecdotally hearing for a very long time. But what it is this time, we have now the evidence base to say that this is not - you know, this is not perception. MARTIN: Can I talk to you about the methodology from it? I understand that there was a combination of machine learning and crowd sourcing - that you got input from more than 6,000 online volunteers from around the world. The data is very compelling. But what about men? Did you compare to men? Because I can imagine where some might argue, fairly or unfairly, that is the Twitter-verse. That is, unfortunately, the culture of the place, and that men are equally likely to be attacked. And it may be unpleasant, but at least it's equal opportunity. What would you say to them? O-CARROLL: Yeah. We didn't actually look at men in this study. And this is partly because this study is the third big piece of research that we've done on the phenomenon of violence against women online. And so it's based off the fact that we already knew and we know that the way that women are targeted online is very different, and it's very gendered. It's stuff like doxing and hacking and violent rape threats. And so, in this case, we are very specifically interested in the experience of women because we know, just as offline discrimination and violence against women is rife. MARTIN: We reached out to Twitter for comment on the report, and an official responded via email - and I'll just read here, quoting - \"Twitter has publicly committed to improving the collective health, openness and civility of public conversation on our service\" - unquote. They added that abuse, malicious automation and manipulation detract from the health of Twitter and that we are committed to holding ourselves publicly accountable towards progress in this regard. I understand that members of the Amnesty International team have spoken with a Twitter CEO, Jack Dorsey, about the report and its findings. Can you share any more information about that? Was there any meeting of the minds about what might happen going forward? O-CARROLL: Yes. I mean, this is the interesting thing, right, which is that normally when amnesty are calling out a company, we're very much on the other side of the table. In this case, what we find from Twitter is that they get that this is a big issue. They acknowledge that this is happening on their platform. The question next is, how? How do we address it? How does Twitter address it? A lot of the abuse that we detected and that many people report should be taken down from the platform. The second issue is transparency. They've started to release some data, but the data isn't currently dis-aggregated. It doesn't tell us much about the number of tweets in total that are sent to women that may be abusive every year, how many of them are removed. And it was a - where those moderators are based? What languages did they speak? You know, Twitter is a global platform, and the abuse is global, too. MARTIN: I want to drill down just a little bit more on what you think the practical effect of what you've found is that causes us to rise to the level of a human rights issue. O-CARROLL: Yeah. I mean, the practical effect is censorship. You know, I think when we talk about censorship, so often people's instinct is to think about the protected speech - so the ability to offend, the ability to even abuse. We don't think very often about, what the censorship consequences for people who are regularly abused and intimidated and have essentially become scared over time to speak out online, to express themselves freely. Really, anytime a woman has the audacity to hold an opinion and express it in a proud or confident way online, they may become victim to a backlash and potentially an orchestrated backlash. So I think when we talk about this issue and think about the freedom of expression consequences, it's really important to realize whose freedom of expression are we prioritizing. MARTIN: That's Tanya O'Carroll, director of Amnesty Tech at Amnesty International. We reached her in Oxford. Tanya, thanks so much for talking with us. O-CARROLL: No worries. Thanks so much. MICHEL MARTIN, HOST:  And now we'll return to our Troll Watch series. (SOUNDBITE OF MUSIC) MARTIN: That's where we bring you stories of cybersecurity attacks, bots and, of course, Internet trolls. This week, we're going to hear about a report from the international human rights organization Amnesty International that focused on online abuse directed at women. Their report, called \"Troll Patrol,\" analyzed millions of tweets sent to women last year, and it found what many people already suspected - that women and particularly women of color are targeted for abuse on Twitter. Tanya O'Carroll is one of the report's authors, and she directs Amnesty Tech. That's an arm of the organization focused on the intersection of technology and human rights. And I started our conversation by asking her what made the organization decide to look at the environment for women on Twitter. TANYA O'CARROLL: Well, I think that just the space that we've been operating in as a human rights organization has changed so much in the past 10 years. So in the last couple of years, we've been documenting everything from the use of surveillance, spyware attacks against human rights defenders. And, on the other hand, it's stuff like manipulation of social media. So we tracked in Mexico, for example, the use of trolls to undermine and discredit very prominent critical voices of journalists and human rights defenders. And so it's about trying to upscale ourselves in order to really understand the way that state control and repression actually takes place now that the Internet is here. MARTIN: So tell us about some of the findings from the report that stood out for you. I just want to mention here that the model estimated that of the 14. 5 million tweets mentioning women, a million of them were abusive or, as you describe them, problematic. But you also said that some of the findings on race really stood out. Tell me a little bit more about that. O-CARROLL: Yeah. So we found that women of color were targeted more. So they were, in general, 34 percent more likely to receive abuse than white women, and black women particularly. So I think the most striking finding really is this - 84 percent more likely to receive abuse than white women. Asian women - 70 percent more likely to receive racist or ethnic slurs. I think this is not shocking to a lot of people in the sense that it's what we've been anecdotally hearing for a very long time. But what it is this time, we have now the evidence base to say that this is not - you know, this is not perception. MARTIN: Can I talk to you about the methodology from it? I understand that there was a combination of machine learning and crowd sourcing - that you got input from more than 6,000 online volunteers from around the world. The data is very compelling. But what about men? Did you compare to men? Because I can imagine where some might argue, fairly or unfairly, that is the Twitter-verse. That is, unfortunately, the culture of the place, and that men are equally likely to be attacked. And it may be unpleasant, but at least it's equal opportunity. What would you say to them? O-CARROLL: Yeah. We didn't actually look at men in this study. And this is partly because this study is the third big piece of research that we've done on the phenomenon of violence against women online. And so it's based off the fact that we already knew and we know that the way that women are targeted online is very different, and it's very gendered. It's stuff like doxing and hacking and violent rape threats. And so, in this case, we are very specifically interested in the experience of women because we know, just as offline discrimination and violence against women is rife. MARTIN: We reached out to Twitter for comment on the report, and an official responded via email - and I'll just read here, quoting - \"Twitter has publicly committed to improving the collective health, openness and civility of public conversation on our service\" - unquote. They added that abuse, malicious automation and manipulation detract from the health of Twitter and that we are committed to holding ourselves publicly accountable towards progress in this regard. I understand that members of the Amnesty International team have spoken with a Twitter CEO, Jack Dorsey, about the report and its findings. Can you share any more information about that? Was there any meeting of the minds about what might happen going forward? O-CARROLL: Yes. I mean, this is the interesting thing, right, which is that normally when amnesty are calling out a company, we're very much on the other side of the table. In this case, what we find from Twitter is that they get that this is a big issue. They acknowledge that this is happening on their platform. The question next is, how? How do we address it? How does Twitter address it? A lot of the abuse that we detected and that many people report should be taken down from the platform. The second issue is transparency. They've started to release some data, but the data isn't currently dis-aggregated. It doesn't tell us much about the number of tweets in total that are sent to women that may be abusive every year, how many of them are removed. And it was a - where those moderators are based? What languages did they speak? You know, Twitter is a global platform, and the abuse is global, too. MARTIN: I want to drill down just a little bit more on what you think the practical effect of what you've found is that causes us to rise to the level of a human rights issue. O-CARROLL: Yeah. I mean, the practical effect is censorship. You know, I think when we talk about censorship, so often people's instinct is to think about the protected speech - so the ability to offend, the ability to even abuse. We don't think very often about, what the censorship consequences for people who are regularly abused and intimidated and have essentially become scared over time to speak out online, to express themselves freely. Really, anytime a woman has the audacity to hold an opinion and express it in a proud or confident way online, they may become victim to a backlash and potentially an orchestrated backlash. So I think when we talk about this issue and think about the freedom of expression consequences, it's really important to realize whose freedom of expression are we prioritizing. MARTIN: That's Tanya O'Carroll, director of Amnesty Tech at Amnesty International. We reached her in Oxford. Tanya, thanks so much for talking with us. O-CARROLL: No worries. Thanks so much.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-06-682607997": {"title": "Viral Hashtag Celebrates Palestinian-American Representation : NPR", "url": "https://www.npr.org/2019/01/06/682607997/viral-hashtag-celebrates-palestinian-american-representation", "author": "No author found", "published_date": "2019-01-06", "content": "LEILA FADEL, HOST:  Last week, the 116th Congress was sworn in. And it's the most diverse in American history. It includes the first Muslim women, the first Native American women and the youngest woman ever to serve in Congress. And during their swearing in, some new members chose to celebrate their heritage with traditional garments. Michigan Democrat Rashida Tlaib, one of two Muslim women sworn into Congress, wore a thobe to honor her Palestinian background. The thobe is a gown with elaborate, cross-stitched embroidery. On social media, her supporters did the same. They tweeted themselves using the hashtag #TweetYourThobe to congratulate the new congresswoman and to share their own stories. Susan Muaddi Darraj is the founder of that campaign. She's a writer and professor at Harford Community College in Maryland. She joins us now from member station WYPR in Baltimore. Welcome. SUSAN MUADDI DARRAJ: Thank you for having me. FADEL: Well, I guess, for those who don't know, can you talk about what a Palestinian thobe is for women? DARRAJ: Sure. Well, a thobe is - it simply means a dress in Arabic. And they are very special dresses. And they're worn at special occasions - like baptisms, weddings, graduations. FADEL: Right. DARRAJ: They're gorgeous. And every region has its own color patterns and particular preferred fabrics. They're made by hand. And they're often passed down from mother to daughter. I thought it was wonderful that the congresswoman decided to wear hers at her swearing-in. FADEL: So she's really wearing her history on her body, basically. DARRAJ: Yes, and I believe her dress was, actually, handmade by her mother for her. So that makes it even more special. FADEL: You came up with this hashtag #TweetYourThobe. How did this all start? DARRAJ: So I was really excited when I heard that she was going to wear her dress at her swearing-in. And then the backlash on Twitter was immediate and fierce. People were calling it un-American, nasty comments about promoting Sharia law and Palestinian heritage and these sorts of unbelievable things. FADEL: Because she's Muslim and because she's Arab. DARRAJ: Because she's Muslim, right. And so what can I say? I mean, for example, there's so much diversity among Palestinians. I'm a Palestinian Christian, so I decided that I wanted to try to promote awareness of what this dress means and a little bit about Palestinian culture. So I had this movement to sort of educate people and also celebrate her achievement. There were people who participated in our campaign who were Jewish, who were Muslim, who were Christian, Buddhists, who were atheists. FADEL: What were your favorites? Or what were the things that you were seeing? DARRAJ: I saw several women who posted pictures of their thobes that had been made by their great-grandmothers. One woman posted a thobe that was 100 years old. People asking questions about what the dresses mean. Actually, the other day, we just posted a map of Palestinian areas that somebody made. And on the map is superimposed the pieces of fabric, so you can look at a dress and see where the person is from based on the map. FADEL: Oh, wow. And what did you tweet? DARRAJ: I tweeted a picture of myself wearing a beautiful thobe that I just received last year from my aunt. And she had a thobe made for me in the West Bank. And I was thrilled to wear that. FADEL: So it wasn't just Congresswoman Tlaib who wore cultural garments. Representative Deb Haaland, who's one of the first Native American women in Congress along with Sharice Davids - she wore a traditional Pueblo dress. Representative Ilhan Omar - she's the first woman Somali-American Muslim to wear hijab - the religious head covering - in Congress. So it's a record year for women, women of color. You're a woman of color. What was it like to see that? DARRAJ: I love seeing these women bring their heritage with them to the government. You know, we need to recognize that there are differences among us, but those differences don't have to be obstacles between us. They can be moments for celebration. FADEL: That was Susan Muaddi Darraj, founder of the social media campaign #TweetYourThobe. Thanks for talking with us. DARRAJ: Thank you for having me. LEILA FADEL, HOST:   Last week, the 116th Congress was sworn in. And it's the most diverse in American history. It includes the first Muslim women, the first Native American women and the youngest woman ever to serve in Congress. And during their swearing in, some new members chose to celebrate their heritage with traditional garments. Michigan Democrat Rashida Tlaib, one of two Muslim women sworn into Congress, wore a thobe to honor her Palestinian background. The thobe is a gown with elaborate, cross-stitched embroidery. On social media, her supporters did the same. They tweeted themselves using the hashtag #TweetYourThobe to congratulate the new congresswoman and to share their own stories. Susan Muaddi Darraj is the founder of that campaign. She's a writer and professor at Harford Community College in Maryland. She joins us now from member station WYPR in Baltimore. Welcome. SUSAN MUADDI DARRAJ: Thank you for having me. FADEL: Well, I guess, for those who don't know, can you talk about what a Palestinian thobe is for women? DARRAJ: Sure. Well, a thobe is - it simply means a dress in Arabic. And they are very special dresses. And they're worn at special occasions - like baptisms, weddings, graduations. FADEL: Right. DARRAJ: They're gorgeous. And every region has its own color patterns and particular preferred fabrics. They're made by hand. And they're often passed down from mother to daughter. I thought it was wonderful that the congresswoman decided to wear hers at her swearing-in. FADEL: So she's really wearing her history on her body, basically. DARRAJ: Yes, and I believe her dress was, actually, handmade by her mother for her. So that makes it even more special. FADEL: You came up with this hashtag #TweetYourThobe. How did this all start? DARRAJ: So I was really excited when I heard that she was going to wear her dress at her swearing-in. And then the backlash on Twitter was immediate and fierce. People were calling it un-American, nasty comments about promoting Sharia law and Palestinian heritage and these sorts of unbelievable things. FADEL: Because she's Muslim and because she's Arab. DARRAJ: Because she's Muslim, right. And so what can I say? I mean, for example, there's so much diversity among Palestinians. I'm a Palestinian Christian, so I decided that I wanted to try to promote awareness of what this dress means and a little bit about Palestinian culture. So I had this movement to sort of educate people and also celebrate her achievement. There were people who participated in our campaign who were Jewish, who were Muslim, who were Christian, Buddhists, who were atheists. FADEL: What were your favorites? Or what were the things that you were seeing? DARRAJ: I saw several women who posted pictures of their thobes that had been made by their great-grandmothers. One woman posted a thobe that was 100 years old. People asking questions about what the dresses mean. Actually, the other day, we just posted a map of Palestinian areas that somebody made. And on the map is superimposed the pieces of fabric, so you can look at a dress and see where the person is from based on the map. FADEL: Oh, wow. And what did you tweet? DARRAJ: I tweeted a picture of myself wearing a beautiful thobe that I just received last year from my aunt. And she had a thobe made for me in the West Bank. And I was thrilled to wear that. FADEL: So it wasn't just Congresswoman Tlaib who wore cultural garments. Representative Deb Haaland, who's one of the first Native American women in Congress along with Sharice Davids - she wore a traditional Pueblo dress. Representative Ilhan Omar - she's the first woman Somali-American Muslim to wear hijab - the religious head covering - in Congress. So it's a record year for women, women of color. You're a woman of color. What was it like to see that? DARRAJ: I love seeing these women bring their heritage with them to the government. You know, we need to recognize that there are differences among us, but those differences don't have to be obstacles between us. They can be moments for celebration. FADEL: That was Susan Muaddi Darraj, founder of the social media campaign #TweetYourThobe. Thanks for talking with us. DARRAJ: Thank you for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-06-682608011": {"title": "U.S. Army Tries to Recruit Gen Z to Military With New Strategies, Like E-Sports : NPR", "url": "https://www.npr.org/2019/01/06/682608011/after-falling-short-u-s-army-gets-creative-with-new-recruiting-strategy", "author": "No author found", "published_date": "2019-01-06", "content": "LEILA FADEL, HOST:  In the last fiscal year, the Army fell short of its recruiting goal by 6,500 people. In part, it's driven by the 50-year lows in unemployment numbers. Often, young recruits join when they don't have a job prospect or money for college. And so the Army is getting creative about selling itself, shifting its focus from places that traditionally fill its ranks - conservative parts of the country from Virginia through the South and into Texas - to 22 left-leaning cities across the country, like Boston, Seattle and San Francisco. Joining us now to discuss this new strategy is Gen. Frank Muth, the head of the Army Recruiting Command. Gen. Muth, thanks so much for being with us. FRANK MUTH: Thank you so much for taking the time to have a discussion about this great topic. FADEL: So why don't you start by telling me - what's the thinking behind this strategy? It's a pretty big pivot. MUTH: It's a huge pivot. So with us not making mission last year, we wanted to truly look at some of the issues that were going on to try to accomplish the mission for this year. And we started with how we were recruiting. Calling the Z Generation on the phone doesn't work anymore. FADEL: Right. MUTH: They want to meet us online first, find out, one, if - are we a bot that's just probing them for information? Or are we a real person? And once that dialogue starts online, then it leads to a discussion on the phone. And then that leads to an interview. And it all starts with providing information about, you know, the Army's got 150 different jobs. There's the GI Bill when they get out to pay for college - all the different benefits for serving the military. But it has to start with reaching the Z Generation on the digital plane. FADEL: So when you're shifting to these urban areas, is there anything you're doing specifically to talk to urban minorities about this, especially at a time where the military's being leaned on at the border and that get sometimes seen as part of a politicized, racially motivated fight between the administration and his opponents? MUTH: Yeah, no. You know, the recruiters talk about the opportunities that exist in the Army, all the different specialties, all the different training, the qualifications that you get while you're in the Army and what you leave with. FADEL: In urban spaces, among women, among minority communities who must ask questions about issues of racism and sexism in the military, what do your recruiters say to those questions? MUTH: You know, I'm sure they tell stories that are very positive. I mean, because, you know, we don't hear about it. So they tell their Army story, essentially. FADEL: But there are stories where certain communities are treated differently at times. And I'm just wondering how they allay those fears and how they say you are also welcome. MUTH: You know, I'm not going to put words in their mouths. But I would suspect that they say. . . FADEL: How would you do it? MUTH: How would I do it? FADEL: Mmm hmm. MUTH: There is a very, very low percentage of those incidents or events that occur. But know this - that it's one team. And if any of that occurs, it is immediately addressed by the chain of command because, one, we want to create a safe and secure environment and dignity and respect for everybody in the United States Army. So that is pervasive throughout the commands. FADEL: The other thing is, you know, the military's been mired in wars for a lot of this generation's lives - right? - Iraq, Afghanistan, the longest-running war. Is it a difficult sell because - if there is real danger for young people that will be going out there to possibly die for their country? MUTH: You're right. That's out there. It's a very small percentage that are, one, participating and, two, that participate in direct combat operations alone. FADEL: Right. MUTH: There is risk in anything. But it's a low risk in terms of being part of the direct combat fight. FADEL: So what's the number you need to reach this year? MUTH: We're still working on that. It won't be 76,500, which was our mission last year. I think it's going to be a lower number. FADEL: Gen. Frank Muth of the Army Recruiting Command, thank you so much. MUTH: It has been my honor and pleasure. And thank you for the time. LEILA FADEL, HOST:   In the last fiscal year, the Army fell short of its recruiting goal by 6,500 people. In part, it's driven by the 50-year lows in unemployment numbers. Often, young recruits join when they don't have a job prospect or money for college. And so the Army is getting creative about selling itself, shifting its focus from places that traditionally fill its ranks - conservative parts of the country from Virginia through the South and into Texas - to 22 left-leaning cities across the country, like Boston, Seattle and San Francisco. Joining us now to discuss this new strategy is Gen. Frank Muth, the head of the Army Recruiting Command. Gen. Muth, thanks so much for being with us. FRANK MUTH: Thank you so much for taking the time to have a discussion about this great topic. FADEL: So why don't you start by telling me - what's the thinking behind this strategy? It's a pretty big pivot. MUTH: It's a huge pivot. So with us not making mission last year, we wanted to truly look at some of the issues that were going on to try to accomplish the mission for this year. And we started with how we were recruiting. Calling the Z Generation on the phone doesn't work anymore. FADEL: Right. MUTH: They want to meet us online first, find out, one, if - are we a bot that's just probing them for information? Or are we a real person? And once that dialogue starts online, then it leads to a discussion on the phone. And then that leads to an interview. And it all starts with providing information about, you know, the Army's got 150 different jobs. There's the GI Bill when they get out to pay for college - all the different benefits for serving the military. But it has to start with reaching the Z Generation on the digital plane. FADEL: So when you're shifting to these urban areas, is there anything you're doing specifically to talk to urban minorities about this, especially at a time where the military's being leaned on at the border and that get sometimes seen as part of a politicized, racially motivated fight between the administration and his opponents? MUTH: Yeah, no. You know, the recruiters talk about the opportunities that exist in the Army, all the different specialties, all the different training, the qualifications that you get while you're in the Army and what you leave with. FADEL: In urban spaces, among women, among minority communities who must ask questions about issues of racism and sexism in the military, what do your recruiters say to those questions? MUTH: You know, I'm sure they tell stories that are very positive. I mean, because, you know, we don't hear about it. So they tell their Army story, essentially. FADEL: But there are stories where certain communities are treated differently at times. And I'm just wondering how they allay those fears and how they say you are also welcome. MUTH: You know, I'm not going to put words in their mouths. But I would suspect that they say. . . FADEL: How would you do it? MUTH: How would I do it? FADEL: Mmm hmm. MUTH: There is a very, very low percentage of those incidents or events that occur. But know this - that it's one team. And if any of that occurs, it is immediately addressed by the chain of command because, one, we want to create a safe and secure environment and dignity and respect for everybody in the United States Army. So that is pervasive throughout the commands. FADEL: The other thing is, you know, the military's been mired in wars for a lot of this generation's lives - right? - Iraq, Afghanistan, the longest-running war. Is it a difficult sell because - if there is real danger for young people that will be going out there to possibly die for their country? MUTH: You're right. That's out there. It's a very small percentage that are, one, participating and, two, that participate in direct combat operations alone. FADEL: Right. MUTH: There is risk in anything. But it's a low risk in terms of being part of the direct combat fight. FADEL: So what's the number you need to reach this year? MUTH: We're still working on that. It won't be 76,500, which was our mission last year. I think it's going to be a lower number. FADEL: Gen. Frank Muth of the Army Recruiting Command, thank you so much. MUTH: It has been my honor and pleasure. And thank you for the time.", "section": "National", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-08-683037354": {"title": "'Bye Bye Twitter Und Facebook': German Green Party Chief Quits Social Media : NPR", "url": "https://www.npr.org/2019/01/08/683037354/bye-bye-twitter-und-facebook-german-green-party-chief-quits-social-media", "author": "No author found", "published_date": "2019-01-08", "content": "", "section": "Europe", "disclaimer": ""}, "2019-01-10-683009762": {"title": "Alexa Helps With Homework, But Problem-Solving Skills Are Key : NPR", "url": "https://www.npr.org/2019/01/10/683009762/alexa-can-help-kids-with-homework-but-dont-forget-problem-solving-skills", "author": "No author found", "published_date": "2019-01-10", "content": "DAVID GREENE, HOST:  There's a video that went viral on Twitter recently. It has a mom sneaking up on her 6-year-old son as he's doing his math homework. And she catches him asking Alexa, Amazon's smart speaker, for help. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED CHILD: Alexa, what's five minus three? GREENE: What's five minus three? Well, Alexa gives him the answer. And his mom, who's listening in the background, chastises him. This is all pretty cute. But it gets at a deeper question, right? As kids have more access to virtual assistance, does that interfere with the learning process? Here's NPR's Jasmine Garsd. JASMINE GARSD, BYLINE: Clint Hill is an English teacher at Patrick Henry High School in Roanoke, Va. He says in his classroom, this often happens. CLINT HILL: Kids quietly talking into their phones and asking Google or other services, hey. How do you spell - some complicated word that they don't know. GARSD: Hill, who co-hosts the education podcast Schooled Ya! , says he actually doesn't mind. HILL: I struggle with spelling. And spell check on my word processing has been a lifesaver for me. And I think being able to use those technological aids is not hurting anybody. I think it is just improving our ability to use our brains for other things. GARSD: This is one of the big debates in education today. On the one hand, why deprive kids of technology most adults use every day? But some experts say it's not just about learning basic math or spelling. DIANE LEVIN: One of the best gifts we can give our children is doing that kind of problem-solving together. . . GARSD: Diane Levin is a professor of applied human development at Boston University and the founder of the nonprofit TRUCE, or Teachers Resisting Unhealthy Children's Entertainment. LEVIN: . . . Because they will use those skills that they're learning for all kinds of things that come along, where, if they're a good problem-solver, they'll do better than kids who just try to go to a screen to get the answer. GARSD: Levin believes not allowing a child to even struggle a little for the answers leads to what she calls Problem Solving Deficit Disorder. Dimitri Christakis is the director of the Center for Child Health, Behavior and Development at Seattle's Children's Research Institute. DIMITRI CHRISTAKIS: There is reason to be concerned but not panicked. And there's also reason to be optimistic and hopeful. It's really about how we deploy these technologies. GARSD: Christakis says every wave of technology elicits a panic about its effect on children and nostalgia over a more wholesome past. Consider this old clip of Kermit the Frog stuck on an elevator with \"Sesame Street's\" Count von Count, who is maniacally counting the floors. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTOR #1: (As Count von Count) Eighth floor, ninth floor. . . UNIDENTIFIED ACTOR #2: (As Kermit the Frog) Wait a second, Count. I wanted to get off on the seventh floor. UNIDENTIFIED ACTOR #1: (As Count von Count) Ten - that's 10. . . GARSD: It's sweet and educational. But Christakis points out that a child watching television - it's a completely passive experience. And he says, for children, the interactive aspect of new technology. . . CHRISTAKIS: It helps them understand how the world works. And whereas watching television, of course, doesn't allow that to happen because you play no role in the content, interacting with touchscreens and, for that matter, interacting with these voice-activated technologies, allows that to happen in spades. GARSD: Still, he agrees that this debate is about much more than knowing what five minus three is. It's about developing the patience to solve problems. CHRISTAKIS: That ability to stay focused, particularly when something is not interesting, is one of the most important developmental skills that children acquire. GARSD: In other words, it's not just about having the answers. It's about the work you put in to get them. Jasmine Garsd, NPR News, New York. DAVID GREENE, HOST:   There's a video that went viral on Twitter recently. It has a mom sneaking up on her 6-year-old son as he's doing his math homework. And she catches him asking Alexa, Amazon's smart speaker, for help. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED CHILD: Alexa, what's five minus three? GREENE: What's five minus three? Well, Alexa gives him the answer. And his mom, who's listening in the background, chastises him. This is all pretty cute. But it gets at a deeper question, right? As kids have more access to virtual assistance, does that interfere with the learning process? Here's NPR's Jasmine Garsd. JASMINE GARSD, BYLINE: Clint Hill is an English teacher at Patrick Henry High School in Roanoke, Va. He says in his classroom, this often happens. CLINT HILL: Kids quietly talking into their phones and asking Google or other services, hey. How do you spell - some complicated word that they don't know. GARSD: Hill, who co-hosts the education podcast Schooled Ya! , says he actually doesn't mind. HILL: I struggle with spelling. And spell check on my word processing has been a lifesaver for me. And I think being able to use those technological aids is not hurting anybody. I think it is just improving our ability to use our brains for other things. GARSD: This is one of the big debates in education today. On the one hand, why deprive kids of technology most adults use every day? But some experts say it's not just about learning basic math or spelling. DIANE LEVIN: One of the best gifts we can give our children is doing that kind of problem-solving together. . . GARSD: Diane Levin is a professor of applied human development at Boston University and the founder of the nonprofit TRUCE, or Teachers Resisting Unhealthy Children's Entertainment. LEVIN: . . . Because they will use those skills that they're learning for all kinds of things that come along, where, if they're a good problem-solver, they'll do better than kids who just try to go to a screen to get the answer. GARSD: Levin believes not allowing a child to even struggle a little for the answers leads to what she calls Problem Solving Deficit Disorder. Dimitri Christakis is the director of the Center for Child Health, Behavior and Development at Seattle's Children's Research Institute. DIMITRI CHRISTAKIS: There is reason to be concerned but not panicked. And there's also reason to be optimistic and hopeful. It's really about how we deploy these technologies. GARSD: Christakis says every wave of technology elicits a panic about its effect on children and nostalgia over a more wholesome past. Consider this old clip of Kermit the Frog stuck on an elevator with \"Sesame Street's\" Count von Count, who is maniacally counting the floors. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED ACTOR #1: (As Count von Count) Eighth floor, ninth floor. . . UNIDENTIFIED ACTOR #2: (As Kermit the Frog) Wait a second, Count. I wanted to get off on the seventh floor. UNIDENTIFIED ACTOR #1: (As Count von Count) Ten - that's 10. . . GARSD: It's sweet and educational. But Christakis points out that a child watching television - it's a completely passive experience. And he says, for children, the interactive aspect of new technology. . . CHRISTAKIS: It helps them understand how the world works. And whereas watching television, of course, doesn't allow that to happen because you play no role in the content, interacting with touchscreens and, for that matter, interacting with these voice-activated technologies, allows that to happen in spades. GARSD: Still, he agrees that this debate is about much more than knowing what five minus three is. It's about developing the patience to solve problems. CHRISTAKIS: That ability to stay focused, particularly when something is not interesting, is one of the most important developmental skills that children acquire. GARSD: In other words, it's not just about having the answers. It's about the work you put in to get them. Jasmine Garsd, NPR News, New York.", "section": "Education", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-11-684723473": {"title": "SpaceX To Lay Off 10 Percent Of Its Workforce : NPR", "url": "https://www.npr.org/2019/01/11/684723473/spacex-to-lay-off-10-percent-of-its-workforce", "author": "No author found", "published_date": "2019-01-11", "content": "", "section": "Space", "disclaimer": ""}, "2019-01-11-684378595": {"title": "Polish Police Arrest Huawei Executive On Suspicion Of Spying For China : NPR", "url": "https://www.npr.org/2019/01/11/684378595/polish-police-arrest-huawei-executive-accused-of-spying-for-china", "author": "No author found", "published_date": "2019-01-11", "content": "", "section": "Asia", "disclaimer": ""}, "2019-01-12-684854958": {"title": "High-Tech Vibrator Ban From CES Show Stirs Claims Of Sexism : NPR", "url": "https://www.npr.org/2019/01/12/684854958/high-tech-vibrator-ban-from-ces-show-stirs-claims-of-sexism", "author": "No author found", "published_date": "2019-01-12", "content": "SARAH MCCAMMON, HOST: The Consumer Electronics Show, or CES, just wrapped up this week in Las Vegas. It featured the usual assortment of virtual reality goggles, smart cars, next generation smartphones. But arguably, the biggest buzz was about a product geared toward women that was conspicuously absent from the showroom floor. And here's where we want to mention that the conversation we're about to have may not be appropriate for younger listeners. For more, we turn now to Emily Dreyfuss, a senior staff writer for Wired. Emily, thanks for joining us. EMILY DREYFUSS: Thanks for having me. MCCAMMON: OK, so here's the big reveal, now that hopefully all the little ones are out of the room. The banned product was a robotic vibrator. And before it was banned, it actually won an innovation award. For those who haven't been following this story, Emily, can you tell us what all went down? DREYFUSS: Yes. So the product is called the Ose, and it's a vibrator that uses micro-robotics and biomimicry. Now, the creators of this device, Lora DiCarlo, had submitted this to CES robotics category, into which it was accepted, and then they actually gave it an innovation award. And that's an award that is given by a jury of experts before you show up to the show. But then, before the company was able to show up to exhibit at CES in Las Vegas, CTA, who are the leadership behind CBS, changed their mind, sent the company an email saying that actually, they had decided that this device did not fit into the robotics category and, in fact, was going to be excluded from the show floor because it was deemed to be either immoral, obscene, indecent or profane. MCCAMMON: And that has struck people as a sexist move, right, given that one of the most talked about products at last year's CES was, to put it bluntly, a sex robot. And virtual reality pornography has been featured in the past as well. What do you make of all this? DREYFUSS: Exactly. I think that the hypocrisy is one of the biggest reasons why this has gotten so much attention. They have gone out of their way over the years to not be a sex device show. It's not like every year there are tons of sex gadgets. But they have over the years had some, especially if they had some sort of interesting technology to offer, which this device clearly does. It has, you know, a lot of interesting 3D printing and rapid prototyping that went into it that really does justify it as a technology device. MCCAMMON: The health category at CES includes lots of products geared toward pregnancy, motherhood, early parenthood. I mean, how much of the blowback here is about what was allowed in the show compared to what was actually banned? DREYFUSS: Yeah. So I think that's another double standard. You know, five years ago, you really wouldn't have found hardly any devices that were specifically tailored to women. But now there has been this change. You know, the market has recognized that mothers and new parents represent a very lucrative category for innovation. This year, there were all sorts of devices catering specifically to women as mothers. And that's on the one hand wonderful. But, on the other hand, the double standard that there were breast massagers on the show floor this year that were geared toward treating women who are breastfeeding and experiencing mastitis, which is a absolutely legitimate and important device - that is allowable, whereas a device that gives pleasure is not allowable. It really does just play into existing stereotypes. Shows like this have the ability to legitimize a topic so that scientific grants can give money toward the study of female sexuality and venture capitalists feel comfortable giving money to companies that are geared toward women and their pleasure. MCCAMMON: Emily Dreyfuss, senior writer for Wired. Thank you so much. DREYFUSS: Thank you. MCCAMMON: And we should add, Emily Dreyfuss tells us the Consumer Technology Association, which runs the CES, did not respond to her requests for comment. (SOUNDBITE OF MUSIC)MCCAMMON: This is NPR News. SARAH MCCAMMON, HOST:  The Consumer Electronics Show, or CES, just wrapped up this week in Las Vegas. It featured the usual assortment of virtual reality goggles, smart cars, next generation smartphones. But arguably, the biggest buzz was about a product geared toward women that was conspicuously absent from the showroom floor. And here's where we want to mention that the conversation we're about to have may not be appropriate for younger listeners. For more, we turn now to Emily Dreyfuss, a senior staff writer for Wired. Emily, thanks for joining us. EMILY DREYFUSS: Thanks for having me. MCCAMMON: OK, so here's the big reveal, now that hopefully all the little ones are out of the room. The banned product was a robotic vibrator. And before it was banned, it actually won an innovation award. For those who haven't been following this story, Emily, can you tell us what all went down? DREYFUSS: Yes. So the product is called the Ose, and it's a vibrator that uses micro-robotics and biomimicry. Now, the creators of this device, Lora DiCarlo, had submitted this to CES robotics category, into which it was accepted, and then they actually gave it an innovation award. And that's an award that is given by a jury of experts before you show up to the show. But then, before the company was able to show up to exhibit at CES in Las Vegas, CTA, who are the leadership behind CBS, changed their mind, sent the company an email saying that actually, they had decided that this device did not fit into the robotics category and, in fact, was going to be excluded from the show floor because it was deemed to be either immoral, obscene, indecent or profane. MCCAMMON: And that has struck people as a sexist move, right, given that one of the most talked about products at last year's CES was, to put it bluntly, a sex robot. And virtual reality pornography has been featured in the past as well. What do you make of all this? DREYFUSS: Exactly. I think that the hypocrisy is one of the biggest reasons why this has gotten so much attention. They have gone out of their way over the years to not be a sex device show. It's not like every year there are tons of sex gadgets. But they have over the years had some, especially if they had some sort of interesting technology to offer, which this device clearly does. It has, you know, a lot of interesting 3D printing and rapid prototyping that went into it that really does justify it as a technology device. MCCAMMON: The health category at CES includes lots of products geared toward pregnancy, motherhood, early parenthood. I mean, how much of the blowback here is about what was allowed in the show compared to what was actually banned? DREYFUSS: Yeah. So I think that's another double standard. You know, five years ago, you really wouldn't have found hardly any devices that were specifically tailored to women. But now there has been this change. You know, the market has recognized that mothers and new parents represent a very lucrative category for innovation. This year, there were all sorts of devices catering specifically to women as mothers. And that's on the one hand wonderful. But, on the other hand, the double standard that there were breast massagers on the show floor this year that were geared toward treating women who are breastfeeding and experiencing mastitis, which is a absolutely legitimate and important device - that is allowable, whereas a device that gives pleasure is not allowable. It really does just play into existing stereotypes. Shows like this have the ability to legitimize a topic so that scientific grants can give money toward the study of female sexuality and venture capitalists feel comfortable giving money to companies that are geared toward women and their pleasure. MCCAMMON: Emily Dreyfuss, senior writer for Wired. Thank you so much. DREYFUSS: Thank you. MCCAMMON: And we should add, Emily Dreyfuss tells us the Consumer Technology Association, which runs the CES, did not respond to her requests for comment. (SOUNDBITE OF MUSIC) MCCAMMON: This is NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-12-684748454": {"title": "Privacy Concerns Largely Ignored At Annual Consumer Electronics Show : NPR", "url": "https://www.npr.org/2019/01/12/684748454/privacy-concerns-largely-ignored-at-annual-consumer-electronics-show", "author": "No author found", "published_date": "2019-01-12", "content": "SCOTT SIMON, HOST: The massive Consumer Electronics Show in Las Vegas wrapped up yesterday. Tech companies showcased their latest gadgets and predicted trends. But something you didn't hear, according to our next guest, was a frank conversation about privacy. Tech writer Pete Pachal wrote about this from Mashable. He just got back from CES and joins us from New York. Thanks so much for being with us. PETE PACHAL: Hey, my pleasure. SIMON: So many scandals over the past year involving platforms violating privacy, data harvesting, data breaches. What were people talking about, if not that? PACHAL: Well, they were talking about a lot of hype for the new technologies just around the corner. I mean, CES is very much a kind of a tech bubble in some ways. So it's nonstop hype about new technologies like 5G, like smart devices and, that I found, not really a ton of talk about what has become a very prevalent conversation in tech, which is, how are companies using the data that they kind of need to offer a lot of these services? SIMON: Haven't a lot of Americans become aware of the fact that we are the product? PACHAL: I think that's a - sometimes an oversimplification. But, you know, there's a bit of a creep factor now, to put it in the plainest terms. You click agree and maybe, you know, ads follow you around the net or - that's sort of the mildest form of these things. But in the sort of worst-case scenario, as you said, you know, your data might be sold in sort of an unethical way. We're all a little bit more wary, at least over here, about that kind of thing. SIMON: Over here in contrast to where? PACHAL: Well, in contrast to other parts of the world. I would say what I saw at CES, which tends to favor big companies like Samsung, like LG, Sony - you know, obviously, are based mostly in Asia - the conversation around privacy there - it hasn't quite penetrated in the same way, at least by my reckoning from what I saw at CES because - I'll give you one example. LG - they talked very highly about its smart home sort of platform, that all these devices have a connection to the Internet and they're using their sensors to kind of learn your preferences and better tailor what they can do for you. That all sounds good until you kind of realize, well, by definition, what they're doing is profiling you. SIMON: Yeah. PACHAL: Now, that sounds benevolent. And I certainly realize these technologies generally can't work without doing that. But what protections do they have on that data? All of that is still very opaque. SIMON: Are there are companies that are trying, in a sense, to take advantage of growing suspicion among some consumers by offering more privacy? PACHAL: Yeah. There are some. There's what I would describe as kind of a cottage industry of sort of these privacy products that - it's usually routers and sort of Internet of things devices. You know, apart from the one billboard put up by Apple, there really wasn't a lot of discussion about privacy at CES this year. SIMON: Well, tell us about that billboard because I understand that Apple wasn't there as a company, but they did have an impact. PACHAL: That's correct. Apple decided to put on the side of a hotel - and this is very common at CES, so it's these big ads that are all over town - that what happens in your iPhone stays on your iPhone, and pointing users to their website on privacy. So it was a very pointed point they were making. And I would've loved to see the companies there sort of run with it and make some reassurances, at least, about privacy and how much they want to secure your data and make sure you have control over it. But that was very absent. In Apple's case, it was probably more of a tap on the shoulder to Google, one of their rivals. And Google made a huge splash at the show. There was Google advertising all over Vegas, certainly at the show. And, you know, Google certainly had its share of privacy and data scandals. SIMON: Tech writer Pete Pachal in New York, thanks so much for being with us. PACHAL: You're welcome. Thanks for having me. (SOUNDBITE OF MUSIC) SCOTT SIMON, HOST:  The massive Consumer Electronics Show in Las Vegas wrapped up yesterday. Tech companies showcased their latest gadgets and predicted trends. But something you didn't hear, according to our next guest, was a frank conversation about privacy. Tech writer Pete Pachal wrote about this from Mashable. He just got back from CES and joins us from New York. Thanks so much for being with us. PETE PACHAL: Hey, my pleasure. SIMON: So many scandals over the past year involving platforms violating privacy, data harvesting, data breaches. What were people talking about, if not that? PACHAL: Well, they were talking about a lot of hype for the new technologies just around the corner. I mean, CES is very much a kind of a tech bubble in some ways. So it's nonstop hype about new technologies like 5G, like smart devices and, that I found, not really a ton of talk about what has become a very prevalent conversation in tech, which is, how are companies using the data that they kind of need to offer a lot of these services? SIMON: Haven't a lot of Americans become aware of the fact that we are the product? PACHAL: I think that's a - sometimes an oversimplification. But, you know, there's a bit of a creep factor now, to put it in the plainest terms. You click agree and maybe, you know, ads follow you around the net or - that's sort of the mildest form of these things. But in the sort of worst-case scenario, as you said, you know, your data might be sold in sort of an unethical way. We're all a little bit more wary, at least over here, about that kind of thing. SIMON: Over here in contrast to where? PACHAL: Well, in contrast to other parts of the world. I would say what I saw at CES, which tends to favor big companies like Samsung, like LG, Sony - you know, obviously, are based mostly in Asia - the conversation around privacy there - it hasn't quite penetrated in the same way, at least by my reckoning from what I saw at CES because - I'll give you one example. LG - they talked very highly about its smart home sort of platform, that all these devices have a connection to the Internet and they're using their sensors to kind of learn your preferences and better tailor what they can do for you. That all sounds good until you kind of realize, well, by definition, what they're doing is profiling you. SIMON: Yeah. PACHAL: Now, that sounds benevolent. And I certainly realize these technologies generally can't work without doing that. But what protections do they have on that data? All of that is still very opaque. SIMON: Are there are companies that are trying, in a sense, to take advantage of growing suspicion among some consumers by offering more privacy? PACHAL: Yeah. There are some. There's what I would describe as kind of a cottage industry of sort of these privacy products that - it's usually routers and sort of Internet of things devices. You know, apart from the one billboard put up by Apple, there really wasn't a lot of discussion about privacy at CES this year. SIMON: Well, tell us about that billboard because I understand that Apple wasn't there as a company, but they did have an impact. PACHAL: That's correct. Apple decided to put on the side of a hotel - and this is very common at CES, so it's these big ads that are all over town - that what happens in your iPhone stays on your iPhone, and pointing users to their website on privacy. So it was a very pointed point they were making. And I would've loved to see the companies there sort of run with it and make some reassurances, at least, about privacy and how much they want to secure your data and make sure you have control over it. But that was very absent. In Apple's case, it was probably more of a tap on the shoulder to Google, one of their rivals. And Google made a huge splash at the show. There was Google advertising all over Vegas, certainly at the show. And, you know, Google certainly had its share of privacy and data scandals. SIMON: Tech writer Pete Pachal in New York, thanks so much for being with us. PACHAL: You're welcome. Thanks for having me. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-13-684894851": {"title": "Dread Opening Your Inbox? There's A New Approach To Embracing All Those Emails : NPR", "url": "https://www.npr.org/2019/01/13/684894851/dread-opening-your-inbox-theres-a-new-approach-to-embracing-all-those-emails", "author": "No author found", "published_date": "2019-01-13", "content": "LULU GARCIA-NAVARRO, HOST:  Let's face it. Email can be exhausting - the notification, the replies, the dreaded reply-alls and so much spam. Here's how a couple of people around NPR deal with their inboxes. UNIDENTIFIED NPR EMPLOYEE #1: I manage my inbox by filing and deleting emails religiously. If I have more than 5 unreads, I get major anxiety about it. UNIDENTIFIED NPR EMPLOYEE #2: I follow the Marie Kondo method of it either sparks joy or it gets thrown out. So that's how I treat my email. UNIDENTIFIED NPR EMPLOYEE #3: I wouldn't say that I always get to Inbox Zero. But every once in a while, I get to Inbox Zero. GARCIA-NAVARRO: Oh, that dreaded Inbox Zero. But this week, we heard about a new, intriguing approach to try in 2019 - inbox infinity. In a recent piece in The Atlantic, tech writer Taylor Lorenz argues, in 2019, you should lose the zero and embrace the Zen. Let all those emails flooding your inbox wash over you. Respond to what you can, and ignore the rest. Key to inbox infinity - telling close contacts and family that your email replies might be slow in coming - if at all - as well as alternative ways to reach you. It's that easy. Or maybe not, depending on how email-dependent your boss, your colleagues and your best friend, your mom and your husband are. As for me, I've apparently been embracing inbox infinity for years without knowing it. And let me tell you, it feels great. Don't expect a reply anytime soon. (SOUNDBITE OF ZONY MASH'S \"MEET THE ZONY MASH\") LULU GARCIA-NAVARRO, HOST:   Let's face it. Email can be exhausting - the notification, the replies, the dreaded reply-alls and so much spam. Here's how a couple of people around NPR deal with their inboxes. UNIDENTIFIED NPR EMPLOYEE #1: I manage my inbox by filing and deleting emails religiously. If I have more than 5 unreads, I get major anxiety about it. UNIDENTIFIED NPR EMPLOYEE #2: I follow the Marie Kondo method of it either sparks joy or it gets thrown out. So that's how I treat my email. UNIDENTIFIED NPR EMPLOYEE #3: I wouldn't say that I always get to Inbox Zero. But every once in a while, I get to Inbox Zero. GARCIA-NAVARRO: Oh, that dreaded Inbox Zero. But this week, we heard about a new, intriguing approach to try in 2019 - inbox infinity. In a recent piece in The Atlantic, tech writer Taylor Lorenz argues, in 2019, you should lose the zero and embrace the Zen. Let all those emails flooding your inbox wash over you. Respond to what you can, and ignore the rest. Key to inbox infinity - telling close contacts and family that your email replies might be slow in coming - if at all - as well as alternative ways to reach you. It's that easy. Or maybe not, depending on how email-dependent your boss, your colleagues and your best friend, your mom and your husband are. As for me, I've apparently been embracing inbox infinity for years without knowing it. And let me tell you, it feels great. Don't expect a reply anytime soon. (SOUNDBITE OF ZONY MASH'S \"MEET THE ZONY MASH\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-14-685341451": {"title": "Regulators To Ease Restrictions On Drones, Clearing The Way For More Commercial Uses : NPR", "url": "https://www.npr.org/2019/01/14/685341451/regulators-to-ease-restrictions-on-drones-clearing-the-way-for-more-commercial-u", "author": "No author found", "published_date": "2019-01-14", "content": "", "section": "National", "disclaimer": ""}, "2019-01-14-684467347": {"title": "Iran Is Preparing A Launch. But Is It For A Space Rocket Or A Missile? : NPR", "url": "https://www.npr.org/2019/01/14/684467347/iran-is-preparing-a-launch-but-is-it-for-a-space-rocket-or-a-missile", "author": "No author found", "published_date": "2019-01-14", "content": "", "section": "World", "disclaimer": ""}, "2019-01-15-685741087": {"title": "Netflix Increases Subscription Prices As It Churns Out Original Content : NPR", "url": "https://www.npr.org/2019/01/15/685741087/netflix-increases-subscription-prices-as-it-churns-out-original-content", "author": "No author found", "published_date": "2019-01-15", "content": "", "section": "Business", "disclaimer": ""}, "2019-01-15-685484428": {"title": "Huawei Founder Denies His Firm Spies For China : NPR", "url": "https://www.npr.org/2019/01/15/685484428/huawei-founder-denies-his-firm-spies-for-china", "author": "No author found", "published_date": "2019-01-15", "content": "", "section": "World", "disclaimer": ""}, "2019-01-15-685656036": {"title": "Congress May Soon Impose New Regulations On Facebook : NPR", "url": "https://www.npr.org/2019/01/15/685656036/congress-may-soon-impose-new-regulations-on-facebook", "author": "No author found", "published_date": "2019-01-15", "content": "ARI SHAPIRO, HOST: Is Facebook ripe for disruption in 2019? That's a question we're asking in this week's All Tech Considered. (SOUNDBITE OF MUSIC)SHAPIRO: Generally, Washington has taken a hands-off approach to tech platforms, not wanting to slow down an economic powerhouse. After multiple scandals involving Facebook, though, this attitude seems to be changing. Conservative lawmakers have criticized the social media giant for what they view as censorship of their end of the political spectrum. And now, as NPR's Tim Mak reports, progressives are becoming critics, too, for different reasons. TIM MAK, BYLINE: Power - this one word sums up the rise in concerns on the left about tech behemoth Facebook. SARAH MILLER: The fundamental problem with Facebook is its power - its market power, its power over the discourse, its power over the way we think and consume information. MAK: That's Sarah Miller. She's a co-chair of Freedom from Facebook, a coalition of progressive groups opposed to the social media company. MILLER: Because of its control over information, it is really one of the world's most dangerous monopolies. MAK: Over the past year, Facebook endured a scandal over Cambridge Analytica's use of personal data and has seen its stock price plummet over privacy concerns. Facebook has been accused by Congress of being used to spread Russian disinformation. And it has also been accused by U. N. experts of being used to incite violence in Myanmar. Here's Miller again. MILLER: We don't want to live in a country where Mark Zuckerberg decides who lives or who dies. MAK: As criticism has mounted, Facebook hired an opposition research firm that attempted to discredit Freedom from Facebook by linking the group to George Soros. By the way, Soros' foundation have supported NPR in the past. This tactic of linking Freedom from Facebook to Soros, who is Jewish, struck some of Facebook's critics as anti-Semitic. So Facebook severed ties with the research firm. The social network did not comment on the story by deadline, but Facebook has said in the past that it is open to privacy regulation. So following these scandals, lawmakers on the left are increasingly turning their attention to regulating Facebook. RON WYDEN: On issue after issue, Facebook has not told the truth to their customers. MAK: That Senator Ron Wyden, the Democrat from Oregon who has been a leading progressive voice on issues from consumer protection to killer drones to the big banks. He is championing new data privacy legislation and believes that the left is increasingly passionate about the need for reforms to Facebook and other big tech firms. WYDEN: This is an issue where our side that wants some accountability and some dramatic reforms to put the consumer in the driver's seat - I think we're picking up grassroots support continually. MAK: Wyden told NPR that Facebook is in an endless cycle in which it lies to consumers, then apologizes, then undertakes what Wyden called meaningless reforms. WYDEN: We're going to have to say, if these CEOs lie to their customers and lie to the federal government, there's going to have to be a real deterrent. That means significant fines and the possibility they will serve jail time. MAK: Wyden's frustration is shared by Senator Mark Warner, who is the top Democrat on the Senate Intelligence Committee, has been investigating Facebook's role in Russian interference with the U. S. political system. MARK WARNER: This is not going to be an area where we can rely upon the goodwill or self-policing of these platforms. MAK: Sarah Miller, who, as you might remember, leads the progressive anti-Facebook coalition, is proposing one solution. MILLER: We have kind of all come together around a set of solutions that includes restructuring the company, so breaking up Facebook, allowing competition to kind of flourish again in the social media space. MAK: This thinking has at least one ideological ally on the right. Conservative Senator Ted Cruz has suggested he supports using anti-trust laws to break up big tech firms. With Republicans concerned about censorship, Democrats concerned about privacy and no end in sight for the scandals, Facebook finds itself in the unenviable position of having to weather a rare bipartisan storm. The question now is what action Congress will take in the year ahead. Tim Mak, NPR News, Washington. ARI SHAPIRO, HOST:  Is Facebook ripe for disruption in 2019? That's a question we're asking in this week's All Tech Considered. (SOUNDBITE OF MUSIC) SHAPIRO: Generally, Washington has taken a hands-off approach to tech platforms, not wanting to slow down an economic powerhouse. After multiple scandals involving Facebook, though, this attitude seems to be changing. Conservative lawmakers have criticized the social media giant for what they view as censorship of their end of the political spectrum. And now, as NPR's Tim Mak reports, progressives are becoming critics, too, for different reasons. TIM MAK, BYLINE: Power - this one word sums up the rise in concerns on the left about tech behemoth Facebook. SARAH MILLER: The fundamental problem with Facebook is its power - its market power, its power over the discourse, its power over the way we think and consume information. MAK: That's Sarah Miller. She's a co-chair of Freedom from Facebook, a coalition of progressive groups opposed to the social media company. MILLER: Because of its control over information, it is really one of the world's most dangerous monopolies. MAK: Over the past year, Facebook endured a scandal over Cambridge Analytica's use of personal data and has seen its stock price plummet over privacy concerns. Facebook has been accused by Congress of being used to spread Russian disinformation. And it has also been accused by U. N. experts of being used to incite violence in Myanmar. Here's Miller again. MILLER: We don't want to live in a country where Mark Zuckerberg decides who lives or who dies. MAK: As criticism has mounted, Facebook hired an opposition research firm that attempted to discredit Freedom from Facebook by linking the group to George Soros. By the way, Soros' foundation have supported NPR in the past. This tactic of linking Freedom from Facebook to Soros, who is Jewish, struck some of Facebook's critics as anti-Semitic. So Facebook severed ties with the research firm. The social network did not comment on the story by deadline, but Facebook has said in the past that it is open to privacy regulation. So following these scandals, lawmakers on the left are increasingly turning their attention to regulating Facebook. RON WYDEN: On issue after issue, Facebook has not told the truth to their customers. MAK: That Senator Ron Wyden, the Democrat from Oregon who has been a leading progressive voice on issues from consumer protection to killer drones to the big banks. He is championing new data privacy legislation and believes that the left is increasingly passionate about the need for reforms to Facebook and other big tech firms. WYDEN: This is an issue where our side that wants some accountability and some dramatic reforms to put the consumer in the driver's seat - I think we're picking up grassroots support continually. MAK: Wyden told NPR that Facebook is in an endless cycle in which it lies to consumers, then apologizes, then undertakes what Wyden called meaningless reforms. WYDEN: We're going to have to say, if these CEOs lie to their customers and lie to the federal government, there's going to have to be a real deterrent. That means significant fines and the possibility they will serve jail time. MAK: Wyden's frustration is shared by Senator Mark Warner, who is the top Democrat on the Senate Intelligence Committee, has been investigating Facebook's role in Russian interference with the U. S. political system. MARK WARNER: This is not going to be an area where we can rely upon the goodwill or self-policing of these platforms. MAK: Sarah Miller, who, as you might remember, leads the progressive anti-Facebook coalition, is proposing one solution. MILLER: We have kind of all come together around a set of solutions that includes restructuring the company, so breaking up Facebook, allowing competition to kind of flourish again in the social media space. MAK: This thinking has at least one ideological ally on the right. Conservative Senator Ted Cruz has suggested he supports using anti-trust laws to break up big tech firms. With Republicans concerned about censorship, Democrats concerned about privacy and no end in sight for the scandals, Facebook finds itself in the unenviable position of having to weather a rare bipartisan storm. The question now is what action Congress will take in the year ahead. Tim Mak, NPR News, Washington.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-15-679304393": {"title": "Forget Screen Time Rules \u2014 Lean In To Parenting Your Wired Child, Author Says  : NPR", "url": "https://www.npr.org/2019/01/15/679304393/forget-screen-time-rules-lean-in-to-parenting-your-wired-child", "author": "No author found", "published_date": "2019-01-15", "content": "", "section": "Education", "disclaimer": ""}, "2019-01-20-686897486": {"title": "10-Year Challenge On Facebook Could Train Facial Recognition Technology On Aging : NPR", "url": "https://www.npr.org/2019/01/20/686897486/could-the-10-year-challenge-be-putting-your-data-at-risk", "author": "No author found", "published_date": "2019-01-20", "content": "LULU GARCIA-NAVARRO, HOST: Have you posted a picture of yourself from 10 years ago, side by side with a picture of you today? It's a thing that's popular on Facebook and Instagram at the moment. And our next guest says that you should think twice before joining in on things like the 10-year challenge. Kate O'Neill explained her opinion on wired. com. She's an author and tech consultant. And she joins us now from New York. Welcome. KATE O'NEILL: Thank you. GARCIA-NAVARRO: So we should be clear Facebook says it didn't start this meme and that people are using photos already on Facebook. But you're saying it's more about how people should think about the potential for misuse. O'NEILL: Yeah, exactly. The particular scenario I talked about in my Wired article was the potential that someone could mine that data and use it to train a facial recognition algorithm. GARCIA-NAVARRO: What makes you really nervous, though, about facial recognition technology in particular? O'NEILL: Yeah, and I mentioned three types of scenarios. One I described as benign. And that's the scenario where, you know, it could be used to help with recognizing the progression of age in children who have been lost. So the police in New Delhi, India had used facial recognition technology in an experiment for just four days. And they recognized 3,000 kids who were missing. But the more mundane scenario and the more common scenario is advertising. More than likely, this will become commonplace where displays can have some sort of camera or sensor that can recognize visual characteristics and serve up a more relevant ad, which will be better performing for the business who's advertising, which is potentially good, too, for us as as consumers - that we'll see more relevant messaging. But then as that data kind of blends with everything else that's downstream from that - all the location data, all of our movements and tracking through the world, all of our financial information, everything that's out there about us - that does start to pose, I think, some risky consequences. And then there was one last scenario that I had talked about, which was the potential that the facial recognition, in particular age progression types of uses of facial recognition, could be used to, say, assess your risk for health characteristics and could say, maybe you're not a good candidate for health insurance. GARCIA-NAVARRO: It's so hard, though, right? Because I see those memes pop up. And I'm like, oh. This is fun. Look at those pictures. It's so funny. Let me go - I was, in fact, just looking back at my Facebook pictures from 2008 and 2009 because I was like, oh, wow. I'd forgotten that happened. And yet we're having to learn how to protect ourselves more. O'NEILL: You're right. And it is - and I absolutely want us to have fun. And I want us to connect with each other. And I think that's what the benefit of technology is. So, yeah - let's participate. Let's have fun. Let's communicate, stay connected with our friends and family. But I think one note of caution is we can look out for opportunities when we're being encouraged to tag photos of ourselves. That's one way we can opt out and maybe not always tag every photo with every face. We can not necessarily participate in every game or meme that asks us to provide data about ourselves in structured, specific ways. GARCIA-NAVARRO: I'm assuming that you didn't do the 10-year challenge. O'NEILL: Well, my 10-year challenge response was the one that - that was the tweet that kind of started the whole thing that said, you know, 10 years ago, I probably would have participated and shared my photos. And now I look at it as an opportunity for harvesting all that data for a facial recognition training process. GARCIA-NAVARRO: Wah-wah-wah. O'NEILL: Right. GARCIA-NAVARRO: That's Kate O'Neill. She's the author most recently of the book \"Tech Humanist. \" Thank you so much. O'NEILL: Thank you. LULU GARCIA-NAVARRO, HOST:  Have you posted a picture of yourself from 10 years ago, side by side with a picture of you today? It's a thing that's popular on Facebook and Instagram at the moment. And our next guest says that you should think twice before joining in on things like the 10-year challenge. Kate O'Neill explained her opinion on wired. com. She's an author and tech consultant. And she joins us now from New York. Welcome. KATE O'NEILL: Thank you. GARCIA-NAVARRO: So we should be clear Facebook says it didn't start this meme and that people are using photos already on Facebook. But you're saying it's more about how people should think about the potential for misuse. O'NEILL: Yeah, exactly. The particular scenario I talked about in my Wired article was the potential that someone could mine that data and use it to train a facial recognition algorithm. GARCIA-NAVARRO: What makes you really nervous, though, about facial recognition technology in particular? O'NEILL: Yeah, and I mentioned three types of scenarios. One I described as benign. And that's the scenario where, you know, it could be used to help with recognizing the progression of age in children who have been lost. So the police in New Delhi, India had used facial recognition technology in an experiment for just four days. And they recognized 3,000 kids who were missing. But the more mundane scenario and the more common scenario is advertising. More than likely, this will become commonplace where displays can have some sort of camera or sensor that can recognize visual characteristics and serve up a more relevant ad, which will be better performing for the business who's advertising, which is potentially good, too, for us as as consumers - that we'll see more relevant messaging. But then as that data kind of blends with everything else that's downstream from that - all the location data, all of our movements and tracking through the world, all of our financial information, everything that's out there about us - that does start to pose, I think, some risky consequences. And then there was one last scenario that I had talked about, which was the potential that the facial recognition, in particular age progression types of uses of facial recognition, could be used to, say, assess your risk for health characteristics and could say, maybe you're not a good candidate for health insurance. GARCIA-NAVARRO: It's so hard, though, right? Because I see those memes pop up. And I'm like, oh. This is fun. Look at those pictures. It's so funny. Let me go - I was, in fact, just looking back at my Facebook pictures from 2008 and 2009 because I was like, oh, wow. I'd forgotten that happened. And yet we're having to learn how to protect ourselves more. O'NEILL: You're right. And it is - and I absolutely want us to have fun. And I want us to connect with each other. And I think that's what the benefit of technology is. So, yeah - let's participate. Let's have fun. Let's communicate, stay connected with our friends and family. But I think one note of caution is we can look out for opportunities when we're being encouraged to tag photos of ourselves. That's one way we can opt out and maybe not always tag every photo with every face. We can not necessarily participate in every game or meme that asks us to provide data about ourselves in structured, specific ways. GARCIA-NAVARRO: I'm assuming that you didn't do the 10-year challenge. O'NEILL: Well, my 10-year challenge response was the one that - that was the tweet that kind of started the whole thing that said, you know, 10 years ago, I probably would have participated and shared my photos. And now I look at it as an opportunity for harvesting all that data for a facial recognition training process. GARCIA-NAVARRO: Wah-wah-wah. O'NEILL: Right. GARCIA-NAVARRO: That's Kate O'Neill. She's the author most recently of the book \"Tech Humanist. \" Thank you so much. O'NEILL: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-21-687255712": {"title": "Google Employees Hope To Turn Last Year's Walkouts Into Real Change : NPR", "url": "https://www.npr.org/2019/01/21/687255712/google-employees-hope-to-turn-last-years-walkouts-into-real-change", "author": "No author found", "published_date": "2019-01-21", "content": "AUDIE CORNISH, HOST: And it's time now for All Tech Considered. (SOUNDBITE OF MUSIC)CORNISH: This month, we've been examining what's ripe for disruption in the tech world. And today we're going to look at where some of that disruption is coming from. In November, 20,000 Google employees worldwide took to the streets to protest how the company handled sexual harassment claims. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PROTESTERS: (Chanting) Time is up. CORNISH: It was the first show of force of this size among tech workers and one that their boss, CEO Sundar Pichai, said he supported. This year, the walkout organizers are hoping to channel their momentum into real change in how Google operates. Google linguist Vicki Tardif led the protest in Cambridge, Mass. She joins us now. Welcome to the program. VICKI TARDIF: Thank you. CORNISH: So can you take me back to that moment in November. I'm from Massachusetts, so I'm guessing what the weather was like. But what did it feel like to actually walk out of the job? TARDIF: I think for - if you weren't within Google's four walls, it seemed like it was sudden. But there was a lot that led up to this. There was - the sort of internal communications had broken down of how people voiced dissent about things. It was this moment of saying, you know, we're sick of asking for changes and not seeing them. You know, I'm tired of seeing women colleagues transfer teams to avoid some creep on their team. I'm wondering why it is I have colleagues who aren't treated as fairly simply because they're contractors and I'm a full-time employee. I'm wondering why it is there all these diversity and inclusion efforts, and yet we don't see increased representation for people of color or for trans people or nonbinary people. And so it was this moment of, like, being very tired of that and wanting to do something. CORNISH: And at the time, the CEO Sundar Pichai said like, look. We don't always get it right. We're committed to doing better. But since then, do you see evidence of that? TARDIF: So I think it's important to separate the rhetoric and real meaningful change. CORNISH: But they did, for instance, drop arbitration for full-time employees when it comes to sexual harassment cases. TARDIF: So to clarify, they issued a press release saying they would do that, but our contracts haven't been updated. So it's unclear what that means legally. And for our colleagues who are vendors or contractors, it's unclear if they'll ever get that carved out. CORNISH: So now you have the social media campaign, and it seems to be sending the signal that you feel like you guys aren't being listened to. TARDIF: I think that is the feeling - is that we aren't being listened to. You know, when we talk about this, we aren't just talking about Googlers (ph). Things like forced arbitration affect people outside of the tech industry even. There are more than 60 million Americans who are bound by forced arbitration agreements. And so some of this is building a coalition of workers - not just tech workers, not just Googlers - to say, how do we address inequity and unfairness in workplaces across the country? CORNISH: It's interesting because I think to the broader public for so long we were used to hearing a couple of kind of threads out of Silicon Valley - you know, number one, that you guys, like, were creating this vague utopia-like tools how would make our lives better, even if none of us really knew what they were, and that the workers were really happy because they had, like, free bowling and, like, never wanted to go home because there was awesome food. So it did feel sudden. Do you think there is a shift in how workers themselves see the industry? TARDIF: Yes, particularly for members of underrepresented groups. It was easy at one time to say, oh, these issues exist, but we're going to fix them. But it was four years ago that a Googler named Erica Baker started a spreadsheet to say, hey, let's track how people are paid and see if it's equal across the board across demographics. That was four years ago. And we haven't seen meaningful change since then. For all of the industry patting itself on the back, for all its diversity and inclusion efforts, we aren't seeing numbers shift at all. CORNISH: We've seen these conversations bubble up in places like Amazon as well. Looking ahead into 2019, where do you see these movements going? Do you see them actually getting stronger? TARDIF: I hope so. One of the goals for 2019 is definitely to continue to build broad coalitions because our power really is in our numbers. And you see that in things like the LA teachers' strike, right? It's important we all support each other as workers. CORNISH: I don't know if you've feared retaliation at Google, but there is a sense that workers at other big tech companies are afraid. And do you think that's valid? TARDIF: I think it's valid. I think a lot of us are knowingly taking a risk. But at some point, you get so tired that it's the best alternative to live with yourself. CORNISH: Vicki Tardif is a linguist at Google. Thank you for speaking with us. TARDIF: Thank you. AUDIE CORNISH, HOST:  And it's time now for All Tech Considered. (SOUNDBITE OF MUSIC) CORNISH: This month, we've been examining what's ripe for disruption in the tech world. And today we're going to look at where some of that disruption is coming from. In November, 20,000 Google employees worldwide took to the streets to protest how the company handled sexual harassment claims. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PROTESTERS: (Chanting) Time is up. CORNISH: It was the first show of force of this size among tech workers and one that their boss, CEO Sundar Pichai, said he supported. This year, the walkout organizers are hoping to channel their momentum into real change in how Google operates. Google linguist Vicki Tardif led the protest in Cambridge, Mass. She joins us now. Welcome to the program. VICKI TARDIF: Thank you. CORNISH: So can you take me back to that moment in November. I'm from Massachusetts, so I'm guessing what the weather was like. But what did it feel like to actually walk out of the job? TARDIF: I think for - if you weren't within Google's four walls, it seemed like it was sudden. But there was a lot that led up to this. There was - the sort of internal communications had broken down of how people voiced dissent about things. It was this moment of saying, you know, we're sick of asking for changes and not seeing them. You know, I'm tired of seeing women colleagues transfer teams to avoid some creep on their team. I'm wondering why it is I have colleagues who aren't treated as fairly simply because they're contractors and I'm a full-time employee. I'm wondering why it is there all these diversity and inclusion efforts, and yet we don't see increased representation for people of color or for trans people or nonbinary people. And so it was this moment of, like, being very tired of that and wanting to do something. CORNISH: And at the time, the CEO Sundar Pichai said like, look. We don't always get it right. We're committed to doing better. But since then, do you see evidence of that? TARDIF: So I think it's important to separate the rhetoric and real meaningful change. CORNISH: But they did, for instance, drop arbitration for full-time employees when it comes to sexual harassment cases. TARDIF: So to clarify, they issued a press release saying they would do that, but our contracts haven't been updated. So it's unclear what that means legally. And for our colleagues who are vendors or contractors, it's unclear if they'll ever get that carved out. CORNISH: So now you have the social media campaign, and it seems to be sending the signal that you feel like you guys aren't being listened to. TARDIF: I think that is the feeling - is that we aren't being listened to. You know, when we talk about this, we aren't just talking about Googlers (ph). Things like forced arbitration affect people outside of the tech industry even. There are more than 60 million Americans who are bound by forced arbitration agreements. And so some of this is building a coalition of workers - not just tech workers, not just Googlers - to say, how do we address inequity and unfairness in workplaces across the country? CORNISH: It's interesting because I think to the broader public for so long we were used to hearing a couple of kind of threads out of Silicon Valley - you know, number one, that you guys, like, were creating this vague utopia-like tools how would make our lives better, even if none of us really knew what they were, and that the workers were really happy because they had, like, free bowling and, like, never wanted to go home because there was awesome food. So it did feel sudden. Do you think there is a shift in how workers themselves see the industry? TARDIF: Yes, particularly for members of underrepresented groups. It was easy at one time to say, oh, these issues exist, but we're going to fix them. But it was four years ago that a Googler named Erica Baker started a spreadsheet to say, hey, let's track how people are paid and see if it's equal across the board across demographics. That was four years ago. And we haven't seen meaningful change since then. For all of the industry patting itself on the back, for all its diversity and inclusion efforts, we aren't seeing numbers shift at all. CORNISH: We've seen these conversations bubble up in places like Amazon as well. Looking ahead into 2019, where do you see these movements going? Do you see them actually getting stronger? TARDIF: I hope so. One of the goals for 2019 is definitely to continue to build broad coalitions because our power really is in our numbers. And you see that in things like the LA teachers' strike, right? It's important we all support each other as workers. CORNISH: I don't know if you've feared retaliation at Google, but there is a sense that workers at other big tech companies are afraid. And do you think that's valid? TARDIF: I think it's valid. I think a lot of us are knowingly taking a risk. But at some point, you get so tired that it's the best alternative to live with yourself. CORNISH: Vicki Tardif is a linguist at Google. Thank you for speaking with us. TARDIF: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-22-687439015": {"title": "WhatsApp Tries To Curb Misinformation, And Annoying Aunts And Uncles : NPR", "url": "https://www.npr.org/2019/01/22/687439015/whatsapp-tries-to-curb-spread-of-misinformation-by-limiting-message-forwarding", "author": "No author found", "published_date": "2019-01-22", "content": "", "section": "Technology", "disclaimer": ""}, "2019-01-22-685852007": {"title": "Electric Truck Startup Steps In Where Other Automakers Left Off : NPR", "url": "https://www.npr.org/2019/01/22/685852007/new-electric-pickup-truck-faces-an-uphill-climb-to-get-americans-on-board", "author": "No author found", "published_date": "2019-01-22", "content": "MARY LOUISE KELLY, HOST: At the Detroit Auto Show this week, the big automakers are promoting trucks and SUVs, once again the bestselling vehicles in the U. S. There's also a new company knocking on the door, Rivian, which wants to be the first to sell an all-electric pickup truck. Ryan Denham of member station WGLT reports. RYAN DENHAM, BYLINE: I'm riding in a golf cart around the massive auto plant in Normal, Ill. , a hundred miles southwest of Chicago. My tour guide is Wade Jensen. He worked here three decades ago when Mitsubishi and Chrysler built cars here, cranking out hundreds of thousands every year. But three years ago, Mitsubishi shut it down and moved production to Japan. Jensen and 1,200 others lost their jobs. Now he's back as the engineering manager for electric automaker Rivian's first assembly line. The startup plans to hire a thousand workers here in the next four years. WADE JENSEN: When you've done it for 28 years, it's your passion. I mean, it's - it's what's in you. It's what's in your heart. It's your desire, and to have the opportunity to see this plant producing cars and putting them out the back door again, I was all in. DENHAM: The man who's recycling this plant is Rivian founder and CEO R. J. Scaringe, a 36-year-old car geek with a Ph. D. from M. I. T. in mechanical engineering. He started work on a gas-powered, eco-sports car 10 years ago, about the time when another ambitious entrepreneur, Elon Musk and Tesla, started bringing electric cars into the mainstream. Here's Scaringe on the sidelines of the recent LA Auto Show. RJ SCARINGE: They took the untruth that electric cars are boring and slow and flipped that and showed the world electric cars can be exciting and certainly very quick. DENHAM: Musk is known for his bombast, tweets that move stock prices and promotions like shooting a car into space. Scaringe spent the past few years doing the opposite, staying quiet, hiring auto-industry veterans and raising a half billion dollars from Saudi and Japanese conglomerates. At the LA Auto Show, Rivian finally revealed its electric pickup and SUV with a charging range of 400 miles. He beat Detroit to the punch. SCARINGE: That's the opportunity we have is to show the world that this is a space that actually badly needs electrification, and electrification can make those products better than what their gasoline diesel counterparts had been in the past. DENHAM: Rivian has only 600 employees so far. Design and engineering are done outside Detroit and in the U. K. , batteries and tech in California. And about 70 people are getting the plant in Illinois up and running. Starting a car company from scratch isn't easy. Just ask Tesla. It's hemorrhaged money, missed deadlines and freaked out investors. And it's considered a success. Other EV startups haven't even made it to market, one reason electric vehicles here still represent a tiny part of the market, 1 percent of sales. While they may be the future, low gas prices are a challenge to electric vehicles, especially for legacy automakers. If that changes in a few years and Ford finally puts an electric version of its bestselling F-150 on the market, Rivian would be facing stiff competition. Industry watcher Chelsea Sexton was at the LA Auto Show for Rivian's debut. CHELSEA SEXTON: We root for all the startups, but a lot happens between concept and showroom. And it's most vulnerable for the startups. DENHAM: Rivian's high price tag - trucks starting around $70,000 didn't - scare off Ariel Fernandez from Florida. He was among the first to plop down a thousand bucks to preorder a Rivian SUV. ARIEL FERNANDEZ: I'm willing to invest in this company and basically put my trust in them that they're gonna produce the vehicle and make me happy when - when I pick it up. DENHAM: Fernandez's SUV'll be made here in Illinois. But that may not be it. Rivian also plans a side business, selling its battery technology to other companies. So if electric trucks don't take off, maybe battery-powered tractors and Jet Skis will. For NPR News, I'm Ryan Denham in Normal, Ill. (SOUNDBITE OF VARIOUS & DEEP-WATER RECORDINGS' \"COUNT YOUR BLESSINGS\") MARY LOUISE KELLY, HOST:  At the Detroit Auto Show this week, the big automakers are promoting trucks and SUVs, once again the bestselling vehicles in the U. S. There's also a new company knocking on the door, Rivian, which wants to be the first to sell an all-electric pickup truck. Ryan Denham of member station WGLT reports. RYAN DENHAM, BYLINE: I'm riding in a golf cart around the massive auto plant in Normal, Ill. , a hundred miles southwest of Chicago. My tour guide is Wade Jensen. He worked here three decades ago when Mitsubishi and Chrysler built cars here, cranking out hundreds of thousands every year. But three years ago, Mitsubishi shut it down and moved production to Japan. Jensen and 1,200 others lost their jobs. Now he's back as the engineering manager for electric automaker Rivian's first assembly line. The startup plans to hire a thousand workers here in the next four years. WADE JENSEN: When you've done it for 28 years, it's your passion. I mean, it's - it's what's in you. It's what's in your heart. It's your desire, and to have the opportunity to see this plant producing cars and putting them out the back door again, I was all in. DENHAM: The man who's recycling this plant is Rivian founder and CEO R. J. Scaringe, a 36-year-old car geek with a Ph. D. from M. I. T. in mechanical engineering. He started work on a gas-powered, eco-sports car 10 years ago, about the time when another ambitious entrepreneur, Elon Musk and Tesla, started bringing electric cars into the mainstream. Here's Scaringe on the sidelines of the recent LA Auto Show. RJ SCARINGE: They took the untruth that electric cars are boring and slow and flipped that and showed the world electric cars can be exciting and certainly very quick. DENHAM: Musk is known for his bombast, tweets that move stock prices and promotions like shooting a car into space. Scaringe spent the past few years doing the opposite, staying quiet, hiring auto-industry veterans and raising a half billion dollars from Saudi and Japanese conglomerates. At the LA Auto Show, Rivian finally revealed its electric pickup and SUV with a charging range of 400 miles. He beat Detroit to the punch. SCARINGE: That's the opportunity we have is to show the world that this is a space that actually badly needs electrification, and electrification can make those products better than what their gasoline diesel counterparts had been in the past. DENHAM: Rivian has only 600 employees so far. Design and engineering are done outside Detroit and in the U. K. , batteries and tech in California. And about 70 people are getting the plant in Illinois up and running. Starting a car company from scratch isn't easy. Just ask Tesla. It's hemorrhaged money, missed deadlines and freaked out investors. And it's considered a success. Other EV startups haven't even made it to market, one reason electric vehicles here still represent a tiny part of the market, 1 percent of sales. While they may be the future, low gas prices are a challenge to electric vehicles, especially for legacy automakers. If that changes in a few years and Ford finally puts an electric version of its bestselling F-150 on the market, Rivian would be facing stiff competition. Industry watcher Chelsea Sexton was at the LA Auto Show for Rivian's debut. CHELSEA SEXTON: We root for all the startups, but a lot happens between concept and showroom. And it's most vulnerable for the startups. DENHAM: Rivian's high price tag - trucks starting around $70,000 didn't - scare off Ariel Fernandez from Florida. He was among the first to plop down a thousand bucks to preorder a Rivian SUV. ARIEL FERNANDEZ: I'm willing to invest in this company and basically put my trust in them that they're gonna produce the vehicle and make me happy when - when I pick it up. DENHAM: Fernandez's SUV'll be made here in Illinois. But that may not be it. Rivian also plans a side business, selling its battery technology to other companies. So if electric trucks don't take off, maybe battery-powered tractors and Jet Skis will. For NPR News, I'm Ryan Denham in Normal, Ill. (SOUNDBITE OF VARIOUS & DEEP-WATER RECORDINGS' \"COUNT YOUR BLESSINGS\")", "section": "National", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-22-686588951": {"title": "With Amazon's Arrival, A New York Community Pushes To Be Included : NPR", "url": "https://www.npr.org/2019/01/22/686588951/with-amazons-arrival-a-new-york-community-pushes-to-be-included", "author": "No author found", "published_date": "2019-01-22", "content": "RACHEL MARTIN, HOST: One of Amazon's shiny new headquarters will be located in a part of New York City that has long struggled. It'll be just a few blocks away from Queensbridge Houses, which is the name of the largest public housing complex in North America. So what will that mean for locals? NPR's Jasmine Garsd reports from Queensbridge. JASMINE GARSD, BYLINE: The Queensbridge public houses are just two subway stops away from bustling Manhattan. But Queensbridge feels like a completely different world. Chris Hanway is the executive director of the Jacob A. Riis Neighborhood Settlement, a nonprofit which provides services to the community. CHRIS HANWAY: Queensbridge has always been literally and psychologically isolated. It's got the river on one side, the bridge on another and then sort of light manufacturing buildings around it. GARSD: In his seminal 1994 album \"Illmatic,\" New York rapper Nas, who is from Queensbridge, penned an ode to the neighborhood. . . (SOUNDBITE OF SONG, \"MEMORY LANE (SITTIN' IN DA PARK)\")NAS: (Rapping) It's real - grew up a trife life, the times of white lines. . . GARSD: . . . And the strength of its people. (SOUNDBITE OF SONG, \"MEMORY LANE (SITTIN' IN DA PARK)\")NAS: (Rapping) Coming out of Queensbridge - now let me. . . GARSD: It didn't take long for big companies to notice how close the neighborhood is to Manhattan. In the last decade or so, Ralph Lauren opened an office nearby, and so did JetBlue. But activists say, for many Queensbridge residents, it's like it never happened. Hanway says unemployment here is high. And that's why when Amazon announced it's setting up shop just a few blocks away. . . HANWAY: People literally shrugged their shoulders and said, well, we've been down this road before. It's not really going to affect us in any way - because they've lived this experience. GARSD: Amazon, which is an NPR sponsor, declined to comment. But the company has promised to fund infrastructure, a school and a tech incubator. It also says it will host job fairs for locals. Chris Hanway has met with Amazon officials and made it clear he wants more concrete promises than that. HANWAY: What are our goals? How many local residents are we going to hire - into what kind of jobs? How will we get those residents ready for these jobs? And there have to be benchmarks, and Amazon has to be held accountable for that. GARSD: On a chilly weeknight, I head over to an area near the Queensbridge Houses. It's a lot of warehouses and storage units. But inside one building, I find a software developer training class. UNIDENTIFIED LECTURER: In other words, you're able to take your data from your app and. . . GARSD: The students listening intently to the lecture are mostly Latino, African-American, and there's plenty of women - faces that are scarce in Silicon Valley. The class is part of a nonprofit called Pursuit, which trains low-income adults for tech jobs. Pursuit is also the designated community partner, co-developer and part owner of the new Amazon headquarters. Here's CEO and founder Jukay Hsu. JUKAY HSU: I think we have a unique opportunity here and for New York to be a place where the technology community can thrive but also be inclusive. GARSD: Hsu, who was once a classmate of Facebook creator Mark Zuckerberg at Harvard University, says he saw how his school friends revolutionized technology. But then he'd come home to his native Queens and wonder about those left behind. He wants to make sure Amazon's new headquarters relies heavily on local talent. HSU: Amazon, when it's here, can be embedded in Queens. Amazon's coming to Queens, but we want to bring Queens to Amazon. GARSD: In a lounge area outside the Pursuit lecture room, I meet a 26-year-old new coding student, Ivy Strickland. She's from Harlem. She tells me she's excited about Amazon coming to Queens. IVY STRICKLAND: I'm the youngest of three children of a single mom. My mom had us when she was a teenager. So imagine me, someone who now makes, like, under $20,000 a year, able to get a job that could pay me enough that I would be able to do certain things, like pay my mom's mortgage or help her out. GARSD: Although she only recently started coding, she loves it. She says she sees it as a metaphor of how, piece by piece, you can build something amazing. STRICKLAND: To see the way that you can take something so small and grow, I guess for me personally, to know where I've come from, it's like the same thing. I can see myself growing. GARSD: Whether or not this city and this neighborhood will be able to grow and build something good with Amazon remains to be seen. For the time being, there's hope, a good measure of distrust and plenty of that legendary Queens strength. Jasmine Garsd, NPR News, Queens. RACHEL MARTIN, HOST:  One of Amazon's shiny new headquarters will be located in a part of New York City that has long struggled. It'll be just a few blocks away from Queensbridge Houses, which is the name of the largest public housing complex in North America. So what will that mean for locals? NPR's Jasmine Garsd reports from Queensbridge. JASMINE GARSD, BYLINE: The Queensbridge public houses are just two subway stops away from bustling Manhattan. But Queensbridge feels like a completely different world. Chris Hanway is the executive director of the Jacob A. Riis Neighborhood Settlement, a nonprofit which provides services to the community. CHRIS HANWAY: Queensbridge has always been literally and psychologically isolated. It's got the river on one side, the bridge on another and then sort of light manufacturing buildings around it. GARSD: In his seminal 1994 album \"Illmatic,\" New York rapper Nas, who is from Queensbridge, penned an ode to the neighborhood. . . (SOUNDBITE OF SONG, \"MEMORY LANE (SITTIN' IN DA PARK)\") NAS: (Rapping) It's real - grew up a trife life, the times of white lines. . . GARSD: . . . And the strength of its people. (SOUNDBITE OF SONG, \"MEMORY LANE (SITTIN' IN DA PARK)\") NAS: (Rapping) Coming out of Queensbridge - now let me. . . GARSD: It didn't take long for big companies to notice how close the neighborhood is to Manhattan. In the last decade or so, Ralph Lauren opened an office nearby, and so did JetBlue. But activists say, for many Queensbridge residents, it's like it never happened. Hanway says unemployment here is high. And that's why when Amazon announced it's setting up shop just a few blocks away. . . HANWAY: People literally shrugged their shoulders and said, well, we've been down this road before. It's not really going to affect us in any way - because they've lived this experience. GARSD: Amazon, which is an NPR sponsor, declined to comment. But the company has promised to fund infrastructure, a school and a tech incubator. It also says it will host job fairs for locals. Chris Hanway has met with Amazon officials and made it clear he wants more concrete promises than that. HANWAY: What are our goals? How many local residents are we going to hire - into what kind of jobs? How will we get those residents ready for these jobs? And there have to be benchmarks, and Amazon has to be held accountable for that. GARSD: On a chilly weeknight, I head over to an area near the Queensbridge Houses. It's a lot of warehouses and storage units. But inside one building, I find a software developer training class. UNIDENTIFIED LECTURER: In other words, you're able to take your data from your app and. . . GARSD: The students listening intently to the lecture are mostly Latino, African-American, and there's plenty of women - faces that are scarce in Silicon Valley. The class is part of a nonprofit called Pursuit, which trains low-income adults for tech jobs. Pursuit is also the designated community partner, co-developer and part owner of the new Amazon headquarters. Here's CEO and founder Jukay Hsu. JUKAY HSU: I think we have a unique opportunity here and for New York to be a place where the technology community can thrive but also be inclusive. GARSD: Hsu, who was once a classmate of Facebook creator Mark Zuckerberg at Harvard University, says he saw how his school friends revolutionized technology. But then he'd come home to his native Queens and wonder about those left behind. He wants to make sure Amazon's new headquarters relies heavily on local talent. HSU: Amazon, when it's here, can be embedded in Queens. Amazon's coming to Queens, but we want to bring Queens to Amazon. GARSD: In a lounge area outside the Pursuit lecture room, I meet a 26-year-old new coding student, Ivy Strickland. She's from Harlem. She tells me she's excited about Amazon coming to Queens. IVY STRICKLAND: I'm the youngest of three children of a single mom. My mom had us when she was a teenager. So imagine me, someone who now makes, like, under $20,000 a year, able to get a job that could pay me enough that I would be able to do certain things, like pay my mom's mortgage or help her out. GARSD: Although she only recently started coding, she loves it. She says she sees it as a metaphor of how, piece by piece, you can build something amazing. STRICKLAND: To see the way that you can take something so small and grow, I guess for me personally, to know where I've come from, it's like the same thing. I can see myself growing. GARSD: Whether or not this city and this neighborhood will be able to grow and build something good with Amazon remains to be seen. For the time being, there's hope, a good measure of distrust and plenty of that legendary Queens strength. Jasmine Garsd, NPR News, Queens.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-23-687619696": {"title": "Suspended Twitter Account Plays A Role In Misleading Viral Video : NPR", "url": "https://www.npr.org/2019/01/23/687619696/suspended-twitter-account-plays-a-role-in-misleading-viral-video", "author": "No author found", "published_date": "2019-01-23", "content": "STEVE INSKEEP, HOST: People outraged by a confrontation in front of the Lincoln Memorial had help in getting outraged. Nobody was harmed as a Native American drummer faced a group of primarily white Catholic school students. But the incident spread on social media, and it was boosted by a single Twitter account that has since been suspended. NPR's Laura Sydell reports Twitter says the account was using misleading information to manipulate the public conversation. LAURA SYDELL, BYLINE: The video went viral in a number of different ways, but a major player was an account with the handle @2020fight. The video shows a Native American elder banging on a small hand-held drum surrounded by a group of high school boys, some of whom are wearing Make America Great Again hats. One of the boys stands a few feet from the man's face, staring at him with a slight smile. It's a brief moment in a much longer, more complicated situation. The @2020fight account has been in the sights of Rob McDonagh with Storyful, a firm that analyzes social media conversations. ROB MCDONAGH: And I had spotted it before tweeting out very hyperpartisan views very much in Democratic talking points. SYDELL: While that in itself didn't necessarily mean anything was wrong, there were other signs. MCDONAGH: What made this account stand out is its high rate of tweets and highly political tweets. You're talking 130-plus tweets a day. And it had a fake profile pic. It was using the profile photo of a Brazilian blogger. SYDELL: McDonagh says the account also had over 40,000 followers, but it wasn't verified by Twitter. He says that's unusual for an account with so many followers. According to McDonagh, CNN pointed out the account to Twitter, and it was taken down. Twitter has not disclosed who it believes might be behind the @2020fight account. But Molly McKew thinks it bears the hallmarks of an account designed to spread discord. MOLLY MCKEW: I think this little bit of video content really hit a nerve with a lot of people, and I think that's exactly what it was intended to do. SYDELL: McKew is a researcher who's worked for the governments of the countries of Georgia and Moldova, consulting on how to fight Russian disinformation. She says the video hit a nerve with progressives because it looked like a member of a minority group, a Native American elder, was being attacked by a group of white boys with MAGA hats. MCKEW: Everybody rushes to their polls (ph) as quickly as possible. SYDELL: McKew says she also noticed that @2020fight is followed by accounts she thinks are suspicious. Those accounts retweeted the video and sent it out to more people. Whitney Phillips, a professor at Syracuse University, has studied the way stories like this blow up on social media. Phillips says this situation is a near-perfect model of how social media and the news media end up working together to heighten conflicts between Americans. WHITNEY PHILLIPS: You basically throw a match into some kindling, and then the American people supply the oxygen. SYDELL: Then it rises to the top on Twitter, and the professional media notices it. PHILLIPS: And so you have this race to cover the story first, and then once the story has been covered, every publication needs to, you know, publish their own take. SYDELL: Including NPR. Phillips says the end result is that a small group that wants to keep Americans fighting amongst themselves is able to leverage social media and manipulate the traditional media. Unfortunately, she says, social media companies like Twitter are ultimately not prioritizing the good of society. PHILLIPS: Sometimes it takes down really offensive content and sometimes it keeps that content up because it is good for their bottom line. SYDELL: Phillips says divisions in our society are real, and the video may have taken off without fake accounts. But she would like to see people look at the source of the information before retweeting. Laura Sydell, NPR News. (SOUNDBITE OF LAURENCE GUY'S \"CLAUDI\") STEVE INSKEEP, HOST:  People outraged by a confrontation in front of the Lincoln Memorial had help in getting outraged. Nobody was harmed as a Native American drummer faced a group of primarily white Catholic school students. But the incident spread on social media, and it was boosted by a single Twitter account that has since been suspended. NPR's Laura Sydell reports Twitter says the account was using misleading information to manipulate the public conversation. LAURA SYDELL, BYLINE: The video went viral in a number of different ways, but a major player was an account with the handle @2020fight. The video shows a Native American elder banging on a small hand-held drum surrounded by a group of high school boys, some of whom are wearing Make America Great Again hats. One of the boys stands a few feet from the man's face, staring at him with a slight smile. It's a brief moment in a much longer, more complicated situation. The @2020fight account has been in the sights of Rob McDonagh with Storyful, a firm that analyzes social media conversations. ROB MCDONAGH: And I had spotted it before tweeting out very hyperpartisan views very much in Democratic talking points. SYDELL: While that in itself didn't necessarily mean anything was wrong, there were other signs. MCDONAGH: What made this account stand out is its high rate of tweets and highly political tweets. You're talking 130-plus tweets a day. And it had a fake profile pic. It was using the profile photo of a Brazilian blogger. SYDELL: McDonagh says the account also had over 40,000 followers, but it wasn't verified by Twitter. He says that's unusual for an account with so many followers. According to McDonagh, CNN pointed out the account to Twitter, and it was taken down. Twitter has not disclosed who it believes might be behind the @2020fight account. But Molly McKew thinks it bears the hallmarks of an account designed to spread discord. MOLLY MCKEW: I think this little bit of video content really hit a nerve with a lot of people, and I think that's exactly what it was intended to do. SYDELL: McKew is a researcher who's worked for the governments of the countries of Georgia and Moldova, consulting on how to fight Russian disinformation. She says the video hit a nerve with progressives because it looked like a member of a minority group, a Native American elder, was being attacked by a group of white boys with MAGA hats. MCKEW: Everybody rushes to their polls (ph) as quickly as possible. SYDELL: McKew says she also noticed that @2020fight is followed by accounts she thinks are suspicious. Those accounts retweeted the video and sent it out to more people. Whitney Phillips, a professor at Syracuse University, has studied the way stories like this blow up on social media. Phillips says this situation is a near-perfect model of how social media and the news media end up working together to heighten conflicts between Americans. WHITNEY PHILLIPS: You basically throw a match into some kindling, and then the American people supply the oxygen. SYDELL: Then it rises to the top on Twitter, and the professional media notices it. PHILLIPS: And so you have this race to cover the story first, and then once the story has been covered, every publication needs to, you know, publish their own take. SYDELL: Including NPR. Phillips says the end result is that a small group that wants to keep Americans fighting amongst themselves is able to leverage social media and manipulate the traditional media. Unfortunately, she says, social media companies like Twitter are ultimately not prioritizing the good of society. PHILLIPS: Sometimes it takes down really offensive content and sometimes it keeps that content up because it is good for their bottom line. SYDELL: Phillips says divisions in our society are real, and the video may have taken off without fake accounts. But she would like to see people look at the source of the information before retweeting. Laura Sydell, NPR News. (SOUNDBITE OF LAURENCE GUY'S \"CLAUDI\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-24-687374834": {"title": "Quest To Replace Styrofoam Packaging Started With Fungus : NPR", "url": "https://www.npr.org/2019/01/24/687374834/video-inventor-inspired-by-childhood-memories-of-fungus", "author": "No author found", "published_date": "2019-01-24", "content": "", "section": "Changing The World One Invention At A Time", "disclaimer": ""}, "2019-01-24-688101860": {"title": "China Restores Public Access To Microsoft's Bing Search Engine : NPR", "url": "https://www.npr.org/2019/01/24/688101860/china-appears-to-block-microsofts-bing-search-engine", "author": "No author found", "published_date": "2019-01-24", "content": "", "section": "Technology", "disclaimer": ""}, "2019-01-25-688232647": {"title": "A Speed Limit On Germany's Autobahns: 'Like Talking Gun Control In The U.S.' : NPR", "url": "https://www.npr.org/2019/01/25/688232647/a-speed-limit-on-german-highways-like-talking-gun-control-in-the-u-s", "author": "No author found", "published_date": "2019-01-25", "content": "", "section": "Business", "disclaimer": ""}, "2019-01-26-688868687": {"title": "Amid Chaos, Venezuelans Struggle To Find The Truth, Online : NPR", "url": "https://www.npr.org/2019/01/26/688868687/amid-chaos-venezuelans-struggle-to-find-the-truth-online", "author": "No author found", "published_date": "2019-01-26", "content": "SCOTT SIMON, HOST: Venezuelan media is controlled by the government. Figuring out what is truth, rumor, propaganda has always been difficult. In recent days, though, it's been even more confusing. President Nicolas Maduro has refused to cede power to the opposition party. There have been widespread protests and looting. And the rumor mill continues to churn on social media. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: Javier Rojo (ph) owns a pharmacy in the capital city of Caracas. This week, as chaos took over the country, he gave his workers the day off, went home and turned on the TV - only to find nothing was being reported. JAVIER ROJO: (Speaking Spanish). GARSD: Rojo says right now people get their news on social media and WhatsApp. Professor Gregory Weeks teaches Latin American politics at the University of North Carolina at Charlotte. He says in Venezuela. . . GREGORY WEEKS: Independent media has been gradually attacked and shut down over time so that, in general, social media becomes the means by which you learn what's going on on an ongoing basis. GARSD: Back at his house, Rojo says he started getting messages on WhatsApp from one of his workers. UNIDENTIFIED PERSON #1: (Speaking Spanish). GARSD: \"Tanks are rolling into the park,\" she says. \"They're launching tear gas. \" She's one of his employees. He trusts her. But then he started getting WhatsApp voice messages from people he doesn't even know. ROJO: (Speaking Spanish). GARSD: One guy, who says his aunt's husband is a military officer - and he swears Nicolas Maduro has resigned. Rojo is getting bombarded by fake news and wild rumors. And it's happening to a lot of people in the country. Professor Raisa Urribarri researches technology and politics at Universidad de Los Andes in Venezuela. She says it's hard to trace the origins of some messages. It can be panicked citizens or the opposition. The government has also gotten savvy at digital propaganda. RAISA URRIBARRI: (Speaking Spanish). GARSD: In the last few days, she says, there's been a wave of tweets in favor of the current regime - #ImWithMaduro - from many accounts she and her colleagues have traced back to Turkey, a country that has backed Maduro. It's not the first time social media has been caught up in international turmoil. Last year, the military in Myanmar used Facebook to spread rumors about the Rohingya Muslim minority, eventually leading to atrocities. In Brazil's last presidential elections, misinformation about candidates spread like wildfire on WhatsApp. Just this week, WhatsApp announced it will no longer allow users to forward messages to more than five people. But professor Urribarri says Venezuelans have lived with fake news for so long, they've become smart news consumers. She points me to this trusted WhatsApp group. UNIDENTIFIED PERSON #2: (Speaking Spanish). GARSD: The audio bulletin is so low production it almost sounds fake. But it's a public information broadcast created by a group of Venezuelan journalists. Every few hours, they release audio which gets shared countless times on Whatsapp, Facebook and Twitter. UNIDENTIFIED PERSON #2: (Speaking Spanish). GARSD: In this one broadcast, the announcer lists the neighborhoods and streets that have been experiencing violent clashes and looting. Follow us, he says. We are online. Jasmine Garsd, NPR News, New York. SCOTT SIMON, HOST:  Venezuelan media is controlled by the government. Figuring out what is truth, rumor, propaganda has always been difficult. In recent days, though, it's been even more confusing. President Nicolas Maduro has refused to cede power to the opposition party. There have been widespread protests and looting. And the rumor mill continues to churn on social media. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: Javier Rojo (ph) owns a pharmacy in the capital city of Caracas. This week, as chaos took over the country, he gave his workers the day off, went home and turned on the TV - only to find nothing was being reported. JAVIER ROJO: (Speaking Spanish). GARSD: Rojo says right now people get their news on social media and WhatsApp. Professor Gregory Weeks teaches Latin American politics at the University of North Carolina at Charlotte. He says in Venezuela. . . GREGORY WEEKS: Independent media has been gradually attacked and shut down over time so that, in general, social media becomes the means by which you learn what's going on on an ongoing basis. GARSD: Back at his house, Rojo says he started getting messages on WhatsApp from one of his workers. UNIDENTIFIED PERSON #1: (Speaking Spanish). GARSD: \"Tanks are rolling into the park,\" she says. \"They're launching tear gas. \" She's one of his employees. He trusts her. But then he started getting WhatsApp voice messages from people he doesn't even know. ROJO: (Speaking Spanish). GARSD: One guy, who says his aunt's husband is a military officer - and he swears Nicolas Maduro has resigned. Rojo is getting bombarded by fake news and wild rumors. And it's happening to a lot of people in the country. Professor Raisa Urribarri researches technology and politics at Universidad de Los Andes in Venezuela. She says it's hard to trace the origins of some messages. It can be panicked citizens or the opposition. The government has also gotten savvy at digital propaganda. RAISA URRIBARRI: (Speaking Spanish). GARSD: In the last few days, she says, there's been a wave of tweets in favor of the current regime - #ImWithMaduro - from many accounts she and her colleagues have traced back to Turkey, a country that has backed Maduro. It's not the first time social media has been caught up in international turmoil. Last year, the military in Myanmar used Facebook to spread rumors about the Rohingya Muslim minority, eventually leading to atrocities. In Brazil's last presidential elections, misinformation about candidates spread like wildfire on WhatsApp. Just this week, WhatsApp announced it will no longer allow users to forward messages to more than five people. But professor Urribarri says Venezuelans have lived with fake news for so long, they've become smart news consumers. She points me to this trusted WhatsApp group. UNIDENTIFIED PERSON #2: (Speaking Spanish). GARSD: The audio bulletin is so low production it almost sounds fake. But it's a public information broadcast created by a group of Venezuelan journalists. Every few hours, they release audio which gets shared countless times on Whatsapp, Facebook and Twitter. UNIDENTIFIED PERSON #2: (Speaking Spanish). GARSD: In this one broadcast, the announcer lists the neighborhoods and streets that have been experiencing violent clashes and looting. Follow us, he says. We are online. Jasmine Garsd, NPR News, New York.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-28-689457275": {"title": "U.S. Charges Chinese Telecom Giant Huawei, Asks Canada For CFO Extradition : NPR", "url": "https://www.npr.org/2019/01/28/689457275/u-s-charges-chinese-telecom-giant-huawei-asks-canada-for-cfo-extradition", "author": "No author found", "published_date": "2019-01-28", "content": "", "section": "National Security", "disclaimer": ""}, "2019-01-28-689473868": {"title": "This Time Humans Triumph Over Robots As They Take Back Hotel Jobs : NPR", "url": "https://www.npr.org/2019/01/28/689473868/this-time-humans-triumph-over-robots-as-they-take-back-hotel-jobs", "author": "No author found", "published_date": "2019-01-28", "content": "AUDIE CORNISH, HOST: Now a story about a disruptive technology that's not quite ready to disrupt, at least the hospitality industry. ARI SHAPIRO, HOST: Back in 2015, there was a lot of buzz around a new hotel in a remote area of Japan. (SOUNDBITE OF ARCHIVED RECORDING)SETH DOANE: The opening of a small, low-cost hotel doesn't usually warrant international attention. (SOUNDBITE OF ARCHIVED RECORDING)TIM HORNYAK: Would you check into a hotel staffed by robots? (SOUNDBITE OF ARCHIVED RECORDING)GAYLE KING: The machines can check you in. They can carry your luggage. They can even offer travel suggestions. CORNISH: The Henn-na Hotel was the world's first robot hotel, even got that title from \"The Guinness Book Of World Records\" - a robot concierge, robot porters and entertainers and robot vacuums. ALASTAIR GALE: One of the first things you see is this massive robotic arm that is used to store people's luggage. SHAPIRO: That's Alastair Gale, who covers Japan for The Wall Street Journal. He recently spent a night at the hotel. GALE: And you go in, and you turn the corner, and then you see these two dinosaurs - velociraptors, to be precise. SHAPIRO: Yes, talking robot dinosaurs at the reception desk. (SOUNDBITE OF ARCHIVED RECORDING)COMPUTER-GENERATED VOICE: (Speaking Japanese). CORNISH: Gale says those dinosaurs were the first red flag that something was amiss in robot paradise. GALE: The dinosaur is not capable of copying your passport or other ID, so what happens is a human member of staff comes out from sort of behind a curtain and completes the job for the dinosaur. SHAPIRO: Probably those tiny dinosaur arms. Turns out many of the robots are not great at their jobs. CORNISH: Gale says the piano playing robot in the lobby doesn't actually play the piano. SHAPIRO: The robot porter can't reach most of the rooms because it can't climb stairs or go outside. CORNISH: And the front desk, well. . . (SOUNDBITE OF ARCHIVED RECORDING)COMPUTER-GENERATED VOICE: Please ask me a request, but don't ask me a difficult question because I am a robot. SHAPIRO: Gale says the seven humans on staff were spending most of their time trying to recharge the robots or help guests when the robots failed. GALE: It was just impossible for the staff to keep on top of everything. And what happened was they, you know, told the staff, well, you're just going to have to work extra overtime, you know, to deal with the robot problems. CORNISH: One guest on TripAdvisor commented, this robot hotel is badly in need of some humanizing. SHAPIRO: Which is eventually what happened. Gale reports that as of this month, the Henn-na robot hotel has, for lack of a better word, fired over half its 243 robots. GALE: You know, it's quite sad because it does feel like - a bit like a sort of robot graveyard where there's robots around the place that are, you know, unplugged or in bags or sort of just been chained off. CORNISH: Elsewhere you might hear about technology making human jobs obsolete. But at the robot hotel, the robots' jobs are being reclaimed by humans. GALE: Interacting with human beings is very complicated, and it's something that if it doesn't go smoothly, it could be a very jarring experience. You know, you just get frustrated. That's not the kind of experience you want to have. So you don't want to annoy your guests. And it's going to take a while before they've - you know, they get it right. SHAPIRO: But don't worry. Those dinosaurs at the front desk - they made the cut. The velociraptors are still gainfully employed. (SOUNDBITE OF SONG, \"MR. ROBOTO\")STYX: (Singing) Domo arigato, Mr. Roboto - domo, domo. Domo arigato, Mr. Roboto. AUDIE CORNISH, HOST:  Now a story about a disruptive technology that's not quite ready to disrupt, at least the hospitality industry. ARI SHAPIRO, HOST:  Back in 2015, there was a lot of buzz around a new hotel in a remote area of Japan. (SOUNDBITE OF ARCHIVED RECORDING) SETH DOANE: The opening of a small, low-cost hotel doesn't usually warrant international attention. (SOUNDBITE OF ARCHIVED RECORDING) TIM HORNYAK: Would you check into a hotel staffed by robots? (SOUNDBITE OF ARCHIVED RECORDING) GAYLE KING: The machines can check you in. They can carry your luggage. They can even offer travel suggestions. CORNISH: The Henn-na Hotel was the world's first robot hotel, even got that title from \"The Guinness Book Of World Records\" - a robot concierge, robot porters and entertainers and robot vacuums. ALASTAIR GALE: One of the first things you see is this massive robotic arm that is used to store people's luggage. SHAPIRO: That's Alastair Gale, who covers Japan for The Wall Street Journal. He recently spent a night at the hotel. GALE: And you go in, and you turn the corner, and then you see these two dinosaurs - velociraptors, to be precise. SHAPIRO: Yes, talking robot dinosaurs at the reception desk. (SOUNDBITE OF ARCHIVED RECORDING) COMPUTER-GENERATED VOICE: (Speaking Japanese). CORNISH: Gale says those dinosaurs were the first red flag that something was amiss in robot paradise. GALE: The dinosaur is not capable of copying your passport or other ID, so what happens is a human member of staff comes out from sort of behind a curtain and completes the job for the dinosaur. SHAPIRO: Probably those tiny dinosaur arms. Turns out many of the robots are not great at their jobs. CORNISH: Gale says the piano playing robot in the lobby doesn't actually play the piano. SHAPIRO: The robot porter can't reach most of the rooms because it can't climb stairs or go outside. CORNISH: And the front desk, well. . . (SOUNDBITE OF ARCHIVED RECORDING) COMPUTER-GENERATED VOICE: Please ask me a request, but don't ask me a difficult question because I am a robot. SHAPIRO: Gale says the seven humans on staff were spending most of their time trying to recharge the robots or help guests when the robots failed. GALE: It was just impossible for the staff to keep on top of everything. And what happened was they, you know, told the staff, well, you're just going to have to work extra overtime, you know, to deal with the robot problems. CORNISH: One guest on TripAdvisor commented, this robot hotel is badly in need of some humanizing. SHAPIRO: Which is eventually what happened. Gale reports that as of this month, the Henn-na robot hotel has, for lack of a better word, fired over half its 243 robots. GALE: You know, it's quite sad because it does feel like - a bit like a sort of robot graveyard where there's robots around the place that are, you know, unplugged or in bags or sort of just been chained off. CORNISH: Elsewhere you might hear about technology making human jobs obsolete. But at the robot hotel, the robots' jobs are being reclaimed by humans. GALE: Interacting with human beings is very complicated, and it's something that if it doesn't go smoothly, it could be a very jarring experience. You know, you just get frustrated. That's not the kind of experience you want to have. So you don't want to annoy your guests. And it's going to take a while before they've - you know, they get it right. SHAPIRO: But don't worry. Those dinosaurs at the front desk - they made the cut. The velociraptors are still gainfully employed. (SOUNDBITE OF SONG, \"MR. ROBOTO\") STYX: (Singing) Domo arigato, Mr. Roboto - domo, domo. Domo arigato, Mr. Roboto.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-28-689473801": {"title": "YouTube To Stop Promoting Videos That Spread Misinformation : NPR", "url": "https://www.npr.org/2019/01/28/689473801/youtube-to-stop-promoting-videos-that-spread-misinformation", "author": "No author found", "published_date": "2019-01-28", "content": "AUDIE CORNISH, HOST: Watch a video on YouTube, and you'll see a list of recommendations for what to watch next based on what you're watching at the moment and your search history. But it doesn't take much to go from a video that's fairly innocuous to one that promotes conspiracy theories. That happens frequently enough that YouTube has come under pressure to change its algorithm. It says it will now promote fewer videos of what it calls borderline content. NPR's Andrew Limbong has more. ANDREW LIMBONG, BYLINE: In a blog post, YouTube defines borderline content as things that, quote, \"misinform users in harmful ways\" but don't quite violate their community guidelines. The company specifically cites flat Earth conspiracies. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: We do not believe that we're flying in space whatsoever. We don't believe the Earth moves at all. LIMBONG: . . . Phony miracle cures and 9/11 truth videos. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: Western civilization is doomed unless we face the unanswered questions of 9/11. LIMBONG: There are plenty of other misinformation videos on YouTube, from anti-vaccine rants to conspiracies of school shootings being faked. (SOUNDBITE OF ARCHIVED RECORDING)JOHN BOUCHELL: In my utterly qualified, expert opinion, there are several troubling facts being dispensed that I refuse to accept. LIMBONG: These misinformation and conspiracy videos will still all exist on YouTube. They just won't be recommended to you. You'll have to look for them. Google, which owns YouTube, declined to offer anyone up for an interview, but the company says it will, quote, \"work with human evaluators and experts from all over the United States to help train the machine learning systems that generate recommendations. \"Zeynep Tufekci is an associate professor at the University of North Carolina studying the social impacts of digital technology and artificial intelligence. She says the big problem with the YouTube recommendation machine is that it's designed to get you to spend as much time on the platform as possible so they can sell more ads. The accuracy of the content doesn't matter. ZEYNEP TUFEKCI: Just like a cafeteria, you're going to get people to eat more if you serve unhealthy food again and again and again before they even have a chance to finish their plate. LIMBONG: And its effect - she adds that it's increasingly schoolchildren turning to YouTube for information and getting fed these types of videos. She wrote about the issue a while back in The New York Times. TUFEKCI: And I got flooded with examples and comments. Like, parents would put their kid in front of YouTube with a video from NASA - right? - some very innocuous, interesting content, which YouTube is full of. And 45 minutes later, the kid would come back and say, Mom, the moon landing never happened. LIMBONG: YouTube is rolling out these changes to its recommendation machine gradually in the United States first, affecting less than 1 percent of all YouTube content. But Tufekci says it's around the rest of the world - Brazil, Indonesia, Sri Lanka - where misinformation on YouTube truly has the power to destabilize societies. Andrew Limbong, NPR News. AUDIE CORNISH, HOST:  Watch a video on YouTube, and you'll see a list of recommendations for what to watch next based on what you're watching at the moment and your search history. But it doesn't take much to go from a video that's fairly innocuous to one that promotes conspiracy theories. That happens frequently enough that YouTube has come under pressure to change its algorithm. It says it will now promote fewer videos of what it calls borderline content. NPR's Andrew Limbong has more. ANDREW LIMBONG, BYLINE: In a blog post, YouTube defines borderline content as things that, quote, \"misinform users in harmful ways\" but don't quite violate their community guidelines. The company specifically cites flat Earth conspiracies. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: We do not believe that we're flying in space whatsoever. We don't believe the Earth moves at all. LIMBONG: . . . Phony miracle cures and 9/11 truth videos. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #2: Western civilization is doomed unless we face the unanswered questions of 9/11. LIMBONG: There are plenty of other misinformation videos on YouTube, from anti-vaccine rants to conspiracies of school shootings being faked. (SOUNDBITE OF ARCHIVED RECORDING) JOHN BOUCHELL: In my utterly qualified, expert opinion, there are several troubling facts being dispensed that I refuse to accept. LIMBONG: These misinformation and conspiracy videos will still all exist on YouTube. They just won't be recommended to you. You'll have to look for them. Google, which owns YouTube, declined to offer anyone up for an interview, but the company says it will, quote, \"work with human evaluators and experts from all over the United States to help train the machine learning systems that generate recommendations. \" Zeynep Tufekci is an associate professor at the University of North Carolina studying the social impacts of digital technology and artificial intelligence. She says the big problem with the YouTube recommendation machine is that it's designed to get you to spend as much time on the platform as possible so they can sell more ads. The accuracy of the content doesn't matter. ZEYNEP TUFEKCI: Just like a cafeteria, you're going to get people to eat more if you serve unhealthy food again and again and again before they even have a chance to finish their plate. LIMBONG: And its effect - she adds that it's increasingly schoolchildren turning to YouTube for information and getting fed these types of videos. She wrote about the issue a while back in The New York Times. TUFEKCI: And I got flooded with examples and comments. Like, parents would put their kid in front of YouTube with a video from NASA - right? - some very innocuous, interesting content, which YouTube is full of. And 45 minutes later, the kid would come back and say, Mom, the moon landing never happened. LIMBONG: YouTube is rolling out these changes to its recommendation machine gradually in the United States first, affecting less than 1 percent of all YouTube content. But Tufekci says it's around the rest of the world - Brazil, Indonesia, Sri Lanka - where misinformation on YouTube truly has the power to destabilize societies. Andrew Limbong, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-28-689215618": {"title": "'Team Human' Stresses That The Future Lies In Connection And Cooperation : NPR", "url": "https://www.npr.org/2019/01/28/689215618/team-human-stresses-that-the-future-lies-in-connection-and-cooperation", "author": "No author found", "published_date": "2019-01-28", "content": "", "section": "Book Reviews", "disclaimer": ""}, "2019-01-29-689663720": {"title": "A Robot Named 'Tappy': Huawei Conspired To Steal T-Mobile's Trade Secrets, Says DOJ : NPR", "url": "https://www.npr.org/2019/01/29/689663720/a-robot-named-tappy-huawei-conspired-to-steal-t-mobile-s-trade-secrets-says-doj", "author": "No author found", "published_date": "2019-01-29", "content": "", "section": "National Security", "disclaimer": ""}, "2019-01-29-689636708": {"title": "Spy Boss Coats Warns That Russia, Others Plot New Interference Techniques For 2020 : NPR", "url": "https://www.npr.org/2019/01/29/689636708/spy-boss-coats-warns-russia-others-plot-new-interference-techniques-for-2020", "author": "No author found", "published_date": "2019-01-29", "content": "", "section": "National Security", "disclaimer": ""}, "2019-01-29-689581417": {"title": "Apple Disables Group FaceTime After Security Flaw Let Callers Secretly Eavesdrop : NPR", "url": "https://www.npr.org/2019/01/29/689581417/apple-disables-group-facetime-after-security-flaw-let-callers-secretly-eavesdrop", "author": "No author found", "published_date": "2019-01-29", "content": "", "section": "Technology", "disclaimer": ""}, "2019-01-30-690172103": {"title": "Facebook, Google Draw Scrutiny Over Apps That Collected Data From Teens : NPR", "url": "https://www.npr.org/2019/01/30/690172103/facebook-google-draw-scrutiny-over-apps-that-collected-data-from-teens", "author": "No author found", "published_date": "2019-01-30", "content": "STEVE INSKEEP, HOST: Embarrassments continue for Facebook over its collection of users' data. Both Facebook and Google have been offering cash and gift incentives to users, persuading them to share information about almost everything people do on their phones. At least in this case, unlike some others, Facebook did ask first. But some targeted users were as young as 13, which is awkward. Although, apparently, it's only awkward up to a point for Facebook because the company also posted record profits on Wednesday. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: According to an investigative report by the news site TechCrunch, Facebook has been paying young users up to $20 a month to install a Facebook research app since 2016. It gives Facebook unlimited access to their phones. Katie Moussouris is the founder and CEO of Luta, a cybersecurity company. She says Facebook's motivation is that it's losing ground among young people. KATIE MOUSSOURIS: If teenagers are sharing a lot of memes, Facebook would then say, OK. You know what? We should build something into the platform that lets you share memes a lot easier. And then maybe we can attract those users back. GARSD: This is not the first time Facebook is accused of going to extreme lengths to get user data. Back in 2013, Facebook bought a company called Onavo and allegedly used the Onavo app to get more information about a competitor, the messaging platform WhatsApp, which Facebook ultimately bought. The term corporate espionage was thrown around. And Apple was not happy that all this was happening on its app store. Here's Apple CEO Tim Cook on MSNBC indirectly chastising Facebook's disregard for privacy. (SOUNDBITE OF ARCHIVED RECORDING)TIM COOK: We could make a ton of money if we monetized our customer. But you are not our product. GARSD: Apple forced Facebook to remove Onavo from the app store, but Facebook came right back under a different guise. Will Strafach is a mobile security researcher. He studied the app for TechCrunch's recent expose. WILL STRAFACH: To use it this way and under their own name is just amazing to me because I don't understand what they thought they were doing or how they thought they could get away with this. GARSD: Apple has banned Facebook's research app. And it's clearly not happy about what happened yet again. It was widely reported yesterday that Apple dropped some of Facebook's in-house apps - apps that Facebook employees use to look up bus schedules and lunch menus. Facebook is not the only tech company to be caught in this way. Yesterday, Google pulled the app Screenwise Meter, which basically collects information on how people use the Internet. And while a lot of users might find this shocking, Katie Moussouris from Luta Security is not surprised. What she does wonder about is a generation that would give any amount of access to their private lives. She gets why. MOUSSOURIS: Some of these children have grown up with virtually no privacy at all. Their photos were shared by their parents, by their families before they could ever consent to it. So I think for them, it feels - it probably feels like there's nothing left to hide. GARSD: You want big tech to change its behavior. Time and again, it hasn't. Moussouris says change will come from users figuring out when enough is enough. Jasmine Garsd, NPR News, New York. INSKEEP: And in the interest of full disclosure, we note that Facebook is an NPR underwriter. STEVE INSKEEP, HOST:  Embarrassments continue for Facebook over its collection of users' data. Both Facebook and Google have been offering cash and gift incentives to users, persuading them to share information about almost everything people do on their phones. At least in this case, unlike some others, Facebook did ask first. But some targeted users were as young as 13, which is awkward. Although, apparently, it's only awkward up to a point for Facebook because the company also posted record profits on Wednesday. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: According to an investigative report by the news site TechCrunch, Facebook has been paying young users up to $20 a month to install a Facebook research app since 2016. It gives Facebook unlimited access to their phones. Katie Moussouris is the founder and CEO of Luta, a cybersecurity company. She says Facebook's motivation is that it's losing ground among young people. KATIE MOUSSOURIS: If teenagers are sharing a lot of memes, Facebook would then say, OK. You know what? We should build something into the platform that lets you share memes a lot easier. And then maybe we can attract those users back. GARSD: This is not the first time Facebook is accused of going to extreme lengths to get user data. Back in 2013, Facebook bought a company called Onavo and allegedly used the Onavo app to get more information about a competitor, the messaging platform WhatsApp, which Facebook ultimately bought. The term corporate espionage was thrown around. And Apple was not happy that all this was happening on its app store. Here's Apple CEO Tim Cook on MSNBC indirectly chastising Facebook's disregard for privacy. (SOUNDBITE OF ARCHIVED RECORDING) TIM COOK: We could make a ton of money if we monetized our customer. But you are not our product. GARSD: Apple forced Facebook to remove Onavo from the app store, but Facebook came right back under a different guise. Will Strafach is a mobile security researcher. He studied the app for TechCrunch's recent expose. WILL STRAFACH: To use it this way and under their own name is just amazing to me because I don't understand what they thought they were doing or how they thought they could get away with this. GARSD: Apple has banned Facebook's research app. And it's clearly not happy about what happened yet again. It was widely reported yesterday that Apple dropped some of Facebook's in-house apps - apps that Facebook employees use to look up bus schedules and lunch menus. Facebook is not the only tech company to be caught in this way. Yesterday, Google pulled the app Screenwise Meter, which basically collects information on how people use the Internet. And while a lot of users might find this shocking, Katie Moussouris from Luta Security is not surprised. What she does wonder about is a generation that would give any amount of access to their private lives. She gets why. MOUSSOURIS: Some of these children have grown up with virtually no privacy at all. Their photos were shared by their parents, by their families before they could ever consent to it. So I think for them, it feels - it probably feels like there's nothing left to hide. GARSD: You want big tech to change its behavior. Time and again, it hasn't. Moussouris says change will come from users figuring out when enough is enough. Jasmine Garsd, NPR News, New York. INSKEEP: And in the interest of full disclosure, we note that Facebook is an NPR underwriter.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-31-690126426": {"title": "NASA's Mars Rover May Have Solved A Mountain Mystery : NPR", "url": "https://www.npr.org/2019/01/31/690126426/exploring-the-mysterious-origins-of-mars-3-mile-high-sand-pile", "author": "No author found", "published_date": "2019-01-31", "content": "ARI SHAPIRO, HOST: There is a three-mile tall mountain in the middle of a crater on Mars, and scientists have been debating how it got there. A new study suggests the mountain is largely made from just dust and sand. To get the data for that conclusion, the researchers MacGyvered a navigation instrument on the NASA rover called Curiosity. NPR science correspondent Joe Palca has the story. JOE PALCA, BYLINE: How do you know what a planet is made of? Well, you can learn a lot about the geology of a planet by measuring subtle changes in its gravity. High-density rocks give us stronger gravity signal than low-density rocks. But to make gravity measurements, you usually have to have an instrument called a gravimeter. KEVIN LEWIS: And it kind of frustrated me that we didn't have a surface gravimeter on Mars. PALCA: Kevin Lewis is a planetary scientist at Johns Hopkins University and a member of the Curiosity rover science team. One day, Lewis started thinking about something that popped up regularly on the rover's daily activity schedule. LEWIS: Turns out every day we don't drive with the rover, there's this little five-minute activity called the SAPP-RIMU data collection activity. PALCA: The RIMU is a navigational device, and the SAPP-RIMU data activity tells engineers the rover's precise orientation on the planet's surface. Inside the RIMU are three accelerometers, and accelerometers measure acceleration - duh. You actually have accelerometers inside your smartphone that measure your movements. Anyway, thinking about these accelerometers in the RIMU, Kevin Lewis had a kind of epiphany. LEWIS: We don't have a gravimeter on the surface of Mars. But we have accelerometers, and gravity is just an acceleration. PALCA: You may not think of gravity that way. But you can, and scientists do. So by adjusting the way the data from the RIMU were handled, Lewis now had his gravimeter. And he knew just what he wanted to do with it - try to figure out how a 15,000-foot tall mountain could form in the middle of Gale crater, the crater Curiosity landed in. It's a question many scientists have puzzled over. MACKENZIE DAY: And there sort of have been two different schools of thought. PALCA: Mackenzie Day is a planetary scientist at UCLA. DAY: Craters are fundamentally big holes in the ground, so they're a really good place to accumulate things - to accumulate sediment, accumulate dust and sand. PALCA: Day says the question is, was Gale crater once filled to the rim with sediment, and then most of that material eroded away leaving behind what's now called Mount Sharp? DAY: Or is Mount Sharp something that developed in the middle of the crater as sort of a stacking of material in the crater center from winds coming down the sides of the crater rim? PALCA: Could blowing sand and dust really pack together to build a three-mile tall mountain? Possibly, although Kevin Lewis says it runs against our expectations. LEWIS: We don't normally see mountains just growing up as a haystack on the Earth. PALCA: If the haystack model is right, the rocks at the base of Mount Sharp wouldn't be very dense. And as he and his colleagues report in the journal Science. . . LEWIS: And what we found in this study is that the rocks are surprisingly low-density. PALCA: So the haystack theory may be right. Lewis says he plans to keep collecting data from his MacGyvered instrument as Curiosity climbs up Mount Sharp to see if the initial results hold up. Joe Palca, NPR News. (SOUNDBITE OF THE POLISH AMBASSADOR'S \"FOREST FUNK\") ARI SHAPIRO, HOST:  There is a three-mile tall mountain in the middle of a crater on Mars, and scientists have been debating how it got there. A new study suggests the mountain is largely made from just dust and sand. To get the data for that conclusion, the researchers MacGyvered a navigation instrument on the NASA rover called Curiosity. NPR science correspondent Joe Palca has the story. JOE PALCA, BYLINE: How do you know what a planet is made of? Well, you can learn a lot about the geology of a planet by measuring subtle changes in its gravity. High-density rocks give us stronger gravity signal than low-density rocks. But to make gravity measurements, you usually have to have an instrument called a gravimeter. KEVIN LEWIS: And it kind of frustrated me that we didn't have a surface gravimeter on Mars. PALCA: Kevin Lewis is a planetary scientist at Johns Hopkins University and a member of the Curiosity rover science team. One day, Lewis started thinking about something that popped up regularly on the rover's daily activity schedule. LEWIS: Turns out every day we don't drive with the rover, there's this little five-minute activity called the SAPP-RIMU data collection activity. PALCA: The RIMU is a navigational device, and the SAPP-RIMU data activity tells engineers the rover's precise orientation on the planet's surface. Inside the RIMU are three accelerometers, and accelerometers measure acceleration - duh. You actually have accelerometers inside your smartphone that measure your movements. Anyway, thinking about these accelerometers in the RIMU, Kevin Lewis had a kind of epiphany. LEWIS: We don't have a gravimeter on the surface of Mars. But we have accelerometers, and gravity is just an acceleration. PALCA: You may not think of gravity that way. But you can, and scientists do. So by adjusting the way the data from the RIMU were handled, Lewis now had his gravimeter. And he knew just what he wanted to do with it - try to figure out how a 15,000-foot tall mountain could form in the middle of Gale crater, the crater Curiosity landed in. It's a question many scientists have puzzled over. MACKENZIE DAY: And there sort of have been two different schools of thought. PALCA: Mackenzie Day is a planetary scientist at UCLA. DAY: Craters are fundamentally big holes in the ground, so they're a really good place to accumulate things - to accumulate sediment, accumulate dust and sand. PALCA: Day says the question is, was Gale crater once filled to the rim with sediment, and then most of that material eroded away leaving behind what's now called Mount Sharp? DAY: Or is Mount Sharp something that developed in the middle of the crater as sort of a stacking of material in the crater center from winds coming down the sides of the crater rim? PALCA: Could blowing sand and dust really pack together to build a three-mile tall mountain? Possibly, although Kevin Lewis says it runs against our expectations. LEWIS: We don't normally see mountains just growing up as a haystack on the Earth. PALCA: If the haystack model is right, the rocks at the base of Mount Sharp wouldn't be very dense. And as he and his colleagues report in the journal Science. . . LEWIS: And what we found in this study is that the rocks are surprisingly low-density. PALCA: So the haystack theory may be right. Lewis says he plans to keep collecting data from his MacGyvered instrument as Curiosity climbs up Mount Sharp to see if the initial results hold up. Joe Palca, NPR News. (SOUNDBITE OF THE POLISH AMBASSADOR'S \"FOREST FUNK\")", "section": "Joe's Big Idea", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-01-31-690291785": {"title": "Could Chinese Telecom Giant Huawei Put U.S. Cyber-Security At Risk? : NPR", "url": "https://www.npr.org/2019/01/31/690291785/could-chinese-telecom-giant-huawei-put-u-s-cyber-security-at-risk", "author": "No author found", "published_date": "2019-01-31", "content": "TERRY GROSS, HOST: This is FRESH AIR. I'm Terry Gross. There's good news about the future of the Internet. A new 5G network is being created now, which will not only offer faster downloading on cell phones. It will provide the kind of connectivity we need in the era of the Internet of Things - driverless cars, Internet-connected medical devices, smart TVs and virtual assistants. But there are dangers that could be lurking in the equipment needed to build the new network. The Chinese telecommunications equipment giant Huawei is dominating the creation of 5G networks around the world. For years, classified intelligence reports from the U. S. have warned that China would one day use Huawei to penetrate American networks for cyber-espionage or cyberattacks. In the U. S. , the National Security Agency has banned AT&T and Verizon from using Huawei products in America's 5G network. And last month, the U. S. had a top executive from Huawei arrested in Canada so she could be extradited to the U. S. The growing cyberthreat posed by China was stressed in the Worldwide Threat Assessment - a report from the U. S. intelligence community - that was released this week. And all this is part of the backdrop for this week's trade negotiations between the U. S. and China. My guest David Sanger is the author of a book about cyberwar and cyber-sabotage called \"The Perfect Weapon. \" He's a national security correspondent for The New York Times. David Sanger, welcome back to FRESH AIR. Let's start with the 5G network. What is it? And how will it affect our phones, our devices and all our interconnectivity? DAVID SANGER: Well, at its simplest, the 5G network is an increase in speed and range for what you see on your cell phone. So 5G means just fifth generation. But it's actually much more than that. The hope is that when you're using your phone or some other device over Wi-Fi, you'll get no lag time and that you'll get near instantaneous download of data, webpages and so forth. But as 5G was being rolled out, there was a recognition that the Internet had fundamentally changed, that this was a moment to roll out something that could accommodate a world in which the Internet of Things was connecting up to all of these other wireless devices. And so that's autonomous cars, which, of course, need to constantly get data back and forth from the cloud, constant connectivity so that they know where they are in addition to their sensors helping you drive. It's for every other Internet-connected device that you have. And, you know, when you think about it, Terry, it was just about 10 years ago that in your own house, you probably only had one or two Internet-connected devices - a laptop computer and a desktop computer, maybe. But today you walk into your house and, you know, you've got your Fitbit. And you're waking up Alexa and getting it to play you music. And you've got a smart TV. And you probably have an Internet-connected car parked outside. Even if it isn't a fancy car, most basic cars have some Internet connectivity to them. You might have an Internet-connected refrigerator. You have all of these different devices. And right now worldwide - at the end of last year, we think that there were about 14 billion Internet of Things devices around the world. And by the end of next year, 2020, the estimate is there will be 20 billion. So that gives you a sense of how rapidly we're changing the environment. And the next network has to be able to handle all of that and, of course, handle the GPS needs for navigation, handle greater government and military needs. So this next 5G network is more than just something that'll make your phone faster. It's actually going to be the central nervous system, the backbone of the next generation of the Internet. GROSS: And that's exactly the concern about the Chinese telecom giant Huawei because they're building some of these 5G networks around the world. So what is the concern about this Chinese telecom giant and the 5G network? SANGER: Well, the first thing about Huawei is that while most Americans haven't come in direct contact with it because Huawei phones are not sold that widely in the United States, they are sold nearly universally when you're in Asia and very widely in Europe, in Africa, in Latin America. In fact, at the end of last year, Huawei actually just edged out Apple as the second-largest provider of cell phones in the world. The only one ahead of it is Samsung. But the other part of their business and the part that we really worry about the most is the construction of the giant switches that make up these 5G networks. Now, in the old days of switching, you would think of switches as big, physical devices. What's happened and what is particularly notable about 5G is that the network itself, while it has some hardware to it and, obviously, there are cell phone towers and sort of a radio part of it, the switches are almost entirely software. And they constantly reconfigure themselves. And they are enormously complex. So the old days of doing what the defense department and the National Security Agency and others used to do - which is take a piece of foreign equipment, put it in a laboratory, poke around it, try to figure out if there are flaws or back doors or something that could help an adversary - that's virtually impossible to do when the product is an ever-evolving piece of software. It gets updated as often as your iPhone gets updated when you have it sitting on your bedside table and Apple sends a new, updated operating system to it overnight. So that's what will happen with this 5G network. The concern is that for the first time in our history, we would be reliant on a foreign manufacturer - in this case, a foreign manufacturer of a potential adversary that's also the world's second largest economy - building the highway, backbone, central nervous system of a system that we rely on for everything from our financial transactions to our - many of our military operations to, of course, our communications. And the question is, can you trust a foreign actor to be responsible for that? And what happens when Huawei gets that command under Chinese law from the Chinese government to either grab a piece of information or close down part of the network? GROSS: So describe some more of the kind of trouble that China could create with Huawei building 5G networks in countries around the world where China is becoming increasingly powerful. SANGER: If China is in command of the network itself and has sort of end end control from phones for which it makes its own chips to the software on the switch to all of the other tentacles of the central nervous system, that it, basically, can do whatever it wants. And the chances that you would see it are relatively diminished. Big network operators like AT&T and Verizon, if they bought Huawei equipment - and it's pretty clear the government is not going to allow them to do that - would have some visibility into the system. But it's also possible that Huawei might be able to reach back from China directly into the equipment and software it's put in to go manipulate data. What could you do with that? Well, in the Worldwide Threat Assessment that came out earlier this week, the nation's intelligence chiefs mentioned, in particular, that China already has the capability to shut down, at least briefly, the natural gas network. They also said the Russians could do the same briefly with the electric grid. If you had a country that was in full control of your networks, they could shut it down. They could siphon the traffic off to a place you didn't want it to go. They could siphon it back to China. And they would probably have a easier time intercepting it. Now, of course, a lot of that traffic is going to run encrypted. It's not as if the Chinese would be able to look at everything or would want to. But the more network equipment they put in, the more control they would have. And, of course, the Chinese government reserves the right to tell them what to go do with it. GROSS: So is Huawei involved in any aspect of the construction of the 5G network in the U. S. ? SANGER: Not anything notable in the United States now - there were a number of classified meetings that took place between the intelligence agencies, the executives of the big telecom firms - AT&T, Verizon, T-Mobile, so forth. And there was a lot of discussion about letting - whether or not to let Huawei bid on the construction of some parts of this 5G network as they are doing around the world. And, in fact, some of the telecom companies argued to Congress and to the intelligence agencies that it would make sense to let Huawei bid. And the reason is that if they wanted to bid, they would have to provide their software, their equipment to a test facility in the United States. And the National Security Agency, which is the nation's largest electronic spy agency, and the telecom providers would all be able to crawl around in that software and see if there were any backdoors, see how it was designed. But in the end, the intelligence community didn't even want to take that risk. GROSS: The founder of Huawei is a former engineer with the People's Liberation Army in China, and some people think he's still connected to the People's Liberation Army. And some people also argue that private companies like Huawei are still under the control of Chinese - of China's authoritarian government. So what's your assessment of how much control the Chinese government has over Huawei and how much control Chinese intelligence has over Huawei and has access to whatever data Huawei gets or can tell Huawei, collect data from this country; collect this data from that country? SANGER: Well, the founder who you're describing, Ren Zhengfei, is, as you said, a former PLA officer. He then, when he left the PLA, built Huawei initially in the - wiring up the rural parts of China. And he's become obviously one of the most powerful businessmen in China. He's a member of the Communist Party. He's enormously influential there. He has insisted that the Chinese military, the PLA, which has done much of the hacking against the United States, has no role in his company and no continuing control. I've never found any evidence that the United States could prove that the PLA has operative control over Huawei. And of course, Mr. Zheng (ph) has denied it. Now, that issue consumed the U. S. government for years and years until about two years ago when the Chinese government issued a new set of laws under President Xi, Xi Jinping, that basically said any Chinese company - but particularly the telecom companies - would have to participate in Chinese intelligence operations if they are so instructed, that they would have to turn over data that they had. It's not clear, of course, in China what the legal process would be. And so now people say to me, you know, it doesn't make any difference, David, whether or not the PLA has control over Huawei because the law means that the Chinese government has turned the company into its agent. GROSS: Well, let's take a short break here, and then we'll talk some more. If you're just joining us, my guest is David Sanger. He's a national security correspondent for The New York Times, and his latest book is called \"The Perfect Weapon: War, Sabotage, And Fear In The Cyber Age. \" We'll be right back. This is FRESH AIR. (SOUNDBITE OF ALEXANDRE DESPLAT'S \"SPY MEETING\")GROSS: This is FRESH AIR. Let's get back to my interview with David Sanger, a national security correspondent for The New York Times. We're talking about the 5G network that is being created now and will soon become the central nervous system of the Internet. The Chinese telecom equipment giant Huawei is dominating the creation of 5G networks around the world. For years, classified U. S. intelligence reports have warned that China would one day use Huawei to penetrate American networks for cyber espionage or cyberattacks. So we were talking about the founder of Huawei and his connections to the Chinese military and Huawei's connections to the Chinese government. The founder of Huawei - his daughter was arrested in Canada at the request of the U. S. and indicted last week. So what are those charges? And what are their significance? SANGER: Well, the first thing we know about the daughter is, in her own right, she's a very powerful figure within Huawei. She's the chief financial officer. But she's also been the architect of a lot of Huawei's spread around the world. When she was arrested in Canada in December at the request of the United States, it was not for any charges that Huawei had participated in espionage against the U. S. It was not for - on any charges that it had participated in cyberattacks on the United States. Instead, it was based on a charge that she had been behind a giant fraud in which Huawei used a cut-out company to violate the sanctions that the United States had against Iran and that this company was in fact a Huawei - secretly a Huawei subsidiary and was doing business with Iran in violation of those sanctions. So the U. S. is using the Iran sanctions violation to go after the company. Now, the other interesting part about this is that President Trump at one point in December after she was arrested but before she was indicted publicly mused about the fact that he might trade her away in the course of the trade negotiations. Thus the president was politicizing what until then had been a legal action going through normal legal channels. And one of the interesting questions is whether the Canadians are going to extradite her because there have been several Canadians now arrested in China so that they have counter-hostages. And it's possible that the Canadians may determine that this case is more political than criminal. GROSS: President Trump has said that he knows more about technology than anyone. What's your understanding of how much he understands the issue of Huawei and its potential ability to hack America, to spy on America? SANGER: Well, I have a couple of concerns. The first is the president's mind turns, as he has said himself many times, to things you can build. And so issues around software networks and so forth just don't come naturally to him. He hasn't shown much of an inclination to learn about it during the time he's president according to people who used to be in the administration, including in the national security field. The president has a somewhat hazy understanding about the risks of cyber-escalation. So when you think about our cyber-risk, one concern is one we've discussed already today, which is surveillance. Basically, you can use these networks to steal data. But the coming concerns, Terry, have to do more with data manipulation - what happens if you change the data? That's the problem of deep fakes. Something that would look like a politician was speaking, but it wasn't really the words coming out of his mouth. It's been faked. It could be substituting numbers and financial transactions. It could be substituting the targeting information in nuclear or non-nuclear weapons. It could be changing the blood types of every soldier and sailor in the United States if you got into the databases of the military. So there's data manipulation that's a concern. And if any country that had access to the networks, you would worry about that. And then the third is cyberattack, and that is that if we went to war or were conducting covert operations, every country in the world now has cyber in its battle plans, and usually in the first 24 hours of its battle plans. In \"The Perfect Weapon,\" I describe a plan the United States had if we went to war with Iran, called Nitro Zeus, to basically unplug Iran's communications and electricity grids. Well, imagine that that's in the Chinese plans for the United States. If they're in control of the communications grid of the U. S. or its allies, you can imagine how much easier that is to do. Now, there is a concern here that we could get into a world of Red Scare, and the president himself might be fueling that some. And I have concerns that we're blaming too much on the Chinese. But the fact of the matter is, these are all major, complex vulnerabilities that, as Henry Kissinger said to me as I was working on the book, are so much more complex than the issues that came up with China in the Cold War. GROSS: And you're concerned that our president doesn't really comprehend those issues and therefore can't adequately address them. SANGER: That's right. And, you know, there are escalation issues here, as well. I mean, there's still a big debate in the United States government about how you respond to a cyberattack. When the Chinese got into Google and other companies in 2009, there were Google engineers who wanted to retaliate directly against the servers where the attack was coming from. Fortunately, they were stopped. But had that gone ahead, or had it gone ahead with another company, the question would be to the Chinese, is this attack, this counterattack, coming from a private company, coming from some hackers, has it been commanded by the United States? Is a company operating on behalf of the U. S. ? And then they would escalate. So you get all the same kind of escalation issues that we worried about in the nuclear age, but you get them in this technology world in which shutting down or diverting data becomes your new weapon. And we don't really understand that escalatory response. Now, the president, in August of last year, issued a new classified order to the National Security Agency that basically gave the director of the National Security Agency more leeway to go respond to offensive cyberactions and to initiate some without presidential approval. But still we don't understand who's in control of the escalation. GROSS: My guest is David Sanger, a national security correspondent for The New York Times who is also the author of the book, \"The Perfect Weapon,\" about cyberwar and cyber-espionage. We'll talk more after we take a short break. And our jazz critic Kevin Whitehead will review a reissue featuring pianist Oscar Peterson and a studio orchestra playing 1960s pop covers. This is FRESH AIR. (SOUNDBITE OF MUSIC)GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to my interview with David Sanger, a national security correspondent for The New York Times and author of \"The Perfect Weapon: War, Sabotage, And Fear In The Cyber Age. \" We're talking about the new 5G network that will soon become the central nervous system of the Internet, providing faster speeds and greater interconnectivity for the era we're entering where nearly everything, from cars to medical devices, connects to the Internet. The Chinese telecom equipment giant Huawei is dominating the creation of 5G networks around the world. For years, classified U. S. intelligence reports have warned that China would one day use Huawei to penetrate American networks for cyber-espionage or cyberattacks. The Worldwide Threat Assessment that was released by intelligence agencies this week said that there's a growing cyberthreat from Russia and China, and that Russia and China are now more aligned than at any point since the mid-1950s. So what does that say to you about the cyberthreat posed by this alignment of Russia and China? SANGER: Well, it was a fascinating assertion that they made, And I think it was accurate. We have not seen Russia and China cooperate this way since the mid-'50s. And, of course, China only came into being as the People's Republic in 1949. So it was very early in its history, and it was an extremely poor country. It's worth remembering today that Russia and China have very different objectives. Russia's main objective is one of disruption. It does not have much economic power. Its economy is basically the size of Italy's. It does not have the ability to go build these networks. It does not have much economic power. Its economy is basically the size of Italy's. It does not have the ability to go build these networks around the world the way China is doing. It does not have the technology to do it, but it can be a huge disruptor. And of course, we saw them act to disrupt networks and voting systems in Ukraine. We saw their interference in the 2016 election here in the United States, and we've seen them take cyber action elsewhere in the world. We've seen their submarines go out and track where the fiber optic cables are laid around the world. They're only, you know, less than 200 major fiber optic undersea cables. And the Russians have the ability to cut those cables deep undersea. That would be a huge disruption. That could black out communications in the United States. So that's the Russian side. The Chinese have a much different set of objectives. If the world gets disrupted, no one's going to suffer more than they will because their economy is so interdependent with ours and with other major economies around the world. So they're less likely to disrupt, but they're much more likely to want control and subtle ability to divert traffic in those networks. And that's the concern about Huawei. GROSS: But with Russia and China being in closer cooperation than at any other time since the 1950s, what's the combination of those two countries looking like? Like, if Russia's about disruption and China is about control, when you put the two together, what's the new formula? SANGER: Well, the new formula is a diminished role for the United States. That's part of the concern about the degree to which we have alienated our allies. And this issue about Huawei intersects with the alienation of the allies very closely because what's going on now is the United States is going around the world to allies and say, hey, we're living in a new world. The Russians and the Chinese are cooperating more than we've ever seen. We're trying to keep everybody from spinning into a new form of a cold war - Cold War 2. 0. And while the Russians and the Chinese have very different strategic objectives, there may be moments - there will be moments when they will have a common objective in diminishing the power of the United States. That's the one area where they both have great common interests. And so it's important that the United States be able to go work with its allies to figure out how you both contain this threat and respond to it, but also how you retain control of your own networks. So what the U. S. is doing right now is it's going around to its allies, particularly the NATO allies, and saying, don't build Huawei into your systems. And there's some urgency to this because the big decisions about contracts to build the 5G networks will be made in the next six months or so. The U. S. has been in Poland, where they have rather unsubtly suggested that if the Poles really want a new, small military - American military base in Poland - it's been referred to sort of informally as Fort Trump by the Polish leadership - they better build a network that does not use Huawei. There's been pressure on the Canadians, the British. And of course, in Latin America and in Africa, the Chinese have been mounting their own counteroffensive where they're coming in offering very low-cost building of the networks, frequently with Chinese government loans to go do it. What's that remind you of? It's a lot like what the United States did in the 1950s and '60s when we tried to use our foreign aid to go build up close allies by building up their technology and their industry. But now the Chinese are doing it, and they're doing it with the networking technology. GROSS: So our unilateral approach to world affairs, our alienation of our allies is really working against us when it comes to this new technological era of the 5G network. SANGER: You know, Terry, the most interesting observation in that worldwide threat assessment was the assertion that the unilateralism of the United States - they didn't use the phrase America first, but they could've - has made American allies and partners - meaning people who were not necessarily full allies - reassess their relationship with the U. S. and look for power relationships and protection elsewhere. And of course, that means they're looking mostly to the Chinese because they're the only other ones who have a market big enough and have an economy big enough to actually be of significant help to them. You've seen this happen in the Philippines - a country that used to be, at one point, an American colony - where the leadership of the Philippines, a great American ally, is getting closer and closer to the Chinese leadership. You're seeing it happen to some degree in South Korea where - by the way, in Seoul, Huawei was a big player in the competition to rebuild the cell network for Seoul, which is used by American forces who are based around Seoul. You're seeing this happen throughout Africa, where the United States has put a whole lot less money into rebuilding infrastructure, helping countries along than the Chinese have. GROSS: Some of the things you've described about China's capabilities of spying on us and hacking data and interfering with cyberwar, even - I mean, that's - it's some pretty terrifying stuff. At the same time, the U. S. has done similar things to China and other countries. Would you describe an operation that was named Shotgiant? SANGER: Terry, Shotgiant was a National Security Agency operation that happened around 2010. We know about it because some of the details were leaked out in the Snowden documents. It was an effort by the NSA to do to Huawei exactly what we have accused Huawei of doing to us, which is breaking into networks, figuring out how they operate and setting ourselves up to either steal information from those networks or cripple them in the future. What did the NSA do? It got into Huawei's corporate systems in Shenzhen, the Chinese industrial city. It looked for any evidence that the Chinese PLA was actually secretly controlling the company - doesn't appear they found any. It looked to understand how Huawei's equipment operated, how the software worked so that if Huawei sold a network switching system to an American adversary - say, Venezuela or Cuba or someplace that clearly wouldn't buy American equipment - then the NSA would have an easier time breaking into that equipment. The - this is not unusual. This is what the United States government created the NSA to go do. This is the kind of offensive cyber activity that the NSA conducts not only against China but against Russia, Iran, North Korea and so forth. But what it gets at - and the Chinese use it for these purposes - is that the United States is not above any of these kind of network manipulation issues that we've been worrying about in regard to Huawei. We do it ourselves. And Huawei's argument is, why would you be any safer in the world if you're a foreign country with an American or European-built network than with a Chinese-built network? - because they make the argument that the NSA is going to get into an American-built network or a network in Europe. And, certainly, the Snowden docs are full of examples of cases where we have done that. SANGER: Well, let's take a short break here. And then we'll talk some more. If you're just joining us, my guest is David Sanger. He's a national security correspondent for The New York Times. And his latest book is called \"The Perfect Weapon: War, Sabotage And Fear In The Cyber Age. \" We'll be right back. This is FRESH AIR. (SOUNDBITE OF AMANDA GARDIER'S \"FJORD\")GROSS: This is FRESH AIR. Let's get back to my interview with David Sanger, a national security correspondent for The New York Times. We've been talking about how the deep involvement of the Chinese telecom equipment giant Huawei in the development of the new 5G network in many countries could threaten our cybersecurity. If you're negotiating an arms control deal, there are ways that you can verify or come close to verifying whether a nuclear program is continuing, how many weapons the country has, where they're being stored. You can have inspectors go to observe facilities that you know exist. When it comes to, like, cyber issues, it's so much harder to verify what's going on. So what kind of treaty is even possible when it comes to something that is invisible like cyber-control, cyberattack, cyber-espionage, cyber-sabotage? SANGER: Terry, you've raised the fundamental question that has been haunting us in the cyber age. And it's one of the reasons I went to write \"The Perfect Weapon\" because this is so much more of a vexing problem than we had in the days of the 1950s, when we had nuclear weapons, the Russians did and the Chinese were about to. They first got theirs in the 1960s. You could do treaties in the nuclear age because there was a very limited number of players. First, it was just us and the Soviets, then the Chinese, of course, some other NATO players, later on, Israel, India, Pakistan. And you can count the weapons. And more importantly, you can count missiles. You can send inspectors just as you described. None of this is true in the cyber age. In the cyber age, the aggressor could be a state - might be Russia or China or Iran or North Korea. But it could be a criminal group. It could be a terrorist group. It could be teenagers. And none of those groups sign treaties, particularly teenagers, right? So the first problem is there's just too many players. The second is the technology is so inexpensive. This is what makes it the perfect weapon because it's so cheap that you don't need to be a China or a Russia to play in the cyber arena. By my count, there are probably around 35 countries today that have sophisticated cybercapability and could mount a sophisticated cyberattack, not just a denial of service attack that turned out the lights but something more sophisticated. So treaties will not work here. So what are the other options? Well, there are a lot of other ideas. But most of them circulate around codes of conduct - a sort of digital Geneva Convention. And it's an interesting concept because the real Geneva Conventions were not created by governments. The Geneva Convention meetings were organized by the Red Cross. And the idea was to protect civilians. So if you and I, Terry, were trying to come up with a list of things to protect, I think we'd probably sit down and say, OK. Electric grid should be off-limits because if you turn those off, you hurt the most vulnerable people. Communication systems and especially emergency communications - and that's where Huawei intersects with this. You'd want those to be off-limits from cyberattack. You'd probably want election systems to be off-limits. So I could imagine a digital Geneva Convention in which you gathered countries together and they agree to this. And it's not enforceable or inspectable, but you're beginning to set some global standards. One of the difficulties with this idea - even though I think it's sort of the best of the bad ideas that are out there - is that I'm not sure the United States would sign on to that. GROSS: Why not? SANGER: Imagine the intelligence leaders gathering in the situation room, saying, do we really want to limit the next president of the United States or this president from interfering in an election if it would be a way to get Maduro out of office? Do you really want to stop us from turning off electric power grids if we might be able to bring a country to its knees without firing a shot? After all, we had a plan - Nitro Zeus - to turn off all the power in Iran if we got into a conflict with them. So I don't even think the U. S. would sign on to this. GROSS: You know, the power equation, when it comes to cyber, has really changed because there was a period when the U. S. was the kind of ruler of cyberweapons and cyber potential. And that's just, like, no longer true. So if we do anything like that, there will be a cyber-counterattack and vice versa. So the stakes are really higher than they've ever been, I think. SANGER: And our control of the technology, as you point out, is less than it's ever been. Look. The Internet was first invented in the United States - you know, the ARPANET - what became the Internet later on - only 35 years ago. And then, of course, it's Silicon Valley companies that have dominated the technology that's come out of that. That era is ending, just as the era in which Britain and Portugal and a few other major naval powers could rule the world because they had the best. That era is ending, just as the era in which Britain and Portugal and a few other major naval powers could rule the world because they had the best ships. And we're not going to get this back. It's not as if we're going to go back to an era when we were the ones who dominated all of this technology. And now, for the first time in our modern history, we are facing a peer adversary in China that has an economy that will sooner or later overtake the United States in size and that is investing heavily in the major technologies on which 5G will allow big progress - artificial intelligence, autonomous vehicles, quantum computing. And, you know, you go into that worldwide threat assessment, I thought one of the most interesting points on it was that our absence of big strategy in many of these technologies is allowing adversaries to close the gap very quickly. Now, if you wanted to declare a national emergency about something, that threat assessment would suggest that's the issue on which the president might want to go declare a national emergency and come up with a strategy, rather than just focusing on the wall. GROSS: Is there anyone in the Trump administration who you look to as being, like, the foremost expert on cyber issues? SANGER: You know, there were, Terry. I thought that the president got off to a pretty good start on this. He hired a homeland security adviser, Tom Bossert, who had had some fairly good experience in cyber issues in the Bush administration and had spent time on it when he was out of office. There was also a White House coordinator for cyber issues, a man named Rob Joyce, who had spent his entire career at the National Security Agency. And he ran something called the Tailored Access Operations unit. That's sort of the special forces of the NSA that breaks into foreign computing systems. And the job of the White House cybersecurity coordinator was to try to bring together all of these complex defensive and offensive issues and the policy issues together. They were focused on it. So what happened? John Bolton came in in the spring as the new national security adviser after the firing of H. R. McMaster. Mr. Bolton, in his first week, got rid of the homeland security adviser, Tom Bossert, and replaced him with a Coast Guard admiral who is, by his own admission, not very familiar with cyber issues. Then Mr. Bolton eliminated the position of cybersecurity coordinator. I guess he must have concluded we were over-coordinated in this government, I suspect because he didn't like the fact that the cybersecurity coordinator had a sort of direct line to the president. In eliminating the position, he has downgraded the number of people within the White House who deal with this. And it doesn't seem to me that the policy is being debated at the level at which it needs to be or coordinated between the Pentagon, the NSA, the Department of Homeland Security, the Commerce Department and so many others who need to work on this. They have gotten policies out on Huawei. They may get this executive order out. But I'm afraid there's no big strategic thinking going on at the White House level. GROSS: Well, David Sanger, thank you so much for talking with us. SANGER: Great to be with you again. GROSS: David Sanger is a national security correspondent for The New York Times and author of \"The Perfect Weapon: War, Sabotage, And Fear In The Cyber Age. \" After we take a short break, our jazz critic, Kevin Whitehead, will review a reissue featuring pianist Oscar Peterson and a studio orchestra playing 1960s pop covers. This is FRESH AIR. (SOUNDBITE OF AARON PARKS' \"SMALL PLANET\") TERRY GROSS, HOST:  This is FRESH AIR. I'm Terry Gross. There's good news about the future of the Internet. A new 5G network is being created now, which will not only offer faster downloading on cell phones. It will provide the kind of connectivity we need in the era of the Internet of Things - driverless cars, Internet-connected medical devices, smart TVs and virtual assistants. But there are dangers that could be lurking in the equipment needed to build the new network. The Chinese telecommunications equipment giant Huawei is dominating the creation of 5G networks around the world. For years, classified intelligence reports from the U. S. have warned that China would one day use Huawei to penetrate American networks for cyber-espionage or cyberattacks. In the U. S. , the National Security Agency has banned AT&T and Verizon from using Huawei products in America's 5G network. And last month, the U. S. had a top executive from Huawei arrested in Canada so she could be extradited to the U. S. The growing cyberthreat posed by China was stressed in the Worldwide Threat Assessment - a report from the U. S. intelligence community - that was released this week. And all this is part of the backdrop for this week's trade negotiations between the U. S. and China. My guest David Sanger is the author of a book about cyberwar and cyber-sabotage called \"The Perfect Weapon. \" He's a national security correspondent for The New York Times. David Sanger, welcome back to FRESH AIR. Let's start with the 5G network. What is it? And how will it affect our phones, our devices and all our interconnectivity? DAVID SANGER: Well, at its simplest, the 5G network is an increase in speed and range for what you see on your cell phone. So 5G means just fifth generation. But it's actually much more than that. The hope is that when you're using your phone or some other device over Wi-Fi, you'll get no lag time and that you'll get near instantaneous download of data, webpages and so forth. But as 5G was being rolled out, there was a recognition that the Internet had fundamentally changed, that this was a moment to roll out something that could accommodate a world in which the Internet of Things was connecting up to all of these other wireless devices. And so that's autonomous cars, which, of course, need to constantly get data back and forth from the cloud, constant connectivity so that they know where they are in addition to their sensors helping you drive. It's for every other Internet-connected device that you have. And, you know, when you think about it, Terry, it was just about 10 years ago that in your own house, you probably only had one or two Internet-connected devices - a laptop computer and a desktop computer, maybe. But today you walk into your house and, you know, you've got your Fitbit. And you're waking up Alexa and getting it to play you music. And you've got a smart TV. And you probably have an Internet-connected car parked outside. Even if it isn't a fancy car, most basic cars have some Internet connectivity to them. You might have an Internet-connected refrigerator. You have all of these different devices. And right now worldwide - at the end of last year, we think that there were about 14 billion Internet of Things devices around the world. And by the end of next year, 2020, the estimate is there will be 20 billion. So that gives you a sense of how rapidly we're changing the environment. And the next network has to be able to handle all of that and, of course, handle the GPS needs for navigation, handle greater government and military needs. So this next 5G network is more than just something that'll make your phone faster. It's actually going to be the central nervous system, the backbone of the next generation of the Internet. GROSS: And that's exactly the concern about the Chinese telecom giant Huawei because they're building some of these 5G networks around the world. So what is the concern about this Chinese telecom giant and the 5G network? SANGER: Well, the first thing about Huawei is that while most Americans haven't come in direct contact with it because Huawei phones are not sold that widely in the United States, they are sold nearly universally when you're in Asia and very widely in Europe, in Africa, in Latin America. In fact, at the end of last year, Huawei actually just edged out Apple as the second-largest provider of cell phones in the world. The only one ahead of it is Samsung. But the other part of their business and the part that we really worry about the most is the construction of the giant switches that make up these 5G networks. Now, in the old days of switching, you would think of switches as big, physical devices. What's happened and what is particularly notable about 5G is that the network itself, while it has some hardware to it and, obviously, there are cell phone towers and sort of a radio part of it, the switches are almost entirely software. And they constantly reconfigure themselves. And they are enormously complex. So the old days of doing what the defense department and the National Security Agency and others used to do - which is take a piece of foreign equipment, put it in a laboratory, poke around it, try to figure out if there are flaws or back doors or something that could help an adversary - that's virtually impossible to do when the product is an ever-evolving piece of software. It gets updated as often as your iPhone gets updated when you have it sitting on your bedside table and Apple sends a new, updated operating system to it overnight. So that's what will happen with this 5G network. The concern is that for the first time in our history, we would be reliant on a foreign manufacturer - in this case, a foreign manufacturer of a potential adversary that's also the world's second largest economy - building the highway, backbone, central nervous system of a system that we rely on for everything from our financial transactions to our - many of our military operations to, of course, our communications. And the question is, can you trust a foreign actor to be responsible for that? And what happens when Huawei gets that command under Chinese law from the Chinese government to either grab a piece of information or close down part of the network? GROSS: So describe some more of the kind of trouble that China could create with Huawei building 5G networks in countries around the world where China is becoming increasingly powerful. SANGER: If China is in command of the network itself and has sort of end end control from phones for which it makes its own chips to the software on the switch to all of the other tentacles of the central nervous system, that it, basically, can do whatever it wants. And the chances that you would see it are relatively diminished. Big network operators like AT&T and Verizon, if they bought Huawei equipment - and it's pretty clear the government is not going to allow them to do that - would have some visibility into the system. But it's also possible that Huawei might be able to reach back from China directly into the equipment and software it's put in to go manipulate data. What could you do with that? Well, in the Worldwide Threat Assessment that came out earlier this week, the nation's intelligence chiefs mentioned, in particular, that China already has the capability to shut down, at least briefly, the natural gas network. They also said the Russians could do the same briefly with the electric grid. If you had a country that was in full control of your networks, they could shut it down. They could siphon the traffic off to a place you didn't want it to go. They could siphon it back to China. And they would probably have a easier time intercepting it. Now, of course, a lot of that traffic is going to run encrypted. It's not as if the Chinese would be able to look at everything or would want to. But the more network equipment they put in, the more control they would have. And, of course, the Chinese government reserves the right to tell them what to go do with it. GROSS: So is Huawei involved in any aspect of the construction of the 5G network in the U. S. ? SANGER: Not anything notable in the United States now - there were a number of classified meetings that took place between the intelligence agencies, the executives of the big telecom firms - AT&T, Verizon, T-Mobile, so forth. And there was a lot of discussion about letting - whether or not to let Huawei bid on the construction of some parts of this 5G network as they are doing around the world. And, in fact, some of the telecom companies argued to Congress and to the intelligence agencies that it would make sense to let Huawei bid. And the reason is that if they wanted to bid, they would have to provide their software, their equipment to a test facility in the United States. And the National Security Agency, which is the nation's largest electronic spy agency, and the telecom providers would all be able to crawl around in that software and see if there were any backdoors, see how it was designed. But in the end, the intelligence community didn't even want to take that risk. GROSS: The founder of Huawei is a former engineer with the People's Liberation Army in China, and some people think he's still connected to the People's Liberation Army. And some people also argue that private companies like Huawei are still under the control of Chinese - of China's authoritarian government. So what's your assessment of how much control the Chinese government has over Huawei and how much control Chinese intelligence has over Huawei and has access to whatever data Huawei gets or can tell Huawei, collect data from this country; collect this data from that country? SANGER: Well, the founder who you're describing, Ren Zhengfei, is, as you said, a former PLA officer. He then, when he left the PLA, built Huawei initially in the - wiring up the rural parts of China. And he's become obviously one of the most powerful businessmen in China. He's a member of the Communist Party. He's enormously influential there. He has insisted that the Chinese military, the PLA, which has done much of the hacking against the United States, has no role in his company and no continuing control. I've never found any evidence that the United States could prove that the PLA has operative control over Huawei. And of course, Mr. Zheng (ph) has denied it. Now, that issue consumed the U. S. government for years and years until about two years ago when the Chinese government issued a new set of laws under President Xi, Xi Jinping, that basically said any Chinese company - but particularly the telecom companies - would have to participate in Chinese intelligence operations if they are so instructed, that they would have to turn over data that they had. It's not clear, of course, in China what the legal process would be. And so now people say to me, you know, it doesn't make any difference, David, whether or not the PLA has control over Huawei because the law means that the Chinese government has turned the company into its agent. GROSS: Well, let's take a short break here, and then we'll talk some more. If you're just joining us, my guest is David Sanger. He's a national security correspondent for The New York Times, and his latest book is called \"The Perfect Weapon: War, Sabotage, And Fear In The Cyber Age. \" We'll be right back. This is FRESH AIR. (SOUNDBITE OF ALEXANDRE DESPLAT'S \"SPY MEETING\") GROSS: This is FRESH AIR. Let's get back to my interview with David Sanger, a national security correspondent for The New York Times. We're talking about the 5G network that is being created now and will soon become the central nervous system of the Internet. The Chinese telecom equipment giant Huawei is dominating the creation of 5G networks around the world. For years, classified U. S. intelligence reports have warned that China would one day use Huawei to penetrate American networks for cyber espionage or cyberattacks. So we were talking about the founder of Huawei and his connections to the Chinese military and Huawei's connections to the Chinese government. The founder of Huawei - his daughter was arrested in Canada at the request of the U. S. and indicted last week. So what are those charges? And what are their significance? SANGER: Well, the first thing we know about the daughter is, in her own right, she's a very powerful figure within Huawei. She's the chief financial officer. But she's also been the architect of a lot of Huawei's spread around the world. When she was arrested in Canada in December at the request of the United States, it was not for any charges that Huawei had participated in espionage against the U. S. It was not for - on any charges that it had participated in cyberattacks on the United States. Instead, it was based on a charge that she had been behind a giant fraud in which Huawei used a cut-out company to violate the sanctions that the United States had against Iran and that this company was in fact a Huawei - secretly a Huawei subsidiary and was doing business with Iran in violation of those sanctions. So the U. S. is using the Iran sanctions violation to go after the company. Now, the other interesting part about this is that President Trump at one point in December after she was arrested but before she was indicted publicly mused about the fact that he might trade her away in the course of the trade negotiations. Thus the president was politicizing what until then had been a legal action going through normal legal channels. And one of the interesting questions is whether the Canadians are going to extradite her because there have been several Canadians now arrested in China so that they have counter-hostages. And it's possible that the Canadians may determine that this case is more political than criminal. GROSS: President Trump has said that he knows more about technology than anyone. What's your understanding of how much he understands the issue of Huawei and its potential ability to hack America, to spy on America? SANGER: Well, I have a couple of concerns. The first is the president's mind turns, as he has said himself many times, to things you can build. And so issues around software networks and so forth just don't come naturally to him. He hasn't shown much of an inclination to learn about it during the time he's president according to people who used to be in the administration, including in the national security field. The president has a somewhat hazy understanding about the risks of cyber-escalation. So when you think about our cyber-risk, one concern is one we've discussed already today, which is surveillance. Basically, you can use these networks to steal data. But the coming concerns, Terry, have to do more with data manipulation - what happens if you change the data? That's the problem of deep fakes. Something that would look like a politician was speaking, but it wasn't really the words coming out of his mouth. It's been faked. It could be substituting numbers and financial transactions. It could be substituting the targeting information in nuclear or non-nuclear weapons. It could be changing the blood types of every soldier and sailor in the United States if you got into the databases of the military. So there's data manipulation that's a concern. And if any country that had access to the networks, you would worry about that. And then the third is cyberattack, and that is that if we went to war or were conducting covert operations, every country in the world now has cyber in its battle plans, and usually in the first 24 hours of its battle plans. In \"The Perfect Weapon,\" I describe a plan the United States had if we went to war with Iran, called Nitro Zeus, to basically unplug Iran's communications and electricity grids. Well, imagine that that's in the Chinese plans for the United States. If they're in control of the communications grid of the U. S. or its allies, you can imagine how much easier that is to do. Now, there is a concern here that we could get into a world of Red Scare, and the president himself might be fueling that some. And I have concerns that we're blaming too much on the Chinese. But the fact of the matter is, these are all major, complex vulnerabilities that, as Henry Kissinger said to me as I was working on the book, are so much more complex than the issues that came up with China in the Cold War. GROSS: And you're concerned that our president doesn't really comprehend those issues and therefore can't adequately address them. SANGER: That's right. And, you know, there are escalation issues here, as well. I mean, there's still a big debate in the United States government about how you respond to a cyberattack. When the Chinese got into Google and other companies in 2009, there were Google engineers who wanted to retaliate directly against the servers where the attack was coming from. Fortunately, they were stopped. But had that gone ahead, or had it gone ahead with another company, the question would be to the Chinese, is this attack, this counterattack, coming from a private company, coming from some hackers, has it been commanded by the United States? Is a company operating on behalf of the U. S. ? And then they would escalate. So you get all the same kind of escalation issues that we worried about in the nuclear age, but you get them in this technology world in which shutting down or diverting data becomes your new weapon. And we don't really understand that escalatory response. Now, the president, in August of last year, issued a new classified order to the National Security Agency that basically gave the director of the National Security Agency more leeway to go respond to offensive cyberactions and to initiate some without presidential approval. But still we don't understand who's in control of the escalation. GROSS: My guest is David Sanger, a national security correspondent for The New York Times who is also the author of the book, \"The Perfect Weapon,\" about cyberwar and cyber-espionage. We'll talk more after we take a short break. And our jazz critic Kevin Whitehead will review a reissue featuring pianist Oscar Peterson and a studio orchestra playing 1960s pop covers. This is FRESH AIR. (SOUNDBITE OF MUSIC) GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to my interview with David Sanger, a national security correspondent for The New York Times and author of \"The Perfect Weapon: War, Sabotage, And Fear In The Cyber Age. \" We're talking about the new 5G network that will soon become the central nervous system of the Internet, providing faster speeds and greater interconnectivity for the era we're entering where nearly everything, from cars to medical devices, connects to the Internet. The Chinese telecom equipment giant Huawei is dominating the creation of 5G networks around the world. For years, classified U. S. intelligence reports have warned that China would one day use Huawei to penetrate American networks for cyber-espionage or cyberattacks. The Worldwide Threat Assessment that was released by intelligence agencies this week said that there's a growing cyberthreat from Russia and China, and that Russia and China are now more aligned than at any point since the mid-1950s. So what does that say to you about the cyberthreat posed by this alignment of Russia and China? SANGER: Well, it was a fascinating assertion that they made, And I think it was accurate. We have not seen Russia and China cooperate this way since the mid-'50s. And, of course, China only came into being as the People's Republic in 1949. So it was very early in its history, and it was an extremely poor country. It's worth remembering today that Russia and China have very different objectives. Russia's main objective is one of disruption. It does not have much economic power. Its economy is basically the size of Italy's. It does not have the ability to go build these networks. It does not have much economic power. Its economy is basically the size of Italy's. It does not have the ability to go build these networks around the world the way China is doing. It does not have the technology to do it, but it can be a huge disruptor. And of course, we saw them act to disrupt networks and voting systems in Ukraine. We saw their interference in the 2016 election here in the United States, and we've seen them take cyber action elsewhere in the world. We've seen their submarines go out and track where the fiber optic cables are laid around the world. They're only, you know, less than 200 major fiber optic undersea cables. And the Russians have the ability to cut those cables deep undersea. That would be a huge disruption. That could black out communications in the United States. So that's the Russian side. The Chinese have a much different set of objectives. If the world gets disrupted, no one's going to suffer more than they will because their economy is so interdependent with ours and with other major economies around the world. So they're less likely to disrupt, but they're much more likely to want control and subtle ability to divert traffic in those networks. And that's the concern about Huawei. GROSS: But with Russia and China being in closer cooperation than at any other time since the 1950s, what's the combination of those two countries looking like? Like, if Russia's about disruption and China is about control, when you put the two together, what's the new formula? SANGER: Well, the new formula is a diminished role for the United States. That's part of the concern about the degree to which we have alienated our allies. And this issue about Huawei intersects with the alienation of the allies very closely because what's going on now is the United States is going around the world to allies and say, hey, we're living in a new world. The Russians and the Chinese are cooperating more than we've ever seen. We're trying to keep everybody from spinning into a new form of a cold war - Cold War 2. 0. And while the Russians and the Chinese have very different strategic objectives, there may be moments - there will be moments when they will have a common objective in diminishing the power of the United States. That's the one area where they both have great common interests. And so it's important that the United States be able to go work with its allies to figure out how you both contain this threat and respond to it, but also how you retain control of your own networks. So what the U. S. is doing right now is it's going around to its allies, particularly the NATO allies, and saying, don't build Huawei into your systems. And there's some urgency to this because the big decisions about contracts to build the 5G networks will be made in the next six months or so. The U. S. has been in Poland, where they have rather unsubtly suggested that if the Poles really want a new, small military - American military base in Poland - it's been referred to sort of informally as Fort Trump by the Polish leadership - they better build a network that does not use Huawei. There's been pressure on the Canadians, the British. And of course, in Latin America and in Africa, the Chinese have been mounting their own counteroffensive where they're coming in offering very low-cost building of the networks, frequently with Chinese government loans to go do it. What's that remind you of? It's a lot like what the United States did in the 1950s and '60s when we tried to use our foreign aid to go build up close allies by building up their technology and their industry. But now the Chinese are doing it, and they're doing it with the networking technology. GROSS: So our unilateral approach to world affairs, our alienation of our allies is really working against us when it comes to this new technological era of the 5G network. SANGER: You know, Terry, the most interesting observation in that worldwide threat assessment was the assertion that the unilateralism of the United States - they didn't use the phrase America first, but they could've - has made American allies and partners - meaning people who were not necessarily full allies - reassess their relationship with the U. S. and look for power relationships and protection elsewhere. And of course, that means they're looking mostly to the Chinese because they're the only other ones who have a market big enough and have an economy big enough to actually be of significant help to them. You've seen this happen in the Philippines - a country that used to be, at one point, an American colony - where the leadership of the Philippines, a great American ally, is getting closer and closer to the Chinese leadership. You're seeing it happen to some degree in South Korea where - by the way, in Seoul, Huawei was a big player in the competition to rebuild the cell network for Seoul, which is used by American forces who are based around Seoul. You're seeing this happen throughout Africa, where the United States has put a whole lot less money into rebuilding infrastructure, helping countries along than the Chinese have. GROSS: Some of the things you've described about China's capabilities of spying on us and hacking data and interfering with cyberwar, even - I mean, that's - it's some pretty terrifying stuff. At the same time, the U. S. has done similar things to China and other countries. Would you describe an operation that was named Shotgiant? SANGER: Terry, Shotgiant was a National Security Agency operation that happened around 2010. We know about it because some of the details were leaked out in the Snowden documents. It was an effort by the NSA to do to Huawei exactly what we have accused Huawei of doing to us, which is breaking into networks, figuring out how they operate and setting ourselves up to either steal information from those networks or cripple them in the future. What did the NSA do? It got into Huawei's corporate systems in Shenzhen, the Chinese industrial city. It looked for any evidence that the Chinese PLA was actually secretly controlling the company - doesn't appear they found any. It looked to understand how Huawei's equipment operated, how the software worked so that if Huawei sold a network switching system to an American adversary - say, Venezuela or Cuba or someplace that clearly wouldn't buy American equipment - then the NSA would have an easier time breaking into that equipment. The - this is not unusual. This is what the United States government created the NSA to go do. This is the kind of offensive cyber activity that the NSA conducts not only against China but against Russia, Iran, North Korea and so forth. But what it gets at - and the Chinese use it for these purposes - is that the United States is not above any of these kind of network manipulation issues that we've been worrying about in regard to Huawei. We do it ourselves. And Huawei's argument is, why would you be any safer in the world if you're a foreign country with an American or European-built network than with a Chinese-built network? - because they make the argument that the NSA is going to get into an American-built network or a network in Europe. And, certainly, the Snowden docs are full of examples of cases where we have done that. SANGER: Well, let's take a short break here. And then we'll talk some more. If you're just joining us, my guest is David Sanger. He's a national security correspondent for The New York Times. And his latest book is called \"The Perfect Weapon: War, Sabotage And Fear In The Cyber Age. \" We'll be right back. This is FRESH AIR. (SOUNDBITE OF AMANDA GARDIER'S \"FJORD\") GROSS: This is FRESH AIR. Let's get back to my interview with David Sanger, a national security correspondent for The New York Times. We've been talking about how the deep involvement of the Chinese telecom equipment giant Huawei in the development of the new 5G network in many countries could threaten our cybersecurity. If you're negotiating an arms control deal, there are ways that you can verify or come close to verifying whether a nuclear program is continuing, how many weapons the country has, where they're being stored. You can have inspectors go to observe facilities that you know exist. When it comes to, like, cyber issues, it's so much harder to verify what's going on. So what kind of treaty is even possible when it comes to something that is invisible like cyber-control, cyberattack, cyber-espionage, cyber-sabotage? SANGER: Terry, you've raised the fundamental question that has been haunting us in the cyber age. And it's one of the reasons I went to write \"The Perfect Weapon\" because this is so much more of a vexing problem than we had in the days of the 1950s, when we had nuclear weapons, the Russians did and the Chinese were about to. They first got theirs in the 1960s. You could do treaties in the nuclear age because there was a very limited number of players. First, it was just us and the Soviets, then the Chinese, of course, some other NATO players, later on, Israel, India, Pakistan. And you can count the weapons. And more importantly, you can count missiles. You can send inspectors just as you described. None of this is true in the cyber age. In the cyber age, the aggressor could be a state - might be Russia or China or Iran or North Korea. But it could be a criminal group. It could be a terrorist group. It could be teenagers. And none of those groups sign treaties, particularly teenagers, right? So the first problem is there's just too many players. The second is the technology is so inexpensive. This is what makes it the perfect weapon because it's so cheap that you don't need to be a China or a Russia to play in the cyber arena. By my count, there are probably around 35 countries today that have sophisticated cybercapability and could mount a sophisticated cyberattack, not just a denial of service attack that turned out the lights but something more sophisticated. So treaties will not work here. So what are the other options? Well, there are a lot of other ideas. But most of them circulate around codes of conduct - a sort of digital Geneva Convention. And it's an interesting concept because the real Geneva Conventions were not created by governments. The Geneva Convention meetings were organized by the Red Cross. And the idea was to protect civilians. So if you and I, Terry, were trying to come up with a list of things to protect, I think we'd probably sit down and say, OK. Electric grid should be off-limits because if you turn those off, you hurt the most vulnerable people. Communication systems and especially emergency communications - and that's where Huawei intersects with this. You'd want those to be off-limits from cyberattack. You'd probably want election systems to be off-limits. So I could imagine a digital Geneva Convention in which you gathered countries together and they agree to this. And it's not enforceable or inspectable, but you're beginning to set some global standards. One of the difficulties with this idea - even though I think it's sort of the best of the bad ideas that are out there - is that I'm not sure the United States would sign on to that. GROSS: Why not? SANGER: Imagine the intelligence leaders gathering in the situation room, saying, do we really want to limit the next president of the United States or this president from interfering in an election if it would be a way to get Maduro out of office? Do you really want to stop us from turning off electric power grids if we might be able to bring a country to its knees without firing a shot? After all, we had a plan - Nitro Zeus - to turn off all the power in Iran if we got into a conflict with them. So I don't even think the U. S. would sign on to this. GROSS: You know, the power equation, when it comes to cyber, has really changed because there was a period when the U. S. was the kind of ruler of cyberweapons and cyber potential. And that's just, like, no longer true. So if we do anything like that, there will be a cyber-counterattack and vice versa. So the stakes are really higher than they've ever been, I think. SANGER: And our control of the technology, as you point out, is less than it's ever been. Look. The Internet was first invented in the United States - you know, the ARPANET - what became the Internet later on - only 35 years ago. And then, of course, it's Silicon Valley companies that have dominated the technology that's come out of that. That era is ending, just as the era in which Britain and Portugal and a few other major naval powers could rule the world because they had the best. That era is ending, just as the era in which Britain and Portugal and a few other major naval powers could rule the world because they had the best ships. And we're not going to get this back. It's not as if we're going to go back to an era when we were the ones who dominated all of this technology. And now, for the first time in our modern history, we are facing a peer adversary in China that has an economy that will sooner or later overtake the United States in size and that is investing heavily in the major technologies on which 5G will allow big progress - artificial intelligence, autonomous vehicles, quantum computing. And, you know, you go into that worldwide threat assessment, I thought one of the most interesting points on it was that our absence of big strategy in many of these technologies is allowing adversaries to close the gap very quickly. Now, if you wanted to declare a national emergency about something, that threat assessment would suggest that's the issue on which the president might want to go declare a national emergency and come up with a strategy, rather than just focusing on the wall. GROSS: Is there anyone in the Trump administration who you look to as being, like, the foremost expert on cyber issues? SANGER: You know, there were, Terry. I thought that the president got off to a pretty good start on this. He hired a homeland security adviser, Tom Bossert, who had had some fairly good experience in cyber issues in the Bush administration and had spent time on it when he was out of office. There was also a White House coordinator for cyber issues, a man named Rob Joyce, who had spent his entire career at the National Security Agency. And he ran something called the Tailored Access Operations unit. That's sort of the special forces of the NSA that breaks into foreign computing systems. And the job of the White House cybersecurity coordinator was to try to bring together all of these complex defensive and offensive issues and the policy issues together. They were focused on it. So what happened? John Bolton came in in the spring as the new national security adviser after the firing of H. R. McMaster. Mr. Bolton, in his first week, got rid of the homeland security adviser, Tom Bossert, and replaced him with a Coast Guard admiral who is, by his own admission, not very familiar with cyber issues. Then Mr. Bolton eliminated the position of cybersecurity coordinator. I guess he must have concluded we were over-coordinated in this government, I suspect because he didn't like the fact that the cybersecurity coordinator had a sort of direct line to the president. In eliminating the position, he has downgraded the number of people within the White House who deal with this. And it doesn't seem to me that the policy is being debated at the level at which it needs to be or coordinated between the Pentagon, the NSA, the Department of Homeland Security, the Commerce Department and so many others who need to work on this. They have gotten policies out on Huawei. They may get this executive order out. But I'm afraid there's no big strategic thinking going on at the White House level. GROSS: Well, David Sanger, thank you so much for talking with us. SANGER: Great to be with you again. GROSS: David Sanger is a national security correspondent for The New York Times and author of \"The Perfect Weapon: War, Sabotage, And Fear In The Cyber Age. \" After we take a short break, our jazz critic, Kevin Whitehead, will review a reissue featuring pianist Oscar Peterson and a studio orchestra playing 1960s pop covers. This is FRESH AIR. (SOUNDBITE OF AARON PARKS' \"SMALL PLANET\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-01-690632548": {"title": "U.S., And Now Russia, Announce Plans To Withdraw From Nuclear Arms Control Treaty : NPR", "url": "https://www.npr.org/2019/02/01/690632548/u-s-announces-it-will-withdraw-from-nuclear-arms-control-treaty-with-russia", "author": "No author found", "published_date": "2019-02-01", "content": "", "section": "World", "disclaimer": ""}, "2019-02-01-690071045": {"title": "One Man's Quest To Prove Vermont Has Terrible Cell Service : NPR", "url": "https://www.npr.org/2019/02/01/690071045/one-mans-quest-to-prove-vermont-has-terrible-cell-service", "author": "No author found", "published_date": "2019-02-01", "content": "ARI SHAPIRO, HOST: Cellphone service in many rural areas of the country is not as good as advertised. That point was recently made in Vermont. A state employee drove 6,000 miles to test cell signals, and the results showed the claims of the cellphone carriers did not match the reality on the ground. As John Dillon from Vermont Public Radio reports, Vermont is now 1 of 37 states challenging the company's data. JOHN DILLON, BYLINE: The truth is I have a stake in this story. Like many Vermonters, I live on a back road. It's not far from civilization, just seven miles from the state capital of Montpelier and only a few hundred yards off a state highway. But this very snowy back road could be the other side of the moon as far as cell service goes. But I'll let Corey Chase pick it up from here. COREY CHASE: We're in the state of Vermont's Prius, and we're getting ready to take a short drive to demonstrate how the cellphone tracking project works. DILLON: Chase is a communications specialist with the state. This fall, he drove all over Vermont on roads good and bad and sometimes no road at all. CHASE: I can tell you that the Prius is not made for off road adventuring. DILLON: Equipped with six cellphones and an app customized by a coder in Bulgaria, Chase ran detailed download tests every 300 meters. He ground truthed what every Vermonter with a cellphone knows - there are many, many places where you simply can't get a signal even though the phone companies' coverage maps show they're solid service. CHASE: I think that their maps are not credible. Their maps show coverage essentially throughout the entirety of the state. DILLON: The cell industry trade group did not respond to repeated requests for comment, yet here's why the issue is important. There's $4. 5 billion in federal money available to help companies boost coverage in underserved areas, but the carriers serving Vermont did not apply for the funds perhaps because that would acknowledge their claims of coverage are wrong. They say that between them, they serve all of Vermont, except for its most isolated areas. Chase and his colleagues knew that wasn't true, so they set out to prove it with a road trip. CHASE: And had we not done this test, there would have been no incentive for anybody to bid because those areas that were eligible are moose habitat. DILLON: Thirty-six other states also contested the carrier's claims of good coverage, and the Federal Communications Commission has delayed the grant program while it investigates. JUNE TIERNEY: Good policy is based on good data. DILLON: June Tierney heads the state agency that represents consumers in telecom and utility issues. She says the Vermont challenge is not just aimed at landing money to improve service, it's also about economic development and public safety. TIERNEY: All you have to do is look as far as California and to see the firefighting incidents that they had this past year and the crucial role that cell coverage played in making sure that those fires got fought well. CHASE: And we're turning down a dirt road. We'll try not to lose a wheel in the pothole. DILLON: We're back in the Prius testing coverage, and our route takes us right by my road. Chase turns in and what happens next was not planned. Here are my neighbors. CHASE: Say hello to the neighbors who happen to be walking down the street. DILLON: My neighbor, Jim Bezak, is a retired state worker, and he's read about the drive test. JIM BEZAK: It's a great thing. Take up a collection. This is just the data we're missing. DILLON: Chase tests the coverage in my mailbox. Three of the six companies showed no service, the rest had very weak signals below what they claimed - but I could have told you that. For NPR News, I'm John Dillon in Montpelier, Vt. ARI SHAPIRO, HOST:  Cellphone service in many rural areas of the country is not as good as advertised. That point was recently made in Vermont. A state employee drove 6,000 miles to test cell signals, and the results showed the claims of the cellphone carriers did not match the reality on the ground. As John Dillon from Vermont Public Radio reports, Vermont is now 1 of 37 states challenging the company's data. JOHN DILLON, BYLINE: The truth is I have a stake in this story. Like many Vermonters, I live on a back road. It's not far from civilization, just seven miles from the state capital of Montpelier and only a few hundred yards off a state highway. But this very snowy back road could be the other side of the moon as far as cell service goes. But I'll let Corey Chase pick it up from here. COREY CHASE: We're in the state of Vermont's Prius, and we're getting ready to take a short drive to demonstrate how the cellphone tracking project works. DILLON: Chase is a communications specialist with the state. This fall, he drove all over Vermont on roads good and bad and sometimes no road at all. CHASE: I can tell you that the Prius is not made for off road adventuring. DILLON: Equipped with six cellphones and an app customized by a coder in Bulgaria, Chase ran detailed download tests every 300 meters. He ground truthed what every Vermonter with a cellphone knows - there are many, many places where you simply can't get a signal even though the phone companies' coverage maps show they're solid service. CHASE: I think that their maps are not credible. Their maps show coverage essentially throughout the entirety of the state. DILLON: The cell industry trade group did not respond to repeated requests for comment, yet here's why the issue is important. There's $4. 5 billion in federal money available to help companies boost coverage in underserved areas, but the carriers serving Vermont did not apply for the funds perhaps because that would acknowledge their claims of coverage are wrong. They say that between them, they serve all of Vermont, except for its most isolated areas. Chase and his colleagues knew that wasn't true, so they set out to prove it with a road trip. CHASE: And had we not done this test, there would have been no incentive for anybody to bid because those areas that were eligible are moose habitat. DILLON: Thirty-six other states also contested the carrier's claims of good coverage, and the Federal Communications Commission has delayed the grant program while it investigates. JUNE TIERNEY: Good policy is based on good data. DILLON: June Tierney heads the state agency that represents consumers in telecom and utility issues. She says the Vermont challenge is not just aimed at landing money to improve service, it's also about economic development and public safety. TIERNEY: All you have to do is look as far as California and to see the firefighting incidents that they had this past year and the crucial role that cell coverage played in making sure that those fires got fought well. CHASE: And we're turning down a dirt road. We'll try not to lose a wheel in the pothole. DILLON: We're back in the Prius testing coverage, and our route takes us right by my road. Chase turns in and what happens next was not planned. Here are my neighbors. CHASE: Say hello to the neighbors who happen to be walking down the street. DILLON: My neighbor, Jim Bezak, is a retired state worker, and he's read about the drive test. JIM BEZAK: It's a great thing. Take up a collection. This is just the data we're missing. DILLON: Chase tests the coverage in my mailbox. Three of the six companies showed no service, the rest had very weak signals below what they claimed - but I could have told you that. For NPR News, I'm John Dillon in Montpelier, Vt.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-01-689946997": {"title": "Ashley Judd: How Can We\u2014As A Society\u2014Heal From Sexual Violence? : NPR", "url": "https://www.npr.org/2019/02/01/689946997/ashley-judd-how-can-we-as-a-society-heal-from-sexual-violence", "author": "No author found", "published_date": "2019-02-01", "content": "GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about the new conversation we're having around Gender, Power and Fairness. And just a warning, some of the stories and language in this segment may be hard to hear. (SOUNDBITE OF MUSIC)ASHLEY JUDD: Hello, this is Ashley. RAZ: Ashley, good morning. This is Guy Raz. I'm the host of the program. How are you? JUDD: I know your voice well, Guy. RAZ: I know yours, too. This is Ashley Judd. JUDD: And I am a writer, a humanitarian and an actor. RAZ: Ashley was one of the first women to speak on the record about being sexually harassed by Harvey Weinstein. But she's actually been speaking out about gender violence for much longer. For years, she's been on the receiving end of intense online harassment, harassment that is routine for so many women. JUDD: I would venture to say that I began receiving gendered hate speech and misogynistic messages on social media from the very moment that I joined. RAZ: Like in one instance at a basketball game in 2015, when Ashley tweeted a complaint about the refs. JUDD: The response to that was a huge sexist pile-on, where it really started with, you know, the outrageousness of my thinking that as a female basketball fan I was entitled to have an opinion about officiating, to just a generalized, you should die; I want to rape you; I want to ejaculate on your face; you shouldn't be taking up oxygen; there was a picture of you, I wish it was a picture of your deathbed. You know, all of this stuff that I got. RAZ: Ashley Judd picks up her story from the TED stage. (SOUNDBITE OF TED TALK)JUDD: It is routine for me to be treated in the ways I've already described to you. It happens to me every single day on social media platforms, such as Twitter and Facebook. And I have responded to this with various strategies. I've tried to rise above it. I've tried to get in the trenches. But mostly, I would scroll through these social media platforms with one eye partially closed, trying not to see it, but you can't make a cucumber out of a pickle. What is seen, goes in. It's traumatic. And I was always secretly hoping, in some part of me, that what was being said to me and about me wasn't true. Because even I, an avowed, self-declared feminist who worships at the altar of Gloria, internalized the patriarchy. Patriarchy is not boys and men. It is a system in which we all participate, including me. On that particular day, for some reason, that particular tweet after the basketball game, when I was sitting at home alone in my nightgown, I got a phone call, and it was my beloved former husband. And he said on a voicemail, Loved One, what is happening to you is not OK. And there was something about him taking a stand for me that night that allowed me to take a stand for myself. And I started to write, sharing the fact that I'm a survivor of all forms of sexual abuse, including three rapes. So I wrote this feminist op-ed. It is entitled, \"Forget Your Team: It Is Your Online Gendered Violence Toward Girls And Women That Can Kiss My Righteous Ass. \"(APPLAUSE)JUDD: And I did that alone, and I published it alone, because my chief adviser said, please don't. The reign of retaliatory garbage that is inevitable, I fear for you. But I trust girls, and I trust women, and I trust our allies. It was published. It went viral. It proves that every single day online misogyny is a phenomenon endured by us all, all over the world, and when it is intersectional, it is worse. Sexual orientation, gender identity, race, ethnicity, religion - you name it, it is worse. Online misogyny is a global, gender rights tragedy, and it is imperative that it ends. (SOUNDBITE OF MUSIC)RAZ: I mean, the digital space has opened up a whole new world that was almost unanticipated, right? When this technology came out, I think most of us thought, well, this is going to bring the world together. It's going to democratize the ability to amplify and share messages and views. But I don't think anybody anticipated that it would become this repository for violence. JUDD: I think it's a concentrated space in which violence against girls and women happen. Just like #MeToo and Time's Up pulled the curtain back from the everyday sexual aggression with which hundreds of millions of us live, the Internet, I think, simply exposed some of the patterns of thinking that boys and men hold about girls and women, specifically the sexual objectification and commodification of our bodies. And you know, I think that all gender and sexual violence is on a continuum. One end is nuanced and subtle. It's unspoken, but it's thought. And then it's the microaggressions, and of course, it goes all the way to the other end of the spectrum with homicide. And the Internet is a place where all of that can flourish. And I do believe that the Internet itself is neutral and it's a tool and it can be used for good or it can be used for ill. When it's in the hands of misogynists, it's a powerfully destructive force. RAZ: You know, I think they're stories we tell ourselves, right? Like you look at black-and-white films of angry mobs protesting against civil rights. And most of us will see those and think, who are those people, how could they behave that way, right? JUDD: Right. RAZ: And we think, well, we've progressed, you know. And I think there's a similar story we've told ourselves about gender equity, you know. But I wonder whether - do you think that social media has fueled a regression? Or do you think that that's just the way we have always been as a society and social media just allows it to be amplified? JUDD: I absolutely believe that social media just exposes the thinking that was already there. And an example is I was the speaker at Nashville Sexual Assault Center's recent annual fundraiser. And a man said something to me that was - he made a reference to my pubic hair - so outrageous, so inappropriate. But that's what was on his mind. And he just said it. And the Internet has simply facilitated and exposed what is already on people's minds. And in that sense, it's a helpful tool because it shows us that our thinking needs to heal and to change and that so much of what #MeToo is really about - it's about centering survivors. And it's about radical community healing. And what's, you know, ultimately helpful about that remark that was made to me is I talked about it from the podium that night when I spoke. I said, look, I'm here for an event about sexual assault and this remark was made to me by someone who's a good person with good intentions and puts their money where their aspirational, at least, values are. And, of course, he knew I was talking about him. And he's reached out to me. And I've offered to have coffee with him so that we can hash this out. And when I can sit down with this guy and say, look, this is how it made me feel and this is why I hope if that thought ever occurs to you again that you have the integrity to examine your own thinking and change it and certainly not let it pass out of the gate of your mouth, that is so important. You know, we have to get together to do our radical community healing. (SOUNDBITE OF TED TALK)JUDD: There are a lot of solutions - thank goodness. I'm going to offer just a few. And, of course, I challenge you to create and contribute your own. No. 1, we have to start with digital media literacy, and clearly it must have a gendered lens. Kids, schools, caregivers, parents - it's essential. Two, let's talk about our friends. Men, you have a role to play and a choice to make. You can do something or you can do nothing. Online violence is an extension of in-person violence. In 2015, 72,828 women used intimate partner violence services in this country. That is not counting the girls and women and boys who needed them. We need to grow support lines and help groups so victims can help each other when their lives and finances have been derailed. We must as individuals disrupt gender violence as it is unfolding. And lastly, believe her. Believe her. (APPLAUSE)RAZ: So, Ashley, you gave this TED Talk in 2016, and what was remarkable was that just a year later when stories about Harvey Weinstein came out, you spoke publicly about what happened to you. I mean, you were - you knew all these things. And you couldn't talk about them onstage because of, I guess, the fear and the whole infrastructure of Hollywood that prevented people like you from talking about this for so long. I mean, you gave this talk, and yet there was so much more that you couldn't say or, I guess, didn't feel safe saying. JUDD: Well, the good news is I'm a teller. And when I was molested for the first time when I was 7 years old, the first thing I did was run to two adults and express exactly what had just happened to me. Now, they were neither equipped nor prepared to respond to me in an appropriate way because they said, he's a nice, old man. That's not what he meant. And when, you know, I was harassed in that Peninsula hotel room in the summer of 1997 when I was making \"Kiss the Girls,\" my dad was visiting me from Kentucky. And he was downstairs in the lobby. I came straight down. And he could tell by the look on my face that something devastating had just happened to me. And I told him right away. But we didn't know what to do with the information except to try to, you know, steer clear of Harvey Weinstein, which was a really difficult thing to do at the Peninsula hotel. He loomed ominously large at that place. And then Variety was doing one of their Women In Film issues, and I was speaking with them. And they asked me - and this was in 2015 - whether or not I'd ever experienced sexual harassment. And I told them the entire Harvey story in even greater detail than is included in the New York Times piece that Megan Twohey and Jodi Kantor wrote. But the difference is the world wasn't ready to hear it yet, no one was paying attention. You know, and when I made the decision to be the named source in The New York Times, I went on a run on my favorite little country road near where I live in rural Tennessee. And I thought, you know, I've made tougher decisions. This is not a significant decision. It's simply my truth. And I am entitled to share my truth and to be autonomous and dignified and hold my head high in my lived reality. RAZ: What is it that - in your experience, what is it that you think many men and some women don't understand about the conversations we are having around gender now? JUDD: I think that one of the difficulties for boys and men is to accept that this really is the water in which we swim and the air that we breathe, and that these microaggressions and more overt, explicit aggressions occur on a routine basis. And so it takes courage on my part - and my stomach even feels funny when I say that - to be really honest about my lived experience as a woman. You know, the invitation hopefully is that men can have the stamina to listen to our experiences in equal measure to the way that we have endured those experiences. And when I talk with men who are honest and vulnerable enough to express their discomfort, it's not a competition to say, oh, good, you're uncomfortable. Well, I've been uncomfortable for a long time. It's about empathy and shared understanding. I recently spoke at the International School in Leipzig, Germany, and it was very interesting that the girls were crying because of the street harassment they experienced. This young woman shared that she saw a woman being harassed at a tram stop, and all these people stood there and watched, and she was the one who walked over and disrupted it. And when she did, the perpetrator then followed her from car to car in the tram and then followed her home. And so the girls are already experiencing gendered violence. And interestingly, the boys asked more questions than girls, which is consistent with the data, when kids get to high school. And so I said, hey, I want to be conscious and intentional here. I've called on about five or six boys in a row. I'd like to call on a girl. And this boy shouted at me, that's sexist. And I said, well, let's talk about that. And it was interesting to experience a little microcosm of what we might call the backlash. But as long as we stay in dialogue with each other, and we have the spaciousness to hold complexity and to hold paradox and to allow for everyone to be exactly where they are in their evolution of this journey, then we're going to get there together. RAZ: Ashley, there must be people who say, OK, I hear you. You know, you're having these empathetic conversations with people, but, like, why should they get our empathy? JUDD: Systems of power don't change easily, and those with power are generally reluctant to let it go. But I can sit with those with whom I differ with dignity and respect, even as I oppose everything about the way they're thinking. And I don't know that I can explain that. It's just the way that I'm walking in my life right now. You know, someone asked me how I could forgive Harvey Weinstein, and I said, because I do it for myself. It's no favor to him. I do it for my own peace of mind, to cut the string of resentment. RAZ: You can forgive Harvey Weinstein? JUDD: Of course. Of course. I don't - you know, he's a sexual predator, he's done reprehensible things that hurt hundreds of women, and, you know, my career is very different, my pocket book is very different because of him. But I don't like to drink poison hoping someone else is going to die. RAZ: Do you think things are getting better, that this conversation is slowly starting to change things? JUDD: We are making strides. We are living in an age that is probably revolutionary. I think we'll look back and go, wow, you know, it was all happening. It was messy and imperfect and joyful and difficult and exciting and radical. I mean, I go to bed with hope, and I wake up with hope. I've done a lot of work on that. I've been to treatment for sexual trauma. I regard my recovery as the most important thing in my life. I have a place to go and people to talk to and folks who understand. And the creation of egalitarian systems like Time's Up, you know, social movements like #MeToo that allow me to have the dignity of my experience with my truth and my integrity without it being minimized the way it was when I was 7 years old, and I went to those adults and said, oh, no, no, no he is a nice old man. That's not what he meant. No, it is what he meant. It is what Harvey Weinstein meant. And it's not OK anymore. And that's - that is a day that will come. (SOUNDBITE OF MUSIC)RAZ: That's Ashley Judd. She's an activist and actor. You can find her full talk at ted. com. Hey, thanks for listening to our episode on Gender, Power and Fairness this week. If you want to find out more about who was on it, go to ted. npr. org. And to see hundreds more TED Talks, check out ted. com or the TED app. Our production staff at NPR includes Jeff Rogers, Sanaz Meshkinpour, Jinae West, Neva Grant, Casey Herman, Rachel Faulkner, Diba Mohtasham, James Delahoussaye and J. C. Howard, with help from Daniel Shukin and Daryth Gayles. Our intern is Katie Monteleone. Our partners at TED are Chris Anderson, Colin Helms, Anna Phelan and Janet Lee. I'm Guy Raz, and you've been listening to Ideas Worth Spreading right here on the TED Radio Hour from NPR. GUY RAZ, HOST:   It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about the new conversation we're having around Gender, Power and Fairness. And just a warning, some of the stories and language in this segment may be hard to hear. (SOUNDBITE OF MUSIC) ASHLEY JUDD: Hello, this is Ashley. RAZ: Ashley, good morning. This is Guy Raz. I'm the host of the program. How are you? JUDD: I know your voice well, Guy. RAZ: I know yours, too. This is Ashley Judd. JUDD: And I am a writer, a humanitarian and an actor. RAZ: Ashley was one of the first women to speak on the record about being sexually harassed by Harvey Weinstein. But she's actually been speaking out about gender violence for much longer. For years, she's been on the receiving end of intense online harassment, harassment that is routine for so many women. JUDD: I would venture to say that I began receiving gendered hate speech and misogynistic messages on social media from the very moment that I joined. RAZ: Like in one instance at a basketball game in 2015, when Ashley tweeted a complaint about the refs. JUDD: The response to that was a huge sexist pile-on, where it really started with, you know, the outrageousness of my thinking that as a female basketball fan I was entitled to have an opinion about officiating, to just a generalized, you should die; I want to rape you; I want to ejaculate on your face; you shouldn't be taking up oxygen; there was a picture of you, I wish it was a picture of your deathbed. You know, all of this stuff that I got. RAZ: Ashley Judd picks up her story from the TED stage. (SOUNDBITE OF TED TALK) JUDD: It is routine for me to be treated in the ways I've already described to you. It happens to me every single day on social media platforms, such as Twitter and Facebook. And I have responded to this with various strategies. I've tried to rise above it. I've tried to get in the trenches. But mostly, I would scroll through these social media platforms with one eye partially closed, trying not to see it, but you can't make a cucumber out of a pickle. What is seen, goes in. It's traumatic. And I was always secretly hoping, in some part of me, that what was being said to me and about me wasn't true. Because even I, an avowed, self-declared feminist who worships at the altar of Gloria, internalized the patriarchy. Patriarchy is not boys and men. It is a system in which we all participate, including me. On that particular day, for some reason, that particular tweet after the basketball game, when I was sitting at home alone in my nightgown, I got a phone call, and it was my beloved former husband. And he said on a voicemail, Loved One, what is happening to you is not OK. And there was something about him taking a stand for me that night that allowed me to take a stand for myself. And I started to write, sharing the fact that I'm a survivor of all forms of sexual abuse, including three rapes. So I wrote this feminist op-ed. It is entitled, \"Forget Your Team: It Is Your Online Gendered Violence Toward Girls And Women That Can Kiss My Righteous Ass. \" (APPLAUSE) JUDD: And I did that alone, and I published it alone, because my chief adviser said, please don't. The reign of retaliatory garbage that is inevitable, I fear for you. But I trust girls, and I trust women, and I trust our allies. It was published. It went viral. It proves that every single day online misogyny is a phenomenon endured by us all, all over the world, and when it is intersectional, it is worse. Sexual orientation, gender identity, race, ethnicity, religion - you name it, it is worse. Online misogyny is a global, gender rights tragedy, and it is imperative that it ends. (SOUNDBITE OF MUSIC) RAZ: I mean, the digital space has opened up a whole new world that was almost unanticipated, right? When this technology came out, I think most of us thought, well, this is going to bring the world together. It's going to democratize the ability to amplify and share messages and views. But I don't think anybody anticipated that it would become this repository for violence. JUDD: I think it's a concentrated space in which violence against girls and women happen. Just like #MeToo and Time's Up pulled the curtain back from the everyday sexual aggression with which hundreds of millions of us live, the Internet, I think, simply exposed some of the patterns of thinking that boys and men hold about girls and women, specifically the sexual objectification and commodification of our bodies. And you know, I think that all gender and sexual violence is on a continuum. One end is nuanced and subtle. It's unspoken, but it's thought. And then it's the microaggressions, and of course, it goes all the way to the other end of the spectrum with homicide. And the Internet is a place where all of that can flourish. And I do believe that the Internet itself is neutral and it's a tool and it can be used for good or it can be used for ill. When it's in the hands of misogynists, it's a powerfully destructive force. RAZ: You know, I think they're stories we tell ourselves, right? Like you look at black-and-white films of angry mobs protesting against civil rights. And most of us will see those and think, who are those people, how could they behave that way, right? JUDD: Right. RAZ: And we think, well, we've progressed, you know. And I think there's a similar story we've told ourselves about gender equity, you know. But I wonder whether - do you think that social media has fueled a regression? Or do you think that that's just the way we have always been as a society and social media just allows it to be amplified? JUDD: I absolutely believe that social media just exposes the thinking that was already there. And an example is I was the speaker at Nashville Sexual Assault Center's recent annual fundraiser. And a man said something to me that was - he made a reference to my pubic hair - so outrageous, so inappropriate. But that's what was on his mind. And he just said it. And the Internet has simply facilitated and exposed what is already on people's minds. And in that sense, it's a helpful tool because it shows us that our thinking needs to heal and to change and that so much of what #MeToo is really about - it's about centering survivors. And it's about radical community healing. And what's, you know, ultimately helpful about that remark that was made to me is I talked about it from the podium that night when I spoke. I said, look, I'm here for an event about sexual assault and this remark was made to me by someone who's a good person with good intentions and puts their money where their aspirational, at least, values are. And, of course, he knew I was talking about him. And he's reached out to me. And I've offered to have coffee with him so that we can hash this out. And when I can sit down with this guy and say, look, this is how it made me feel and this is why I hope if that thought ever occurs to you again that you have the integrity to examine your own thinking and change it and certainly not let it pass out of the gate of your mouth, that is so important. You know, we have to get together to do our radical community healing. (SOUNDBITE OF TED TALK) JUDD: There are a lot of solutions - thank goodness. I'm going to offer just a few. And, of course, I challenge you to create and contribute your own. No. 1, we have to start with digital media literacy, and clearly it must have a gendered lens. Kids, schools, caregivers, parents - it's essential. Two, let's talk about our friends. Men, you have a role to play and a choice to make. You can do something or you can do nothing. Online violence is an extension of in-person violence. In 2015, 72,828 women used intimate partner violence services in this country. That is not counting the girls and women and boys who needed them. We need to grow support lines and help groups so victims can help each other when their lives and finances have been derailed. We must as individuals disrupt gender violence as it is unfolding. And lastly, believe her. Believe her. (APPLAUSE) RAZ: So, Ashley, you gave this TED Talk in 2016, and what was remarkable was that just a year later when stories about Harvey Weinstein came out, you spoke publicly about what happened to you. I mean, you were - you knew all these things. And you couldn't talk about them onstage because of, I guess, the fear and the whole infrastructure of Hollywood that prevented people like you from talking about this for so long. I mean, you gave this talk, and yet there was so much more that you couldn't say or, I guess, didn't feel safe saying. JUDD: Well, the good news is I'm a teller. And when I was molested for the first time when I was 7 years old, the first thing I did was run to two adults and express exactly what had just happened to me. Now, they were neither equipped nor prepared to respond to me in an appropriate way because they said, he's a nice, old man. That's not what he meant. And when, you know, I was harassed in that Peninsula hotel room in the summer of 1997 when I was making \"Kiss the Girls,\" my dad was visiting me from Kentucky. And he was downstairs in the lobby. I came straight down. And he could tell by the look on my face that something devastating had just happened to me. And I told him right away. But we didn't know what to do with the information except to try to, you know, steer clear of Harvey Weinstein, which was a really difficult thing to do at the Peninsula hotel. He loomed ominously large at that place. And then Variety was doing one of their Women In Film issues, and I was speaking with them. And they asked me - and this was in 2015 - whether or not I'd ever experienced sexual harassment. And I told them the entire Harvey story in even greater detail than is included in the New York Times piece that Megan Twohey and Jodi Kantor wrote. But the difference is the world wasn't ready to hear it yet, no one was paying attention. You know, and when I made the decision to be the named source in The New York Times, I went on a run on my favorite little country road near where I live in rural Tennessee. And I thought, you know, I've made tougher decisions. This is not a significant decision. It's simply my truth. And I am entitled to share my truth and to be autonomous and dignified and hold my head high in my lived reality. RAZ: What is it that - in your experience, what is it that you think many men and some women don't understand about the conversations we are having around gender now? JUDD: I think that one of the difficulties for boys and men is to accept that this really is the water in which we swim and the air that we breathe, and that these microaggressions and more overt, explicit aggressions occur on a routine basis. And so it takes courage on my part - and my stomach even feels funny when I say that - to be really honest about my lived experience as a woman. You know, the invitation hopefully is that men can have the stamina to listen to our experiences in equal measure to the way that we have endured those experiences. And when I talk with men who are honest and vulnerable enough to express their discomfort, it's not a competition to say, oh, good, you're uncomfortable. Well, I've been uncomfortable for a long time. It's about empathy and shared understanding. I recently spoke at the International School in Leipzig, Germany, and it was very interesting that the girls were crying because of the street harassment they experienced. This young woman shared that she saw a woman being harassed at a tram stop, and all these people stood there and watched, and she was the one who walked over and disrupted it. And when she did, the perpetrator then followed her from car to car in the tram and then followed her home. And so the girls are already experiencing gendered violence. And interestingly, the boys asked more questions than girls, which is consistent with the data, when kids get to high school. And so I said, hey, I want to be conscious and intentional here. I've called on about five or six boys in a row. I'd like to call on a girl. And this boy shouted at me, that's sexist. And I said, well, let's talk about that. And it was interesting to experience a little microcosm of what we might call the backlash. But as long as we stay in dialogue with each other, and we have the spaciousness to hold complexity and to hold paradox and to allow for everyone to be exactly where they are in their evolution of this journey, then we're going to get there together. RAZ: Ashley, there must be people who say, OK, I hear you. You know, you're having these empathetic conversations with people, but, like, why should they get our empathy? JUDD: Systems of power don't change easily, and those with power are generally reluctant to let it go. But I can sit with those with whom I differ with dignity and respect, even as I oppose everything about the way they're thinking. And I don't know that I can explain that. It's just the way that I'm walking in my life right now. You know, someone asked me how I could forgive Harvey Weinstein, and I said, because I do it for myself. It's no favor to him. I do it for my own peace of mind, to cut the string of resentment. RAZ: You can forgive Harvey Weinstein? JUDD: Of course. Of course. I don't - you know, he's a sexual predator, he's done reprehensible things that hurt hundreds of women, and, you know, my career is very different, my pocket book is very different because of him. But I don't like to drink poison hoping someone else is going to die. RAZ: Do you think things are getting better, that this conversation is slowly starting to change things? JUDD: We are making strides. We are living in an age that is probably revolutionary. I think we'll look back and go, wow, you know, it was all happening. It was messy and imperfect and joyful and difficult and exciting and radical. I mean, I go to bed with hope, and I wake up with hope. I've done a lot of work on that. I've been to treatment for sexual trauma. I regard my recovery as the most important thing in my life. I have a place to go and people to talk to and folks who understand. And the creation of egalitarian systems like Time's Up, you know, social movements like #MeToo that allow me to have the dignity of my experience with my truth and my integrity without it being minimized the way it was when I was 7 years old, and I went to those adults and said, oh, no, no, no he is a nice old man. That's not what he meant. No, it is what he meant. It is what Harvey Weinstein meant. And it's not OK anymore. And that's - that is a day that will come. (SOUNDBITE OF MUSIC) RAZ: That's Ashley Judd. She's an activist and actor. You can find her full talk at ted. com. Hey, thanks for listening to our episode on Gender, Power and Fairness this week. If you want to find out more about who was on it, go to ted. npr. org. And to see hundreds more TED Talks, check out ted. com or the TED app. Our production staff at NPR includes Jeff Rogers, Sanaz Meshkinpour, Jinae West, Neva Grant, Casey Herman, Rachel Faulkner, Diba Mohtasham, James Delahoussaye and J. C. Howard, with help from Daniel Shukin and Daryth Gayles. Our intern is Katie Monteleone. Our partners at TED are Chris Anderson, Colin Helms, Anna Phelan and Janet Lee. I'm Guy Raz, and you've been listening to Ideas Worth Spreading right here on the TED Radio Hour from NPR.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-01-690609162": {"title": "Net Neutrality Goes Back To Court : NPR", "url": "https://www.npr.org/2019/02/01/690609162/net-neutrality-goes-back-to-court", "author": "No author found", "published_date": "2019-02-01", "content": "", "section": "Technology", "disclaimer": ""}, "2019-02-05-691403968": {"title": "Charges Against Chinese Hackers Are Now Common. Why Don't They Deter Cyberattacks? : NPR", "url": "https://www.npr.org/2019/02/05/691403968/charges-against-chinese-hackers-are-now-common-why-dont-they-deter-cyberattacks", "author": "No author found", "published_date": "2019-02-05", "content": "STEVE INSKEEP, HOST: All right. The Justice Department has brought many cases against suspected Chinese hackers by now. They're accused of stealing American companies' intellectual property and trade secrets. But cyber theft, by all accounts, continues at a rapid pace. So is the Justice Department's strategy working? Here's NPR justice reporter Ryan Lucas. RYAN LUCAS, BYLINE: May of 2014 and front-page news out of the Justice Department. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER: The United States has charged China's military with cyber espionage. Five Chinese officers are accused of hacking into big American companies for their trade secrets. LUCAS: It was a big deal because it was the first time the U. S. had charged state-sponsored actors for hacking American companies. Now, fast-forward to this past December. Here's President Trump's deputy attorney general, Rod Rosenstein, announcing new charges against two more suspected Chinese hackers. (SOUNDBITE OF ARCHIVED RECORDING)ROD ROSENSTEIN: America and its many allies know what China is doing. We know why they're doing it. And in some cases, we even know exactly who is sitting at the keyboard perpetrating these crimes. LUCAS: It is a scene that has played out repeatedly over the past five years. The Justice Department has announced charges against suspected hackers allegedly working at the behest of the Chinese government to steal American intellectual property. Despite the slew of indictments, officials say China has not stopped targeting American companies. And in nearly all of the hacking cases, the accused are in China and unlikely to ever see the inside of a U. S. courtroom. That has led some observers to question just how effective this indictment strategy is. ADAM SEGAL: It does not seem to have stopped the Chinese, and it certainly doesn't seem to have imposed any cost on them to get them to the point where they think it's not worth the attacks. LUCAS: That's Adam Segal. He directs the Digital and Cyberspace Policy Program at the Council on Foreign Relations. Segal is not alone in his criticism. Jack Goldsmith is a Harvard law professor and former DOJ official in the George W. Bush administration. Goldsmith argues that because indictments do not pose much of a cost, they have failed to deter China from further hacking. The charges may embarrass the defendants and show that the U. S. can pinpoint who's behind the hacking, Goldsmith says. But those costs are paltry compared to the billions of dollars' worth of secrets the Chinese are allegedly stealing. JACK LANDMAN GOLDSMITH: I would say that, to date, the United States has not found a strategy - over many, many years after much discussion and much thought and much effort, it has not found a strategy to get the Chinese to tamp this down. LUCAS: Supporters acknowledge that China has not stopped hacking, but they say indictments have had a positive effect. They note that a year after the DOJ first brought charges, China reached an agreement with the Obama administration to not conduct cyber economic espionage. Officials and cybersecurity experts say the pace and scale of Chinese cyberattacks dropped off after the agreement. The hacking has ramped back up since then, according to U. S. officials. Although some of the current hacking may fall into a gray zone that is arguably not covered by the 2015 deal. The indictments also have been critical in publicizing information that previously had been kept under lock and key by the U. S. government. That has helped raise public awareness, particularly among American companies, about China's pervasive hacking. JOHN HULTQUIST: What the indictments do is they put all this information about this in the hands of the people who are now being targeted. LUCAS: That's John Hultquist. He's the director of intelligence analysis at the cybersecurity firm FireEye. HULTQUIST: So it's really important to get this into their hands and in some cases even prove to them that it's happening. LUCAS: That information has prompted some American and other Western companies to reconsider doing business in China or with Chinese partners. Hultquist says the indictments also have had a disruptive effect. HULTQUIST: The adversary seems to have, at least for a short term, changed up operations, burned their infrastructure, had to change their tooling, go back to square one. LUCAS: John Carlin led the Justice Department's National Security Division in the Obama administration. His new book, \"Dawn Of The Code War,\" details how the DOJ built its first case against Chinese hackers back in 2014. He says indictments should not be viewed as a solution on their own. Instead, he says, they send a public signal while at the same time laying the groundwork for the U. S. to use other tools to get China to stop. JOHN CARLIN: But if we're going to change this behavior, it has to be part of a larger strategy of raising the cost and includes all the instruments of U. S. power, including the power to sanction under the Treasury Department. LUCAS: To date, the U. S. Treasury has not used that power against China over its cyber industrial espionage. Ryan Lucas, NPR News, Washington. (SOUNDBITE OF PLAID'S \"SLAM\") STEVE INSKEEP, HOST:  All right. The Justice Department has brought many cases against suspected Chinese hackers by now. They're accused of stealing American companies' intellectual property and trade secrets. But cyber theft, by all accounts, continues at a rapid pace. So is the Justice Department's strategy working? Here's NPR justice reporter Ryan Lucas. RYAN LUCAS, BYLINE: May of 2014 and front-page news out of the Justice Department. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER: The United States has charged China's military with cyber espionage. Five Chinese officers are accused of hacking into big American companies for their trade secrets. LUCAS: It was a big deal because it was the first time the U. S. had charged state-sponsored actors for hacking American companies. Now, fast-forward to this past December. Here's President Trump's deputy attorney general, Rod Rosenstein, announcing new charges against two more suspected Chinese hackers. (SOUNDBITE OF ARCHIVED RECORDING) ROD ROSENSTEIN: America and its many allies know what China is doing. We know why they're doing it. And in some cases, we even know exactly who is sitting at the keyboard perpetrating these crimes. LUCAS: It is a scene that has played out repeatedly over the past five years. The Justice Department has announced charges against suspected hackers allegedly working at the behest of the Chinese government to steal American intellectual property. Despite the slew of indictments, officials say China has not stopped targeting American companies. And in nearly all of the hacking cases, the accused are in China and unlikely to ever see the inside of a U. S. courtroom. That has led some observers to question just how effective this indictment strategy is. ADAM SEGAL: It does not seem to have stopped the Chinese, and it certainly doesn't seem to have imposed any cost on them to get them to the point where they think it's not worth the attacks. LUCAS: That's Adam Segal. He directs the Digital and Cyberspace Policy Program at the Council on Foreign Relations. Segal is not alone in his criticism. Jack Goldsmith is a Harvard law professor and former DOJ official in the George W. Bush administration. Goldsmith argues that because indictments do not pose much of a cost, they have failed to deter China from further hacking. The charges may embarrass the defendants and show that the U. S. can pinpoint who's behind the hacking, Goldsmith says. But those costs are paltry compared to the billions of dollars' worth of secrets the Chinese are allegedly stealing. JACK LANDMAN GOLDSMITH: I would say that, to date, the United States has not found a strategy - over many, many years after much discussion and much thought and much effort, it has not found a strategy to get the Chinese to tamp this down. LUCAS: Supporters acknowledge that China has not stopped hacking, but they say indictments have had a positive effect. They note that a year after the DOJ first brought charges, China reached an agreement with the Obama administration to not conduct cyber economic espionage. Officials and cybersecurity experts say the pace and scale of Chinese cyberattacks dropped off after the agreement. The hacking has ramped back up since then, according to U. S. officials. Although some of the current hacking may fall into a gray zone that is arguably not covered by the 2015 deal. The indictments also have been critical in publicizing information that previously had been kept under lock and key by the U. S. government. That has helped raise public awareness, particularly among American companies, about China's pervasive hacking. JOHN HULTQUIST: What the indictments do is they put all this information about this in the hands of the people who are now being targeted. LUCAS: That's John Hultquist. He's the director of intelligence analysis at the cybersecurity firm FireEye. HULTQUIST: So it's really important to get this into their hands and in some cases even prove to them that it's happening. LUCAS: That information has prompted some American and other Western companies to reconsider doing business in China or with Chinese partners. Hultquist says the indictments also have had a disruptive effect. HULTQUIST: The adversary seems to have, at least for a short term, changed up operations, burned their infrastructure, had to change their tooling, go back to square one. LUCAS: John Carlin led the Justice Department's National Security Division in the Obama administration. His new book, \"Dawn Of The Code War,\" details how the DOJ built its first case against Chinese hackers back in 2014. He says indictments should not be viewed as a solution on their own. Instead, he says, they send a public signal while at the same time laying the groundwork for the U. S. to use other tools to get China to stop. JOHN CARLIN: But if we're going to change this behavior, it has to be part of a larger strategy of raising the cost and includes all the instruments of U. S. power, including the power to sanction under the Treasury Department. LUCAS: To date, the U. S. Treasury has not used that power against China over its cyber industrial espionage. Ryan Lucas, NPR News, Washington. (SOUNDBITE OF PLAID'S \"SLAM\")", "section": "National Security", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-06-691553066": {"title": "Google Super Bowl Ad Talks Up Veterans, But Does It Walk The Walk? : NPR", "url": "https://www.npr.org/2019/02/06/691553066/google-talks-up-vets-in-super-bowl-ad-does-it-walk-the-walk", "author": "No author found", "published_date": "2019-02-06", "content": "", "section": "Business", "disclaimer": ""}, "2019-02-07-692466456": {"title": "Cryptocurrency Exchange Operator Dies Without Sharing Passwords With Anyone  : NPR", "url": "https://www.npr.org/2019/02/07/692466456/cryptocurrency-exchange-operator-dies-without-sharing-passwords-with-anyone", "author": "No author found", "published_date": "2019-02-07", "content": "MARY LOUISE KELLY, HOST: Cash may still be king, but there are more ways to buy and sell and store our money than ever before. We're looking beyond cash in this month's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")KELLY: You can imagine what might happen if a bank lost the keys to the safe, and thousands of people couldn't get their hands on their own money and valuables. That is a simplified low-tech version of what has happened in Canada. The operator of a cryptocurrency exchange based there died in December without telling anyone what amounts to the passwords - the digital keys to the digital safes. Nolan Bauerle is head of research for the trade hub CoinDesk. He joins us from New York to explain what's going on here. Hi, Nolan. NOLAN BAUERLE: Hi. KELLY: So this Canadian cryptocurrency exchange is called QuadrigaCX. It allows people to trade and store money through online platforms. Is that the most basic explanation? BAUERLE: Yeah. And it does something a little niche in the market in that it's an on-ramp for cash transactions. KELLY: So you could park your cash there, too. BAUERLE: You sure can. KELLY: The CEO who died - he knew what the digital keys were, like a password. He had them stored on his personal laptop, and nobody else can get onto his laptop. Is that the situation right now? BAUERLE: Yeah. This is a really unsophisticated way to run a company that supposedly has this many digital assets under control. KELLY: OK. BAUERLE: It looks like the key management policies of this exchange were almost old-fashioned in this industry. KELLY: Ye olden days of digital currency (laughter). Go on. BAUERLE: We have developed as an industry multiple ways to manage these keys safely where three or four people would sign off on a transaction. It looks like what they did here was they gave all the authority to one person. So there's a single point of failure in this exchange. KELLY: Getting into a laptop, cracking the password for that - that seems like a relatively manageable problem for a smart hacker, or no? BAUERLE: I think so. And right now what's happening in Canada is Ernst and Young has involved themselves in this case. And they are going to be looking into getting into this laptop. So Quadriga has tried to get into the laptop for a while. We'll find out now where the case is with Ernst and Young - if this is possible. KELLY: It is also theoretically possible that all this money - and we're talking well over $100 million - that it could maybe be locked away forever. BAUERLE: Yeah, that is absolutely a scenario here. KELLY: This is making me want to hide all my money in a big wad under the mattress. (LAUGHTER)KELLY: Which prompts me to ask, who had put their money into Quadriga - elite investors investing hundreds of thousands, millions of dollars, or is this ordinary. . . BAUERLE: No, this is hundreds of thousands of accounts of small retail investors. KELLY: Are there laws that are supposed to regulate how you manage your crypto keys? BAUERLE: Different jurisdictions have different laws. In Canada, what it looks like is, mostly, money service business is law. So they have to show that they have, you know, a certain amount of reserves. KELLY: But meanwhile, is this giving people who might be thinking about investing in cryptocurrency pause? BAUERLE: I think it definitely should give everyone a chance to think again, but these are lessons that have been spoken about very openly by the community for a long time. Don't put your keys with an exchange for an extended period of time or a long time. KELLY: Nolan Bauerle. He's head of research for CoinDesk. Thanks so much. BAUERLE: Thank you for having me. MARY LOUISE KELLY, HOST:  Cash may still be king, but there are more ways to buy and sell and store our money than ever before. We're looking beyond cash in this month's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") KELLY: You can imagine what might happen if a bank lost the keys to the safe, and thousands of people couldn't get their hands on their own money and valuables. That is a simplified low-tech version of what has happened in Canada. The operator of a cryptocurrency exchange based there died in December without telling anyone what amounts to the passwords - the digital keys to the digital safes. Nolan Bauerle is head of research for the trade hub CoinDesk. He joins us from New York to explain what's going on here. Hi, Nolan. NOLAN BAUERLE: Hi. KELLY: So this Canadian cryptocurrency exchange is called QuadrigaCX. It allows people to trade and store money through online platforms. Is that the most basic explanation? BAUERLE: Yeah. And it does something a little niche in the market in that it's an on-ramp for cash transactions. KELLY: So you could park your cash there, too. BAUERLE: You sure can. KELLY: The CEO who died - he knew what the digital keys were, like a password. He had them stored on his personal laptop, and nobody else can get onto his laptop. Is that the situation right now? BAUERLE: Yeah. This is a really unsophisticated way to run a company that supposedly has this many digital assets under control. KELLY: OK. BAUERLE: It looks like the key management policies of this exchange were almost old-fashioned in this industry. KELLY: Ye olden days of digital currency (laughter). Go on. BAUERLE: We have developed as an industry multiple ways to manage these keys safely where three or four people would sign off on a transaction. It looks like what they did here was they gave all the authority to one person. So there's a single point of failure in this exchange. KELLY: Getting into a laptop, cracking the password for that - that seems like a relatively manageable problem for a smart hacker, or no? BAUERLE: I think so. And right now what's happening in Canada is Ernst and Young has involved themselves in this case. And they are going to be looking into getting into this laptop. So Quadriga has tried to get into the laptop for a while. We'll find out now where the case is with Ernst and Young - if this is possible. KELLY: It is also theoretically possible that all this money - and we're talking well over $100 million - that it could maybe be locked away forever. BAUERLE: Yeah, that is absolutely a scenario here. KELLY: This is making me want to hide all my money in a big wad under the mattress. (LAUGHTER) KELLY: Which prompts me to ask, who had put their money into Quadriga - elite investors investing hundreds of thousands, millions of dollars, or is this ordinary. . . BAUERLE: No, this is hundreds of thousands of accounts of small retail investors. KELLY: Are there laws that are supposed to regulate how you manage your crypto keys? BAUERLE: Different jurisdictions have different laws. In Canada, what it looks like is, mostly, money service business is law. So they have to show that they have, you know, a certain amount of reserves. KELLY: But meanwhile, is this giving people who might be thinking about investing in cryptocurrency pause? BAUERLE: I think it definitely should give everyone a chance to think again, but these are lessons that have been spoken about very openly by the community for a long time. Don't put your keys with an exchange for an extended period of time or a long time. KELLY: Nolan Bauerle. He's head of research for CoinDesk. Thanks so much. BAUERLE: Thank you for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-07-692328192": {"title": "'Brave, Not Perfect' Speaks To The Scarcity Of Women In Tech : NPR", "url": "https://www.npr.org/2019/02/07/692328192/brave-not-perfect-speaks-to-the-scarcity-of-women-in-tech", "author": "No author found", "published_date": "2019-02-07", "content": "", "section": "Book Reviews", "disclaimer": ""}, "2019-02-07-692312687": {"title": "German Regulator Says Facebook Can't Use Data From Instagram, WhatsApp : NPR", "url": "https://www.npr.org/2019/02/07/692312687/facebook-cant-gather-users-data-from-other-websites-german-antitrust-office-says", "author": "No author found", "published_date": "2019-02-07", "content": "", "section": "Technology", "disclaimer": ""}, "2019-02-07-692260599": {"title": "New Emojis Coming: Wheelchairs, Canes, Interracial Couples : NPR", "url": "https://www.npr.org/2019/02/07/692260599/interracial-couples-and-disability-friendly-emojis-coming-soon-to-smartphones", "author": "No author found", "published_date": "2019-02-07", "content": "", "section": "Technology", "disclaimer": ""}, "2019-02-08-692823620": {"title": "What Bezos' Allegations Could Mean For American Media's Immunity Deal : NPR", "url": "https://www.npr.org/2019/02/08/692823620/what-bezos-allegations-could-mean-for-american-media-s-immunity-deal", "author": "No author found", "published_date": "2019-02-08", "content": "MARY LOUISE KELLY, HOST: OK, back to the legal threats of this fight between Jeff Bezos and American Media. As Alina just mentioned, AMI signed an immunity deal with federal prosecutors in New York. The deal was in exchange for cooperation and pledging not to break the law again, AMI admitted to violating campaign finance laws in order to help President Trump get elected. As Alina also just mentioned, Bezos now alleging blackmail and extortion hangs a big old question mark over the fate of this immunity deal. To talk about that, we are joined by legal analyst Jeffrey Toobin. Hi, Jeff. JEFFERY TOOBIN: Hi. KELLY: Hi. So AMI put out a statement this morning saying they believe fervently that the company acted lawfully but also saying they're going to look into Bezos's claim. So if he is proven right, if they find something, did AMI break the law? TOOBIN: Well, this is a very interesting question. Extortion is obtaining a thing of value by threats. In other words, if I say to you, give me a million dollars or I'll kill you, that's extortion. Well, here there's clearly a threat. We will release these embarrassing photographs if you don't drop this investigation and clear our name. KELLY: So the question is. . . TOOBIN: So the question is. . . KELLY: . . . What's the thing of value? TOOBIN: Exactly. The question is, is that service from the Washington Post a thing of value? And I think the answer clearly to that is yes. I mean, it is enormous value to AMI to be cleared by the Washington Post. However, it is not certainly a kind of extortion that I've ever heard of actually being prosecuted. And one possible defense that AMI might offer is that, look; we are simply trying to get our name cleared through preventing unfair, inaccurate publicity, and you can't punish us for that. Now, it's not usually done through the threat of, you know, dirty pictures, but I think that sets up the argument. I don't really know the answer. KELLY: That sets up my next question, which is, what standard would have to be met to prove that AMI violated its immunity deal? TOOBIN: Well, the - all immunity deals and non-prosecution agreements have the condition that you must not commit any more crimes. So it's really the same question. KELLY: Yeah, we looked it up, and it's shall not commit - shall commit no crimes whatsoever. TOOBIN: Right. KELLY: Right. TOOBIN: And so if AMI were even prosecuted, I think, for this extortion or blackmail, that could void the immunity agreement right there. So - but they come down to the same question, which is was this interaction with Bezos a violation of the law? KELLY: And without venturing too far down the road of hypotheticals, what would be the consequences if AMI was found to have violated the deal? TOOBIN: Well, then they could be prosecuted for the unlawful campaign contribution, which is - was the money given to Karen McDougal to help the Trump campaign. KELLY: Karen McDougal, one of the. . . TOOBIN: I'm sorry. KELLY: . . . The women. . . TOOBIN: Yes. KELLY: . . . Who says she had an affair with President Trump before he was president. . . TOOBIN: Correct. KELLY: . . . Which he denies, OK. TOOBIN: That $150,000 payment to her could be prosecuted as an illegal campaign contribution, which is one of the things that Michael Cohen already pleaded guilty to. That's Trump's former personal lawyer. KELLY: Real quick in the moments we have left, you were a federal prosecutor once. Game out for me how the attorneys who signed that immunity deal are thinking about all these developments. TOOBIN: Well, I think they have to do an investigation. You know, the Bezos post on medium. com was certainly very provocative and interesting and potentially incriminating, but I think what you have to do is start talking to everyone involved. . . KELLY: Yeah. TOOBIN: . . . You have to see what the other emails were between everyone, what - were there any phone calls between them? You have to do a thorough investigation. But it is certainly worthwhile to do that investigation because, as Bezos says, it does sound like blackmail or extortion. KELLY: Jeffery Toobin, thank you. He is a legal analyst at CNN and a staff writer at The New Yorker. MARY LOUISE KELLY, HOST:  OK, back to the legal threats of this fight between Jeff Bezos and American Media. As Alina just mentioned, AMI signed an immunity deal with federal prosecutors in New York. The deal was in exchange for cooperation and pledging not to break the law again, AMI admitted to violating campaign finance laws in order to help President Trump get elected. As Alina also just mentioned, Bezos now alleging blackmail and extortion hangs a big old question mark over the fate of this immunity deal. To talk about that, we are joined by legal analyst Jeffrey Toobin. Hi, Jeff. JEFFERY TOOBIN: Hi. KELLY: Hi. So AMI put out a statement this morning saying they believe fervently that the company acted lawfully but also saying they're going to look into Bezos's claim. So if he is proven right, if they find something, did AMI break the law? TOOBIN: Well, this is a very interesting question. Extortion is obtaining a thing of value by threats. In other words, if I say to you, give me a million dollars or I'll kill you, that's extortion. Well, here there's clearly a threat. We will release these embarrassing photographs if you don't drop this investigation and clear our name. KELLY: So the question is. . . TOOBIN: So the question is. . . KELLY: . . . What's the thing of value? TOOBIN: Exactly. The question is, is that service from the Washington Post a thing of value? And I think the answer clearly to that is yes. I mean, it is enormous value to AMI to be cleared by the Washington Post. However, it is not certainly a kind of extortion that I've ever heard of actually being prosecuted. And one possible defense that AMI might offer is that, look; we are simply trying to get our name cleared through preventing unfair, inaccurate publicity, and you can't punish us for that. Now, it's not usually done through the threat of, you know, dirty pictures, but I think that sets up the argument. I don't really know the answer. KELLY: That sets up my next question, which is, what standard would have to be met to prove that AMI violated its immunity deal? TOOBIN: Well, the - all immunity deals and non-prosecution agreements have the condition that you must not commit any more crimes. So it's really the same question. KELLY: Yeah, we looked it up, and it's shall not commit - shall commit no crimes whatsoever. TOOBIN: Right. KELLY: Right. TOOBIN: And so if AMI were even prosecuted, I think, for this extortion or blackmail, that could void the immunity agreement right there. So - but they come down to the same question, which is was this interaction with Bezos a violation of the law? KELLY: And without venturing too far down the road of hypotheticals, what would be the consequences if AMI was found to have violated the deal? TOOBIN: Well, then they could be prosecuted for the unlawful campaign contribution, which is - was the money given to Karen McDougal to help the Trump campaign. KELLY: Karen McDougal, one of the. . . TOOBIN: I'm sorry. KELLY: . . . The women. . . TOOBIN: Yes. KELLY: . . . Who says she had an affair with President Trump before he was president. . . TOOBIN: Correct. KELLY: . . . Which he denies, OK. TOOBIN: That $150,000 payment to her could be prosecuted as an illegal campaign contribution, which is one of the things that Michael Cohen already pleaded guilty to. That's Trump's former personal lawyer. KELLY: Real quick in the moments we have left, you were a federal prosecutor once. Game out for me how the attorneys who signed that immunity deal are thinking about all these developments. TOOBIN: Well, I think they have to do an investigation. You know, the Bezos post on medium. com was certainly very provocative and interesting and potentially incriminating, but I think what you have to do is start talking to everyone involved. . . KELLY: Yeah. TOOBIN: . . . You have to see what the other emails were between everyone, what - were there any phone calls between them? You have to do a thorough investigation. But it is certainly worthwhile to do that investigation because, as Bezos says, it does sound like blackmail or extortion. KELLY: Jeffery Toobin, thank you. He is a legal analyst at CNN and a staff writer at The New Yorker.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-08-692823610": {"title": "The Big Bezos Question: Will Investigators Take A New Look At American Media? : NPR", "url": "https://www.npr.org/2019/02/08/692823610/the-big-bezos-question-will-investigators-take-a-new-look-at-american-media", "author": "No author found", "published_date": "2019-02-08", "content": "ARI SHAPIRO, HOST: The world's richest man published an explosive blog post last night - a 9-minute read that could have huge repercussions. Jeff Bezos, the Amazon founder and Washington Post owner, accused the publisher of the National Enquirer of trying to blackmail him with sexually explicit photos. The National Enquirer is also caught up in the federal investigations tied to the Trump campaign. NPR's Alina Selyukh is here to help us untangle the many threads here. Alina, this is a crazy story. ALINA SELYUKH, BYLINE: So many threads. SHAPIRO: What are the latest developments today? SELYUKH: The big question today is whether federal investigators will take a new look at the publisher of the National Enquirer, as you said. It's called American Media. It's run by David Pecker, and Pecker is or at least used to be a longtime friend of President Trump. And Jeff Bezos is accusing him and his company of extortion and blackmail. And that's big because - bear with me here - Pecker has an immunity deal with federal prosecutors. It's part of a case of a hush money payment to a woman who said she had an affair with Trump. But the deal hinges on Becker and his company staying out of trouble, and Bezos alleging blackmail puts a huge, bold question mark over the fate of this immunity deal. SHAPIRO: Wow. So one question here is has he just nullified the immunity deal? How did Bezos get involved in this entire controversy that until now was about President Trump and hush money payments to women? SELYUKH: Well, a few weeks ago, the National Enquirer published very private, awkward, embarrassing text messages that Bezos had sent to the woman with whom he was having an affair. And Bezos funded an investigation to find the leaker of these messages. Well, fast-forward to last night. He decided to publish emails that he says he's received from officials at American Media and the National Enquirer. And the emails say the tabloid has many more salacious texts and photos, which are described in painful detail. And the emails demand that Bezos stop investigating the leak and also publicly declare that the coverage of his affair was not politically motivated. So today, American Media responded. They're basically saying they were reporting on Bezos, and they did it legally. Though notably, they did not comment on the content of the emails that were posted by Bezos. And I just want to say this is unusual for Bezos. He's very rich of course, but he is a tech nerd who founded Amazon. He's not your typical celebrity. This is not a man who's lived out his life in the public eye. And with this blog post, he's started a pretty public crusade against a media organization that time and again has gone to bat for President Trump. And now Bezos is accusing them of going after him for political reasons because he owns the Washington Post, which Trump has repeatedly criticized. SHAPIRO: We should also say that while the National Enquirer says what it did was legal, it is not typical journalistic practice to say, we have embarrassing material, and if you don't back down, we will publish it. SELYUKH: Right. SHAPIRO: But how typical is it for the National Enquirer and its publishing - its owner company to do this sort of thing? SELYUKH: It definitely has, as you're saying, a history of practices that are considered journalistically unethical - for example, paying sources for stories, buying embarrassing stories about celebrities in exchange for favors from them, the tactic known as catch-and-kill in which the tabloid buys rights to stories purely to suppress them. SHAPIRO: That was what was alleged in the Trump incident. SELYUKH: We'll get to that in just a minute. SHAPIRO: (Laughter) OK. SELYUKH: That is actually what got the National Enquirer in the federal investigation - involved in the federal investigation with the Trump campaign, as you were saying. American Media and Becker admitted to paying former Playboy model Karen McDougal to buy the rights to her story. She says she had an affair with Trump, and the publisher's plan was to kill the story to protect the Trump campaign during the 2016 election. Then Becker cut an immunity deal, as I mentioned, with the prosecutors. And this is the deal that's now in question because of Bezos's accusations of essentially criminal wrongdoing. SHAPIRO: NPR's Alina Selyukh, thank you very much. SELYUKH: Thank you. ARI SHAPIRO, HOST:  The world's richest man published an explosive blog post last night - a 9-minute read that could have huge repercussions. Jeff Bezos, the Amazon founder and Washington Post owner, accused the publisher of the National Enquirer of trying to blackmail him with sexually explicit photos. The National Enquirer is also caught up in the federal investigations tied to the Trump campaign. NPR's Alina Selyukh is here to help us untangle the many threads here. Alina, this is a crazy story. ALINA SELYUKH, BYLINE: So many threads. SHAPIRO: What are the latest developments today? SELYUKH: The big question today is whether federal investigators will take a new look at the publisher of the National Enquirer, as you said. It's called American Media. It's run by David Pecker, and Pecker is or at least used to be a longtime friend of President Trump. And Jeff Bezos is accusing him and his company of extortion and blackmail. And that's big because - bear with me here - Pecker has an immunity deal with federal prosecutors. It's part of a case of a hush money payment to a woman who said she had an affair with Trump. But the deal hinges on Becker and his company staying out of trouble, and Bezos alleging blackmail puts a huge, bold question mark over the fate of this immunity deal. SHAPIRO: Wow. So one question here is has he just nullified the immunity deal? How did Bezos get involved in this entire controversy that until now was about President Trump and hush money payments to women? SELYUKH: Well, a few weeks ago, the National Enquirer published very private, awkward, embarrassing text messages that Bezos had sent to the woman with whom he was having an affair. And Bezos funded an investigation to find the leaker of these messages. Well, fast-forward to last night. He decided to publish emails that he says he's received from officials at American Media and the National Enquirer. And the emails say the tabloid has many more salacious texts and photos, which are described in painful detail. And the emails demand that Bezos stop investigating the leak and also publicly declare that the coverage of his affair was not politically motivated. So today, American Media responded. They're basically saying they were reporting on Bezos, and they did it legally. Though notably, they did not comment on the content of the emails that were posted by Bezos. And I just want to say this is unusual for Bezos. He's very rich of course, but he is a tech nerd who founded Amazon. He's not your typical celebrity. This is not a man who's lived out his life in the public eye. And with this blog post, he's started a pretty public crusade against a media organization that time and again has gone to bat for President Trump. And now Bezos is accusing them of going after him for political reasons because he owns the Washington Post, which Trump has repeatedly criticized. SHAPIRO: We should also say that while the National Enquirer says what it did was legal, it is not typical journalistic practice to say, we have embarrassing material, and if you don't back down, we will publish it. SELYUKH: Right. SHAPIRO: But how typical is it for the National Enquirer and its publishing - its owner company to do this sort of thing? SELYUKH: It definitely has, as you're saying, a history of practices that are considered journalistically unethical - for example, paying sources for stories, buying embarrassing stories about celebrities in exchange for favors from them, the tactic known as catch-and-kill in which the tabloid buys rights to stories purely to suppress them. SHAPIRO: That was what was alleged in the Trump incident. SELYUKH: We'll get to that in just a minute. SHAPIRO: (Laughter) OK. SELYUKH: That is actually what got the National Enquirer in the federal investigation - involved in the federal investigation with the Trump campaign, as you were saying. American Media and Becker admitted to paying former Playboy model Karen McDougal to buy the rights to her story. She says she had an affair with Trump, and the publisher's plan was to kill the story to protect the Trump campaign during the 2016 election. Then Becker cut an immunity deal, as I mentioned, with the prosecutors. And this is the deal that's now in question because of Bezos's accusations of essentially criminal wrongdoing. SHAPIRO: NPR's Alina Selyukh, thank you very much. SELYUKH: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-08-692762383": {"title": "Are You Frustrated With TV Subscription Services? Tell Us About It : NPR", "url": "https://www.npr.org/2019/02/08/692762383/are-you-frustrated-with-tv-subscription-services-tell-us-about-it", "author": "No author found", "published_date": "2019-02-08", "content": "", "section": "Technology", "disclaimer": ""}, "2019-02-10-692877140": {"title": "Why We Can't Break Up With Big Tech : NPR", "url": "https://www.npr.org/2019/02/10/692877140/why-we-cant-break-up-with-big-tech", "author": "No author found", "published_date": "2019-02-10", "content": "LULU GARCIA-NAVARRO, HOST: Facebook and Google and Amazon have become tech giants because they've made a science of being a part of everything we do, from shopping to taking care of our kids to connecting with friends and relatives. Despite the scandals and concerns about their practices, modern life is seemingly impossible without them - or is it? Kashmir Hill is a reporter for Gizmodo. She tried to cut Amazon, Facebook, Google, Microsoft and Apple out of her life. And she joins us now to explain her experiment. Welcome. KASHMIR HILL: Thank you. GARCIA-NAVARRO: I got to ask you, first of all, how did this idea come up? HILL: It was inspired partly by, you know, when people are criticizing these companies or complaining about how powerful they are, how privacy-invasive they are, people will say, well, if you - if you don't like the company, then just stop using their products. And so I wanted to find out if that was possible. GARCIA-NAVARRO: Right. HILL: And spoiler - spoiler, it's not possible. GARCIA-NAVARRO: (Laughter). It's not possible. Right, OK. HILL: (Laughter). GARCIA-NAVARRO: So you took us to the end. But I want to hear about the process because I think that can be instructive to us all. You wanted to completely remove one tech giant from your life each week. It's sort of like - (laughter) - it's sort of like cutting down your alcohol intake. HILL: (Laughter). GARCIA-NAVARRO: So tell me - tell me how how you got started. HILL: Right. In part, I thought it would be too hard to go cold turkey. So I wanted to do it step by step. And I also wanted to go beyond just boycotting their products. I really wanted to completely stop all my data, all my attention and any of my money from going to the tech giants. So I consulted a technologist named Dhruv Mehrotra. And he did a little research and came back to me and said, you know what? I can build a network tool for you - a virtual, private network, or VPN. And you can connect all your devices to it, send all your network traffic through it. And I will block a tech giant each week - or all of them together. And I'll keep your devices from being able to communicate with their servers. GARCIA-NAVARRO: So tell me about Amazon because that seems like it's going to be really hard to take out of your life. HILL: Yeah. When I started pulling stats about Amazon, I was shocked that, you know, they control 50 percent of online commerce in the U. S. That seems like so much. The crazier thing is that's not their most profitable business. Their most profitable business is AWS, or Amazon Web Services. They host websites. They host apps. And so by blocking Amazon and blocking AWS, I just took out a vast amount of the Internet. And so basically, all digital entertainment was wiped out for me. GARCIA-NAVARRO: Oh. HILL: But this raises real questions about just, like, how much data Amazon is gathering from the fact that it just controls the infrastructure of commerce and Internet activity. GARCIA-NAVARRO: The last week of the experiment, you tried blocking all five companies at the same time. God bless you. So how did that go? HILL: So the hardest thing about blocking all the tech giants that last week was getting a phone. Google and Apple have a duopoly on the smartphone market. And so when I went out trying to find a smartphone that was not, you know, made or touched by either tech giant, it wasn't possible. GARCIA-NAVARRO: So what did you do? You just didn't have a cell phone? HILL: Oh, I had a cell phone. I had a dumb phone. I had a Nokia 3310, little, tiny, bright orange brick with t9 texting. It just had, like, the number buttons. And all it - all it basically did was calling, texting. And it had the Snake game. GARCIA-NAVARRO: You went back to the '90s. HILL: I went back to the '90s. This - this experiment was a time machine. GARCIA-NAVARRO: So now that you've sort of come out of this, what did you learn? And did it have any lasting impact on your relationship with these companies? And what can we take away? HILL: I mean, the big thing I learned is that it's not possible to navigate the modern world without coming into contact with these companies. They are unavoidable. It made me certainly sympathetic to some of the critics who are saying that these companies are too dominant in their spaces. You know, when I went off of Facebook's products, because Facebook bought Instagram, it controls everywhere where my friends are. And so by rejecting Facebook, I had to reject a lot of people in my life. And it was very hard to stay in contact with them. But there were certainly benefits to rejecting the tech giants because it forced me to reject technology completely and in many cases. Like, I couldn't watch TV because we don't have cable, and Internet TV didn't work. And - and I think that was really good for me. I got out of some bad tech habits. And I'm just kind of looking at screens less. So if nothing else, I'm glad I did this experiment in terms of becoming kind of a healthier tech user. GARCIA-NAVARRO: Kashmir Hill is a reporter for Gizmodo. Thank you so much. HILL: My pleasure. GARCIA-NAVARRO: And one note - Amazon, Facebook, Google and Microsoft are financial sponsors of NPR. LULU GARCIA-NAVARRO, HOST:  Facebook and Google and Amazon have become tech giants because they've made a science of being a part of everything we do, from shopping to taking care of our kids to connecting with friends and relatives. Despite the scandals and concerns about their practices, modern life is seemingly impossible without them - or is it? Kashmir Hill is a reporter for Gizmodo. She tried to cut Amazon, Facebook, Google, Microsoft and Apple out of her life. And she joins us now to explain her experiment. Welcome. KASHMIR HILL: Thank you. GARCIA-NAVARRO: I got to ask you, first of all, how did this idea come up? HILL: It was inspired partly by, you know, when people are criticizing these companies or complaining about how powerful they are, how privacy-invasive they are, people will say, well, if you - if you don't like the company, then just stop using their products. And so I wanted to find out if that was possible. GARCIA-NAVARRO: Right. HILL: And spoiler - spoiler, it's not possible. GARCIA-NAVARRO: (Laughter). It's not possible. Right, OK. HILL: (Laughter). GARCIA-NAVARRO: So you took us to the end. But I want to hear about the process because I think that can be instructive to us all. You wanted to completely remove one tech giant from your life each week. It's sort of like - (laughter) - it's sort of like cutting down your alcohol intake. HILL: (Laughter). GARCIA-NAVARRO: So tell me - tell me how how you got started. HILL: Right. In part, I thought it would be too hard to go cold turkey. So I wanted to do it step by step. And I also wanted to go beyond just boycotting their products. I really wanted to completely stop all my data, all my attention and any of my money from going to the tech giants. So I consulted a technologist named Dhruv Mehrotra. And he did a little research and came back to me and said, you know what? I can build a network tool for you - a virtual, private network, or VPN. And you can connect all your devices to it, send all your network traffic through it. And I will block a tech giant each week - or all of them together. And I'll keep your devices from being able to communicate with their servers. GARCIA-NAVARRO: So tell me about Amazon because that seems like it's going to be really hard to take out of your life. HILL: Yeah. When I started pulling stats about Amazon, I was shocked that, you know, they control 50 percent of online commerce in the U. S. That seems like so much. The crazier thing is that's not their most profitable business. Their most profitable business is AWS, or Amazon Web Services. They host websites. They host apps. And so by blocking Amazon and blocking AWS, I just took out a vast amount of the Internet. And so basically, all digital entertainment was wiped out for me. GARCIA-NAVARRO: Oh. HILL: But this raises real questions about just, like, how much data Amazon is gathering from the fact that it just controls the infrastructure of commerce and Internet activity. GARCIA-NAVARRO: The last week of the experiment, you tried blocking all five companies at the same time. God bless you. So how did that go? HILL: So the hardest thing about blocking all the tech giants that last week was getting a phone. Google and Apple have a duopoly on the smartphone market. And so when I went out trying to find a smartphone that was not, you know, made or touched by either tech giant, it wasn't possible. GARCIA-NAVARRO: So what did you do? You just didn't have a cell phone? HILL: Oh, I had a cell phone. I had a dumb phone. I had a Nokia 3310, little, tiny, bright orange brick with t9 texting. It just had, like, the number buttons. And all it - all it basically did was calling, texting. And it had the Snake game. GARCIA-NAVARRO: You went back to the '90s. HILL: I went back to the '90s. This - this experiment was a time machine. GARCIA-NAVARRO: So now that you've sort of come out of this, what did you learn? And did it have any lasting impact on your relationship with these companies? And what can we take away? HILL: I mean, the big thing I learned is that it's not possible to navigate the modern world without coming into contact with these companies. They are unavoidable. It made me certainly sympathetic to some of the critics who are saying that these companies are too dominant in their spaces. You know, when I went off of Facebook's products, because Facebook bought Instagram, it controls everywhere where my friends are. And so by rejecting Facebook, I had to reject a lot of people in my life. And it was very hard to stay in contact with them. But there were certainly benefits to rejecting the tech giants because it forced me to reject technology completely and in many cases. Like, I couldn't watch TV because we don't have cable, and Internet TV didn't work. And - and I think that was really good for me. I got out of some bad tech habits. And I'm just kind of looking at screens less. So if nothing else, I'm glad I did this experiment in terms of becoming kind of a healthier tech user. GARCIA-NAVARRO: Kashmir Hill is a reporter for Gizmodo. Thank you so much. HILL: My pleasure. GARCIA-NAVARRO: And one note - Amazon, Facebook, Google and Microsoft are financial sponsors of NPR.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-11-693758734": {"title": "Apple CEO Tim Cook Optimistic About U.S.-China Trade Talks : NPR", "url": "https://www.npr.org/2019/02/11/693758734/apple-ceo-tim-cook-optimistic-about-u-s-china-trade-talks", "author": "No author found", "published_date": "2019-02-11", "content": "", "section": "Business", "disclaimer": ""}, "2019-02-11-693538900": {"title": "Russia May Unplug From Internet To Test Its Cyberdefenses : NPR", "url": "https://www.npr.org/2019/02/11/693538900/russia-is-considering-an-experiment-to-disconnect-from-the-internet", "author": "No author found", "published_date": "2019-02-11", "content": "", "section": "Technology", "disclaimer": ""}, "2019-02-11-691334123": {"title": "Sweden's Cashless Experiment: Is It Too Much Too Fast? : NPR", "url": "https://www.npr.org/2019/02/11/691334123/swedens-cashless-experiment-is-it-too-much-too-fast", "author": "No author found", "published_date": "2019-02-11", "content": "MARY LOUISE KELLY, HOST:  Cash may be king, but there are more ways to buy and sell and store our money than ever before. We're looking at what's beyond cash in this month's All Tech Considered. (SOUNDBITE OF MUSIC)KELLY: For a glimpse of what the U. S. economy might look like in the future, we head to Scandinavia. No country is dropping cash as fast as Sweden. Just 13 percent of people there reported using cash to buy something last year. But as Maddy Savage reports from Stockholm, some Swedes are concerned that things have changed too quickly. MADDY SAVAGE, BYLINE: Next to the cinnamon buns and open sandwiches at this restaurant, there's a large blue sign with the English word cash crossed out. Notes and coins stopped being accepted here a year ago after staff noticed most people were hardly using them. CHRISTOPHER LOOB: It was super rare, I'd say 5 percent - something like that - in the restaurant was using cash. SAVAGE: That's general manager Christopher Loob, who's been keeping an eye on how customers have responded. LOOB: There's hardly been any reaction. Almost everybody has the alternative payment method - a credit card - and it's good for both the guests and for us. SAVAGE: He says scrapping cash saves time at the till and helps protect against theft. That's been a worry for many Swedish businesses since a string of high-profile robberies in the early 2000s. A strong digital infrastructure and a small tech-savvy population have also encouraged the cashless trend here. But there are growing concerns not everyone is benefiting. CHRISTINA TALLBERG: I'm Christina Tallberg. I'm the precedent of the National Pensioners Organization. I'm 75 years old. In Sweden, there are about 1 million inhabitants out of 10 million who are not familiar to use digital way. SAVAGE: She says the majority of this group are over 65, but it also includes people with certain disabilities and newly arrived refugees. LOOB: You need it in many ways for restaurants, shops, when you are going to park your car, then you should also use an app or a card. So that's making a lot of difficulties. SAVAGE: She says even going to public toilets can pose a problem. They often cost 10 kroner, which must be paid by card. Another concern is that most banks have stopped letting people take out or pay in cash over the counter. Sweden's central bank, the Riksbank, has been pushing local branches to keep notes and coins in circulation. (SOUNDBITE OF KNOCKING)SAVAGE: Hello. BJORN SEGENDOR: Hi. I'm Bjorn Segendor. Welcome to the Riksbank. We would like to see the banks continuing supplying their customers with cash services. SAVAGE: Do you think Sweden thought things were going to go this quickly? SEGENDOR: I think most of us has been taken by surprise by the speed of this development. Most countries are pushing digital technology, and if you are successful, this will have consequences for cash. SAVAGE: The security of digital payments is another issue for the authorities, but Claire Ingram Bogusz, a postdoctoral researcher at Stockholm School of Economics, says that, in general, Swedes have very high levels of trust in banks and institutions. CLAIRE INGRAM BOGUSZ: Ordinary Swedes are not concerned at all. The convenience of having your bank account, your money at your fingertips and increasingly on your smart watch vastly outweighs any concerns that they have about security or about being tracked. SAVAGE: Back at Urban Deli restaurant, the coffees haven't stopped flowing all day. Most of the customers here are in their 20s or 30s, and it is hard to find anyone with major worries about the cashless trend. ALEXANDER BLOMQVIST: Cash is a hassle. For instance, alot use digital taxi apps like Uber. You don't have to hassle with cash. SAVAGE: That's Alexander Blomqvist. Another customer, Fabien Asytn, says he would even support a feature where notes and coins are phased out completely. FABIEN ASYTN: No one uses it anymore, so it's just easier to just get rid of it. SAVAGE: But even in this innovative country, it seems most Swedes are not ready for a completely cashless future. A nationwide poll says 7 in 10 still want cash to stick around for now. For NPR News, I'm Maddy Savage in Stockholm. MARY LOUISE KELLY, HOST:   Cash may be king, but there are more ways to buy and sell and store our money than ever before. We're looking at what's beyond cash in this month's All Tech Considered. (SOUNDBITE OF MUSIC) KELLY: For a glimpse of what the U. S. economy might look like in the future, we head to Scandinavia. No country is dropping cash as fast as Sweden. Just 13 percent of people there reported using cash to buy something last year. But as Maddy Savage reports from Stockholm, some Swedes are concerned that things have changed too quickly. MADDY SAVAGE, BYLINE: Next to the cinnamon buns and open sandwiches at this restaurant, there's a large blue sign with the English word cash crossed out. Notes and coins stopped being accepted here a year ago after staff noticed most people were hardly using them. CHRISTOPHER LOOB: It was super rare, I'd say 5 percent - something like that - in the restaurant was using cash. SAVAGE: That's general manager Christopher Loob, who's been keeping an eye on how customers have responded. LOOB: There's hardly been any reaction. Almost everybody has the alternative payment method - a credit card - and it's good for both the guests and for us. SAVAGE: He says scrapping cash saves time at the till and helps protect against theft. That's been a worry for many Swedish businesses since a string of high-profile robberies in the early 2000s. A strong digital infrastructure and a small tech-savvy population have also encouraged the cashless trend here. But there are growing concerns not everyone is benefiting. CHRISTINA TALLBERG: I'm Christina Tallberg. I'm the precedent of the National Pensioners Organization. I'm 75 years old. In Sweden, there are about 1 million inhabitants out of 10 million who are not familiar to use digital way. SAVAGE: She says the majority of this group are over 65, but it also includes people with certain disabilities and newly arrived refugees. LOOB: You need it in many ways for restaurants, shops, when you are going to park your car, then you should also use an app or a card. So that's making a lot of difficulties. SAVAGE: She says even going to public toilets can pose a problem. They often cost 10 kroner, which must be paid by card. Another concern is that most banks have stopped letting people take out or pay in cash over the counter. Sweden's central bank, the Riksbank, has been pushing local branches to keep notes and coins in circulation. (SOUNDBITE OF KNOCKING) SAVAGE: Hello. BJORN SEGENDOR: Hi. I'm Bjorn Segendor. Welcome to the Riksbank. We would like to see the banks continuing supplying their customers with cash services. SAVAGE: Do you think Sweden thought things were going to go this quickly? SEGENDOR: I think most of us has been taken by surprise by the speed of this development. Most countries are pushing digital technology, and if you are successful, this will have consequences for cash. SAVAGE: The security of digital payments is another issue for the authorities, but Claire Ingram Bogusz, a postdoctoral researcher at Stockholm School of Economics, says that, in general, Swedes have very high levels of trust in banks and institutions. CLAIRE INGRAM BOGUSZ: Ordinary Swedes are not concerned at all. The convenience of having your bank account, your money at your fingertips and increasingly on your smart watch vastly outweighs any concerns that they have about security or about being tracked. SAVAGE: Back at Urban Deli restaurant, the coffees haven't stopped flowing all day. Most of the customers here are in their 20s or 30s, and it is hard to find anyone with major worries about the cashless trend. ALEXANDER BLOMQVIST: Cash is a hassle. For instance, alot use digital taxi apps like Uber. You don't have to hassle with cash. SAVAGE: That's Alexander Blomqvist. Another customer, Fabien Asytn, says he would even support a feature where notes and coins are phased out completely. FABIEN ASYTN: No one uses it anymore, so it's just easier to just get rid of it. SAVAGE: But even in this innovative country, it seems most Swedes are not ready for a completely cashless future. A nationwide poll says 7 in 10 still want cash to stick around for now. For NPR News, I'm Maddy Savage in Stockholm.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-12-693994447": {"title": "Apple, Google In Crosshairs For Carrying App That Lets Saudi Men Track Wives : NPR", "url": "https://www.npr.org/2019/02/12/693994447/apple-google-criticized-for-carrying-app-that-lets-saudi-men-track-their-wives", "author": "No author found", "published_date": "2019-02-12", "content": "", "section": "Technology", "disclaimer": ""}, "2019-02-13-694315163": {"title": "Former Apple Executive Accused Of Insider Trading By SEC : NPR", "url": "https://www.npr.org/2019/02/13/694315163/ex-apple-exec-who-oversaw-insider-trading-policy-profited-on-inside-info-sec-say", "author": "No author found", "published_date": "2019-02-13", "content": "", "section": "Law", "disclaimer": ""}, "2019-02-13-654737444": {"title": "Opportunity Is Dead: NASA Declares End To Mars Rover's Mission : NPR", "url": "https://www.npr.org/2019/02/13/654737444/nasas-mars-rover-opportunity-is-officially-declared-dead", "author": "No author found", "published_date": "2019-02-13", "content": "AUDIE CORNISH, HOST: NASA has officially declared an end to the mission of Mars rover known as Opportunity. It operated for more than 14 years - not bad for a mission that was originally scheduled for 90 days. NPR's Joe Palca has this remembrance. JOE PALCA, BYLINE: On Saturday night, June 24, 2004, engineers, scientists and VIPs jammed into Mission Control at the Jet Propulsion Laboratory in Pasadena filled with hope, but also a kind of dread. Just three weeks earlier, Opportunity's twin rover, Spirit, had landed successfully. And the question was, could NASA engineers really pull off the devilishly difficult feat of landing on Mars twice in a row? Turns out they could. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: Retrorocket ignition on my mark. Mark. At this time the RAD rocket. . . UNIDENTIFIED PERSON #2: RAD has fired. UNIDENTIFIED PERSON #1: We have confirmation. . . PALCA: Retrorockets brought Opportunity to a halt just above the surface, then air bags were to inflate, allowing the golf-cart-sized rover to bounce safely to a landing. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #3: We're getting a bounce signal. PALCA: And with that, NASA was two-for-two. A few hours later, Opportunity sent back pictures of its landing site at Meridiani Planum. (SOUNDBITE OF ARCHIVED RECORDING)STEVEN SQUYRES: I will attempt no science analysis because it looks like nothing I've ever seen before in my life. PALCA: That's mission principal investigator Steven Squyres of Cornell University speaking from the control room to reporters assembled nearby as pictures from the rover streamed in. (SOUNDBITE OF ARCHIVED RECORDING)SQUYRES: We knew going into this that at a fine scale, the texture of Meridiani Planum was unlike almost anything else on Mars. As we had expected - holy smokes. UNIDENTIFIED PEOPLE: (Laughter). SQUYRES: I'm sorry. I'm just blown away by this. PALCA: Looking back, Squyres says Opportunity wasn't just a technical triumph. He says there were numerous scientific discoveries. SQUYRES: OK, I'll give you two. The first was right at the beginning at the landing site. PALCA: Squyres said Opportunity found evidence that briny water had once sloshed around on what is now a very dry planet. SQUYRES: The other thing was - then, years and years later, we got to the rim of a very ancient crater, which is where Opportunity is now. PALCA: There, Squyres says the rover found evidence of what's called hydrothermal events, where hot water percolates through rocks, changing their mineral content. But as important as Opportunity was for science, it was also important for the future of science. Squyres has given lots of talks about the mission. And he says, often, a young scientist or engineer will come up to him afterwards and say. . . SQUYRES: You know, when I was 8 years old, I saw you guys land on Mars, and it made me decide I wanted to do this. And that's happened to me not once or twice. It's happened to me a bunch of times. PALCA: Do this, meaning become a scientist. That's certainly what happened to Keri Bean. She was in high school when she saw a documentary about Opportunity called \"Roving Mars. \"KERI BEAN: I especially remember them showing the landing footage. And when they got the confirmation the spacecraft landed, they were all cheering. They were so excited. And I was really drawn to the idea of exploring and being so interested and caring about something that much. PALCA: Bean went to graduate school, where she worked on the rover as a student and ultimately landed a job at JPL, where she joined the Opportunity mission team. Last night, she helped send the last radio signals trying to wake up the rover. BEAN: Me, personally, it's been really hard because this is a project I've worked on for over a third of my life at this point. And so just to lose that all of a sudden is really tough. But at least it was Mars that killed her. It wasn't the rover failing or something else. It was Mars. And I feel like that's really the only appropriate death for her at this point. PALCA: It was a good run, but it was going to end someday. And today's the day. Joe Palca, NPR News. (SOUNDBITE OF MIEUX'S \"RUST\") AUDIE CORNISH, HOST:  NASA has officially declared an end to the mission of Mars rover known as Opportunity. It operated for more than 14 years - not bad for a mission that was originally scheduled for 90 days. NPR's Joe Palca has this remembrance. JOE PALCA, BYLINE: On Saturday night, June 24, 2004, engineers, scientists and VIPs jammed into Mission Control at the Jet Propulsion Laboratory in Pasadena filled with hope, but also a kind of dread. Just three weeks earlier, Opportunity's twin rover, Spirit, had landed successfully. And the question was, could NASA engineers really pull off the devilishly difficult feat of landing on Mars twice in a row? Turns out they could. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: Retrorocket ignition on my mark. Mark. At this time the RAD rocket. . . UNIDENTIFIED PERSON #2: RAD has fired. UNIDENTIFIED PERSON #1: We have confirmation. . . PALCA: Retrorockets brought Opportunity to a halt just above the surface, then air bags were to inflate, allowing the golf-cart-sized rover to bounce safely to a landing. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #3: We're getting a bounce signal. PALCA: And with that, NASA was two-for-two. A few hours later, Opportunity sent back pictures of its landing site at Meridiani Planum. (SOUNDBITE OF ARCHIVED RECORDING) STEVEN SQUYRES: I will attempt no science analysis because it looks like nothing I've ever seen before in my life. PALCA: That's mission principal investigator Steven Squyres of Cornell University speaking from the control room to reporters assembled nearby as pictures from the rover streamed in. (SOUNDBITE OF ARCHIVED RECORDING) SQUYRES: We knew going into this that at a fine scale, the texture of Meridiani Planum was unlike almost anything else on Mars. As we had expected - holy smokes. UNIDENTIFIED PEOPLE: (Laughter). SQUYRES: I'm sorry. I'm just blown away by this. PALCA: Looking back, Squyres says Opportunity wasn't just a technical triumph. He says there were numerous scientific discoveries. SQUYRES: OK, I'll give you two. The first was right at the beginning at the landing site. PALCA: Squyres said Opportunity found evidence that briny water had once sloshed around on what is now a very dry planet. SQUYRES: The other thing was - then, years and years later, we got to the rim of a very ancient crater, which is where Opportunity is now. PALCA: There, Squyres says the rover found evidence of what's called hydrothermal events, where hot water percolates through rocks, changing their mineral content. But as important as Opportunity was for science, it was also important for the future of science. Squyres has given lots of talks about the mission. And he says, often, a young scientist or engineer will come up to him afterwards and say. . . SQUYRES: You know, when I was 8 years old, I saw you guys land on Mars, and it made me decide I wanted to do this. And that's happened to me not once or twice. It's happened to me a bunch of times. PALCA: Do this, meaning become a scientist. That's certainly what happened to Keri Bean. She was in high school when she saw a documentary about Opportunity called \"Roving Mars. \" KERI BEAN: I especially remember them showing the landing footage. And when they got the confirmation the spacecraft landed, they were all cheering. They were so excited. And I was really drawn to the idea of exploring and being so interested and caring about something that much. PALCA: Bean went to graduate school, where she worked on the rover as a student and ultimately landed a job at JPL, where she joined the Opportunity mission team. Last night, she helped send the last radio signals trying to wake up the rover. BEAN: Me, personally, it's been really hard because this is a project I've worked on for over a third of my life at this point. And so just to lose that all of a sudden is really tough. But at least it was Mars that killed her. It wasn't the rover failing or something else. It was Mars. And I feel like that's really the only appropriate death for her at this point. PALCA: It was a good run, but it was going to end someday. And today's the day. Joe Palca, NPR News. (SOUNDBITE OF MIEUX'S \"RUST\")", "section": "Joe's Big Idea", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-13-693679858": {"title": "Harley-Davidson Launches LiveWire Electric Motorcycle. What About The Sound?  : NPR", "url": "https://www.npr.org/2019/02/13/693679858/harley-davidson-embraces-a-new-sound-as-it-enters-the-electric-era", "author": "No author found", "published_date": "2019-02-13", "content": "MARY LOUISE KELLY, HOST: An American classic, the Harley-Davidson motorcycle, is getting an eco-friendly makeover. The first electric Harley is hitting the roads after years of development. But will riders go hog-wild for it? NPR's Camila Domonoske reports. (SOUNDBITE OF MOTORCYCLE IDLING)CAMILA DOMONOSKE, BYLINE: A Harley-Davidson motorcycle has a familiar sound. (SOUNDBITE OF MOTORCYCLE REVVING)DOMONOSKE: But the LiveWire, Harley's upcoming electric offering, sounds different. (SOUNDBITE OF ELECTRIC MOTORCYCLE REVVING)DOMONOSKE: Sound is essential on a motorcycle, and it's not just for the mystique. It's for safety so you can hear riders coming. But electric motors can be nearly silent, a problem Harley had to solve. Marc McAllister of Harley-Davidson wants you to know the LiveWire's sound is real. MARC MCALLISTER: The sound is a mechanical sound that comes from the operation of the vehicle, and it sounds like - when you're at highway speeds, it almost sounds like a jet engine. DOMONOSKE: Many companies believe electric is the future. As people pay more attention to climate change, Harley sees an opportunity especially with younger riders and in urban centers in Asian markets. A number of startups are already making electric motorcycles, but those companies don't have Harley's historic reputation to consider. McAllister says an electric Harley-Davidson is not a contradiction in terms. MCALLISTER: Because Harley-Davidson - after 115 years, we've had to reinvent ourselves a number of times, and this is just the next step. DOMONOSKE: But some people are skeptical. Kelley O'Brien is the marketing manager of a Washington, D. C. , Harley dealership. She was working at the D. C. motorcycle show last weekend. KELLEY O'BRIEN: You know, that demographic that have been a Harley rider for 30 years and they're like - they don't like it. They don't like the sound. It's not the same sound. DOMONOSKE: Still, she says, the sound is unique, and the bike has gotten a lot of buzz. Preorders opened a few weeks ago. DAVID LUTZOW: I would love to sit on one, here one up close. DOMONOSKE: David Lutzow was a 59-year-old Air Force veteran and a proud Harley rider. He's heard the LiveWire is fast, and for the sake of the planet, he's glad Harley is making an electric bike. LUTZOW: I mean, ultimately for the ecosystem and stuff, I think it's going to be good. A lot of things are going electric. DOMONOSKE: With that said. . . LUTZOW: I'm sorry. I'll still take my big tour bike. If they can make a big tour bike electric, then, you know, maybe I'll think about it. DOMONOSKE: The LiveWire is not a big tour bike. It's a smaller street bike, more sporty. LUTZOW: I think it will attract what - I guess the younger people. They call them - what? - millennials or whatever. But I don't know - not - maybe not initially old guys like me. DOMONOSKE: So how about those millennials? Andrew Delgado is 28. He doesn't own a bike, but he'd like to. He's admiring the Harleys on display at the show. LUTZOW: I've just wanted one since I was a kid - freedom and being able to carve up a road. DOMONOSKE: He's exactly the target audience Harley would like to reach as they look to expand their ridership. And he thinks an electric motorcycle is cool, but then he finds out it costs $30,000. ANDREW DELGADO: That's not cool (laughter). I do like the idea of it but just not the price of it. DOMONOSKE: He'd be looking to spend less than $10,000, and there are plenty of options at that price. Thirty-thousand is a lot. Jaime Katz is an analyst with Morningstar. JAIME KATZ: The overall target market for $30,000 dollar bikes, whether it's electric or, you know, traditional, is not significant. DOMONOSKE: And it's especially out of reach for younger people. As a result, she's not predicting huge sales numbers. KATZ: We don't think this is going to really sway the profit potential for the business in any significant way. DOMONOSKE: Not yet, anyway, although future, cheaper electric bikes might be a different story. Camila Domonoske, NPR News. MARY LOUISE KELLY, HOST:  An American classic, the Harley-Davidson motorcycle, is getting an eco-friendly makeover. The first electric Harley is hitting the roads after years of development. But will riders go hog-wild for it? NPR's Camila Domonoske reports. (SOUNDBITE OF MOTORCYCLE IDLING) CAMILA DOMONOSKE, BYLINE: A Harley-Davidson motorcycle has a familiar sound. (SOUNDBITE OF MOTORCYCLE REVVING) DOMONOSKE: But the LiveWire, Harley's upcoming electric offering, sounds different. (SOUNDBITE OF ELECTRIC MOTORCYCLE REVVING) DOMONOSKE: Sound is essential on a motorcycle, and it's not just for the mystique. It's for safety so you can hear riders coming. But electric motors can be nearly silent, a problem Harley had to solve. Marc McAllister of Harley-Davidson wants you to know the LiveWire's sound is real. MARC MCALLISTER: The sound is a mechanical sound that comes from the operation of the vehicle, and it sounds like - when you're at highway speeds, it almost sounds like a jet engine. DOMONOSKE: Many companies believe electric is the future. As people pay more attention to climate change, Harley sees an opportunity especially with younger riders and in urban centers in Asian markets. A number of startups are already making electric motorcycles, but those companies don't have Harley's historic reputation to consider. McAllister says an electric Harley-Davidson is not a contradiction in terms. MCALLISTER: Because Harley-Davidson - after 115 years, we've had to reinvent ourselves a number of times, and this is just the next step. DOMONOSKE: But some people are skeptical. Kelley O'Brien is the marketing manager of a Washington, D. C. , Harley dealership. She was working at the D. C. motorcycle show last weekend. KELLEY O'BRIEN: You know, that demographic that have been a Harley rider for 30 years and they're like - they don't like it. They don't like the sound. It's not the same sound. DOMONOSKE: Still, she says, the sound is unique, and the bike has gotten a lot of buzz. Preorders opened a few weeks ago. DAVID LUTZOW: I would love to sit on one, here one up close. DOMONOSKE: David Lutzow was a 59-year-old Air Force veteran and a proud Harley rider. He's heard the LiveWire is fast, and for the sake of the planet, he's glad Harley is making an electric bike. LUTZOW: I mean, ultimately for the ecosystem and stuff, I think it's going to be good. A lot of things are going electric. DOMONOSKE: With that said. . . LUTZOW: I'm sorry. I'll still take my big tour bike. If they can make a big tour bike electric, then, you know, maybe I'll think about it. DOMONOSKE: The LiveWire is not a big tour bike. It's a smaller street bike, more sporty. LUTZOW: I think it will attract what - I guess the younger people. They call them - what? - millennials or whatever. But I don't know - not - maybe not initially old guys like me. DOMONOSKE: So how about those millennials? Andrew Delgado is 28. He doesn't own a bike, but he'd like to. He's admiring the Harleys on display at the show. LUTZOW: I've just wanted one since I was a kid - freedom and being able to carve up a road. DOMONOSKE: He's exactly the target audience Harley would like to reach as they look to expand their ridership. And he thinks an electric motorcycle is cool, but then he finds out it costs $30,000. ANDREW DELGADO: That's not cool (laughter). I do like the idea of it but just not the price of it. DOMONOSKE: He'd be looking to spend less than $10,000, and there are plenty of options at that price. Thirty-thousand is a lot. Jaime Katz is an analyst with Morningstar. JAIME KATZ: The overall target market for $30,000 dollar bikes, whether it's electric or, you know, traditional, is not significant. DOMONOSKE: And it's especially out of reach for younger people. As a result, she's not predicting huge sales numbers. KATZ: We don't think this is going to really sway the profit potential for the business in any significant way. DOMONOSKE: Not yet, anyway, although future, cheaper electric bikes might be a different story. Camila Domonoske, NPR News.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-13-694171341": {"title": "Americans Lost $143 Million In Online Relationship Scams Last Year : NPR", "url": "https://www.npr.org/2019/02/13/694171341/americans-lost-143-million-in-online-relationship-scams-last-year", "author": "No author found", "published_date": "2019-02-13", "content": "", "section": "Your Money", "disclaimer": ""}, "2019-02-14-694641578": {"title": "3D-Printed Gun Maker Sentenced To 8 Years In Prison : NPR", "url": "https://www.npr.org/2019/02/14/694641578/texas-man-with-3d-printed-gun-and-hit-list-of-lawmakers-sentenced-to-8-years", "author": "No author found", "published_date": "2019-02-14", "content": "", "section": "National", "disclaimer": ""}, "2019-02-14-693566073": {"title": "Sharing Netflix, Spotify Accounts After Couples Break Up : NPR", "url": "https://www.npr.org/2019/02/14/693566073/when-your-shared-netflix-account-outlasts-the-relationship", "author": "No author found", "published_date": "2019-02-14", "content": "STEVE INSKEEP, HOST:  Now that we're past Valentine's Day, we can tell this story about sharing. You know, people try to share things when they love each other, including passwords for their video and music streaming services. But what happens when they break up? NPR's Yuki Noguchi finds out. YUKI NOGUCHI, BYLINE: A couple years ago, 20-year-old graphic design student Aleta Dignard-Fung got dumped by her boyfriend. ALETA DIGNARD-FUNG: It was a pretty bad breakup. NOGUCHI: Only later did she remember he still had the password to her streaming music account. DIGNARD-FUNG: You know, part of, like, getting over someone is being able to listen to your jams in the shower and maybe cry or something like that. (SOUNDBITE OF SONG, \"SORRY\")JUSTIN BIEBER: (Singing) Is it too late now to say sorry? 'Cause I'm. . . DIGNARD-FUNG: I remember I'd just, like, blast my music in the shower, and then it'd change. And it'd start playing Bulgarian folk music because he's Bulgarian. (SOUNDBITE OF MUSIC)UNIDENTIFIED SINGER: (Singing in Bulgarian). NOGUCHI: They continued battling for control, pushing each other off the shared account. DIGNARD-FUNG: So I'd hop out of the shower all mad. And I'd switch the song, and then I'd get back in the shower. And then it'd switch again. And I'd hop out of the shower again. And, like, it was just kind of like the Spotify wars. And we'd just spend, like, 10 minutes trying to override each other's songs. NOGUCHI: Breaking up is hard to do and harder for some who share streaming music and video profiles. The expression Netflix and chill isn't just code for date night. It speaks volumes about how closely relationships entwine with digital life. But on the backside of a breakup, unwinding these entanglements can get messy. Some spurned lovers exact revenge by changing the password just as their ex reaches the climactic season finale. Either way, it's not just love that's lost. It's also playlists, movie recommendations and passwords. Susan Winter is a relationship coach and author. She says accessing those accounts after a split can trigger sorrow and longing. And getting blocked by an ex has the ring of permanent goodbye. SUSAN WINTER: Those are the last little pieces to crumble that signify, oh, we really aren't connected anymore. NOGUCHI: Unless, of course, you remain connected but don't realize it and your ex lurks around incognito. WINTER: I had a client who was trying desperately to get his ex back. And they shared an account on OpenTable. And even though they were separated, she never changed it. So he would track where she went to see if she was on a date. NOGUCHI: And sometimes, joint accounts on Netflix or Spotify far outlast the actual relationship. Brenna Kutch is a 34-year-old human resource manager in Portland, Ore. She says merging digital accounts is a sign of commitment, like wearing a boyfriend's sweater. BRENNA KUTCH: We don't necessarily, like, get married and have kids at 21 anymore. But we do combine all of our accounts and share passwords. NOGUCHI: A couple years ago, Kutch shared her ex-boyfriend's Netflix password with her now-spouse. She acknowledges it was odd having her new love piggybacking off her ex. But Kutch says breaking up with the old account wasn't easy. KUTCH: Because I was too lazy to, like, go through, make a spreadsheet, figure out, like, what shows I was watching and which season and episode I was on for each one. NOGUCHI: So she lingered for months. KUTCH: And you can't extract your profile. That's got to be a feature that they're coming up with at some point. NOGUCHI: Netflix says it hasn't. Jill Hill is executive vice president at Magid, a media and entertainment strategy firm. She says more than half of young adults use someone else's streaming video accounts, even people they haven't dated or don't really know. JILL HILL: This is like the story of the password sharing from the neighbor who got it from the neighbor who gave it to the neighbor. NOGUCHI: This is familiar territory for Charlotte Russell. Russell works as a barista in Philadelphia and has access to Hulu, Netflix and Spotify all courtesy of other people. One ex-boyfriend let her set up her own profile on his Spotify family plan. CHARLOTTE RUSSELL: And I'm still on it. And that was, like, two years ago. NOGUCHI: She even shared it with another man she dated. Russell subsequently dated a woman in New York. They broke up last year, but Russell remains logged into her Netflix and Hulu accounts. RUSSELL: I think about it sometimes. Like, when is this going to end? NOGUCHI: Russell sheepishly admits she's saving 30 dollars a month using various borrowed accounts from people she's no longer dating. And it leads to some awkward moments. RUSSELL: Last time I, like, couldn't log into it, I texted her. And I was like, hey. Like, what's the Hulu password? And she replied with, we're not dating anymore (laughter). But she, like, gave it to me immediately after saying that. So I don't know. NOGUCHI: Russell says in a strange way, it enables her to maintain connections with the people who've passed through her life. And besides, now yet another friend is using her ex's Hulu password. Yuki Noguchi, NPR News, Washington. STEVE INSKEEP, HOST:   Now that we're past Valentine's Day, we can tell this story about sharing. You know, people try to share things when they love each other, including passwords for their video and music streaming services. But what happens when they break up? NPR's Yuki Noguchi finds out. YUKI NOGUCHI, BYLINE: A couple years ago, 20-year-old graphic design student Aleta Dignard-Fung got dumped by her boyfriend. ALETA DIGNARD-FUNG: It was a pretty bad breakup. NOGUCHI: Only later did she remember he still had the password to her streaming music account. DIGNARD-FUNG: You know, part of, like, getting over someone is being able to listen to your jams in the shower and maybe cry or something like that. (SOUNDBITE OF SONG, \"SORRY\") JUSTIN BIEBER: (Singing) Is it too late now to say sorry? 'Cause I'm. . . DIGNARD-FUNG: I remember I'd just, like, blast my music in the shower, and then it'd change. And it'd start playing Bulgarian folk music because he's Bulgarian. (SOUNDBITE OF MUSIC) UNIDENTIFIED SINGER: (Singing in Bulgarian). NOGUCHI: They continued battling for control, pushing each other off the shared account. DIGNARD-FUNG: So I'd hop out of the shower all mad. And I'd switch the song, and then I'd get back in the shower. And then it'd switch again. And I'd hop out of the shower again. And, like, it was just kind of like the Spotify wars. And we'd just spend, like, 10 minutes trying to override each other's songs. NOGUCHI: Breaking up is hard to do and harder for some who share streaming music and video profiles. The expression Netflix and chill isn't just code for date night. It speaks volumes about how closely relationships entwine with digital life. But on the backside of a breakup, unwinding these entanglements can get messy. Some spurned lovers exact revenge by changing the password just as their ex reaches the climactic season finale. Either way, it's not just love that's lost. It's also playlists, movie recommendations and passwords. Susan Winter is a relationship coach and author. She says accessing those accounts after a split can trigger sorrow and longing. And getting blocked by an ex has the ring of permanent goodbye. SUSAN WINTER: Those are the last little pieces to crumble that signify, oh, we really aren't connected anymore. NOGUCHI: Unless, of course, you remain connected but don't realize it and your ex lurks around incognito. WINTER: I had a client who was trying desperately to get his ex back. And they shared an account on OpenTable. And even though they were separated, she never changed it. So he would track where she went to see if she was on a date. NOGUCHI: And sometimes, joint accounts on Netflix or Spotify far outlast the actual relationship. Brenna Kutch is a 34-year-old human resource manager in Portland, Ore. She says merging digital accounts is a sign of commitment, like wearing a boyfriend's sweater. BRENNA KUTCH: We don't necessarily, like, get married and have kids at 21 anymore. But we do combine all of our accounts and share passwords. NOGUCHI: A couple years ago, Kutch shared her ex-boyfriend's Netflix password with her now-spouse. She acknowledges it was odd having her new love piggybacking off her ex. But Kutch says breaking up with the old account wasn't easy. KUTCH: Because I was too lazy to, like, go through, make a spreadsheet, figure out, like, what shows I was watching and which season and episode I was on for each one. NOGUCHI: So she lingered for months. KUTCH: And you can't extract your profile. That's got to be a feature that they're coming up with at some point. NOGUCHI: Netflix says it hasn't. Jill Hill is executive vice president at Magid, a media and entertainment strategy firm. She says more than half of young adults use someone else's streaming video accounts, even people they haven't dated or don't really know. JILL HILL: This is like the story of the password sharing from the neighbor who got it from the neighbor who gave it to the neighbor. NOGUCHI: This is familiar territory for Charlotte Russell. Russell works as a barista in Philadelphia and has access to Hulu, Netflix and Spotify all courtesy of other people. One ex-boyfriend let her set up her own profile on his Spotify family plan. CHARLOTTE RUSSELL: And I'm still on it. And that was, like, two years ago. NOGUCHI: She even shared it with another man she dated. Russell subsequently dated a woman in New York. They broke up last year, but Russell remains logged into her Netflix and Hulu accounts. RUSSELL: I think about it sometimes. Like, when is this going to end? NOGUCHI: Russell sheepishly admits she's saving 30 dollars a month using various borrowed accounts from people she's no longer dating. And it leads to some awkward moments. RUSSELL: Last time I, like, couldn't log into it, I texted her. And I was like, hey. Like, what's the Hulu password? And she replied with, we're not dating anymore (laughter). But she, like, gave it to me immediately after saying that. So I don't know. NOGUCHI: Russell says in a strange way, it enables her to maintain connections with the people who've passed through her life. And besides, now yet another friend is using her ex's Hulu password. Yuki Noguchi, NPR News, Washington.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-15-695156096": {"title": "San Jose Told Amazon And Google: No Incentives For You : NPR", "url": "https://www.npr.org/2019/02/15/695156096/this-city-told-amazon-and-google-no-incentives-for-you", "author": "No author found", "published_date": "2019-02-15", "content": "LULU GARCIA-NAVARRO, HOST: It was rolled out like a rose ceremony. Who would be chosen to marry Amazon? Every city tried to make themselves the most attractive. As we know, it didn't quite work out as planned. Activists and local politicians in New York were not happy about offering big tax breaks to Amazon, and Amazon has pulled out of that deal. As big companies continue to expand, one city in California says it's trying to do things differently. They're asking not what we can do for big tech, but what can tech do for us? NPR's Jasmine Garsd reports from San Jose. JASMINE GARSD, BYLINE: A few years ago, Joseph Chavez's (ph) parents had to do like so many people in San Jose - leave. JOSEPH CHAVEZ: The prices are so outrageous, we've got to move to the Central Valley to actually have affordable homes. GARSD: Joseph stayed behind, but he says that like in most of the Bay Area, the cost of living in San Jose has skyrocketed, although it's still not as bad as San Francisco. People who live in San Jose tend to commute to other cities for work. Here's Mayor Sam Liccardo. SAM LICCARDO: We're the only major city in the United States that actually has a smaller daytime population than nighttime population. And as a result, our residents spend a lot of time commuting. We're right up there with the worst cities in commutes. And, obviously, it drives up the housing costs. And so we get the worst of both worlds. GARSD: Mayor Liccardo would like that to change. In 2017, when Amazon started looking for a city to house its second headquarters, San Jose threw its hat in the ring. Around the country, it became a circus. Cities did everything they could to lure Amazon. Many offered juicy incentives. Chicago even got William Shatner to narrate a proposal. But San Jose made a point of offering no subsidies. LICCARDO: If you're offering incentives, those are dollars you could use to be building out transit, to be helping supporting an ecosystem of talent development. GARSD: This is exactly what many activists argued when Amazon ultimately picked New York for one of its second headquarters. Why does one of the wealthiest companies in the world get these tax breaks and incentives? It caused such a local outrage, last week, Amazon announced it was pulling out. LICCARDO: The lesson for cities really ought to be don't take the bait. And don't even offer the bait. GARSD: Liccardo has also been talking to another tech giant, Google, which is headquartered in a town nearby. It wanted to build a campus in San Jose. Again, Liccardo says he offered no incentives. Rather, he says his government asked that 25 percent of the housing built around the campus be affordable. He says this will bring tens of thousands of jobs to the area. LICCARDO: And they've also agreed that we can impose a fee on development in that area and throughout the downtown that will generate dollars we need for affordable housing. GARSD: Google is planning to develop as much as 8 million square feet in downtown San Jose. Walking through the area, it lacks the allure of San Francisco or neighboring Palo Alto. It's not hard to see why some are so excited to develop it. But, here in the Bay Area, that word, development, touches a raw nerve. Will development actually mean affordable housing? And all those shiny tech jobs that keep getting promised - how many actually will go to the locals? Jeff Buchanan is a policy director at Working Partnerships USA, a community labor coalition. JEFF BUCHANAN: So when you look at Google's workforce, only about 7 percent are either Latino or African-American. You look at the population of San Jose, and it just looks incredibly different than who it is that Google's actually hiring. Even if you're a student that graduates from San Jose State University, it ranks nowhere in the top 10 of the universities where Google recruits from. And so I think when we've gone around and talked with people in the community, they don't think their kids are going to be able to work on the Google campus. GARSD: Buchanan says, don't be fooled. The land that San Jose sold to Google could've been used for public works to serve the community. BUCHANAN: Maybe we're not offering billions in tax rebates, but we're offering really valuable public land in an area where land prices are going through the roof. GARSD: Affordable housing is on everyone's mind out here. But Joseph Chavez, the man whose family had to leave San Jose because they just couldn't afford it anymore, says he's hopeful about all those new jobs Google will bring in. Chavez is in construction. He says he's worked on other Google buildings in the past. Maybe he'll get to work on the San Jose site. CHAVEZ: More jobs means more opportunities. More opportunity means everybody gets to eat. If it pays well, you might be able to make it. GARSD: Whether or not he'll be able to make it here after Google gets built - that is an open question. Jasmine Garsd, NPR News, San Jose, Calif. LULU GARCIA-NAVARRO, HOST:  It was rolled out like a rose ceremony. Who would be chosen to marry Amazon? Every city tried to make themselves the most attractive. As we know, it didn't quite work out as planned. Activists and local politicians in New York were not happy about offering big tax breaks to Amazon, and Amazon has pulled out of that deal. As big companies continue to expand, one city in California says it's trying to do things differently. They're asking not what we can do for big tech, but what can tech do for us? NPR's Jasmine Garsd reports from San Jose. JASMINE GARSD, BYLINE: A few years ago, Joseph Chavez's (ph) parents had to do like so many people in San Jose - leave. JOSEPH CHAVEZ: The prices are so outrageous, we've got to move to the Central Valley to actually have affordable homes. GARSD: Joseph stayed behind, but he says that like in most of the Bay Area, the cost of living in San Jose has skyrocketed, although it's still not as bad as San Francisco. People who live in San Jose tend to commute to other cities for work. Here's Mayor Sam Liccardo. SAM LICCARDO: We're the only major city in the United States that actually has a smaller daytime population than nighttime population. And as a result, our residents spend a lot of time commuting. We're right up there with the worst cities in commutes. And, obviously, it drives up the housing costs. And so we get the worst of both worlds. GARSD: Mayor Liccardo would like that to change. In 2017, when Amazon started looking for a city to house its second headquarters, San Jose threw its hat in the ring. Around the country, it became a circus. Cities did everything they could to lure Amazon. Many offered juicy incentives. Chicago even got William Shatner to narrate a proposal. But San Jose made a point of offering no subsidies. LICCARDO: If you're offering incentives, those are dollars you could use to be building out transit, to be helping supporting an ecosystem of talent development. GARSD: This is exactly what many activists argued when Amazon ultimately picked New York for one of its second headquarters. Why does one of the wealthiest companies in the world get these tax breaks and incentives? It caused such a local outrage, last week, Amazon announced it was pulling out. LICCARDO: The lesson for cities really ought to be don't take the bait. And don't even offer the bait. GARSD: Liccardo has also been talking to another tech giant, Google, which is headquartered in a town nearby. It wanted to build a campus in San Jose. Again, Liccardo says he offered no incentives. Rather, he says his government asked that 25 percent of the housing built around the campus be affordable. He says this will bring tens of thousands of jobs to the area. LICCARDO: And they've also agreed that we can impose a fee on development in that area and throughout the downtown that will generate dollars we need for affordable housing. GARSD: Google is planning to develop as much as 8 million square feet in downtown San Jose. Walking through the area, it lacks the allure of San Francisco or neighboring Palo Alto. It's not hard to see why some are so excited to develop it. But, here in the Bay Area, that word, development, touches a raw nerve. Will development actually mean affordable housing? And all those shiny tech jobs that keep getting promised - how many actually will go to the locals? Jeff Buchanan is a policy director at Working Partnerships USA, a community labor coalition. JEFF BUCHANAN: So when you look at Google's workforce, only about 7 percent are either Latino or African-American. You look at the population of San Jose, and it just looks incredibly different than who it is that Google's actually hiring. Even if you're a student that graduates from San Jose State University, it ranks nowhere in the top 10 of the universities where Google recruits from. And so I think when we've gone around and talked with people in the community, they don't think their kids are going to be able to work on the Google campus. GARSD: Buchanan says, don't be fooled. The land that San Jose sold to Google could've been used for public works to serve the community. BUCHANAN: Maybe we're not offering billions in tax rebates, but we're offering really valuable public land in an area where land prices are going through the roof. GARSD: Affordable housing is on everyone's mind out here. But Joseph Chavez, the man whose family had to leave San Jose because they just couldn't afford it anymore, says he's hopeful about all those new jobs Google will bring in. Chavez is in construction. He says he's worked on other Google buildings in the past. Maybe he'll get to work on the San Jose site. CHAVEZ: More jobs means more opportunities. More opportunity means everybody gets to eat. If it pays well, you might be able to make it. GARSD: Whether or not he'll be able to make it here after Google gets built - that is an open question. Jasmine Garsd, NPR News, San Jose, Calif.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-15-695125573": {"title": "Is Venmo Changing Your Money Habits? Tell Us About It : NPR", "url": "https://www.npr.org/2019/02/15/695125573/is-venmo-changing-your-money-habits-tell-us-about-it", "author": "No author found", "published_date": "2019-02-15", "content": "", "section": "Business", "disclaimer": ""}, "2019-02-15-694292327": {"title": "Andreas Ekstr\u00f6m: Can We Solve For Bias In Tech? : NPR", "url": "https://www.npr.org/2019/02/15/694292327/andreas-ekstr-m-can-we-solve-for-bias-in-tech", "author": "No author found", "published_date": "2019-02-15", "content": "GUY RAZ, HOST: On the show today, ideas about the biases we carry and the ways we try to address them. And like many other problems, one way to get around our biases could be with technology because a lot of us assume that technology is always objective. ANDREAS EKSTROM: Well, we don't think that a machine has an opinion, and we forget that there's a programmer behind every machine that has told it how to prioritize. So that means that, you know, if you want to really run all the way with that ball, you can say that there's not a single search query that you can make that is unbiased because it's all an effect of what a real person has decided that the algorithm should do. RAZ: This is Andreas Ekstrom. EKSTROM: Yeah, I am a Swedish reporter and author turned speaker-educator. I try to understand a little bit about what's happening with society through the digital revolution. RAZ: Is it possible for anything or anyone or any search result to be totally unbiased, to be completely objective? I mean, is objectivity even possible? EKSTROM: Yes, for undisputed scientific facts. What is the capital of Sweden? It's Stockholm. There - that's undisputed. You can Google that, and I don't think you'll find a single website that will tell you differently. So yes, there are, but they're very singular, very isolated facts like that. You want to try to Google an answer to the question, why is there an armed conflict between Israel and Palestine? That's not a great thing to Google because that takes a lot of knowledge and historical context to even begin to understand. And sometimes we tend to mix these things up. What we're trying now, large scale, really, is we're trying to see, can we replace human judgment with an algorithm? We can gather the facts, sure. But can we gather knowledge the way we gather facts? Absolutely not. That's two completely different things. RAZ: Andreas Ekstrom picks up this idea from the TED stage. (SOUNDBITE OF TEDx TALK)EKSTROM: And to get to knowledge, you have to bring 10 or 20 or a hundred facts to the table and acknowledge them and say, yes, these are all true. But because of who I am - young or old, or black or white, or gay or straight - I will value them differently. And I will say, yes, this is true, but this is more important to me than that. And this is where it becomes interesting because this is where we become human. This is when we start to argue, to form society. And to really get somewhere, we need to filter all our facts here - through friends and neighbors and parents and children and coworkers and newspapers and magazines - to finally be grounded in real knowledge, which is something that a search engine is a poor help to achieve. RAZ: When we come back, Andreas Ekstrom explains how Google search results can be manipulated. On the show today, ideas about bias and perception. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC)RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about bias and perception. And before the break, we were hearing from the writer Andreas Ekstrom about the inherent bias in search engines and how they can't always tell the difference between what's true and what's popular. EKSTROM: A lot of the algorithm is based upon popularity. So if an answer to a question is really popular, then Google tends to think that it's also correct and relevant. And why? Well, because those people are super active and link to each other and update often, and all those things, you know, is credibility in the Google universe. RAZ: Andreas Ekstrom picks up his idea from the TED stage with a Google search. (SOUNDBITE OF ARCHIVED RECORDING)EKSTROM: We'll start by Michelle Obama, First Lady of the United States. And we'll click for pictures, say - perfect search result, more or less. It's just her in the picture, not even the president. How does this work? They look at two things more than anything. First, what does it say in the caption? What does it say under the picture on each website? Does it say Michelle Obama under the picture? Pretty good indication it's actually her on there. Second, Google looks at the picture file, the name of the file as such uploaded to the website. Again, is it called michelleobama. jpeg? Pretty good indication it's not Clint Eastwood in the picture. So you got those two, and you get a search result like this - almost. Now, in 2009, Michelle Obama was the victim of a racist campaign where people set out to insult her through her search results. There was a picture distributed widely over the Internet where her face was distorted to look like a monkey. And that picture was published all over, and people published it very, very purposefully to get it up here in the search result. They made sure to write Michelle Obama in the caption, and they made sure to upload the picture as michelleobama. jpeg or the like. So you get why - to manipulate the search result. And it worked too. So when you picture-Googled for Michelle Obama in 2009, that distorted monkey picture showed up among the first results. Now, the results are self-cleansing. And that's sort of the beauty of it because Google measures relevance every hour, every day. However, Google didn't settle for that this time. They just thought that's racist, and it's a bad search result. And we're going to go back and clean that up manually. We are going to write some code and fix it, which they did. And I don't think that anyone in this room thinks that that was a bad idea. Me neither. But then, couple of years go by. And the world's most Googled Anders, Anders Behring Breivik, did what he did. This is July 22nd in 2011 and a terrible day in Norwegian history. This man, a terrorist, blew up couple of government buildings, walking distance from where we are right now in Oslo, Norway. And he traveled out to the island of Utoya and shot and killed a group of kids. Almost 80 people died that day. And a lot of people would describe this act of terror as two steps, that he did two things - he blew up the buildings, and he shot those kids. It's not true. It was three steps. He blew up those buildings, he shot those kids, and he sat down and waited for the world to Google him. And if there was somebody who immediately understood this, it was a Swedish web developer, a search engine optimization expert, in Stockholm named Nikke Lindqvist. He told everybody, if there's something that this guy wants right now, it's to control the image of himself. Let's see if we can distort that. Let's see if we in the civilized world can protest against what he did through insulting him in his search results. And how? He told all of his readers the following. Go out there on the Internet, find pictures of dog poop on sidewalks, publish them in your feeds, on your websites, on your blogs. Make sure to write the terrorist's name in the caption. Make sure to name the picture file breivik. jpeg. Let's teach Google that that's the face of a terrorist. And it worked. Strangely enough, Google didn't intervene this time. They did not step in and manually clean those search results up. So the million-dollar question - is there anything different between these two happenings here? Is there anything different between what happened to Michelle Obama and what happened to Anders Behring Breivik? Of course not. It's the exact same thing, yet Google intervened in one case and not in the other. RAZ: In this example of Anders Breivik and Michelle Obama, I think most of us would say, yeah, that was a right thing to do, right? You don't want. . . EKSTROM: Sure. RAZ: . . . One, and you do want the other thing to happen. So - but I guess what you're arguing is that, yes, in this case, it is a good outcome. But what if it's a - more of a gray area, right? Like, what happens then? EKSTROM: Yeah. And this is - and I use the example just because it's so easy to just - we can agree, you know, that a mass murderer is not somebody that we need to look after a whole lot when it comes to his search results, right? We don't have to care so much about that because he - maybe he has consumed those rights, if you will. However, just make it a little more difficult. Let's just, you know, make it about two regular people who are fighting for the same political office, or let's just make it whatever we want to make it. Immediately, you get to a point where you have to say, well, Google, you did manually actually change this. That means that you have an opinion. You have a bias. And that makes you editors, you know? So let's just take an easy example. If you google for the Holocaust, you don't immediately see the worst cases of people saying that it never happened, right? So they have actually manually gone into their search results just to make sure that people who said the Holocaust was a hoax, they don't get that top-ranking space. And everybody, you know, would say that, OK, well, that was a good decision, right? Because that's a hoax, and those people are bad people, and of course we should do that. I agree with that. But then that also means that, you know, human judgment has just taken place. Where do we draw the line? Where - what else, Google? You know, there are other bad things out there. What else is it that you shouldn't be linking to? What else is there? And the moment where Google accept that they have that responsibility, oh, congratulations, you're the editors of the world. RAZ: I think I understand your point here. But, I mean, what do you expect Google or anyone in that position to do? To not intervene? EKSTROM: I'd like to - for the battle to be not necessarily to fight bias because I think maybe that's impossible. There are just some experiences that are so profound and so - they're shaping us so strongly that I think that we can probably never be completely neutral and free from them. And you know what? I'm not even sure that I would want to be that person. I'd like to - I don't mind carrying a set of values with me. I think maybe that's a part of being human. But how about making sure that we're already aware that we have them and then be able to talk about them and identify when they come into play and really mess with our judgment? Because it - you know, sometimes that happens. That would be probably a better starting point or even a better end goal. Let's just agree that this is something that we all have and carry. Let's make sure that it doesn't influence us in an unhealthy way. RAZ: That's Andreas Ekstrom. He's a journalist and author of several books in Swedish, including \"The Google Code. \" You can see his full talk at ted. com. GUY RAZ, HOST:  On the show today, ideas about the biases we carry and the ways we try to address them. And like many other problems, one way to get around our biases could be with technology because a lot of us assume that technology is always objective. ANDREAS EKSTROM: Well, we don't think that a machine has an opinion, and we forget that there's a programmer behind every machine that has told it how to prioritize. So that means that, you know, if you want to really run all the way with that ball, you can say that there's not a single search query that you can make that is unbiased because it's all an effect of what a real person has decided that the algorithm should do. RAZ: This is Andreas Ekstrom. EKSTROM: Yeah, I am a Swedish reporter and author turned speaker-educator. I try to understand a little bit about what's happening with society through the digital revolution. RAZ: Is it possible for anything or anyone or any search result to be totally unbiased, to be completely objective? I mean, is objectivity even possible? EKSTROM: Yes, for undisputed scientific facts. What is the capital of Sweden? It's Stockholm. There - that's undisputed. You can Google that, and I don't think you'll find a single website that will tell you differently. So yes, there are, but they're very singular, very isolated facts like that. You want to try to Google an answer to the question, why is there an armed conflict between Israel and Palestine? That's not a great thing to Google because that takes a lot of knowledge and historical context to even begin to understand. And sometimes we tend to mix these things up. What we're trying now, large scale, really, is we're trying to see, can we replace human judgment with an algorithm? We can gather the facts, sure. But can we gather knowledge the way we gather facts? Absolutely not. That's two completely different things. RAZ: Andreas Ekstrom picks up this idea from the TED stage. (SOUNDBITE OF TEDx TALK) EKSTROM: And to get to knowledge, you have to bring 10 or 20 or a hundred facts to the table and acknowledge them and say, yes, these are all true. But because of who I am - young or old, or black or white, or gay or straight - I will value them differently. And I will say, yes, this is true, but this is more important to me than that. And this is where it becomes interesting because this is where we become human. This is when we start to argue, to form society. And to really get somewhere, we need to filter all our facts here - through friends and neighbors and parents and children and coworkers and newspapers and magazines - to finally be grounded in real knowledge, which is something that a search engine is a poor help to achieve. RAZ: When we come back, Andreas Ekstrom explains how Google search results can be manipulated. On the show today, ideas about bias and perception. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC) RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about bias and perception. And before the break, we were hearing from the writer Andreas Ekstrom about the inherent bias in search engines and how they can't always tell the difference between what's true and what's popular. EKSTROM: A lot of the algorithm is based upon popularity. So if an answer to a question is really popular, then Google tends to think that it's also correct and relevant. And why? Well, because those people are super active and link to each other and update often, and all those things, you know, is credibility in the Google universe. RAZ: Andreas Ekstrom picks up his idea from the TED stage with a Google search. (SOUNDBITE OF ARCHIVED RECORDING) EKSTROM: We'll start by Michelle Obama, First Lady of the United States. And we'll click for pictures, say - perfect search result, more or less. It's just her in the picture, not even the president. How does this work? They look at two things more than anything. First, what does it say in the caption? What does it say under the picture on each website? Does it say Michelle Obama under the picture? Pretty good indication it's actually her on there. Second, Google looks at the picture file, the name of the file as such uploaded to the website. Again, is it called michelleobama. jpeg? Pretty good indication it's not Clint Eastwood in the picture. So you got those two, and you get a search result like this - almost. Now, in 2009, Michelle Obama was the victim of a racist campaign where people set out to insult her through her search results. There was a picture distributed widely over the Internet where her face was distorted to look like a monkey. And that picture was published all over, and people published it very, very purposefully to get it up here in the search result. They made sure to write Michelle Obama in the caption, and they made sure to upload the picture as michelleobama. jpeg or the like. So you get why - to manipulate the search result. And it worked too. So when you picture-Googled for Michelle Obama in 2009, that distorted monkey picture showed up among the first results. Now, the results are self-cleansing. And that's sort of the beauty of it because Google measures relevance every hour, every day. However, Google didn't settle for that this time. They just thought that's racist, and it's a bad search result. And we're going to go back and clean that up manually. We are going to write some code and fix it, which they did. And I don't think that anyone in this room thinks that that was a bad idea. Me neither. But then, couple of years go by. And the world's most Googled Anders, Anders Behring Breivik, did what he did. This is July 22nd in 2011 and a terrible day in Norwegian history. This man, a terrorist, blew up couple of government buildings, walking distance from where we are right now in Oslo, Norway. And he traveled out to the island of Utoya and shot and killed a group of kids. Almost 80 people died that day. And a lot of people would describe this act of terror as two steps, that he did two things - he blew up the buildings, and he shot those kids. It's not true. It was three steps. He blew up those buildings, he shot those kids, and he sat down and waited for the world to Google him. And if there was somebody who immediately understood this, it was a Swedish web developer, a search engine optimization expert, in Stockholm named Nikke Lindqvist. He told everybody, if there's something that this guy wants right now, it's to control the image of himself. Let's see if we can distort that. Let's see if we in the civilized world can protest against what he did through insulting him in his search results. And how? He told all of his readers the following. Go out there on the Internet, find pictures of dog poop on sidewalks, publish them in your feeds, on your websites, on your blogs. Make sure to write the terrorist's name in the caption. Make sure to name the picture file breivik. jpeg. Let's teach Google that that's the face of a terrorist. And it worked. Strangely enough, Google didn't intervene this time. They did not step in and manually clean those search results up. So the million-dollar question - is there anything different between these two happenings here? Is there anything different between what happened to Michelle Obama and what happened to Anders Behring Breivik? Of course not. It's the exact same thing, yet Google intervened in one case and not in the other. RAZ: In this example of Anders Breivik and Michelle Obama, I think most of us would say, yeah, that was a right thing to do, right? You don't want. . . EKSTROM: Sure. RAZ: . . . One, and you do want the other thing to happen. So - but I guess what you're arguing is that, yes, in this case, it is a good outcome. But what if it's a - more of a gray area, right? Like, what happens then? EKSTROM: Yeah. And this is - and I use the example just because it's so easy to just - we can agree, you know, that a mass murderer is not somebody that we need to look after a whole lot when it comes to his search results, right? We don't have to care so much about that because he - maybe he has consumed those rights, if you will. However, just make it a little more difficult. Let's just, you know, make it about two regular people who are fighting for the same political office, or let's just make it whatever we want to make it. Immediately, you get to a point where you have to say, well, Google, you did manually actually change this. That means that you have an opinion. You have a bias. And that makes you editors, you know? So let's just take an easy example. If you google for the Holocaust, you don't immediately see the worst cases of people saying that it never happened, right? So they have actually manually gone into their search results just to make sure that people who said the Holocaust was a hoax, they don't get that top-ranking space. And everybody, you know, would say that, OK, well, that was a good decision, right? Because that's a hoax, and those people are bad people, and of course we should do that. I agree with that. But then that also means that, you know, human judgment has just taken place. Where do we draw the line? Where - what else, Google? You know, there are other bad things out there. What else is it that you shouldn't be linking to? What else is there? And the moment where Google accept that they have that responsibility, oh, congratulations, you're the editors of the world. RAZ: I think I understand your point here. But, I mean, what do you expect Google or anyone in that position to do? To not intervene? EKSTROM: I'd like to - for the battle to be not necessarily to fight bias because I think maybe that's impossible. There are just some experiences that are so profound and so - they're shaping us so strongly that I think that we can probably never be completely neutral and free from them. And you know what? I'm not even sure that I would want to be that person. I'd like to - I don't mind carrying a set of values with me. I think maybe that's a part of being human. But how about making sure that we're already aware that we have them and then be able to talk about them and identify when they come into play and really mess with our judgment? Because it - you know, sometimes that happens. That would be probably a better starting point or even a better end goal. Let's just agree that this is something that we all have and carry. Let's make sure that it doesn't influence us in an unhealthy way. RAZ: That's Andreas Ekstrom. He's a journalist and author of several books in Swedish, including \"The Google Code. \" You can see his full talk at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-16-695293679": {"title": "Opinion: Good Night Oppy, A Farewell To NASA's Mars Rover  : NPR", "url": "https://www.npr.org/2019/02/16/695293679/opinion-good-night-oppy-a-farewell-to-nasas-mars-rover", "author": "No author found", "published_date": "2019-02-16", "content": "SCOTT SIMON, HOST: We probably should not project human traits onto machines, but if you spend a lot of time with a mechanism - talk to it, wait to hear from it and worry about it - even scientists begin to see personality in machinery. When the Opportunity Mars Exploration Rover ended its mission this week after more than 5,000 Martian days, NASA scientists mourned. This is a hard day, Opportunity's project manager, John Callas, told reporters. Even though it's a machine and we're saying goodbye, it's still very hard and very poignant. Opportunity and its cousin rover, Spirit, both landed on Mars in January of 2004. They were supposed to carry on for just three months, scratching and scouring for less than a mile over the Martian landscape. But Spirit roamed for almost five miles and lasted six years. Oppy, as scientists began to call the Opportunity rover, rolled over Mars for 28 miles and stayed on the job for more than 14 years. It transmitted 217,594 images, including a selfie. Spirit and Opportunity helped establish that there was once liquid water on Mars. This doesn't mean there will soon be beach resorts on Mars, but it does confirm that some of the elements of life may have once existed there on a world that now looks pretty dry, lifeless and cold. It's a reminder not to judge too much by appearance. Planets and people have histories. Oppy got stuck in a dune in 2005, but NASA scientists working over a distance of millions of miles were able to free their rover. Oppy also suffered from recurrent wheel and robotic arm problems for most of his - her - its - life but kept on rolling, searching, digging and sending back information. A dust storm enveloped much of Mars last June. Oppy foundered in a gully on the western rim of the Endeavour Crater - in a gully the scientists called Perseverance Valley. The storm robbed Oppy of the solar power to recharge its batteries. NASA scientists sent it more than 830 rescue commands. They beamed music to Oppy to try to awaken their Martian explorer - David Bowie's \"Life On Mars,\" Gloria Gaynor's \"I Will Survive,\" \"Here Comes The Sun\" by - you know. Oppy was too depleted to reply. The rover did send a last image of a dark world cloaked in dust. Jacob Margolis, a science reporter for KPCC in Pasadena, made a poetic translation of the digital bursts, bytes and squeaks Oppy sent out before going silent - my battery is low and it's getting dark. We might all hope for such a gentle end to a useful life. (SOUNDBITE OF SONG, \"LIFE ON MARS? \")DAVID BOWIE: (Singing) Take a look at the law man beating up the wrong guy. Oh, man, wonder if he'll ever know he's in the bestselling show. Is there life on Mars? SCOTT SIMON, HOST:  We probably should not project human traits onto machines, but if you spend a lot of time with a mechanism - talk to it, wait to hear from it and worry about it - even scientists begin to see personality in machinery. When the Opportunity Mars Exploration Rover ended its mission this week after more than 5,000 Martian days, NASA scientists mourned. This is a hard day, Opportunity's project manager, John Callas, told reporters. Even though it's a machine and we're saying goodbye, it's still very hard and very poignant. Opportunity and its cousin rover, Spirit, both landed on Mars in January of 2004. They were supposed to carry on for just three months, scratching and scouring for less than a mile over the Martian landscape. But Spirit roamed for almost five miles and lasted six years. Oppy, as scientists began to call the Opportunity rover, rolled over Mars for 28 miles and stayed on the job for more than 14 years. It transmitted 217,594 images, including a selfie. Spirit and Opportunity helped establish that there was once liquid water on Mars. This doesn't mean there will soon be beach resorts on Mars, but it does confirm that some of the elements of life may have once existed there on a world that now looks pretty dry, lifeless and cold. It's a reminder not to judge too much by appearance. Planets and people have histories. Oppy got stuck in a dune in 2005, but NASA scientists working over a distance of millions of miles were able to free their rover. Oppy also suffered from recurrent wheel and robotic arm problems for most of his - her - its - life but kept on rolling, searching, digging and sending back information. A dust storm enveloped much of Mars last June. Oppy foundered in a gully on the western rim of the Endeavour Crater - in a gully the scientists called Perseverance Valley. The storm robbed Oppy of the solar power to recharge its batteries. NASA scientists sent it more than 830 rescue commands. They beamed music to Oppy to try to awaken their Martian explorer - David Bowie's \"Life On Mars,\" Gloria Gaynor's \"I Will Survive,\" \"Here Comes The Sun\" by - you know. Oppy was too depleted to reply. The rover did send a last image of a dark world cloaked in dust. Jacob Margolis, a science reporter for KPCC in Pasadena, made a poetic translation of the digital bursts, bytes and squeaks Oppy sent out before going silent - my battery is low and it's getting dark. We might all hope for such a gentle end to a useful life. (SOUNDBITE OF SONG, \"LIFE ON MARS? \") DAVID BOWIE: (Singing) Take a look at the law man beating up the wrong guy. Oh, man, wonder if he'll ever know he's in the bestselling show. Is there life on Mars?", "section": "Simon Says", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-18-695729829": {"title": "Facebook Has Behaved Like 'Digital Gangsters,' U.K. Parliament Report Says : NPR", "url": "https://www.npr.org/2019/02/18/695729829/facebook-has-been-behaving-like-digital-gangsters-u-k-parliament-report-says", "author": "No author found", "published_date": "2019-02-18", "content": "", "section": "Business", "disclaimer": ""}, "2019-02-20-696198626": {"title": "Southwest Links Labor Union Trouble To Unprecedented Number Of Cancelled Flights : NPR", "url": "https://www.npr.org/2019/02/20/696198626/southwest-grounds-planes-blames-labor-dispute-with-the-union", "author": "No author found", "published_date": "2019-02-20", "content": "", "section": "National", "disclaimer": ""}, "2019-02-21-696597381": {"title": "Apple And Goldman Sachs Will Reportedly Launch An iPhone-Connected Credit Card : NPR", "url": "https://www.npr.org/2019/02/21/696597381/apple-and-goldman-sachs-will-reportedly-launch-an-iphone-connected-credit-card", "author": "No author found", "published_date": "2019-02-21", "content": "", "section": "Business", "disclaimer": ""}, "2019-02-21-696430478": {"title": "Children's Advocates Ask FTC To Investigate Facebook For Fraud : NPR", "url": "https://www.npr.org/2019/02/21/696430478/advocates-ask-ftc-to-investigate-facebook-deception-over-kids-in-game-purchases", "author": "No author found", "published_date": "2019-02-21", "content": "", "section": "Technology", "disclaimer": ""}, "2019-02-22-697110641": {"title": "Microsoft Workers Protest Army Contract With Tech 'Designed To Help People Kill' : NPR", "url": "https://www.npr.org/2019/02/22/697110641/microsoft-workers-protest-army-contract-with-tech-designed-to-help-people-kill", "author": "No author found", "published_date": "2019-02-22", "content": "", "section": "Business", "disclaimer": ""}, "2019-02-22-697152943": {"title": "How Pinterest Jumped Into The Fight Against Health Misinformation : NPR", "url": "https://www.npr.org/2019/02/22/697152943/how-pinterest-jumped-into-the-fight-against-health-misinformation", "author": "No author found", "published_date": "2019-02-22", "content": "AUDIE CORNISH, HOST: More than 120 people have been diagnosed with measles so far this year in outbreaks in Texas, New York and Washington. One factor public health officials suspect is contributing to these outbreaks - anti-vaccine information on social media. Pinterest, the visual bookmarking site, has decided to intervene. A 2017 policy limits search results related to, \"quote, health misinformation, including about vaccines. \" Ifeoma Ozoma is the public policy and social impact manager for Pinterest. Before this policy was in place. . . IFEOMA OZOMA: For a term like vaccine, if you had searched vaccine, much of the content was in violation of our community guidelines because it was anti-vaccine advice. CORNISH: A search on the site for cancer cures might have brought up pins or bookmarks for pages about herbs and juices that work better than chemotherapy. Now you'll find a message that says pins about this topic often violate our community guidelines, so we're currently unable to show search results. OZOMA: Our goal, really, is harm reduction. And so because we're humble about our limitations and our own expertise here, we look to outside experts like the WHO, CDC and the American Academy of Pediatrics and their guidance on what's harmful. CORNISH: Now, there are some critics of this move. Jennifer Granick of the ACLU told The Wall Street Journal that this is dangerous, that it's essentially a secretive process, no real appeal. People are making very difficult subject calls when it comes to politics and culture and religion. What's your response? OZOMA: So to that, we have clear and transparent community guidelines. And this is just one way of enforcing, like. . . CORNISH: Like buried in the terms and conditions or what do you mean by that? OZOMA: No. Nope. They're clear in our community guidelines on our website. And we also, whenever we have a search that we've removed results for, we explain right in there in the search advisory why we removed it, and we link to those community guidelines. And we also have an appeals process for any content that's taken down. CORNISH: Is this essentially censorship? OZOMA: For us, we don't see it as that. There's an enthusiasm gap between those who save harmful health misinformation and organizations like the CDC and WHO and American Academy of Pediatrics. And so because of that, you're going to find more health misinformation than, say, journal articles on the virtues of vaccination or other science-based health interventions. We've taken the view that further sharing that harmful content through our search results isn't in line with enforcing our community guidelines. CORNISH: Your title is public policy and social impact manager. None of those things are things we thought about when we thought about social media when it was first starting up, right? We called them platforms. They were just places we put things that we wanted to share. When do you think this mindset changed? OZOMA: We have had content policy and trust and safety teams since the beginning. And so safety has always been a consideration when you think about different types of harmful content, whether they're illegal or not illegal. Safety has been top of mind and still is for every team across the company. CORNISH: People go to the Internet and go to these platforms to find like-minded communities and to share information. Are you doing damage to that, that kind of agreement that they think they have with you? OZOMA: Yeah. So harmful misinformation is not inspiring, and it's not the kind of content that our platform hopes to promote. And because. . . CORNISH: But what if people think you shouldn't be the one to make that decision for them? As adults who are on the Internet doing research on their own, why should you get to make that call? OZOMA: Well, we aren't making the call because vaccines are settled science. And we also are very clear because we know that there may be questions about the decisions that we've made. We're really clear and transparent in our community guidelines and use simple language so that everyone can understand why we're considering certain content harmful. CORNISH: Ifeoma Ozoma is the Pinterest public policy social impact manager. Thank you so much for speaking with us. OZOMA: Thanks so much for talking with us today. AUDIE CORNISH, HOST:  More than 120 people have been diagnosed with measles so far this year in outbreaks in Texas, New York and Washington. One factor public health officials suspect is contributing to these outbreaks - anti-vaccine information on social media. Pinterest, the visual bookmarking site, has decided to intervene. A 2017 policy limits search results related to, \"quote, health misinformation, including about vaccines. \" Ifeoma Ozoma is the public policy and social impact manager for Pinterest. Before this policy was in place. . . IFEOMA OZOMA: For a term like vaccine, if you had searched vaccine, much of the content was in violation of our community guidelines because it was anti-vaccine advice. CORNISH: A search on the site for cancer cures might have brought up pins or bookmarks for pages about herbs and juices that work better than chemotherapy. Now you'll find a message that says pins about this topic often violate our community guidelines, so we're currently unable to show search results. OZOMA: Our goal, really, is harm reduction. And so because we're humble about our limitations and our own expertise here, we look to outside experts like the WHO, CDC and the American Academy of Pediatrics and their guidance on what's harmful. CORNISH: Now, there are some critics of this move. Jennifer Granick of the ACLU told The Wall Street Journal that this is dangerous, that it's essentially a secretive process, no real appeal. People are making very difficult subject calls when it comes to politics and culture and religion. What's your response? OZOMA: So to that, we have clear and transparent community guidelines. And this is just one way of enforcing, like. . . CORNISH: Like buried in the terms and conditions or what do you mean by that? OZOMA: No. Nope. They're clear in our community guidelines on our website. And we also, whenever we have a search that we've removed results for, we explain right in there in the search advisory why we removed it, and we link to those community guidelines. And we also have an appeals process for any content that's taken down. CORNISH: Is this essentially censorship? OZOMA: For us, we don't see it as that. There's an enthusiasm gap between those who save harmful health misinformation and organizations like the CDC and WHO and American Academy of Pediatrics. And so because of that, you're going to find more health misinformation than, say, journal articles on the virtues of vaccination or other science-based health interventions. We've taken the view that further sharing that harmful content through our search results isn't in line with enforcing our community guidelines. CORNISH: Your title is public policy and social impact manager. None of those things are things we thought about when we thought about social media when it was first starting up, right? We called them platforms. They were just places we put things that we wanted to share. When do you think this mindset changed? OZOMA: We have had content policy and trust and safety teams since the beginning. And so safety has always been a consideration when you think about different types of harmful content, whether they're illegal or not illegal. Safety has been top of mind and still is for every team across the company. CORNISH: People go to the Internet and go to these platforms to find like-minded communities and to share information. Are you doing damage to that, that kind of agreement that they think they have with you? OZOMA: Yeah. So harmful misinformation is not inspiring, and it's not the kind of content that our platform hopes to promote. And because. . . CORNISH: But what if people think you shouldn't be the one to make that decision for them? As adults who are on the Internet doing research on their own, why should you get to make that call? OZOMA: Well, we aren't making the call because vaccines are settled science. And we also are very clear because we know that there may be questions about the decisions that we've made. We're really clear and transparent in our community guidelines and use simple language so that everyone can understand why we're considering certain content harmful. CORNISH: Ifeoma Ozoma is the Pinterest public policy social impact manager. Thank you so much for speaking with us. OZOMA: Thanks so much for talking with us today.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-22-696924139": {"title": "Tell NPR about your experiences sharing your kids' lives online. : NPR", "url": "https://www.npr.org/2019/02/22/696924139/do-you-post-about-your-kids-online-and-talk-with-them-about-it-tell-us-your-stor", "author": "No author found", "published_date": "2019-02-22", "content": "", "section": "Technology", "disclaimer": ""}, "2019-02-22-696720512": {"title": "Ever Regretted Your Online Behavior? We Want To Hear From You : NPR", "url": "https://www.npr.org/2019/02/22/696720512/ever-regretted-your-online-behavior-we-want-to-hear-from-you", "author": "No author found", "published_date": "2019-02-22", "content": "", "section": "Technology", "disclaimer": ""}, "2019-02-22-696949013": {"title": "Advertisers Abandon YouTube Over Concerns That Pedophiles Lurk In Comments Section : NPR", "url": "https://www.npr.org/2019/02/22/696949013/advertisers-abandon-youtube-over-concerns-that-pedophiles-lurk-in-comments-secti", "author": "No author found", "published_date": "2019-02-22", "content": "", "section": "Technology", "disclaimer": ""}, "2019-02-24-697481316": {"title": "Twitter Trolls And 2020 : NPR", "url": "https://www.npr.org/2019/02/24/697481316/twitter-trolls-and-2020", "author": "No author found", "published_date": "2019-02-24", "content": "LULU GARCIA-NAVARRO, HOST: The 2020 campaign is just getting started, but digital disinformation efforts are already well underway. That's according to a new report from Politico, which found that Kamala Harris, Elizabeth Warren, Bernie Sanders and Beto O'Rourke are already major targets of misinformation online. Natasha Korecki is a correspondent at Politico, and she joins us now to explain. Welcome. NATASHA KORECKI: Thanks for having me. GARCIA-NAVARRO: Tell me - what is new here? How is this information being disseminated? KORECKI: Sure. So what's new here is that it's an - if you look at it holistically of what's happening on all these different social media platforms, it's happening quickly; it's happening aggressively. What the Guardian. ai (ph) - the ones who did the research for us - found was that there was this core group of accounts that were driving a disproportionate percentage of the conversation. And they tracked these accounts and found that a lot of them were real people who really believed this. But there was a larger group of accounts around that that were amplifying it in what appeared to be a more coordinated way. GARCIA-NAVARRO: Can you give me an example of what it is that they're spreading? KORECKI: Sure. So one of the most blatant, I think, happened with regard to Elizabeth Warren. And it was that, you know, over Instagram, she had livestreamed something from her kitchen. And someone posted something saying she had a blackface doll in her kitchen. And that appeared. . . GARCIA-NAVARRO: I think I saw that actually. KORECKI: Yeah. It appeared on one platform, and then somebody actually put money behind an ad on a different platform kind of furthering that. That was, like, blatant fake news. But the major underlying theme here is that it's playing to the most extreme parts of the conversation around all these candidates - you know, very sexist, racist, that type of things - and pushing those things out. GARCIA-NAVARRO: Do we know who is behind this? Are these state actors as we saw in the 2016 election, or are these American individuals? KORECKI: Yeah. Well, so there's no clear evidence of who's involved. What they can say - and we talked to data scientists, campaign operatives, digital strategists. And a lot of them said they saw some signs of coordination, and that was mainly over Twitter. So there's some belief that there's some state actor involvement. However, it isn't isolated to that. One of the data scientists we talked to said - you know, they described this as an unholy alliance. And that's probably the best way to look at it is there is just a different hodgepodge of people who are pushing this for a variety of reasons, but it's getting amplified in a more coordinated way. GARCIA-NAVARRO: So you're talking about specific accounts that are known quantities for spreading disinformation. Why aren't they just shut down? KORECKI: Well, I think some of them have been shut down. But one core thing to remember here is that there are real people involved here, and this is what they believe. That is not what we're pointing to. What we're pointing to is what's being amplified. And it's a new strategy where, instead of creating tens of thousands of bots or - and so forth, the new strategy is taking real people, finding the message that you want and then amplifying that message. So if you're Twitter, you can't really shut down the real person who's the messenger. And these other people are much more difficult to detect. GARCIA-NAVARRO: What are you hearing from the candidates' campaigns? I mean, how are they protecting themselves or combating this, if at all? KORECKI: Right. So there's not a whole lot that they're doing right now. To the extent that they have digital operations, a lot of those operations are focusing on things like fundraising, organizing events and getting people to their events, donating money, that type of thing. And it's meant that defense tactics have taken a backseat. What we're hearing from the campaigns is they think this issue is bigger than them. They think it's something that no one individual campaign can really combat, that it's something that maybe Democrats, maybe Republicans holistically have to come up with some kind of solution. And that - when you talk to, then, the digital world and the data scientists and stuff, they say that's very troubling because here we are. We're rushing toward 2020, and there's no real game plan from a lot of these campaigns. GARCIA-NAVARRO: Natasha Korecki of Politico, thank you so much. KORECKI: Thank you. LULU GARCIA-NAVARRO, HOST:  The 2020 campaign is just getting started, but digital disinformation efforts are already well underway. That's according to a new report from Politico, which found that Kamala Harris, Elizabeth Warren, Bernie Sanders and Beto O'Rourke are already major targets of misinformation online. Natasha Korecki is a correspondent at Politico, and she joins us now to explain. Welcome. NATASHA KORECKI: Thanks for having me. GARCIA-NAVARRO: Tell me - what is new here? How is this information being disseminated? KORECKI: Sure. So what's new here is that it's an - if you look at it holistically of what's happening on all these different social media platforms, it's happening quickly; it's happening aggressively. What the Guardian. ai (ph) - the ones who did the research for us - found was that there was this core group of accounts that were driving a disproportionate percentage of the conversation. And they tracked these accounts and found that a lot of them were real people who really believed this. But there was a larger group of accounts around that that were amplifying it in what appeared to be a more coordinated way. GARCIA-NAVARRO: Can you give me an example of what it is that they're spreading? KORECKI: Sure. So one of the most blatant, I think, happened with regard to Elizabeth Warren. And it was that, you know, over Instagram, she had livestreamed something from her kitchen. And someone posted something saying she had a blackface doll in her kitchen. And that appeared. . . GARCIA-NAVARRO: I think I saw that actually. KORECKI: Yeah. It appeared on one platform, and then somebody actually put money behind an ad on a different platform kind of furthering that. That was, like, blatant fake news. But the major underlying theme here is that it's playing to the most extreme parts of the conversation around all these candidates - you know, very sexist, racist, that type of things - and pushing those things out. GARCIA-NAVARRO: Do we know who is behind this? Are these state actors as we saw in the 2016 election, or are these American individuals? KORECKI: Yeah. Well, so there's no clear evidence of who's involved. What they can say - and we talked to data scientists, campaign operatives, digital strategists. And a lot of them said they saw some signs of coordination, and that was mainly over Twitter. So there's some belief that there's some state actor involvement. However, it isn't isolated to that. One of the data scientists we talked to said - you know, they described this as an unholy alliance. And that's probably the best way to look at it is there is just a different hodgepodge of people who are pushing this for a variety of reasons, but it's getting amplified in a more coordinated way. GARCIA-NAVARRO: So you're talking about specific accounts that are known quantities for spreading disinformation. Why aren't they just shut down? KORECKI: Well, I think some of them have been shut down. But one core thing to remember here is that there are real people involved here, and this is what they believe. That is not what we're pointing to. What we're pointing to is what's being amplified. And it's a new strategy where, instead of creating tens of thousands of bots or - and so forth, the new strategy is taking real people, finding the message that you want and then amplifying that message. So if you're Twitter, you can't really shut down the real person who's the messenger. And these other people are much more difficult to detect. GARCIA-NAVARRO: What are you hearing from the candidates' campaigns? I mean, how are they protecting themselves or combating this, if at all? KORECKI: Right. So there's not a whole lot that they're doing right now. To the extent that they have digital operations, a lot of those operations are focusing on things like fundraising, organizing events and getting people to their events, donating money, that type of thing. And it's meant that defense tactics have taken a backseat. What we're hearing from the campaigns is they think this issue is bigger than them. They think it's something that no one individual campaign can really combat, that it's something that maybe Democrats, maybe Republicans holistically have to come up with some kind of solution. And that - when you talk to, then, the digital world and the data scientists and stuff, they say that's very troubling because here we are. We're rushing toward 2020, and there's no real game plan from a lot of these campaigns. GARCIA-NAVARRO: Natasha Korecki of Politico, thank you so much. KORECKI: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-25-697839220": {"title": "Etiquette In A Cashless World : NPR", "url": "https://www.npr.org/2019/02/25/697839220/etiquette-in-a-cashless-world", "author": "No author found", "published_date": "2019-02-25", "content": "MARY LOUISE KELLY, HOST: Well, as we just heard, an app that both allows you to collect your boyfriend's share of the rent and allows your mom to find out about it gives rise to all kinds of etiquette questions. Turns out colleagues here at NPR are wrestling with those questions, and who better to answer them than Amy Dickinson? She writes the syndicated newspaper advice column \"Ask Amy. \" Hi, Amy. AMY DICKINSON: Hi, Mary Louise. KELLY: So we have millennial producers - you'll be shocked to hear - lining up to ask questions. Our first one is here in the studio with me. So let me welcome producer Lauren Hodges. And Lauren, your question. LAUREN HODGES, BYLINE: Amy, what is the etiquette when it comes to heads-up before you Venmo charge somebody? Do you have to talk about it first, or can you just hit them up for cash? DICKINSON: Well, I would say anything where you describe the action as hitting someone up, that's your answer, right? So you don't want to be hitting people up, but you do want to communicate beforehand that you are going to ask to be compensated. This comes up a lot in dating, where there are misunderstandings about like, are we on a date where you pick up the check - the whole check? There's nothing worse than, like, being on your way home after a first date and having somebody Venmo you for your half. KELLY: Not if you want a second date. DICKINSON: That's right. That's right. Venmo has really enhanced the need for people to communicate and be a little more open about their spending. You know, people in my generation - we think that money transactions are like, private. I notice that young people are much more transparent about their expectations, and I think that's a good thing. KELLY: All right, Lauren, an answer to your one-on-one question. Thank you. HODGES: Thanks, Amy. Good to know. DICKINSON: OK. KELLY: Next we turn to Cristina Cala of our staff. She is wondering about something that might bring on jealousy. CHRISTINA CALA, BYLINE: What if you're going out with someone, and you see that they Venmo'd their ex? KELLY: Amy Dickinson, this gets into the whole combining a mobile payment system and a social media platform question. DICKINSON: Yeah, my favorite description of Venmo is it's like your phone and your wallet had a shiny, little baby. But the fact is, anytime anybody posts anything on a social media feed, that means it's up for discussion, if you're game, or some sort of communication about it. Now, I know a lot of millennials might deal with that by posting - sort of sub-tweeting - you know, making a subtle, little jab maybe on the same platform. Like, you might maybe Venmo your ex. You know, it's. . . KELLY: (Laughter) Oh, my God. DICKINSON: It's funny. There's a lot of passive aggression going on in Venmo. Let me tell you. KELLY: OK, next question from one of our producers, Connor Donovan. This one is about tipping via Venmo. CONNOR DONOVAN, BYLINE: If you're in a situation where you would typically give someone a cash tip like a hotel maid or a bellhop but you don't have cash, is there any way out of that situation? I feel like it happens more and more. KELLY: Amy. DICKINSON: OK, here's my little rule. I believe that you should repay people in the currency that they use, not the currency that you use. And so yeah, all my kids and every other younger person out there, you're going have to, when you travel, do what grown-ups do and get some fives and tens to tip people. KELLY: To sum up, is there a golden rule of Venmo etiquette that we should all have in mind? DICKINSON: You know, I think the golden rule that we have in mind for every transaction - non-Venmo and Venmo - do unto others. And so what you want to do is be as kind, as generous to people as you would expect them to be toward you. And I think in some ways, Venmo is actually making that easier. KELLY: Amy Dickinson - she writes the syndicated newspaper advice column \"Ask Amy. \" Thanks so much. DICKINSON: Thank you. (SOUNDBITE OF JUNGLE FIRE'S \"FIREWALKER\") MARY LOUISE KELLY, HOST:  Well, as we just heard, an app that both allows you to collect your boyfriend's share of the rent and allows your mom to find out about it gives rise to all kinds of etiquette questions. Turns out colleagues here at NPR are wrestling with those questions, and who better to answer them than Amy Dickinson? She writes the syndicated newspaper advice column \"Ask Amy. \" Hi, Amy. AMY DICKINSON: Hi, Mary Louise. KELLY: So we have millennial producers - you'll be shocked to hear - lining up to ask questions. Our first one is here in the studio with me. So let me welcome producer Lauren Hodges. And Lauren, your question. LAUREN HODGES, BYLINE: Amy, what is the etiquette when it comes to heads-up before you Venmo charge somebody? Do you have to talk about it first, or can you just hit them up for cash? DICKINSON: Well, I would say anything where you describe the action as hitting someone up, that's your answer, right? So you don't want to be hitting people up, but you do want to communicate beforehand that you are going to ask to be compensated. This comes up a lot in dating, where there are misunderstandings about like, are we on a date where you pick up the check - the whole check? There's nothing worse than, like, being on your way home after a first date and having somebody Venmo you for your half. KELLY: Not if you want a second date. DICKINSON: That's right. That's right. Venmo has really enhanced the need for people to communicate and be a little more open about their spending. You know, people in my generation - we think that money transactions are like, private. I notice that young people are much more transparent about their expectations, and I think that's a good thing. KELLY: All right, Lauren, an answer to your one-on-one question. Thank you. HODGES: Thanks, Amy. Good to know. DICKINSON: OK. KELLY: Next we turn to Cristina Cala of our staff. She is wondering about something that might bring on jealousy. CHRISTINA CALA, BYLINE: What if you're going out with someone, and you see that they Venmo'd their ex? KELLY: Amy Dickinson, this gets into the whole combining a mobile payment system and a social media platform question. DICKINSON: Yeah, my favorite description of Venmo is it's like your phone and your wallet had a shiny, little baby. But the fact is, anytime anybody posts anything on a social media feed, that means it's up for discussion, if you're game, or some sort of communication about it. Now, I know a lot of millennials might deal with that by posting - sort of sub-tweeting - you know, making a subtle, little jab maybe on the same platform. Like, you might maybe Venmo your ex. You know, it's. . . KELLY: (Laughter) Oh, my God. DICKINSON: It's funny. There's a lot of passive aggression going on in Venmo. Let me tell you. KELLY: OK, next question from one of our producers, Connor Donovan. This one is about tipping via Venmo. CONNOR DONOVAN, BYLINE: If you're in a situation where you would typically give someone a cash tip like a hotel maid or a bellhop but you don't have cash, is there any way out of that situation? I feel like it happens more and more. KELLY: Amy. DICKINSON: OK, here's my little rule. I believe that you should repay people in the currency that they use, not the currency that you use. And so yeah, all my kids and every other younger person out there, you're going have to, when you travel, do what grown-ups do and get some fives and tens to tip people. KELLY: To sum up, is there a golden rule of Venmo etiquette that we should all have in mind? DICKINSON: You know, I think the golden rule that we have in mind for every transaction - non-Venmo and Venmo - do unto others. And so what you want to do is be as kind, as generous to people as you would expect them to be toward you. And I think in some ways, Venmo is actually making that easier. KELLY: Amy Dickinson - she writes the syndicated newspaper advice column \"Ask Amy. \" Thanks so much. DICKINSON: Thank you. (SOUNDBITE OF JUNGLE FIRE'S \"FIREWALKER\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-25-697090168": {"title": "As Payments Go Social With Venmo, They're Changing Personal Relationships : NPR", "url": "https://www.npr.org/2019/02/25/697090168/as-payments-go-social-with-venmo-theyre-changing-personal-relationships", "author": "No author found", "published_date": "2019-02-25", "content": "ARI SHAPIRO, HOST: We've been looking at what's beyond cash - cryptocurrency, digital payments and more - in this month's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")SHAPIRO: Today there are more mobile payment apps than ever - Zel, Apple Pay, Square, Cash. But just one doubles as a social network. NPR's Daniella Cheslow explains how Venmo has changed relationships. DANIELLA CHESLOW, BYLINE: When copywriter Kelli Johnson moved to Los Angeles, she didn't tell her parents she'd be living with her boyfriend. Johnson grew up in Bakersfield, a couple hours' drive north. KELLI JOHNSON: Very conservative, very religious - my parents as well (laughter). CHESLOW: In LA, she pays the rent, and her boyfriend chips in his half on Venmo. JOHNSON: When he did it, he put it in the memo line (laughter) - the month plus, like, the little house emoji. And that's how my mom saw. (Laughter). CHESLOW: Her mom uses Venmo too. Johnson was busted. I shared that story with Richard Crone, a payments expert. RICHARD CRONE: (Laughter). That reinforces our findings; the No. 1 use case is paying rent. CHESLOW: He estimates about 39 million people use Venmo, which is owned by PayPal. The company didn't confirm. Here's how it works. You use Venmo to pay or request money from other people on the app. It's usually linked to your bank account. Every transaction has a memo line. And there are emoji for things you pay for - like pizza or wine or rent. But these memos and emoji are public by default. So you can see how your friends spend money and what they're asking others to pay for. Crone says that visibility can be a perk. CRONE: You want to pay it socially so everybody knows that you're not a deadbeat, and you've met your obligation. CHESLOW: PayPal CEO Dan Schulman says Venmo is the app for a generation that grew up on social media. And he says the public feed is the essence of the app. Here he is on CNBC. (SOUNDBITE OF ARCHIVED RECORDING)DAN SCHULMAN: It's really a social experience. Like, you do a payment. You tag it. You put an emoji next to it. You share it with your friends. CHESLOW: Nineteen billion dollars changed hands over the app just between October and December of last year. That's up 80 percent from the previous year. Not everyone knows others are seeing their payments. The Federal Trade Commission last year demanded that Venmo make it clear to customers that these transactions are public. Venmo says it never posts the amount of the transaction, and anyone can make their payments private. Private or not, the app has made it easy to give cash instantly. We heard from a woman in Baltimore. She picked up a $350 grocery bill for a friend who forgot her wallet but Venmo'd the money immediately. A man in San Francisco told us he Venmos birthday money to his friends for a round of drinks. But others noticed when it's so easy to split a bill, it becomes an expectation. MATTHEW MASOUD: I'm definitely less generous when I go out with my friends. CHESLOW: Matthew Masoud studies aerospace engineering at the University of Cincinnati. He says via Skype that he went out to dinner at a restaurant that wouldn't split the check. So he just paid for everyone, and then he sent out Venmo requests. MASOUD: One of my friends ordered pasta dish, I believe. And that one was, like, $13. CHESLOW: Before Venmo, Masoud said he would have just taken care of the whole bill, and next time someone else would. If Venmo has changed Masoud's habits, it doesn't seem to have changed Kelli Johnson. Even after her mom discovered the live-in boyfriend, Johnson says she didn't change her privacy settings. JOHNSON: No, I haven't. CHESLOW: Are you kidding? JOHNSON: I have nothing to hide now (laughter). CHESLOW: Daniella Cheslow, NPR News, Washington. ARI SHAPIRO, HOST:  We've been looking at what's beyond cash - cryptocurrency, digital payments and more - in this month's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") SHAPIRO: Today there are more mobile payment apps than ever - Zel, Apple Pay, Square, Cash. But just one doubles as a social network. NPR's Daniella Cheslow explains how Venmo has changed relationships. DANIELLA CHESLOW, BYLINE: When copywriter Kelli Johnson moved to Los Angeles, she didn't tell her parents she'd be living with her boyfriend. Johnson grew up in Bakersfield, a couple hours' drive north. KELLI JOHNSON: Very conservative, very religious - my parents as well (laughter). CHESLOW: In LA, she pays the rent, and her boyfriend chips in his half on Venmo. JOHNSON: When he did it, he put it in the memo line (laughter) - the month plus, like, the little house emoji. And that's how my mom saw. (Laughter). CHESLOW: Her mom uses Venmo too. Johnson was busted. I shared that story with Richard Crone, a payments expert. RICHARD CRONE: (Laughter). That reinforces our findings; the No. 1 use case is paying rent. CHESLOW: He estimates about 39 million people use Venmo, which is owned by PayPal. The company didn't confirm. Here's how it works. You use Venmo to pay or request money from other people on the app. It's usually linked to your bank account. Every transaction has a memo line. And there are emoji for things you pay for - like pizza or wine or rent. But these memos and emoji are public by default. So you can see how your friends spend money and what they're asking others to pay for. Crone says that visibility can be a perk. CRONE: You want to pay it socially so everybody knows that you're not a deadbeat, and you've met your obligation. CHESLOW: PayPal CEO Dan Schulman says Venmo is the app for a generation that grew up on social media. And he says the public feed is the essence of the app. Here he is on CNBC. (SOUNDBITE OF ARCHIVED RECORDING) DAN SCHULMAN: It's really a social experience. Like, you do a payment. You tag it. You put an emoji next to it. You share it with your friends. CHESLOW: Nineteen billion dollars changed hands over the app just between October and December of last year. That's up 80 percent from the previous year. Not everyone knows others are seeing their payments. The Federal Trade Commission last year demanded that Venmo make it clear to customers that these transactions are public. Venmo says it never posts the amount of the transaction, and anyone can make their payments private. Private or not, the app has made it easy to give cash instantly. We heard from a woman in Baltimore. She picked up a $350 grocery bill for a friend who forgot her wallet but Venmo'd the money immediately. A man in San Francisco told us he Venmos birthday money to his friends for a round of drinks. But others noticed when it's so easy to split a bill, it becomes an expectation. MATTHEW MASOUD: I'm definitely less generous when I go out with my friends. CHESLOW: Matthew Masoud studies aerospace engineering at the University of Cincinnati. He says via Skype that he went out to dinner at a restaurant that wouldn't split the check. So he just paid for everyone, and then he sent out Venmo requests. MASOUD: One of my friends ordered pasta dish, I believe. And that one was, like, $13. CHESLOW: Before Venmo, Masoud said he would have just taken care of the whole bill, and next time someone else would. If Venmo has changed Masoud's habits, it doesn't seem to have changed Kelli Johnson. Even after her mom discovered the live-in boyfriend, Johnson says she didn't change her privacy settings. JOHNSON: No, I haven't. CHESLOW: Are you kidding? JOHNSON: I have nothing to hide now (laughter). CHESLOW: Daniella Cheslow, NPR News, Washington.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-26-689740061": {"title": "Tell Us: Has A Social Media Post Cost You A Job?  : NPR", "url": "https://www.npr.org/2019/02/26/689740061/tell-us-has-a-social-media-post-cost-you-a-job", "author": "No author found", "published_date": "2019-02-26", "content": "", "section": "National", "disclaimer": ""}, "2019-02-27-698700499": {"title": "Apps Give Private Data To Facebook Without User's Knowledge or Permission : NPR", "url": "https://www.npr.org/2019/02/27/698700499/apps-give-private-data-to-facebook-without-users-knowledge-or-permission", "author": "No author found", "published_date": "2019-02-27", "content": "MARY LOUISE KELLY, HOST: Let's dig deeper now into how some of these apps are sharing users' data without their knowledge. Laura mentioned The Wall Street Journal just there. It recently published another story headlined \"You Give Apps Sensitive Personal Information. Then They Tell Facebook. \" Sam Schechner is one of the reporters on the story, and I asked him what sensitive personal information we're talking about here. SAM SCHECHNER: Well, it could be your weight, if you're having your period, your height, your blood pressure. We saw all of that kind of information being transferred from apps directly to Facebook servers in testing that we ran over the last few months. KELLY: Yeah, you give an example of an app that allows women to track when they're getting their period and ovulation. They enter that in, and then it immediately gets fed straight over to Facebook. SCHECHNER: Yeah. What we saw - and this was actually part of what set off the investigation. While we were doing the testing, I was entering information to the app, and I saw that it was immediately sending a notification that I had altered the dates of my period to Facebook. KELLY: Your virtual period. I assume - (laughter) I'll make a wild leap and assume here. SCHECHNER: Sending the dates of my virtual period. I was using the app even though I don't get one. And in addition, it would send a notification to Facebook when you entered pregnancy mode. The app would show kind of confetti on the screen. But behind the scenes, the app was informing Facebook that it was now in pregnancy status. KELLY: Here's the sentence from your article that stopped me cold. I'm just going to read it. (Reading) The social media giant collects intensely personal information from many popular smartphone apps just seconds after users enter it even if the user has no connection to Facebook. Really? I mean, even if I don't have a Facebook account, this is happening. SCHECHNER: Yes, that is correct. And the reason is 'cause apps build in software from Facebook in order to do all kinds of things, including to track their users' behavior. And that software sends the data back to Facebook regardless of whether or not you're a user. In fact, the app doesn't have any way of knowing whether you're a user when it sends the data. KELLY: And what does Facebook say they are doing with this data? SCHECHNER: Facebook says that they offer services to the developers that send it. They offer analytic services so you can see how users are interacting with that app. And they allow the app developer to then target users of the app on Facebook properties with ads. It's worth noting, however, that Facebook's terms of service give it wide latitude to use that information for other purposes, such as targeting ads more generally, for personalizing their service, including the news feed, and for research and development. KELLY: Does it appear based on your reporting that regulators are sitting up and paying attention? SCHECHNER: Well, already New York Governor Andrew Cuomo has directed state agencies to look into the matter. And already since our report, at least five of the apps that we highlighted have stopped sending the information that we highlighted to Facebook. And Facebook has sent out letters to those apps and other major app developers telling them to stop sending any health-related information or other potentially sensitive information. KELLY: Did you find yourself changing settings or deleting apps as you reported this out? SCHECHNER: I definitely did. I advised my wife to use a different app to track her own cycle, and I certainly made sure that, you know, when I exercise, I'm using apps that didn't in my testing turn up to be sending this specific data. Of course I am a tech reporter, not a, you know, software engineer, so the likelihood is that I'm still being tracked. And in fact when I go on my phone, I see plenty of ads for exercise apps probably from the fact that I just went running. KELLY: Wall Street Journal reporter Sam Schechner, thanks so much. SCHECHNER: Thanks for having me. [EDITOR'S NOTE on March 1, 2019: For the record, Facebook is among NPR\u2019s financial supporters. ] MARY LOUISE KELLY, HOST:  Let's dig deeper now into how some of these apps are sharing users' data without their knowledge. Laura mentioned The Wall Street Journal just there. It recently published another story headlined \"You Give Apps Sensitive Personal Information. Then They Tell Facebook. \" Sam Schechner is one of the reporters on the story, and I asked him what sensitive personal information we're talking about here. SAM SCHECHNER: Well, it could be your weight, if you're having your period, your height, your blood pressure. We saw all of that kind of information being transferred from apps directly to Facebook servers in testing that we ran over the last few months. KELLY: Yeah, you give an example of an app that allows women to track when they're getting their period and ovulation. They enter that in, and then it immediately gets fed straight over to Facebook. SCHECHNER: Yeah. What we saw - and this was actually part of what set off the investigation. While we were doing the testing, I was entering information to the app, and I saw that it was immediately sending a notification that I had altered the dates of my period to Facebook. KELLY: Your virtual period. I assume - (laughter) I'll make a wild leap and assume here. SCHECHNER: Sending the dates of my virtual period. I was using the app even though I don't get one. And in addition, it would send a notification to Facebook when you entered pregnancy mode. The app would show kind of confetti on the screen. But behind the scenes, the app was informing Facebook that it was now in pregnancy status. KELLY: Here's the sentence from your article that stopped me cold. I'm just going to read it. (Reading) The social media giant collects intensely personal information from many popular smartphone apps just seconds after users enter it even if the user has no connection to Facebook. Really? I mean, even if I don't have a Facebook account, this is happening. SCHECHNER: Yes, that is correct. And the reason is 'cause apps build in software from Facebook in order to do all kinds of things, including to track their users' behavior. And that software sends the data back to Facebook regardless of whether or not you're a user. In fact, the app doesn't have any way of knowing whether you're a user when it sends the data. KELLY: And what does Facebook say they are doing with this data? SCHECHNER: Facebook says that they offer services to the developers that send it. They offer analytic services so you can see how users are interacting with that app. And they allow the app developer to then target users of the app on Facebook properties with ads. It's worth noting, however, that Facebook's terms of service give it wide latitude to use that information for other purposes, such as targeting ads more generally, for personalizing their service, including the news feed, and for research and development. KELLY: Does it appear based on your reporting that regulators are sitting up and paying attention? SCHECHNER: Well, already New York Governor Andrew Cuomo has directed state agencies to look into the matter. And already since our report, at least five of the apps that we highlighted have stopped sending the information that we highlighted to Facebook. And Facebook has sent out letters to those apps and other major app developers telling them to stop sending any health-related information or other potentially sensitive information. KELLY: Did you find yourself changing settings or deleting apps as you reported this out? SCHECHNER: I definitely did. I advised my wife to use a different app to track her own cycle, and I certainly made sure that, you know, when I exercise, I'm using apps that didn't in my testing turn up to be sending this specific data. Of course I am a tech reporter, not a, you know, software engineer, so the likelihood is that I'm still being tracked. And in fact when I go on my phone, I see plenty of ads for exercise apps probably from the fact that I just went running. KELLY: Wall Street Journal reporter Sam Schechner, thanks so much. SCHECHNER: Thanks for having me. [EDITOR'S NOTE on March 1, 2019: For the record, Facebook is among NPR\u2019s financial supporters. ]", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-27-698562621": {"title": "Russia Says U.S. Cyberattacks Show A Need For Its Own Internet  : NPR", "url": "https://www.npr.org/2019/02/27/698562621/kremlin-says-u-s-cyberattacks-on-russia-are-the-reality-we-live-in", "author": "No author found", "published_date": "2019-02-27", "content": "", "section": "Technology", "disclaimer": ""}, "2019-02-27-697026827": {"title": "Trusting Apple With Health Records. Can Apple Live Up To Privacy Values? : NPR", "url": "https://www.npr.org/2019/02/27/697026827/storing-health-records-on-your-phone-can-apple-live-up-to-its-privacy-values", "author": "No author found", "published_date": "2019-02-27", "content": "ARI SHAPIRO, HOST: Apple is one of several big tech companies trying to get into the health care business, but Apple first has to convince consumers that its health apps are a safer choice when it comes to privacy. NPR's Laura Sydell reports. LAURA SYDELL, BYLINE: 2018 wasn't a great year for Apple; iPhone sales slowed. There's chatter among analysts that its best days of innovation are behind it. Yet Apple has been innovating in ways that go beyond its new hardware as it expands its ambitions into health care. Sam Cavaliere of San Diego is starting to rely on Apple's health app. SAM CAVALIERE: I'm in average health. I can always stand to lose a little weight. SYDELL: Cavaliere works in tech and sees doctors at University of California San Diego Health. He travels a lot for work and likes to keep track of his blood pressure. The app helps him do that. He also decided to let the app access his medical records. CAVALIERE: When I go to the doctor, in addition to my records from there, I get my blood pressure results that I've taken myself, and they can see that and compare it to what they are doing in the office so that they get a bigger picture than just the once or twice a year that I show up at the office. SYDELL: Many of us use our smartphones to track exercise steps and nutrition, but having your health care records inside an app goes a step beyond. The app can record medications taken, visits to psychiatrists, treatments for diseases, stuff you might not want employers, insurers or advertisers to know. Apple first released its own health app in 2014. It's different from the third party health apps that can be downloaded on an iPhone, but it was a big step for Apple. And last March, Apple started coordinating with health care providers like UC San Diego to transfer health records into its app. The advantage to consumers is keeping all their records in one place on one device. UC San Diego Health's chief information officer Dr. Chris Longhurst says Apple's privacy features made them feel more at ease. CHRIS LONGHURST: This data did not go to the cloud. It only resided on the user's device. It is encrypted and is only accessible with user permission. Nothing is more important than keeping the privacy of our patients' health information. SYDELL: Apple can't access the encrypted health records without permission from the user. The health records feature is now used by more than 200 providers around the country. It is subject to strict federal privacy laws, and it's part of how Apple has been trying to distinguish itself as committed to privacy. And that could pay off big in health care. CEO Tim Cook has become a vocal critic of rivals like Google and Facebook for selling ads off user data. In an interview with NPR, Cook says that's something Apple has avoided. (SOUNDBITE OF ARCHIVED BROADCAST)TIM COOK: People will look at this and feel that they can trust Apple, and that's a key part of anyone that you're working with on your health. The reality is that I know for me, I want to do business with people that have my health data, people that I deeply trust and that I have high levels of confidence in. SYDELL: But at a time of heightened scrutiny over how tech companies protect user privacy, Apple, too, has made mistakes. The Wall Street Journal found several top health and fitness apps available on iPhones sent personal information such as heart rate data to Facebook. But those apps don't connect to medical records, and apps that do must undergo heightened scrutiny by Apple. Still, news like this could threaten to undo Apple's reputation for data privacy. Cavaliere of San Diego says Apple has managed to gain his trust because he says the company doesn't treat him like a commodity. CAVALIERE: Because I don't get fed advertisements for them, so I don't see them trying to monetize it, whereas other companies will do that now. But with what Apple has been doing, I feel comfortable with how they're doing it and what they're doing. SYDELL: Of course for Apple to succeed, it needs to convince people that they are a safe choice to protect medical records and won't treat their customers as fodder for advertisers. Laura Sydell, NPR News. ARI SHAPIRO, HOST:  Apple is one of several big tech companies trying to get into the health care business, but Apple first has to convince consumers that its health apps are a safer choice when it comes to privacy. NPR's Laura Sydell reports. LAURA SYDELL, BYLINE: 2018 wasn't a great year for Apple; iPhone sales slowed. There's chatter among analysts that its best days of innovation are behind it. Yet Apple has been innovating in ways that go beyond its new hardware as it expands its ambitions into health care. Sam Cavaliere of San Diego is starting to rely on Apple's health app. SAM CAVALIERE: I'm in average health. I can always stand to lose a little weight. SYDELL: Cavaliere works in tech and sees doctors at University of California San Diego Health. He travels a lot for work and likes to keep track of his blood pressure. The app helps him do that. He also decided to let the app access his medical records. CAVALIERE: When I go to the doctor, in addition to my records from there, I get my blood pressure results that I've taken myself, and they can see that and compare it to what they are doing in the office so that they get a bigger picture than just the once or twice a year that I show up at the office. SYDELL: Many of us use our smartphones to track exercise steps and nutrition, but having your health care records inside an app goes a step beyond. The app can record medications taken, visits to psychiatrists, treatments for diseases, stuff you might not want employers, insurers or advertisers to know. Apple first released its own health app in 2014. It's different from the third party health apps that can be downloaded on an iPhone, but it was a big step for Apple. And last March, Apple started coordinating with health care providers like UC San Diego to transfer health records into its app. The advantage to consumers is keeping all their records in one place on one device. UC San Diego Health's chief information officer Dr. Chris Longhurst says Apple's privacy features made them feel more at ease. CHRIS LONGHURST: This data did not go to the cloud. It only resided on the user's device. It is encrypted and is only accessible with user permission. Nothing is more important than keeping the privacy of our patients' health information. SYDELL: Apple can't access the encrypted health records without permission from the user. The health records feature is now used by more than 200 providers around the country. It is subject to strict federal privacy laws, and it's part of how Apple has been trying to distinguish itself as committed to privacy. And that could pay off big in health care. CEO Tim Cook has become a vocal critic of rivals like Google and Facebook for selling ads off user data. In an interview with NPR, Cook says that's something Apple has avoided. (SOUNDBITE OF ARCHIVED BROADCAST) TIM COOK: People will look at this and feel that they can trust Apple, and that's a key part of anyone that you're working with on your health. The reality is that I know for me, I want to do business with people that have my health data, people that I deeply trust and that I have high levels of confidence in. SYDELL: But at a time of heightened scrutiny over how tech companies protect user privacy, Apple, too, has made mistakes. The Wall Street Journal found several top health and fitness apps available on iPhones sent personal information such as heart rate data to Facebook. But those apps don't connect to medical records, and apps that do must undergo heightened scrutiny by Apple. Still, news like this could threaten to undo Apple's reputation for data privacy. Cavaliere of San Diego says Apple has managed to gain his trust because he says the company doesn't treat him like a commodity. CAVALIERE: Because I don't get fed advertisements for them, so I don't see them trying to monetize it, whereas other companies will do that now. But with what Apple has been doing, I feel comfortable with how they're doing it and what they're doing. SYDELL: Of course for Apple to succeed, it needs to convince people that they are a safe choice to protect medical records and won't treat their customers as fodder for advertisers. Laura Sydell, NPR News.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-02-28-698891898": {"title": "Alleged Russian Spy Arrested In Stockholm : NPR", "url": "https://www.npr.org/2019/02/28/698891898/sweden-arrests-suspected-russian-spy", "author": "No author found", "published_date": "2019-02-28", "content": "", "section": "Europe", "disclaimer": ""}, "2019-03-01-699307312": {"title": "Huawei Tries To Win Over U.S. Media And Public : NPR", "url": "https://www.npr.org/2019/03/01/699307312/huawei-broadens-its-campaign-to-win-over-american-public-and-media", "author": "No author found", "published_date": "2019-03-01", "content": "", "section": "Technology", "disclaimer": ""}, "2019-03-01-699451897": {"title": "Huawei Executive's Extradition Hearing Can Proceed, Canada Says : NPR", "url": "https://www.npr.org/2019/03/01/699451897/canada-says-extradition-hearing-for-huawei-executive-meng-wanzhou-can-proceed", "author": "No author found", "published_date": "2019-03-01", "content": "", "section": "National Security", "disclaimer": ""}, "2019-03-01-699282111": {"title": "YouTube Bans Comments On Videos Deemed Vulnerable To Pedophiles Amid Ad Pullback : NPR", "url": "https://www.npr.org/2019/03/01/699282111/youtube-bans-comments-on-videos-deemed-vulnerable-to-pedophiles", "author": "No author found", "published_date": "2019-03-01", "content": "", "section": "Media", "disclaimer": ""}, "2019-03-02-699663284": {"title": "The Working Lives Of Facebook's Content Moderators : NPR", "url": "https://www.npr.org/2019/03/02/699663284/the-working-lives-of-facebooks-content-moderators", "author": "No author found", "published_date": "2019-03-02", "content": "SCOTT SIMON, HOST: Facebook has pledged to do better at moderating content. The social media company usually employs third-party contractors to do the job. The average moderator makes about $28,000 a year. Meanwhile, the average Facebook employee's salary is around $120,000 a year. And we want to note here that Facebook is a financial supporter of NPR. In a recent article by Casey Newton for The Verge, moderators employed by one of those contractors, Cognizant, talked about the stress of their jobs - not only low pay but high-pressure working conditions and the emotional toll of monitoring hour after hour of graphic content and conspiracy theories. Casey Newton, Silicon Valley editor at The Verge, joins us now from New York. Thanks so much for being with us. CASEY NEWTON: Thanks for having me, Scott. SIMON: Well, help us understand how a lot of these employees live during the workday. NEWTON: Well, every piece of content that gets reported on Facebook needs to be evaluated to see if it breaks the rules or not. And if a moderator makes the wrong call more than a handful of times during the week, their job could be at risk. And so the folks that I spoke with said that they're just under tremendous pressure to try to get it right even though Facebook is changing those guidelines on a near daily basis to account for some nuance. And, of course, a lot of that content they're looking at is extremely graphic or disturbing. And so many of the folks that I spoke with were struggling with mental health issues months after they left the job. SIMON: Because they have to see so much? NEWTON: That's right. You know, there are people in the world who spend a lot of time just sort of uploading the worst of humanity onto Facebook. So almost everyone I spoke with could vividly describe for me at least one thing they saw that continue to haunt them. SIMON: And it sounds as if during their workday, there's not a lot of time to reflect. There's not even really time to go to the bathroom. NEWTON: That's right. One of the things that surprised me most about this story was that the moderators' time is managed down to the second. Every time they want to use the bathroom, they have to click a browser extension to let someone know that they're leaving. They also get nine minutes a day of something called wellness time, which they're supposed to use if they see something really traumatizing and need to stand up and walk away. But many of the folks that I spoke with said that wasn't really adequate to kind of emotionally process what they were seeing. SIMON: What about the effect of seeing so many conspiracy theories? NEWTON: Well - so this was maybe the thing that surprised me the most from my reporting was the majority of the people that I spoke with said that the longer they looked at the kind of fringe conspiracies that get posted on to Facebook, the more they found themselves sympathetic to those ideas. So I spoke to one man who told me that he no longer believes that 9/11 was a terrorist attack. I talked to someone else who said they had begun to question the reality of the Holocaust. And in some cases, these folks knew sort of how wrong that sounded. But they just kept telling me these videos are so persuasive, and we see them all the time. SIMON: Let me share with you some words we got from Facebook, knowing we were going to interview you. We work with our partners to ensure they provide competitive compensation starting at $15 per hour, benefits and a high level of support for their employees. They went on to say that they will regularly audit their partners. They'll try to make working conditions and salaries uniform. And they're going to hold a summit on those issues and talk to employees. How do you react to their statement? NEWTON: Well, I'm glad to hear that Facebook is taking these issues seriously. I would say if they're looking for suggestions, I'm happy to offer two. One would be to pay these folks more. And I think that would be a great place for Facebook to start when it came to compensating employees, who, in many cases, are being asked to evaluate essential questions of speech and security. They're policing the terms of our public debate. That feels like a $60,000-a-year job to me. And then the second thing they could do is just not make these employees have to raise their hand every time they want to go to the bathroom. Just treat these employees the way they treat any Facebook executive, and let them manage their own time. SIMON: Casey Newton at The Verge, thanks so much for being with us. NEWTON: Thank you, Scott. SCOTT SIMON, HOST:  Facebook has pledged to do better at moderating content. The social media company usually employs third-party contractors to do the job. The average moderator makes about $28,000 a year. Meanwhile, the average Facebook employee's salary is around $120,000 a year. And we want to note here that Facebook is a financial supporter of NPR. In a recent article by Casey Newton for The Verge, moderators employed by one of those contractors, Cognizant, talked about the stress of their jobs - not only low pay but high-pressure working conditions and the emotional toll of monitoring hour after hour of graphic content and conspiracy theories. Casey Newton, Silicon Valley editor at The Verge, joins us now from New York. Thanks so much for being with us. CASEY NEWTON: Thanks for having me, Scott. SIMON: Well, help us understand how a lot of these employees live during the workday. NEWTON: Well, every piece of content that gets reported on Facebook needs to be evaluated to see if it breaks the rules or not. And if a moderator makes the wrong call more than a handful of times during the week, their job could be at risk. And so the folks that I spoke with said that they're just under tremendous pressure to try to get it right even though Facebook is changing those guidelines on a near daily basis to account for some nuance. And, of course, a lot of that content they're looking at is extremely graphic or disturbing. And so many of the folks that I spoke with were struggling with mental health issues months after they left the job. SIMON: Because they have to see so much? NEWTON: That's right. You know, there are people in the world who spend a lot of time just sort of uploading the worst of humanity onto Facebook. So almost everyone I spoke with could vividly describe for me at least one thing they saw that continue to haunt them. SIMON: And it sounds as if during their workday, there's not a lot of time to reflect. There's not even really time to go to the bathroom. NEWTON: That's right. One of the things that surprised me most about this story was that the moderators' time is managed down to the second. Every time they want to use the bathroom, they have to click a browser extension to let someone know that they're leaving. They also get nine minutes a day of something called wellness time, which they're supposed to use if they see something really traumatizing and need to stand up and walk away. But many of the folks that I spoke with said that wasn't really adequate to kind of emotionally process what they were seeing. SIMON: What about the effect of seeing so many conspiracy theories? NEWTON: Well - so this was maybe the thing that surprised me the most from my reporting was the majority of the people that I spoke with said that the longer they looked at the kind of fringe conspiracies that get posted on to Facebook, the more they found themselves sympathetic to those ideas. So I spoke to one man who told me that he no longer believes that 9/11 was a terrorist attack. I talked to someone else who said they had begun to question the reality of the Holocaust. And in some cases, these folks knew sort of how wrong that sounded. But they just kept telling me these videos are so persuasive, and we see them all the time. SIMON: Let me share with you some words we got from Facebook, knowing we were going to interview you. We work with our partners to ensure they provide competitive compensation starting at $15 per hour, benefits and a high level of support for their employees. They went on to say that they will regularly audit their partners. They'll try to make working conditions and salaries uniform. And they're going to hold a summit on those issues and talk to employees. How do you react to their statement? NEWTON: Well, I'm glad to hear that Facebook is taking these issues seriously. I would say if they're looking for suggestions, I'm happy to offer two. One would be to pay these folks more. And I think that would be a great place for Facebook to start when it came to compensating employees, who, in many cases, are being asked to evaluate essential questions of speech and security. They're policing the terms of our public debate. That feels like a $60,000-a-year job to me. And then the second thing they could do is just not make these employees have to raise their hand every time they want to go to the bathroom. Just treat these employees the way they treat any Facebook executive, and let them manage their own time. SIMON: Casey Newton at The Verge, thanks so much for being with us. NEWTON: Thank you, Scott.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-02-699663319": {"title": "The Latest Internet Hoax: 'Momo Challenge' : NPR", "url": "https://www.npr.org/2019/03/02/699663319/the-latest-internet-hoax-momo-challenge", "author": "No author found", "published_date": "2019-03-02", "content": "SCOTT SIMON, HOST: Now a public service announcement. The Momo challenge appears to be just a hoax. Laura Hazard Owen is a reporter at Nieman Lab. She joins us now on the line. Thanks so much for being with us. LAURA HAZARD OWEN: Thanks for having me. SIMON: There's been almost - I think I can fairly use the word - panic over these YouTube videos, where this disturbing-looking character, Momo, tells youngsters to harm themselves. It's not real? OWEN: It is not real. So these videos don't exist. The hoax is that it pops up in the middle of a YouTube video that your kids are watching and induces them to join this challenge where they commit suicide, or at least, like, that's one version of. . . SIMON: Yeah. OWEN: . . . The story that's going around. And YouTube says that it has no evidence of these videos existing. We have no screenshots. We have no, you know, like, video clips. There's no proof that this is a real thing. It's not a real thing. SIMON: So is it a hoax or mass hysteria? OWEN: I think it's a mass sort of parental panic, but it's being fueled by mainstream news organizations, which is what makes it a little different. SIMON: Well, and it's only fair to ask. So you and I are doing an interview to emphasize that this threat is not real. Are we, at the same time, promoting this hoax? OWEN: Possibly, just by talking about it, maybe we are. But I think that it sort of needs to be said at this point because, actually, a lot of the coverage that I'm seeing is kind of along the lines of the sort of both sides. Some say it's a hoax. You know, some say it's a viral challenge that's inducing kids to kill themselves. And it needs to be said that this is not a real thing. SIMON: Yeah. How does this compare to other things that have caught fire in recent months? I'm thinking of, you know, the Tide Pods - whatever that was. OWEN: I think that this is extra believable and problematic for a couple of reasons. One is that the image itself - the Momo image is memorable and is scary to a lot of people. I mean, there is this real image that is sticking in people's minds. The second thing that's different about it is that it's true that bad actors use YouTube to prey on kids. And so even if the Momo challenge is a hoax, it fits in with that belief that YouTube is a bad place for people to be spending time. And then finally, people feel really guilty about, you know, screen time for kids. And the guidelines are always changing. There's all this scary research out there. We don't know what it's doing to them. We, as parents, probably feel bad about how much time we're spending on our phones. And so that all plays into it. SIMON: Laura Hazard Owen is a reporter at Nieman Lab. Thanks so much for being with us. OWEN: Thanks for having me. SCOTT SIMON, HOST:  Now a public service announcement. The Momo challenge appears to be just a hoax. Laura Hazard Owen is a reporter at Nieman Lab. She joins us now on the line. Thanks so much for being with us. LAURA HAZARD OWEN: Thanks for having me. SIMON: There's been almost - I think I can fairly use the word - panic over these YouTube videos, where this disturbing-looking character, Momo, tells youngsters to harm themselves. It's not real? OWEN: It is not real. So these videos don't exist. The hoax is that it pops up in the middle of a YouTube video that your kids are watching and induces them to join this challenge where they commit suicide, or at least, like, that's one version of. . . SIMON: Yeah. OWEN: . . . The story that's going around. And YouTube says that it has no evidence of these videos existing. We have no screenshots. We have no, you know, like, video clips. There's no proof that this is a real thing. It's not a real thing. SIMON: So is it a hoax or mass hysteria? OWEN: I think it's a mass sort of parental panic, but it's being fueled by mainstream news organizations, which is what makes it a little different. SIMON: Well, and it's only fair to ask. So you and I are doing an interview to emphasize that this threat is not real. Are we, at the same time, promoting this hoax? OWEN: Possibly, just by talking about it, maybe we are. But I think that it sort of needs to be said at this point because, actually, a lot of the coverage that I'm seeing is kind of along the lines of the sort of both sides. Some say it's a hoax. You know, some say it's a viral challenge that's inducing kids to kill themselves. And it needs to be said that this is not a real thing. SIMON: Yeah. How does this compare to other things that have caught fire in recent months? I'm thinking of, you know, the Tide Pods - whatever that was. OWEN: I think that this is extra believable and problematic for a couple of reasons. One is that the image itself - the Momo image is memorable and is scary to a lot of people. I mean, there is this real image that is sticking in people's minds. The second thing that's different about it is that it's true that bad actors use YouTube to prey on kids. And so even if the Momo challenge is a hoax, it fits in with that belief that YouTube is a bad place for people to be spending time. And then finally, people feel really guilty about, you know, screen time for kids. And the guidelines are always changing. There's all this scary research out there. We don't know what it's doing to them. We, as parents, probably feel bad about how much time we're spending on our phones. And so that all plays into it. SIMON: Laura Hazard Owen is a reporter at Nieman Lab. Thanks so much for being with us. OWEN: Thanks for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-03-699797307": {"title": "Silicon Valley Raises Questions On Ethics Of New Technology And Social Media : NPR", "url": "https://www.npr.org/2019/03/03/699797307/the-ethics-of-new-technology", "author": "No author found", "published_date": "2019-03-03", "content": "LULU GARCIA-NAVARRO, HOST: Tech companies decide to kill products and ideas for products all the time. Maybe it doesn't work; there's no market - whatever. But what about when the tech is too dangerous? That's what Caterina Fake wants Silicon Valley to consider more often, the ethics of new tech. She hosts the podcast \"Should This Exist? \" And she joins us now from San Francisco. Thanks so much for being on WEEKEND EDITION. CATERINA FAKE: And thanks for having me. GARCIA-NAVARRO: So you think about this all the time. But we thought about it because OpenAI, the research nonprofit, announced they weren't releasing a text generator they developed because they feared it could be misused to create fake news. Did that admission take you by surprise? FAKE: Tech companies don't launch products all the time. But it's rare that they announce that they're not launching a product, which is what has happened here. And the announcement of not launching this product is basically to involve people in the conversation around what is and what is not dangerous tech. GARCIA-NAVARRO: You are in Silicon Valley. You're the co-founder of Flickr. One of the things that always struck me was at the beginning, the conversation among developers there was always, like, we are doing this for the greater good. This is part of a good for society. Has that conversation changed? FAKE: I think it has. For example, when we had first started Flickr, we kind of understood that what we were building was online community. Online community is something where you show up. You are yourself. You have to participate. And you have to negotiate the culture of the community in which you are participating. In a social media platform, you are our so-called eyeballs. You are a product that is being sold to advertisers. It's a completely different dynamic. And when things switched from being very early on thought of as online community to being thought of as social media, the dynamics of the entire software changed. GARCIA-NAVARRO: So in your podcast, \"Should This Exist? ,\" I mean, what kinds of questions are you grappling with right now? I'm sure you see things all the time that are being promoted, and you think nah, this isn't going. . . FAKE: That should. . . GARCIA-NAVARRO: . . . To be a good. FAKE: . . . Not exist. GARCIA-NAVARRO: That should not exist. FAKE: (Laughter). I think it would be wonderful if I were the sole arbiter of what should and should not exist. GARCIA-NAVARRO: Me too. FAKE: But really, we have been talking about this in the Valley forever. But it was not getting a lot of attention. And there was a kind of a catastrophic change that happened I think when, unexpectedly, to many of us, the 2016 elections had results that we did not anticipate for reasons that we could suddenly see. GARCIA-NAVARRO: What do you say to people, though, who say that any product or platform can be misused and that it's impossible to plan for every eventuality? FAKE: The important part of this is to acculturate people to asking these questions. And as we all know, millennials and Gen Z and the younger folk that are now, you know, kind of coming into their own, are much more thoughtful about, what are the values behind this product or this program? And what does it do to us? For example, one of the last shows was about a product called Woebot. And what it is, it's an AI-driven bot therapist. And as we know, depression has increased, which has followed very closely the introduction of technology into our lives. GARCIA-NAVARRO: There's studies that show that kids, specifically that are on a lot of technology and social media, feel more depressed, more alienated from their peers, et cetera. FAKE: Yes. And my initial impulse was, gosh, should we use technology to cure the problems of technology? That seems misguided. But by the end of thinking through some of the, you know, possibilities of this technology, I became convinced that, in fact, this was probably a good solution for it. I feel as if technology can always be used for good, right? It has neutral valence. It is the way that humans use it and how we approach it and how we think about it. That is what is the most important part of technology and technology in our lives. GARCIA-NAVARRO: Caterina Fake is a co-founder of Flickr, a venture capitalist and a host of the podcast \"Should This Exist? \" Thank you so much. FAKE: Thank you. (SOUNDBITE OF MYLAB'S \"FANCY PARTY CAKES\") LULU GARCIA-NAVARRO, HOST:  Tech companies decide to kill products and ideas for products all the time. Maybe it doesn't work; there's no market - whatever. But what about when the tech is too dangerous? That's what Caterina Fake wants Silicon Valley to consider more often, the ethics of new tech. She hosts the podcast \"Should This Exist? \" And she joins us now from San Francisco. Thanks so much for being on WEEKEND EDITION. CATERINA FAKE: And thanks for having me. GARCIA-NAVARRO: So you think about this all the time. But we thought about it because OpenAI, the research nonprofit, announced they weren't releasing a text generator they developed because they feared it could be misused to create fake news. Did that admission take you by surprise? FAKE: Tech companies don't launch products all the time. But it's rare that they announce that they're not launching a product, which is what has happened here. And the announcement of not launching this product is basically to involve people in the conversation around what is and what is not dangerous tech. GARCIA-NAVARRO: You are in Silicon Valley. You're the co-founder of Flickr. One of the things that always struck me was at the beginning, the conversation among developers there was always, like, we are doing this for the greater good. This is part of a good for society. Has that conversation changed? FAKE: I think it has. For example, when we had first started Flickr, we kind of understood that what we were building was online community. Online community is something where you show up. You are yourself. You have to participate. And you have to negotiate the culture of the community in which you are participating. In a social media platform, you are our so-called eyeballs. You are a product that is being sold to advertisers. It's a completely different dynamic. And when things switched from being very early on thought of as online community to being thought of as social media, the dynamics of the entire software changed. GARCIA-NAVARRO: So in your podcast, \"Should This Exist? ,\" I mean, what kinds of questions are you grappling with right now? I'm sure you see things all the time that are being promoted, and you think nah, this isn't going. . . FAKE: That should. . . GARCIA-NAVARRO: . . . To be a good. FAKE: . . . Not exist. GARCIA-NAVARRO: That should not exist. FAKE: (Laughter). I think it would be wonderful if I were the sole arbiter of what should and should not exist. GARCIA-NAVARRO: Me too. FAKE: But really, we have been talking about this in the Valley forever. But it was not getting a lot of attention. And there was a kind of a catastrophic change that happened I think when, unexpectedly, to many of us, the 2016 elections had results that we did not anticipate for reasons that we could suddenly see. GARCIA-NAVARRO: What do you say to people, though, who say that any product or platform can be misused and that it's impossible to plan for every eventuality? FAKE: The important part of this is to acculturate people to asking these questions. And as we all know, millennials and Gen Z and the younger folk that are now, you know, kind of coming into their own, are much more thoughtful about, what are the values behind this product or this program? And what does it do to us? For example, one of the last shows was about a product called Woebot. And what it is, it's an AI-driven bot therapist. And as we know, depression has increased, which has followed very closely the introduction of technology into our lives. GARCIA-NAVARRO: There's studies that show that kids, specifically that are on a lot of technology and social media, feel more depressed, more alienated from their peers, et cetera. FAKE: Yes. And my initial impulse was, gosh, should we use technology to cure the problems of technology? That seems misguided. But by the end of thinking through some of the, you know, possibilities of this technology, I became convinced that, in fact, this was probably a good solution for it. I feel as if technology can always be used for good, right? It has neutral valence. It is the way that humans use it and how we approach it and how we think about it. That is what is the most important part of technology and technology in our lives. GARCIA-NAVARRO: Caterina Fake is a co-founder of Flickr, a venture capitalist and a host of the podcast \"Should This Exist? \" Thank you so much. FAKE: Thank you. (SOUNDBITE OF MYLAB'S \"FANCY PARTY CAKES\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-04-699953310": {"title": "Huawei Exec Sues Canada, Argues December Arrest Violated Constitution : NPR", "url": "https://www.npr.org/2019/03/04/699953310/huawei-exec-sues-canada-argues-december-arrest-violated-constitution", "author": "No author found", "published_date": "2019-03-04", "content": "", "section": "World", "disclaimer": ""}, "2019-03-05-700355632": {"title": "5G Coming To A City Near You. Fastest Wireless Yet Will Bring New Services : NPR", "url": "https://www.npr.org/2019/03/05/700355632/coming-to-a-city-near-you-5g-fastest-wireless-yet-will-bring-new-services", "author": "No author found", "published_date": "2019-03-05", "content": "MARY LOUISE KELLY, HOST: Now, if you're like me, you may not know exactly what 5G is, the so-called fifth generation technology. But we are all going to be hearing a lot about it very soon. An international race is underway to control this emerging technology. So we are digging in on this month's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")KELLY: 5G promises faster Internet, better video streaming and real-time traffic reports. As NPR's Laura Sydell reports, some experts say 5G will also propel innovation in transportation, manufacturing, medicine and in areas we still can't anticipate. LAURA SYDELL, BYLINE: There is a lot of buzz around 5G. Not many people have seen it in action. I'm lucky enough to get a demo. What do we got here? JOHN MACIAS: What we have is a basic speed test that will do a series of downloads to assess the downlink. SYDELL: John Macias, an assistant performance manager for Verizon, is showing me a speed test on a laptop in his truck. We're in a suburban neighborhood of Sacramento, Calif. , one of only four cities where Verizon has deployed 5G. Macias says people will really enjoy 5G speeds when it comes to downloads. MACIAS: Imagine downloading HD movies in a minute rather than half an hour. SYDELL: And 5G is wireless, but it's a bit different from the wireless we're used to. Current cell towers covers several miles, but they are as big as pine trees. 5G cells are about the size of a laptop, and they only cover about a couple thousand feet. But they can easily be installed on telephone and light poles. And 5G brings lightning-fast speed without a fiber cable directly into the home. Verizon Vice President of Network Engineering Phillip French. PHILLIP FRENCH: So you're not ripping up the street as much and you're not trying to go in someone's house and deliver the fiber there. One of the biggest advantages is just that. SYDELL: Sacramento is a mid-sized city of about a half a million people two to three hours from Silicon Valley. Mayor Darrell Steinberg is using 5G as part of his pitch to lure businesses. DARRELL STEINBERG: If you are a small business or an entrepreneur and you are trying to make it in the Bay Area and you can't, don't move to Seattle. Move to Sacramento. SYDELL: Steinberg spoke as we drove over to a 5G cell downtown. He says getting 5G is about more than downloading movies faster. We step out of the car a half block away from a traffic light. The speed of 5G-connected cameras allows real-time monitoring of vehicle and pedestrian traffic. High-speed wireless is going to be important to self-driving cars and trucks, which need to communicate rapidly with each other and traffic signals. STEINBERG: We want to be on the forefront of autonomous vehicle technology. SYDELL: Steinberg also wants to draw more manufacturing. Experts say 5G will send assembly line robots new orders faster. It could enable doctors to examine a patient in an ambulance before they arrive at the hospital. One of the biggest changes for consumers will be when smartphones with 5G connections begin to hit the market later this year. Frank Gillett, a principal analyst with Forrester, says there will be more uses for apps with augmented and virtual reality. FRANK GILLETT: That let you hold the phone up and see an overlay of a Tyrannosaurus rex charging down the street. And because it can quickly send rich pictures down to your phone, the idea is that all that will work magically better. SYDELL: While there are a lot of hopes for 5G, there are some concerns that more Internet-connected devices will create new opportunities for hackers and for mass surveillance. But that's not stopping the wireless companies. AT&T has started a limited rollout of its 5G service. Sprint and T-Mobile will follow. Later this year, Verizon says three new 5G smartphones will be available. But Sacramento Mayor Steinberg is hoping 5G will fix something even more basic. STEINBERG: You know what'd be a real miracle? SYDELL: Yeah. STEINBERG: No dropped calls. SYDELL: Yeah. (LAUGHTER)SYDELL: Now that's dreaming big. Laura Sydell, NPR News. [POST-BROADCAST CORRECTION: In the audio, as in a previous Web version of this story, NPR incorrectly identifies John Macias' title. He is a systems performance manager for Verizon, not an assistant performance manager. ] MARY LOUISE KELLY, HOST:  Now, if you're like me, you may not know exactly what 5G is, the so-called fifth generation technology. But we are all going to be hearing a lot about it very soon. An international race is underway to control this emerging technology. So we are digging in on this month's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") KELLY: 5G promises faster Internet, better video streaming and real-time traffic reports. As NPR's Laura Sydell reports, some experts say 5G will also propel innovation in transportation, manufacturing, medicine and in areas we still can't anticipate. LAURA SYDELL, BYLINE: There is a lot of buzz around 5G. Not many people have seen it in action. I'm lucky enough to get a demo. What do we got here? JOHN MACIAS: What we have is a basic speed test that will do a series of downloads to assess the downlink. SYDELL: John Macias, an assistant performance manager for Verizon, is showing me a speed test on a laptop in his truck. We're in a suburban neighborhood of Sacramento, Calif. , one of only four cities where Verizon has deployed 5G. Macias says people will really enjoy 5G speeds when it comes to downloads. MACIAS: Imagine downloading HD movies in a minute rather than half an hour. SYDELL: And 5G is wireless, but it's a bit different from the wireless we're used to. Current cell towers covers several miles, but they are as big as pine trees. 5G cells are about the size of a laptop, and they only cover about a couple thousand feet. But they can easily be installed on telephone and light poles. And 5G brings lightning-fast speed without a fiber cable directly into the home. Verizon Vice President of Network Engineering Phillip French. PHILLIP FRENCH: So you're not ripping up the street as much and you're not trying to go in someone's house and deliver the fiber there. One of the biggest advantages is just that. SYDELL: Sacramento is a mid-sized city of about a half a million people two to three hours from Silicon Valley. Mayor Darrell Steinberg is using 5G as part of his pitch to lure businesses. DARRELL STEINBERG: If you are a small business or an entrepreneur and you are trying to make it in the Bay Area and you can't, don't move to Seattle. Move to Sacramento. SYDELL: Steinberg spoke as we drove over to a 5G cell downtown. He says getting 5G is about more than downloading movies faster. We step out of the car a half block away from a traffic light. The speed of 5G-connected cameras allows real-time monitoring of vehicle and pedestrian traffic. High-speed wireless is going to be important to self-driving cars and trucks, which need to communicate rapidly with each other and traffic signals. STEINBERG: We want to be on the forefront of autonomous vehicle technology. SYDELL: Steinberg also wants to draw more manufacturing. Experts say 5G will send assembly line robots new orders faster. It could enable doctors to examine a patient in an ambulance before they arrive at the hospital. One of the biggest changes for consumers will be when smartphones with 5G connections begin to hit the market later this year. Frank Gillett, a principal analyst with Forrester, says there will be more uses for apps with augmented and virtual reality. FRANK GILLETT: That let you hold the phone up and see an overlay of a Tyrannosaurus rex charging down the street. And because it can quickly send rich pictures down to your phone, the idea is that all that will work magically better. SYDELL: While there are a lot of hopes for 5G, there are some concerns that more Internet-connected devices will create new opportunities for hackers and for mass surveillance. But that's not stopping the wireless companies. AT&T has started a limited rollout of its 5G service. Sprint and T-Mobile will follow. Later this year, Verizon says three new 5G smartphones will be available. But Sacramento Mayor Steinberg is hoping 5G will fix something even more basic. STEINBERG: You know what'd be a real miracle? SYDELL: Yeah. STEINBERG: No dropped calls. SYDELL: Yeah. (LAUGHTER) SYDELL: Now that's dreaming big. Laura Sydell, NPR News. [POST-BROADCAST CORRECTION: In the audio, as in a previous Web version of this story, NPR incorrectly identifies John Macias' title. He is a systems performance manager for Verizon, not an assistant performance manager. ]", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-05-700288695": {"title": "Google Pay Study Finds It Underpaid Men For Some Jobs  : NPR", "url": "https://www.npr.org/2019/03/05/700288695/google-pay-study-finds-its-underpaying-men-for-some-jobs", "author": "No author found", "published_date": "2019-03-05", "content": "", "section": "Technology", "disclaimer": ""}, "2019-03-05-700289762": {"title": "'The Atlantic': 'When Kids Realize Their Whole Life Is Already Online' : NPR", "url": "https://www.npr.org/2019/03/05/700289762/the-atlantic-when-kids-realize-their-whole-life-is-already-online", "author": "No author found", "published_date": "2019-03-05", "content": "RACHEL MARTIN, HOST: We have all seen them - a pregnancy announcement on Facebook accompanied by a sonogram photo, or an Instagram video capturing a child's first steps or first words or first day of school. This is what so many of us do, right? As parents, we document the moments that are important in our child's life, and we post it online for our community to see. And as we do that, we are inevitably shaping our child's online presence. Most of the time, the child in question doesn't really get a say in the matter. And that creates complications as kids get older and realize that much of their life is already online, and anyone can see it. Taylor Lorenz is a staff writer at The Atlantic who's been reporting on this. Her recent piece is called \"When Kids Realize Their Whole Life Is Already Online. \" She joins us via Skype. Taylor, thanks for being here. TAYLOR LORENZ: Yeah. Thank you so much for having me. MARTIN: What provoked you to look into this? LORENZ: You know, there's so much about this stuff from the parent's perspective. But I kind of just wanted to talk to kids themselves about when they started to realize that they had a digital presence. MARTIN: What was the range of responses when you engaged them on this? LORENZ: I mean, it kind of just runs the gamut. Some kids were upset about the information that was out there about them. So they would google themselves and find, you know, in some cases their parents had consented to school websites posting about them. They were upset that maybe their whole sports record was up there or that their parents had posted a lot more public stuff than they realized. And then some kids really liked it. Like, there was one boy who really felt like it made him feel famous to have all these pictures of himself on the internet. Maybe 80 percent of the kids that I spoke to didn't realize the extent of their internet presence, and the ones that did had parents who sort of had proactively warned them about it. But - they had been told, you know, to stay offline, but they thought since they didn't have social media themselves that there wouldn't actually be that much about them. But in some cases, that's just not the case. MARTIN: In your reporting in these conversations, I mean, did it create tension in any of these relationships, especially as you got to, like, the middle school age or high school? LORENZ: That's when it starts to create tension. I think kids in elementary school are the ones that I talk to between third and fifth grade - when they first google themselves, sometimes they're kind of, like, frustrated by what they find, or they kind of think it's novel. But it wasn't till kids got to middle school that I heard anybody really say, like, I want to talk to my mom about this. I'm really frustrated by this, or my dad or, you know, whatever. So I think middle school is often when kids want to get their own social media profiles. And I think they just want to start dictating sort of their online presence for themselves. . . MARTIN: Right. LORENZ: . . . At that time. So I think that's when a lot of the friction occurs. MARTIN: But the bottom line is talk to your kids before you post. LORENZ: Yeah, and help them understand these platforms too. You know, a lot of parents want to shield their kids from these platforms. But I think parents need to have a better understanding of how these platforms work, and they just need to open the dialogue. MARTIN: Taylor Lorenz. She's a staff writer at The Atlantic. Taylor, thanks so much. LORENZ: Yeah. Thank you so much for having me. RACHEL MARTIN, HOST:  We have all seen them - a pregnancy announcement on Facebook accompanied by a sonogram photo, or an Instagram video capturing a child's first steps or first words or first day of school. This is what so many of us do, right? As parents, we document the moments that are important in our child's life, and we post it online for our community to see. And as we do that, we are inevitably shaping our child's online presence. Most of the time, the child in question doesn't really get a say in the matter. And that creates complications as kids get older and realize that much of their life is already online, and anyone can see it. Taylor Lorenz is a staff writer at The Atlantic who's been reporting on this. Her recent piece is called \"When Kids Realize Their Whole Life Is Already Online. \" She joins us via Skype. Taylor, thanks for being here. TAYLOR LORENZ: Yeah. Thank you so much for having me. MARTIN: What provoked you to look into this? LORENZ: You know, there's so much about this stuff from the parent's perspective. But I kind of just wanted to talk to kids themselves about when they started to realize that they had a digital presence. MARTIN: What was the range of responses when you engaged them on this? LORENZ: I mean, it kind of just runs the gamut. Some kids were upset about the information that was out there about them. So they would google themselves and find, you know, in some cases their parents had consented to school websites posting about them. They were upset that maybe their whole sports record was up there or that their parents had posted a lot more public stuff than they realized. And then some kids really liked it. Like, there was one boy who really felt like it made him feel famous to have all these pictures of himself on the internet. Maybe 80 percent of the kids that I spoke to didn't realize the extent of their internet presence, and the ones that did had parents who sort of had proactively warned them about it. But - they had been told, you know, to stay offline, but they thought since they didn't have social media themselves that there wouldn't actually be that much about them. But in some cases, that's just not the case. MARTIN: In your reporting in these conversations, I mean, did it create tension in any of these relationships, especially as you got to, like, the middle school age or high school? LORENZ: That's when it starts to create tension. I think kids in elementary school are the ones that I talk to between third and fifth grade - when they first google themselves, sometimes they're kind of, like, frustrated by what they find, or they kind of think it's novel. But it wasn't till kids got to middle school that I heard anybody really say, like, I want to talk to my mom about this. I'm really frustrated by this, or my dad or, you know, whatever. So I think middle school is often when kids want to get their own social media profiles. And I think they just want to start dictating sort of their online presence for themselves. . . MARTIN: Right. LORENZ: . . . At that time. So I think that's when a lot of the friction occurs. MARTIN: But the bottom line is talk to your kids before you post. LORENZ: Yeah, and help them understand these platforms too. You know, a lot of parents want to shield their kids from these platforms. But I think parents need to have a better understanding of how these platforms work, and they just need to open the dialogue. MARTIN: Taylor Lorenz. She's a staff writer at The Atlantic. Taylor, thanks so much. LORENZ: Yeah. Thank you so much for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-06-700896972": {"title": "Mark Zuckerberg: Facebook To Offer Messages That Are Encrypted and Self Destruct : NPR", "url": "https://www.npr.org/2019/03/06/700896972/facebook-promises-more-private-and-self-destructing-messages", "author": "No author found", "published_date": "2019-03-06", "content": "", "section": "Technology", "disclaimer": ""}, "2019-03-06-700801945": {"title": "Arizona Prosecutor Says Uber Not Criminally Liable In Self-Driving Car Crash : NPR", "url": "https://www.npr.org/2019/03/06/700801945/uber-not-criminally-liable-in-death-of-woman-hit-by-self-driving-car-says-prosec", "author": "No author found", "published_date": "2019-03-06", "content": "", "section": "Law", "disclaimer": ""}, "2019-03-06-700651500": {"title": "Quadriga's Cryptocurrency Wallets Are Empty, With Fate Of $137 Million In Doubt : NPR", "url": "https://www.npr.org/2019/03/06/700651500/crypto-mystery-quadrigas-wallets-are-empty-putting-fate-of-137-million-in-doubt", "author": "No author found", "published_date": "2019-03-06", "content": "", "section": "Business", "disclaimer": ""}, "2019-03-07-701248598": {"title": "Facebook Plans To Improve Privacy : NPR", "url": "https://www.npr.org/2019/03/07/701248598/facebook-plans-to-improve-privacy", "author": "No author found", "published_date": "2019-03-07", "content": "ARI SHAPIRO, HOST: Mark Zuckerberg built Facebook around the idea of connecting the world, and now he says he wants Facebook to be more private and a more secure experience. In a long blog post, Zuckerberg admits the company, quote, \"doesn't currently have a strong reputation for building services with privacy in mind. \" That is a reference to lots of congressional hearings and investigative reports that have shown Facebook sharing user information in uncomfortable ways. Kurt Wagner is writing about this for the tech website Recode. Hi, there. KURT WAGNER: Hi. SHAPIRO: Is there a tension between the idea of connecting the world and these new initiatives like secure messaging with end-to-end encryption, keeping users' photos and messages more secure, not keeping them forever - those sorts of things? WAGNER: There is a little bit of tension there. And primarily, it's because when you think about how Facebook has gotten as big as it is now, which is, you know, more than 2 billion users, a big part of that has been that you have a profile. Everyone else has a profile, and Facebook can see that and then kind of encourage people to connect with one another, right? It can use information or maybe, like, interests that you have and therefore assume that you might want to connect with these other people, you know, in the real world. That's harder to do when there's not all of that same data. That is basically how Facebook got as big as it is. And none of that is private in the way that the company's now talking about. SHAPIRO: All right, so building the web of social connections is one question. Another question is advertisers. The Facebook model has been built off of selling advertisers access to information about Facebook users. What happens when that information about users becomes more private? WAGNER: Well, I think this is the most important question about this whole thing. There's not only that element of it which you just brought up, which is Facebook is a data-driven business model and has been since its inception. And so if they no longer have all of that data that they're used to having, you know, how robust can their ad business be? How targeted can those ads become? At the same time, just messaging, in general, is a service that doesn't really have a strong, stable business right now. We don't know of a messaging platform, at least in the U. S. or in Europe, that has really, you know, nailed the business side. And I think that's because it is a private experience. And you can't necessarily throw ads into someone's messaging inbox in the way you can when they're scrolling through a news feed. SHAPIRO: Facebook has been under so much criticism from lawmakers, from reporters. Is this likely to satisfy those critics? WAGNER: I think some of it might be. You know, it's certainly nice when everyone's ripping on you about not being privacy focused to come out with a really long blog post from the CEO that says, we care about privacy; and that's going to be the focus going forward. At the same time, what I've learned from covering Facebook for about six years now is that the company doesn't really do anything if it doesn't have the user data to back it up. So my guess is that Facebook is noticing that this is where users are spending their time - right? - WhatsApp, Messenger, groups, stories. And so the company is leaning into that because that's where they see people spending their time. And it also kind of provides them an opportunity to say, we're going to do a better job; and we're going to talk about privacy from day one. SHAPIRO: There have been so many instances of Facebook apologizing for lapses, saying they have learned from their mistakes. And then the cycle repeats. And another lapse is revealed, and they're apologizing again. Can Facebook really be trusted to take these steps in good faith? WAGNER: There's going to be a lot of people who say no, that Facebook has already, basically, you know, shown its true colors and that Mark Zuckerberg coming out and talking about how much he cares about privacy is a little bit too little, too late. That being said, this is going to be a transition that, I think, a lot of tech companies in the industry kind of make. I mean, we're already seeing Apple, for example, as - has encrypted messaging with iMessage. Google has, you know, been trying to get into messaging for a long time. And so I don't think Facebook is going to be alone here, and there's really a level of convenience that comes with Facebook products. And I think that if you're a user, the convenience of using a Facebook product may outweigh your concerns around whether or not, you know, Mark Zuckerberg and Facebook truly care about your privacy. We've seen that happen time and time again. And I don't necessarily think it's going to change this time around. SHAPIRO: Kurt Wagner covers Facebook for the tech website Recode. Thanks so much for talking with us. WAGNER: Yeah, thank you for having me. ARI SHAPIRO, HOST:  Mark Zuckerberg built Facebook around the idea of connecting the world, and now he says he wants Facebook to be more private and a more secure experience. In a long blog post, Zuckerberg admits the company, quote, \"doesn't currently have a strong reputation for building services with privacy in mind. \" That is a reference to lots of congressional hearings and investigative reports that have shown Facebook sharing user information in uncomfortable ways. Kurt Wagner is writing about this for the tech website Recode. Hi, there. KURT WAGNER: Hi. SHAPIRO: Is there a tension between the idea of connecting the world and these new initiatives like secure messaging with end-to-end encryption, keeping users' photos and messages more secure, not keeping them forever - those sorts of things? WAGNER: There is a little bit of tension there. And primarily, it's because when you think about how Facebook has gotten as big as it is now, which is, you know, more than 2 billion users, a big part of that has been that you have a profile. Everyone else has a profile, and Facebook can see that and then kind of encourage people to connect with one another, right? It can use information or maybe, like, interests that you have and therefore assume that you might want to connect with these other people, you know, in the real world. That's harder to do when there's not all of that same data. That is basically how Facebook got as big as it is. And none of that is private in the way that the company's now talking about. SHAPIRO: All right, so building the web of social connections is one question. Another question is advertisers. The Facebook model has been built off of selling advertisers access to information about Facebook users. What happens when that information about users becomes more private? WAGNER: Well, I think this is the most important question about this whole thing. There's not only that element of it which you just brought up, which is Facebook is a data-driven business model and has been since its inception. And so if they no longer have all of that data that they're used to having, you know, how robust can their ad business be? How targeted can those ads become? At the same time, just messaging, in general, is a service that doesn't really have a strong, stable business right now. We don't know of a messaging platform, at least in the U. S. or in Europe, that has really, you know, nailed the business side. And I think that's because it is a private experience. And you can't necessarily throw ads into someone's messaging inbox in the way you can when they're scrolling through a news feed. SHAPIRO: Facebook has been under so much criticism from lawmakers, from reporters. Is this likely to satisfy those critics? WAGNER: I think some of it might be. You know, it's certainly nice when everyone's ripping on you about not being privacy focused to come out with a really long blog post from the CEO that says, we care about privacy; and that's going to be the focus going forward. At the same time, what I've learned from covering Facebook for about six years now is that the company doesn't really do anything if it doesn't have the user data to back it up. So my guess is that Facebook is noticing that this is where users are spending their time - right? - WhatsApp, Messenger, groups, stories. And so the company is leaning into that because that's where they see people spending their time. And it also kind of provides them an opportunity to say, we're going to do a better job; and we're going to talk about privacy from day one. SHAPIRO: There have been so many instances of Facebook apologizing for lapses, saying they have learned from their mistakes. And then the cycle repeats. And another lapse is revealed, and they're apologizing again. Can Facebook really be trusted to take these steps in good faith? WAGNER: There's going to be a lot of people who say no, that Facebook has already, basically, you know, shown its true colors and that Mark Zuckerberg coming out and talking about how much he cares about privacy is a little bit too little, too late. That being said, this is going to be a transition that, I think, a lot of tech companies in the industry kind of make. I mean, we're already seeing Apple, for example, as - has encrypted messaging with iMessage. Google has, you know, been trying to get into messaging for a long time. And so I don't think Facebook is going to be alone here, and there's really a level of convenience that comes with Facebook products. And I think that if you're a user, the convenience of using a Facebook product may outweigh your concerns around whether or not, you know, Mark Zuckerberg and Facebook truly care about your privacy. We've seen that happen time and time again. And I don't necessarily think it's going to change this time around. SHAPIRO: Kurt Wagner covers Facebook for the tech website Recode. Thanks so much for talking with us. WAGNER: Yeah, thank you for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-07-701044877": {"title": "Amazon Closing 87 Pop-Up Kiosks As Its Retail Strategy Shifts : NPR", "url": "https://www.npr.org/2019/03/07/701044877/amazons-latest-retail-shift-means-closing-87-pop-up-kiosks", "author": "No author found", "published_date": "2019-03-07", "content": "", "section": "Business", "disclaimer": ""}, "2019-03-08-701417140": {"title": "Woman Wins $10,000 For Reading Fine Print : NPR", "url": "https://www.npr.org/2019/03/08/701417140/when-not-reading-the-fine-print-can-cost-your-soul", "author": "No author found", "published_date": "2019-03-08", "content": "", "section": "Strange News", "disclaimer": ""}, "2019-03-09-701939246": {"title": "Amnesty International: New Twitter Feature Leaves Burden On Users Experiencing Abuse : NPR", "url": "https://www.npr.org/2019/03/09/701939246/amnesty-international-new-twitter-feature-leaves-burden-on-users-experiencing-ab", "author": "No author found", "published_date": "2019-03-09", "content": "MICHEL MARTIN, HOST: We're going to return now to our Troll Watch series. (SOUNDBITE OF MUSIC)MARTIN: This is where we bring you stories of cybersecurity attacks, bots and of course, internet trolls. This week, Twitter confirmed that users will eventually be able to press a button that says hide tweet that would, as you might imagine, allow users to hide certain responses to their tweets. And that means if you tweet something and you get nasty or abusive replies back, you could make those replies invisible to others. Now one reason this is of interest of course is the abuse directed at women on Twitter, something Amnesty International researched extensively in a report recently. We spoke with Amnesty about that report, so we wanted to follow up to ask them what they make of this new planned feature. Joining us now is Rasha Abdul Rahim, deputy director of Amnesty Tech. She's with us via Skype from London. Rasha, thanks so much for talking to us. RASHA ABDUL-RAHIM: No problem. Thank you for having me. MARTIN: So tell us about this new Twitter feature. What's your understanding of how it would work? ABDUL-RAHIM: Yeah, so my understanding is that Twitter's developed this new feature as a way to allow people - women - who received tweets that may not reach the threshold of being abusive or hateful to allow them another way to hide problematic tweets they may be receiving so that they're not as visible to them and to others. But my understanding is that people will still be able to view those tweets if they click the tab that shows the hidden tweets. MARTIN: So what are the pros and the cons of this? ABDUL-RAHIM: I see four different issues with this. I think the first one is the - I think there's a danger here of brushing the issue under the carpet, so brushing the issue of problematic tweets under the carpet and not holding people who are sending those tweets accountable. So these kinds of tweets, as I said before, may not necessarily reach the threshold of abuse or hateful conduct, but they still contain hurtful or hostile content, and especially if they're repeated to an individual on multiple occasions. And these are the kinds of things that can reinforce negative or harmful stereotypes against a group of individuals, such as women, such as women of color. And they may still have a silencing effect on them. So I think here is - the key is, you know, will the effect of this be that those kinds of repeat offenders will not have any kind of accountability leveled to them for sending, you know, a barrage of these kinds of problematic tweets? MARTIN: Two criticisms I've seen come from different angles - one is that this still puts the onus on women to solve the problem. . . ABDUL-RAHIM: Absolutely. Yeah, that was the next point I was going to make. MARTIN: . . . As opposed to putting the onus on Twitter. But the other side of the equation is some are arguing that this allows for censorship. I mean, it could allow for say, public officials to, you know, decide that they don't want other people to see legitimate criticism directed at them just cause they don't like it. ABDUL-RAHIM: Yeah, totally. These are two issues as well that we've spotted. So the burden is still, as you say, on the individuals experiencing the abuse to label or to hide every single tweet. And this doesn't only take time, but it also takes an emotional toll on those individuals who are receiving that abuse. And it's almost as if Twitter is kind of outsourcing that responsibility to the people who are experiencing this abuse. And as you say, it could also have a silencing effect on free speech if powerful public figures such as politicians can hide dissent or prevent users from holding public figures to account. MARTIN: Could you just remind people for those who did not hear our prior conversation why you feel that abusive tweets, this kind of communication, rises to the level of a human rights concern, such that Amnesty would take as much time and effort as it has to investigate it? Why do you think this is a problem? ABDUL-RAHIM: It's a problem because if women are disproportionately experiencing abuse or harassment or are targets of problematic tweets, then this means that this could have a silencing effect on them. And this is something that we found in our research that women tend to change the way in which they interact on these platforms. They tend to, you know, withdraw from Twitter or change the way in which they use their language on Twitter in order to not subject themselves or open themselves up to abuse. And, you know, our research has shown that 7. 1 percent of tweets that were sent to women in this study were problematic or abusive, which amounted to 1. 1 million tweets mentioning 778 women across the year, which amounts also to one every 30 seconds. And we also found that women of color were 34 percent more likely to be mentioned in abusive or problematic tweets than white women. Black women were disproportionately targeted, being 84 percent more likely than white women to be mentioned in abusive or problematic tweets. So this is clearly a problem. And if Twitter is not responding or addressing this problem effectively, then this obviously will have an impact on women's ability to freely express themselves on the platform. MARTIN: That's Rasha Abdul Rahim, deputy director of Amnesty Tech at Amnesty International. We reached her via Skype in London. Rasha, thank you so much for talking with us. ABDUL-RAHIM: Thank you, too. Good bye. MICHEL MARTIN, HOST:  We're going to return now to our Troll Watch series. (SOUNDBITE OF MUSIC) MARTIN: This is where we bring you stories of cybersecurity attacks, bots and of course, internet trolls. This week, Twitter confirmed that users will eventually be able to press a button that says hide tweet that would, as you might imagine, allow users to hide certain responses to their tweets. And that means if you tweet something and you get nasty or abusive replies back, you could make those replies invisible to others. Now one reason this is of interest of course is the abuse directed at women on Twitter, something Amnesty International researched extensively in a report recently. We spoke with Amnesty about that report, so we wanted to follow up to ask them what they make of this new planned feature. Joining us now is Rasha Abdul Rahim, deputy director of Amnesty Tech. She's with us via Skype from London. Rasha, thanks so much for talking to us. RASHA ABDUL-RAHIM: No problem. Thank you for having me. MARTIN: So tell us about this new Twitter feature. What's your understanding of how it would work? ABDUL-RAHIM: Yeah, so my understanding is that Twitter's developed this new feature as a way to allow people - women - who received tweets that may not reach the threshold of being abusive or hateful to allow them another way to hide problematic tweets they may be receiving so that they're not as visible to them and to others. But my understanding is that people will still be able to view those tweets if they click the tab that shows the hidden tweets. MARTIN: So what are the pros and the cons of this? ABDUL-RAHIM: I see four different issues with this. I think the first one is the - I think there's a danger here of brushing the issue under the carpet, so brushing the issue of problematic tweets under the carpet and not holding people who are sending those tweets accountable. So these kinds of tweets, as I said before, may not necessarily reach the threshold of abuse or hateful conduct, but they still contain hurtful or hostile content, and especially if they're repeated to an individual on multiple occasions. And these are the kinds of things that can reinforce negative or harmful stereotypes against a group of individuals, such as women, such as women of color. And they may still have a silencing effect on them. So I think here is - the key is, you know, will the effect of this be that those kinds of repeat offenders will not have any kind of accountability leveled to them for sending, you know, a barrage of these kinds of problematic tweets? MARTIN: Two criticisms I've seen come from different angles - one is that this still puts the onus on women to solve the problem. . . ABDUL-RAHIM: Absolutely. Yeah, that was the next point I was going to make. MARTIN: . . . As opposed to putting the onus on Twitter. But the other side of the equation is some are arguing that this allows for censorship. I mean, it could allow for say, public officials to, you know, decide that they don't want other people to see legitimate criticism directed at them just cause they don't like it. ABDUL-RAHIM: Yeah, totally. These are two issues as well that we've spotted. So the burden is still, as you say, on the individuals experiencing the abuse to label or to hide every single tweet. And this doesn't only take time, but it also takes an emotional toll on those individuals who are receiving that abuse. And it's almost as if Twitter is kind of outsourcing that responsibility to the people who are experiencing this abuse. And as you say, it could also have a silencing effect on free speech if powerful public figures such as politicians can hide dissent or prevent users from holding public figures to account. MARTIN: Could you just remind people for those who did not hear our prior conversation why you feel that abusive tweets, this kind of communication, rises to the level of a human rights concern, such that Amnesty would take as much time and effort as it has to investigate it? Why do you think this is a problem? ABDUL-RAHIM: It's a problem because if women are disproportionately experiencing abuse or harassment or are targets of problematic tweets, then this means that this could have a silencing effect on them. And this is something that we found in our research that women tend to change the way in which they interact on these platforms. They tend to, you know, withdraw from Twitter or change the way in which they use their language on Twitter in order to not subject themselves or open themselves up to abuse. And, you know, our research has shown that 7. 1 percent of tweets that were sent to women in this study were problematic or abusive, which amounted to 1. 1 million tweets mentioning 778 women across the year, which amounts also to one every 30 seconds. And we also found that women of color were 34 percent more likely to be mentioned in abusive or problematic tweets than white women. Black women were disproportionately targeted, being 84 percent more likely than white women to be mentioned in abusive or problematic tweets. So this is clearly a problem. And if Twitter is not responding or addressing this problem effectively, then this obviously will have an impact on women's ability to freely express themselves on the platform. MARTIN: That's Rasha Abdul Rahim, deputy director of Amnesty Tech at Amnesty International. We reached her via Skype in London. Rasha, thank you so much for talking with us. ABDUL-RAHIM: Thank you, too. Good bye.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-09-701838219": {"title": "California Lawmakers Say There's More To Be Done On Data Privacy : NPR", "url": "https://www.npr.org/2019/03/09/701838219/california-lawmakers-say-theres-more-to-be-done-on-data-privacy", "author": "No author found", "published_date": "2019-03-09", "content": "SCOTT SIMON, HOST: California lawmakers unanimously passed a landmark data privacy law last year in response to concerns over how online companies are using people's personal information. But as Marisa Lagos from member station KQED in San Francisco reports, the issue's far from settled. MARISA LAGOS, BYLINE: The California Consumer Privacy Act was hailed as the most sweeping data privacy law in the U. S. But even here, lawmakers don't think their work is done. Republican Assemblyman Jordan Cunningham says it's an issue that transcends partisan differences. JORDAN CUNNINGHAM: And, you know, when I talked to my constituents about it, they're overwhelmingly supportive of the notion of individual privacy and the importance of passing laws that make sure that the individual consumer doesn't get lost in this rush to new technologies. LAGOS: This focus on privacy picked up steam after a spate of scandals at Facebook, including last year's revelation that a political research firm improperly captured information about tens of millions of Facebook users. California's law is the first of its kind in the U. S. , allowing Californians to ask a business what personal information has been collected by a company about them and to tell the company to erase that information. The thing is the law hasn't even taken effect yet. It kicks in next January. But there are already more than 40 new bills in Sacramento that run the gamut from tweaking technical language to more sweeping changes, like allowing consumers to sue companies for violating the law. And at least eight other states are now considering legislation with some going even further. In Massachusetts, for example, a proposed bill lets the public sue companies directly. Vermont has enacted a law that will force companies that buy and sell personal data to register with the state. All that state action has spurred Congress to at least start talking about whether tech companies are too big and powerful. Connecticut Senator Richard Blumenthal grilled tech lobbyists at a recent hearing. (SOUNDBITE OF ARCHIVED RECORDING)RICHARD BLUMENTHAL: Let me begin by asking how many of you believe that Americans deserve the same level of privacy now as a floor that California provides for its people. LAGOS: There's one Senate bill with broad Democratic support to require websites and apps to protect personal data. And Senators Elizabeth Warren and Amy Klobuchar, both presidential candidates, are campaigning on getting tough on big tech. Could all this movement lead to a rare bipartisan agreement in Congress? Terrell McSweeny is a former federal trade commissioner. TERRELL MCSWEENY: I would say there are a lot of efforts that are bipartisan. I think we can reasonably assume that there's going to be a lot of focus and energy around this discussion at the federal level. LAGOS: But getting anything done in D. C. is difficult, McSweeny says. MCSWEENY: So we might see more action at the state level in the next year than we will at the federal level. LAGOS: All these moves could even be influencing the big tech companies. Earlier this week, Facebook CEO Mark Zuckerberg announced plans to transform the social media company into a privacy-focused platform that is more centered around private messages than public posts. Mary Stone Ross, a former CIA analyst who helped craft the California law, says Zuckerberg's move is clearly a response to pressure from states like California. MARY STONE ROSS: I think that they're trying to get ahead of all these proposed regulations by saying, look; like, we realize that we have had mistakes around privacy in the past. But we're changing. This is a play to look better when they go walk through the halls of Congress. LAGOS: In the meantime, California lawmakers are pushing ahead to make its data privacy laws stronger. And in other states around the country, lawmakers are just getting started. For NPR News, I'm Marisa Lagos in San Francisco. SCOTT SIMON, HOST:  California lawmakers unanimously passed a landmark data privacy law last year in response to concerns over how online companies are using people's personal information. But as Marisa Lagos from member station KQED in San Francisco reports, the issue's far from settled. MARISA LAGOS, BYLINE: The California Consumer Privacy Act was hailed as the most sweeping data privacy law in the U. S. But even here, lawmakers don't think their work is done. Republican Assemblyman Jordan Cunningham says it's an issue that transcends partisan differences. JORDAN CUNNINGHAM: And, you know, when I talked to my constituents about it, they're overwhelmingly supportive of the notion of individual privacy and the importance of passing laws that make sure that the individual consumer doesn't get lost in this rush to new technologies. LAGOS: This focus on privacy picked up steam after a spate of scandals at Facebook, including last year's revelation that a political research firm improperly captured information about tens of millions of Facebook users. California's law is the first of its kind in the U. S. , allowing Californians to ask a business what personal information has been collected by a company about them and to tell the company to erase that information. The thing is the law hasn't even taken effect yet. It kicks in next January. But there are already more than 40 new bills in Sacramento that run the gamut from tweaking technical language to more sweeping changes, like allowing consumers to sue companies for violating the law. And at least eight other states are now considering legislation with some going even further. In Massachusetts, for example, a proposed bill lets the public sue companies directly. Vermont has enacted a law that will force companies that buy and sell personal data to register with the state. All that state action has spurred Congress to at least start talking about whether tech companies are too big and powerful. Connecticut Senator Richard Blumenthal grilled tech lobbyists at a recent hearing. (SOUNDBITE OF ARCHIVED RECORDING) RICHARD BLUMENTHAL: Let me begin by asking how many of you believe that Americans deserve the same level of privacy now as a floor that California provides for its people. LAGOS: There's one Senate bill with broad Democratic support to require websites and apps to protect personal data. And Senators Elizabeth Warren and Amy Klobuchar, both presidential candidates, are campaigning on getting tough on big tech. Could all this movement lead to a rare bipartisan agreement in Congress? Terrell McSweeny is a former federal trade commissioner. TERRELL MCSWEENY: I would say there are a lot of efforts that are bipartisan. I think we can reasonably assume that there's going to be a lot of focus and energy around this discussion at the federal level. LAGOS: But getting anything done in D. C. is difficult, McSweeny says. MCSWEENY: So we might see more action at the state level in the next year than we will at the federal level. LAGOS: All these moves could even be influencing the big tech companies. Earlier this week, Facebook CEO Mark Zuckerberg announced plans to transform the social media company into a privacy-focused platform that is more centered around private messages than public posts. Mary Stone Ross, a former CIA analyst who helped craft the California law, says Zuckerberg's move is clearly a response to pressure from states like California. MARY STONE ROSS: I think that they're trying to get ahead of all these proposed regulations by saying, look; like, we realize that we have had mistakes around privacy in the past. But we're changing. This is a play to look better when they go walk through the halls of Congress. LAGOS: In the meantime, California lawmakers are pushing ahead to make its data privacy laws stronger. And in other states around the country, lawmakers are just getting started. For NPR News, I'm Marisa Lagos in San Francisco.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-09-701838226": {"title": "How Easy It Is To Identify Strangers With Google : NPR", "url": "https://www.npr.org/2019/03/09/701838226/how-easy-it-is-to-identify-strangers-with-google", "author": "No author found", "published_date": "2019-03-09", "content": "SCOTT SIMON, HOST: Kate Klonick tells her students at St. John's University law school to deanonymize someone they come across in public. She uses that as a lesson on privacy, or how little really there is when we're in public. Professor Kate Klonick joins us now. Thanks so much for being with us. KATE KLONICK: Thanks for having me. SIMON: So how do you deanonymize somebody? KLONICK: Well, it's really quite easy, which is part of what I think is the scary kind of learning tool about this exercise. I told my students that they should over break - it was an optional assignment, but if they wanted to over break to, using only their phone and Google, try to figure out who a person was - their full name - just based on things that they reveal loudly in public while other people are around and things that are displayed on their clothing or bags, like monograms or a school logo. SIMON: And this works? KLONICK: Yes with kind of remarkable accuracy. I've had my students talking to me over break and sending me their stories about how it took them two minutes or three minutes or four minutes to have full information about the person sitting in front of them on an airplane or next to them on the subway based really on things as little as their first name and the college that was on their shirt or something like the details for picking them up from the airport. And it's just been a really wonderful exercise in how obscurity does a really poor job of masking our privacy. SIMON: Oh, my. I mean, the implication of this could be something like, you know, you could be wearing a St. John's basketball sweatshirt, as quite a few people do, someone could call you Kate in public and you're carrying a law school book, and someone could put all that together. KLONICK: That's exactly right. And, of course, it's harder for people who have kind of common names. But my last name, for instance, is very uncommon. And so I am - if they heard my last name and just my last name, they would be able to very easily find out who exactly I was in just a few seconds. SIMON: Of course, in the movie version, one of your students would deanonymize somebody and discover that they're a serial killer that escaped from prison. KLONICK: Yeah. I was thinking a nicer version is they would deanonymize someone and then maybe, like, fall in love with them on the subway train or their profile and. . . SIMON: Oh, much nicer. KLONICK: . . . We'd have a wonderful romantic comedy (laughter) but sure they could of course. . . SIMON: A rom-com, you're right, much better idea. KLONICK: Yes, exactly, a fun romp through the world of privacy. But yeah, no, I think that that's exactly right, that there's a lot of things that you can find out about a person now. We know that you can find out about someone - tons of stuff - when you have their name and a few identifying details on Google. The kind of crazy part about this experiment was that it shows you that you don't even need their name necessarily. You don't even need a Social Security number, and you can do a fair amount of research on a person and find out a fair amount about their everyday lives. SIMON: Without spoiling the end for your students, what do you want them to learn from this experience? KLONICK: I think that what I was really trying to teach them is that we have very few laws that do this type of protection and we might not even want them but that there have to be some type of protections for privacy. And whether they come from normative understandings of how our privacy actually is protected, such as people respecting each other's privacy and obscurity in public places or not, that we have to be really conscious of those and do a little more education on some of those things if we want to create future protections, either in law or just in society in general. SIMON: So could somebody see a tall, bearded man in an airport with a guitar and figure out that's BJ Leiderman, who writes our theme music? KLONICK: (Laughter) I don't know. I think that there - I think that anything's possible. SIMON: St. John's University law professor Kate Klonick, thanks so much for being with us. KLONICK: Thank you for having me. SCOTT SIMON, HOST:  Kate Klonick tells her students at St. John's University law school to deanonymize someone they come across in public. She uses that as a lesson on privacy, or how little really there is when we're in public. Professor Kate Klonick joins us now. Thanks so much for being with us. KATE KLONICK: Thanks for having me. SIMON: So how do you deanonymize somebody? KLONICK: Well, it's really quite easy, which is part of what I think is the scary kind of learning tool about this exercise. I told my students that they should over break - it was an optional assignment, but if they wanted to over break to, using only their phone and Google, try to figure out who a person was - their full name - just based on things that they reveal loudly in public while other people are around and things that are displayed on their clothing or bags, like monograms or a school logo. SIMON: And this works? KLONICK: Yes with kind of remarkable accuracy. I've had my students talking to me over break and sending me their stories about how it took them two minutes or three minutes or four minutes to have full information about the person sitting in front of them on an airplane or next to them on the subway based really on things as little as their first name and the college that was on their shirt or something like the details for picking them up from the airport. And it's just been a really wonderful exercise in how obscurity does a really poor job of masking our privacy. SIMON: Oh, my. I mean, the implication of this could be something like, you know, you could be wearing a St. John's basketball sweatshirt, as quite a few people do, someone could call you Kate in public and you're carrying a law school book, and someone could put all that together. KLONICK: That's exactly right. And, of course, it's harder for people who have kind of common names. But my last name, for instance, is very uncommon. And so I am - if they heard my last name and just my last name, they would be able to very easily find out who exactly I was in just a few seconds. SIMON: Of course, in the movie version, one of your students would deanonymize somebody and discover that they're a serial killer that escaped from prison. KLONICK: Yeah. I was thinking a nicer version is they would deanonymize someone and then maybe, like, fall in love with them on the subway train or their profile and. . . SIMON: Oh, much nicer. KLONICK: . . . We'd have a wonderful romantic comedy (laughter) but sure they could of course. . . SIMON: A rom-com, you're right, much better idea. KLONICK: Yes, exactly, a fun romp through the world of privacy. But yeah, no, I think that that's exactly right, that there's a lot of things that you can find out about a person now. We know that you can find out about someone - tons of stuff - when you have their name and a few identifying details on Google. The kind of crazy part about this experiment was that it shows you that you don't even need their name necessarily. You don't even need a Social Security number, and you can do a fair amount of research on a person and find out a fair amount about their everyday lives. SIMON: Without spoiling the end for your students, what do you want them to learn from this experience? KLONICK: I think that what I was really trying to teach them is that we have very few laws that do this type of protection and we might not even want them but that there have to be some type of protections for privacy. And whether they come from normative understandings of how our privacy actually is protected, such as people respecting each other's privacy and obscurity in public places or not, that we have to be really conscious of those and do a little more education on some of those things if we want to create future protections, either in law or just in society in general. SIMON: So could somebody see a tall, bearded man in an airport with a guitar and figure out that's BJ Leiderman, who writes our theme music? KLONICK: (Laughter) I don't know. I think that there - I think that anything's possible. SIMON: St. John's University law professor Kate Klonick, thanks so much for being with us. KLONICK: Thank you for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-10-702028545": {"title": "Googling Strangers: One Professor's Lesson On Privacy In Public Spaces : NPR", "url": "https://www.npr.org/2019/03/10/702028545/googling-strangers-one-professors-lesson-on-privacy-in-public-spaces", "author": "No author found", "published_date": "2019-03-10", "content": "", "section": "Technology", "disclaimer": ""}, "2019-03-11-702347714": {"title": "Protesters Decry Internet Isolation Bill Expected To Pass In Russia : NPR", "url": "https://www.npr.org/2019/03/11/702347714/russians-fearing-internet-isolation-protest-government-plan", "author": "No author found", "published_date": "2019-03-11", "content": "", "section": "Technology", "disclaimer": ""}, "2019-03-11-702355542": {"title": "The Race Is On For Control Of 5G Wireless Communications \u2014 And China Is In The Lead : NPR", "url": "https://www.npr.org/2019/03/11/702355542/the-race-is-on-for-control-of-5g-wireless-communications-and-china-is-in-the-lea", "author": "No author found", "published_date": "2019-03-11", "content": "AUDIE CORNISH, HOST: The race is on for control of 5G, the fifth generation of wireless communications. And we take a look in this month's All Tech Considered. (SOUNDBITE OF MUSIC)CORNISH: Now, if you were to look at a map of 5G networks worldwide, you'd see that one company appears to be winning this race. That company is Huawei, the Chinese telecom giant. And the Trump administration considers that a national security threat, so much so that Secretary of State Mike Pompeo is warning allies like the Philippines that there are risks to choosing Huawei. (SOUNDBITE OF ARCHIVED RECORDING)MIKE POMPEO: The risks to the Philippine people, the risks to Philippine security, the risk that America may not be able to operate in certain environments if there is Huawei technology adjacent to that. CORNISH: Now, to understand why the U. S. government is saying this and what it's actually doing about 5G, we've invited Harvard Law School professor Susan Crawford to the studio. Welcome to the program. SUSAN CRAWFORD: Glad to be here. CORNISH: First, help us understand why Huawei seems to have the competitive advantage. CRAWFORD: Well, Huawei really is China. You can think of it that way. And China, through its Belt Road Initiative and its other industrial policies, is planning to connect most homes to fiber and advanced wireless. And also, more than 80 countries and 65 percent of the world's population is touched by the Belt Road Initiative, which will include fiber and advanced wireless. Huawei is a big part of that story. And the risk from the American point of view is that if China can control or influence all of that, it means all the elements of the information services supply chain from data transport to what apps can be used, as Secretary Pompeo mentions, to the analytics and artificial intelligence apply to those apps for advertising and very targeted locational services, that will all be in China's hands. CORNISH: Has the U. S. been successful so far in actually convincing allies to stay away? CRAWFORD: It's a mixed bag. In fact, even today, the U. S. said to Germany, you better not let Huawei in or we won't share our signals intelligence with you, which is a major development. But so far, both the U. K. and Germany are resisting. Look. The fact is that Huawei gear is 20 to 30 percent cheaper because it's subsidized by the Chinese government. So choosing patriotism over profits is tough for any developed country. CORNISH: We've told allies, please don't do this, but have we offered an alternative? CRAWFORD: This seems to be a purely defensive initiative by the Trump administration. We don't have a story on our side except stay with us, we're the U. S. And increasingly, around the world, that's not accepted narrative. CORNISH: We're talking about this in the context of a national security threat, but what about for an economic threat for the tech industry? Is this something they're up in arms about? CRAWFORD: It's really both because China will have the ability to surveil absolutely everything going on for their own analytical and intelligence purposes. But it's also a huge economic threat because our wonderful Silicon Valley companies won't necessarily have access to these Chinese networks to sell their services. And that's - if you're touching 65 percent of the world's population, that's a tremendous hole that we won't be able to fill using just our own market. CORNISH: I want to turn to domestic policy because President Trump has tweeted about the idea of the U. S. lagging. And he said, quote, \"5G and even 6G technology in the United States as soon as possible. \" Let's set aside 6G, which is not real, what is the government doing to make 5G happen in the U. S. ? CRAWFORD: Well, the challenge here in the U. S. is that we're relying on private carriers to have the incentives from their private profits to develop what is basic infrastructure. And we know that's likely not going to happen. There's a real chicken-and-egg problem here. They're looking for immediate revenue streams that will cover the cost of deployment, and the costs of deployment is very high. CORNISH: And we don't have a government who's going to lay all that groundwork for them. CRAWFORD: Well, we used to be the country that did great things. We built a wonderful federal highway system, and we, you know, spanned the country with the railroads. Today, the Trump administration's approach to all this seems more like a backdoor to essentially privatize public assets while not necessarily spurring much private investment. CORNISH: Bottom line, how concerned should we be about this conversation about 5G? Does this feel like a lot of hype? CRAWFORD: Look. These networks are going to make possible eye contact, which means really human services over a distance in health care, in education, presence in business meetings without having to travel, all kinds of new ideas. And Paul Romer just won the Nobel Prize for saying these basic infrastructure moves are what make nations successful - so yes. CORNISH: Susan Crawford is a professor at Harvard Law School and author of \"Fiber: The Coming Tech Revolution - And Why America Might Miss It. \" Thanks for speaking with us. CRAWFORD: Delighted to be here. AUDIE CORNISH, HOST:  The race is on for control of 5G, the fifth generation of wireless communications. And we take a look in this month's All Tech Considered. (SOUNDBITE OF MUSIC) CORNISH: Now, if you were to look at a map of 5G networks worldwide, you'd see that one company appears to be winning this race. That company is Huawei, the Chinese telecom giant. And the Trump administration considers that a national security threat, so much so that Secretary of State Mike Pompeo is warning allies like the Philippines that there are risks to choosing Huawei. (SOUNDBITE OF ARCHIVED RECORDING) MIKE POMPEO: The risks to the Philippine people, the risks to Philippine security, the risk that America may not be able to operate in certain environments if there is Huawei technology adjacent to that. CORNISH: Now, to understand why the U. S. government is saying this and what it's actually doing about 5G, we've invited Harvard Law School professor Susan Crawford to the studio. Welcome to the program. SUSAN CRAWFORD: Glad to be here. CORNISH: First, help us understand why Huawei seems to have the competitive advantage. CRAWFORD: Well, Huawei really is China. You can think of it that way. And China, through its Belt Road Initiative and its other industrial policies, is planning to connect most homes to fiber and advanced wireless. And also, more than 80 countries and 65 percent of the world's population is touched by the Belt Road Initiative, which will include fiber and advanced wireless. Huawei is a big part of that story. And the risk from the American point of view is that if China can control or influence all of that, it means all the elements of the information services supply chain from data transport to what apps can be used, as Secretary Pompeo mentions, to the analytics and artificial intelligence apply to those apps for advertising and very targeted locational services, that will all be in China's hands. CORNISH: Has the U. S. been successful so far in actually convincing allies to stay away? CRAWFORD: It's a mixed bag. In fact, even today, the U. S. said to Germany, you better not let Huawei in or we won't share our signals intelligence with you, which is a major development. But so far, both the U. K. and Germany are resisting. Look. The fact is that Huawei gear is 20 to 30 percent cheaper because it's subsidized by the Chinese government. So choosing patriotism over profits is tough for any developed country. CORNISH: We've told allies, please don't do this, but have we offered an alternative? CRAWFORD: This seems to be a purely defensive initiative by the Trump administration. We don't have a story on our side except stay with us, we're the U. S. And increasingly, around the world, that's not accepted narrative. CORNISH: We're talking about this in the context of a national security threat, but what about for an economic threat for the tech industry? Is this something they're up in arms about? CRAWFORD: It's really both because China will have the ability to surveil absolutely everything going on for their own analytical and intelligence purposes. But it's also a huge economic threat because our wonderful Silicon Valley companies won't necessarily have access to these Chinese networks to sell their services. And that's - if you're touching 65 percent of the world's population, that's a tremendous hole that we won't be able to fill using just our own market. CORNISH: I want to turn to domestic policy because President Trump has tweeted about the idea of the U. S. lagging. And he said, quote, \"5G and even 6G technology in the United States as soon as possible. \" Let's set aside 6G, which is not real, what is the government doing to make 5G happen in the U. S. ? CRAWFORD: Well, the challenge here in the U. S. is that we're relying on private carriers to have the incentives from their private profits to develop what is basic infrastructure. And we know that's likely not going to happen. There's a real chicken-and-egg problem here. They're looking for immediate revenue streams that will cover the cost of deployment, and the costs of deployment is very high. CORNISH: And we don't have a government who's going to lay all that groundwork for them. CRAWFORD: Well, we used to be the country that did great things. We built a wonderful federal highway system, and we, you know, spanned the country with the railroads. Today, the Trump administration's approach to all this seems more like a backdoor to essentially privatize public assets while not necessarily spurring much private investment. CORNISH: Bottom line, how concerned should we be about this conversation about 5G? Does this feel like a lot of hype? CRAWFORD: Look. These networks are going to make possible eye contact, which means really human services over a distance in health care, in education, presence in business meetings without having to travel, all kinds of new ideas. And Paul Romer just won the Nobel Prize for saying these basic infrastructure moves are what make nations successful - so yes. CORNISH: Susan Crawford is a professor at Harvard Law School and author of \"Fiber: The Coming Tech Revolution - And Why America Might Miss It. \" Thanks for speaking with us. CRAWFORD: Delighted to be here.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-11-702102576": {"title": "At SXSW, Democratic Candidates Target Tech Giants, Who Are Major Party Donors : NPR", "url": "https://www.npr.org/2019/03/11/702102576/democratic-candidates-target-tech-giants-who-are-major-party-donors", "author": "No author found", "published_date": "2019-03-11", "content": "DAVID GREENE, HOST: The first full week of South by Southwest is underway with a big focus on the 2020 election. More than half a dozen candidates showed up in Austin, Texas, over the weekend for the annual music, arts and technology convention. And Democrats seem to be competing with one another to be the tough-on-tech candidate. Here's more from NPR's Aarti Shahani. (SOUNDBITE OF SPEECH)ELIZABETH WARREN: Thank you very much. Thank you. AARTI SHAHANI, BYLINE: Hundreds of fans in the Moody Theater cheered as Senator Elizabeth Warren hopped on stage. She'd just rallied in Queens, N. Y. , where Amazon pulled its plan to build a new headquarters in the face of protest. And now, at a conference full of tech workers, she came with the same message - big tech is killing competition, and that's bad. (SOUNDBITE OF SPEECH)WARREN: We want to keep that marketplace competitive, not let a giant who has an incredible information advantage and a manipulative advantage be able to snuff you out. SHAHANI: Her new policy position - break up the tech giants. Amazon is exhibit one. It's the popular site for shopping. And it also makes bed frames, yoga mats - a growing list. Warren says you can't run the marketplace and make the goods. That's too much power in too few hands. While consumers benefit from low prices, she says small businesses are losing. If she's president, the losers will change. (SOUNDBITE OF SPEECH)WARREN: The monopolist will make fewer monopoly profits. Boo hoo. (LAUGHTER)SHAHANI: After years of scandal - privacy violations, election interference, mega hacks - Silicon Valley has emerged, early on, as a presidential campaign issue. Warren's call is pushing her party to the left, a place where many Democrats aren't ready to go given the party's reliance on tech donors. In the 2018 midterms, tech companies clearly favored Democrats over the GOP. Sixty-nine percent of Amazon money went to Democratic federal campaigns, 79 percent of Facebook money, 83 percent of Alphabet - Google money. SHEILA KRUMHOLZ: It's going to be awkward. They're doing a dance. SHAHANI: Sheila Krumholz directs the Center for Responsive Politics, which tracks campaign finance. The dance is this - use populist messages to raise money online, but don't burn bridges with top executives. Senator Kamala Harris, for example, has spent years advocating for stronger data privacy. But her home base is California, and, as strategists say, Silicon Valley is the ATM machine. According to Krumholz, Harris and other candidates are waiting to see whether the leftist message pays off. . . KRUMHOLZ: Or whether they take a more moderated approach and that allows them access to money from Silicon Valley. SHAHANI: Another presidential candidate, Senator Amy Klobuchar, wants to stand out for her track record taking on Silicon Valley. In 2018, she got checks from powerful tech donors - former Google chairman Eric Schmidt; Facebook chief operating officer, Sheryl Sandberg; and Amazon's top lawyer, David Zapolsky. But she's also led efforts in the Senate, introducing and sponsoring several bills to regulate the industry. And she came to South By with another bold new idea, which she shared with NPR shortly before jumping on stage. AMY KLOBUCHAR: When they do use your data, there should be some kind of a tax on it. SHAHANI: In other words, every time Facebook or Google makes ad money by selling access to your eyeballs, make the tech titan pay. KLOBUCHAR: They're going to scream when they hear this. SHAHANI: The problem Klobuchar faced here was that her message - to create a new tax and unleash investigations - didn't excite people as much as Warren's blunter call. The Warren speech ended in a mob. JASON: Hi, Senator. I'm Jason. I work in tech, and I think you hit the nail right on the head with all of it. SHAHANI: Adoring fans, including a few tech workers, surrounded her to pledge their support and money and ask for selfies. For the Democratic candidates, the challenge will be to galvanize their supporters in tech while taking on the industry. Aarti Shahani, NPR News, Austin. (SOUNDBITE OF LAKEY INSPIRED'S \"STREET DREAMS\") DAVID GREENE, HOST:  The first full week of South by Southwest is underway with a big focus on the 2020 election. More than half a dozen candidates showed up in Austin, Texas, over the weekend for the annual music, arts and technology convention. And Democrats seem to be competing with one another to be the tough-on-tech candidate. Here's more from NPR's Aarti Shahani. (SOUNDBITE OF SPEECH) ELIZABETH WARREN: Thank you very much. Thank you. AARTI SHAHANI, BYLINE: Hundreds of fans in the Moody Theater cheered as Senator Elizabeth Warren hopped on stage. She'd just rallied in Queens, N. Y. , where Amazon pulled its plan to build a new headquarters in the face of protest. And now, at a conference full of tech workers, she came with the same message - big tech is killing competition, and that's bad. (SOUNDBITE OF SPEECH) WARREN: We want to keep that marketplace competitive, not let a giant who has an incredible information advantage and a manipulative advantage be able to snuff you out. SHAHANI: Her new policy position - break up the tech giants. Amazon is exhibit one. It's the popular site for shopping. And it also makes bed frames, yoga mats - a growing list. Warren says you can't run the marketplace and make the goods. That's too much power in too few hands. While consumers benefit from low prices, she says small businesses are losing. If she's president, the losers will change. (SOUNDBITE OF SPEECH) WARREN: The monopolist will make fewer monopoly profits. Boo hoo. (LAUGHTER) SHAHANI: After years of scandal - privacy violations, election interference, mega hacks - Silicon Valley has emerged, early on, as a presidential campaign issue. Warren's call is pushing her party to the left, a place where many Democrats aren't ready to go given the party's reliance on tech donors. In the 2018 midterms, tech companies clearly favored Democrats over the GOP. Sixty-nine percent of Amazon money went to Democratic federal campaigns, 79 percent of Facebook money, 83 percent of Alphabet - Google money. SHEILA KRUMHOLZ: It's going to be awkward. They're doing a dance. SHAHANI: Sheila Krumholz directs the Center for Responsive Politics, which tracks campaign finance. The dance is this - use populist messages to raise money online, but don't burn bridges with top executives. Senator Kamala Harris, for example, has spent years advocating for stronger data privacy. But her home base is California, and, as strategists say, Silicon Valley is the ATM machine. According to Krumholz, Harris and other candidates are waiting to see whether the leftist message pays off. . . KRUMHOLZ: Or whether they take a more moderated approach and that allows them access to money from Silicon Valley. SHAHANI: Another presidential candidate, Senator Amy Klobuchar, wants to stand out for her track record taking on Silicon Valley. In 2018, she got checks from powerful tech donors - former Google chairman Eric Schmidt; Facebook chief operating officer, Sheryl Sandberg; and Amazon's top lawyer, David Zapolsky. But she's also led efforts in the Senate, introducing and sponsoring several bills to regulate the industry. And she came to South By with another bold new idea, which she shared with NPR shortly before jumping on stage. AMY KLOBUCHAR: When they do use your data, there should be some kind of a tax on it. SHAHANI: In other words, every time Facebook or Google makes ad money by selling access to your eyeballs, make the tech titan pay. KLOBUCHAR: They're going to scream when they hear this. SHAHANI: The problem Klobuchar faced here was that her message - to create a new tax and unleash investigations - didn't excite people as much as Warren's blunter call. The Warren speech ended in a mob. JASON: Hi, Senator. I'm Jason. I work in tech, and I think you hit the nail right on the head with all of it. SHAHANI: Adoring fans, including a few tech workers, surrounded her to pledge their support and money and ask for selfies. For the Democratic candidates, the challenge will be to galvanize their supporters in tech while taking on the industry. Aarti Shahani, NPR News, Austin. (SOUNDBITE OF LAKEY INSPIRED'S \"STREET DREAMS\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-12-702477741": {"title": "U.S. Warns Germany About Possible Repercussions Of Using Huawei For 5G Network : NPR", "url": "https://www.npr.org/2019/03/12/702477741/u-s-warns-germany-using-huawei-could-crimp-intelligence-sharing-between-agencies", "author": "No author found", "published_date": "2019-03-12", "content": "", "section": "Business", "disclaimer": ""}, "2019-03-13-701130673": {"title": "Electric Scooters: This Guy Juices Up Limes And Birds Each Night : NPR", "url": "https://www.npr.org/2019/03/13/701130673/who-charges-all-those-electric-scooters-follow-a-nocturnal-juicer", "author": "No author found", "published_date": "2019-03-13", "content": "STEVE INSKEEP, HOST: Walk through an American city, and there's an increasing chance you will need to maneuver around an electric scooter just parked there on the sidewalk. While driving, you pass people riding those scooters. They're for rent by the minute. They run on batteries, and those batteries are at the center of this story because somebody has to charge them. NPR's Camila Domonoske met a man who has a side hustle as he sleeps. CAMILA DOMONOSKE, BYLINE: Joel Kirzner just finished watching the sunset from his 13th-floor office in Arlington, Va. His day job is over, but his evening gig is just beginning. He pulls out his phone and opens his scooter apps. People use these apps to ride scooters during the day. But it's night. Kirzner is hunting for scooters that need to be charged. JOEL KIRZNER: It's like a \"Pokemon Go,\" and you make money. DOMONOSKE: He heads down to the parking garage and hops in his car, a burgundy Subaru Impreza. KIRZNER: Yes. We have two scooters that are supposed to be right over there. DOMONOSKE: The scooters are standing on a sidewalk just around the corner. Kirzner can make $4 on each. The amount varies based on how low the battery is or how long the scooter's been left there. He hops out, scans the scooters and puts them in the back of his compact hatchback. KIRZNER: Now, remember, we just left my office building, and that took around 30 seconds. So that's $8 in a matter of one minute. So I think that's a pretty good deal. (SOUNDBITE OF CAR DOOR SHUTTING)KIRZNER: All right. Next one. DOMONOSKE: Kirzner is just getting started. KIRZNER: There's another one right across the street. It's too easy tonight. This is perfect. DOMONOSKE: Kirzner picks up electric scooters from two companies, Lime and Bird. Scooters are hot. These companies are both valued at billions of dollars, and they rely on gig economy workers like him to charge their scooters. KIRZNER: And it should beep in five, four, three, two, one. (SOUNDBITE OF SCOOTER BEEPING)KIRZNER: Beep. DOMONOSKE: Some people charge a few scooters a night. Others charge dozens. KIRZNER: All right. So that's three. So we have made $12. DOMONOSKE: Back in the car, Kirzner puts on his go-to scooter-charging song. KIRZNER: Let's go find our next Bird. (SOUNDBITE OF SONG, \"I NEED A DOLLAR\")ALOE BLACC: (Singing) I need a dollar, dollar. Dollar is what I need. Hey, hey. DOMONOSKE: Kirzner says that over the last six months, he's made more than $9,000 at this side hustle. From the scooter company's perspective, Kirzner's not just charging scooters. He's also retrieving them from out-of-the-way locations, like one that's practically under a bush and hasn't been ridden in four days. KIRZNER: They can drop them off wherever the heck they want to take 'em. You know? I mean, I've - it's right over there. DOMONOSKE: The next morning, he'll put them in a better spot. Lime scooters are green, but the Birds are black. They beep and flash to help Kirzner find them in the dark. The electric scooters stack up in his hatchback. He's got a system for squeezing them in. (SOUNDBITE OF SCOOTERS BEING STACKED)DOMONOSKE: His record is 14 in one load. Some nights, he does several runs. KIRZNER: If I want to go crazy tonight, I can capture 30 Birds tonight. There's a ton of Birds right now. (SOUNDBITE OF CAR HORN HONKING)KIRZNER: Let's go, buddy. DOMONOSKE: A few months ago, there was fierce competition to pick up these scooters. But it's cold and windy tonight, and the companies are paying less per scooter than they used to. So there aren't as many rivals to contend with. After collecting a dozen scooters and wrestling the last one into position. . . KIRZNER: There we go. DOMONOSKE: . . . Kirzner heads home and starts to carry the scooters to his patio. He's 42, a consultant with a background in architecture. He's got a good job. He says his income is decent, but he's glad to have a side gig. KIRZNER: I have expensive rent, expensive car payments, cable bills. You know? You know, they talk about the, like, you know, disappearing middle class. I feel like I'm in that zone where I can live fine, comfortably. But, you know, you want to have a little more financial stability, this definitely helps. DOMONOSKE: He runs an extension cord from his kitchen to his patio to power up the scooters. His chargers are neatly organized. KIRZNER: I feel like these Birds are almost my kids at this point. You know? And I've been taking care of them so much. DOMONOSKE: The scooters will take a few hours to charge. Kirzner says the impact on his electric bill is negligible. Before 7 a. m. tomorrow, Kirzner will unplug his flock, load them back up and neatly place the charged-up scooters in spots preselected by Bird. And then he'll head to his day job, where his co-workers have a nickname for him. They call him the Bird Man. Camila Domonoske, NPR. (SOUNDBITE OF SONG, \"I NEED A DOLLAR\")ALOE BLACC: (Singing) Hey, hey. Well, I need a dollar, dollar. STEVE INSKEEP, HOST:  Walk through an American city, and there's an increasing chance you will need to maneuver around an electric scooter just parked there on the sidewalk. While driving, you pass people riding those scooters. They're for rent by the minute. They run on batteries, and those batteries are at the center of this story because somebody has to charge them. NPR's Camila Domonoske met a man who has a side hustle as he sleeps. CAMILA DOMONOSKE, BYLINE: Joel Kirzner just finished watching the sunset from his 13th-floor office in Arlington, Va. His day job is over, but his evening gig is just beginning. He pulls out his phone and opens his scooter apps. People use these apps to ride scooters during the day. But it's night. Kirzner is hunting for scooters that need to be charged. JOEL KIRZNER: It's like a \"Pokemon Go,\" and you make money. DOMONOSKE: He heads down to the parking garage and hops in his car, a burgundy Subaru Impreza. KIRZNER: Yes. We have two scooters that are supposed to be right over there. DOMONOSKE: The scooters are standing on a sidewalk just around the corner. Kirzner can make $4 on each. The amount varies based on how low the battery is or how long the scooter's been left there. He hops out, scans the scooters and puts them in the back of his compact hatchback. KIRZNER: Now, remember, we just left my office building, and that took around 30 seconds. So that's $8 in a matter of one minute. So I think that's a pretty good deal. (SOUNDBITE OF CAR DOOR SHUTTING) KIRZNER: All right. Next one. DOMONOSKE: Kirzner is just getting started. KIRZNER: There's another one right across the street. It's too easy tonight. This is perfect. DOMONOSKE: Kirzner picks up electric scooters from two companies, Lime and Bird. Scooters are hot. These companies are both valued at billions of dollars, and they rely on gig economy workers like him to charge their scooters. KIRZNER: And it should beep in five, four, three, two, one. (SOUNDBITE OF SCOOTER BEEPING) KIRZNER: Beep. DOMONOSKE: Some people charge a few scooters a night. Others charge dozens. KIRZNER: All right. So that's three. So we have made $12. DOMONOSKE: Back in the car, Kirzner puts on his go-to scooter-charging song. KIRZNER: Let's go find our next Bird. (SOUNDBITE OF SONG, \"I NEED A DOLLAR\") ALOE BLACC: (Singing) I need a dollar, dollar. Dollar is what I need. Hey, hey. DOMONOSKE: Kirzner says that over the last six months, he's made more than $9,000 at this side hustle. From the scooter company's perspective, Kirzner's not just charging scooters. He's also retrieving them from out-of-the-way locations, like one that's practically under a bush and hasn't been ridden in four days. KIRZNER: They can drop them off wherever the heck they want to take 'em. You know? I mean, I've - it's right over there. DOMONOSKE: The next morning, he'll put them in a better spot. Lime scooters are green, but the Birds are black. They beep and flash to help Kirzner find them in the dark. The electric scooters stack up in his hatchback. He's got a system for squeezing them in. (SOUNDBITE OF SCOOTERS BEING STACKED) DOMONOSKE: His record is 14 in one load. Some nights, he does several runs. KIRZNER: If I want to go crazy tonight, I can capture 30 Birds tonight. There's a ton of Birds right now. (SOUNDBITE OF CAR HORN HONKING) KIRZNER: Let's go, buddy. DOMONOSKE: A few months ago, there was fierce competition to pick up these scooters. But it's cold and windy tonight, and the companies are paying less per scooter than they used to. So there aren't as many rivals to contend with. After collecting a dozen scooters and wrestling the last one into position. . . KIRZNER: There we go. DOMONOSKE: . . . Kirzner heads home and starts to carry the scooters to his patio. He's 42, a consultant with a background in architecture. He's got a good job. He says his income is decent, but he's glad to have a side gig. KIRZNER: I have expensive rent, expensive car payments, cable bills. You know? You know, they talk about the, like, you know, disappearing middle class. I feel like I'm in that zone where I can live fine, comfortably. But, you know, you want to have a little more financial stability, this definitely helps. DOMONOSKE: He runs an extension cord from his kitchen to his patio to power up the scooters. His chargers are neatly organized. KIRZNER: I feel like these Birds are almost my kids at this point. You know? And I've been taking care of them so much. DOMONOSKE: The scooters will take a few hours to charge. Kirzner says the impact on his electric bill is negligible. Before 7 a. m. tomorrow, Kirzner will unplug his flock, load them back up and neatly place the charged-up scooters in spots preselected by Bird. And then he'll head to his day job, where his co-workers have a nickname for him. They call him the Bird Man. Camila Domonoske, NPR. (SOUNDBITE OF SONG, \"I NEED A DOLLAR\") ALOE BLACC: (Singing) Hey, hey. Well, I need a dollar, dollar.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-13-702619020": {"title": "Online Privacy And User Data: Congress Sets New Tone With Big Tech : NPR", "url": "https://www.npr.org/2019/03/13/702619020/targeting-online-privacy-congress-sets-a-new-tone-with-big-tech", "author": "No author found", "published_date": "2019-03-13", "content": "", "section": "Politics", "disclaimer": ""}, "2019-03-14-703566696": {"title": "Emma Haruka Iwao Sets Guinness World Record For Most Accurate Value Of Pi : NPR", "url": "https://www.npr.org/2019/03/14/703566696/the-woman-who-calculated-31-trillion-digits-of-pi", "author": "No author found", "published_date": "2019-03-14", "content": "", "section": "Technology", "disclaimer": ""}, "2019-03-15-703912101": {"title": "New Zealand Mosque Attacks Raise Questions About Internet's Role In Radicalization : NPR", "url": "https://www.npr.org/2019/03/15/703912101/new-zealand-mosque-attacks-raise-questions-about-internets-role-in-radicalizatio", "author": "No author found", "published_date": "2019-03-15", "content": "ARI SHAPIRO, HOST: The terrorist attack in New Zealand is raising familiar questions about the Internet's role in radicalization. One question today is whether the man accused of organizing the attack on two mosques was inspired by racist ideas from the U. S. NPR's Martin Kaste joins us now. Hi, Martin. MARTIN KASTE, BYLINE: Good afternoon. SHAPIRO: You spent some time today reading the long document that this young Australian man posted online. NPR and many other news organizations have chosen not to share it, but what do you think our listeners ought to know about it? KASTE: Well, it's mainly about his fear that white people in Western countries are going to be replaced by immigrants who have higher birth rates. Now, this is hardly a new idea. There've been books about this notion for decades, and the attacker's hero, apparently, in this document, is this British fascist from the World War II era. And he also says he reached his racist conclusions a couple of years ago when he, this attacker, was touring Europe, and he was shocked by how many non-white people lived in some parts of France, for example. SHAPIRO: OK, so how does the Internet factor into this? KASTE: Well, the tone of this document is all about the Internet. He writes it in this sort of snarky adolescent style. And especially, there's this lengthy self-interview he does, sort of imagining what the media might ask him. It's really self-important, but it's also full of all these winking references to Internet memes, and it's very self-aware. At one point, he even says that his racist project relies on, quote, \"edgy humor and memes in the vanguard stage\" in order to attract young people. SHAPIRO: Can you give us an example of that? KASTE: Well, probably the one everyone's talking about is how, when he was streaming live video during his attack, right at the start, he says subscribe to PewDiePie. Now, that sounds bizarre to the average grown-up, but you need to understand that PewDiePie is this Swedish video game-playing YouTube star, and his crown as the most subscribed to person on YouTube had been threatened by another YouTube account out of India. So for a lot of people, or some people, subscribe to PewDiePie is sort of Internet code for supporting white Europeans. SHAPIRO: This echoes some of the things we hear from \"alt-right\" groups. What have those groups said in the way of reaction today? KASTE: Well, they're kind of all over the map. It's sort of hard sometimes to understand what they - where they really stand. You know, this attacker is not easy to pigeonhole politically. He's a racist, but he also says he admires China. He cares about the environment. So when you read these \"alt-right\" sites, it's kind of a hall of mirrors of snark and sarcasm. But here's some reaction from the conspiracy theory news website Infowars. (SOUNDBITE OF ARCHIVED RECORDING)ALEX JONES: There's a whole globalist concerted effort by Soros and others to ban PewDiePie, and now this happens. PieDiePie's come out and said it's disgusting, it's sickening. He has nothing to do with it. KASTE: Now, that's the Infowars star, Alex Jones, and he right away started talking about how this atrocity in New Zealand will be used, as he puts it, by the media here in America. (SOUNDBITE OF ARCHIVED RECORDING)JONES: They're going to make him into the right-wing white guy they want. But we're going to show you where he worships communist China, where he hates Christians, where he hates conservatives, and he just sounds like your standard leftist devil-worshipper. SHAPIRO: Martin Kaste, is there any connection between what this guy writes and American politics? KASTE: Well, none that we've seen so far; certainly no connection to any organizations that we've seen evidence for. But the attacker seems very aware of our politics, that he says he hopes that this attack will actually gin up some of the tension in our country over gun rights. And in fact, he's just admitting in another way that this is all sort of him once again trying to troll us. SHAPIRO: NPR's Martin Kaste. Thank you. KASTE: You're welcome. ARI SHAPIRO, HOST:  The terrorist attack in New Zealand is raising familiar questions about the Internet's role in radicalization. One question today is whether the man accused of organizing the attack on two mosques was inspired by racist ideas from the U. S. NPR's Martin Kaste joins us now. Hi, Martin. MARTIN KASTE, BYLINE: Good afternoon. SHAPIRO: You spent some time today reading the long document that this young Australian man posted online. NPR and many other news organizations have chosen not to share it, but what do you think our listeners ought to know about it? KASTE: Well, it's mainly about his fear that white people in Western countries are going to be replaced by immigrants who have higher birth rates. Now, this is hardly a new idea. There've been books about this notion for decades, and the attacker's hero, apparently, in this document, is this British fascist from the World War II era. And he also says he reached his racist conclusions a couple of years ago when he, this attacker, was touring Europe, and he was shocked by how many non-white people lived in some parts of France, for example. SHAPIRO: OK, so how does the Internet factor into this? KASTE: Well, the tone of this document is all about the Internet. He writes it in this sort of snarky adolescent style. And especially, there's this lengthy self-interview he does, sort of imagining what the media might ask him. It's really self-important, but it's also full of all these winking references to Internet memes, and it's very self-aware. At one point, he even says that his racist project relies on, quote, \"edgy humor and memes in the vanguard stage\" in order to attract young people. SHAPIRO: Can you give us an example of that? KASTE: Well, probably the one everyone's talking about is how, when he was streaming live video during his attack, right at the start, he says subscribe to PewDiePie. Now, that sounds bizarre to the average grown-up, but you need to understand that PewDiePie is this Swedish video game-playing YouTube star, and his crown as the most subscribed to person on YouTube had been threatened by another YouTube account out of India. So for a lot of people, or some people, subscribe to PewDiePie is sort of Internet code for supporting white Europeans. SHAPIRO: This echoes some of the things we hear from \"alt-right\" groups. What have those groups said in the way of reaction today? KASTE: Well, they're kind of all over the map. It's sort of hard sometimes to understand what they - where they really stand. You know, this attacker is not easy to pigeonhole politically. He's a racist, but he also says he admires China. He cares about the environment. So when you read these \"alt-right\" sites, it's kind of a hall of mirrors of snark and sarcasm. But here's some reaction from the conspiracy theory news website Infowars. (SOUNDBITE OF ARCHIVED RECORDING) ALEX JONES: There's a whole globalist concerted effort by Soros and others to ban PewDiePie, and now this happens. PieDiePie's come out and said it's disgusting, it's sickening. He has nothing to do with it. KASTE: Now, that's the Infowars star, Alex Jones, and he right away started talking about how this atrocity in New Zealand will be used, as he puts it, by the media here in America. (SOUNDBITE OF ARCHIVED RECORDING) JONES: They're going to make him into the right-wing white guy they want. But we're going to show you where he worships communist China, where he hates Christians, where he hates conservatives, and he just sounds like your standard leftist devil-worshipper. SHAPIRO: Martin Kaste, is there any connection between what this guy writes and American politics? KASTE: Well, none that we've seen so far; certainly no connection to any organizations that we've seen evidence for. But the attacker seems very aware of our politics, that he says he hopes that this attack will actually gin up some of the tension in our country over gun rights. And in fact, he's just admitting in another way that this is all sort of him once again trying to troll us. SHAPIRO: NPR's Martin Kaste. Thank you. KASTE: You're welcome.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-15-703911997": {"title": "Social Media Companies Struggle To Contain Violent, Graphic Postings : NPR", "url": "https://www.npr.org/2019/03/15/703911997/the-role-social-media-plays-in-mass-shootings", "author": "No author found", "published_date": "2019-03-15", "content": "AUDIE CORNISH, HOST: The alleged gunman streamed his attack live on Facebook. Then the video went viral. Facebook, Twitter and YouTube are still scrambling to take it down. NPR's Jasmine Garsd has been digging into that aspect of the story. JASMINE GARSD, BYLINE: The alleged shooter seems to have first advertised the attack on the online forum 8chan, a message board known for right-wing extremist users. He included a link to a Facebook account. That Facebook account is where a 17-minute long video was livestreamed in real-time. The video starts behind the wheel of a car. It appears to come from a body-mounted camera. He pulls up to Al Noor Mosque. That was one of the two mosques that were attacked. And what comes next is sheer horror. He starts shooting worshippers - at one point, going back to his car for another gun. Then he proceeds to shoot people at short range. Facebook took down the video after it happened, but it was too late. It had already gone viral on YouTube and Twitter. The social media platforms had lost control. And this raises a lot of questions about live broadcasting. ALEX LONDON: When your point is to strike fear into the hearts of people, livestreaming allows you to broadcast your message much farther. GARSD: Professor Alex London teaches ethics and philosophy at Carnegie Mellon University. He says horrific events like the one in New Zealand go viral using livestreaming technology. But that technology also allows people to call out things like police brutality. He says it used to be that. . . LONDON: When people would say, you know, the government is perpetrating acts of violence or the police were abusive, you'd have to believe their testimony. GARSD: Perhaps that's why when Philando Castile was shot by a Minnesota police officer, his girlfriend's first instinct was to start broadcasting live. That went viral, causing national outrage. LONDON: It gives people a much better sense of, you know, of the event and the event in real time. GARSD: But as much as livestreaming can document brutality, it can also make atrocities go viral and impossible to control, like in the New Zealand shooting. In the last year, Facebook has come under intense scrutiny over allowing hate groups on its platform. Recently, CEO Mark Zuckerberg announced that he was adding more moderators to crack down on disturbing content. Katie Moussouris is a cybersecurity expert, and she says perhaps we should consider the possibility that not everyone should be able to livestream. KATIE MOUSSOURIS: It's not a bad idea to potentially have only specific verified accounts allowed to post. And if something that they post that is livestreamed does contain violence or hate speech, that that privilege goes away. GARSD: But she understands that's going to cause a lot of controversy about who is allowed freedom of speech. AL TOMPKINS: Look, here's the thing about free speech and free expression. It's a messy proposition. And there's always going to be abuse. GARSD: Al Tompkins is a senior faculty at the Poynter Institute. He teaches ethics. TOMPKINS: It's true offline. It's true online. And if you intend to give people the ability to communicate freely with each other, it's going to be messy. And some people are going to abuse it, but most people won't. GARSD: Twitter and YouTube have both condemned the attacks and said they are working to bring down any video of the shooting. Facebook said in a statement it moved quickly to take down the shooter's Facebook and Instagram accounts and the video. It also said it is, quote, \"removing any praise or support for the crime and the shooter or shooters. \" Jasmine Garsd, NPR News, New York. AUDIE CORNISH, HOST:  The alleged gunman streamed his attack live on Facebook. Then the video went viral. Facebook, Twitter and YouTube are still scrambling to take it down. NPR's Jasmine Garsd has been digging into that aspect of the story. JASMINE GARSD, BYLINE: The alleged shooter seems to have first advertised the attack on the online forum 8chan, a message board known for right-wing extremist users. He included a link to a Facebook account. That Facebook account is where a 17-minute long video was livestreamed in real-time. The video starts behind the wheel of a car. It appears to come from a body-mounted camera. He pulls up to Al Noor Mosque. That was one of the two mosques that were attacked. And what comes next is sheer horror. He starts shooting worshippers - at one point, going back to his car for another gun. Then he proceeds to shoot people at short range. Facebook took down the video after it happened, but it was too late. It had already gone viral on YouTube and Twitter. The social media platforms had lost control. And this raises a lot of questions about live broadcasting. ALEX LONDON: When your point is to strike fear into the hearts of people, livestreaming allows you to broadcast your message much farther. GARSD: Professor Alex London teaches ethics and philosophy at Carnegie Mellon University. He says horrific events like the one in New Zealand go viral using livestreaming technology. But that technology also allows people to call out things like police brutality. He says it used to be that. . . LONDON: When people would say, you know, the government is perpetrating acts of violence or the police were abusive, you'd have to believe their testimony. GARSD: Perhaps that's why when Philando Castile was shot by a Minnesota police officer, his girlfriend's first instinct was to start broadcasting live. That went viral, causing national outrage. LONDON: It gives people a much better sense of, you know, of the event and the event in real time. GARSD: But as much as livestreaming can document brutality, it can also make atrocities go viral and impossible to control, like in the New Zealand shooting. In the last year, Facebook has come under intense scrutiny over allowing hate groups on its platform. Recently, CEO Mark Zuckerberg announced that he was adding more moderators to crack down on disturbing content. Katie Moussouris is a cybersecurity expert, and she says perhaps we should consider the possibility that not everyone should be able to livestream. KATIE MOUSSOURIS: It's not a bad idea to potentially have only specific verified accounts allowed to post. And if something that they post that is livestreamed does contain violence or hate speech, that that privilege goes away. GARSD: But she understands that's going to cause a lot of controversy about who is allowed freedom of speech. AL TOMPKINS: Look, here's the thing about free speech and free expression. It's a messy proposition. And there's always going to be abuse. GARSD: Al Tompkins is a senior faculty at the Poynter Institute. He teaches ethics. TOMPKINS: It's true offline. It's true online. And if you intend to give people the ability to communicate freely with each other, it's going to be messy. And some people are going to abuse it, but most people won't. GARSD: Twitter and YouTube have both condemned the attacks and said they are working to bring down any video of the shooting. Facebook said in a statement it moved quickly to take down the shooter's Facebook and Instagram accounts and the video. It also said it is, quote, \"removing any praise or support for the crime and the shooter or shooters. \" Jasmine Garsd, NPR News, New York.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-15-702519127": {"title": "Eddie Woo: How Can Math Help Us Understand The Complexity Of The Universe? : NPR", "url": "https://www.npr.org/2019/03/15/702519127/eddie-woo-how-can-math-help-us-understand-the-complexity-of-the-universe", "author": "No author found", "published_date": "2019-03-15", "content": "EDDIE WOO: I definitely - when I was going through school, I had no joy or delight in mathematics. In fact, I was one of those kids who kind of just - I survived mathematics. GUY RAZ, HOST: This is Eddie Woo. WOO: But I certainly didn't ever experience it as a really positive. Like, oh, wow, this is subject to be enjoyed and appreciated. It was really far away from my mind all the way through high school. RAZ: But, Eddie, he actually ended up becoming the one thing he'd never thought he'd be, a math teacher. WOO: It's really a testament to how my own teachers changed my own life. I turned up at university with the full intent to become an English and history teacher. I went so far as to have written that on my enrollment form. But it was at university, through some of the educators whom I met there, that I discovered that we don't have enough mathematics teachers and educators in our schools, really, in any level. And when I heard about this, suddenly all these pieces kind of fell into place for me because I realized maybe this is not just about me and my own experience of a subject. This is about something which is shaping our society. We've got - if we went out on the street today, Guy, and we asked 10 people, do you like mathematics, I think nine of them would tell us no. And that would probably be on a good day. I think on a bad day, 10 would tell us no. We'd have to get further than that to get to the first person who actually likes it. And this is the effect that this shortage of skilled, and passionate and engaged mathematics educators, that's the consequence of that shortage in our culture and society. And I thought, I want to be an educator to make a difference in people's lives. If this is where the need is, I'm going to go down the rabbit hole. (SOUNDBITE OF MUSIC)WOO: You know, there's a universal reality and truth to this subject that humanity has been fascinated with for centuries, for millennia. And so I got to sort of stumble upon that, you know, really by accident and realize, wow, there's a reason why mathematicians describe mathematics in these incredible terms. They describe it as elegant, and it has this austere beauty. It was as though, Guy, you and I, we've kind of been born into this world where no one likes music. (SOUNDBITE OF PIANO KEY PLAYING)WOO: They actually loathe it because it's something they are forced to do when they're young. Everyone really hates this particular style of music, but you have to go through learning how to write the notes and how to memorize the notes in sequence so you can recite them in a time-pressured assessment task. And then, thank goodness, after 13 years of all, compulsory music, we all just escape it, and we're very glad that we survived, having never actually listened to music ourselves. (SOUNDBITE OF DRUM BEATING)WOO: That's a terrible way to describe a world. But that's the world we live in because that's how people think of mathematics. (SOUNDBITE OF MUSIC)RAZ: Here's more from Eddie Woo on the TED stage. (SOUNDBITE OF TEDx TALK)WOO: I used to believe that math was about rote-learning inscrutable formulas to solve abstract problems that didn't mean anything to me. But at university, I began to see that mathematics is immensely practical and even beautiful. That it's not just about finding answers, but also about learning to ask the right questions. It gradually dawned on me that mathematics is a sense. Mathematics is a sense, just like sight and touch. It's a sense that allows us to perceive realities which would be otherwise intangible to us. Now, I want to show you a mathematical reality that I guarantee you've seen before but perhaps never really perceived. It's been hidden in plain sight your entire life. This is a river delta. It's a beautiful piece of geometry. And when we hear the word geometry, most of us think of triangles and circles. But geometry is the mathematics of all shapes. And this meeting of land and sea has created shapes with an undeniable pattern. It has a mathematically recursive structure. Every part of the river delta, with its twists and turns, is a micro-version of the greater whole. So I want you to see the mathematics in this. But that's not all. There's a mathematical reality woven into the fabric of the universe that you share with winding rivers, towering trees and raging storms. These shapes are examples of what we call fractals, as mathematicians. Fractals get their name from the same place as fractions and fractures. It's a reference to the broken and shattered shapes we find around us in nature. And once you have a sense for fractals, you really do start to see them everywhere - a head of broccoli. The leaves of a fern. Even clouds in the sky. Like the other senses, our mathematical sense can be refined with practice. It's just like developing perfect pitch or a taste for wines. You can learn to perceive the mathematics around you with time and the right guidance. RAZ: When people ask you a version of, you know, what am I going to do with this - like, when am I ever going to use math, what do you say? WOO: Yeah. It's - there's such a deep and profound connection between all the mathematics that I learned and that I teach to my students and literally their everyday lives. I think we need to understand that mathematics is so much more than numbers. In fact, you know, if we go back to the - because I'm such an English nerd, I love etymology and where words come from. And if you dig into where the word mathematics comes from, literally, it just means understanding, and that is as broad as the universe that we live in, the cosmos. You know, biology is the study of living things. Chemistry is the study of substances and materials. Physics is the study of matter and movement. But mathematics is the study of patterns, which are literally everywhere. Now, what does this mean in our everyday lives? Well, we are doing that. . . RAZ: Yeah. WOO: . . . Every hour of every day. We are looking out at the world, and we are - I'm just thinking about, OK, when I get in my car and I drive home, you know, I'm sitting in this traffic and thinking about how to get home. I'm thinking about all the paths that I can take. Which one is going to be the fastest and most efficient so I can see my kids sooner? All of that, you're calculating, you're thinking logically in your mind. It goes to my perception of the world. When I look out at a tree or at a rainbow, I don't just want to, you know, let these pieces of beauty pass me by. What it means to be human is actually to marvel at this universe around us and to say, wow, there's a reason why rainbows are round, why they're - you know when you see a rainbow after rain, it looks like a semicircle, you know, you can see it, you know, hitting the horizon? RAZ: Yeah, sure. WOO: But if you're lucky enough and in the right place at the right time, if you're in the sky or on a mountain, you'll actually see that rainbow is not a semicircle, it's a complete full circle. It goes all the way around. That's not a coincidence. That's geometry. That's beautiful. What it means to be human is to appreciate that and say, wow, there's something to wonder at here. And so for me, you know, when do I use mathematics? When do I not use mathematics? The real question is, do I know that I'm doing it when I am? (SOUNDBITE OF MUSIC)RAZ: Eddie, I asked a version of this question to Dan Finkel, but what's the danger of a society where people don't engage with math or don't have math competency or literacy? I mean, do you think there are consequences? WOO: There are really severe consequences, and we've actually seen this in our society today. In so many ways, I could illustrate this. Firstly, can I play a really, really quick game with you guys? Can I play a game with you? RAZ: Of course. Yeah, I'd love to. WOO: OK. (SOUNDBITE OF MUSIC)WOO: We're going to go back and forth. RAZ: OK. WOO: And we're going to say numbers. RAZ: OK. WOO: The goal is, either you or me is going to say the number 23, OK? RAZ: OK. WOO: And whoever says the number 23, they're the winner, OK? That's the goal. RAZ: OK. WOO: Now, Guy, I'll let you go first. I want you to choose a number between one and four. RAZ: Three. WOO: Three. I'm going to add a number to that, and it's always going to be between one and four, OK? RAZ: OK. WOO: So I'm going to say - I'm going to add one to that. So you said three, that means I say four. It's your turn now. RAZ: OK. So I'm going to add a four. WOO: Ah, I said four. So it's eight now. RAZ: Yep. WOO: Yep. I'm going to say two, so 10. RAZ: And I'm going to say four. WOO: Fourteen. OK, no worries. I'm going to say four now, so that means we're up to 18. RAZ: Yeah. (SOUNDBITE OF MUSIC)WOO: What would you like to say now, Guy? RAZ: (Laughter) One? WOO: (Laughter). RAZ: I've lost the game. WOO: Now, I'm going to make a bet. I'm going to bet you laughed because you realized - took you about seven seconds, which is not bad, Guy. Now, I won't give you the full explanation here, but how did I win that? I knew something about patterns underneath that game which you didn't know. RAZ: Yeah. WOO: And so I could take advantage of that fact. And, you know, in this case, what's the harm? Aw, I win. No big deal, right? But when you don't know what's going on mathematically underneath something, and increasingly today, Guy, our world is built on and is run by algorithms that have been mathematically designed by people and are hidden from view, you know. When you're scrolling through your social media feed, you're not thinking that there's mathematics happening underneath there. When you order something online and there's recommended stuff on the side for what - people like you, Guy, have purchased these as well. You're not thinking about there's a formula doing that, but mathematics is underneath all of those things, guiding all of those people's decision-making. And as I've just demonstrated, if you're not aware of that, someone can use that in a really malicious way, and people have used that in really malicious ways. And so, you know, from a negative point of view, what are the consequences of us having a mathematically-illiterate general population? They're huge. There's people who are having the wool pulled over their eyes, and they don't even know it. (SOUNDBITE OF MUSIC)WOO: But at the same time, I think we're missing a part of who we are as human beings. This is what I was sort of getting at in the talk. Can you imagine if we walked around all day with our eyes closed? Can you imagine if we went into the world ignoring our senses? We have our senses because they're a wonderful way to understand and appreciate the world. So not only are there really negative consequences, but there are really positive things that we miss out and that I want people to be able to feel and experience. That, for me, is a really fundamental reason I think mathematics should be something everyone embraces and learns. (SOUNDBITE OF MUSIC)RAZ: That's Eddie Woo. He's a high school math teacher in Sydney, Australia. He also has his own channel on YouTube, which, of course, is all about math. You can see Eddie's full talk at ted. com. (SOUNDBITE OF MUSIC)RAZ: On the show today, ideas about the beauty of math. Stay with us. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC) EDDIE WOO: I definitely - when I was going through school, I had no joy or delight in mathematics. In fact, I was one of those kids who kind of just - I survived mathematics. GUY RAZ, HOST:  This is Eddie Woo. WOO: But I certainly didn't ever experience it as a really positive. Like, oh, wow, this is subject to be enjoyed and appreciated. It was really far away from my mind all the way through high school. RAZ: But, Eddie, he actually ended up becoming the one thing he'd never thought he'd be, a math teacher. WOO: It's really a testament to how my own teachers changed my own life. I turned up at university with the full intent to become an English and history teacher. I went so far as to have written that on my enrollment form. But it was at university, through some of the educators whom I met there, that I discovered that we don't have enough mathematics teachers and educators in our schools, really, in any level. And when I heard about this, suddenly all these pieces kind of fell into place for me because I realized maybe this is not just about me and my own experience of a subject. This is about something which is shaping our society. We've got - if we went out on the street today, Guy, and we asked 10 people, do you like mathematics, I think nine of them would tell us no. And that would probably be on a good day. I think on a bad day, 10 would tell us no. We'd have to get further than that to get to the first person who actually likes it. And this is the effect that this shortage of skilled, and passionate and engaged mathematics educators, that's the consequence of that shortage in our culture and society. And I thought, I want to be an educator to make a difference in people's lives. If this is where the need is, I'm going to go down the rabbit hole. (SOUNDBITE OF MUSIC) WOO: You know, there's a universal reality and truth to this subject that humanity has been fascinated with for centuries, for millennia. And so I got to sort of stumble upon that, you know, really by accident and realize, wow, there's a reason why mathematicians describe mathematics in these incredible terms. They describe it as elegant, and it has this austere beauty. It was as though, Guy, you and I, we've kind of been born into this world where no one likes music. (SOUNDBITE OF PIANO KEY PLAYING) WOO: They actually loathe it because it's something they are forced to do when they're young. Everyone really hates this particular style of music, but you have to go through learning how to write the notes and how to memorize the notes in sequence so you can recite them in a time-pressured assessment task. And then, thank goodness, after 13 years of all, compulsory music, we all just escape it, and we're very glad that we survived, having never actually listened to music ourselves. (SOUNDBITE OF DRUM BEATING) WOO: That's a terrible way to describe a world. But that's the world we live in because that's how people think of mathematics. (SOUNDBITE OF MUSIC) RAZ: Here's more from Eddie Woo on the TED stage. (SOUNDBITE OF TEDx TALK) WOO: I used to believe that math was about rote-learning inscrutable formulas to solve abstract problems that didn't mean anything to me. But at university, I began to see that mathematics is immensely practical and even beautiful. That it's not just about finding answers, but also about learning to ask the right questions. It gradually dawned on me that mathematics is a sense. Mathematics is a sense, just like sight and touch. It's a sense that allows us to perceive realities which would be otherwise intangible to us. Now, I want to show you a mathematical reality that I guarantee you've seen before but perhaps never really perceived. It's been hidden in plain sight your entire life. This is a river delta. It's a beautiful piece of geometry. And when we hear the word geometry, most of us think of triangles and circles. But geometry is the mathematics of all shapes. And this meeting of land and sea has created shapes with an undeniable pattern. It has a mathematically recursive structure. Every part of the river delta, with its twists and turns, is a micro-version of the greater whole. So I want you to see the mathematics in this. But that's not all. There's a mathematical reality woven into the fabric of the universe that you share with winding rivers, towering trees and raging storms. These shapes are examples of what we call fractals, as mathematicians. Fractals get their name from the same place as fractions and fractures. It's a reference to the broken and shattered shapes we find around us in nature. And once you have a sense for fractals, you really do start to see them everywhere - a head of broccoli. The leaves of a fern. Even clouds in the sky. Like the other senses, our mathematical sense can be refined with practice. It's just like developing perfect pitch or a taste for wines. You can learn to perceive the mathematics around you with time and the right guidance. RAZ: When people ask you a version of, you know, what am I going to do with this - like, when am I ever going to use math, what do you say? WOO: Yeah. It's - there's such a deep and profound connection between all the mathematics that I learned and that I teach to my students and literally their everyday lives. I think we need to understand that mathematics is so much more than numbers. In fact, you know, if we go back to the - because I'm such an English nerd, I love etymology and where words come from. And if you dig into where the word mathematics comes from, literally, it just means understanding, and that is as broad as the universe that we live in, the cosmos. You know, biology is the study of living things. Chemistry is the study of substances and materials. Physics is the study of matter and movement. But mathematics is the study of patterns, which are literally everywhere. Now, what does this mean in our everyday lives? Well, we are doing that. . . RAZ: Yeah. WOO: . . . Every hour of every day. We are looking out at the world, and we are - I'm just thinking about, OK, when I get in my car and I drive home, you know, I'm sitting in this traffic and thinking about how to get home. I'm thinking about all the paths that I can take. Which one is going to be the fastest and most efficient so I can see my kids sooner? All of that, you're calculating, you're thinking logically in your mind. It goes to my perception of the world. When I look out at a tree or at a rainbow, I don't just want to, you know, let these pieces of beauty pass me by. What it means to be human is actually to marvel at this universe around us and to say, wow, there's a reason why rainbows are round, why they're - you know when you see a rainbow after rain, it looks like a semicircle, you know, you can see it, you know, hitting the horizon? RAZ: Yeah, sure. WOO: But if you're lucky enough and in the right place at the right time, if you're in the sky or on a mountain, you'll actually see that rainbow is not a semicircle, it's a complete full circle. It goes all the way around. That's not a coincidence. That's geometry. That's beautiful. What it means to be human is to appreciate that and say, wow, there's something to wonder at here. And so for me, you know, when do I use mathematics? When do I not use mathematics? The real question is, do I know that I'm doing it when I am? (SOUNDBITE OF MUSIC) RAZ: Eddie, I asked a version of this question to Dan Finkel, but what's the danger of a society where people don't engage with math or don't have math competency or literacy? I mean, do you think there are consequences? WOO: There are really severe consequences, and we've actually seen this in our society today. In so many ways, I could illustrate this. Firstly, can I play a really, really quick game with you guys? Can I play a game with you? RAZ: Of course. Yeah, I'd love to. WOO: OK. (SOUNDBITE OF MUSIC) WOO: We're going to go back and forth. RAZ: OK. WOO: And we're going to say numbers. RAZ: OK. WOO: The goal is, either you or me is going to say the number 23, OK? RAZ: OK. WOO: And whoever says the number 23, they're the winner, OK? That's the goal. RAZ: OK. WOO: Now, Guy, I'll let you go first. I want you to choose a number between one and four. RAZ: Three. WOO: Three. I'm going to add a number to that, and it's always going to be between one and four, OK? RAZ: OK. WOO: So I'm going to say - I'm going to add one to that. So you said three, that means I say four. It's your turn now. RAZ: OK. So I'm going to add a four. WOO: Ah, I said four. So it's eight now. RAZ: Yep. WOO: Yep. I'm going to say two, so 10. RAZ: And I'm going to say four. WOO: Fourteen. OK, no worries. I'm going to say four now, so that means we're up to 18. RAZ: Yeah. (SOUNDBITE OF MUSIC) WOO: What would you like to say now, Guy? RAZ: (Laughter) One? WOO: (Laughter). RAZ: I've lost the game. WOO: Now, I'm going to make a bet. I'm going to bet you laughed because you realized - took you about seven seconds, which is not bad, Guy. Now, I won't give you the full explanation here, but how did I win that? I knew something about patterns underneath that game which you didn't know. RAZ: Yeah. WOO: And so I could take advantage of that fact. And, you know, in this case, what's the harm? Aw, I win. No big deal, right? But when you don't know what's going on mathematically underneath something, and increasingly today, Guy, our world is built on and is run by algorithms that have been mathematically designed by people and are hidden from view, you know. When you're scrolling through your social media feed, you're not thinking that there's mathematics happening underneath there. When you order something online and there's recommended stuff on the side for what - people like you, Guy, have purchased these as well. You're not thinking about there's a formula doing that, but mathematics is underneath all of those things, guiding all of those people's decision-making. And as I've just demonstrated, if you're not aware of that, someone can use that in a really malicious way, and people have used that in really malicious ways. And so, you know, from a negative point of view, what are the consequences of us having a mathematically-illiterate general population? They're huge. There's people who are having the wool pulled over their eyes, and they don't even know it. (SOUNDBITE OF MUSIC) WOO: But at the same time, I think we're missing a part of who we are as human beings. This is what I was sort of getting at in the talk. Can you imagine if we walked around all day with our eyes closed? Can you imagine if we went into the world ignoring our senses? We have our senses because they're a wonderful way to understand and appreciate the world. So not only are there really negative consequences, but there are really positive things that we miss out and that I want people to be able to feel and experience. That, for me, is a really fundamental reason I think mathematics should be something everyone embraces and learns. (SOUNDBITE OF MUSIC) RAZ: That's Eddie Woo. He's a high school math teacher in Sydney, Australia. He also has his own channel on YouTube, which, of course, is all about math. You can see Eddie's full talk at ted. com. (SOUNDBITE OF MUSIC) RAZ: On the show today, ideas about the beauty of math. Stay with us. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC)", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-15-702523873": {"title": "Adam Spencer: Why Are Monster Prime Numbers Important? : NPR", "url": "https://www.npr.org/2019/03/15/702523873/adam-spencer-why-are-monster-prime-numbers-important", "author": "No author found", "published_date": "2019-03-15", "content": "GUY RAZ, HOST: Today on the show, ideas about the beauty of math and the people who love it. And for eight years, at 3:20 in the morning, Adam Spencer would roll out of bed and go to work. ADAM SPENCER: Three hours every day - 6 o'clock till 9 o'clock - news, traffic, weather, the very best music and a healthy serve of mathematics to get you on your way. RAZ: Adam hosted the most-listened-to morning radio talk show in Australia. And every chance he'd get, he'd talk about math. SPENCER: Darwin, sunny and 32 degrees. That's two to the power of five. (SOUNDBITE OF MUSIC)RAZ: In 1996, Adam was actually working toward a doctorate in pure mathematics when he won a stand-up comedy contest for a national radio station. He thought working in radio was a better idea at the time, so he dropped out. SPENCER: I'd like to say in a room of randomly selected people, I'm the maths genius. In a room of maths PhDs, I'm as dumb as a box full of hammers. RAZ: These days, Adam makes his living writing and talking about math because Adam Spencer is one of those people who's always loved numbers. Here's more from Adam on the TED stage. (SOUNDBITE OF TED TALK)SPENCER: I cast my mind back when I was in second grade. As we came up towards lunchtime, our teacher Ms. Russell said to the class, what do you want to do after lunch? I've got no plans. It was an exercise in democratic schooling, but we were only 7. And after a while, someone made a particularly silly suggestion, and Ms. Russell patted them down with that gentle aphorism - that wouldn't work. That would be like trying to put a square peg through a round hole. Now, I wasn't trying to be smart. I wasn't trying to be funny. I just politely raised my hand. And when Ms. Russell acknowledged me, I said, but miss, surely if the diagonal of the square is less than the diameter of the circle, well, the square peg will pass quite easily through the round hole. (LAUGHTER)SPENCER: It'd be like putting a piece of toast through a basketball hoop, wouldn't it? RAZ: Do you think that you just had that switch in your brain that was like, yes, math. I love it. It's awesome. SPENCER: It's a really difficult question 'cause with me, it goes back so far that I don't even remember if I had to try all that hard. But I do remember that having loved it, I did more and more. Like practically anything, it is a practice thing. And because it's a subject with that finite correct, incorrect sort of line, it is the thing where, to an extent, you can teach yourself. You know if you're getting it right. You're not teaching yourself bad habits. And I just loved it more than anyone else I knew. (SOUNDBITE OF TED TALK)SPENCER: I fell in love with mathematics from the earliest of ages. I explained it to all my friends. Maths is beautiful. It's natural. It's everywhere. Numbers are the musical notes with which the symphony of the universe is written. Today I want to show you one of those musical notes, a number so beautiful, so massive I think it will blow your mind. Today we're going to talk about prime numbers. RAZ: Prime numbers - let's just remind everybody what a prime number is. SPENCER: OK. So six is not prime. . . RAZ: Right. SPENCER: . . . Because we can break it down into six equals two times three. Seven is prime because seven is one times seven, but you can't break it into any smaller multiplying building blocks. So the primes are the sort of building blocks that all the other numbers come out from. If I throw you a number - if I say 26 - well, turns out that's not prime. RAZ: Nope. SPENCER: It's two times 13. OK. What about 29? RAZ: Yep. SPENCER: That is prime. You can't break it down. Every number has to be prime or composite. Primes go on forever. There is no final, biggest prime number. A beautiful mathematician called Euclid proved that thousands of years ago. So it makes sense. In any given time, there must be a largest prime number that we know about. And my TED talk back in 2013 was the history of the largest prime numbers we've detected. Initially, it was all just humans doing phenomenal things with their brains. And I was going to say pen and paper - not even pen, you know? Quill. . . RAZ: Quill, yeah. SPENCER: . . . And ink and chalk and things like that with equations pulling down that are just unbelievable to think a human mind could come up with free of any device. (SOUNDBITE OF TED TALK)SPENCER: This is the great Swiss mathematician Leonard Euler. In the 1700s, other mathematicians said he is simply the master of us all. Euler discovered, at the time, the world's biggest prime - two to the 31 minus one. It's over 2 billion. You think that's big. We know that two to the power of 127 minus one is a prime number. It's an absolute brute. Look at it here - 39 digits long, proven to be prime in 1876 by a mathematician called Lucas. Word up, L dog. The massive prime numbers all follow a cute little formula. I'll give you a really easy example. RAZ: Sure. SPENCER: Let's take two, and let's multiply two by itself three twos. Two times two is four, times two gets us to eight. Let's take away one from that. So we had two times two times two, take away one is seven, which just happens to be a prime number. RAZ: Yeah. SPENCER: All the massive prime numbers we've ever detected are of the form two multiplied together heaps of times, take away one. And the latest one that we uncovered in December of last year - take the number two. Write down not one two, not three twos, like I had earlier. Write down 82,589,993 twos. You end up with a 24-million-digit-long number. RAZ: Wow. SPENCER: And we know that single number is prime as confidently as we know the number seven is prime. (SOUNDBITE OF MUSIC)SPENCER: I just think that's just mind-numbingly beautiful. (SOUNDBITE OF TED TALK)SPENCER: My laptop at home was looking through four potential candidate primes myself as part of a networked computer hunt around the world for these large numbers. The discovery of that prime was similar to the work people are doing in unraveling RNA sequences, in searching through data from SETI and other astronomical projects. We live in an age where some of the great breakthroughs are not going to happen in the labs or the halls of academia but on laptops, desktops, in the palms of people's hands who are simply helping out for the search. But for me, it's amazing because it's a metaphor for the time in which we live, when human minds and machines can conquer together. (SOUNDBITE OF MUSIC)RAZ: So right now, as we're sitting here talking on the radio, you've got a computer in your house that's just, like, you know, looking for prime numbers. SPENCER: Yeah. There's a project called GIMPS. The more technical, mathematical name is Mersenne - M-E-R-S-E-N-N-E - from a guy who researched a monk back in the 1600s of all things. And so GIMPS is the Great Internet Mersenne Prime Search. Anyone can do this. You take your laptop and download the GIMPS software. It will give you a candidate prime. And in the background, while your computer's doing nothing else, it will just search. Now, it would take four to six weeks before it comes back and says yes or no. And you're almost always going to be disappointed and told no. But if you think about the amount of super computing power that is just sitting on people's desks, in their man caves, in the office at work over the weekend, on their phones, just unused - there are problems out there we want solved. And the GIMPS prime search is just a great, little, nerdy example of that. RAZ: All right. So there are people looking for these monster prime numbers. And the latest one was discovered by this guy Patrick Laroche, right? SPENCER: Yeah. Laroche is the latest one, yes. RAZ: What's the point? Like, what's the practical application of a prime number? SPENCER: Big-sized prime numbers - 20 digits long, those sort of things - underpin all Internet security. And the reason that you can use your credit cards online, et cetera, is to do with algorithms based on very large prime numbers. And the best sort of practical application for large numbers like this is they're a great way to test the speed and accuracy of potential new computer chips. If my laptop is working on a Pentium 15BZ and I think that's the greatest chip in the world, and you say, well, I've come up with the double Pentium 13X - OK. Well, let's ask them the same simple question with the same eight lines of code. And let's let the computers go and decide for us. Now, if your one comes back in only three weeks and it solves something that took my computer five weeks, you've got yourself a really fast, impressive, new computer chip. So speed and accuracy testing of computer chips these days - well worth it. And it's also just another small piece in the deeper puzzle. One of the reasons we're so attracted to prime numbers is they're so basic. They're so fundamental. We know nothing about them. Some of the most famous problems - unsolved problems in the history of mathematics are to do with the distribution of prime numbers, the amount of prime numbers you have after a certain point and things like that. So any small step towards understanding them more, I think, is a good thing. RAZ: That's Adam Spencer. He's the first-ever ambassador of science and mathematics for the University of Sydney in Australia. You can find his full talk at ted. com. (SOUNDBITE OF FILM, \"SCHOOL OF ROCK\")JACK BLACK: (As Dewey, singing) Math is a wonderful thing. Math is a really cool thing. So get off your ath (ph). Let's do some math, math, math, math, math, math. (SOUNDBITE OF MUSIC)RAZ: Hey. Thanks so much for listening to our show on math this week. If you want to find out more about who was on it, go to ted. npr. org. And to see hundreds more TED Talks, check out ted. com or the TED app. Our production staff at NPR includes Jeff Rogers, Sanaz Meshkinpour, Jinae West, Neva Grant, Casey Herman, Rachel Faulkner, Diba Mohtasham, James Delahoussaye, Melissa Gray and J. C. Howard with help from Daniel Shukin. Our intern is Katie Monteleone. Our partners at TED are Chris Anderson, Colin Helms, Anna Phelan and Janet Lee. I'm Guy Raz. And you've been listening to ideas worth spreading right here on the TED Radio Hour from NPR. GUY RAZ, HOST:  Today on the show, ideas about the beauty of math and the people who love it. And for eight years, at 3:20 in the morning, Adam Spencer would roll out of bed and go to work. ADAM SPENCER: Three hours every day - 6 o'clock till 9 o'clock - news, traffic, weather, the very best music and a healthy serve of mathematics to get you on your way. RAZ: Adam hosted the most-listened-to morning radio talk show in Australia. And every chance he'd get, he'd talk about math. SPENCER: Darwin, sunny and 32 degrees. That's two to the power of five. (SOUNDBITE OF MUSIC) RAZ: In 1996, Adam was actually working toward a doctorate in pure mathematics when he won a stand-up comedy contest for a national radio station. He thought working in radio was a better idea at the time, so he dropped out. SPENCER: I'd like to say in a room of randomly selected people, I'm the maths genius. In a room of maths PhDs, I'm as dumb as a box full of hammers. RAZ: These days, Adam makes his living writing and talking about math because Adam Spencer is one of those people who's always loved numbers. Here's more from Adam on the TED stage. (SOUNDBITE OF TED TALK) SPENCER: I cast my mind back when I was in second grade. As we came up towards lunchtime, our teacher Ms. Russell said to the class, what do you want to do after lunch? I've got no plans. It was an exercise in democratic schooling, but we were only 7. And after a while, someone made a particularly silly suggestion, and Ms. Russell patted them down with that gentle aphorism - that wouldn't work. That would be like trying to put a square peg through a round hole. Now, I wasn't trying to be smart. I wasn't trying to be funny. I just politely raised my hand. And when Ms. Russell acknowledged me, I said, but miss, surely if the diagonal of the square is less than the diameter of the circle, well, the square peg will pass quite easily through the round hole. (LAUGHTER) SPENCER: It'd be like putting a piece of toast through a basketball hoop, wouldn't it? RAZ: Do you think that you just had that switch in your brain that was like, yes, math. I love it. It's awesome. SPENCER: It's a really difficult question 'cause with me, it goes back so far that I don't even remember if I had to try all that hard. But I do remember that having loved it, I did more and more. Like practically anything, it is a practice thing. And because it's a subject with that finite correct, incorrect sort of line, it is the thing where, to an extent, you can teach yourself. You know if you're getting it right. You're not teaching yourself bad habits. And I just loved it more than anyone else I knew. (SOUNDBITE OF TED TALK) SPENCER: I fell in love with mathematics from the earliest of ages. I explained it to all my friends. Maths is beautiful. It's natural. It's everywhere. Numbers are the musical notes with which the symphony of the universe is written. Today I want to show you one of those musical notes, a number so beautiful, so massive I think it will blow your mind. Today we're going to talk about prime numbers. RAZ: Prime numbers - let's just remind everybody what a prime number is. SPENCER: OK. So six is not prime. . . RAZ: Right. SPENCER: . . . Because we can break it down into six equals two times three. Seven is prime because seven is one times seven, but you can't break it into any smaller multiplying building blocks. So the primes are the sort of building blocks that all the other numbers come out from. If I throw you a number - if I say 26 - well, turns out that's not prime. RAZ: Nope. SPENCER: It's two times 13. OK. What about 29? RAZ: Yep. SPENCER: That is prime. You can't break it down. Every number has to be prime or composite. Primes go on forever. There is no final, biggest prime number. A beautiful mathematician called Euclid proved that thousands of years ago. So it makes sense. In any given time, there must be a largest prime number that we know about. And my TED talk back in 2013 was the history of the largest prime numbers we've detected. Initially, it was all just humans doing phenomenal things with their brains. And I was going to say pen and paper - not even pen, you know? Quill. . . RAZ: Quill, yeah. SPENCER: . . . And ink and chalk and things like that with equations pulling down that are just unbelievable to think a human mind could come up with free of any device. (SOUNDBITE OF TED TALK) SPENCER: This is the great Swiss mathematician Leonard Euler. In the 1700s, other mathematicians said he is simply the master of us all. Euler discovered, at the time, the world's biggest prime - two to the 31 minus one. It's over 2 billion. You think that's big. We know that two to the power of 127 minus one is a prime number. It's an absolute brute. Look at it here - 39 digits long, proven to be prime in 1876 by a mathematician called Lucas. Word up, L dog. The massive prime numbers all follow a cute little formula. I'll give you a really easy example. RAZ: Sure. SPENCER: Let's take two, and let's multiply two by itself three twos. Two times two is four, times two gets us to eight. Let's take away one from that. So we had two times two times two, take away one is seven, which just happens to be a prime number. RAZ: Yeah. SPENCER: All the massive prime numbers we've ever detected are of the form two multiplied together heaps of times, take away one. And the latest one that we uncovered in December of last year - take the number two. Write down not one two, not three twos, like I had earlier. Write down 82,589,993 twos. You end up with a 24-million-digit-long number. RAZ: Wow. SPENCER: And we know that single number is prime as confidently as we know the number seven is prime. (SOUNDBITE OF MUSIC) SPENCER: I just think that's just mind-numbingly beautiful. (SOUNDBITE OF TED TALK) SPENCER: My laptop at home was looking through four potential candidate primes myself as part of a networked computer hunt around the world for these large numbers. The discovery of that prime was similar to the work people are doing in unraveling RNA sequences, in searching through data from SETI and other astronomical projects. We live in an age where some of the great breakthroughs are not going to happen in the labs or the halls of academia but on laptops, desktops, in the palms of people's hands who are simply helping out for the search. But for me, it's amazing because it's a metaphor for the time in which we live, when human minds and machines can conquer together. (SOUNDBITE OF MUSIC) RAZ: So right now, as we're sitting here talking on the radio, you've got a computer in your house that's just, like, you know, looking for prime numbers. SPENCER: Yeah. There's a project called GIMPS. The more technical, mathematical name is Mersenne - M-E-R-S-E-N-N-E - from a guy who researched a monk back in the 1600s of all things. And so GIMPS is the Great Internet Mersenne Prime Search. Anyone can do this. You take your laptop and download the GIMPS software. It will give you a candidate prime. And in the background, while your computer's doing nothing else, it will just search. Now, it would take four to six weeks before it comes back and says yes or no. And you're almost always going to be disappointed and told no. But if you think about the amount of super computing power that is just sitting on people's desks, in their man caves, in the office at work over the weekend, on their phones, just unused - there are problems out there we want solved. And the GIMPS prime search is just a great, little, nerdy example of that. RAZ: All right. So there are people looking for these monster prime numbers. And the latest one was discovered by this guy Patrick Laroche, right? SPENCER: Yeah. Laroche is the latest one, yes. RAZ: What's the point? Like, what's the practical application of a prime number? SPENCER: Big-sized prime numbers - 20 digits long, those sort of things - underpin all Internet security. And the reason that you can use your credit cards online, et cetera, is to do with algorithms based on very large prime numbers. And the best sort of practical application for large numbers like this is they're a great way to test the speed and accuracy of potential new computer chips. If my laptop is working on a Pentium 15BZ and I think that's the greatest chip in the world, and you say, well, I've come up with the double Pentium 13X - OK. Well, let's ask them the same simple question with the same eight lines of code. And let's let the computers go and decide for us. Now, if your one comes back in only three weeks and it solves something that took my computer five weeks, you've got yourself a really fast, impressive, new computer chip. So speed and accuracy testing of computer chips these days - well worth it. And it's also just another small piece in the deeper puzzle. One of the reasons we're so attracted to prime numbers is they're so basic. They're so fundamental. We know nothing about them. Some of the most famous problems - unsolved problems in the history of mathematics are to do with the distribution of prime numbers, the amount of prime numbers you have after a certain point and things like that. So any small step towards understanding them more, I think, is a good thing. RAZ: That's Adam Spencer. He's the first-ever ambassador of science and mathematics for the University of Sydney in Australia. You can find his full talk at ted. com. (SOUNDBITE OF FILM, \"SCHOOL OF ROCK\") JACK BLACK: (As Dewey, singing) Math is a wonderful thing. Math is a really cool thing. So get off your ath (ph). Let's do some math, math, math, math, math, math. (SOUNDBITE OF MUSIC) RAZ: Hey. Thanks so much for listening to our show on math this week. If you want to find out more about who was on it, go to ted. npr. org. And to see hundreds more TED Talks, check out ted. com or the TED app. Our production staff at NPR includes Jeff Rogers, Sanaz Meshkinpour, Jinae West, Neva Grant, Casey Herman, Rachel Faulkner, Diba Mohtasham, James Delahoussaye, Melissa Gray and J. C. Howard with help from Daniel Shukin. Our intern is Katie Monteleone. Our partners at TED are Chris Anderson, Colin Helms, Anna Phelan and Janet Lee. I'm Guy Raz. And you've been listening to ideas worth spreading right here on the TED Radio Hour from NPR.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-15-702501478": {"title": "Phylecia Jones: How Can We Encourage Girls To Keep Pursuing Math? : NPR", "url": "https://www.npr.org/2019/03/15/702501478/phylecia-jones-how-can-we-encourage-girls-to-keep-pursuing-math", "author": "No author found", "published_date": "2019-03-15", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. I'm Guy Raz. So a couple months ago, a 911 operator in Lafayette, Ind. , got a pretty unusual phone call. (SOUNDBITE OF ARCHIVED RECORDING)ANTONIA BUNDY: Nine one one. UNIDENTIFIED CHILD: Hi. I had a really bad day, and I just have tons of homework. RAZ: Fortunately, it was a quiet day there, so the operator kept talking to the caller. It was a kid who was stressed out about math. (SOUNDBITE OF ARCHIVED RECORDING)BUNDY: So what are you learning in math? What's so difficult? UNIDENTIFIED CHILD: Fractions. BUNDY: Is there a problem you want me to help you? UNIDENTIFIED CHILD: Yeah. What's 3/4 plus 1/4? RAZ: And eventually. . . (SOUNDBITE OF ARCHIVED RECORDING)BUNDY: So what's three plus one? UNIDENTIFIED CHILD: Four. BUNDY: So then four over four is what? UNIDENTIFIED CHILD: One. BUNDY: Yeah. Good job. UNIDENTIFIED CHILD: I'm sorry for calling you, but. . . BUNDY: No, you're. . . UNIDENTIFIED CHILD: . . . I really needed help. BUNDY: You're fine. We're always here to help. RAZ: OK. So we don't recommend calling 911 about a challenging math problem. But most of us have struggled with math before. And at some point, you might have even said, I'm just not a math person. So today on the show, we're going to dig into some of the myths we tell ourselves about math, why so many of us are afraid of it or even hate it, and why we should think about rewiring the way we think about it because math is woven into the patterns of the universe. It allows us to understand big ideas. And in a way, it's kind of beautiful. PHYLECIA JONES: I actually do see the beauty in math. And I see it in so many ways because it can literally change your life if you embrace it. It can take you from one place, change your economics, change your family, change how you see the world and how you interact with it. And I just find it so beautiful. RAZ: This is Phylecia Jones. She's a former computer scientist from the U. S. Navy. So I think it's fair to say that you're pretty good at math. Like, it - you can handle math pretty well. JONES: Yeah, I'm pretty good. I'm pretty good. Not late at night, but I'm pretty good. RAZ: And is it fair to say that you kind of like it? JONES: I do like it. Math is so much fun to me. And I actually quit my job and started money coaching with people, and that's when I came - happened upon this idea that some people don't think they're good at math. RAZ: And that kind of thinking tends to start pretty young. And Phylecia's noticed that oftentimes the kids who think they're bad at math are girls. So she decided to do something about it. Here's more from Phylecia Jones on the TED stage. (SOUNDBITE OF TED TALK)JONES: Did you know that 15 is the exact age a girl loses interest in math? And little does this 15-year-old know is that she's kicked off a domino effect of companies, organizations, institutions and governments, even dining room conversations, asking one simple question - how do we get more girls interested in math? Now, I've been a part of these conversations over the last few months and had some heated debates with friends, and what I've realized is that no one has an answer. And this is kind of sad to me because I see the world through math-colored glasses, and I can see that it can take you anywhere that you want to go, and this is why I care. And you might be saying, OK, Phylecia, that's a good idea. Yay, math. Let's all get on board. And you might be saying in the back of your mind, like, why do I really care? Here's why you care - because Katherine Johnson was a 15-year-old girl who later became a woman and who was encouraged by her parents, and she helped us to get to the moon. (APPLAUSE)JONES: Patricia Bath was a 15-year-old girl who later pursued medicine, and because she liked to tinker, she created a device that corrected cataracts so people could have vision. But there is a 15-year-old girl that might be sitting next to you or in your house right now that saw her parents go through the Great Recession and never fully recover and is sitting on an idea that can change how we manage finances, but she's about to close the book on it because she's not getting the one thing that we can do - and that is support. Now, we know it takes a village to raise a child, but it takes a community of hardcore supporters who give a damn to make sure a young girl stays encouraged and has confidence to pursue a career in math. (SOUNDBITE OF MUSIC)RAZ: So you have a very simple, elegant idea to start to change this paradigm, which I love. It's so simple. It's a big idea. What is it? Explain what it is. JONES: I want us to tell every girl, every woman in our life that she is great at math. And look them in the eye, even if they start getting that little cringe where they say, no, I'm not great at it. I don't like numbers. I try to tell people, stop, you are actually really good at math. We just need to reprogram you. RAZ: Yeah. I mean, people - like, many, many people and many kids think that they just don't have a math brain, right? They either do or they don't. And what it sounds like what you're saying is, that's not the way to talk about this. It's that everybody has a math brain. Some people are going to take more - you know, take to it more easily or faster, but that everybody actually can understand this, like anyone can learn a language. JONES: Yeah, anyone can learn how to do math. We just need to make sure we don't discourage people from learning math. And no, not everyone is going to end up like a Katherine Johnson and doing math to get us to the moon. But even with some of the basic ideas, we need to just make sure that people know that they're great at it because I truly believe that when people know that they're good at something, they will have the confidence to pursue some of these careers that are out there that are running our world. I mean, technology, science, engineering is running our world. It can really change how we interact with it if there are no women around in technology. That's kind of scary for me. (SOUNDBITE OF TEDx TALK)JONES: You know what's at risk - that if you don't do something, that if we don't do something, we run the risk of women being unwanted when it comes to the future innovation that can change our world. Are you willing to risk that? For every woman that says that she's not great at math, I want you to make sure she stands on her two feet. And you look her in the eye and you say, yes, you are, and I believe in you. For every 15-year-old that's about to close the book and say, you know what? Math is not my thing. Get her the support that she needs so that she can get better. For every 11-year-old that is jazzed about her coding club, her robotics team - here's the thing with those girls. They already know they're good at math. Our job is not to screw them up. (LAUGHTER)JONES: And from the moment that they are born, I just want you to do one simple thing. When they are in your arms, I just want you to remind them every day that all girls are great at math. (APPLAUSE)RAZ: That's Phylecia Jones. She's a former computer scientist. And today, Phylecia runs a business helping people manage their finances. You can find her full talk at ted. npr. org. GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. So a couple months ago, a 911 operator in Lafayette, Ind. , got a pretty unusual phone call. (SOUNDBITE OF ARCHIVED RECORDING) ANTONIA BUNDY: Nine one one. UNIDENTIFIED CHILD: Hi. I had a really bad day, and I just have tons of homework. RAZ: Fortunately, it was a quiet day there, so the operator kept talking to the caller. It was a kid who was stressed out about math. (SOUNDBITE OF ARCHIVED RECORDING) BUNDY: So what are you learning in math? What's so difficult? UNIDENTIFIED CHILD: Fractions. BUNDY: Is there a problem you want me to help you? UNIDENTIFIED CHILD: Yeah. What's 3/4 plus 1/4? RAZ: And eventually. . . (SOUNDBITE OF ARCHIVED RECORDING) BUNDY: So what's three plus one? UNIDENTIFIED CHILD: Four. BUNDY: So then four over four is what? UNIDENTIFIED CHILD: One. BUNDY: Yeah. Good job. UNIDENTIFIED CHILD: I'm sorry for calling you, but. . . BUNDY: No, you're. . . UNIDENTIFIED CHILD: . . . I really needed help. BUNDY: You're fine. We're always here to help. RAZ: OK. So we don't recommend calling 911 about a challenging math problem. But most of us have struggled with math before. And at some point, you might have even said, I'm just not a math person. So today on the show, we're going to dig into some of the myths we tell ourselves about math, why so many of us are afraid of it or even hate it, and why we should think about rewiring the way we think about it because math is woven into the patterns of the universe. It allows us to understand big ideas. And in a way, it's kind of beautiful. PHYLECIA JONES: I actually do see the beauty in math. And I see it in so many ways because it can literally change your life if you embrace it. It can take you from one place, change your economics, change your family, change how you see the world and how you interact with it. And I just find it so beautiful. RAZ: This is Phylecia Jones. She's a former computer scientist from the U. S. Navy. So I think it's fair to say that you're pretty good at math. Like, it - you can handle math pretty well. JONES: Yeah, I'm pretty good. I'm pretty good. Not late at night, but I'm pretty good. RAZ: And is it fair to say that you kind of like it? JONES: I do like it. Math is so much fun to me. And I actually quit my job and started money coaching with people, and that's when I came - happened upon this idea that some people don't think they're good at math. RAZ: And that kind of thinking tends to start pretty young. And Phylecia's noticed that oftentimes the kids who think they're bad at math are girls. So she decided to do something about it. Here's more from Phylecia Jones on the TED stage. (SOUNDBITE OF TED TALK) JONES: Did you know that 15 is the exact age a girl loses interest in math? And little does this 15-year-old know is that she's kicked off a domino effect of companies, organizations, institutions and governments, even dining room conversations, asking one simple question - how do we get more girls interested in math? Now, I've been a part of these conversations over the last few months and had some heated debates with friends, and what I've realized is that no one has an answer. And this is kind of sad to me because I see the world through math-colored glasses, and I can see that it can take you anywhere that you want to go, and this is why I care. And you might be saying, OK, Phylecia, that's a good idea. Yay, math. Let's all get on board. And you might be saying in the back of your mind, like, why do I really care? Here's why you care - because Katherine Johnson was a 15-year-old girl who later became a woman and who was encouraged by her parents, and she helped us to get to the moon. (APPLAUSE) JONES: Patricia Bath was a 15-year-old girl who later pursued medicine, and because she liked to tinker, she created a device that corrected cataracts so people could have vision. But there is a 15-year-old girl that might be sitting next to you or in your house right now that saw her parents go through the Great Recession and never fully recover and is sitting on an idea that can change how we manage finances, but she's about to close the book on it because she's not getting the one thing that we can do - and that is support. Now, we know it takes a village to raise a child, but it takes a community of hardcore supporters who give a damn to make sure a young girl stays encouraged and has confidence to pursue a career in math. (SOUNDBITE OF MUSIC) RAZ: So you have a very simple, elegant idea to start to change this paradigm, which I love. It's so simple. It's a big idea. What is it? Explain what it is. JONES: I want us to tell every girl, every woman in our life that she is great at math. And look them in the eye, even if they start getting that little cringe where they say, no, I'm not great at it. I don't like numbers. I try to tell people, stop, you are actually really good at math. We just need to reprogram you. RAZ: Yeah. I mean, people - like, many, many people and many kids think that they just don't have a math brain, right? They either do or they don't. And what it sounds like what you're saying is, that's not the way to talk about this. It's that everybody has a math brain. Some people are going to take more - you know, take to it more easily or faster, but that everybody actually can understand this, like anyone can learn a language. JONES: Yeah, anyone can learn how to do math. We just need to make sure we don't discourage people from learning math. And no, not everyone is going to end up like a Katherine Johnson and doing math to get us to the moon. But even with some of the basic ideas, we need to just make sure that people know that they're great at it because I truly believe that when people know that they're good at something, they will have the confidence to pursue some of these careers that are out there that are running our world. I mean, technology, science, engineering is running our world. It can really change how we interact with it if there are no women around in technology. That's kind of scary for me. (SOUNDBITE OF TEDx TALK) JONES: You know what's at risk - that if you don't do something, that if we don't do something, we run the risk of women being unwanted when it comes to the future innovation that can change our world. Are you willing to risk that? For every woman that says that she's not great at math, I want you to make sure she stands on her two feet. And you look her in the eye and you say, yes, you are, and I believe in you. For every 15-year-old that's about to close the book and say, you know what? Math is not my thing. Get her the support that she needs so that she can get better. For every 11-year-old that is jazzed about her coding club, her robotics team - here's the thing with those girls. They already know they're good at math. Our job is not to screw them up. (LAUGHTER) JONES: And from the moment that they are born, I just want you to do one simple thing. When they are in your arms, I just want you to remind them every day that all girls are great at math. (APPLAUSE) RAZ: That's Phylecia Jones. She's a former computer scientist. And today, Phylecia runs a business helping people manage their finances. You can find her full talk at ted. npr. org.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-16-704039029": {"title": "The Efforts To Misdirect In Shooter's Screed : NPR", "url": "https://www.npr.org/2019/03/16/704039029/the-efforts-to-misdirect-in-shooters-screed", "author": "No author found", "published_date": "2019-03-16", "content": "MELISSA BLOCK, HOST: The man who has claimed responsibility for the attack in New Zealand on worshippers in two mosques was active online, posting a violent, hate-filled screed just before the attacks. Embedded in that document are references that those who follow online networks are very familiar with - a minefield of misdirection. The term for that is profane. And this is the radio, so we will call it scatposting. And our next guest says it's a tool the shooter used to both mask his intent and to make it spread. Robert Evans writes about this in a piece for Bellingcat, an online investigative platform. And he joins me now. Thanks for being with us. ROBERT EVANS: Thanks for having me on. BLOCK: You call this man's screed and his other provocative activity online - you call this booby traps. What do you mean by that? EVANS: I mean his goal is to provoke and sow division between the right and the left. A lot of the comments that he put in his manifesto are, essentially, things that he knew would be picked up by the world media and cause the right and left to get to each other's throats. Like, that was part of the goal - is to get people angry and fighting. BLOCK: And can you give me an example? EVANS: Yeah. He brought up Candace Owens, who is a far-right YouTube personality and credited her for his radicalization, which is - cannot be true. He said that she was more extreme than him. This is a man who just shot dozens of people at a mosque. She's not more extreme than him. BLOCK: He also mentions President Trump. EVANS: He does mention President Trump. He says that he respects President Trump as a white nationalist figure but doesn't think that he's going to fix any of the problems that this guy sees. And, you know, that is probably partly calculated to increase that division between left and right. BLOCK: Which brings up the question of how you know whether this actually reflects his intent and what he believes and how much is just chaff - just stuff he's throwing out there to misdirect. EVANS: The stuff that he mentions only once, like Candace Owens - that is chaff. The stuff that he mentions repeatedly that is a deeper throughline, that's something you can assume he really, truly believes. So there are so many different in-jokes for these little online communities, like 8chan's /pol/ board, that it has to have been a significant part of his radicalization because that's the air he breathes. He's internalized so much of that. BLOCK: And when you mention 8chan, what exactly is 8chan? EVANS: Is essentially the darkest, dankest corner of the Internet. It is basically a neo-Nazi gathering place. And its primary purpose is to radicalize more people into eventual acts of violent, far-right terror. BLOCK: When you think about what we're calling scatposting, there seem to be two things at play here. One would be sort of being a jerk in comments on a Twitter or Facebook post. And then there's this other, much darker stream, which is some sort of attempt to lure people in to radicalize them, I guess. EVANS: Yeah. One of the way - and I want to make it clear, scatposting, the term that you're using, is not an explicitly fascist or neo-Nazi thing. It's a tactic anyone can use for any reason. You know, the goal is to cause disruption and distraction. And the goal is to, essentially, stop any kind of productive conversation from happening. And to an extent, this is a very old tactic on the far right. The founder of the American Nazi Party, the inventor of neo-Nazism and the first Holocaust denier, was a guy named George Lincoln Rockwell, who started the American Nazi Party in 1959 and was active throughout the '60s. Rockwell was a guy who did a lot of in-person what we would call trolling. He would show up at Martin Luther King's marches with very racist signs and people in, you know, racist costumes in an attempt to distract media attention from Martin Luther King Jr. 's rallies to what he was doing. And Rockwell's belief was very much that the only way to change people's minds was to get them in a heightened emotional state. So it didn't matter how you did that. If you could make them angry, you could influence them. BLOCK: I wonder, Robert, even in having this discussion - you and I - whether we are sort of falling into the trap ourselves, whether this is exactly what people want - that they provoke. And they instigate discussion. And more and more people hear about them. And that's their goal. EVANS: I mean, I think the thing that you have to avoid is - part of it is the emotional provocation. So it is important to talk about what these people's goals are because you - you know, they're killing people. You can't ignore them. You have to try to get better at spotting them ahead of time, shutting them down and stopping the spread of radicalization. So I try not to talk about the specifics of their ideology so much. Like, you shouldn't engage them in debates about immigration. But you should know where they're gathering on the Internet, what kind of terms that they are using and what their goal is with a manifesto like this. You can't just ignore them. We ignored them for too long. What we're looking at here is essentially a transnational fascist radicalizing movement. But it's an acephalous movement. It has no head. There's no structure. It's not like al-Qaida or ISIS, where there's a leadership cadre, where there's funding. It's just a lot of people with very similar ideas and a desire to do violence and inspire violence in others. And you have to understand what they're doing if you're going to stop it. BLOCK: That's Robert Evans, who reports on online extremists. He's written about the New Zealand shooter's posts for the online investigative platform Bellingcat. Robert, thanks so much for speaking with us. EVANS: Thanks for having me. MELISSA BLOCK, HOST:  The man who has claimed responsibility for the attack in New Zealand on worshippers in two mosques was active online, posting a violent, hate-filled screed just before the attacks. Embedded in that document are references that those who follow online networks are very familiar with - a minefield of misdirection. The term for that is profane. And this is the radio, so we will call it scatposting. And our next guest says it's a tool the shooter used to both mask his intent and to make it spread. Robert Evans writes about this in a piece for Bellingcat, an online investigative platform. And he joins me now. Thanks for being with us. ROBERT EVANS: Thanks for having me on. BLOCK: You call this man's screed and his other provocative activity online - you call this booby traps. What do you mean by that? EVANS: I mean his goal is to provoke and sow division between the right and the left. A lot of the comments that he put in his manifesto are, essentially, things that he knew would be picked up by the world media and cause the right and left to get to each other's throats. Like, that was part of the goal - is to get people angry and fighting. BLOCK: And can you give me an example? EVANS: Yeah. He brought up Candace Owens, who is a far-right YouTube personality and credited her for his radicalization, which is - cannot be true. He said that she was more extreme than him. This is a man who just shot dozens of people at a mosque. She's not more extreme than him. BLOCK: He also mentions President Trump. EVANS: He does mention President Trump. He says that he respects President Trump as a white nationalist figure but doesn't think that he's going to fix any of the problems that this guy sees. And, you know, that is probably partly calculated to increase that division between left and right. BLOCK: Which brings up the question of how you know whether this actually reflects his intent and what he believes and how much is just chaff - just stuff he's throwing out there to misdirect. EVANS: The stuff that he mentions only once, like Candace Owens - that is chaff. The stuff that he mentions repeatedly that is a deeper throughline, that's something you can assume he really, truly believes. So there are so many different in-jokes for these little online communities, like 8chan's /pol/ board, that it has to have been a significant part of his radicalization because that's the air he breathes. He's internalized so much of that. BLOCK: And when you mention 8chan, what exactly is 8chan? EVANS: Is essentially the darkest, dankest corner of the Internet. It is basically a neo-Nazi gathering place. And its primary purpose is to radicalize more people into eventual acts of violent, far-right terror. BLOCK: When you think about what we're calling scatposting, there seem to be two things at play here. One would be sort of being a jerk in comments on a Twitter or Facebook post. And then there's this other, much darker stream, which is some sort of attempt to lure people in to radicalize them, I guess. EVANS: Yeah. One of the way - and I want to make it clear, scatposting, the term that you're using, is not an explicitly fascist or neo-Nazi thing. It's a tactic anyone can use for any reason. You know, the goal is to cause disruption and distraction. And the goal is to, essentially, stop any kind of productive conversation from happening. And to an extent, this is a very old tactic on the far right. The founder of the American Nazi Party, the inventor of neo-Nazism and the first Holocaust denier, was a guy named George Lincoln Rockwell, who started the American Nazi Party in 1959 and was active throughout the '60s. Rockwell was a guy who did a lot of in-person what we would call trolling. He would show up at Martin Luther King's marches with very racist signs and people in, you know, racist costumes in an attempt to distract media attention from Martin Luther King Jr. 's rallies to what he was doing. And Rockwell's belief was very much that the only way to change people's minds was to get them in a heightened emotional state. So it didn't matter how you did that. If you could make them angry, you could influence them. BLOCK: I wonder, Robert, even in having this discussion - you and I - whether we are sort of falling into the trap ourselves, whether this is exactly what people want - that they provoke. And they instigate discussion. And more and more people hear about them. And that's their goal. EVANS: I mean, I think the thing that you have to avoid is - part of it is the emotional provocation. So it is important to talk about what these people's goals are because you - you know, they're killing people. You can't ignore them. You have to try to get better at spotting them ahead of time, shutting them down and stopping the spread of radicalization. So I try not to talk about the specifics of their ideology so much. Like, you shouldn't engage them in debates about immigration. But you should know where they're gathering on the Internet, what kind of terms that they are using and what their goal is with a manifesto like this. You can't just ignore them. We ignored them for too long. What we're looking at here is essentially a transnational fascist radicalizing movement. But it's an acephalous movement. It has no head. There's no structure. It's not like al-Qaida or ISIS, where there's a leadership cadre, where there's funding. It's just a lot of people with very similar ideas and a desire to do violence and inspire violence in others. And you have to understand what they're doing if you're going to stop it. BLOCK: That's Robert Evans, who reports on online extremists. He's written about the New Zealand shooter's posts for the online investigative platform Bellingcat. Robert, thanks so much for speaking with us. EVANS: Thanks for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-17-703761872": {"title": "Flying Taxis Are Just A Few Years Away, If You Believe Tech Optimists : NPR", "url": "https://www.npr.org/2019/03/17/703761872/flying-taxis-seriously", "author": "No author found", "published_date": "2019-03-17", "content": "LULU GARCIA-NAVARRO, HOST: Two words for you - flying taxis. That's right. In the not-so-distant future, you'll open your ride-hailing app. And next to car, SUV and bicycle, you'll see this option - flying car. At least, that's according to the optimists at the South by Southwest conference. Here's NPR's Aarti Shahani. AARTI SHAHANI, BYLINE: You might be the passenger who hails a taxi and heads to the rooftop, where, at the helipad, your ride is waiting. It might look like a minivan with wings and four seats or more like a gigantic drone. Either way, it won't fly itself any time soon. One seat will be reserved for the driver pilot. CAREY CANNON: If air taxis are going to be what everybody wants them to be, you know, in thousands at a city, for example, you won't be able to find enough conventional pilots. SHAHANI: Carey Cannon, a chief engineer at Bell - we're in a crowded pavilion at South by Southwest, the annual tech, music, film convention in Austin. Cannon has set up a virtual reality simulation of what it feels like to drive one of these small, flying vehicles of the future. CANNON: OK. I'll walk you through it a little bit. She's just about to go. SHAHANI: He puts a headset on me and something like a joystick in my hand. I slip into a gamer chair, only I'm more than a gamer. I'm a trainer for Bell's computer software. CANNON: All right. Now go ahead and roll the thumb wheel up to give it some power. SHAHANI: How I and others drive will become data, training data for the artificial intelligence that'll take over much of the job. My VR flying taxi lifts off into the Las Vegas skyline. CANNON: You're just following the green dots as they go. SHAHANI: All right. CANNON: (Laughter). SHAHANI: Oh. The dream of flying cars is at least as old as the automobile itself. Bell, which makes attack helicopters for the U. S. Navy, is working on this new project with another high-profile partner - Uber Technologies. Boeing and Airbus also have prototypes of these flying cars in the works. Uber has become the face of the aerial mobility movement as it has the most public campaign touting their work so far. Elon Musk says he'll get us to Mars. Uber says they'll get a millennial from San Francisco to San Jose in 15 minutes flat. And their timeline for this flying taxi that does not yet exist - 2023. I moderated a panel at South By. By way of show of hands, how many of you believe it? Half the audience buys that remarkable goal. When I asked if it'll happen within a decade. . . Everyone, pretty much. Jaiwon Shin with NASA, also an Uber partner, was on the panel. While he thinks Uber is being a touch bullish - he put the timeline further out to the mid-2020s - Shin gives his thesis for why it's close. JAIWON SHIN: Convergence of many different technologies are maturing to the level that now aviation can benefit to put these things together. SHAHANI: The batteries that power electric cars can power flight. Companies can stockpile and pull data and build artificial intelligence to take over air traffic control, manage the thousands of drones and taxis in the air. Also, his partner Uber is really well-connected to politicians and regulators. Shin says to move quickly, it'll take technologists and policymakers coordinating. SHIN: If one segment is lagging behind, this is not going to happen. SHAHANI: When we build the whiz-bangy (ph) future, it's good practice to pause and consider the downsides. That's what South by Southwest attendee Cheryl Garabet did when she stepped up to the mic. Right now, in major cities, people in bumper-to-bumper traffic or riding the subway have to see each other. With flying cars, the haves can escape to the air and leave the have-nots forgotten in their potholes. CHERYL GARABET: I think of a very dystopian - all of us with money flying around, you know, looking down at the poor homeless, who have no options in that regard. Like, how can cities prepare so that there's not this awful dystopian future for all of us with flying vehicles? SHAHANI: A strong dose of skepticism to balance the techno-optimism - while no flying taxi exists yet, Uber has dared to estimate the near-term cost. That San Francisco to San Jose trip - $43. Aarti Shahani, NPR News, Austin. (SOUNDBITE OF SNARKY PUPPY'S \"BLING BLING\") LULU GARCIA-NAVARRO, HOST:  Two words for you - flying taxis. That's right. In the not-so-distant future, you'll open your ride-hailing app. And next to car, SUV and bicycle, you'll see this option - flying car. At least, that's according to the optimists at the South by Southwest conference. Here's NPR's Aarti Shahani. AARTI SHAHANI, BYLINE: You might be the passenger who hails a taxi and heads to the rooftop, where, at the helipad, your ride is waiting. It might look like a minivan with wings and four seats or more like a gigantic drone. Either way, it won't fly itself any time soon. One seat will be reserved for the driver pilot. CAREY CANNON: If air taxis are going to be what everybody wants them to be, you know, in thousands at a city, for example, you won't be able to find enough conventional pilots. SHAHANI: Carey Cannon, a chief engineer at Bell - we're in a crowded pavilion at South by Southwest, the annual tech, music, film convention in Austin. Cannon has set up a virtual reality simulation of what it feels like to drive one of these small, flying vehicles of the future. CANNON: OK. I'll walk you through it a little bit. She's just about to go. SHAHANI: He puts a headset on me and something like a joystick in my hand. I slip into a gamer chair, only I'm more than a gamer. I'm a trainer for Bell's computer software. CANNON: All right. Now go ahead and roll the thumb wheel up to give it some power. SHAHANI: How I and others drive will become data, training data for the artificial intelligence that'll take over much of the job. My VR flying taxi lifts off into the Las Vegas skyline. CANNON: You're just following the green dots as they go. SHAHANI: All right. CANNON: (Laughter). SHAHANI: Oh. The dream of flying cars is at least as old as the automobile itself. Bell, which makes attack helicopters for the U. S. Navy, is working on this new project with another high-profile partner - Uber Technologies. Boeing and Airbus also have prototypes of these flying cars in the works. Uber has become the face of the aerial mobility movement as it has the most public campaign touting their work so far. Elon Musk says he'll get us to Mars. Uber says they'll get a millennial from San Francisco to San Jose in 15 minutes flat. And their timeline for this flying taxi that does not yet exist - 2023. I moderated a panel at South By. By way of show of hands, how many of you believe it? Half the audience buys that remarkable goal. When I asked if it'll happen within a decade. . . Everyone, pretty much. Jaiwon Shin with NASA, also an Uber partner, was on the panel. While he thinks Uber is being a touch bullish - he put the timeline further out to the mid-2020s - Shin gives his thesis for why it's close. JAIWON SHIN: Convergence of many different technologies are maturing to the level that now aviation can benefit to put these things together. SHAHANI: The batteries that power electric cars can power flight. Companies can stockpile and pull data and build artificial intelligence to take over air traffic control, manage the thousands of drones and taxis in the air. Also, his partner Uber is really well-connected to politicians and regulators. Shin says to move quickly, it'll take technologists and policymakers coordinating. SHIN: If one segment is lagging behind, this is not going to happen. SHAHANI: When we build the whiz-bangy (ph) future, it's good practice to pause and consider the downsides. That's what South by Southwest attendee Cheryl Garabet did when she stepped up to the mic. Right now, in major cities, people in bumper-to-bumper traffic or riding the subway have to see each other. With flying cars, the haves can escape to the air and leave the have-nots forgotten in their potholes. CHERYL GARABET: I think of a very dystopian - all of us with money flying around, you know, looking down at the poor homeless, who have no options in that regard. Like, how can cities prepare so that there's not this awful dystopian future for all of us with flying vehicles? SHAHANI: A strong dose of skepticism to balance the techno-optimism - while no flying taxi exists yet, Uber has dared to estimate the near-term cost. That San Francisco to San Jose trip - $43. Aarti Shahani, NPR News, Austin. (SOUNDBITE OF SNARKY PUPPY'S \"BLING BLING\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-18-704600310": {"title": "Russia's New Law Banning Fake News Is Censorship, Critics Say  : NPR", "url": "https://www.npr.org/2019/03/18/704600310/russia-criminalizes-the-spread-of-online-news-which-disrespects-the-government", "author": "No author found", "published_date": "2019-03-18", "content": "", "section": "World", "disclaimer": ""}, "2019-03-18-704562373": {"title": "YouTube's Chief Product Officer On How The Company Responded To Mosque Shootings : NPR", "url": "https://www.npr.org/2019/03/18/704562373/youtubes-chief-product-officer-on-how-the-company-responded-to-mosque-shootings", "author": "No author found", "published_date": "2019-03-18", "content": "AILSA CHANG, HOST: As it's been reported, the alleged shooter in the New Zealand attack recorded the massacre on video and streamed it live on Facebook. Even though Facebook took down that video, people who had seen it made copies - millions of copies - and posted them all over the Web, which means social media platforms have been scrambling since Friday to get rid of all of them. To give us an inside look at this challenge, we are now joined by Neal Mohan. He's chief product officer at YouTube. Welcome. NEAL MOHAN: Thank you for having me. CHANG: So give us a sense of the scale of this challenge. Facebook said it removed 1 1/2 million videos of this attack in the first 24 hours. How many did you guys take down at YouTube? MOHAN: Yeah. So as I think it's been widely reported, the volumes at which that content was being copied and then re-uploaded to our platform was unprecedented in nature. To give you a little bit of idea of that, at some points during, you know, the first few hours, we saw on the order of one upload a second. . . CHANG: Wow. MOHAN: . . . To our platform. CHANG: And how many copies of this video or versions of this video did YouTube eventually take down in the first 24 hours? MOHAN: It was one single video as the starting place, and - but that video had multiple permutations and combinations where it was sliced and diced - not just the video stream itself, but, you know, the Facebook page in which that video was being streamed. And so in terms of bringing that content down from our platform, we had to deal with not just the original video and its copies, but also all the permutations and combinations. And we brought down on the orders of tens of thousands of those. CHANG: And can you explain how that intensely complicates the task of taking these videos down - when a video gets sliced and diced or when a user adds a watermark to it or resizes it or animates it? MOHAN: Yeah. So as with many of these challenges, we use a combination of technology, machine learning algorithms, but also human beings to be able to make decisions that tend to be a bit more nuanced. CHANG: But I understand in the middle of all this, YouTube opted to rely solely on artificial intelligence and bypass the human moderators. Why was that? Why did you go completely with AI? MOHAN: Really, to put it simply, it was because of the unprecedented volume at which these uploads were coming to our platform and all the permutations and combinations. And so a few hours into the incident, we made the decision to remove all those videos that were being flagged by our algorithms. And we have a process by which an uploader can appeal that decision. And we understood that, obviously, if we were taking a step like this, legitimate news organizations' content would also get caught up in this. CHANG: Right. MOHAN: And so the mechanics for them are to appeal that decision, and then their video goes up. CHANG: A lot of experts who track the Islamic State have talked about how much progress YouTube has made over the past couple years in removing ISIS-related content. Why do you think YouTube seems to have more trouble when it comes to a different type of terrorism - when it's related to, say, white supremacy? MOHAN: Oftentimes, what happens in the case of those ISIS videos is they're being used for propaganda purposes, and you see things like branding or logos or other clues that might be in the video that allow us to find copies and permutations of them. In the case of an incident like this, literally a split second before the incident happens, there is no reference file for it. Every single one of these is different. In this case, this was particularly different, given the nature of how it was produced from a first-person standpoint. And so our algorithms are having to learn literally on the fly the second the incident happens without having the benefit of, you know, lots and lots of training data on which to have learned. CHANG: Right. So how confident are you at this point that YouTube has removed every single video of the massacre in New Zealand? MOHAN: Well, remember; there's still news content up on our platform. CHANG: Right. Apart from that? MOHAN: Right. Our technology gets better. We learn from every single one of these incidents. In this case, we learned a lot that, hopefully, we will incorporate into our technology and systems. I believe that we have a handle on the re-uploads at this point. But when you ask a question about a hundred percent or not, I think it's hard for me to give you a number like thatCHANG: Neal Mohan is chief product officer for YouTube. And we should note YouTube is among NPR's financial sponsors. Thank you very much for joining us. MOHAN: Thank you, Ailsa. AILSA CHANG, HOST:  As it's been reported, the alleged shooter in the New Zealand attack recorded the massacre on video and streamed it live on Facebook. Even though Facebook took down that video, people who had seen it made copies - millions of copies - and posted them all over the Web, which means social media platforms have been scrambling since Friday to get rid of all of them. To give us an inside look at this challenge, we are now joined by Neal Mohan. He's chief product officer at YouTube. Welcome. NEAL MOHAN: Thank you for having me. CHANG: So give us a sense of the scale of this challenge. Facebook said it removed 1 1/2 million videos of this attack in the first 24 hours. How many did you guys take down at YouTube? MOHAN: Yeah. So as I think it's been widely reported, the volumes at which that content was being copied and then re-uploaded to our platform was unprecedented in nature. To give you a little bit of idea of that, at some points during, you know, the first few hours, we saw on the order of one upload a second. . . CHANG: Wow. MOHAN: . . . To our platform. CHANG: And how many copies of this video or versions of this video did YouTube eventually take down in the first 24 hours? MOHAN: It was one single video as the starting place, and - but that video had multiple permutations and combinations where it was sliced and diced - not just the video stream itself, but, you know, the Facebook page in which that video was being streamed. And so in terms of bringing that content down from our platform, we had to deal with not just the original video and its copies, but also all the permutations and combinations. And we brought down on the orders of tens of thousands of those. CHANG: And can you explain how that intensely complicates the task of taking these videos down - when a video gets sliced and diced or when a user adds a watermark to it or resizes it or animates it? MOHAN: Yeah. So as with many of these challenges, we use a combination of technology, machine learning algorithms, but also human beings to be able to make decisions that tend to be a bit more nuanced. CHANG: But I understand in the middle of all this, YouTube opted to rely solely on artificial intelligence and bypass the human moderators. Why was that? Why did you go completely with AI? MOHAN: Really, to put it simply, it was because of the unprecedented volume at which these uploads were coming to our platform and all the permutations and combinations. And so a few hours into the incident, we made the decision to remove all those videos that were being flagged by our algorithms. And we have a process by which an uploader can appeal that decision. And we understood that, obviously, if we were taking a step like this, legitimate news organizations' content would also get caught up in this. CHANG: Right. MOHAN: And so the mechanics for them are to appeal that decision, and then their video goes up. CHANG: A lot of experts who track the Islamic State have talked about how much progress YouTube has made over the past couple years in removing ISIS-related content. Why do you think YouTube seems to have more trouble when it comes to a different type of terrorism - when it's related to, say, white supremacy? MOHAN: Oftentimes, what happens in the case of those ISIS videos is they're being used for propaganda purposes, and you see things like branding or logos or other clues that might be in the video that allow us to find copies and permutations of them. In the case of an incident like this, literally a split second before the incident happens, there is no reference file for it. Every single one of these is different. In this case, this was particularly different, given the nature of how it was produced from a first-person standpoint. And so our algorithms are having to learn literally on the fly the second the incident happens without having the benefit of, you know, lots and lots of training data on which to have learned. CHANG: Right. So how confident are you at this point that YouTube has removed every single video of the massacre in New Zealand? MOHAN: Well, remember; there's still news content up on our platform. CHANG: Right. Apart from that? MOHAN: Right. Our technology gets better. We learn from every single one of these incidents. In this case, we learned a lot that, hopefully, we will incorporate into our technology and systems. I believe that we have a handle on the re-uploads at this point. But when you ask a question about a hundred percent or not, I think it's hard for me to give you a number like that CHANG: Neal Mohan is chief product officer for YouTube. And we should note YouTube is among NPR's financial sponsors. Thank you very much for joining us. MOHAN: Thank you, Ailsa.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-18-700877189": {"title": "Toyota Ramps Up Hydrogen Fuel Cell Vehicles, As Japan Boosts Investment : NPR", "url": "https://www.npr.org/2019/03/18/700877189/japan-is-betting-big-on-the-future-of-hydrogen-cars", "author": "No author found", "published_date": "2019-03-18", "content": "AILSA CHANG, HOST: Auto companies are making more electric vehicles, ones powered completely by batteries. And sales should get a boost as more countries pass regulations to reduce carbon emissions. Japan, though, is betting big on something else. Susan Phillips of member station WHYY reports. SUSAN PHILLIPS, BYLINE: Mirai means future in Japanese. It's also the name of Toyota's hydrogen-powered fuel cell car. HIROO SASO: This is the engine noise of this car, so quiet. (SOUNDBITE OF DING)COMPUTER-GENERATED VOICE: (Speaking Japanese). PHILLIPS: I'm driving with my translator, Hiroo Saso, west of Tokyo, hoping to spot Mount Fuji. SASO: Then soon, turn right to the viewpoint. PHILLIPS: OK. SASO: Sorry, slow down. Slow down. PHILLIPS: Slow it down? When we pull over, the Mirai is a head-turner. One man wants to check under the hood. Unlike a combustion engine that burns gasoline, a hydrogen fuel cell doesn't burn anything. It uses a chemical reaction between the hydrogen and the oxygen from the air to produce electricity. The only emission out of its tailpipe is water. Tadashi Hirano is looking at the Mirai for the first time. SASO: You want to buy this - tempted to buy? TADASHI HIRANO: Maybe. PHILLIPS: Right now it's too expensive for him, even with a generous government subsidy that brings it down from the equivalent of $70,000 to $50,000. Toyota says the price will drop as production ramps up. (SOUNDBITE OF MACHINERY WHIRRING)PHILLIPS: At the LFA Works factory in Toyota city, workers install the carbon fiber hydrogen tanks on a new Mirai. These are all made by hand, only about 10 each day. Taiyo Kawai helps develop fuel cell infrastructure for Toyota. TAIYO KAWAI: (Speaking Japanese). PHILLIPS: Kawai says right now, Japan relies heavily on Middle East oil. Producing hydrogen fuel is energy intensive, but hydrogen is abundant. And the fuel could be produced anywhere. Toyota and other carmakers like Honda and Hyundai are also banking on the fact that hydrogen fuel cell cars are more convenient. Yoshikazu Tanaka is Toyota's chief engineer. YOSHIKAZU TANAKA: (Speaking Japanese). PHILLIPS: Tanaka says filling up a car with hydrogen is easier and faster than charging a battery on an electric vehicle. It makes more sense for people who live in dense cities and don't have a convenient place to plug in. A tank of hydrogen gets you farther than a fully charged battery, which makes it more efficient for trucks and buses. Ken Koyama from the Institute of Energy Economics, a Japanese think tank, agrees that hydrogen is a good bet. KEN KOYAMA: We are always talking about long-run future. It's not the next year or five-year time horizon. It's 20 years, 30 years, 40 years, 50 years - because if we are really thinking about climate change, it's a very, very long-term strategy. PHILLIPS: But so far there are only about 11,000 fuel-cell vehicles on the road worldwide, nearly half in California. The biggest challenge is building the infrastructure needed to drive them. Our first order of business now is to fill up the tank. SASO: Yeah, fill up the tank. PHILLIPS: Back in Tokyo, we find a station that has a big H2 sign. Nobody's here, just us. SASO: (Speaking Japanese). KEN KAWAKATSU: Maybe maximum - (speaking Japanese) - maximum, 15 cars a day. PHILLIPS: The station attendant, Ken Kawakatsu, says he doesn't get many customers - 15 a day at most. So right now, these stations are operating at a loss. But they're subsidized by the Japanese government. If all its investments pay off, the country hopes to have 200,000 hydrogen cars, trucks and buses on the roads here in Japan in the next six years. For NPR News, I'm Susan Phillips in Tokyo. AILSA CHANG, HOST:  Auto companies are making more electric vehicles, ones powered completely by batteries. And sales should get a boost as more countries pass regulations to reduce carbon emissions. Japan, though, is betting big on something else. Susan Phillips of member station WHYY reports. SUSAN PHILLIPS, BYLINE: Mirai means future in Japanese. It's also the name of Toyota's hydrogen-powered fuel cell car. HIROO SASO: This is the engine noise of this car, so quiet. (SOUNDBITE OF DING) COMPUTER-GENERATED VOICE: (Speaking Japanese). PHILLIPS: I'm driving with my translator, Hiroo Saso, west of Tokyo, hoping to spot Mount Fuji. SASO: Then soon, turn right to the viewpoint. PHILLIPS: OK. SASO: Sorry, slow down. Slow down. PHILLIPS: Slow it down? When we pull over, the Mirai is a head-turner. One man wants to check under the hood. Unlike a combustion engine that burns gasoline, a hydrogen fuel cell doesn't burn anything. It uses a chemical reaction between the hydrogen and the oxygen from the air to produce electricity. The only emission out of its tailpipe is water. Tadashi Hirano is looking at the Mirai for the first time. SASO: You want to buy this - tempted to buy? TADASHI HIRANO: Maybe. PHILLIPS: Right now it's too expensive for him, even with a generous government subsidy that brings it down from the equivalent of $70,000 to $50,000. Toyota says the price will drop as production ramps up. (SOUNDBITE OF MACHINERY WHIRRING) PHILLIPS: At the LFA Works factory in Toyota city, workers install the carbon fiber hydrogen tanks on a new Mirai. These are all made by hand, only about 10 each day. Taiyo Kawai helps develop fuel cell infrastructure for Toyota. TAIYO KAWAI: (Speaking Japanese). PHILLIPS: Kawai says right now, Japan relies heavily on Middle East oil. Producing hydrogen fuel is energy intensive, but hydrogen is abundant. And the fuel could be produced anywhere. Toyota and other carmakers like Honda and Hyundai are also banking on the fact that hydrogen fuel cell cars are more convenient. Yoshikazu Tanaka is Toyota's chief engineer. YOSHIKAZU TANAKA: (Speaking Japanese). PHILLIPS: Tanaka says filling up a car with hydrogen is easier and faster than charging a battery on an electric vehicle. It makes more sense for people who live in dense cities and don't have a convenient place to plug in. A tank of hydrogen gets you farther than a fully charged battery, which makes it more efficient for trucks and buses. Ken Koyama from the Institute of Energy Economics, a Japanese think tank, agrees that hydrogen is a good bet. KEN KOYAMA: We are always talking about long-run future. It's not the next year or five-year time horizon. It's 20 years, 30 years, 40 years, 50 years - because if we are really thinking about climate change, it's a very, very long-term strategy. PHILLIPS: But so far there are only about 11,000 fuel-cell vehicles on the road worldwide, nearly half in California. The biggest challenge is building the infrastructure needed to drive them. Our first order of business now is to fill up the tank. SASO: Yeah, fill up the tank. PHILLIPS: Back in Tokyo, we find a station that has a big H2 sign. Nobody's here, just us. SASO: (Speaking Japanese). KEN KAWAKATSU: Maybe maximum - (speaking Japanese) - maximum, 15 cars a day. PHILLIPS: The station attendant, Ken Kawakatsu, says he doesn't get many customers - 15 a day at most. So right now, these stations are operating at a loss. But they're subsidized by the Japanese government. If all its investments pay off, the country hopes to have 200,000 hydrogen cars, trucks and buses on the roads here in Japan in the next six years. For NPR News, I'm Susan Phillips in Tokyo.", "section": "Environment And Energy Collaborative", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-18-704458168": {"title": "MySpace Says It Lost Years Of User-Uploaded Music : NPR", "url": "https://www.npr.org/2019/03/18/704458168/myspace-says-it-lost-years-of-user-uploaded-music", "author": "No author found", "published_date": "2019-03-18", "content": "", "section": "Technology", "disclaimer": ""}, "2019-03-19-704831866": {"title": "Facebook's Alleged Discriminatory Ad Targeting To Change : NPR", "url": "https://www.npr.org/2019/03/19/704831866/after-lawsuits-facebook-announces-changes-to-alleged-discriminatory-ad-targeting", "author": "No author found", "published_date": "2019-03-19", "content": "", "section": "National", "disclaimer": ""}, "2019-03-19-704690054": {"title": "Facebook Admits Mosque Shooting Video Was Viewed At Least 4,000 Times  : NPR", "url": "https://www.npr.org/2019/03/19/704690054/facebook-admits-mosque-shooting-video-was-viewed-at-least-4-000-times", "author": "No author found", "published_date": "2019-03-19", "content": "", "section": "World", "disclaimer": ""}, "2019-03-19-701498785": {"title": "LIGO Gravitational Wave Detectors That Hunt For Ripples In Space-Time Upgraded : NPR", "url": "https://www.npr.org/2019/03/19/701498785/massive-u-s-machines-that-hunt-for-ripples-in-space-time-just-got-an-upgrade", "author": "No author found", "published_date": "2019-03-19", "content": "RACHEL MARTIN, HOST: Albert Einstein predicted that some cosmic smashups would be so powerful, they'd create ripples in the very fabric of the universe. A century later, physicists proved him right when they detected the ripples created by the collision of two black holes. The massive detectors used to make that discovery have now gotten an upgrade, and they are just about to start back up. NPR's Nell Greenfieldboyce reports. NELL GREENFIELDBOYCE, BYLINE: Gabby Gonzalez is a physicist at Louisiana State University who has spent years, decades, working with a team that was trying to detect something that had never been detected before. GABRIELA GONZALEZ: I have lots of friends that, now, they tell me, I felt so worried about your career because you were working on such a difficult thing that I thought it was never going to happen. And now, I'm so jealous (laughter). GREENFIELDBOYCE: They're jealous because in 2015, it did happen - the first-ever detection of gravitational waves. You can't see them. You can't feel them. But Albert Einstein had it right. Space and time is a kind of jiggly matrix. And when two big things out in the universe collide, they can send shockwaves through spacetime that are like the ripples you make when you toss a pebble into a pond. Being able to sense these waves is brand new for astronomy, which has spent centuries studying light. GONZALEZ: Galileo invented the telescope or used the telescope for the first time to do astronomy 400 years ago. And today, we're still building better telescopes. I think this decade has been the beginning of gravitational wave astronomy. GREENFIELDBOYCE: And she thinks it should just keep getting better and better. The United States has two facilities for detecting gravitational waves - one in Washington state and one in Louisiana. Together, they're called LIGO for the Laser Interferometer Gravitational-Wave Observatory. I drove about an hour and a half north of New Orleans to see the one in rural Louisiana. The head of the observatory, Joe Giaime, took me over to a display case to see a gold medal. JOSEPH GIAIME: People who win Nobel prizes can pay a little extra money, and check a box and get a duplicate. GREENFIELDBOYCE: Each site has one of these since the first detection of gravitational waves was such a big deal that the Nobel Committee, pretty much instantly, honored three American physicists for their work on this project. We walk out onto a bridge that goes over a big concrete pipe. From here, we see the pipe going off into the distance, and we can also see another pipe as well. Giaime says each is more than two miles long. They come together in a shape that, from above, looks like a capital L. GIAIME: I've spoken with pilots who fly over this. And they wonder why there's a pipeline that starts nowhere, travels, you know, a couple miles, turns right and then goes also nowhere. GREENFIELDBOYCE: Inside each stretch of pipe is a powerful laser beam that bounces back and forth between mirrors. Scientists use this laser to precisely measure the length of each arm of the L. When a gravitational wave passes through and distorts space, the lengths change by a tiny, tiny bit like a fraction of the width of a subatomic particle. GIAIME: We're in the control room now, and this is where all of the activities of both the site and the detector are monitored and controlled. GREENFIELDBOYCE: It's a windowless room with people sitting at dozens of computer monitors. Since the first historic detection 3 1/2 years ago, this place has registered 10 more gravitational wave events. Nine were black hole collisions, and one was a pair of neutron stars smashing together. But the science has been shut down for more than a year. That was to let researchers install new hardware and other upgrades. The workers in here, now, are testing them out. On April 1, everything officially comes back online. Giaime says the U. S. detectors plus another one in Italy will all be more sensitive. GIAIME: So, so far, we've seen 11 things. Maybe we'll see twice that many this year. GREENFIELDBOYCE: And they'll be better able to locate the source of the waves in the sky. The team will send out public alerts so that anyone can point their telescopes at the right spot. In case, like the neutron star collision, the event sends out cosmic fireworks. Thousands of astronomers and physicists around the world are now involved in studying gravitational waves because these offer the only way to explore some of the most powerful, exotic events in the universe. And that's the fun of it. Nergis Mavalvala is a physicist at MIT. NERGIS MAVALVALA: We've only seen this handful of black holes of all the possible ones that are out there. There are many, many questions we still don't know how to answer. GREENFIELDBOYCE: Plus, maybe something completely unexpected will go boom. MAVALVALA: That's how discovery happens. As you turn on a new instrument, you point it out at the sky and you see something that you had no idea existed. GREENFIELDBOYCE: She says that's happened time and time again in astronomy, and she bets it'll happen for gravitational waves as well. Nell Greenfieldboyce, NPR News. RACHEL MARTIN, HOST:  Albert Einstein predicted that some cosmic smashups would be so powerful, they'd create ripples in the very fabric of the universe. A century later, physicists proved him right when they detected the ripples created by the collision of two black holes. The massive detectors used to make that discovery have now gotten an upgrade, and they are just about to start back up. NPR's Nell Greenfieldboyce reports. NELL GREENFIELDBOYCE, BYLINE: Gabby Gonzalez is a physicist at Louisiana State University who has spent years, decades, working with a team that was trying to detect something that had never been detected before. GABRIELA GONZALEZ: I have lots of friends that, now, they tell me, I felt so worried about your career because you were working on such a difficult thing that I thought it was never going to happen. And now, I'm so jealous (laughter). GREENFIELDBOYCE: They're jealous because in 2015, it did happen - the first-ever detection of gravitational waves. You can't see them. You can't feel them. But Albert Einstein had it right. Space and time is a kind of jiggly matrix. And when two big things out in the universe collide, they can send shockwaves through spacetime that are like the ripples you make when you toss a pebble into a pond. Being able to sense these waves is brand new for astronomy, which has spent centuries studying light. GONZALEZ: Galileo invented the telescope or used the telescope for the first time to do astronomy 400 years ago. And today, we're still building better telescopes. I think this decade has been the beginning of gravitational wave astronomy. GREENFIELDBOYCE: And she thinks it should just keep getting better and better. The United States has two facilities for detecting gravitational waves - one in Washington state and one in Louisiana. Together, they're called LIGO for the Laser Interferometer Gravitational-Wave Observatory. I drove about an hour and a half north of New Orleans to see the one in rural Louisiana. The head of the observatory, Joe Giaime, took me over to a display case to see a gold medal. JOSEPH GIAIME: People who win Nobel prizes can pay a little extra money, and check a box and get a duplicate. GREENFIELDBOYCE: Each site has one of these since the first detection of gravitational waves was such a big deal that the Nobel Committee, pretty much instantly, honored three American physicists for their work on this project. We walk out onto a bridge that goes over a big concrete pipe. From here, we see the pipe going off into the distance, and we can also see another pipe as well. Giaime says each is more than two miles long. They come together in a shape that, from above, looks like a capital L. GIAIME: I've spoken with pilots who fly over this. And they wonder why there's a pipeline that starts nowhere, travels, you know, a couple miles, turns right and then goes also nowhere. GREENFIELDBOYCE: Inside each stretch of pipe is a powerful laser beam that bounces back and forth between mirrors. Scientists use this laser to precisely measure the length of each arm of the L. When a gravitational wave passes through and distorts space, the lengths change by a tiny, tiny bit like a fraction of the width of a subatomic particle. GIAIME: We're in the control room now, and this is where all of the activities of both the site and the detector are monitored and controlled. GREENFIELDBOYCE: It's a windowless room with people sitting at dozens of computer monitors. Since the first historic detection 3 1/2 years ago, this place has registered 10 more gravitational wave events. Nine were black hole collisions, and one was a pair of neutron stars smashing together. But the science has been shut down for more than a year. That was to let researchers install new hardware and other upgrades. The workers in here, now, are testing them out. On April 1, everything officially comes back online. Giaime says the U. S. detectors plus another one in Italy will all be more sensitive. GIAIME: So, so far, we've seen 11 things. Maybe we'll see twice that many this year. GREENFIELDBOYCE: And they'll be better able to locate the source of the waves in the sky. The team will send out public alerts so that anyone can point their telescopes at the right spot. In case, like the neutron star collision, the event sends out cosmic fireworks. Thousands of astronomers and physicists around the world are now involved in studying gravitational waves because these offer the only way to explore some of the most powerful, exotic events in the universe. And that's the fun of it. Nergis Mavalvala is a physicist at MIT. NERGIS MAVALVALA: We've only seen this handful of black holes of all the possible ones that are out there. There are many, many questions we still don't know how to answer. GREENFIELDBOYCE: Plus, maybe something completely unexpected will go boom. MAVALVALA: That's how discovery happens. As you turn on a new instrument, you point it out at the sky and you see something that you had no idea existed. GREENFIELDBOYCE: She says that's happened time and time again in astronomy, and she bets it'll happen for gravitational waves as well. Nell Greenfieldboyce, NPR News.", "section": "Science", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-20-705106450": {"title": "Google Fined $1.7 Billion By EU Over 'Abusive' Online Ad Strategies : NPR", "url": "https://www.npr.org/2019/03/20/705106450/eu-fines-google-1-7-billion-over-abusive-online-ad-strategies", "author": "No author found", "published_date": "2019-03-20", "content": "", "section": "Technology", "disclaimer": ""}, "2019-03-20-704818011": {"title": "Despite U.S. Pressure, Germany Refuses To Exclude Huawei's 5G Technology : NPR", "url": "https://www.npr.org/2019/03/20/704818011/despite-u-s-pressure-germany-refuses-to-exclude-huaweis-5g-technology", "author": "No author found", "published_date": "2019-03-20", "content": "", "section": "Europe", "disclaimer": ""}, "2019-03-21-705588364": {"title": "Passwords From Millions Of Facebook Users Were Stored Insecurely : NPR", "url": "https://www.npr.org/2019/03/21/705588364/facebook-stored-millions-of-user-passwords-in-plain-readable-text", "author": "No author found", "published_date": "2019-03-21", "content": "", "section": "Technology", "disclaimer": ""}, "2019-03-22-705809811": {"title": "In The Wake Of Ukraine's Civil War, Students Learn How To Identify Fake News : NPR", "url": "https://www.npr.org/2019/03/22/705809811/students-in-ukraine-learn-how-to-spot-fake-stories-propaganda-and-hate-speech", "author": "No author found", "published_date": "2019-03-22", "content": "", "section": "World", "disclaimer": ""}, "2019-03-22-705833854": {"title": "We Want To Hear From You About Facebook Live : NPR", "url": "https://www.npr.org/2019/03/22/705833854/we-want-to-hear-from-you-about-facebook-live", "author": "No author found", "published_date": "2019-03-22", "content": "", "section": "Technology", "disclaimer": ""}, "2019-03-22-705979760": {"title": "Leading Anti-Terror Technologist Says Suspend Facebook Live Following Mosque Shootings : NPR", "url": "https://www.npr.org/2019/03/22/705979760/leading-anti-terror-technologist-says-facebook-failed-in-its-response-to-mosque-", "author": "No author found", "published_date": "2019-03-22", "content": "AILSA CHANG, HOST: A leading expert in anti-terror technology is calling on Facebook to cease to suspend live video in the wake of the New Zealand massacre. He says the company's failure to pull down footage of the tragedy is absolutely inexcusable. The suspect had streamed the shooting live on Facebook, and from there, it was shared hundreds of thousands of times, even after New Zealand police alerted the company. Here's NPR's Aarti Shahani. AARTI SHAHANI, BYLINE: After Facebook removed the video, users attempted to upload it again in various forums about 1. 5 million times. Of those attempts, 300,000 slipped through the cracks. That's a 1 in 5 failure rate. HANY FARID: The repeated uploading is an absolute failure, and it is inexcusable because we have the technology to stop it. SHAHANI: Hany Farid, a leading architect of that technology. FARID: And if your technology isn't working, well, then you haven't innovated enough. You can't claim this is a hard problem. It's the same video. It's the same video. How can this be this hard of a problem? I simply don't buy that argument. SHAHANI: Farid, an incoming professor at the University of California at Berkeley, worked with Microsoft 10 years ago to create Photo DNA, a tool that tech giants rely on to fingerprint digital content. The algorithms have evolved, so a photo video or audio clip can be fingerprinted and automatically blocked even when it's been modified. Facebook says they used automated technology, but the video was recut and rerecorded into formats that made it harder to match copies. Farid says this excuse rings hollow. It's a common problem, and tech giants have had a decade to solve it. FARID: Haven't figured out that problem yet, I think, says a lot about your priorities at these companies. It's simply not your priority. SHAHANI: The U. S. Congress and European regulators have relied on Farid to fact-check the tech giants. He says political leaders should launch an inquiry and insist on honest answers in this recent Facebook failure, which he compares to another public safety debacle - Boeing. FARID: There was a global outcry. We grounded planes. We stopped until we got answers to secure that. SHAHANI: With investors, Facebook leaders talked up their ability to solve the hardest technical problems like getting livestream videos to work for millions of people on smartphones. At the same time, that's a really hard problem. CEO Mark Zuckerberg in November 2016. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: So there aren't that many companies that can do this at the scale that we're talking about, and this has been a big advantage for us. SHAHANI: When it comes to security - building the guardrails - company leaders are much quieter. Facebook declined to say how many views the massacre footage got in total from the 30,000 re-uploads. The company also declined to respond to Farid's comments, which NPR shared in an email. Aarti Shahani, NPR News, Berkeley. CHANG: And we should say Facebook is one of NPR's financial sponsors. AILSA CHANG, HOST:  A leading expert in anti-terror technology is calling on Facebook to cease to suspend live video in the wake of the New Zealand massacre. He says the company's failure to pull down footage of the tragedy is absolutely inexcusable. The suspect had streamed the shooting live on Facebook, and from there, it was shared hundreds of thousands of times, even after New Zealand police alerted the company. Here's NPR's Aarti Shahani. AARTI SHAHANI, BYLINE: After Facebook removed the video, users attempted to upload it again in various forums about 1. 5 million times. Of those attempts, 300,000 slipped through the cracks. That's a 1 in 5 failure rate. HANY FARID: The repeated uploading is an absolute failure, and it is inexcusable because we have the technology to stop it. SHAHANI: Hany Farid, a leading architect of that technology. FARID: And if your technology isn't working, well, then you haven't innovated enough. You can't claim this is a hard problem. It's the same video. It's the same video. How can this be this hard of a problem? I simply don't buy that argument. SHAHANI: Farid, an incoming professor at the University of California at Berkeley, worked with Microsoft 10 years ago to create Photo DNA, a tool that tech giants rely on to fingerprint digital content. The algorithms have evolved, so a photo video or audio clip can be fingerprinted and automatically blocked even when it's been modified. Facebook says they used automated technology, but the video was recut and rerecorded into formats that made it harder to match copies. Farid says this excuse rings hollow. It's a common problem, and tech giants have had a decade to solve it. FARID: Haven't figured out that problem yet, I think, says a lot about your priorities at these companies. It's simply not your priority. SHAHANI: The U. S. Congress and European regulators have relied on Farid to fact-check the tech giants. He says political leaders should launch an inquiry and insist on honest answers in this recent Facebook failure, which he compares to another public safety debacle - Boeing. FARID: There was a global outcry. We grounded planes. We stopped until we got answers to secure that. SHAHANI: With investors, Facebook leaders talked up their ability to solve the hardest technical problems like getting livestream videos to work for millions of people on smartphones. At the same time, that's a really hard problem. CEO Mark Zuckerberg in November 2016. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: So there aren't that many companies that can do this at the scale that we're talking about, and this has been a big advantage for us. SHAHANI: When it comes to security - building the guardrails - company leaders are much quieter. Facebook declined to say how many views the massacre footage got in total from the 30,000 re-uploads. The company also declined to respond to Farid's comments, which NPR shared in an email. Aarti Shahani, NPR News, Berkeley. CHANG: And we should say Facebook is one of NPR's financial sponsors.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-23-702422888": {"title": "Frustration Grows Along With The Number of TV Streaming Services : NPR", "url": "https://www.npr.org/2019/03/23/702422888/too-much-video-streaming-to-choose-from-its-only-going-to-get-worse", "author": "No author found", "published_date": "2019-03-23", "content": "KORVA COLEMAN, HOST:  Here's the good news. There is a lot of high-quality TV right now with strong plots and A-list actors. The bad news - there is a lot of high-quality TV right now. And trying to figure out where to catch your favorite shows from Netflix and Amazon Prime to Hulu can be frustrating. As Laura Sydell reports, it's about to get worse with two more hot streaming services on the way. (SOUNDBITE OF THE IAN RICH ORCHESTRA'S \"JAMES BOND THEME\")LAURA SYDELL, BYLINE: If you love James Bond movies, most of them are online somewhere. Brandon Smith has watched a lot of them on Hulu. Smith says recently he went back to finish watching one of the films and this happened. BRANDON SMITH: Oh, shoot. It's the first of the month. It's not available anymore. There seems to be a much more limited run of some of the content that I like. SYDELL: As of March 1, Hulu lost the rights to several Bond films. The world has certainly changed since the days when the only option for consumers was to purchase a bundle of channels from a local cable, satellite or telco company. More than 70 percent of American households still have some kind of pay TV according to eMarketer, but close to 60 percent now have at least one streaming service. People who want to see a new program have no choice but to subscribe. Hulu's original, \"The Handmaid's Tale,\" coincided with a spike in new subscribers. \"Game Of Thrones\" appears to have given HBO's streaming service a boost. But the costs can add up. Smith, his wife and their two children have accounts with Netflix and Hulu. They also pay for Amazon Prime, cable and broadband at a cost of nearly $340 a month. SMITH: So far, I'm able to absorb the cost. But I still would like to find sort of one service that offers the station she likes. So it's just sort of finding the right mix that offers everything we're looking for. SYDELL: Mr. Smith, that service may not be coming anytime soon. DAN RAYBURN: Some people have this notion and this misbelief that, hey, one day, someone's going to come along, like Apple, and they're just going to aggregate everything into one platform. SYDELL: Dan Rayburn is a principal analyst at Frost & Sullivan, who follows digital media. There will be more streaming services launching later this year, including one from Disney, with all its entertainment and hits from Marvel, \"Star Wars\" and Pixar. And AT&T, which owns Time Warner and the rights to Harry Potter. RAYBURN: These major corporations like Disney, they want to have the direct-to-consumer relationship with the consumer. They don't want to go through a third-party. They don't want to go through another platform. SYDELL: Amanda Lotz, a professor at Queensland University of Technology in Australia, thinks it's a golden age. AMANDA LOTZ: In one household, you may decide that you need these services. In the next household, they may be entirely different. SYDELL: Lotz says customers are experimenting, too. Many subscribe to a service to watch one particular show and cancel the subscription when it's over. Sometimes, they can just buy a show a la carte. Some experts don't think this hodgepodge of services is going to last. Mark Suster is a venture capitalist who's been investing in online video for over a decade. He thinks we're in one of those moments of explosive growth, like the early Internet days, when a lot of companies are fighting it out for dominance. Over the next few years, Suster believes many services will spend a lot on content to woo customers. MARK SUSTER: And after they realize that they're not winning the race, you'll see a lot of those people exit and consolidate around the winners. SYDELL: Some people might actually welcome consolidation. Alexa Conway is a 70-year-old retiree on a fixed income who lives in the Pacific Northwest. She has Netflix, Amazon Prime and Hulu. She canceled her cable subscription to save money and put up an antenna to get local TV stations. She's a fan of the Denver Broncos. On cable CBS, she got all their games. She says now she would have to subscribe to CBS's streaming service All Access to watch them all. She spoke over Skype. ALEXA CONWAY: And it really, really offended me. I just thought, you know, there is enough to do if I want to sit in front of the TV and binge that I don't need to pay yet another service. SYDELL: The kind of anger Conway is feeling may be having an unexpected impact. For the first time in many years, there's growth in online piracy of film and TV. Some experts believe it may be because fans are getting sick of paying for yet another streaming service. Laura Sydell, NPR News. KORVA COLEMAN, HOST:   Here's the good news. There is a lot of high-quality TV right now with strong plots and A-list actors. The bad news - there is a lot of high-quality TV right now. And trying to figure out where to catch your favorite shows from Netflix and Amazon Prime to Hulu can be frustrating. As Laura Sydell reports, it's about to get worse with two more hot streaming services on the way. (SOUNDBITE OF THE IAN RICH ORCHESTRA'S \"JAMES BOND THEME\") LAURA SYDELL, BYLINE: If you love James Bond movies, most of them are online somewhere. Brandon Smith has watched a lot of them on Hulu. Smith says recently he went back to finish watching one of the films and this happened. BRANDON SMITH: Oh, shoot. It's the first of the month. It's not available anymore. There seems to be a much more limited run of some of the content that I like. SYDELL: As of March 1, Hulu lost the rights to several Bond films. The world has certainly changed since the days when the only option for consumers was to purchase a bundle of channels from a local cable, satellite or telco company. More than 70 percent of American households still have some kind of pay TV according to eMarketer, but close to 60 percent now have at least one streaming service. People who want to see a new program have no choice but to subscribe. Hulu's original, \"The Handmaid's Tale,\" coincided with a spike in new subscribers. \"Game Of Thrones\" appears to have given HBO's streaming service a boost. But the costs can add up. Smith, his wife and their two children have accounts with Netflix and Hulu. They also pay for Amazon Prime, cable and broadband at a cost of nearly $340 a month. SMITH: So far, I'm able to absorb the cost. But I still would like to find sort of one service that offers the station she likes. So it's just sort of finding the right mix that offers everything we're looking for. SYDELL: Mr. Smith, that service may not be coming anytime soon. DAN RAYBURN: Some people have this notion and this misbelief that, hey, one day, someone's going to come along, like Apple, and they're just going to aggregate everything into one platform. SYDELL: Dan Rayburn is a principal analyst at Frost & Sullivan, who follows digital media. There will be more streaming services launching later this year, including one from Disney, with all its entertainment and hits from Marvel, \"Star Wars\" and Pixar. And AT&T, which owns Time Warner and the rights to Harry Potter. RAYBURN: These major corporations like Disney, they want to have the direct-to-consumer relationship with the consumer. They don't want to go through a third-party. They don't want to go through another platform. SYDELL: Amanda Lotz, a professor at Queensland University of Technology in Australia, thinks it's a golden age. AMANDA LOTZ: In one household, you may decide that you need these services. In the next household, they may be entirely different. SYDELL: Lotz says customers are experimenting, too. Many subscribe to a service to watch one particular show and cancel the subscription when it's over. Sometimes, they can just buy a show a la carte. Some experts don't think this hodgepodge of services is going to last. Mark Suster is a venture capitalist who's been investing in online video for over a decade. He thinks we're in one of those moments of explosive growth, like the early Internet days, when a lot of companies are fighting it out for dominance. Over the next few years, Suster believes many services will spend a lot on content to woo customers. MARK SUSTER: And after they realize that they're not winning the race, you'll see a lot of those people exit and consolidate around the winners. SYDELL: Some people might actually welcome consolidation. Alexa Conway is a 70-year-old retiree on a fixed income who lives in the Pacific Northwest. She has Netflix, Amazon Prime and Hulu. She canceled her cable subscription to save money and put up an antenna to get local TV stations. She's a fan of the Denver Broncos. On cable CBS, she got all their games. She says now she would have to subscribe to CBS's streaming service All Access to watch them all. She spoke over Skype. ALEXA CONWAY: And it really, really offended me. I just thought, you know, there is enough to do if I want to sit in front of the TV and binge that I don't need to pay yet another service. SYDELL: The kind of anger Conway is feeling may be having an unexpected impact. For the first time in many years, there's growth in online piracy of film and TV. Some experts believe it may be because fans are getting sick of paying for yet another streaming service. Laura Sydell, NPR News.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-24-705966447": {"title": "Boeing Crashes: Software Investigated As Questions Raised Over Potential Flaws : NPR", "url": "https://www.npr.org/2019/03/24/705966447/software-is-everywhere-but-its-not-always-an-upgrade", "author": "No author found", "published_date": "2019-03-24", "content": "", "section": "Business", "disclaimer": ""}, "2019-03-25-706715377": {"title": "Man Pleads Guilty To Swindling Google, Facebook Out Of More Than $100 Million : NPR", "url": "https://www.npr.org/2019/03/25/706715377/man-pleads-guilty-to-phishing-scheme-that-fleeced-facebook-google-of-100-million", "author": "No author found", "published_date": "2019-03-25", "content": "", "section": "Business", "disclaimer": ""}, "2019-03-25-706513097": {"title": "Apple Announces Streaming Video, Payment Card And News Subscription Services : NPR", "url": "https://www.npr.org/2019/03/25/706513097/apple-debuts-streaming-video-payment-card-and-news-subscription-services", "author": "No author found", "published_date": "2019-03-25", "content": "", "section": "Business", "disclaimer": ""}, "2019-03-26-705822275": {"title": "The U.S. Pledges A Harder Line In Cyberspace \u2014 And Drops Some Hints : NPR", "url": "https://www.npr.org/2019/03/26/705822275/the-u-s-pledges-a-harder-line-in-cyberspace-and-drops-some-hints", "author": "No author found", "published_date": "2019-03-26", "content": "RACHEL MARTIN, HOST: Efforts to stop Russia from interfering in U. S. elections come from two of the most secretive parts of the U. S. government the National Security Agency and Cyber Command. They worked side by side at the same sprawling campus in Fort Meade Maryland where the NSA monitors foreign communications. While cyber command takes action in the digital realm the army general who heads them both is a proponent of more aggressive measures. Here's NPR national security correspondent Greg Myre. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: It's a pleasure to introduce to you the commander of U. S. Cyber Command and director of the National Security Agency, General Paul Nakasone. (SOUNDBITE OF MUSIC)GREG MYRE, BYLINE: Paul Nakasone usually doesn't say much in public. But recently, he's been on what amounts to a PR blitz. Here he is, in short sleeves and no tie, addressing a high-tech confab in San Francisco. (SOUNDBITE OF ARCHIVED RECORDING)PAUL NAKASONE: I have all the authorities that I need right now to conduct full-spectrum operations. That's defensive operations all the way to offensive operations. And when I don't have those authorities, I will certainly ask for them. MYRE: Nakasone is driving home the point that the U. S. needs to directly confront rivals in cyberspace. Here's what he said on Capitol Hill about countering Russian attempts to meddle in last fall's midterm elections. (SOUNDBITE OF ARCHIVED RECORDING)NAKASONE: For the first time, we sent our cyberwarriors abroad. We sent defensive teams forward in November to three different European countries. That's acting outside of our borders that imposed costs against our adversaries. MYRE: Over the past decade, the U. S. has been wrestling with the question of how to deal with cyberattacks. What's the proper response when China steals high-tech secrets from a U. S. company or when North Korea hacks into Sony because that country doesn't like a satirical movie about its leader Kim Jong Un? Nakasone made clear he wanted to take a harder line at his confirmation hearing a year ago. Here he responds to Alaska senator Dan Sullivan. (SOUNDBITE OF ARCHIVED RECORDING)DAN SULLIVAN: What do you think our adversaries think right now? If you do a cyberattack on America, what's going to happen to them? NAKASONE: So basically, I would say right now they do not think that much will happen to them. SULLIVAN: They don't fear us? NAKASONE: They don't fear us. MYRE: President Trump has given Nakasone more authority, but this approach raises two big questions. First, will other countries stop attacking the U. S. ? Probably not. Second, will this ignite a cycle of retaliation and escalation? No one really knows. But we are getting a peek behind the curtain, says P. W. Singer, a cyber expert at the New America think tank. P W SINGER: So you're seeing a change from keeping everything classified, not talking about anything to trying to share a little bit more information. And the reason is a belief that if you create awareness, it makes the attacker's job harder. MYRE: The U. S. now routinely names and shames hackers. THOMAS RID: A lot of countries can hack. MYRE: That's Thomas Rid of Johns Hopkins University who says not a lot of countries can figure out who did the hacking. The U. S. can. Robert Mueller's team indicted 25 Russians for election interference by name and with details that could only be obtained by hacking their computers. RID: So finding out who hacked you, finding the evidence and then assessing the evidence in a professional way - the attribution capabilities, these are hard to develop. MYRE: The NSA took another unusual step recently, making one of its own software programs available to the public for free. It's called Ghidra, and it reverse-engineers malware that's been detected in a computer system. Now anyone can download Ghidra to analyze malware and figure out how best to combat it. Of course, the NSA has its own history of planting malware abroad, notes author James Bamford, who's written about the agency for decades. JAMES BAMFORD: The Russians plant malware and look for openings in various infrastructure in the United States. It's exactly the same thing we do in other countries. It's not necessarily an act of aggression; it's just normal espionage. MYRE: Whatever you call it, Nakasone says it's here to stay. (SOUNDBITE OF ARCHIVED RECORDING)NAKASONE: I think this is a new normal. It can't be episodic. You have to be involved every day. You have to be aware of what your adversary is doing. MYRE: Planning, he says, is already underway to protect the 2020 election. Greg Myre, NPR News, Washington. (SOUNDBITE OF EVOCATIV'S \"CASTAWAY\") RACHEL MARTIN, HOST:  Efforts to stop Russia from interfering in U. S. elections come from two of the most secretive parts of the U. S. government the National Security Agency and Cyber Command. They worked side by side at the same sprawling campus in Fort Meade Maryland where the NSA monitors foreign communications. While cyber command takes action in the digital realm the army general who heads them both is a proponent of more aggressive measures. Here's NPR national security correspondent Greg Myre. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: It's a pleasure to introduce to you the commander of U. S. Cyber Command and director of the National Security Agency, General Paul Nakasone. (SOUNDBITE OF MUSIC) GREG MYRE, BYLINE: Paul Nakasone usually doesn't say much in public. But recently, he's been on what amounts to a PR blitz. Here he is, in short sleeves and no tie, addressing a high-tech confab in San Francisco. (SOUNDBITE OF ARCHIVED RECORDING) PAUL NAKASONE: I have all the authorities that I need right now to conduct full-spectrum operations. That's defensive operations all the way to offensive operations. And when I don't have those authorities, I will certainly ask for them. MYRE: Nakasone is driving home the point that the U. S. needs to directly confront rivals in cyberspace. Here's what he said on Capitol Hill about countering Russian attempts to meddle in last fall's midterm elections. (SOUNDBITE OF ARCHIVED RECORDING) NAKASONE: For the first time, we sent our cyberwarriors abroad. We sent defensive teams forward in November to three different European countries. That's acting outside of our borders that imposed costs against our adversaries. MYRE: Over the past decade, the U. S. has been wrestling with the question of how to deal with cyberattacks. What's the proper response when China steals high-tech secrets from a U. S. company or when North Korea hacks into Sony because that country doesn't like a satirical movie about its leader Kim Jong Un? Nakasone made clear he wanted to take a harder line at his confirmation hearing a year ago. Here he responds to Alaska senator Dan Sullivan. (SOUNDBITE OF ARCHIVED RECORDING) DAN SULLIVAN: What do you think our adversaries think right now? If you do a cyberattack on America, what's going to happen to them? NAKASONE: So basically, I would say right now they do not think that much will happen to them. SULLIVAN: They don't fear us? NAKASONE: They don't fear us. MYRE: President Trump has given Nakasone more authority, but this approach raises two big questions. First, will other countries stop attacking the U. S. ? Probably not. Second, will this ignite a cycle of retaliation and escalation? No one really knows. But we are getting a peek behind the curtain, says P. W. Singer, a cyber expert at the New America think tank. P W SINGER: So you're seeing a change from keeping everything classified, not talking about anything to trying to share a little bit more information. And the reason is a belief that if you create awareness, it makes the attacker's job harder. MYRE: The U. S. now routinely names and shames hackers. THOMAS RID: A lot of countries can hack. MYRE: That's Thomas Rid of Johns Hopkins University who says not a lot of countries can figure out who did the hacking. The U. S. can. Robert Mueller's team indicted 25 Russians for election interference by name and with details that could only be obtained by hacking their computers. RID: So finding out who hacked you, finding the evidence and then assessing the evidence in a professional way - the attribution capabilities, these are hard to develop. MYRE: The NSA took another unusual step recently, making one of its own software programs available to the public for free. It's called Ghidra, and it reverse-engineers malware that's been detected in a computer system. Now anyone can download Ghidra to analyze malware and figure out how best to combat it. Of course, the NSA has its own history of planting malware abroad, notes author James Bamford, who's written about the agency for decades. JAMES BAMFORD: The Russians plant malware and look for openings in various infrastructure in the United States. It's exactly the same thing we do in other countries. It's not necessarily an act of aggression; it's just normal espionage. MYRE: Whatever you call it, Nakasone says it's here to stay. (SOUNDBITE OF ARCHIVED RECORDING) NAKASONE: I think this is a new normal. It can't be episodic. You have to be involved every day. You have to be aware of what your adversary is doing. MYRE: Planning, he says, is already underway to protect the 2020 election. Greg Myre, NPR News, Washington. (SOUNDBITE OF EVOCATIV'S \"CASTAWAY\")", "section": "National Security", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-26-705887181": {"title": "Trump Administration Wants To Reverse An Obama-era Lightbulb Efficiency Rule. : NPR", "url": "https://www.npr.org/2019/03/26/705887181/trump-administration-flips-switch-on-energy-efficient-light-bulbs", "author": "No author found", "published_date": "2019-03-26", "content": "RACHEL MARTIN, HOST: The Energy Department wants to undo an Obama-era rule designed to make lightbulbs more efficient. We're not talking about those pear-shaped bulbs. They've already changed over. This new proposal involves just about every other kind of lightbulb. Here's NPR's Jeff Brady to explain. JEFF BRADY, BYLINE: In the past decade, the push for lightbulbs that use less energy has made a lot of progress, and that can make the lightbulb aisle at a big-box store a little confusing. JOHN PENNICK: I'm looking for a bulb to go inside of my refrigerator. BRADY: John Pennick is holding a burned-out, 40-watt incandescent bulb in one hand and a package of LED bulbs in the other. He likes that the LEDs use one-fifth the energy. But on the package, it says they're dimmable. PENNICK: I'm not going to be dimming inside the refrigerator (laughter) so. . . BRADY: He opts for the old incandescent. Though, it turns out, the dimmable LED would have worked just fine. This learning curve is part of a big change under way in the lighting business. Noah Horowitz with the Natural Resources Defense Council says the country has been using incandescent bulbs for more than a century. NOAH HOROWITZ: Today, we have LEDs, and they do everything the old incandescent could do, except waste energy. BRADY: You see the pear-shaped LEDs everywhere now. At issue here is a wide range of other bulbs - decorative globes used in bathrooms, reflectors in recessed lighting, candle-shaped lights and three-way bulbs. At the very end of the Obama administration, the Department of Energy decided these also would have to become more efficient. The lighting industry sued. The Trump administration sided with the industry and now wants to reverse the regulation. Horowitz says that will translate to higher energy bills and more pollution. HOROWITZ: Now we're going to have to generate about 25 large, coal-burning power plants' worth of extra electricity if this rollback goes through. This is really unnecessary and a really bad idea. And on top of that, it's illegal. BRADY: That last point is a question a judge likely will decide. The National Electrical Manufacturers Association argues the Obama administration exceeded its authority by including all these specialty bulbs. General counsel Clark Silcox says it's wrong to call the Trump administration's proposed reversal a rollback. CLARK SILCOX: The Department of Energy cannot illegally roll back from a place that it could not legally stand upon in the first place. BRADY: That is important to the industry because rolling back energy efficiency requirements is not allowed under federal law. And Silcox rejects claims the industry just wants to continue selling existing styles of bulbs. SILCOX: I don't think anybody's focused on we want to sell energy-hogging lightbulbs at all. What they are focused on is, what does their consumer want and making sure that there is a high level of consumer satisfaction. BRADY: The industry learned a hard lesson after the first generation of those curly, compact fluorescent lightbulbs were released. JENNIFER DOLIN: Consumers basically did not like them. They didn't buy them. They complained about them. We heard that loud and clear. BRADY: Jennifer Dolin is with the company LEDVANCE, which manufacturers Sylvania lightbulbs. She says the industry wants to make sure it gets the LED versions of all these other kinds of bulbs perfected before releasing them onto the market. She says more efficient bulbs are coming, just not as fast as most advocates want. DOLIN: The future of lighting is LED. There's no doubt about that. And we see that in the marketplace. We see that consumers are shifting to LEDs at a much more rapid rate than we ever anticipated. BRADY: But given climate change, environmental groups argue this regulation is needed now. When the Trump administration finalizes its reversal, those groups plan to take the issue to court. Jeff Brady, NPR News. RACHEL MARTIN, HOST:  The Energy Department wants to undo an Obama-era rule designed to make lightbulbs more efficient. We're not talking about those pear-shaped bulbs. They've already changed over. This new proposal involves just about every other kind of lightbulb. Here's NPR's Jeff Brady to explain. JEFF BRADY, BYLINE: In the past decade, the push for lightbulbs that use less energy has made a lot of progress, and that can make the lightbulb aisle at a big-box store a little confusing. JOHN PENNICK: I'm looking for a bulb to go inside of my refrigerator. BRADY: John Pennick is holding a burned-out, 40-watt incandescent bulb in one hand and a package of LED bulbs in the other. He likes that the LEDs use one-fifth the energy. But on the package, it says they're dimmable. PENNICK: I'm not going to be dimming inside the refrigerator (laughter) so. . . BRADY: He opts for the old incandescent. Though, it turns out, the dimmable LED would have worked just fine. This learning curve is part of a big change under way in the lighting business. Noah Horowitz with the Natural Resources Defense Council says the country has been using incandescent bulbs for more than a century. NOAH HOROWITZ: Today, we have LEDs, and they do everything the old incandescent could do, except waste energy. BRADY: You see the pear-shaped LEDs everywhere now. At issue here is a wide range of other bulbs - decorative globes used in bathrooms, reflectors in recessed lighting, candle-shaped lights and three-way bulbs. At the very end of the Obama administration, the Department of Energy decided these also would have to become more efficient. The lighting industry sued. The Trump administration sided with the industry and now wants to reverse the regulation. Horowitz says that will translate to higher energy bills and more pollution. HOROWITZ: Now we're going to have to generate about 25 large, coal-burning power plants' worth of extra electricity if this rollback goes through. This is really unnecessary and a really bad idea. And on top of that, it's illegal. BRADY: That last point is a question a judge likely will decide. The National Electrical Manufacturers Association argues the Obama administration exceeded its authority by including all these specialty bulbs. General counsel Clark Silcox says it's wrong to call the Trump administration's proposed reversal a rollback. CLARK SILCOX: The Department of Energy cannot illegally roll back from a place that it could not legally stand upon in the first place. BRADY: That is important to the industry because rolling back energy efficiency requirements is not allowed under federal law. And Silcox rejects claims the industry just wants to continue selling existing styles of bulbs. SILCOX: I don't think anybody's focused on we want to sell energy-hogging lightbulbs at all. What they are focused on is, what does their consumer want and making sure that there is a high level of consumer satisfaction. BRADY: The industry learned a hard lesson after the first generation of those curly, compact fluorescent lightbulbs were released. JENNIFER DOLIN: Consumers basically did not like them. They didn't buy them. They complained about them. We heard that loud and clear. BRADY: Jennifer Dolin is with the company LEDVANCE, which manufacturers Sylvania lightbulbs. She says the industry wants to make sure it gets the LED versions of all these other kinds of bulbs perfected before releasing them onto the market. She says more efficient bulbs are coming, just not as fast as most advocates want. DOLIN: The future of lighting is LED. There's no doubt about that. And we see that in the marketplace. We see that consumers are shifting to LEDs at a much more rapid rate than we ever anticipated. BRADY: But given climate change, environmental groups argue this regulation is needed now. When the Trump administration finalizes its reversal, those groups plan to take the issue to court. Jeff Brady, NPR News.", "section": "Environment And Energy Collaborative", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-27-707258353": {"title": "Facebook To Start Removing Content Promoting White Nationalism And Separatism : NPR", "url": "https://www.npr.org/2019/03/27/707258353/facebook-bans-white-nationalism-and-separatism-content-from-its-platforms", "author": "No author found", "published_date": "2019-03-27", "content": "", "section": "Technology", "disclaimer": ""}, "2019-03-27-707358090": {"title": "Thailand Moves Forward With Chinese Tech Company Huawei To Build 5G Network : NPR", "url": "https://www.npr.org/2019/03/27/707358090/thailand-moves-forward-with-chinese-tech-company-huawei-to-build-5g-network", "author": "No author found", "published_date": "2019-03-27", "content": "AUDIE CORNISH, HOST: Countries around the world are preparing for 5G, the next generation of wireless technology. We take a look in this month's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CORNISH: The U. S. is urging allies not to let the Chinese company Huawei build their 5G networks. U. S. officials say Huawei is a security threat. But some countries don't have much of a choice. NPR's Rob Schmitz reports from Thailand. ROB SCHMITZ, BYLINE: Above a street corner on a college campus in rural Thailand, a single antenna rising a few feet above the third-story roof looks unremarkable. Campus official Udomporn Tundkasiri has to point it out so that I don't miss it. UDOMPORN TUNDKASIRI: (Foreign language spoken). SCHMITZ: That, Udomporn announces, is the first 5G antenna in Thailand. I was expecting more. 5G, after all, is billed as the biggest innovation since perhaps the Internet itself. This next generation of mobile broadband will carry data quicker than ever, connecting phones, cars, homes, ships, electrical grids, armies. Everything that can be connected will be connected with lightning speed along its signal. In other words, 5G is a new type of power, a power that Udomporn's school, Kasetsart University in the state of Si Racha, was chosen by Thailand's government to be the first to test. But when the country's first 5G test bed was announced, Udomporn says one company beat the competition to get in on it. TUNDKASIRI: (Through interpreter) Huawei was faster than the others. They invested millions of dollars, set up their equipment and will test it in the coming weeks. (SOUNDBITE OF CONSTRUCTION EQUIPMENT)SCHMITZ: Amidst a 5G-induced construction boom on campus, Huawei has already established its own office. The university also provided offices to telecom companies Ericsson and Nokia, but they sit empty. Huawei says it's been busy securing at least 30 commercial 5G contracts around the world. Djitt Laowattana chairs the strategy committee for TOT, Thailand's state-owned telecommunications company, which owns most of the country's telecoms towers. He says Huawei's low prices and high-quality equipment are attractive to a developing country like Thailand. DJITT LAOWATTANA: Everybody in Thailand know they come to the market with maybe 50 percent of the price. SCHMITZ: Djitt, also helps run the economic zone that's testing 5G for Thailand, says he's aware of suspicions about how Huawei could use its equipment to enable Chinese espionage. But he says he's not worried about spying as much as he is about Huawei's bargain basement prices beating out the competition and threatening to become a monopoly over the 5G market in Thailand. LAOWATTANA: The fact that they have a much lower price, they will kill all other competitors. Some people in Thailand still concerned that - what happens after that if, in the market, left just a Chinese company, Chinese equipment? SCHMITZ: If, as Djitt says, Thailand is left with only one Chinese company dominating 5G, the results will be pretty clear, says Benjamin Zawacki, author of \"Thailand: Shifting Ground Between The U. S. And A Rising China. \" As the title of his book suggests, Zawacki sees Thailand shifting its alliances from the U. S. to China as Chinese investment in the region reaches historic levels. Huawei, even though it's a private company, has close ties to China's government. And if Huawei continues to win 5G contracts in the country, Zawacki says China will likely use that to solidify its influence over the country. BENJAMIN ZAWACKI: The extent to which this 5G technology is going to control not only telecommunications but so many other things that are absolutely fundamental to any society's ability to function and govern itself means that, well, we better stay onside with China because if we don't, their ability to manipulate our economy, our infrastructure, our energy sources, our databases, et cetera, becomes that much greater. SCHMITZ: Zawacki says the U. S. should be concerned because of Thailand's strategic location. The Trump administration changed the way it referred to this region from Asia-Pacific to the Indo-Pacific. ZAWACKI: Thailand is at the center of that. Geographically, it's right in the middle. And so while it tries to maintain positive relationships with both countries, that sort of neutrality is not something it's going to be able to gift itself forever. SCHMITZ: Especially, he says, if it's being forced to choose in the event of a conflict between the U. S. and China. With a Chinese company controlling all communications and interconnections between machines, the fear is that choice will have already been made. Rob Schmitz, NPR News, Si Racha, Thailand. AUDIE CORNISH, HOST:  Countries around the world are preparing for 5G, the next generation of wireless technology. We take a look in this month's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CORNISH: The U. S. is urging allies not to let the Chinese company Huawei build their 5G networks. U. S. officials say Huawei is a security threat. But some countries don't have much of a choice. NPR's Rob Schmitz reports from Thailand. ROB SCHMITZ, BYLINE: Above a street corner on a college campus in rural Thailand, a single antenna rising a few feet above the third-story roof looks unremarkable. Campus official Udomporn Tundkasiri has to point it out so that I don't miss it. UDOMPORN TUNDKASIRI: (Foreign language spoken). SCHMITZ: That, Udomporn announces, is the first 5G antenna in Thailand. I was expecting more. 5G, after all, is billed as the biggest innovation since perhaps the Internet itself. This next generation of mobile broadband will carry data quicker than ever, connecting phones, cars, homes, ships, electrical grids, armies. Everything that can be connected will be connected with lightning speed along its signal. In other words, 5G is a new type of power, a power that Udomporn's school, Kasetsart University in the state of Si Racha, was chosen by Thailand's government to be the first to test. But when the country's first 5G test bed was announced, Udomporn says one company beat the competition to get in on it. TUNDKASIRI: (Through interpreter) Huawei was faster than the others. They invested millions of dollars, set up their equipment and will test it in the coming weeks. (SOUNDBITE OF CONSTRUCTION EQUIPMENT) SCHMITZ: Amidst a 5G-induced construction boom on campus, Huawei has already established its own office. The university also provided offices to telecom companies Ericsson and Nokia, but they sit empty. Huawei says it's been busy securing at least 30 commercial 5G contracts around the world. Djitt Laowattana chairs the strategy committee for TOT, Thailand's state-owned telecommunications company, which owns most of the country's telecoms towers. He says Huawei's low prices and high-quality equipment are attractive to a developing country like Thailand. DJITT LAOWATTANA: Everybody in Thailand know they come to the market with maybe 50 percent of the price. SCHMITZ: Djitt, also helps run the economic zone that's testing 5G for Thailand, says he's aware of suspicions about how Huawei could use its equipment to enable Chinese espionage. But he says he's not worried about spying as much as he is about Huawei's bargain basement prices beating out the competition and threatening to become a monopoly over the 5G market in Thailand. LAOWATTANA: The fact that they have a much lower price, they will kill all other competitors. Some people in Thailand still concerned that - what happens after that if, in the market, left just a Chinese company, Chinese equipment? SCHMITZ: If, as Djitt says, Thailand is left with only one Chinese company dominating 5G, the results will be pretty clear, says Benjamin Zawacki, author of \"Thailand: Shifting Ground Between The U. S. And A Rising China. \" As the title of his book suggests, Zawacki sees Thailand shifting its alliances from the U. S. to China as Chinese investment in the region reaches historic levels. Huawei, even though it's a private company, has close ties to China's government. And if Huawei continues to win 5G contracts in the country, Zawacki says China will likely use that to solidify its influence over the country. BENJAMIN ZAWACKI: The extent to which this 5G technology is going to control not only telecommunications but so many other things that are absolutely fundamental to any society's ability to function and govern itself means that, well, we better stay onside with China because if we don't, their ability to manipulate our economy, our infrastructure, our energy sources, our databases, et cetera, becomes that much greater. SCHMITZ: Zawacki says the U. S. should be concerned because of Thailand's strategic location. The Trump administration changed the way it referred to this region from Asia-Pacific to the Indo-Pacific. ZAWACKI: Thailand is at the center of that. Geographically, it's right in the middle. And so while it tries to maintain positive relationships with both countries, that sort of neutrality is not something it's going to be able to gift itself forever. SCHMITZ: Especially, he says, if it's being forced to choose in the event of a conflict between the U. S. and China. With a Chinese company controlling all communications and interconnections between machines, the fear is that choice will have already been made. Rob Schmitz, NPR News, Si Racha, Thailand.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-27-707358083": {"title": "EU Votes To Rewrite Its Copyright Laws, Delivering A Blow To Tech Giants : NPR", "url": "https://www.npr.org/2019/03/27/707358083/e-u-votes-to-rewrite-its-copyright-laws-delivering-a-blow-to-tech-giants", "author": "No author found", "published_date": "2019-03-27", "content": "AILSA CHANG, HOST: While U. S. lawmakers have held hearings about big tech and talked about regulating it, it's the political leaders over in Europe who are really lighting the fire under Google and Facebook. The EU delivered a huge blow to Silicon Valley this week with a new copyright directive. Here to discuss that directive is NPR's Aarti Shahani. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. CHANG: So can you just explain? What did the European Parliament do? SHAHANI: Sure. Simply put, a platform like YouTube is responsible when a video that's uploaded by a user violates copyright. So let me explain why this is game changing, OK? Copyright, meaning legal ownership over, say, a song or movie - right now under U. S. law, Internet companies aren't responsible for making sure the video posted on their platform is being shared legally. CHANG: Right. SHAHANI: They just have to take down pirated content - right? - once a user tells them about it. And that might be part of why, you know, every once in a while you can find that bootleg copy of \"Black Panther\" or \"A Star Is Born\" - (laughter) just saying. The European directive puts a whole new level of pressure on the platform to kill the bootlegs. The directive says, hey, tech company, you're responsible the moment that video is uploaded, not just when someone blows the whistle. So you better be proactive and find violations. CHANG: So it sounds like a really dramatic shift in who's responsible. What's the reaction from Silicon Valley? SHAHANI: You know, there's actually a healthy dose of humility here. I was in conversation with a senior employee at Google who said the company really messed up. When the EU began introducing this legislation a couple years back, Google could have said, hey, let's work on this together, OK? We're coming up with solutions for the exact same problems you're talking about. But instead the executives went in with the don't-tell-us-what-to-do approach, and they overplayed their hand. And this has happened before in the EU with the massive privacy law and billion-dollar fines. It's clear Europe is giving big tech a smackdown. And Google, whose motto used to be, don't be evil, might consider a new motto - play nice. CHANG: So what do these new rules mean for, say, artists, for creators? How helpful is all of this to them? SHAHANI: Well, there could be a fascinating rift here. Last summer, Sir Paul McCartney from the Beatles - OK? - he wrote an open letter to the European Parliament. He said, music and culture matter. They don't just happen. The companies exploit artists' work. And he wanted the law overhauled. But he's a big-time artist with lawyers who can produce copyright documents - right? - and negotiate agreements with Google. CHANG: Yeah. SHAHANI: Say you're a little artist who's getting your start. You don't have a legal team. Google and the other platforms could decide it's too much trouble to deal with indies. How do we really know you own the song you're sharing? And they could decide to just not bother publishing you. CHANG: OK, so if you're not Sir Paul McCartney but, say, you want to post yourself singing \"Yesterday\" on karaoke while you were at karaoke - I would never pose myself singing, but if you did, how would these new EU rules play into that? SHAHANI: So the EU did not kill karaoke, OK? There is an allowance for sampling content, for doing parody and memes and also for education purposes like online classes. I think really the Europeans are setting the tone. We've already seen American lawmakers take cues from Europe when it comes to protecting user privacy. We might see the same for creative content on the Internet, too. CHANG: That's NPR's Aarti Shahani. Thanks, Aarti. SHAHANI: Thank you AILSA CHANG, HOST:  While U. S. lawmakers have held hearings about big tech and talked about regulating it, it's the political leaders over in Europe who are really lighting the fire under Google and Facebook. The EU delivered a huge blow to Silicon Valley this week with a new copyright directive. Here to discuss that directive is NPR's Aarti Shahani. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. CHANG: So can you just explain? What did the European Parliament do? SHAHANI: Sure. Simply put, a platform like YouTube is responsible when a video that's uploaded by a user violates copyright. So let me explain why this is game changing, OK? Copyright, meaning legal ownership over, say, a song or movie - right now under U. S. law, Internet companies aren't responsible for making sure the video posted on their platform is being shared legally. CHANG: Right. SHAHANI: They just have to take down pirated content - right? - once a user tells them about it. And that might be part of why, you know, every once in a while you can find that bootleg copy of \"Black Panther\" or \"A Star Is Born\" - (laughter) just saying. The European directive puts a whole new level of pressure on the platform to kill the bootlegs. The directive says, hey, tech company, you're responsible the moment that video is uploaded, not just when someone blows the whistle. So you better be proactive and find violations. CHANG: So it sounds like a really dramatic shift in who's responsible. What's the reaction from Silicon Valley? SHAHANI: You know, there's actually a healthy dose of humility here. I was in conversation with a senior employee at Google who said the company really messed up. When the EU began introducing this legislation a couple years back, Google could have said, hey, let's work on this together, OK? We're coming up with solutions for the exact same problems you're talking about. But instead the executives went in with the don't-tell-us-what-to-do approach, and they overplayed their hand. And this has happened before in the EU with the massive privacy law and billion-dollar fines. It's clear Europe is giving big tech a smackdown. And Google, whose motto used to be, don't be evil, might consider a new motto - play nice. CHANG: So what do these new rules mean for, say, artists, for creators? How helpful is all of this to them? SHAHANI: Well, there could be a fascinating rift here. Last summer, Sir Paul McCartney from the Beatles - OK? - he wrote an open letter to the European Parliament. He said, music and culture matter. They don't just happen. The companies exploit artists' work. And he wanted the law overhauled. But he's a big-time artist with lawyers who can produce copyright documents - right? - and negotiate agreements with Google. CHANG: Yeah. SHAHANI: Say you're a little artist who's getting your start. You don't have a legal team. Google and the other platforms could decide it's too much trouble to deal with indies. How do we really know you own the song you're sharing? And they could decide to just not bother publishing you. CHANG: OK, so if you're not Sir Paul McCartney but, say, you want to post yourself singing \"Yesterday\" on karaoke while you were at karaoke - I would never pose myself singing, but if you did, how would these new EU rules play into that? SHAHANI: So the EU did not kill karaoke, OK? There is an allowance for sampling content, for doing parody and memes and also for education purposes like online classes. I think really the Europeans are setting the tone. We've already seen American lawmakers take cues from Europe when it comes to protecting user privacy. We might see the same for creative content on the Internet, too. CHANG: That's NPR's Aarti Shahani. Thanks, Aarti. SHAHANI: Thank you", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-28-707742550": {"title": "Maryland Man Accused Of Stealing NSA Documents Pleads Guilty : NPR", "url": "https://www.npr.org/2019/03/28/707742550/former-nsa-contractor-pleads-guilty-to-stealing-classified-information", "author": "No author found", "published_date": "2019-03-28", "content": "", "section": "National Security", "disclaimer": ""}, "2019-03-28-707614254": {"title": "Housing Department Slaps Facebook With Discrimination Charge : NPR", "url": "https://www.npr.org/2019/03/28/707614254/hud-slaps-facebook-with-housing-discrimination-charge", "author": "No author found", "published_date": "2019-03-28", "content": "STEVE INSKEEP, HOST: Facebook says that it wants to make it harder for advertisers to use Facebook as a tool for discrimination. But whatever it's trying was apparently too little for the Department of Housing and Urban Development because HUD says it is charging Facebook with violating the Fair Housing Act over the way that it allowed advertisers to target and exclude some users. We should note before going on that Facebook is one of NPR's financial supporters. We are nevertheless reporting independently on it, and NPR's Brakkton Booker is here. Brakkton, good morning. BRAKKTON BOOKER, BYLINE: Hi, Steve. INSKEEP: According to HUD, how does Facebook discriminate? BOOKER: Well, as you said in the intro there, HUD is saying that Facebook violated the Fair Housing Act. And that, of course, is a federal protection that was established in 1968. It protects homebuyers and renters from being discriminated against on the basis of race and religion and other protected classes. Now, HUD is saying that Facebook is - allows advertisers to shield who can see which ads appear on their platform on the basis of race. INSKEEP: Do you mean that if an advertiser only wanted white people to see an apartment ad - just to give a hypothetical - that Facebook would allow that to happen? BOOKER: Yes, there are certain things that advertisers can click on the platform that allow certain people to see. So if they are - if a Facebook user kind of identifies as a African-American dog-lover and Facebook only wants to appear to, say, an Anglo cat-lover, they can select different things on the platform that allows - that shields certain people from seeing certain ads on their platform. INSKEEP: On the most basic level, I'm genuinely curious how Facebook is convinced it knows the race of people because, of course, the platform asks for your name, asks for your phone number, asks for your birthday, a lot of other things. I don't know if they directly ask for race, but they must get it from somewhere. BOOKER: Well, HUD is saying in their charging document that Facebook mined extensive data about users and uses that data to determine which users can see housing-related ads. So in theory, because of things that you like and things that you share, there are some kind of algorithms that allow advertisers and Facebook to know kind of who these people are. INSKEEP: One of the reasons that this feels meaningful to me, Brakkton, is that I've done a lot of reporting on discriminatory housing or some reporting on discriminatory housing in the past and spoke with real estate brokers around Chicago who described the process of racial steering. Like, who do you show a house to? Who do you hint to about whether they should or shouldn't live in a particular neighborhood? It can be very subtle, but very powerful in segregating neighborhoods. You're telling me that is exactly what Facebook did, according to HUD. BOOKER: Well, it's really the advertiser. So it's the advertiser's. . . INSKEEP: Facebook allowed. . . BOOKER: Allow - allows advertisers - when they want to create an ad, when they're trying to look for the perfect tenant or the perfect homebuyer, they can start to select certain characteristics to make sure that, you know, the people that are seeing these ads are kind of the target audience that they want. INSKEEP: And the excluded person doesn't even know they're being excluded. BOOKER: They don't know. They don't know. INSKEEP: They don't see the ad. They never see the availability. What has Facebook been saying about these accusations, which I think have been out for a little while? BOOKER: Yes, so this comes on the heels of what Facebook and some housing advocates say was a historic settlement that happened last week. Facebook said they were going to do better, and they were going to make changes to the platform to not allow advertisers to selective - to. . . INSKEEP: Selectively - yeah. BOOKER: . . . Selectively put out their ads. So they said they were making changes. They vowed to do better, and they said these changes are going to come about later on this year. INSKEEP: And now HUD's coming after them. Brakkton, thanks so much - really appreciate it. BOOKER: Absolutely. INSKEEP: That's NPR's Brakkton Booker. STEVE INSKEEP, HOST:  Facebook says that it wants to make it harder for advertisers to use Facebook as a tool for discrimination. But whatever it's trying was apparently too little for the Department of Housing and Urban Development because HUD says it is charging Facebook with violating the Fair Housing Act over the way that it allowed advertisers to target and exclude some users. We should note before going on that Facebook is one of NPR's financial supporters. We are nevertheless reporting independently on it, and NPR's Brakkton Booker is here. Brakkton, good morning. BRAKKTON BOOKER, BYLINE: Hi, Steve. INSKEEP: According to HUD, how does Facebook discriminate? BOOKER: Well, as you said in the intro there, HUD is saying that Facebook violated the Fair Housing Act. And that, of course, is a federal protection that was established in 1968. It protects homebuyers and renters from being discriminated against on the basis of race and religion and other protected classes. Now, HUD is saying that Facebook is - allows advertisers to shield who can see which ads appear on their platform on the basis of race. INSKEEP: Do you mean that if an advertiser only wanted white people to see an apartment ad - just to give a hypothetical - that Facebook would allow that to happen? BOOKER: Yes, there are certain things that advertisers can click on the platform that allow certain people to see. So if they are - if a Facebook user kind of identifies as a African-American dog-lover and Facebook only wants to appear to, say, an Anglo cat-lover, they can select different things on the platform that allows - that shields certain people from seeing certain ads on their platform. INSKEEP: On the most basic level, I'm genuinely curious how Facebook is convinced it knows the race of people because, of course, the platform asks for your name, asks for your phone number, asks for your birthday, a lot of other things. I don't know if they directly ask for race, but they must get it from somewhere. BOOKER: Well, HUD is saying in their charging document that Facebook mined extensive data about users and uses that data to determine which users can see housing-related ads. So in theory, because of things that you like and things that you share, there are some kind of algorithms that allow advertisers and Facebook to know kind of who these people are. INSKEEP: One of the reasons that this feels meaningful to me, Brakkton, is that I've done a lot of reporting on discriminatory housing or some reporting on discriminatory housing in the past and spoke with real estate brokers around Chicago who described the process of racial steering. Like, who do you show a house to? Who do you hint to about whether they should or shouldn't live in a particular neighborhood? It can be very subtle, but very powerful in segregating neighborhoods. You're telling me that is exactly what Facebook did, according to HUD. BOOKER: Well, it's really the advertiser. So it's the advertiser's. . . INSKEEP: Facebook allowed. . . BOOKER: Allow - allows advertisers - when they want to create an ad, when they're trying to look for the perfect tenant or the perfect homebuyer, they can start to select certain characteristics to make sure that, you know, the people that are seeing these ads are kind of the target audience that they want. INSKEEP: And the excluded person doesn't even know they're being excluded. BOOKER: They don't know. They don't know. INSKEEP: They don't see the ad. They never see the availability. What has Facebook been saying about these accusations, which I think have been out for a little while? BOOKER: Yes, so this comes on the heels of what Facebook and some housing advocates say was a historic settlement that happened last week. Facebook said they were going to do better, and they were going to make changes to the platform to not allow advertisers to selective - to. . . INSKEEP: Selectively - yeah. BOOKER: . . . Selectively put out their ads. So they said they were making changes. They vowed to do better, and they said these changes are going to come about later on this year. INSKEEP: And now HUD's coming after them. Brakkton, thanks so much - really appreciate it. BOOKER: Absolutely. INSKEEP: That's NPR's Brakkton Booker.", "section": "National", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-30-708386364": {"title": "Does Instagram Have A Problem With Hate Speech And Extremism?  : NPR", "url": "https://www.npr.org/2019/03/30/708386364/does-instagram-have-a-problem-with-hate-speech-and-extremism", "author": "No author found", "published_date": "2019-03-30", "content": "KORVA COLEMAN, HOST:  We've had many conversations on this program about how misinformation and conspiracy theories spread on social media platforms like Facebook and Twitter. But one platform used by more than 1 billion people has gone relatively under the radar, and that's Instagram. For those of you who don't know, Instagram is a social media site that is owned by Facebook. On Instagram, users can post pictures, look at pictures from other people's accounts and message other users. Instagram is increasingly providing a home for hate speech and extremist content. That's according to reporting by Taylor Lorenz, who wrote about this for The Atlantic. She says that Instagram is likely to be, quote, \"where the next great battle against misinformation will be fought, and yet it has largely escaped scrutiny. \" And we should mention here that Facebook is among NPR's financial supporters. Taylor Lorenz joins us on the line from New York. Welcome, Taylor. TAYLOR LORENZ: Hi. Thanks for having me. COLEMAN: Taylor, for people who mainly use, say, Facebook and Twitter, what does Instagram look like? How is it different? LORENZ: Well, Instagram is primarily a visual platform. So unlike Twitter, where most tweets are made up of text and Facebook, where there's a lot of mix of links, videos, things like that, Instagram is primarily images and video. So you can see content from friends, family, news organizations, meme pages - really anyone. COLEMAN: So what does extremist content look like on Instagram? LORENZ: Extremist content on Instagram is essentially just a more visual way of presenting classic misinformation that we've seen on other platforms - so a lot of racist memes, white nationalist content, sometimes screenshots of fake news articles, sometimes people like Alex Jones ranting and promoting conspiracy via YouTube clips that are uploaded to Instagram TV, which is their sort of YouTube competitor. So it can take a lot of different forms. COLEMAN: Who follows these accounts? LORENZ: Millions and millions of people follow accounts that post content like this. A lot of these accounts are actually targeted towards younger people. So some of the heaviest engagers on Instagram are teenagers and sort of young millennials. And so a lot of these big right-wing extremist meme pages consider those people their audience. And those are the users that they're targeting. COLEMAN: How easy is it to, say, follow one account and then get attracted to another account, and then another, and then another that might feature white supremacism? LORENZ: It's extremely easy. I mean, Instagram actually pushes this and facilitates it. So Instagram is built on a bunch of different algorithms. And one big algorithm that stimulates growth in the site is the page recommendation algorithm. So that's when like - when you follow one Instagram page, you're immediately prompted to follow more pages. So you can follow what's considered a mainstream conservative meme page and you're immediately recommended very extremist content from people like Alex Jones and other notorious conspiracy theorists. COLEMAN: Now, Facebook has announced that next week it'll begin banning white nationalism and white separatism content on both Facebook and Instagram, which it controls. How did white separatism and white nationalism begin to flourish there in the first place? LORENZ: All of these platforms have really taken a hands-off approach. They really haven't policed white nationalism or white separatism to the extent that they have other extremist movements. I mean, the New Zealand shooting, I think, was hopefully an inflection point, where it's becoming increasingly clear that they have to crack down on this stuff because not only are they - is this kind of extremist content running rampant on the platforms, these platforms are facilitating its growth. It's a big problem with Instagram though, as opposed to Facebook and Twitter, is that a lot of these big white nationalist figures, for example, there's a huge cadre of people that are part of the Identity Evropa movement. This is a white nationalist, white supremacist movement. And they're not exactly espousing their ideas on Instagram, but they're normalizing themselves. So a lot of them are adopting influencer strategies, where they're kind of actually just posting about their lifestyle, posting themselves at nice events, dressed up. And people will follow some of these white nationalist figures, aspire to their lifestyle and then end up becoming introduced to their ideas. You know, they'll go ahead and Google them. They'll start watching their YouTube video. They'll start reading contents on a blog, maybe. So they're more susceptible to that. COLEMAN: That was Taylor Lorenz. She reports on tech news for The Atlantic. Thank you for joining us, Taylor. LORENZ: Thank you so much for having me. KORVA COLEMAN, HOST:   We've had many conversations on this program about how misinformation and conspiracy theories spread on social media platforms like Facebook and Twitter. But one platform used by more than 1 billion people has gone relatively under the radar, and that's Instagram. For those of you who don't know, Instagram is a social media site that is owned by Facebook. On Instagram, users can post pictures, look at pictures from other people's accounts and message other users. Instagram is increasingly providing a home for hate speech and extremist content. That's according to reporting by Taylor Lorenz, who wrote about this for The Atlantic. She says that Instagram is likely to be, quote, \"where the next great battle against misinformation will be fought, and yet it has largely escaped scrutiny. \" And we should mention here that Facebook is among NPR's financial supporters. Taylor Lorenz joins us on the line from New York. Welcome, Taylor. TAYLOR LORENZ: Hi. Thanks for having me. COLEMAN: Taylor, for people who mainly use, say, Facebook and Twitter, what does Instagram look like? How is it different? LORENZ: Well, Instagram is primarily a visual platform. So unlike Twitter, where most tweets are made up of text and Facebook, where there's a lot of mix of links, videos, things like that, Instagram is primarily images and video. So you can see content from friends, family, news organizations, meme pages - really anyone. COLEMAN: So what does extremist content look like on Instagram? LORENZ: Extremist content on Instagram is essentially just a more visual way of presenting classic misinformation that we've seen on other platforms - so a lot of racist memes, white nationalist content, sometimes screenshots of fake news articles, sometimes people like Alex Jones ranting and promoting conspiracy via YouTube clips that are uploaded to Instagram TV, which is their sort of YouTube competitor. So it can take a lot of different forms. COLEMAN: Who follows these accounts? LORENZ: Millions and millions of people follow accounts that post content like this. A lot of these accounts are actually targeted towards younger people. So some of the heaviest engagers on Instagram are teenagers and sort of young millennials. And so a lot of these big right-wing extremist meme pages consider those people their audience. And those are the users that they're targeting. COLEMAN: How easy is it to, say, follow one account and then get attracted to another account, and then another, and then another that might feature white supremacism? LORENZ: It's extremely easy. I mean, Instagram actually pushes this and facilitates it. So Instagram is built on a bunch of different algorithms. And one big algorithm that stimulates growth in the site is the page recommendation algorithm. So that's when like - when you follow one Instagram page, you're immediately prompted to follow more pages. So you can follow what's considered a mainstream conservative meme page and you're immediately recommended very extremist content from people like Alex Jones and other notorious conspiracy theorists. COLEMAN: Now, Facebook has announced that next week it'll begin banning white nationalism and white separatism content on both Facebook and Instagram, which it controls. How did white separatism and white nationalism begin to flourish there in the first place? LORENZ: All of these platforms have really taken a hands-off approach. They really haven't policed white nationalism or white separatism to the extent that they have other extremist movements. I mean, the New Zealand shooting, I think, was hopefully an inflection point, where it's becoming increasingly clear that they have to crack down on this stuff because not only are they - is this kind of extremist content running rampant on the platforms, these platforms are facilitating its growth. It's a big problem with Instagram though, as opposed to Facebook and Twitter, is that a lot of these big white nationalist figures, for example, there's a huge cadre of people that are part of the Identity Evropa movement. This is a white nationalist, white supremacist movement. And they're not exactly espousing their ideas on Instagram, but they're normalizing themselves. So a lot of them are adopting influencer strategies, where they're kind of actually just posting about their lifestyle, posting themselves at nice events, dressed up. And people will follow some of these white nationalist figures, aspire to their lifestyle and then end up becoming introduced to their ideas. You know, they'll go ahead and Google them. They'll start watching their YouTube video. They'll start reading contents on a blog, maybe. So they're more susceptible to that. COLEMAN: That was Taylor Lorenz. She reports on tech news for The Atlantic. Thank you for joining us, Taylor. LORENZ: Thank you so much for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-31-708209142": {"title": "Former Homeland Security Head Janet Napolitano Says Cybersecurity Should Be A Top Priority  : NPR", "url": "https://www.npr.org/2019/03/31/708209142/former-homeland-security-head-napolitano-says-cybersecurity-should-be-a-top-prio", "author": "No author found", "published_date": "2019-03-31", "content": "KORVA COLEMAN, HOST: We're going to turn now to immigration. President Trump announced a plan on Friday to cut aid to Guatemala, Honduras and El Salvador. He says those countries aren't doing enough to stop migrants from coming to the United States and that it's creating a national emergency. One person who's thought a lot about border issues is former secretary of Homeland Security, Janet Napolitano. In her new book, \"How Safe Are We? : Homeland Security Since 9/11,\" which she co-wrote with Karen Breslau, Napolitano outlines what she believes are the biggest security threats to America. We began our conversation with immigration. I asked for her thoughts on President Trump's threat to close large sections of the southern U. S. border. JANET NAPOLITANO: I think it's unnecessary and unwise. First of all, the economic impact would be huge. Mexico is our number two trading partner. There are thousands of trucks and vehicles that go through those ports of entry day in and day out, responsible for hundreds of thousands of jobs within the United States. So just the plain economic impact of that would be immediate, and it would be deep. I would recommend that the president approach the border as a border zone, that he flood the zone with the rule of law, that he bring on board more immigration judges and more immigration courts, station them right at the border so that those who are presenting themselves with their applications for asylum can have their cases adjudicated fairly and expeditiously. NAPOLITANO: Journalists reporting from the U. S. -Mexico border, including NPR's own John Burnett, have said that Border Patrol agents are swamped and that border crossings show no signs of slowing down. Is there something to be said for President Trump's hardline approach? NAPOLITANO: You know, I actually think we should take a step back and analyze, what is the source of this migration? And the source of it now is - are families fleeing the conditions in Guatemala, El Salvador and Honduras. We ought to reinvest as a country in improving conditions in those countries, working on gang violence prevention, working on strengthening the institutions of government, the judicial systems, the law enforcement systems. You know, we did something like that with Colombia when Colombia was essentially a narco-state. And the United States said, look - that's, you know, not acceptable. And there were people in Colombia who didn't want to live in a narco-state. And working together and putting some American resources into it, now Colombia is essentially a tourist destination. We can achieve the same kind of progress in the northern countries of Central America. COLEMAN: You argue in the book that the Trump administration is focusing too much on issues such as the U. S. -Mexico border and not enough on climate change. What needs to be done differently? NAPOLITANO: We must do more by way of adaptation to the climate change that already has occurred. How do we build our roads? Where do we locate our bridges? What kind of building materials can be used? When a community is destroyed by a natural disaster, where is it allowed to be rebuilt? These are the kinds of questions that I think FEMA should be taking on in a climate adaptation regime. COLEMAN: You've talked about your book as being a type of report card on how secure the nation actually is. And one of the areas you've marked as needs improvement is our response to foreign cyberincursions - not just into, say, states' voting systems but into power companies, private companies that hold private records such as health or banking data - and, of course, social media. How can we better protect ourselves against foreign saboteurs? NAPOLITANO: We should have a commission on cyber-take up from clarifying the jurisdictions of the various federal agencies that touch upon cybersecurity all the way up to and including what constitutes an act of cyberwarfare and what are the sanctions that attend to that. Part of that, obviously, is working internationally because cyber doesn't respect or know national boundaries. And a lot of it involves working with the private sector because so much of our nation's critical infrastructure is in private sector hands - our banking system, telecommunications. All of these need to be brought to bear on this very complicated topic. But it is the homeland security issue of this decade. COLEMAN: That's Janet Napolitano, former secretary of homeland security and current president of the University of California. Her book \"How Safe Are We? : Homeland Security Since 9/11\" is out now. Secretary Napolitano, Janet, thank you for joining me. NAPOLITANO: Thank you. KORVA COLEMAN, HOST:  We're going to turn now to immigration. President Trump announced a plan on Friday to cut aid to Guatemala, Honduras and El Salvador. He says those countries aren't doing enough to stop migrants from coming to the United States and that it's creating a national emergency. One person who's thought a lot about border issues is former secretary of Homeland Security, Janet Napolitano. In her new book, \"How Safe Are We? : Homeland Security Since 9/11,\" which she co-wrote with Karen Breslau, Napolitano outlines what she believes are the biggest security threats to America. We began our conversation with immigration. I asked for her thoughts on President Trump's threat to close large sections of the southern U. S. border. JANET NAPOLITANO: I think it's unnecessary and unwise. First of all, the economic impact would be huge. Mexico is our number two trading partner. There are thousands of trucks and vehicles that go through those ports of entry day in and day out, responsible for hundreds of thousands of jobs within the United States. So just the plain economic impact of that would be immediate, and it would be deep. I would recommend that the president approach the border as a border zone, that he flood the zone with the rule of law, that he bring on board more immigration judges and more immigration courts, station them right at the border so that those who are presenting themselves with their applications for asylum can have their cases adjudicated fairly and expeditiously. NAPOLITANO: Journalists reporting from the U. S. -Mexico border, including NPR's own John Burnett, have said that Border Patrol agents are swamped and that border crossings show no signs of slowing down. Is there something to be said for President Trump's hardline approach? NAPOLITANO: You know, I actually think we should take a step back and analyze, what is the source of this migration? And the source of it now is - are families fleeing the conditions in Guatemala, El Salvador and Honduras. We ought to reinvest as a country in improving conditions in those countries, working on gang violence prevention, working on strengthening the institutions of government, the judicial systems, the law enforcement systems. You know, we did something like that with Colombia when Colombia was essentially a narco-state. And the United States said, look - that's, you know, not acceptable. And there were people in Colombia who didn't want to live in a narco-state. And working together and putting some American resources into it, now Colombia is essentially a tourist destination. We can achieve the same kind of progress in the northern countries of Central America. COLEMAN: You argue in the book that the Trump administration is focusing too much on issues such as the U. S. -Mexico border and not enough on climate change. What needs to be done differently? NAPOLITANO: We must do more by way of adaptation to the climate change that already has occurred. How do we build our roads? Where do we locate our bridges? What kind of building materials can be used? When a community is destroyed by a natural disaster, where is it allowed to be rebuilt? These are the kinds of questions that I think FEMA should be taking on in a climate adaptation regime. COLEMAN: You've talked about your book as being a type of report card on how secure the nation actually is. And one of the areas you've marked as needs improvement is our response to foreign cyberincursions - not just into, say, states' voting systems but into power companies, private companies that hold private records such as health or banking data - and, of course, social media. How can we better protect ourselves against foreign saboteurs? NAPOLITANO: We should have a commission on cyber-take up from clarifying the jurisdictions of the various federal agencies that touch upon cybersecurity all the way up to and including what constitutes an act of cyberwarfare and what are the sanctions that attend to that. Part of that, obviously, is working internationally because cyber doesn't respect or know national boundaries. And a lot of it involves working with the private sector because so much of our nation's critical infrastructure is in private sector hands - our banking system, telecommunications. All of these need to be brought to bear on this very complicated topic. But it is the homeland security issue of this decade. COLEMAN: That's Janet Napolitano, former secretary of homeland security and current president of the University of California. Her book \"How Safe Are We? : Homeland Security Since 9/11\" is out now. Secretary Napolitano, Janet, thank you for joining me. NAPOLITANO: Thank you.", "section": "National Security", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-31-708537582": {"title": "Woman Petitions For More Representation of Black, Natural Hair With Afro Emoji  : NPR", "url": "https://www.npr.org/2019/03/31/708537582/one-woman-wants-to-create-this-insert-afro-emoji-here", "author": "No author found", "published_date": "2019-03-31", "content": "", "section": "Race", "disclaimer": ""}, "2019-03-31-708503930": {"title": "Petitioning For An Afro Emoji : NPR", "url": "https://www.npr.org/2019/03/31/708503930/petitioning-for-an-afro-emoji", "author": "No author found", "published_date": "2019-03-31", "content": "LULU GARCIA-NAVARRO, HOST: There are over 2,800 official emojis. You know those little icons you use to spice up your texts - a cow, a doughnut, a queen, and my personal favorite, the eye roll. The Unicode Consortium, the nonprofit that oversees emojis - yes, there is such a thing - has been criticized for its lack of diversity and inclusivity when it comes to LGBTQ or religious expression. And now they're under fire for one more - the afro. That's right. Of the more than 15 hairstyles offered in emoji form, not one is an afro. Rhianna Jones wants to change that. She's a freelance writer based in New York. And she joins me now from our studios there. Rhianna, welcome. RHIANNA JONES: Hi, Lulu. Thank you so much for having me. GARCIA-NAVARRO: It is an absolute pleasure to have you. Let's talk through this. There's already been some diversity in the emoji world. We have different skin tones included in emoji form. But why do you think an afro should be included too? JONES: I think an afro should be included too because there's an entire community of people - black, Afro-Latinx, diasporic. We were just talking about the do for others. It's a lot of people that have hair that grows up, forward and spherically and, you know, defies gravity. And there's been - and there's just been a big dearth and lack of representation of natural hair and Afro hair in the media. And I think the lack of Afro hair in our keyboards is a subtle but constant reminder of that. You know, we're a world that interacts largely in digital spaces. And emoji are this universal language of self-expression and kind of the closest way to, really, inject our personality into our conversations. And the fact that a very large community and culture of people has no reflection of themselves in those conversations is something that I'm trying to change. GARCIA-NAVARRO: Describe the emoji for us. JONES: The emoji - so we started off with - we called her Frolange because we'd been listening to Solange's new album for like. . . (LAUGHTER)JONES: . . . A couple weeks straight. GARCIA-NAVARRO: And she's got great hair. JONES: And she's got great hair. She's got absolute fly girl hair. But like I said, it grows spherically and upward. And it takes up as much space as you can in the very, very minute parameters of an emoji. GARCIA-NAVARRO: Who designed it? JONES: My friend Kerrilyn Gibson, who is - she's a graphic designer and a fellow Afro-ed female. And we really wanted it to be by us and for us because we, like so many of our friends, you know - and for almost a year now, I'd sign a lot of my emails, insert Afro emoji here. And that's kind of what precipitated all of this is one day, I just - I was coming off of Black History Month and celebrating all these - you know, our big stories and our big voices and our being hair. And I just realized that I shouldn't have to do that anymore because it's more than just an emoji. It's about people being able to see themselves reflected in the conversations they're having not only on screen but in real life. GARCIA-NAVARRO: And any idea how long it'll take until we can see the Afro emoji? JONES: It's a pretty lengthy process. I hope that they're listening. And I hope that they know that we all really want this. (LAUGHTER)GARCIA-NAVARRO: Rihanna Jones, freelance writer and Afro emoji proponent, thank you so much. JONES: Thank you. GARCIA-NAVARRO: Rihanna Jones plans to submit the Afro emoji to Unicode tonight. If you want to see it, you can go to change. org. LULU GARCIA-NAVARRO, HOST:  There are over 2,800 official emojis. You know those little icons you use to spice up your texts - a cow, a doughnut, a queen, and my personal favorite, the eye roll. The Unicode Consortium, the nonprofit that oversees emojis - yes, there is such a thing - has been criticized for its lack of diversity and inclusivity when it comes to LGBTQ or religious expression. And now they're under fire for one more - the afro. That's right. Of the more than 15 hairstyles offered in emoji form, not one is an afro. Rhianna Jones wants to change that. She's a freelance writer based in New York. And she joins me now from our studios there. Rhianna, welcome. RHIANNA JONES: Hi, Lulu. Thank you so much for having me. GARCIA-NAVARRO: It is an absolute pleasure to have you. Let's talk through this. There's already been some diversity in the emoji world. We have different skin tones included in emoji form. But why do you think an afro should be included too? JONES: I think an afro should be included too because there's an entire community of people - black, Afro-Latinx, diasporic. We were just talking about the do for others. It's a lot of people that have hair that grows up, forward and spherically and, you know, defies gravity. And there's been - and there's just been a big dearth and lack of representation of natural hair and Afro hair in the media. And I think the lack of Afro hair in our keyboards is a subtle but constant reminder of that. You know, we're a world that interacts largely in digital spaces. And emoji are this universal language of self-expression and kind of the closest way to, really, inject our personality into our conversations. And the fact that a very large community and culture of people has no reflection of themselves in those conversations is something that I'm trying to change. GARCIA-NAVARRO: Describe the emoji for us. JONES: The emoji - so we started off with - we called her Frolange because we'd been listening to Solange's new album for like. . . (LAUGHTER) JONES: . . . A couple weeks straight. GARCIA-NAVARRO: And she's got great hair. JONES: And she's got great hair. She's got absolute fly girl hair. But like I said, it grows spherically and upward. And it takes up as much space as you can in the very, very minute parameters of an emoji. GARCIA-NAVARRO: Who designed it? JONES: My friend Kerrilyn Gibson, who is - she's a graphic designer and a fellow Afro-ed female. And we really wanted it to be by us and for us because we, like so many of our friends, you know - and for almost a year now, I'd sign a lot of my emails, insert Afro emoji here. And that's kind of what precipitated all of this is one day, I just - I was coming off of Black History Month and celebrating all these - you know, our big stories and our big voices and our being hair. And I just realized that I shouldn't have to do that anymore because it's more than just an emoji. It's about people being able to see themselves reflected in the conversations they're having not only on screen but in real life. GARCIA-NAVARRO: And any idea how long it'll take until we can see the Afro emoji? JONES: It's a pretty lengthy process. I hope that they're listening. And I hope that they know that we all really want this. (LAUGHTER) GARCIA-NAVARRO: Rihanna Jones, freelance writer and Afro emoji proponent, thank you so much. JONES: Thank you. GARCIA-NAVARRO: Rihanna Jones plans to submit the Afro emoji to Unicode tonight. If you want to see it, you can go to change. org.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-03-31-708039892": {"title": "Is Civility Possible On Social Media? : NPR", "url": "https://www.npr.org/2019/03/31/708039892/in-an-increasingly-polarized-america-is-it-possible-to-be-civil-on-social-media", "author": "No author found", "published_date": "2019-03-31", "content": "LULU GARCIA-NAVARRO, HOST: Twitter - a bastion of kindness and empathy; yeah, not so much. If you spend any time on social media, you know it can easily turn into a public brawl full of insults, curse words and vile threats. And maybe you've found yourself asking, who are these people? Why would anyone write stuff like this? NPR's Jasmine Garsd wondered the same thing. As part of our series on civility and incivility in a polarized America, she set out to meet two people who have done battle in a very noisy public space. JASMINE GARSD, BYLINE: Before I could even interview these guys, I had to agree not to use their real names. The online circle they roll in can get really aggressive. One of them says he's even gotten death threats in the past. They asked to be called Tyler and Larry. They've never met. If they crossed paths on the streets, they wouldn't recognize each other. But almost every day for the last year or so, they've engaged on Twitter. And some of their posts are benign, even wonkish (ph). LARRY: Privatize Medicare and Medicaid and create a plan to phase out Social Security without ripping off. . . GARSD: That's Larry, reading a recent tweet. He's a Trump supporter on the East Coast. The other one, Tyler, is a liberal. TYLER: Trump wasn't exonerated or did you not see that? GARSD: He lives in Indiana. He's been tweeting a lot lately about the Mueller investigation. TYLER: You should care more about the country than some orange, racist peon. GARSD: OK. So Twitter users, you know. This type of thing rarely ends well. The conversation almost always turns awful. Larry will snap and say. . . LARRY: Your idiotic posts are serious. Get the [expletive] out of here, cretin. GARSD: . . . Which will drive Tyler nuts. TYLER: Oh, shut the [expletive], you inept [expletive]. GARSD: And it just goes on and on. LARRY: Everything you tweet is stupid. Now [expletive], moron. GARSD: Even Larry can't believe he wrote this. LARRY: Yeah, I'm not so proud about that. GARSD: He told me his wife is a liberal. LARRY: She's never even seen me on Twitter. I'm like, please don't divorce me if you read my tweets because I just sort of shift modes and become - I might call it a troll mode, where I'm not really trying to make a point anymore. I'm just trying to battle. And I don't like that. I don't feel good about it. GARSD: And Tyler, Larry's liberal Twitter nemesis, he knows he goes overboard too. TYLER: I said, sit the [expletive] down. The adults are talking. That was a low moment. GARSD: Remember. These guys are two complete strangers, but they've been going at it since last year. And the obvious question is. . . TYLER: Why do you even bother? Why are you on that thread? GARSD: Tyler says it's a question his husband asks a lot. Tyler is a gay man who lives in a conservative state. Even though he says Larry isn't one of them, there are people in his Twitter world who, Tyler says, post really homophobic and racist things. And he's sick of it. Like a lot of Americans, Tyler says he feels the stakes are really high right now. Maybe it's just not the time to be civil on Twitter. TYLER: If we stop talking about racism, if we stop talking about homophobia and hate towards gays and just everything under the sun, then it goes un-taken care of. It goes to the side that's promoting that. GARSD: But this constant fighting, Tyler says it takes a toll. TYLER: You find yourself thinking and feeling about the thread, even if you're not on your phone or the computer to where you're having panic attacks or you're hyperventilating. GARSD: Larry, the conservative, says he doesn't take Twitter so personally. But he's been thinking about quitting. He doesn't always like the way he behaves online. He has trouble breaking away, though. Like, he'll be at a restaurant and on his phone. LARRY: I'll just think, I just got to send this one last one, you know? And it's, like, taking 10 minutes. And we're - the waitress is tapping her pencil. It's a fun thing to do, right? So every time you send a tweet and you think you made a point, it feels good. And you want to keep doing it. GARSD: The pleasure and anxiety they're both describing, there's a growing body of scientific research on this topic that suggests social media can have addictive qualities, just like a drug or a gambling habit. Consider the very nature of Twitter. It's more public than other media platforms. It's easier to just jump into an argument and pile on, so the whole thing can be very performative. More than talking, we're showing ourselves off. Larry says that's not why he joined. LARRY: The reason I joined was to have enlightening discussion. And that's the frustration is that there's no mindshare. It just always seems to blow up. And at that point, it just becomes two trolls trolling one another. GARSD: Even though I interviewed them separately, both men came to a similar conclusion. TYLER: You'd have a better chance of solving your issues over a beer or a meal than sitting behind a phone, typing - in caps - your point across. LARRY: You know, we used to have barbecues and - liberals and conservatives. And if you're my neighbor, come over and eat and drink. You know, Twitter just is not good for that kind of thing, I think. GARSD: And if they were sitting at that barbecue, they'd be able to see all the things they can't see on Twitter - a hurt look, a furrowed brow, a face flushed with embarrassment. Two-hundred-eighty characters, hashtags and memes might just not be enough for the difficult conversations we need to have right now. Jasmine Garsd, NPR News, New York. (SOUNDBITE OF SAXON SHORE'S \"WITH A RED SUIT YOU WILL BECOME A MAN\") LULU GARCIA-NAVARRO, HOST:  Twitter - a bastion of kindness and empathy; yeah, not so much. If you spend any time on social media, you know it can easily turn into a public brawl full of insults, curse words and vile threats. And maybe you've found yourself asking, who are these people? Why would anyone write stuff like this? NPR's Jasmine Garsd wondered the same thing. As part of our series on civility and incivility in a polarized America, she set out to meet two people who have done battle in a very noisy public space. JASMINE GARSD, BYLINE: Before I could even interview these guys, I had to agree not to use their real names. The online circle they roll in can get really aggressive. One of them says he's even gotten death threats in the past. They asked to be called Tyler and Larry. They've never met. If they crossed paths on the streets, they wouldn't recognize each other. But almost every day for the last year or so, they've engaged on Twitter. And some of their posts are benign, even wonkish (ph). LARRY: Privatize Medicare and Medicaid and create a plan to phase out Social Security without ripping off. . . GARSD: That's Larry, reading a recent tweet. He's a Trump supporter on the East Coast. The other one, Tyler, is a liberal. TYLER: Trump wasn't exonerated or did you not see that? GARSD: He lives in Indiana. He's been tweeting a lot lately about the Mueller investigation. TYLER: You should care more about the country than some orange, racist peon. GARSD: OK. So Twitter users, you know. This type of thing rarely ends well. The conversation almost always turns awful. Larry will snap and say. . . LARRY: Your idiotic posts are serious. Get the [expletive] out of here, cretin. GARSD: . . . Which will drive Tyler nuts. TYLER: Oh, shut the [expletive], you inept [expletive]. GARSD: And it just goes on and on. LARRY: Everything you tweet is stupid. Now [expletive], moron. GARSD: Even Larry can't believe he wrote this. LARRY: Yeah, I'm not so proud about that. GARSD: He told me his wife is a liberal. LARRY: She's never even seen me on Twitter. I'm like, please don't divorce me if you read my tweets because I just sort of shift modes and become - I might call it a troll mode, where I'm not really trying to make a point anymore. I'm just trying to battle. And I don't like that. I don't feel good about it. GARSD: And Tyler, Larry's liberal Twitter nemesis, he knows he goes overboard too. TYLER: I said, sit the [expletive] down. The adults are talking. That was a low moment. GARSD: Remember. These guys are two complete strangers, but they've been going at it since last year. And the obvious question is. . . TYLER: Why do you even bother? Why are you on that thread? GARSD: Tyler says it's a question his husband asks a lot. Tyler is a gay man who lives in a conservative state. Even though he says Larry isn't one of them, there are people in his Twitter world who, Tyler says, post really homophobic and racist things. And he's sick of it. Like a lot of Americans, Tyler says he feels the stakes are really high right now. Maybe it's just not the time to be civil on Twitter. TYLER: If we stop talking about racism, if we stop talking about homophobia and hate towards gays and just everything under the sun, then it goes un-taken care of. It goes to the side that's promoting that. GARSD: But this constant fighting, Tyler says it takes a toll. TYLER: You find yourself thinking and feeling about the thread, even if you're not on your phone or the computer to where you're having panic attacks or you're hyperventilating. GARSD: Larry, the conservative, says he doesn't take Twitter so personally. But he's been thinking about quitting. He doesn't always like the way he behaves online. He has trouble breaking away, though. Like, he'll be at a restaurant and on his phone. LARRY: I'll just think, I just got to send this one last one, you know? And it's, like, taking 10 minutes. And we're - the waitress is tapping her pencil. It's a fun thing to do, right? So every time you send a tweet and you think you made a point, it feels good. And you want to keep doing it. GARSD: The pleasure and anxiety they're both describing, there's a growing body of scientific research on this topic that suggests social media can have addictive qualities, just like a drug or a gambling habit. Consider the very nature of Twitter. It's more public than other media platforms. It's easier to just jump into an argument and pile on, so the whole thing can be very performative. More than talking, we're showing ourselves off. Larry says that's not why he joined. LARRY: The reason I joined was to have enlightening discussion. And that's the frustration is that there's no mindshare. It just always seems to blow up. And at that point, it just becomes two trolls trolling one another. GARSD: Even though I interviewed them separately, both men came to a similar conclusion. TYLER: You'd have a better chance of solving your issues over a beer or a meal than sitting behind a phone, typing - in caps - your point across. LARRY: You know, we used to have barbecues and - liberals and conservatives. And if you're my neighbor, come over and eat and drink. You know, Twitter just is not good for that kind of thing, I think. GARSD: And if they were sitting at that barbecue, they'd be able to see all the things they can't see on Twitter - a hurt look, a furrowed brow, a face flushed with embarrassment. Two-hundred-eighty characters, hashtags and memes might just not be enough for the difficult conversations we need to have right now. Jasmine Garsd, NPR News, New York. (SOUNDBITE OF SAXON SHORE'S \"WITH A RED SUIT YOU WILL BECOME A MAN\")", "section": "Civility Wars", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-01-708724283": {"title": "Gone Live On Facebook? Share Your Thoughts About Facebook Live : NPR", "url": "https://www.npr.org/2019/04/01/708724283/go-live-share-your-thoughts-about-facebook-live", "author": "No author found", "published_date": "2019-04-01", "content": "", "section": "Technology", "disclaimer": ""}, "2019-04-01-708738804": {"title": "Flight Delays Hit U.S. Airlines; Computer Problems Blamed : NPR", "url": "https://www.npr.org/2019/04/01/708738804/u-s-flight-delays-hit-several-airlines-computer-problems-blamed", "author": "No author found", "published_date": "2019-04-01", "content": "", "section": "National", "disclaimer": ""}, "2019-04-01-707967899": {"title": "Young Astronomer Uses Artificial Intelligence To Discover 2 Exoplanets  : NPR", "url": "https://www.npr.org/2019/04/01/707967899/young-astronomer-uses-artificial-intelligence-to-discover-2-exoplanets", "author": "No author found", "published_date": "2019-04-01", "content": "STEVE INSKEEP, HOST: Astronomers know of about 4,000 planets orbiting stars outside our solar system. Now they know of two more, thanks to an undergraduate college student using artificial intelligence. Here's NPR's Joe Palca. JOE PALCA, BYLINE: Anne Dattilo is a senior at The University of Texas at Austin. Last year, an astronomer talked to her class about his research using a NASA satellite called Kepler to hunt for planets orbiting distant stars. ANNE DATTILO: And at the very end, he was like, I'm taking undergrads, if any of you want to do research on the subject, finding planets, and I decided that's what I wanted to do. So I emailed him, and a year and a half later, here I am. PALCA: She led a team that discovered two Earth-sized planets orbiting stars more than 1,200 light-years from Earth. To find the planets, Dattilo used an artificial intelligence approach called machine learning to comb through a Kepler data set called K2; K2 contains measurements of the light coming from tens of thousands of stars. Dattilo says when a star is what she calls boring, the light coming from it is constant. DATTILO: But if you can imagine something passing in front of that star, the light we receive would dim. And so if you see that periodically, that would be a signal that a planet is in front of that. PALCA: The artificial intelligence program looks for these fluctuations in a star's light that might be associated with a planet passing in front. Now, you don't have to be a NASA scientist to use data from a NASA satellite. JESSIE CHRISTIANSEN: NASA makes all of the data publicly available. You just have to think of a new idea to do with the data that no one's done before. PALCA: Jessie Christiansen is a research scientist at the NASA Exoplanet Science Institute at Caltech in Pasadena. CHRISTIANSEN: This is the first time someone's gone through and done a machine learning process on the K2 data to come up with a uniform list of planet candidates. PALCA: And that will be valuable beyond just getting a good grade on an undergraduate class? CHRISTIANSEN: Absolutely. PALCA: In fact, Michelle Ntampaka at the Harvard-Smithsonian Institute for Astrophysics in Cambridge says she's seen something remarkable happen in the last five years or so. MICHELLE NTAMPAKA: And that is that there has been a dramatic increase in the amount of machine learning research that's happening for astronomy applications. PALCA: That's because newer telescopes don't so much collect images of stars and galaxies as digital data about these celestial objects. NTAMPAKA: We're just going to see unprecedented data volumes, and we're going to have to come up with new ways to deal with that. PALCA: Ntampaka says the next generation of astronomers will have to be comfortable working with artificial intelligence to make sense of all this data. So writing a machine learning program as an undergrad is good preparation for Anne Dattilo as she heads off to get her graduate degree in astronomy. Joe Palca, NPR News. (SOUNDBITE OF FYZE'S \"WANDERER\") STEVE INSKEEP, HOST:  Astronomers know of about 4,000 planets orbiting stars outside our solar system. Now they know of two more, thanks to an undergraduate college student using artificial intelligence. Here's NPR's Joe Palca. JOE PALCA, BYLINE: Anne Dattilo is a senior at The University of Texas at Austin. Last year, an astronomer talked to her class about his research using a NASA satellite called Kepler to hunt for planets orbiting distant stars. ANNE DATTILO: And at the very end, he was like, I'm taking undergrads, if any of you want to do research on the subject, finding planets, and I decided that's what I wanted to do. So I emailed him, and a year and a half later, here I am. PALCA: She led a team that discovered two Earth-sized planets orbiting stars more than 1,200 light-years from Earth. To find the planets, Dattilo used an artificial intelligence approach called machine learning to comb through a Kepler data set called K2; K2 contains measurements of the light coming from tens of thousands of stars. Dattilo says when a star is what she calls boring, the light coming from it is constant. DATTILO: But if you can imagine something passing in front of that star, the light we receive would dim. And so if you see that periodically, that would be a signal that a planet is in front of that. PALCA: The artificial intelligence program looks for these fluctuations in a star's light that might be associated with a planet passing in front. Now, you don't have to be a NASA scientist to use data from a NASA satellite. JESSIE CHRISTIANSEN: NASA makes all of the data publicly available. You just have to think of a new idea to do with the data that no one's done before. PALCA: Jessie Christiansen is a research scientist at the NASA Exoplanet Science Institute at Caltech in Pasadena. CHRISTIANSEN: This is the first time someone's gone through and done a machine learning process on the K2 data to come up with a uniform list of planet candidates. PALCA: And that will be valuable beyond just getting a good grade on an undergraduate class? CHRISTIANSEN: Absolutely. PALCA: In fact, Michelle Ntampaka at the Harvard-Smithsonian Institute for Astrophysics in Cambridge says she's seen something remarkable happen in the last five years or so. MICHELLE NTAMPAKA: And that is that there has been a dramatic increase in the amount of machine learning research that's happening for astronomy applications. PALCA: That's because newer telescopes don't so much collect images of stars and galaxies as digital data about these celestial objects. NTAMPAKA: We're just going to see unprecedented data volumes, and we're going to have to come up with new ways to deal with that. PALCA: Ntampaka says the next generation of astronomers will have to be comfortable working with artificial intelligence to make sense of all this data. So writing a machine learning program as an undergrad is good preparation for Anne Dattilo as she heads off to get her graduate degree in astronomy. Joe Palca, NPR News. (SOUNDBITE OF FYZE'S \"WANDERER\")", "section": "Joe's Big Idea", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-04-709751602": {"title": "New Australian Bill Would Punish Internet Platforms For Violent Content : NPR", "url": "https://www.npr.org/2019/04/04/709751602/australia-criminalizes-failure-to-remove-violent-content-from-internet-platforms", "author": "No author found", "published_date": "2019-04-04", "content": "", "section": "World", "disclaimer": ""}, "2019-04-04-709766379": {"title": "Preliminary Report: Ethiopian Airlines Flight Repeatedly Nose-Dived Before Crash : NPR", "url": "https://www.npr.org/2019/04/04/709766379/preliminary-crash-report-says-ethiopian-airlines-crew-complied-with-procedures", "author": "No author found", "published_date": "2019-04-04", "content": "", "section": "Africa", "disclaimer": ""}, "2019-04-05-710317470": {"title": "College Students Accused Of $900,000 Scam Using Counterfeit iPhones From China : NPR", "url": "https://www.npr.org/2019/04/05/710317470/two-students-allegedly-cheated-apple-out-of-nearly-900-000-in-fake-iphone-scheme", "author": "No author found", "published_date": "2019-04-05", "content": "", "section": "National", "disclaimer": ""}, "2019-04-05-710313380": {"title": "Facebook's Ban On White Extremism Comes Amid International Pressure : NPR", "url": "https://www.npr.org/2019/04/05/710313380/facebooks-ban-on-white-extremism-comes-amid-international-pressure", "author": "No author found", "published_date": "2019-04-05", "content": "AUDIE CORNISH, HOST: Facebook was once praised for spreading free speech values, but with the company's ban on white extremist content, we have reached an inflection point. The world is pushing back with different values, which are now being imported by Facebook to the U. S. Here's NPR's Aarti Shahani. AARTI SHAHANI, BYLINE: The worldwide ban goes into effect this week, and the move by Facebook, which is wrapped up in the U. S. culture wars, is actually the result of international pressure forcing the company's hand. Facebook says under the new rules, you cannot post in a celebratory way on its news feed or Instagram, I'm a white nationalist, but you can post, I'm a black nationalist. Some Americans find this outrageous or. . . JOHN SPIER: Ridiculous. SHAHANI: John Spier, self-described libertarian Facebook user in central California. SPIER: I feel like everyone should be able to have freedom of expression, even if they're an idiot. There's a lot of idiots in the world who say a lot of stupid things; we don't need to protect people from that. SHAHANI: What racism is, who can be racist - it's a debate that's getting louder in the U. S. Spier believes that, while Facebook claims to be a neutral platform, the company is taking the liberal side. SPIER: I know that the current popular mode of thought is that only white people can be racist, but I don't agree with that. I grew up as a minority white person in a largely Latino community, and believe me, I know what racism feels like. SHAHANI: Facebook is an NPR sponsor. According to Facebook leaders and civil rights advocates, this issue is not about speech but safety. It's a well-documented fact. White extremists around the world are radicalizing men online, luring them into organized hate groups and promoting lone wolf acts of terror. HEIDI BEIRICH: White supremacists are as much a global movement and interconnected - in other words, sharing ideas, sharing money, sharing tactics, sharing propaganda, visiting each other, et cetera, et cetera - just like you see with Islamic extremists. SHAHANI: Heidi Beirich with the Southern Poverty Law Center has been making this point to Facebook for years. Dylann Roof, who murdered nine people in a church basement in Charleston, got radicalized through Google search. The white nationalist rally in Charlottesville that left three dead and dozens injured was organized on a Facebook page. The Christchurch shooter in New Zealand used Facebook Live, an incredibly powerful broadcast tool. Beirich says the company's latest move is in reaction to the disaster in New Zealand and pressure from law enforcement, particularly those in Europe who are worried about white extremist gunmen. BEIRICH: That realization is dawning on the intelligence communities worldwide, and Facebook is hearing it from them. SHAHANI: Facebook made the unusual move of adding Christchurch massacre footage to a terrorism database that had been focused on Islamic extremism. A Facebook spokesperson says the company will continue to add white extremist content to this database which a handful of tech giants share to censor the most violent content. Again, this is in the face of international pressure. Australia just passed a stringent bill that threatens social media employees with prison time if they don't remove violent content expeditiously. The United Kingdom is about to unveil legislation. Germany has passed tough hate speech laws that carry heavy fines. Heidi Beirich. . . BEIRICH: The U. S. is behind the eight ball on this; Trump doesn't seem to be interested in these issues at all. And I think Facebook is reacting to that in a good way, I would argue. SHAHANI: About 90 percent of Facebook users are outside the U. S. Last week, Mark Zuckerberg called on governments around the world to create and effect a global standard for speech. That has never existed before. It's a long shot, and as Zuckerberg sees it, that's what needs to be engineered next. Aarti Shahani, NPR News. AUDIE CORNISH, HOST:  Facebook was once praised for spreading free speech values, but with the company's ban on white extremist content, we have reached an inflection point. The world is pushing back with different values, which are now being imported by Facebook to the U. S. Here's NPR's Aarti Shahani. AARTI SHAHANI, BYLINE: The worldwide ban goes into effect this week, and the move by Facebook, which is wrapped up in the U. S. culture wars, is actually the result of international pressure forcing the company's hand. Facebook says under the new rules, you cannot post in a celebratory way on its news feed or Instagram, I'm a white nationalist, but you can post, I'm a black nationalist. Some Americans find this outrageous or. . . JOHN SPIER: Ridiculous. SHAHANI: John Spier, self-described libertarian Facebook user in central California. SPIER: I feel like everyone should be able to have freedom of expression, even if they're an idiot. There's a lot of idiots in the world who say a lot of stupid things; we don't need to protect people from that. SHAHANI: What racism is, who can be racist - it's a debate that's getting louder in the U. S. Spier believes that, while Facebook claims to be a neutral platform, the company is taking the liberal side. SPIER: I know that the current popular mode of thought is that only white people can be racist, but I don't agree with that. I grew up as a minority white person in a largely Latino community, and believe me, I know what racism feels like. SHAHANI: Facebook is an NPR sponsor. According to Facebook leaders and civil rights advocates, this issue is not about speech but safety. It's a well-documented fact. White extremists around the world are radicalizing men online, luring them into organized hate groups and promoting lone wolf acts of terror. HEIDI BEIRICH: White supremacists are as much a global movement and interconnected - in other words, sharing ideas, sharing money, sharing tactics, sharing propaganda, visiting each other, et cetera, et cetera - just like you see with Islamic extremists. SHAHANI: Heidi Beirich with the Southern Poverty Law Center has been making this point to Facebook for years. Dylann Roof, who murdered nine people in a church basement in Charleston, got radicalized through Google search. The white nationalist rally in Charlottesville that left three dead and dozens injured was organized on a Facebook page. The Christchurch shooter in New Zealand used Facebook Live, an incredibly powerful broadcast tool. Beirich says the company's latest move is in reaction to the disaster in New Zealand and pressure from law enforcement, particularly those in Europe who are worried about white extremist gunmen. BEIRICH: That realization is dawning on the intelligence communities worldwide, and Facebook is hearing it from them. SHAHANI: Facebook made the unusual move of adding Christchurch massacre footage to a terrorism database that had been focused on Islamic extremism. A Facebook spokesperson says the company will continue to add white extremist content to this database which a handful of tech giants share to censor the most violent content. Again, this is in the face of international pressure. Australia just passed a stringent bill that threatens social media employees with prison time if they don't remove violent content expeditiously. The United Kingdom is about to unveil legislation. Germany has passed tough hate speech laws that carry heavy fines. Heidi Beirich. . . BEIRICH: The U. S. is behind the eight ball on this; Trump doesn't seem to be interested in these issues at all. And I think Facebook is reacting to that in a good way, I would argue. SHAHANI: About 90 percent of Facebook users are outside the U. S. Last week, Mark Zuckerberg called on governments around the world to create and effect a global standard for speech. That has never existed before. It's a long shot, and as Zuckerberg sees it, that's what needs to be engineered next. Aarti Shahani, NPR News.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-06-710552590": {"title": "The Global Positioning System Resets : NPR", "url": "https://www.npr.org/2019/04/06/710552590/the-global-positioning-system-resets", "author": "No author found", "published_date": "2019-04-06", "content": "SCOTT SIMON, HOST: It's Y2K for GPS. The Global Positioning System was designed with a limit for the number of weeks it could count. Every 19 years, the program reaches that limit and the count resets. That happens tonight. What might happen tonight? Frank Cilluffo is director of the McCrary Institute for Critical Infrastructure Protection and Cyber Systems. He joins us now from the campus of Auburn University. Thanks so much for being with us. FRANK CILLUFFO: Thank you, Scott. My privilege. SIMON: Will anything happen? CILLUFFO: You know, that's something we don't have a clear picture on. I think by and large, most people are quite confident. No need to panic at this stage. The government has been speaking to industry that are dependent upon GPS for the past two years now. And actually, this is the second time we'll go through a rollover. The first one occurred in 1999. What's different between today and then is our exponential growth and dependence upon GPS. You've got billions of devices now connected to GPS, so obviously, dependence is much greater. SIMON: Most of us obviously encounter it using GPS if we're trying to get somewhere, but I gather it's key to things like communications and the power grid, even finance. CILLUFFO: Absolutely - basically all of our critical infrastructure. It's probably the most significant interdependency to critical functions - time, that is, and precise time. Billions of transactions clear daily, all of which are time-stamped and in all of the global markets. The electric grid is dependent upon GPS. Navigation obviously is as well. It is one of these cross-cutting interdependencies that critical infrastructure is completely reliant upon, not to mention individuals. Anyone who's a parent - I'm sure they've heard, are we there yet, and, what time is it - something I've heard many times from my kids over the years. So it touches pretty much everyone. SIMON: Is the system secure? CILLUFFO: You know, GPS - it's cheap in cost. It's quite efficient, but it is vulnerable. It's vulnerable to jamming. It's also vulnerable to spoofing, where you can insert bad data into the system to cause the system to do something it wasn't intended to do. The system is - handled itself incredibly well. But our dependence has become almost that of a single-point failure that we need to be looking to alternatives to GPS. We need to be investing in new technologies. And this is something the Department of Defense and DARPA in particular have been keeping their eyes on the ball for quite some time 'cause they, too, are concerned that weapons systems and logistics may not work as advertised in the event of a crisis. SIMON: So if something happens, when will we know? CILLUFFO: The actual rollover itself is at 11:59 Greenwich Mean Time, 7:59 p. m. East Coast time. So I think there'd be a clear indicator. If something is askew, you'd find out relatively quickly. But the thing to keep in mind here is modern systems and modern devices, receivers and the like - they're actually moving to a 13 binary bit system. So in other words, right now, the storage space is able to maintain 1,024 weeks. Going forward, hopefully we don't have to worry about this for another 157 years. SIMON: Oh, all right. Well, I'll take those odds. CILLUFFO: There's some good news there. Same here. SIMON: Frank Cilluffo of the McCrary Institute for Critical Infrastructure Protection and Cyber Systems. Thanks so much for being with us, and may all your rollovers be completed. (LAUGHTER)CILLUFFO: Very good. May the Force be with us. Thank you, Scott. SCOTT SIMON, HOST:  It's Y2K for GPS. The Global Positioning System was designed with a limit for the number of weeks it could count. Every 19 years, the program reaches that limit and the count resets. That happens tonight. What might happen tonight? Frank Cilluffo is director of the McCrary Institute for Critical Infrastructure Protection and Cyber Systems. He joins us now from the campus of Auburn University. Thanks so much for being with us. FRANK CILLUFFO: Thank you, Scott. My privilege. SIMON: Will anything happen? CILLUFFO: You know, that's something we don't have a clear picture on. I think by and large, most people are quite confident. No need to panic at this stage. The government has been speaking to industry that are dependent upon GPS for the past two years now. And actually, this is the second time we'll go through a rollover. The first one occurred in 1999. What's different between today and then is our exponential growth and dependence upon GPS. You've got billions of devices now connected to GPS, so obviously, dependence is much greater. SIMON: Most of us obviously encounter it using GPS if we're trying to get somewhere, but I gather it's key to things like communications and the power grid, even finance. CILLUFFO: Absolutely - basically all of our critical infrastructure. It's probably the most significant interdependency to critical functions - time, that is, and precise time. Billions of transactions clear daily, all of which are time-stamped and in all of the global markets. The electric grid is dependent upon GPS. Navigation obviously is as well. It is one of these cross-cutting interdependencies that critical infrastructure is completely reliant upon, not to mention individuals. Anyone who's a parent - I'm sure they've heard, are we there yet, and, what time is it - something I've heard many times from my kids over the years. So it touches pretty much everyone. SIMON: Is the system secure? CILLUFFO: You know, GPS - it's cheap in cost. It's quite efficient, but it is vulnerable. It's vulnerable to jamming. It's also vulnerable to spoofing, where you can insert bad data into the system to cause the system to do something it wasn't intended to do. The system is - handled itself incredibly well. But our dependence has become almost that of a single-point failure that we need to be looking to alternatives to GPS. We need to be investing in new technologies. And this is something the Department of Defense and DARPA in particular have been keeping their eyes on the ball for quite some time 'cause they, too, are concerned that weapons systems and logistics may not work as advertised in the event of a crisis. SIMON: So if something happens, when will we know? CILLUFFO: The actual rollover itself is at 11:59 Greenwich Mean Time, 7:59 p. m. East Coast time. So I think there'd be a clear indicator. If something is askew, you'd find out relatively quickly. But the thing to keep in mind here is modern systems and modern devices, receivers and the like - they're actually moving to a 13 binary bit system. So in other words, right now, the storage space is able to maintain 1,024 weeks. Going forward, hopefully we don't have to worry about this for another 157 years. SIMON: Oh, all right. Well, I'll take those odds. CILLUFFO: There's some good news there. Same here. SIMON: Frank Cilluffo of the McCrary Institute for Critical Infrastructure Protection and Cyber Systems. Thanks so much for being with us, and may all your rollovers be completed. (LAUGHTER) CILLUFFO: Very good. May the Force be with us. Thank you, Scott.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-07-706161006": {"title": "Police Are Investing In New Technology. 'Thin Blue Lie' Asks, 'Does It Work?' : NPR", "url": "https://www.npr.org/2019/04/07/706161006/police-are-investing-in-new-technology-thin-blue-lie-asks-does-it-work", "author": "No author found", "published_date": "2019-04-07", "content": "MICHEL MARTIN, HOST: Tasers, body cameras, facial recognition technology - after a high-profile incident that brings policing practices into question, such as after Michael Brown's death in Ferguson, Mo. , in 2014, you often hear officials or policymakers tout technologies as the solution. Reporter Matt Stroud wanted to know if they really work. He gives his answer to the question in his new book, \"The Thin Blue Lie: The Failure Of High-Tech Policing\" (ph). He joined me from member station WESA in Pittsburgh, and I started by asking him about that title, which includes that very powerful word, lie. And I asked him what made him arrive at the conclusion that these technologies don't do what they're supposed to do. He started with a non-lethal weapon that has been used by police departments since the late 1970s - the Taser. MATT STROUD: They're sold as a way to get police officers to stop using firearms. And in circumstances where police officers feel that they are under lethal threat, they are still going to use firearms. The data has shown that Tasers do not reduce the number of firearms that are used on the streets, and they have been shown to be lethal. It's under much question. But you have somebody who's shocked, and then, shortly thereafter, they go into cardiac arrest - or, during the event itself, they go into cardiac arrest and die. MARTIN: And what about body cameras? I know that's become another discussion point for a number of departments. The argument is that they discipline the interactions of both parties. Both the police are more likely to be more respectful of the public and that the public is more likely to be respectful if they know that they're being filmed. What does the data show on that score? STROUD: The data that I've seen on that score - if you have limited studies - and there are limited studies that have been done by Taser International - they tend to show what you say. But independent studies have shown that they actually don't influence the way that police interact with anybody. When they were initially adopted, the premise was that when you have a controversial police interaction, that body camera footage is made public. And what has transpired as body cameras have been adopted by more and more police departments is that those police departments and government officials who want to appease those police departments have decided, no, we're not going to make that body camera footage public. We are going to withhold it and consider it evidence. So it never gets out. MARTIN: Well, except that it has sometimes shown the truth, hasn't it? Not - I mean, unfortunately. . . STROUD: Absolutely. MARTIN: . . . Didn't prevent a lethal incident, like in the Laquan McDonald case in Chicago. I mean, even though an investigative journalist had to go and demand the footage, it did result in a conviction, didn't it? I mean, it didn't prevent. . . STROUD: Absolutely. MARTIN: . . . A lethal incident, but it did bring accountability after the fact. So. . . STROUD: Sure. I believe that was dashcam footage. MARTIN: Dashcam footage - not the same thing. STROUD: And yeah. Like I said, the video can be used for good. And that's why I think the premise of body camera footage is good. But where problems emerge is when government officials and police officials push back against making that footage public because that's the whole reason. The reason is to bring transparency to police interactions. And if government officials push against that, it goes against the entire premise. MARTIN: One of the very interesting points that you make in your book is that the federal government has investigated alternatives to lethal force and improving police-community relations for decades. I mean, you highlight a report that was commissioned by President Johnson, President Lyndon Johnson in 1967 in response to the Watts riots. And you said that it proposes really not so much an emphasis on technology but an emphasis on relationships, on communication and teaching police officers how to communicate better with the public, how to - it's more of like a - I don't know what word to use. Would you say humanistic approach? And. . . STROUD: Absolutely. That's a great word for it. MARTIN: And that you say that there's - these kinds of - these techniques have never really been implemented because they were deemed to be too expensive. But you also point out that technology is expensive. The police departments spend a huge amount on these technologies that you say don't work. So what - my question to you is, why do you think these techniques have not gotten more traction? STROUD: Because they're complicated. If you had a situation - you brought up Laquan McDonald, right? So that video emerges. People are very upset, as they should be, about the interaction that occurs and the police officers' role - I mean, just a horrendous situation. And then Rahm Emanuel has to come up with something to tell the public. Is Rahm Emanuel's response to that going to be, we're going to completely change the way that police do their job? We're going to invest in social workers. We're going to take a more humanistic approach to the way that police do their job. Or is it easier for him to say, listen - we have a very simple solution that we can institute right now. We're going to spend $5 million and make sure that everybody has a non-lethal weapon on their duty belt, a Taser. And we're going to invest, you know, millions dollars more - millions of dollars more so that every police officer has a body camera, and all of the interactions will be transparent from now on. That is a solution that is concrete, that can happen right now and that he can invest in. He has the budget to do it. And so I think that since you have politicians that are making these decisions, they want to make a quick solution, and they want to make it happen right now. And so that's what they end up doing. And they have done this for decades. That's part of what the book shows, is that they go for the simple solution all the time. MARTIN: You report on the data that demonstrates that these technologies don't actually solve the problem that they are purported to solve. And you say that you had sources within the police departments who were willing to share their own experiences about the effectiveness or lack thereof of these specific technologies. But what I didn't see in the book is reporting on what would be persuadable for these officers. I mean, you pointed out that a lot of the reasons that these technologies stick around is that police officers like them. And I just. . . STROUD: It's true. MARTIN: . . . Wondered if you had any reporting on what would make them not like them. STROUD: One of the main sources that I have - and he's cited in the book - is a gentleman by the name of Matt Masters. He's a SWAT officer, and he didn't start to see Tasers as not a good thing until his son was shot and nearly killed with a Taser. And he as a result of that started doing more digging into the history of Tasers and getting into all of the data that I report on in the book. And he started to understand, and he started to feel empathy for other people. And so he is the optimal - you know, for tragic reasons the optimal example of the kind of police officer who could help to institute this kind of change because he's been through it himself. MARTIN: That's Matt Stroud. His book is \"The Thin Blue Lie: The Failure Of High-Tech Policing\" (ph). And he was with us from WESA in Pittsburgh. Matt Stroud, thanks so much for talking to us. STROUD: Thank you so much for having me. MICHEL MARTIN, HOST:  Tasers, body cameras, facial recognition technology - after a high-profile incident that brings policing practices into question, such as after Michael Brown's death in Ferguson, Mo. , in 2014, you often hear officials or policymakers tout technologies as the solution. Reporter Matt Stroud wanted to know if they really work. He gives his answer to the question in his new book, \"The Thin Blue Lie: The Failure Of High-Tech Policing\" (ph). He joined me from member station WESA in Pittsburgh, and I started by asking him about that title, which includes that very powerful word, lie. And I asked him what made him arrive at the conclusion that these technologies don't do what they're supposed to do. He started with a non-lethal weapon that has been used by police departments since the late 1970s - the Taser. MATT STROUD: They're sold as a way to get police officers to stop using firearms. And in circumstances where police officers feel that they are under lethal threat, they are still going to use firearms. The data has shown that Tasers do not reduce the number of firearms that are used on the streets, and they have been shown to be lethal. It's under much question. But you have somebody who's shocked, and then, shortly thereafter, they go into cardiac arrest - or, during the event itself, they go into cardiac arrest and die. MARTIN: And what about body cameras? I know that's become another discussion point for a number of departments. The argument is that they discipline the interactions of both parties. Both the police are more likely to be more respectful of the public and that the public is more likely to be respectful if they know that they're being filmed. What does the data show on that score? STROUD: The data that I've seen on that score - if you have limited studies - and there are limited studies that have been done by Taser International - they tend to show what you say. But independent studies have shown that they actually don't influence the way that police interact with anybody. When they were initially adopted, the premise was that when you have a controversial police interaction, that body camera footage is made public. And what has transpired as body cameras have been adopted by more and more police departments is that those police departments and government officials who want to appease those police departments have decided, no, we're not going to make that body camera footage public. We are going to withhold it and consider it evidence. So it never gets out. MARTIN: Well, except that it has sometimes shown the truth, hasn't it? Not - I mean, unfortunately. . . STROUD: Absolutely. MARTIN: . . . Didn't prevent a lethal incident, like in the Laquan McDonald case in Chicago. I mean, even though an investigative journalist had to go and demand the footage, it did result in a conviction, didn't it? I mean, it didn't prevent. . . STROUD: Absolutely. MARTIN: . . . A lethal incident, but it did bring accountability after the fact. So. . . STROUD: Sure. I believe that was dashcam footage. MARTIN: Dashcam footage - not the same thing. STROUD: And yeah. Like I said, the video can be used for good. And that's why I think the premise of body camera footage is good. But where problems emerge is when government officials and police officials push back against making that footage public because that's the whole reason. The reason is to bring transparency to police interactions. And if government officials push against that, it goes against the entire premise. MARTIN: One of the very interesting points that you make in your book is that the federal government has investigated alternatives to lethal force and improving police-community relations for decades. I mean, you highlight a report that was commissioned by President Johnson, President Lyndon Johnson in 1967 in response to the Watts riots. And you said that it proposes really not so much an emphasis on technology but an emphasis on relationships, on communication and teaching police officers how to communicate better with the public, how to - it's more of like a - I don't know what word to use. Would you say humanistic approach? And. . . STROUD: Absolutely. That's a great word for it. MARTIN: And that you say that there's - these kinds of - these techniques have never really been implemented because they were deemed to be too expensive. But you also point out that technology is expensive. The police departments spend a huge amount on these technologies that you say don't work. So what - my question to you is, why do you think these techniques have not gotten more traction? STROUD: Because they're complicated. If you had a situation - you brought up Laquan McDonald, right? So that video emerges. People are very upset, as they should be, about the interaction that occurs and the police officers' role - I mean, just a horrendous situation. And then Rahm Emanuel has to come up with something to tell the public. Is Rahm Emanuel's response to that going to be, we're going to completely change the way that police do their job? We're going to invest in social workers. We're going to take a more humanistic approach to the way that police do their job. Or is it easier for him to say, listen - we have a very simple solution that we can institute right now. We're going to spend $5 million and make sure that everybody has a non-lethal weapon on their duty belt, a Taser. And we're going to invest, you know, millions dollars more - millions of dollars more so that every police officer has a body camera, and all of the interactions will be transparent from now on. That is a solution that is concrete, that can happen right now and that he can invest in. He has the budget to do it. And so I think that since you have politicians that are making these decisions, they want to make a quick solution, and they want to make it happen right now. And so that's what they end up doing. And they have done this for decades. That's part of what the book shows, is that they go for the simple solution all the time. MARTIN: You report on the data that demonstrates that these technologies don't actually solve the problem that they are purported to solve. And you say that you had sources within the police departments who were willing to share their own experiences about the effectiveness or lack thereof of these specific technologies. But what I didn't see in the book is reporting on what would be persuadable for these officers. I mean, you pointed out that a lot of the reasons that these technologies stick around is that police officers like them. And I just. . . STROUD: It's true. MARTIN: . . . Wondered if you had any reporting on what would make them not like them. STROUD: One of the main sources that I have - and he's cited in the book - is a gentleman by the name of Matt Masters. He's a SWAT officer, and he didn't start to see Tasers as not a good thing until his son was shot and nearly killed with a Taser. And he as a result of that started doing more digging into the history of Tasers and getting into all of the data that I report on in the book. And he started to understand, and he started to feel empathy for other people. And so he is the optimal - you know, for tragic reasons the optimal example of the kind of police officer who could help to institute this kind of change because he's been through it himself. MARTIN: That's Matt Stroud. His book is \"The Thin Blue Lie: The Failure Of High-Tech Policing\" (ph). And he was with us from WESA in Pittsburgh. Matt Stroud, thanks so much for talking to us. STROUD: Thank you so much for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-07-710825996": {"title": "The Robots Are Here: At George Mason University, They Deliver Food To Students : NPR", "url": "https://www.npr.org/2019/04/07/710825996/the-robots-are-here-at-george-mason-university-they-deliver-food-to-students", "author": "No author found", "published_date": "2019-04-07", "content": "", "section": "Technology", "disclaimer": ""}, "2019-04-07-710781494": {"title": "College Campus Tries Out Robot Delivery : NPR", "url": "https://www.npr.org/2019/04/07/710781494/college-campus-tries-out-robot-delivery", "author": "No author found", "published_date": "2019-04-07", "content": "LULU GARCIA-NAVARRO, HOST: Delivery robots have been popping up in high-end hotels. Now they are making their way to a younger clientele. Patrick Madden of member station WAMU reports from one university in Northern Virginia. PATRICK MADDEN, BYLINE: George Mason University looks like any other big college campus - tall buildings, dorms, green grass and wide sidewalks where streams of students make their way to classes, except here on this campus, there's something not so ordinary - robots. STARSHIP DELIVERY ROBOT: Hello. Here's your delivery. MADDEN: This is one of several dozen Starship food delivery robots here on campus. Picture a cooler on wheels that resembles R2-D2, if you're a \"Star Wars\" fan. This one was kind enough to bring me doughnuts and coffee. STARSHIP DELIVERY ROBOT: Thank you. Have a nice day. MADDEN: For the students at George Mason, these little robots are a trip. GRACE PERREIRA-PLAZA: I think it's pretty cool and kind of adorable to be honest. MADDEN: Twenty-year-old student Grace Perreira-Plaza says, at first, students were obsessed with these little bots, taking pictures, selfies, dressing them up for the holidays. Now. . . PERREIRA-PLAZA: We find it actually a little bit normal now just seeing them go by. It's like, oh, there they are. MADDEN: George Mason University says it's the first college in the U. S. to incorporate robots into its student dining plan. The school is partnering with food-service provider Sodexo for the program. It works like this. Instead of, say, walking to the campus Dunkin' Donuts or the Subway sandwich shop, you can place that order from an app, and the robot will bring it to you anywhere on campus. Mark Kraner, head of the school's retail operations, says, when he was first approached about the idea, he couldn't resist. MARK KRANER: This is going to be a fun ride. We knew the students would just jump all over it. MADDEN: For the robots, perhaps, the biggest hurdle has been crossing the main road that cuts through campus. But these robots are able to adapt and learn. KRANER: Working with their algorithms that if they see a student start across the road, they'll be right behind them. So they are learning. MADDEN: The company behind these robots, Starship Technologies, is planning to roll out these robotic food delivery systems at other schools and corporate campuses. Mark Touhy, an executive at Starship, says the goal of this technology is to save people time. MARK TOUHY: I think, like most technologies, they'll quickly become taken for granted and just a part of our everyday lives. MADDEN: And that's basically what happened at George Mason. The college students don't seem to bat an eye when these things scooter along delivering coffee and doughnuts. STARSHIP DELIVERY ROBOT: I'm a Starship delivery robot. MADDEN: For the rest of us, it might take some time getting used to. For NPR News, I'm Patrick Madden. LULU GARCIA-NAVARRO, HOST:  Delivery robots have been popping up in high-end hotels. Now they are making their way to a younger clientele. Patrick Madden of member station WAMU reports from one university in Northern Virginia. PATRICK MADDEN, BYLINE: George Mason University looks like any other big college campus - tall buildings, dorms, green grass and wide sidewalks where streams of students make their way to classes, except here on this campus, there's something not so ordinary - robots. STARSHIP DELIVERY ROBOT: Hello. Here's your delivery. MADDEN: This is one of several dozen Starship food delivery robots here on campus. Picture a cooler on wheels that resembles R2-D2, if you're a \"Star Wars\" fan. This one was kind enough to bring me doughnuts and coffee. STARSHIP DELIVERY ROBOT: Thank you. Have a nice day. MADDEN: For the students at George Mason, these little robots are a trip. GRACE PERREIRA-PLAZA: I think it's pretty cool and kind of adorable to be honest. MADDEN: Twenty-year-old student Grace Perreira-Plaza says, at first, students were obsessed with these little bots, taking pictures, selfies, dressing them up for the holidays. Now. . . PERREIRA-PLAZA: We find it actually a little bit normal now just seeing them go by. It's like, oh, there they are. MADDEN: George Mason University says it's the first college in the U. S. to incorporate robots into its student dining plan. The school is partnering with food-service provider Sodexo for the program. It works like this. Instead of, say, walking to the campus Dunkin' Donuts or the Subway sandwich shop, you can place that order from an app, and the robot will bring it to you anywhere on campus. Mark Kraner, head of the school's retail operations, says, when he was first approached about the idea, he couldn't resist. MARK KRANER: This is going to be a fun ride. We knew the students would just jump all over it. MADDEN: For the robots, perhaps, the biggest hurdle has been crossing the main road that cuts through campus. But these robots are able to adapt and learn. KRANER: Working with their algorithms that if they see a student start across the road, they'll be right behind them. So they are learning. MADDEN: The company behind these robots, Starship Technologies, is planning to roll out these robotic food delivery systems at other schools and corporate campuses. Mark Touhy, an executive at Starship, says the goal of this technology is to save people time. MARK TOUHY: I think, like most technologies, they'll quickly become taken for granted and just a part of our everyday lives. MADDEN: And that's basically what happened at George Mason. The college students don't seem to bat an eye when these things scooter along delivering coffee and doughnuts. STARSHIP DELIVERY ROBOT: I'm a Starship delivery robot. MADDEN: For the rest of us, it might take some time getting used to. For NPR News, I'm Patrick Madden.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-08-711091689": {"title": "Internet Companies To Face Broad Speech Regulations In U.K. : NPR", "url": "https://www.npr.org/2019/04/08/711091689/u-k-regulators-propose-broad-social-media-regulations-to-counter-online-harms", "author": "No author found", "published_date": "2019-04-08", "content": "", "section": "Technology", "disclaimer": ""}, "2019-04-08-711169802": {"title": "Cornell Professor Ifeoma Ajunwa Discusses Artificial Intelligence Used In Hiring : NPR", "url": "https://www.npr.org/2019/04/08/711169802/cornell-professor-ifeoma-ajunwa-discusses-artificial-intelligence-used-in-hiring", "author": "No author found", "published_date": "2019-04-08", "content": "AILSA CHANG, HOST: All right. Let's take a couple minutes now to examine some of the questions you might have that this Swedish hiring robot poses. Ifeoma Ajunwa of Cornell University has studied the use of artificial intelligence in the hiring process here in the U. S. Welcome. IFEOMA AJUNWA: Thank you very much for having me. CHANG: Is it possible for AI to completely eliminate human bias in the hiring process? AJUNWA: I would say no because you still have to remember that AI isn't fully automated. What we call AI are really machine learning algorithms. And so the people who are creating them do have to be conscious of the ways that human bias could still be encoded in those algorithms. And they would have to make really conscious decisions to eliminate those bias to begin with. CHANG: So we just heard about this, quote, unquote, \"unbiased\" social robot that's going to be used in job interviews in Sweden. Is there anything remotely similar to that being used now here in the U. S. ? AJUNWA: Certainly, yes. There are many hiring algorithms that are now in use in the United States. My co-author and I conducted an informal survey of the top 20 Fortune 500 companies, which are mostly retail companies. And many of those companies require candidates to submit their resumes online, where ostensibly the resume will then pass through some hiring algorithms. And before you think, oh, this is just, you know, relegated to the blue collar market or just the hourly workforce, also, white-shoe companies like Goldman Sachs have also moved to automated hiring. So starting in 2016, Goldman Sachs moved to video interviews, for example, to interview its entering class of summer associates. CHANG: Well, what about legal concerns with using AI for hiring? I mean, would someone even have grounds to sue for, say, discrimination if they didn't get a job and the hiring decision was made by an algorithm? AJUNWA: So that's where it gets more complicated - right? - because a job applicant could suspect that the reason they were refused a job was based on characteristics such as race or gender, and this is certainly prohibited by law. But the problem is how to prove this. So the law requires that you prove either intent to discriminate or you show a pattern of discrimination. Automated hiring platforms actually make it much harder to do either of those. And a lot of times, the algorithms that are part of the hiring system, they are considered proprietary, meaning that they're a trade secret. So you may not actually be able to be privy to exactly how the algorithms were programmed and also to exactly what attributes were considered. So that actually makes it quite difficult for a job applicant. CHANG: Ifeoma Ajunwa teaches employment and labor law at Cornell University. Thank you so much for joining us today. AJUNWA: Thank you so much. AILSA CHANG, HOST:  All right. Let's take a couple minutes now to examine some of the questions you might have that this Swedish hiring robot poses. Ifeoma Ajunwa of Cornell University has studied the use of artificial intelligence in the hiring process here in the U. S. Welcome. IFEOMA AJUNWA: Thank you very much for having me. CHANG: Is it possible for AI to completely eliminate human bias in the hiring process? AJUNWA: I would say no because you still have to remember that AI isn't fully automated. What we call AI are really machine learning algorithms. And so the people who are creating them do have to be conscious of the ways that human bias could still be encoded in those algorithms. And they would have to make really conscious decisions to eliminate those bias to begin with. CHANG: So we just heard about this, quote, unquote, \"unbiased\" social robot that's going to be used in job interviews in Sweden. Is there anything remotely similar to that being used now here in the U. S. ? AJUNWA: Certainly, yes. There are many hiring algorithms that are now in use in the United States. My co-author and I conducted an informal survey of the top 20 Fortune 500 companies, which are mostly retail companies. And many of those companies require candidates to submit their resumes online, where ostensibly the resume will then pass through some hiring algorithms. And before you think, oh, this is just, you know, relegated to the blue collar market or just the hourly workforce, also, white-shoe companies like Goldman Sachs have also moved to automated hiring. So starting in 2016, Goldman Sachs moved to video interviews, for example, to interview its entering class of summer associates. CHANG: Well, what about legal concerns with using AI for hiring? I mean, would someone even have grounds to sue for, say, discrimination if they didn't get a job and the hiring decision was made by an algorithm? AJUNWA: So that's where it gets more complicated - right? - because a job applicant could suspect that the reason they were refused a job was based on characteristics such as race or gender, and this is certainly prohibited by law. But the problem is how to prove this. So the law requires that you prove either intent to discriminate or you show a pattern of discrimination. Automated hiring platforms actually make it much harder to do either of those. And a lot of times, the algorithms that are part of the hiring system, they are considered proprietary, meaning that they're a trade secret. So you may not actually be able to be privy to exactly how the algorithms were programmed and also to exactly what attributes were considered. So that actually makes it quite difficult for a job applicant. CHANG: Ifeoma Ajunwa teaches employment and labor law at Cornell University. Thank you so much for joining us today. AJUNWA: Thank you so much.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-08-711169794": {"title": "Can Artificial Intelligence Make The Hiring Process More Fair?  : NPR", "url": "https://www.npr.org/2019/04/08/711169794/can-artificial-intelligence-make-the-hiring-process-more-fair", "author": "No author found", "published_date": "2019-04-08", "content": "ARI SHAPIRO, HOST: Can artificial intelligence make the hiring process fairer? We look at that question in All Tech Considered. (SOUNDBITE OF MUSIC)SHAPIRO: Lots of Fortune 500 companies use some sort of AI to screen job candidates. In Sweden, recruiters are testing an AI-powered recruitment robot. Reporter Maddy Savage went to check it out. MADDY SAVAGE, BYLINE: I'm outside the offices of TNG, one of Sweden's largest recruiting companies, which has a glass-fronted office in downtown Stockholm. And I'm waiting for Avni Dervishi, a job-seeker who's here to take part in trials of the firm's new robots. AVNI DERVISHI: I have a master's degree in European politics, European affairs. I applied for quite many, many, many jobs. Some interviews I did manage to get, and I know that I did comply with the requirements. SAVAGE: But Dervishi thinks he's missed out on being hired because of discrimination or unconscious bias. He's in his 40s and is originally from Kosovo. DERVISHI: What I do think it's missing is more open-mindedness. Swedish people are cautious. They are careful. SAVAGE: OK. Well, I'm going to leave you. . . DERVISHI: OK. SAVAGE: . . . To try out this robot interview. DERVISHI: All right. SAVAGE: Best of luck. DERVISHI: Thank you. (SOUNDBITE OF DOOR CLOSING)SAVAGE: Sweden has experienced record immigration in recent years. And the unemployment rate amongst people born abroad is around 19 percent, compared to just 4 percent of Swedes. To get some more context, I went for coffee with Matt Kriteman, an American living in Stockholm who works at a nonprofit that campaigns for inclusion in the labor market. MATT KRITEMAN: Sweden is relatively new, comparatively speaking, with other countries that have been multicultural, for example, like, the U. S. , from the beginning. We see more - growing pains is what we call it. SAVAGE: What do you think about the idea of using AI or robots for recruitment? KRITEMAN: We're in Stockholm, the tech startup capital of Europe. So we're really, really interested in this. At the same time, AI is only as good and as diverse as the people who create the algorithm. SAVAGE: That's the programming instructions that determine how the robot will respond. The robot being used by TNG, called Tengai, has been given a female voice and interacts with candidates by means of a talking head. Her face glows, and she can even mimic human facial expressions like blinking and smiling. COMPUTER-GENERATED VOICE: (Speaking Swedish). SAVAGE: At the moment, she can only carry out job interviews in Swedish, but she is learning to speak English. CHARLOTTE ULVROS: My name is Charlotte Ulvros, and I'm the chief marketing and experience officer. Tengai is an unbiased social robot to make sure that we find out who is the best candidate for the job, not anything that's connected to bias, for example, your looks or anything like that. SAVAGE: She says TNG has sought to avoid any potential algorithm bias by ensuring a diverse group of recruiters and test candidates have been involved in training the robot. But most of the developers working on the project are men. ULVROS: There could be, for example, sexist algorithms, but that's when we will use external experts, who will continuously check the code because we're so aware of this issue, and we're trying to do an unbiased product. And we need to keep on top of things. SAVAGE: That's not the only concern. Dr. Malin Lindelow, a psychologist who specializes in recruitment, says there's much more to a job interview than just answering a list of questions. MALIN LINDELOW: We have a lot of areas where there is lack of employees, so actually attracting candidates is a big issue. I'm concerned that it will be an impersonal and not-very-selling experience for the candidate. They need to meet the people they're going to work with in the future. SAVAGE: Back at TNG, Avni Dervishi's just finished his job interview, and he's got mixed feelings. DERVISHI: In the beginning, it felt like, is this sci-fi - science fiction? And at the end, it felt that I would like to see those robots start to be applicable in some working places which lack diversity but not take the jobs of the humans in charge of recruiting people. SAVAGE: TNG plans to start using the robot in real job interviews for May, although it will still use human recruiters for second-round interviews. But her developers say they can see a future where robots like Tengai could eventually make the final call. For NPR News, I'm Maddy Savage in Stockholm. ARI SHAPIRO, HOST:  Can artificial intelligence make the hiring process fairer? We look at that question in All Tech Considered. (SOUNDBITE OF MUSIC) SHAPIRO: Lots of Fortune 500 companies use some sort of AI to screen job candidates. In Sweden, recruiters are testing an AI-powered recruitment robot. Reporter Maddy Savage went to check it out. MADDY SAVAGE, BYLINE: I'm outside the offices of TNG, one of Sweden's largest recruiting companies, which has a glass-fronted office in downtown Stockholm. And I'm waiting for Avni Dervishi, a job-seeker who's here to take part in trials of the firm's new robots. AVNI DERVISHI: I have a master's degree in European politics, European affairs. I applied for quite many, many, many jobs. Some interviews I did manage to get, and I know that I did comply with the requirements. SAVAGE: But Dervishi thinks he's missed out on being hired because of discrimination or unconscious bias. He's in his 40s and is originally from Kosovo. DERVISHI: What I do think it's missing is more open-mindedness. Swedish people are cautious. They are careful. SAVAGE: OK. Well, I'm going to leave you. . . DERVISHI: OK. SAVAGE: . . . To try out this robot interview. DERVISHI: All right. SAVAGE: Best of luck. DERVISHI: Thank you. (SOUNDBITE OF DOOR CLOSING) SAVAGE: Sweden has experienced record immigration in recent years. And the unemployment rate amongst people born abroad is around 19 percent, compared to just 4 percent of Swedes. To get some more context, I went for coffee with Matt Kriteman, an American living in Stockholm who works at a nonprofit that campaigns for inclusion in the labor market. MATT KRITEMAN: Sweden is relatively new, comparatively speaking, with other countries that have been multicultural, for example, like, the U. S. , from the beginning. We see more - growing pains is what we call it. SAVAGE: What do you think about the idea of using AI or robots for recruitment? KRITEMAN: We're in Stockholm, the tech startup capital of Europe. So we're really, really interested in this. At the same time, AI is only as good and as diverse as the people who create the algorithm. SAVAGE: That's the programming instructions that determine how the robot will respond. The robot being used by TNG, called Tengai, has been given a female voice and interacts with candidates by means of a talking head. Her face glows, and she can even mimic human facial expressions like blinking and smiling. COMPUTER-GENERATED VOICE: (Speaking Swedish). SAVAGE: At the moment, she can only carry out job interviews in Swedish, but she is learning to speak English. CHARLOTTE ULVROS: My name is Charlotte Ulvros, and I'm the chief marketing and experience officer. Tengai is an unbiased social robot to make sure that we find out who is the best candidate for the job, not anything that's connected to bias, for example, your looks or anything like that. SAVAGE: She says TNG has sought to avoid any potential algorithm bias by ensuring a diverse group of recruiters and test candidates have been involved in training the robot. But most of the developers working on the project are men. ULVROS: There could be, for example, sexist algorithms, but that's when we will use external experts, who will continuously check the code because we're so aware of this issue, and we're trying to do an unbiased product. And we need to keep on top of things. SAVAGE: That's not the only concern. Dr. Malin Lindelow, a psychologist who specializes in recruitment, says there's much more to a job interview than just answering a list of questions. MALIN LINDELOW: We have a lot of areas where there is lack of employees, so actually attracting candidates is a big issue. I'm concerned that it will be an impersonal and not-very-selling experience for the candidate. They need to meet the people they're going to work with in the future. SAVAGE: Back at TNG, Avni Dervishi's just finished his job interview, and he's got mixed feelings. DERVISHI: In the beginning, it felt like, is this sci-fi - science fiction? And at the end, it felt that I would like to see those robots start to be applicable in some working places which lack diversity but not take the jobs of the humans in charge of recruiting people. SAVAGE: TNG plans to start using the robot in real job interviews for May, although it will still use human recruiters for second-round interviews. But her developers say they can see a future where robots like Tengai could eventually make the final call. For NPR News, I'm Maddy Savage in Stockholm.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-08-707689746": {"title": "Trump's Plan To Zap Incoming Missiles With Lasers Is Back To The Future : NPR", "url": "https://www.npr.org/2019/04/08/707689746/trumps-plan-to-zap-incoming-missiles-with-lasers-is-back-to-the-future", "author": "No author found", "published_date": "2019-04-08", "content": "DAVID GREENE, HOST: OK. Imagine space lasers and particle beams being used to zap incoming missiles. Sounds like something out of \"Star Wars,\" right? Well, studying the use of those things for real is part of the Trump administration's new defense budget. (SOUNDBITE OF ARCHIVED RECORDING)PRESIDENT DONALD TRUMP: Our goal is simple - to ensure that we can detect and destroy any missile launched against the United States anywhere, anytime, anyplace. GREENE: President Trump there, as he laid out his plan for the nation's missile defense system earlier this year. So how realistic is a space-based missile shield? Well, we asked NPR's Geoff Brumfiel. GEOFF BRUMFIEL, BYLINE: Trump's not the first president to suggest space lasers. Back in 1983, Ronald Reagan unveiled a similar vision. (SOUNDBITE OF ARCHIVED RECORDING)RONALD REAGAN: It is part of a careful, long-term plan to make America strong again. BRUMFIEL: Reagan called it the Strategic Defense Initiative. The press called it Star Wars. The program imagined an impenetrable shield that would include orbiting lasers and particle beams to zap Soviet missiles before they could hit their targets. (SOUNDBITE OF ARCHIVED RECORDING)REAGAN: I know this is a formidable, technical task - one that may not be accomplished before the end of this century - yet current technology has attained a level of sophistication where it's reasonable for us to begin this effort. (SOUNDBITE OF ARCHIVED RECORDING)TRUMP: My upcoming budget will invest in a space-based missile defense layer. It's new technology. BRUMFIEL: That second voice was President Trump speaking in January. Except that new technology? On paper, it looks exactly like the old technology Reagan devoted billions to study. JAMES ACTON: It's remarkable how similar all this stuff is. I'm actually not sure it's surprising. BRUMFIEL: James Acton is a physicist at the Carnegie Endowment. ACTON: You know, at the end of the day, the missile defense is a very, very tough problem. And there are a very limited number of ways of solving that problem. BRUMFIEL: It's virtually impossible to make a nationwide missile shield without building it in space. It's only by taking the ultimate high ground that one can defend a target as big as the U. S. Despite many billions, Reagan's Star Wars program never produced that shield. But Rebeccah Heinrichs of the Hudson Institute says it's worth reconsidering. REBECCAH HEINRICHS: It would be negligent on our part not to go back and look at these technologies. BRUMFIEL: Things have changed since the 1980s. Lasers are much smaller and much more powerful. Satellites that once had to be the size of a school bus can be shrunk to the size of a shoe box. HEINRICHS: And we can get launch costs down, which has been one of the biggest cost drivers for the whole thing, is just the cost of launch. BRUMFIEL: Those little satellites, combined with new, cheap commercial rockets, might make a space-based defense program more affordable. Laura Grego is a physicist with the Union of Concerned Scientists, which tracks missile defense. She says, yeah, there's been progress, but much of the technology needed is still so far away. Take particle beams, focused streams of atoms designed to fry a target. Here on Earth, the equipment to generate a powerful beam could be miles long and use as much electricity as a small city. No one's found a way to shrink that technology to satellite size. LAURA GREGO: I don't know why they think this is practical again. BRUMFIEL: And there's another problem. A space-based system is constantly moving around the Earth in orbit. GREGO: A single weapon will almost never be where it's supposed to be for it to work well. So you'd need a constellation of them. And so it becomes really expensive, really quickly. BRUMFIEL: A 2012 study by the U. S. National Academy of Sciences said a space-based defense system would require many hundreds of satellites. It might cost as much as $300 billion. The president's budget is asking for a few hundred-million for R and D. It remains to be seen whether Democrats, who now control the House, will be willing to pay even that. Geoff Brumfiel, NPR News, Washington. DAVID GREENE, HOST:  OK. Imagine space lasers and particle beams being used to zap incoming missiles. Sounds like something out of \"Star Wars,\" right? Well, studying the use of those things for real is part of the Trump administration's new defense budget. (SOUNDBITE OF ARCHIVED RECORDING) PRESIDENT DONALD TRUMP: Our goal is simple - to ensure that we can detect and destroy any missile launched against the United States anywhere, anytime, anyplace. GREENE: President Trump there, as he laid out his plan for the nation's missile defense system earlier this year. So how realistic is a space-based missile shield? Well, we asked NPR's Geoff Brumfiel. GEOFF BRUMFIEL, BYLINE: Trump's not the first president to suggest space lasers. Back in 1983, Ronald Reagan unveiled a similar vision. (SOUNDBITE OF ARCHIVED RECORDING) RONALD REAGAN: It is part of a careful, long-term plan to make America strong again. BRUMFIEL: Reagan called it the Strategic Defense Initiative. The press called it Star Wars. The program imagined an impenetrable shield that would include orbiting lasers and particle beams to zap Soviet missiles before they could hit their targets. (SOUNDBITE OF ARCHIVED RECORDING) REAGAN: I know this is a formidable, technical task - one that may not be accomplished before the end of this century - yet current technology has attained a level of sophistication where it's reasonable for us to begin this effort. (SOUNDBITE OF ARCHIVED RECORDING) TRUMP: My upcoming budget will invest in a space-based missile defense layer. It's new technology. BRUMFIEL: That second voice was President Trump speaking in January. Except that new technology? On paper, it looks exactly like the old technology Reagan devoted billions to study. JAMES ACTON: It's remarkable how similar all this stuff is. I'm actually not sure it's surprising. BRUMFIEL: James Acton is a physicist at the Carnegie Endowment. ACTON: You know, at the end of the day, the missile defense is a very, very tough problem. And there are a very limited number of ways of solving that problem. BRUMFIEL: It's virtually impossible to make a nationwide missile shield without building it in space. It's only by taking the ultimate high ground that one can defend a target as big as the U. S. Despite many billions, Reagan's Star Wars program never produced that shield. But Rebeccah Heinrichs of the Hudson Institute says it's worth reconsidering. REBECCAH HEINRICHS: It would be negligent on our part not to go back and look at these technologies. BRUMFIEL: Things have changed since the 1980s. Lasers are much smaller and much more powerful. Satellites that once had to be the size of a school bus can be shrunk to the size of a shoe box. HEINRICHS: And we can get launch costs down, which has been one of the biggest cost drivers for the whole thing, is just the cost of launch. BRUMFIEL: Those little satellites, combined with new, cheap commercial rockets, might make a space-based defense program more affordable. Laura Grego is a physicist with the Union of Concerned Scientists, which tracks missile defense. She says, yeah, there's been progress, but much of the technology needed is still so far away. Take particle beams, focused streams of atoms designed to fry a target. Here on Earth, the equipment to generate a powerful beam could be miles long and use as much electricity as a small city. No one's found a way to shrink that technology to satellite size. LAURA GREGO: I don't know why they think this is practical again. BRUMFIEL: And there's another problem. A space-based system is constantly moving around the Earth in orbit. GREGO: A single weapon will almost never be where it's supposed to be for it to work well. So you'd need a constellation of them. And so it becomes really expensive, really quickly. BRUMFIEL: A 2012 study by the U. S. National Academy of Sciences said a space-based defense system would require many hundreds of satellites. It might cost as much as $300 billion. The president's budget is asking for a few hundred-million for R and D. It remains to be seen whether Democrats, who now control the House, will be willing to pay even that. Geoff Brumfiel, NPR News, Washington.", "section": "National Security", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-09-711399357": {"title": "Deceased Facebook Users Will Stop Haunting Profiles After Facebook Update : NPR", "url": "https://www.npr.org/2019/04/09/711399357/facebook-promises-to-stop-asking-you-to-wish-happy-birthday-to-your-friend-who-d", "author": "No author found", "published_date": "2019-04-09", "content": "", "section": "Technology", "disclaimer": ""}, "2019-04-10-711951898": {"title": "Advanced Safety Technologies For Cars Come At A Price That Many Won't Pay  : NPR", "url": "https://www.npr.org/2019/04/10/711951898/advanced-safety-technologies-for-cars-come-at-a-price-that-many-wont-pay", "author": "No author found", "published_date": "2019-04-10", "content": "ARI SHAPIRO, HOST: The investigations into the fatal Lion Air and Ethiopian Airlines crashes have revealed troubling details about safety. For example, Boeing was charging extra for a safety feature - one that might've helped the pilots in those flights. Boeing says it will now make that feature standard. We wondered whether the same could be true of safety features in cars. Some new cars have features to avoid collisions or stay in a lane. Depending on the make and model, features like those could come standard or cost extra. David Friedman was acting administrator of the National Highway Traffic Safety Administration, and he's now on the policy side of Consumer Reports. Welcome to the studio. DAVID FRIEDMAN: Thanks a lot for having me, Ari. SHAPIRO: Let's start by talking about a feature called automatic emergency braking. I understand this is a technology that has been really widely adopted, largely because of a voluntary agreement where automakers said they'll make it standard by 2022. What does this do? How important is it? FRIEDMAN: Well, this technology is fantastic. Basically, if you're about to rear-end someone, this technology will first warn you. And if you don't act, it will hit the brakes for you. And the data show that it could reduce rear-end crashes by 40 percent or more. SHAPIRO: Now, I said it will be standard by 2022, but if I buy a new car today, am I likely to have this feature? FRIEDMAN: Honestly, it's 50-50. If we look at 2019 model year data, about half of the automakers sell vehicles where the technology comes standard. Toyota, for example, and Honda have been making a lot of this technology standard on their vehicles. With the others, you may have to pay thousands of dollars to get that technology. SHAPIRO: Is there something troubling about offering a safety feature that can save lives that costs thousands of dollars extra? FRIEDMAN: I mean, to me, safety should never be a luxury, and that's effectively what the car companies are creating in these scenarios, or at least some of them. SHAPIRO: So that's automatic emergency braking. Tell me about another technology that you think might not be standard but perhaps should be. FRIEDMAN: Well, there's a few that we could think of. I mean, first, automatic emergency braking itself is great, but what about adding pedestrian detection so that if you're driving around town, you're much less likely to run into a pedestrian or bicyclist? Or how about if you're driving down the highway and you're about to steer into another lane and you don't know a car's in your blind spot? A blind spot warning system could help save your life or help avoid you running someone else off the road. SHAPIRO: You know, features like air bags and seat belts used to not be in every car. Today, they are. Do you think it's just a matter of time until these kinds of features are standard in every new vehicle? FRIEDMAN: Well, time and again, automakers have developed amazing technology, but they have tried to get consumers to pay a lot for it and refused to put them standard in every car. So usually, regulators have to force them to put them standard in each car. And honestly, that's part of what happened with automatic emergency braking. The federal government was getting ready to require all car companies to equip it, and so the companies, instead, said, hey, let's do this voluntarily. SHAPIRO: So it's voluntary, but voluntary under duress. FRIEDMAN: Exactly. SHAPIRO: Do you see the kind of regulatory pressure that could make automakers standardize these other safety features that you're talking about? FRIEDMAN: Certainly not right now, not under this administration. SHAPIRO: If I'm an automaker and I've invested a huge amount of money in research and development of a safety technology, why shouldn't I try to recoup some of that money by asking buyers to pay a little extra for their car? FRIEDMAN: If it was a little extra, that wouldn't bug me one bit. I mean, you look at Toyota, who has all that same technology. Almost all of it's standard in their base model of the RAV4, a car that costs around $27,000, $28,000. And Toyota's not having a problem making a profit. This is technology that can cost a few hundred dollars at the end of the day. This isn't bank-breaking technology. SHAPIRO: Commercial pilots do get so much more training than anyone who gets behind the wheel of a car. Is there a risk that if you put all of these technologies in place - from automatic breaking to lane change, pedestrian sensor - that drivers start getting lazy? FRIEDMAN: Well, the key to a lot of the technologies that we are talking about is they're proven to have on-road safety benefits. So they're out there in the market, and we can see them saving lives. It's some of the more advanced technologies, where you can truly take your hands off the wheel and your foot off the accelerator and brake - those are the ones that concern us because of the way they're implemented today. Another technology we'd like to start seeing coming standard in cars is a simple driver monitoring system that can tell whether or not you're actually still driving. SHAPIRO: David Friedman is vice president of advocacy for Consumer Reports. Thanks for coming into the studio today. FRIEDMAN: Thank you. ARI SHAPIRO, HOST:  The investigations into the fatal Lion Air and Ethiopian Airlines crashes have revealed troubling details about safety. For example, Boeing was charging extra for a safety feature - one that might've helped the pilots in those flights. Boeing says it will now make that feature standard. We wondered whether the same could be true of safety features in cars. Some new cars have features to avoid collisions or stay in a lane. Depending on the make and model, features like those could come standard or cost extra. David Friedman was acting administrator of the National Highway Traffic Safety Administration, and he's now on the policy side of Consumer Reports. Welcome to the studio. DAVID FRIEDMAN: Thanks a lot for having me, Ari. SHAPIRO: Let's start by talking about a feature called automatic emergency braking. I understand this is a technology that has been really widely adopted, largely because of a voluntary agreement where automakers said they'll make it standard by 2022. What does this do? How important is it? FRIEDMAN: Well, this technology is fantastic. Basically, if you're about to rear-end someone, this technology will first warn you. And if you don't act, it will hit the brakes for you. And the data show that it could reduce rear-end crashes by 40 percent or more. SHAPIRO: Now, I said it will be standard by 2022, but if I buy a new car today, am I likely to have this feature? FRIEDMAN: Honestly, it's 50-50. If we look at 2019 model year data, about half of the automakers sell vehicles where the technology comes standard. Toyota, for example, and Honda have been making a lot of this technology standard on their vehicles. With the others, you may have to pay thousands of dollars to get that technology. SHAPIRO: Is there something troubling about offering a safety feature that can save lives that costs thousands of dollars extra? FRIEDMAN: I mean, to me, safety should never be a luxury, and that's effectively what the car companies are creating in these scenarios, or at least some of them. SHAPIRO: So that's automatic emergency braking. Tell me about another technology that you think might not be standard but perhaps should be. FRIEDMAN: Well, there's a few that we could think of. I mean, first, automatic emergency braking itself is great, but what about adding pedestrian detection so that if you're driving around town, you're much less likely to run into a pedestrian or bicyclist? Or how about if you're driving down the highway and you're about to steer into another lane and you don't know a car's in your blind spot? A blind spot warning system could help save your life or help avoid you running someone else off the road. SHAPIRO: You know, features like air bags and seat belts used to not be in every car. Today, they are. Do you think it's just a matter of time until these kinds of features are standard in every new vehicle? FRIEDMAN: Well, time and again, automakers have developed amazing technology, but they have tried to get consumers to pay a lot for it and refused to put them standard in every car. So usually, regulators have to force them to put them standard in each car. And honestly, that's part of what happened with automatic emergency braking. The federal government was getting ready to require all car companies to equip it, and so the companies, instead, said, hey, let's do this voluntarily. SHAPIRO: So it's voluntary, but voluntary under duress. FRIEDMAN: Exactly. SHAPIRO: Do you see the kind of regulatory pressure that could make automakers standardize these other safety features that you're talking about? FRIEDMAN: Certainly not right now, not under this administration. SHAPIRO: If I'm an automaker and I've invested a huge amount of money in research and development of a safety technology, why shouldn't I try to recoup some of that money by asking buyers to pay a little extra for their car? FRIEDMAN: If it was a little extra, that wouldn't bug me one bit. I mean, you look at Toyota, who has all that same technology. Almost all of it's standard in their base model of the RAV4, a car that costs around $27,000, $28,000. And Toyota's not having a problem making a profit. This is technology that can cost a few hundred dollars at the end of the day. This isn't bank-breaking technology. SHAPIRO: Commercial pilots do get so much more training than anyone who gets behind the wheel of a car. Is there a risk that if you put all of these technologies in place - from automatic breaking to lane change, pedestrian sensor - that drivers start getting lazy? FRIEDMAN: Well, the key to a lot of the technologies that we are talking about is they're proven to have on-road safety benefits. So they're out there in the market, and we can see them saving lives. It's some of the more advanced technologies, where you can truly take your hands off the wheel and your foot off the accelerator and brake - those are the ones that concern us because of the way they're implemented today. Another technology we'd like to start seeing coming standard in cars is a simple driver monitoring system that can tell whether or not you're actually still driving. SHAPIRO: David Friedman is vice president of advocacy for Consumer Reports. Thanks for coming into the studio today. FRIEDMAN: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-10-711819433": {"title": "Oregon Officers Say Burglary Suspect Turned Out To Be Industrious Roomba Vacuum : NPR", "url": "https://www.npr.org/2019/04/10/711819433/oregon-man-called-police-about-a-burglar-armed-officers-found-a-rogue-roomba", "author": "No author found", "published_date": "2019-04-10", "content": "", "section": "Strange News", "disclaimer": ""}, "2019-04-10-709490855": {"title": "GitHub Is Now Where China's Tech Workers Vent Outside Reach Of Censors : NPR", "url": "https://www.npr.org/2019/04/10/709490855/github-has-become-a-haven-for-chinas-censored-internet-users", "author": "No author found", "published_date": "2019-04-10", "content": "DAVID GREENE, HOST: China is pretty successful at scrubbing its Internet of social dissent. But this month, dissent popped up in an unexpected place. GitHub, the world's largest open-source site, where programmers collaborate on code. Chinese tech workers have flooded GitHub with demands for better working conditions. NPR's Emily Feng reports on why this puts Beijing in a tight spot. EMILY FENG, BYLINE: Hal was thrilled to find work at a big Internet company soon after graduation. That is, until he found out about his brutal work schedule, nicknamed 996, working from 9 a. m. to 9 p. m. , six days a week. HAL: So - which means, there is no working-life balance because there was just working, no life. FENG: Like many programmers NPR spoke to, Hal is nervous about retribution from his employer and China's Communist the CCP. He's careful not to speak in Chinese or reveal other identifying features. HAL: I am intentionally hiding my family name in order to get rid of the surveillance of CCP. FENG: There are tens of thousands of programmers who, like Hal, wanted to coordinate a campaign. But Facebook, Twitter and other social media sites are blocked or censored in China. So they turned to GitHub. engineers worldwide use GitHub to share and design code. These programmers use those functions to wage a high-tech labor campaign. They created a GitHub project, 996. ICU based on a joke that a 996 schedule will send you to the intensive care unit. Within days, the project was trending globally as one of GitHub's most popular open-source projects. JAMES GRIFFITHS: GitHub's always presents something of a dilemma. FENG: That's James Griffiths, the author of the recent book, \"The Great Firewall of China. \"GRIFFITHS: The service is kind of so successful, in terms of how developers use it to share code and share software and stuff. If you cut off China from it, it can present genuine problems for developers and for, you know, tech firms. FENG: Griffiths explains how GitHub has always walked a fine line with sensors. In 2015, it was even briefly taken down by Chinese government hackers. GRIFFITHS: But then, access was reinstated. GitHub is, you know, unfortunately from the censors' perspective. GitHub is kind of too useful. It's very, very difficult to block it entirely. FENG: The anti-996 campaign is also a test for Microsoft, which bought GitHub last year. It also owns LinkedIn, which is allowed to operate in China because it is censored. GRIFFITHS: So it remains to see if the government does make a request of Microsoft to take down these projects and to, you know, exercise a level of censorship, or maybe even just to kind of block these projects being viewed in China. FENG: Microsoft is a financial sponsor of NPR. Chinese authorities harshly suppress labor campaigns. For example, some 30 activists and workers are still detained for trying to unionize a factory last summer. But 996 campaigners stress, their campaign is not political. They simply want companies to follow existing labor laws. And they say companies are receptive. SUJI YAN: Yeah, and the response is very radicalized. FENG: That's Suji Yan, a Shanghai-based programmer. He and his wife, Katt Gu, designed software so companies can show they follow labor laws. SUJI: A lot of companies - small, medium companies, they start to put all their work in an anti-996 license to show that, OK, can we are, like, the good company, you know. We respect law and people's life. It's still going very fast. FENG: Unlike factory workers or migrant laborers, China's tech workers are educated and middle class. They hope that, and the international reach of GitHub's online community will force the tech sector's hand before the censors figure out a way to shut them down. Emily Feng, NPR News, Washington. DAVID GREENE, HOST:  China is pretty successful at scrubbing its Internet of social dissent. But this month, dissent popped up in an unexpected place. GitHub, the world's largest open-source site, where programmers collaborate on code. Chinese tech workers have flooded GitHub with demands for better working conditions. NPR's Emily Feng reports on why this puts Beijing in a tight spot. EMILY FENG, BYLINE: Hal was thrilled to find work at a big Internet company soon after graduation. That is, until he found out about his brutal work schedule, nicknamed 996, working from 9 a. m. to 9 p. m. , six days a week. HAL: So - which means, there is no working-life balance because there was just working, no life. FENG: Like many programmers NPR spoke to, Hal is nervous about retribution from his employer and China's Communist the CCP. He's careful not to speak in Chinese or reveal other identifying features. HAL: I am intentionally hiding my family name in order to get rid of the surveillance of CCP. FENG: There are tens of thousands of programmers who, like Hal, wanted to coordinate a campaign. But Facebook, Twitter and other social media sites are blocked or censored in China. So they turned to GitHub. engineers worldwide use GitHub to share and design code. These programmers use those functions to wage a high-tech labor campaign. They created a GitHub project, 996. ICU based on a joke that a 996 schedule will send you to the intensive care unit. Within days, the project was trending globally as one of GitHub's most popular open-source projects. JAMES GRIFFITHS: GitHub's always presents something of a dilemma. FENG: That's James Griffiths, the author of the recent book, \"The Great Firewall of China. \" GRIFFITHS: The service is kind of so successful, in terms of how developers use it to share code and share software and stuff. If you cut off China from it, it can present genuine problems for developers and for, you know, tech firms. FENG: Griffiths explains how GitHub has always walked a fine line with sensors. In 2015, it was even briefly taken down by Chinese government hackers. GRIFFITHS: But then, access was reinstated. GitHub is, you know, unfortunately from the censors' perspective. GitHub is kind of too useful. It's very, very difficult to block it entirely. FENG: The anti-996 campaign is also a test for Microsoft, which bought GitHub last year. It also owns LinkedIn, which is allowed to operate in China because it is censored. GRIFFITHS: So it remains to see if the government does make a request of Microsoft to take down these projects and to, you know, exercise a level of censorship, or maybe even just to kind of block these projects being viewed in China. FENG: Microsoft is a financial sponsor of NPR. Chinese authorities harshly suppress labor campaigns. For example, some 30 activists and workers are still detained for trying to unionize a factory last summer. But 996 campaigners stress, their campaign is not political. They simply want companies to follow existing labor laws. And they say companies are receptive. SUJI YAN: Yeah, and the response is very radicalized. FENG: That's Suji Yan, a Shanghai-based programmer. He and his wife, Katt Gu, designed software so companies can show they follow labor laws. SUJI: A lot of companies - small, medium companies, they start to put all their work in an anti-996 license to show that, OK, can we are, like, the good company, you know. We respect law and people's life. It's still going very fast. FENG: Unlike factory workers or migrant laborers, China's tech workers are educated and middle class. They hope that, and the international reach of GitHub's online community will force the tech sector's hand before the censors figure out a way to shut them down. Emily Feng, NPR News, Washington.", "section": "World", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-10-710985799": {"title": "Buying College Essays Is Now Easier Than Ever. But Buyer Beware  : NPR", "url": "https://www.npr.org/2019/04/10/710985799/contract-cheating-colleges-crack-down-on-ghostwritten-essays", "author": "No author found", "published_date": "2019-04-10", "content": "", "section": "National", "disclaimer": ""}, "2019-04-12-711779130": {"title": "American Businesses Stayed Quiet On Chinese Hackers, Amid Concerns For Profits : NPR", "url": "https://www.npr.org/2019/04/12/711779130/as-china-hacked-u-s-businesses-turned-a-blind-eye", "author": "No author found", "published_date": "2019-04-12", "content": "AILSA CHANG, HOST:  Chinese cyber theft costs the U. S. economy at least $57 billion a year. That's what top government officials tell NPR. And that theft means lost jobs and lost wages for Americans. Three successive administrations have known about this problem and tried to deal with it. Their efforts have been largely unsuccessful. The cyberattacks have continued for nearly 15 years. ARI SHAPIRO, HOST:  NPR and the PBS show \"Frontline\" have been trying to figure out why it's so hard to stop the theft. We found that one of the biggest hurdles isn't China. It's the victims - U. S. businesses. They've kept the U. S. government from acting against China, and they've made millions playing both sides of the fence. NPR's Laura Sullivan reports. (SOUNDBITE OF TRAIN WHISTLE)LAURA SULLIVAN, BYLINE: High above Pittsburgh on the 25th floor of an old Gothic revival building, former U. S. Attorney David Hickton sits at his desk under the photographs of five men who once worked for the Chinese government. DAVID HICKTON: That's the wanted poster for the China case. SULLIVAN: When Hickton took over for the Western District of Pennsylvania in 2010, he'd only been on the job a few weeks when he said he started getting calls from local companies. They told him they thought China might be inside their computer systems. HICKTON: I literally received an avalanche of concern and complaints from companies and organizations who said, we are losing our technology - drip, drip, drip. And we don't have any apparatus in place to deal with it. SULLIVAN: Hickton opened an investigation and set his sights on a particular unit of the Chinese military - Unit 61398. Hickman and others watched how unit officers sitting in an office building in Shanghai broke into American companies' computers at night. They stopped for an hour break at lunch and continued in the afternoon. HICKTON: They really were using a large rake - think of a rake you rake leaves in the fall - they were taking everything. They were taking personal information. They were taking strategic plans. Then they just figured out later how they were going to use it. SULLIVAN: It had been going on for years. HICKTON: When I learned that we could actually pin the tail on the cyber donkey, the cyber donkey being Unit 61398, it just meant that we had a chance to actually bring the case. SULLIVAN: But when he went to the companies, eager for them to be plaintiffs, an odd thing happened. None of them wanted any part of it. The same companies that had been complaining and new companies that had no idea they had been stolen from didn't want to be involved. Hickton says they told him they had too much money on the line in China. Even today, five years later, Hickton still won't name most of the companies involved, and they have never come forward. Eventually, he was able to convince a handful of Pittsburgh-based companies to join the case. Mostly, he says, because he grew up here and went to school with a lot of the managers. How many other companies do you think you could have included in this case? HICKTON: How high can you count? SULLIVAN: No. HICKTON: Yeah. How high can you count? We've made a terrible mistake by being so secret about our cyber work. We have not fairly told the people we represent what the threats are. SULLIVAN: For years, that threat remain largely underground. Government and business leaders say that wasn't an accident. U. S. companies have demanded secrecy, even in the face of outright theft. In interviews with NPR, U. S. companies said they had too much money at stake and said they had a responsibility to shareholders to manage theft problems quietly. But now the impact of that secrecy is coming to light. Companies face hundreds of millions of dollars in future losses by keeping a secret that hand-tied U. S. officials and ultimately failed to hold China to account. It wasn't supposed to be this way. U. S. officials had high hopes when China joined the World Trade Organization in 2001. MICHAEL WESSEL: There was a honeymoon period in the first six or seven years, a desire to try and make things work. SULLIVAN: Michael Wessel has been a commissioner on the U. S. government's U. S. -China Economic and Security Review Commission. He says starting around 2006, businesses began coming to him saying China had stolen their designs or ideas or had pressured them into taking partnerships and taken their technology. But just like with David Hickton, Wessel says they wouldn't come forward. WESSEL: The business community wanted the administration to come in hard without anyone's fingerprints being on the reasoning behind it. They wanted the profits, but they also didn't want the possible retribution. SULLIVAN: Wessel says that was never going to work. The U. S. could have brought criminal cases forward, enacted sanctions or opened investigations if a company would let them. Wendy Cutler was a veteran negotiator at the office that could have done some of that enforcement, the Office of the U. S. Trade Representative. She said it wasn't just that U. S. businesses were hesitant to come forward in specific cases. She says the businesses didn't want them to take action in any cases. WENDY CUTLER: U. S. enforcement officials were not as effective if we don't really have the U. S. business community supporting us but also providing us the information. You know, looking back on it, in retrospect, I think we probably should have been more active and more responsive. We kind of lost the big picture of what really was happening. SULLIVAN: Court cases and documents from recent years offer a clue into what experts believe was happening. The Chinese government has been accused of stealing everything from vacuum cleaner designs to solar panel technology, to the designs of Boeing's C-17 aircraft. China has broken into gas companies, steel companies and chemical companies. Not long ago, Chinese government companies were indicted for dealing the secret chemical makeup of the color white from DuPont. Chinese hacking made occasional headlines, but none really grabbed Americans' attention until January 2010. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED JOURNALIST #1: Finally tonight, Google's China threat. SULLIVAN: . . . When an American company did come forward. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED JOURNALIST #2: The world's leading search company announced yesterday it had discovered what it called a highly sophisticated and targeted attack. SULLIVAN: . . . Google decided to do what no other company had done. They announced the theft and publicly blamed the Chinese government. Thirty-four other well-known American companies had also been hacked, But to this day, most have kept it a secret. NPR tracked down 11 of the companies - none of them would comment on the hack. James McGregor is the former chairman of the American Chamber of Commerce in China and was there at the time. He says the companies even kept the business organizations from speaking out. JAMES MCGREGOR: What they should have done is held a press conference and said, we 35 businesses have been hacked, and you would put it right back on China. Instead, they just all hid under a rock and pretended it didn't happen. SULLIVAN: McGregor says their silence left little room for punishment. And worse, he said, it hid the extent of the problem. Dmitri Alperovitch was one of the first to see it. He was working at a security firm in Atlanta during the Google hack. One afternoon, Google called, said they needed backup. Alperovitch is a well-known cyber sleuth. He says when he took a look, he was stunned. DMITRI ALPEROVITCH: I knew pretty much right away that this is something very different. For the first time ever, we were facing a nation state, an intelligence service that was breaking into companies - not governments, not militaries, but private sector organizations. SULLIVAN: Where was the U. S. government on all of this? ALPEROVITCH: The U. S. government was nowhere to be seen. SULLIVAN: Evan Medeiros was on staff at the National Security Council at the time and a top China specialist under President Obama. He says they didn't turn a blind eye. Obama signed an agreement with China to address the hacking. But he says the administration also had other priorities - North Korea, Iran, the economy, climate change. EVAN MEDEIROS: Direct confrontation with China does not usually result in lasting solutions. SULLIVAN: It seems like not confronting China did not provide any solutions either. MEDEIROS: I mean, if you want big structural change, it takes time. The question is, do you want to play checkers? Do you want to play chess? SULLIVAN: But without repercussions, the attacks continued. In the year after the Google hack, Dmitri Alperovitch uncovered two more serious intrusions into American companies. In the fall of 2011, he went to the White House to warn officials about what he had found. He sat down in the Situation Room with half a dozen top administration leaders. ALPEROVITCH: I got the distinct impression that none of this was news. And when I pressed them on why they were not taking stronger action against China, their response was it's complicated. SULLIVAN: Did they explain that? ALPEROVITCH: Yes. Essentially, the answer was, we have a multifaceted relationship with China. Some of those same companies that were being victimized by China also wanted to continue doing business in China. SULLIVAN: So I asked James McGregor, the American business representative from China. How can businesses walk into United States agencies and complain about being treated unfairly if they're the ones that are preventing any action from being taken? MCGREGOR: Well, sometimes two things can be true at the same time. Companies were afraid of China. There's this whole Tony Soprano side of the party that will come in and teach you a lesson and you better not complain. American business companies did what they - their incentives are to make money, you know. SULLIVAN: How's that working out for them? MCGREGOR: Well, it's - let's just say they're now facing - they're woke, you know. SULLIVAN: Today, McGregor advises dozens of companies doing business in China and says that awakening has meant acknowledging they've got a problem. China's no longer an up-and-comer. It's a true competitor closing in on America's high-tech sector. And officials are beginning to ask whether years of theft and hacking have given China an edge the United States will no longer be able to stay in front of or whether U. S. government agencies will be able to catch up on enforcement. Top government officials told NPR federal agencies are years behind where they could have been if the theft had been openly addressed. Even at the Pentagon, as late as 2014, cyber theft from China was not one of the department's top priorities. ROBERT SPALDING: Our intelligence agencies were looking at the Middle East. They were looking at the Russians. We had more Pashto Urdu speakers than we had Chinese speakers in the intelligence apparatus. SULLIVAN: Air Force Brigadier General Robert Spalding worked for the Joint Chiefs of Staff and was a China expert for the National Security Council. He had never given the issue much thought. But in the fall of 2014, he loaded a confidential briefing into his computer. It was case after case where the Chinese government had stolen the product designs from almost a dozen high-tech American companies. SPALDING: It immediately changed my conception, my view of the world. I realized I did not know how the world worked. SULLIVAN: Spalding says he made it his mission to get the word out to other government agencies. But even in 2015, he says he was met mostly with a shrug. SPALDING: We went to Commerce, and we went to Treasury and U. S. Trade Representative and State Department. The two responses we got were, oh, my gosh, this is really, really bad. And the second one is, that's not my job. And that was almost the universal answer that we got. Every time we went to a senior leader - bad problem, but not my problem. SULLIVAN: Spalding, who retired from the Air Force last year, says in the final years under President Obama and now under President Trump, agencies are finally starting to take some action. The Justice Department is bringing criminal cases. The trade office is investigating China's dealings. And both administrations have brought concerns to the Chinese directly. But Spalding says it may have come 10 years too late. SPALDING: We all missed it. We have to understand the problem and then get to work on it. SULLIVAN: Today, the Trump administration is wielding billions in trade tariffs to bring China to the table on a host of issues and hopes to negotiate an end to cyber theft as part of any trade agreement. But it may be years before the American public has a full understanding of what it has lost. Laura Sullivan, NPR News. AILSA CHANG, HOST:   Chinese cyber theft costs the U. S. economy at least $57 billion a year. That's what top government officials tell NPR. And that theft means lost jobs and lost wages for Americans. Three successive administrations have known about this problem and tried to deal with it. Their efforts have been largely unsuccessful. The cyberattacks have continued for nearly 15 years. ARI SHAPIRO, HOST:   NPR and the PBS show \"Frontline\" have been trying to figure out why it's so hard to stop the theft. We found that one of the biggest hurdles isn't China. It's the victims - U. S. businesses. They've kept the U. S. government from acting against China, and they've made millions playing both sides of the fence. NPR's Laura Sullivan reports. (SOUNDBITE OF TRAIN WHISTLE) LAURA SULLIVAN, BYLINE: High above Pittsburgh on the 25th floor of an old Gothic revival building, former U. S. Attorney David Hickton sits at his desk under the photographs of five men who once worked for the Chinese government. DAVID HICKTON: That's the wanted poster for the China case. SULLIVAN: When Hickton took over for the Western District of Pennsylvania in 2010, he'd only been on the job a few weeks when he said he started getting calls from local companies. They told him they thought China might be inside their computer systems. HICKTON: I literally received an avalanche of concern and complaints from companies and organizations who said, we are losing our technology - drip, drip, drip. And we don't have any apparatus in place to deal with it. SULLIVAN: Hickton opened an investigation and set his sights on a particular unit of the Chinese military - Unit 61398. Hickman and others watched how unit officers sitting in an office building in Shanghai broke into American companies' computers at night. They stopped for an hour break at lunch and continued in the afternoon. HICKTON: They really were using a large rake - think of a rake you rake leaves in the fall - they were taking everything. They were taking personal information. They were taking strategic plans. Then they just figured out later how they were going to use it. SULLIVAN: It had been going on for years. HICKTON: When I learned that we could actually pin the tail on the cyber donkey, the cyber donkey being Unit 61398, it just meant that we had a chance to actually bring the case. SULLIVAN: But when he went to the companies, eager for them to be plaintiffs, an odd thing happened. None of them wanted any part of it. The same companies that had been complaining and new companies that had no idea they had been stolen from didn't want to be involved. Hickton says they told him they had too much money on the line in China. Even today, five years later, Hickton still won't name most of the companies involved, and they have never come forward. Eventually, he was able to convince a handful of Pittsburgh-based companies to join the case. Mostly, he says, because he grew up here and went to school with a lot of the managers. How many other companies do you think you could have included in this case? HICKTON: How high can you count? SULLIVAN: No. HICKTON: Yeah. How high can you count? We've made a terrible mistake by being so secret about our cyber work. We have not fairly told the people we represent what the threats are. SULLIVAN: For years, that threat remain largely underground. Government and business leaders say that wasn't an accident. U. S. companies have demanded secrecy, even in the face of outright theft. In interviews with NPR, U. S. companies said they had too much money at stake and said they had a responsibility to shareholders to manage theft problems quietly. But now the impact of that secrecy is coming to light. Companies face hundreds of millions of dollars in future losses by keeping a secret that hand-tied U. S. officials and ultimately failed to hold China to account. It wasn't supposed to be this way. U. S. officials had high hopes when China joined the World Trade Organization in 2001. MICHAEL WESSEL: There was a honeymoon period in the first six or seven years, a desire to try and make things work. SULLIVAN: Michael Wessel has been a commissioner on the U. S. government's U. S. -China Economic and Security Review Commission. He says starting around 2006, businesses began coming to him saying China had stolen their designs or ideas or had pressured them into taking partnerships and taken their technology. But just like with David Hickton, Wessel says they wouldn't come forward. WESSEL: The business community wanted the administration to come in hard without anyone's fingerprints being on the reasoning behind it. They wanted the profits, but they also didn't want the possible retribution. SULLIVAN: Wessel says that was never going to work. The U. S. could have brought criminal cases forward, enacted sanctions or opened investigations if a company would let them. Wendy Cutler was a veteran negotiator at the office that could have done some of that enforcement, the Office of the U. S. Trade Representative. She said it wasn't just that U. S. businesses were hesitant to come forward in specific cases. She says the businesses didn't want them to take action in any cases. WENDY CUTLER: U. S. enforcement officials were not as effective if we don't really have the U. S. business community supporting us but also providing us the information. You know, looking back on it, in retrospect, I think we probably should have been more active and more responsive. We kind of lost the big picture of what really was happening. SULLIVAN: Court cases and documents from recent years offer a clue into what experts believe was happening. The Chinese government has been accused of stealing everything from vacuum cleaner designs to solar panel technology, to the designs of Boeing's C-17 aircraft. China has broken into gas companies, steel companies and chemical companies. Not long ago, Chinese government companies were indicted for dealing the secret chemical makeup of the color white from DuPont. Chinese hacking made occasional headlines, but none really grabbed Americans' attention until January 2010. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED JOURNALIST #1: Finally tonight, Google's China threat. SULLIVAN: . . . When an American company did come forward. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED JOURNALIST #2: The world's leading search company announced yesterday it had discovered what it called a highly sophisticated and targeted attack. SULLIVAN: . . . Google decided to do what no other company had done. They announced the theft and publicly blamed the Chinese government. Thirty-four other well-known American companies had also been hacked, But to this day, most have kept it a secret. NPR tracked down 11 of the companies - none of them would comment on the hack. James McGregor is the former chairman of the American Chamber of Commerce in China and was there at the time. He says the companies even kept the business organizations from speaking out. JAMES MCGREGOR: What they should have done is held a press conference and said, we 35 businesses have been hacked, and you would put it right back on China. Instead, they just all hid under a rock and pretended it didn't happen. SULLIVAN: McGregor says their silence left little room for punishment. And worse, he said, it hid the extent of the problem. Dmitri Alperovitch was one of the first to see it. He was working at a security firm in Atlanta during the Google hack. One afternoon, Google called, said they needed backup. Alperovitch is a well-known cyber sleuth. He says when he took a look, he was stunned. DMITRI ALPEROVITCH: I knew pretty much right away that this is something very different. For the first time ever, we were facing a nation state, an intelligence service that was breaking into companies - not governments, not militaries, but private sector organizations. SULLIVAN: Where was the U. S. government on all of this? ALPEROVITCH: The U. S. government was nowhere to be seen. SULLIVAN: Evan Medeiros was on staff at the National Security Council at the time and a top China specialist under President Obama. He says they didn't turn a blind eye. Obama signed an agreement with China to address the hacking. But he says the administration also had other priorities - North Korea, Iran, the economy, climate change. EVAN MEDEIROS: Direct confrontation with China does not usually result in lasting solutions. SULLIVAN: It seems like not confronting China did not provide any solutions either. MEDEIROS: I mean, if you want big structural change, it takes time. The question is, do you want to play checkers? Do you want to play chess? SULLIVAN: But without repercussions, the attacks continued. In the year after the Google hack, Dmitri Alperovitch uncovered two more serious intrusions into American companies. In the fall of 2011, he went to the White House to warn officials about what he had found. He sat down in the Situation Room with half a dozen top administration leaders. ALPEROVITCH: I got the distinct impression that none of this was news. And when I pressed them on why they were not taking stronger action against China, their response was it's complicated. SULLIVAN: Did they explain that? ALPEROVITCH: Yes. Essentially, the answer was, we have a multifaceted relationship with China. Some of those same companies that were being victimized by China also wanted to continue doing business in China. SULLIVAN: So I asked James McGregor, the American business representative from China. How can businesses walk into United States agencies and complain about being treated unfairly if they're the ones that are preventing any action from being taken? MCGREGOR: Well, sometimes two things can be true at the same time. Companies were afraid of China. There's this whole Tony Soprano side of the party that will come in and teach you a lesson and you better not complain. American business companies did what they - their incentives are to make money, you know. SULLIVAN: How's that working out for them? MCGREGOR: Well, it's - let's just say they're now facing - they're woke, you know. SULLIVAN: Today, McGregor advises dozens of companies doing business in China and says that awakening has meant acknowledging they've got a problem. China's no longer an up-and-comer. It's a true competitor closing in on America's high-tech sector. And officials are beginning to ask whether years of theft and hacking have given China an edge the United States will no longer be able to stay in front of or whether U. S. government agencies will be able to catch up on enforcement. Top government officials told NPR federal agencies are years behind where they could have been if the theft had been openly addressed. Even at the Pentagon, as late as 2014, cyber theft from China was not one of the department's top priorities. ROBERT SPALDING: Our intelligence agencies were looking at the Middle East. They were looking at the Russians. We had more Pashto Urdu speakers than we had Chinese speakers in the intelligence apparatus. SULLIVAN: Air Force Brigadier General Robert Spalding worked for the Joint Chiefs of Staff and was a China expert for the National Security Council. He had never given the issue much thought. But in the fall of 2014, he loaded a confidential briefing into his computer. It was case after case where the Chinese government had stolen the product designs from almost a dozen high-tech American companies. SPALDING: It immediately changed my conception, my view of the world. I realized I did not know how the world worked. SULLIVAN: Spalding says he made it his mission to get the word out to other government agencies. But even in 2015, he says he was met mostly with a shrug. SPALDING: We went to Commerce, and we went to Treasury and U. S. Trade Representative and State Department. The two responses we got were, oh, my gosh, this is really, really bad. And the second one is, that's not my job. And that was almost the universal answer that we got. Every time we went to a senior leader - bad problem, but not my problem. SULLIVAN: Spalding, who retired from the Air Force last year, says in the final years under President Obama and now under President Trump, agencies are finally starting to take some action. The Justice Department is bringing criminal cases. The trade office is investigating China's dealings. And both administrations have brought concerns to the Chinese directly. But Spalding says it may have come 10 years too late. SPALDING: We all missed it. We have to understand the problem and then get to work on it. SULLIVAN: Today, the Trump administration is wielding billions in trade tariffs to bring China to the table on a host of issues and hopes to negotiate an end to cyber theft as part of any trade agreement. But it may be years before the American public has a full understanding of what it has lost. Laura Sullivan, NPR News.", "section": "Investigations", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-12-711195968": {"title": "Jeremy Heimans: How Can We Harness Technology To Fuel Social Change? : NPR", "url": "https://www.npr.org/2019/04/12/711195968/jeremy-heimans-how-can-we-harness-technology-to-fuel-social-change", "author": "No author found", "published_date": "2019-04-12", "content": "GUY RAZ, HOST: So when many of us think of the word activist, we think of participating in a grassroots movement, going out on the streets. Is that, like, a good working definition of what it means to be an activist? JEREMY HEIMANS: Well, look; I think the basic impulse is the same, right? It's people binding together in pursuit of some goal, challenging power in some way. RAZ: Yeah. HEIMANS: But the way that plays out in the 21st century does look pretty different. And the repertoire of activism, I think, is broader. RAZ: This is Jeremy Heimans. He works to mobilize activists around the world. HEIMANS: Yeah. I've been an activist really all my life. I started as a child activist back in Australia, where I grew up. RAZ: In fact, in the early '90s, when Jeremy was 12, he tried to stop a war with a fax machine. Jeremy Heimans picks up his story from the TED stage. (SOUNDBITE OF TED TALK)HEIMANS: It was the eve of the Gulf War. And I organized a global campaign to flood the hotel - the Intercontinental in Geneva - where James Baker and Tariq Aziz were meeting on the eve of the war. And I thought, if I could flood them with faxes, we'll stop the war. Well, unsurprisingly, that campaign was wholly unsuccessful. You know, and there are lots of reasons for that, but there's no doubt that one sputtering fax machine in Geneva was a little bit of a bandwidth constraint in terms of the ability to get a message to lots of people. And so I went on to discover some better tools. I co-founded Avaaz, which uses the Internet to mobilize people and now has almost 40 million members. And I now run Purpose, which is a home for these kinds of technology-powered movements. So what's the moral of this story? Is the moral of the story, you know what? The fax is kind of eclipsed by the mobile phone. This is another story of tech determinism. Well, I would argue that there's actually more to it than that. I'd argue that in the last 20 years, something more fundamental has changed than just new tech, that there has been a fundamental shift in the balance of power in the world. (SOUNDBITE OF MUSIC)RAZ: So in your talk, you introduce this idea of new power. What is it? HEIMANS: So we think of new power as this kind of critical method, this critical mindset that you need in the 21st century. And that is this ability to harness the energy of these connected crowds that are all around us. So the metaphor that we use - we contrast old power and new power. Now, old power is power as currency. It's the kind of power that you can hoard up. So the more of it that you have, the more powerful you are. You use that power. You spend it to maintain your position. But new power works differently. It isn't the kind of power you can hoard up. It's power as a current. What we mean by current is, like water or electricity, it's most powerful when it surges. (SOUNDBITE OF MUSIC)HEIMANS: It's most powerful when people are participating. And the more people participate, the stronger the current gets. (SOUNDBITE OF MUSIC)HEIMANS: And so that's how we think about the difference between old power and new power, right? So in a world where everybody's connected, where everybody can spread ideas, can mobilize communities and followers very quickly, the realm of digital activism is a whole new space that's opened up in the last 20 years that has enabled a series of different kinds of movements to emerge. RAZ: So - like what? HEIMANS: So you think about the #MeToo movement. I think it's a great example of the new kinds of movements that you see in this kind of new-power world, right? So to come back to that metaphor of new power works like a current, with the #MeToo movement, you get this incredible surge of energy that kind of, more or less, comes from nowhere. So Tarana Burke had been seeding this idea for a decade. But then all of a sudden, it catches fire. And the way that it does - many people take that energy. They adapt it and make it their own. So in France, the #MeToo movement becomes Denounce Your Pig - much more French, right? - Balance Ton Porc. In Brazil, it becomes My First Assault because the problem is so prolific there. And the structure of these movements is different. The way people participate in them is different. The speed, the scale, the density of participation is unprecedented in a movement like that. (SOUNDBITE OF TED TALK)HEIMANS: What's interesting about new power is the way it feeds on itself. Once you have an experience of new power, you tend to expect and want more of it. So let's say you've used a peer-to-peer lending platform like LendingTree or Prosper. Then you've figured out that you don't need the bank. And who wants the bank, right? And so that experience tends to embolden you. It tends to want - make you want more participation across more aspects of your life. And what this gives rise to is a set of values. (SOUNDBITE OF MUSIC)RAZ: So if you were thinking about something like the civil rights movement and you were to sort of say, OK, this is what it would look like today under a new power structure, what would it be? HEIMANS: I think you'd - you could look at Black Lives Matter. The founders were women. Two of them were queer women. And they had a very particular perspective about how to lead in a movement. They felt that if they made the movement all about them and they didn't create a context in which many leaders could emerge in a decentralized way, that the power of the movement would be limited. And I think that was very effective for creating a lot of energy around criminal justice, police brutality. That kind of movement can be less effective in pushing very specific policy outcomes. And that's where you kind of need almost, like, a relay between old power and new power, where new power creates the energy, creates a lot of decentralized activity, spreads an idea, and then old power institutions can sort of help push that into, for example, a state House legislature, where you've got to do sort of particular kinds of gritty work in order to get a particular bill passed. The most effective movements today are combining old power and new power. Now, the NRA is a great example of this, right? It's got a brilliant, old-power strategy. You know, it's got a fearsome brand. They project this power. They project this ability to change an election. And at the same time, they're very good at new power - at kind of releasing control, cultivating the energy of their supporters. And those supporters go far beyond the people who pay dues to the NRA. And what they do is they, basically, cultivate that energy. They fund little blogs and gun clubs and local activists. And then they, essentially, see the stuff that's bubbling up, the stuff that's taking off. And then they bring some of their old power might and resources in, and they really amplify. (SOUNDBITE OF MUSIC)RAZ: So you're talking about really big movements, right? But what about, like, on a smaller scale? - because it's almost a truism that the squeaky wheel is always going to get some grease, right? But in the past, to be a squeaky wheel, you had to show up. You to be a pain in someone's butt, right? HEIMANS: Yeah. RAZ: And today if you want to be a squeaky wheel, it's not - it's, like, not that hard to mobilize people quickly, to irritate somebody to the point where, you know, a politician or a journalist or somebody in the public eye is going to respond and react as a result of your complaints. HEIMANS: Exactly. And companies and organizations still haven't quite learned how to respond to these huge kind of currents of new power. Institutions are good at dealing with other institutions. They're not very good at dealing with movements. And I think you're also right that anyone can be a squeaky wheel. I mean, I think one of the fascinating things - I mean, if you're a kid today, you know, what you're learning every day are these skills of mobilization. You know, every kid has followers. Every kid is thinking, in a way, about how to build community around the content that they produce. So that's why it's so much easier now for anyone to take that up. And that, unfortunately, is also why it's easier for extreme ideas to spread. But, you know, it also gives me a lot of hope because, you know, kids today - and we saw this wonderfully with the Parkland kids - are using these skills to fight for justice. Digital activism is an entree to those more-committed forms of activism. (SOUNDBITE OF MUSIC)HEIMANS: I think that the version of activism that we have, maybe, in our heads, the version that, maybe, we tell stories about in our films, is an incredibly important form of activism. But it's not the only form of activism that matters. And it's not the only form of activism that has brought about change. So we need all these kinds of participation. (SOUNDBITE OF MUSIC)RAZ: That's Jeremy Heimans, co-author of \"New Power. \" He's also the CEO of Purpose. It's an organization that helps build and support movements around the world. You can see his full talk at ted. npr. org. On the show today, ideas about Changing The World. Stay with us. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC) GUY RAZ, HOST:  So when many of us think of the word activist, we think of participating in a grassroots movement, going out on the streets. Is that, like, a good working definition of what it means to be an activist? JEREMY HEIMANS: Well, look; I think the basic impulse is the same, right? It's people binding together in pursuit of some goal, challenging power in some way. RAZ: Yeah. HEIMANS: But the way that plays out in the 21st century does look pretty different. And the repertoire of activism, I think, is broader. RAZ: This is Jeremy Heimans. He works to mobilize activists around the world. HEIMANS: Yeah. I've been an activist really all my life. I started as a child activist back in Australia, where I grew up. RAZ: In fact, in the early '90s, when Jeremy was 12, he tried to stop a war with a fax machine. Jeremy Heimans picks up his story from the TED stage. (SOUNDBITE OF TED TALK) HEIMANS: It was the eve of the Gulf War. And I organized a global campaign to flood the hotel - the Intercontinental in Geneva - where James Baker and Tariq Aziz were meeting on the eve of the war. And I thought, if I could flood them with faxes, we'll stop the war. Well, unsurprisingly, that campaign was wholly unsuccessful. You know, and there are lots of reasons for that, but there's no doubt that one sputtering fax machine in Geneva was a little bit of a bandwidth constraint in terms of the ability to get a message to lots of people. And so I went on to discover some better tools. I co-founded Avaaz, which uses the Internet to mobilize people and now has almost 40 million members. And I now run Purpose, which is a home for these kinds of technology-powered movements. So what's the moral of this story? Is the moral of the story, you know what? The fax is kind of eclipsed by the mobile phone. This is another story of tech determinism. Well, I would argue that there's actually more to it than that. I'd argue that in the last 20 years, something more fundamental has changed than just new tech, that there has been a fundamental shift in the balance of power in the world. (SOUNDBITE OF MUSIC) RAZ: So in your talk, you introduce this idea of new power. What is it? HEIMANS: So we think of new power as this kind of critical method, this critical mindset that you need in the 21st century. And that is this ability to harness the energy of these connected crowds that are all around us. So the metaphor that we use - we contrast old power and new power. Now, old power is power as currency. It's the kind of power that you can hoard up. So the more of it that you have, the more powerful you are. You use that power. You spend it to maintain your position. But new power works differently. It isn't the kind of power you can hoard up. It's power as a current. What we mean by current is, like water or electricity, it's most powerful when it surges. (SOUNDBITE OF MUSIC) HEIMANS: It's most powerful when people are participating. And the more people participate, the stronger the current gets. (SOUNDBITE OF MUSIC) HEIMANS: And so that's how we think about the difference between old power and new power, right? So in a world where everybody's connected, where everybody can spread ideas, can mobilize communities and followers very quickly, the realm of digital activism is a whole new space that's opened up in the last 20 years that has enabled a series of different kinds of movements to emerge. RAZ: So - like what? HEIMANS: So you think about the #MeToo movement. I think it's a great example of the new kinds of movements that you see in this kind of new-power world, right? So to come back to that metaphor of new power works like a current, with the #MeToo movement, you get this incredible surge of energy that kind of, more or less, comes from nowhere. So Tarana Burke had been seeding this idea for a decade. But then all of a sudden, it catches fire. And the way that it does - many people take that energy. They adapt it and make it their own. So in France, the #MeToo movement becomes Denounce Your Pig - much more French, right? - Balance Ton Porc. In Brazil, it becomes My First Assault because the problem is so prolific there. And the structure of these movements is different. The way people participate in them is different. The speed, the scale, the density of participation is unprecedented in a movement like that. (SOUNDBITE OF TED TALK) HEIMANS: What's interesting about new power is the way it feeds on itself. Once you have an experience of new power, you tend to expect and want more of it. So let's say you've used a peer-to-peer lending platform like LendingTree or Prosper. Then you've figured out that you don't need the bank. And who wants the bank, right? And so that experience tends to embolden you. It tends to want - make you want more participation across more aspects of your life. And what this gives rise to is a set of values. (SOUNDBITE OF MUSIC) RAZ: So if you were thinking about something like the civil rights movement and you were to sort of say, OK, this is what it would look like today under a new power structure, what would it be? HEIMANS: I think you'd - you could look at Black Lives Matter. The founders were women. Two of them were queer women. And they had a very particular perspective about how to lead in a movement. They felt that if they made the movement all about them and they didn't create a context in which many leaders could emerge in a decentralized way, that the power of the movement would be limited. And I think that was very effective for creating a lot of energy around criminal justice, police brutality. That kind of movement can be less effective in pushing very specific policy outcomes. And that's where you kind of need almost, like, a relay between old power and new power, where new power creates the energy, creates a lot of decentralized activity, spreads an idea, and then old power institutions can sort of help push that into, for example, a state House legislature, where you've got to do sort of particular kinds of gritty work in order to get a particular bill passed. The most effective movements today are combining old power and new power. Now, the NRA is a great example of this, right? It's got a brilliant, old-power strategy. You know, it's got a fearsome brand. They project this power. They project this ability to change an election. And at the same time, they're very good at new power - at kind of releasing control, cultivating the energy of their supporters. And those supporters go far beyond the people who pay dues to the NRA. And what they do is they, basically, cultivate that energy. They fund little blogs and gun clubs and local activists. And then they, essentially, see the stuff that's bubbling up, the stuff that's taking off. And then they bring some of their old power might and resources in, and they really amplify. (SOUNDBITE OF MUSIC) RAZ: So you're talking about really big movements, right? But what about, like, on a smaller scale? - because it's almost a truism that the squeaky wheel is always going to get some grease, right? But in the past, to be a squeaky wheel, you had to show up. You to be a pain in someone's butt, right? HEIMANS: Yeah. RAZ: And today if you want to be a squeaky wheel, it's not - it's, like, not that hard to mobilize people quickly, to irritate somebody to the point where, you know, a politician or a journalist or somebody in the public eye is going to respond and react as a result of your complaints. HEIMANS: Exactly. And companies and organizations still haven't quite learned how to respond to these huge kind of currents of new power. Institutions are good at dealing with other institutions. They're not very good at dealing with movements. And I think you're also right that anyone can be a squeaky wheel. I mean, I think one of the fascinating things - I mean, if you're a kid today, you know, what you're learning every day are these skills of mobilization. You know, every kid has followers. Every kid is thinking, in a way, about how to build community around the content that they produce. So that's why it's so much easier now for anyone to take that up. And that, unfortunately, is also why it's easier for extreme ideas to spread. But, you know, it also gives me a lot of hope because, you know, kids today - and we saw this wonderfully with the Parkland kids - are using these skills to fight for justice. Digital activism is an entree to those more-committed forms of activism. (SOUNDBITE OF MUSIC) HEIMANS: I think that the version of activism that we have, maybe, in our heads, the version that, maybe, we tell stories about in our films, is an incredibly important form of activism. But it's not the only form of activism that matters. And it's not the only form of activism that has brought about change. So we need all these kinds of participation. (SOUNDBITE OF MUSIC) RAZ: That's Jeremy Heimans, co-author of \"New Power. \" He's also the CEO of Purpose. It's an organization that helps build and support movements around the world. You can see his full talk at ted. npr. org. On the show today, ideas about Changing The World. Stay with us. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC)", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-13-702555175": {"title": "Generation Z, Under Employers' Gaze, Is Biting Its Tongue On Social Media : NPR", "url": "https://www.npr.org/2019/04/13/702555175/under-employers-gaze-gen-z-is-biting-its-tongue-on-social-media", "author": "No author found", "published_date": "2019-04-13", "content": "", "section": "National", "disclaimer": ""}, "2019-04-15-712809955": {"title": "Weight Loss Apps Are Forcing Industry To Embrace Digital Alternatives : NPR", "url": "https://www.npr.org/2019/04/15/712809955/my-new-diet-is-an-app-weight-loss-goes-digital", "author": "No author found", "published_date": "2019-04-15", "content": "AILSA CHANG, HOST:  Our phones help us do so many things. And you can add one more thing to that list - losing weight. Health apps are gaining popularity, and that's forcing older weight loss companies to adapt, as NPR's Yuki Noguchi reports. YUKI NOGUCHI, BYLINE: Like many consumers these days, Jessica Holloway-Haytcher uses words like mindful and wellness when talking about weight loss. JESSICA HOLLOWAY-HAYTCHER: In our household, diet is, like, a four-letter word. We don't use it. It's about a lifestyle change. It's about trying to figure out what is the healthy way for us to live. NOGUCHI: Holloway-Haytcher owns a staffing firm in Kennewick, Wash. Two years ago, she tried diet shakes and supplements. She hated them. HOLLOWAY-HAYTCHER: Even though they say they have a lot of flavor, all I taste is salt. NOGUCHI: She also hired a personal trainer, but his schedule never matched hers. She spent $600 a month and wasn't able to keep the weight off. Now Holloway-Haytcher uses an app called Noom. She says it's taught her new habits. She's shed over 30 pounds by preparing healthy meals in advance or focusing on conversation to slow her eating. The app - Noom is an NPR sponsor - helps her track meals, exercise and keep in touch with an online coach. She says sometimes it even feels like the app knows what she's thinking. HOLLOWAY-HAYTCHER: It's kind of funny how, like, I'll open the app one day, and it'll be exactly what I'm struggling with is what they're talking about. Like, I hit a plateau, so they talked about how that can affect you and how to work through it and then how to work through the negative self-talk that you have. NOGUCHI: John LaRosa is president of Marketdata, which tracks the $4 billion U. S. weight loss industry. He says apps like MyFitnessPal, Fitbit and Fooducate appeal to do-it-yourselfers who make up 80 percent of people trying to lose weight. He says the downside of apps is that users often tire of them just as they do gym memberships. But they're also cheaper than most programs. And LaRosa says they appeal to the younger demographic that traditional chains have struggled to attract. JOHN LAROSA: The average age of a customer of Jenny Craig or Nutrisystem or Weight Watchers is about 48, and it's probably going up. It's going to be a shrinking market if they just cater to the baby boomers. NOGUCHI: Nutrisystem, which was acquired by Tivity Health last year, revamped its digital strategy. Tivity President Dawn Zier says this is why. DAWN ZIER: The younger generation is all about being on demand. I want the food when I want it. I want to talk to a counselor when I actually have an issue, which may be 10 o'clock on Saturday night. NOGUCHI: Weight Watchers also overhauled its brand. Debra Benovitz is senior vice president for the company, which last year changed his name to WW. DEBRA BENOVITZ: Three years ago, millennials told us that this was my grandmother's brand. NOGUCHI: The 56-year-old company shifted gears. It still champions support groups at its retail locations. Those physical stores made it and Jenny Craig popular in the 1980s and are its biggest difference from the digital upstarts. Benovitz says WW's own app helps to stay in touch between, or instead of, those in-person meetings. BENOVITZ: It used to be that we hesitated to even show the app in our commercials, and that has so shift. I think the future is being a really strong science-based technology partner in the health and wellness space. NOGUCHI: And that has broad appeal. Thirty-four-year-old Favin Gebremariam uses WW's app along with her mom. They chat daily about weight and exchange photos with other members. FAVIN GEBREMARIAM: And the interactions - they happen all day. You get feedback, and you get congratulations or you get support. We want to track our food, and we want to, you know, track out activity and check in on our friends. And that's happening on the phone. And so I think that that's been a crucial element to my success, for sure. NOGUCHI: Success Gebremariam measures in lower weight and higher self-esteem. Yuki Noguchi, NPR News. AILSA CHANG, HOST:   Our phones help us do so many things. And you can add one more thing to that list - losing weight. Health apps are gaining popularity, and that's forcing older weight loss companies to adapt, as NPR's Yuki Noguchi reports. YUKI NOGUCHI, BYLINE: Like many consumers these days, Jessica Holloway-Haytcher uses words like mindful and wellness when talking about weight loss. JESSICA HOLLOWAY-HAYTCHER: In our household, diet is, like, a four-letter word. We don't use it. It's about a lifestyle change. It's about trying to figure out what is the healthy way for us to live. NOGUCHI: Holloway-Haytcher owns a staffing firm in Kennewick, Wash. Two years ago, she tried diet shakes and supplements. She hated them. HOLLOWAY-HAYTCHER: Even though they say they have a lot of flavor, all I taste is salt. NOGUCHI: She also hired a personal trainer, but his schedule never matched hers. She spent $600 a month and wasn't able to keep the weight off. Now Holloway-Haytcher uses an app called Noom. She says it's taught her new habits. She's shed over 30 pounds by preparing healthy meals in advance or focusing on conversation to slow her eating. The app - Noom is an NPR sponsor - helps her track meals, exercise and keep in touch with an online coach. She says sometimes it even feels like the app knows what she's thinking. HOLLOWAY-HAYTCHER: It's kind of funny how, like, I'll open the app one day, and it'll be exactly what I'm struggling with is what they're talking about. Like, I hit a plateau, so they talked about how that can affect you and how to work through it and then how to work through the negative self-talk that you have. NOGUCHI: John LaRosa is president of Marketdata, which tracks the $4 billion U. S. weight loss industry. He says apps like MyFitnessPal, Fitbit and Fooducate appeal to do-it-yourselfers who make up 80 percent of people trying to lose weight. He says the downside of apps is that users often tire of them just as they do gym memberships. But they're also cheaper than most programs. And LaRosa says they appeal to the younger demographic that traditional chains have struggled to attract. JOHN LAROSA: The average age of a customer of Jenny Craig or Nutrisystem or Weight Watchers is about 48, and it's probably going up. It's going to be a shrinking market if they just cater to the baby boomers. NOGUCHI: Nutrisystem, which was acquired by Tivity Health last year, revamped its digital strategy. Tivity President Dawn Zier says this is why. DAWN ZIER: The younger generation is all about being on demand. I want the food when I want it. I want to talk to a counselor when I actually have an issue, which may be 10 o'clock on Saturday night. NOGUCHI: Weight Watchers also overhauled its brand. Debra Benovitz is senior vice president for the company, which last year changed his name to WW. DEBRA BENOVITZ: Three years ago, millennials told us that this was my grandmother's brand. NOGUCHI: The 56-year-old company shifted gears. It still champions support groups at its retail locations. Those physical stores made it and Jenny Craig popular in the 1980s and are its biggest difference from the digital upstarts. Benovitz says WW's own app helps to stay in touch between, or instead of, those in-person meetings. BENOVITZ: It used to be that we hesitated to even show the app in our commercials, and that has so shift. I think the future is being a really strong science-based technology partner in the health and wellness space. NOGUCHI: And that has broad appeal. Thirty-four-year-old Favin Gebremariam uses WW's app along with her mom. They chat daily about weight and exchange photos with other members. FAVIN GEBREMARIAM: And the interactions - they happen all day. You get feedback, and you get congratulations or you get support. We want to track our food, and we want to, you know, track out activity and check in on our friends. And that's happening on the phone. And so I think that that's been a crucial element to my success, for sure. NOGUCHI: Success Gebremariam measures in lower weight and higher self-esteem. Yuki Noguchi, NPR News.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-16-714073933": {"title": "Want To Know How Far Artificial Intelligence Has Come? Just Look At CAPTCHA : NPR", "url": "https://www.npr.org/2019/04/16/714073933/want-to-know-how-far-artificial-intelligence-has-come-just-look-at-captcha", "author": "No author found", "published_date": "2019-04-16", "content": "AUDIE CORNISH, HOST:  We're going to look now at the state of artificial intelligence this month in All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CORNISH: I'm not a robot. You've probably seen that statement online alongside a prompt that says something like, type the letters you see, or, click on all the stoplights. Do it right, and you get to go on to the next page. These games are developed by Google. They're called CAPTCHAs. Researcher Jason Polakis of the University of Illinois at Chicago has proven that, in fact, robots are pretty good at CAPTCHAs. JASON POLAKIS: It's a very basic class of challenges that have been created in a way to have little tasks that are easy for humans to solve, but difficult for computers. CORNISH: Is this truly a security feature, then, or a way to train artificial intelligence programs? POLAKIS: It's actually a combination of both. So when some of the seminal original papers came out describing CAPTCHAs, they were seen both as a challenge that can prevent automated actions from computers - so something that has a very specific security spin to it - but also, the fact that you get feedback from users - they can help you train your system and train your models and then have stronger AI and machine learning techniques moving forward. CORNISH: Now, let's talk about the moving forward, then, because they have changed over time. Have they gotten harder? And if so, why? POLAKIS: They've gotten both harder and easier. So CAPTCHAs are actually a really good example of the arms race between defenders and attackers and security. As attackers get better and new techniques come out and machine learning improves and you can actually automatically infer, for example, what words or what letters were in the CAPTCHA, then defenders try to prevent that by making their challenges harder. And then once you reach a point where text CAPTCHAs are very hard for humans. . . CORNISH: Just to jump in here, then - so what you're saying is we started out with the text CAPTCHAs. POLAKIS: Yes. CORNISH: So this is maybe a combination of words and letters, but they may be distorted or the letters look all kind of wiggly and wavy. And a human should be able to discern it, even if a computer can't. POLAKIS: Exactly. CORNISH: Why, though, (laughter) are we now in a phase of, like, click - find the three stoplights or find the three foot bridges? Like, I have, sometimes, this feeling of, like, oh, God (laughter). Like, I'm having trouble with this. POLAKIS: The thing is that at some point, the text CAPTCHAs became so hard for humans and so unusable, whereas computers were really good at breaking them where it just didn't make sense anymore to have them. So it was necessary to switch to different design. And Google actually released version 2 of reCAPTCHA in, like, late 2014. And the whole idea there was that the challenge are now going to be much easier because they will ask you to, like, identify images with cats or dogs or, you know, cups of coffee. However, once we did our research when that system first came out and we showed that deep learning systems were already at a point where they could, let's say, return keywords that describe the content of images, and we showed how you could misuse those to automatically solve reCAPTCHA's challenges, then again, they were in a spot where they would - where they needed to make these systems even harder, the challenges harder to prevent, you know, machine learning systems from solving them. CORNISH: In the end, people may look at this as a kind of nuisance. For them, it's a little bit like coming to a stoplight, and they're traveling the Web. But what are the implications, you think, for computer science more broadly? POLAKIS: I definitely agree that it's a nuisance for many people, but we need to keep in mind that both from the security perspective and the improving machine learning perspective, it's something that's positive for all of us. When a website can actually use CAPTCHAs to prevent automated attacks, that means that as users, we're going to be affected less. And on the other hand, there been multiple attempts by major companies and researchers to actually harness, let's say, the feedback from users to actually improve their systems. So, for example, anything that has to do with computer vision or, like, autonomous vehicles - the fact that we can provide feedback through the CAPTCHA system, which can allow companies to retrain their classifiers and have more accurate models - that's also beneficial to all of us. CORNISH: Jason Polakis, thank you so much for speaking with us and for explaining it. POLAKIS: Thanks for having me on the show. CORNISH: Jason Polakis is an assistant professor of computer science at the University of Illinois at Chicago. AUDIE CORNISH, HOST:   We're going to look now at the state of artificial intelligence this month in All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CORNISH: I'm not a robot. You've probably seen that statement online alongside a prompt that says something like, type the letters you see, or, click on all the stoplights. Do it right, and you get to go on to the next page. These games are developed by Google. They're called CAPTCHAs. Researcher Jason Polakis of the University of Illinois at Chicago has proven that, in fact, robots are pretty good at CAPTCHAs. JASON POLAKIS: It's a very basic class of challenges that have been created in a way to have little tasks that are easy for humans to solve, but difficult for computers. CORNISH: Is this truly a security feature, then, or a way to train artificial intelligence programs? POLAKIS: It's actually a combination of both. So when some of the seminal original papers came out describing CAPTCHAs, they were seen both as a challenge that can prevent automated actions from computers - so something that has a very specific security spin to it - but also, the fact that you get feedback from users - they can help you train your system and train your models and then have stronger AI and machine learning techniques moving forward. CORNISH: Now, let's talk about the moving forward, then, because they have changed over time. Have they gotten harder? And if so, why? POLAKIS: They've gotten both harder and easier. So CAPTCHAs are actually a really good example of the arms race between defenders and attackers and security. As attackers get better and new techniques come out and machine learning improves and you can actually automatically infer, for example, what words or what letters were in the CAPTCHA, then defenders try to prevent that by making their challenges harder. And then once you reach a point where text CAPTCHAs are very hard for humans. . . CORNISH: Just to jump in here, then - so what you're saying is we started out with the text CAPTCHAs. POLAKIS: Yes. CORNISH: So this is maybe a combination of words and letters, but they may be distorted or the letters look all kind of wiggly and wavy. And a human should be able to discern it, even if a computer can't. POLAKIS: Exactly. CORNISH: Why, though, (laughter) are we now in a phase of, like, click - find the three stoplights or find the three foot bridges? Like, I have, sometimes, this feeling of, like, oh, God (laughter). Like, I'm having trouble with this. POLAKIS: The thing is that at some point, the text CAPTCHAs became so hard for humans and so unusable, whereas computers were really good at breaking them where it just didn't make sense anymore to have them. So it was necessary to switch to different design. And Google actually released version 2 of reCAPTCHA in, like, late 2014. And the whole idea there was that the challenge are now going to be much easier because they will ask you to, like, identify images with cats or dogs or, you know, cups of coffee. However, once we did our research when that system first came out and we showed that deep learning systems were already at a point where they could, let's say, return keywords that describe the content of images, and we showed how you could misuse those to automatically solve reCAPTCHA's challenges, then again, they were in a spot where they would - where they needed to make these systems even harder, the challenges harder to prevent, you know, machine learning systems from solving them. CORNISH: In the end, people may look at this as a kind of nuisance. For them, it's a little bit like coming to a stoplight, and they're traveling the Web. But what are the implications, you think, for computer science more broadly? POLAKIS: I definitely agree that it's a nuisance for many people, but we need to keep in mind that both from the security perspective and the improving machine learning perspective, it's something that's positive for all of us. When a website can actually use CAPTCHAs to prevent automated attacks, that means that as users, we're going to be affected less. And on the other hand, there been multiple attempts by major companies and researchers to actually harness, let's say, the feedback from users to actually improve their systems. So, for example, anything that has to do with computer vision or, like, autonomous vehicles - the fact that we can provide feedback through the CAPTCHA system, which can allow companies to retrain their classifiers and have more accurate models - that's also beneficial to all of us. CORNISH: Jason Polakis, thank you so much for speaking with us and for explaining it. POLAKIS: Thanks for having me on the show. CORNISH: Jason Polakis is an assistant professor of computer science at the University of Illinois at Chicago.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-17-711820160": {"title": "After Boeing 737 Max Crashes, More Enrolling In Fear-Of-Flying Courses : NPR", "url": "https://www.npr.org/2019/04/17/711820160/after-boeing-crashes-more-people-want-help-taming-fear-of-flying", "author": "No author found", "published_date": "2019-04-17", "content": "DAVID GREENE, HOST: Boeing 737 MAX airplanes have been grounded worldwide. This after two deadly crashes within less than five months. There was particular concern here because it was the same kind of plane. But crashes remain really rare. And yet an entire industry exists to help fearful fliers, including online classes. And since the two Boeing crashes, enrollment in some cases has more than doubled. Here's NPR's Jasmine Garsd. JASMINE GARSD, BYLINE: This summer, musician Katie Sucha will be touring England, and she's scared. KATIE SUCHA: It really is a serious mental challenge to sort of walk through those doors and get on the plane. You know, I often have to take anti-anxiety meds in order to make it possible for me. GARSD: Her fear is so bad, when she was a teacher in Mississippi and Sucha wanted to visit her family in Michigan, she'd take a 14-hour bus ride rather than spend two hours in the air. And then there's that upcoming trip to Europe. It's a great career opportunity. But still, she's freaked out. She can't stop watching the news about the Boeing 737 MAX. She gets nervous just talking about it. SUCHA: You know, if this happens with one type of plane, how many other examples of, you know, a faulty sensor or something malfunctioning, and pilots who are highly trained have no idea how to deal with it? GARSD: There's this whole industry built around fear of flying. For $2. 99, you can buy an app called, Am I Going Down? There's another one offering hypnosis relaxation for $4. 99. The list goes on and on. There's also a myriad of classes, many taught by retired pilots. There's fearlessflight. com, fearofflyinghelp. com and another one, SOAR, which has been around since 1982. Here's an excerpt from the SOAR online video seminar. (SOUNDBITE OF VIDEO SEMINAR)TOM BUNN: It's like being in an elevator. And you're coming up to the floor where you're going to stop - you feel a little lightheaded. So you might think the airplane's falling, but no, it's not. It's just slowing down its climb. GARSD: That's SOAR's founder, Captain Tom Bunn, a retired United Airlines pilot and licensed therapist. His course, which includes videos and counseling, starts at $200. Bunn explains to clients how planes fly and why they don't just drop out of the sky. It is one common concern he hears. Bunn also teaches techniques to handle stress and fear. Graduates of SOAR, like Ben Kaminow, say it's worth the price. BEN KAMINOW: It was debilitating to my life. I would not go away with my family. GARSD: Kaminow's phobia started in 1993 when his vacation flight from New York to Mexico hit strong turbulence. He was terrified. When he got back to New York, he vowed never to get on a plane again. And for the next eight years, he didn't. He even canceled his honeymoon to Greece. They drove to North Carolina instead. Eventually, he enrolled in SOAR. KAMINOW: He basically went all about how planes fly, how they get up in the air, what turbulence really is. GARSD: His package even included a letter to give to your flight attendant explaining that you're working on your fear and would love to meet the captain before takeoff. Kaminow could fly again. And then 9/11 happened. He was working at the World Trade Center. KAMINOW: I was in the first tower, plane hit the building. And I could not get - I couldn't get in a plane again. So all these trips I had planned after going through this whole thing to be able to fly again - I shut it down. GARSD: It would take Kaminow another four years to board a plane again. KAMINOW: The fear was just being inside, being uncomfortable and not being able to be in control. You know, when you drive a car, you have the control. Here I didn't have the control. I didn't know how to handle it. GARSD: He says he finally did the SOAR training sessions again - the techniques to handle stress and fear and trusting that pilots are highly trained, and they, too, want to get home safely. I asked Kaminow if the recent Boeing 737 crashes worry him. He says no. In fact, just last week, he took a trip to Los Angeles. For him, being able to even get on that plane without being afraid - that was the real journey. Jasmine Garsd, NPR News, New York. DAVID GREENE, HOST:  Boeing 737 MAX airplanes have been grounded worldwide. This after two deadly crashes within less than five months. There was particular concern here because it was the same kind of plane. But crashes remain really rare. And yet an entire industry exists to help fearful fliers, including online classes. And since the two Boeing crashes, enrollment in some cases has more than doubled. Here's NPR's Jasmine Garsd. JASMINE GARSD, BYLINE: This summer, musician Katie Sucha will be touring England, and she's scared. KATIE SUCHA: It really is a serious mental challenge to sort of walk through those doors and get on the plane. You know, I often have to take anti-anxiety meds in order to make it possible for me. GARSD: Her fear is so bad, when she was a teacher in Mississippi and Sucha wanted to visit her family in Michigan, she'd take a 14-hour bus ride rather than spend two hours in the air. And then there's that upcoming trip to Europe. It's a great career opportunity. But still, she's freaked out. She can't stop watching the news about the Boeing 737 MAX. She gets nervous just talking about it. SUCHA: You know, if this happens with one type of plane, how many other examples of, you know, a faulty sensor or something malfunctioning, and pilots who are highly trained have no idea how to deal with it? GARSD: There's this whole industry built around fear of flying. For $2. 99, you can buy an app called, Am I Going Down? There's another one offering hypnosis relaxation for $4. 99. The list goes on and on. There's also a myriad of classes, many taught by retired pilots. There's fearlessflight. com, fearofflyinghelp. com and another one, SOAR, which has been around since 1982. Here's an excerpt from the SOAR online video seminar. (SOUNDBITE OF VIDEO SEMINAR) TOM BUNN: It's like being in an elevator. And you're coming up to the floor where you're going to stop - you feel a little lightheaded. So you might think the airplane's falling, but no, it's not. It's just slowing down its climb. GARSD: That's SOAR's founder, Captain Tom Bunn, a retired United Airlines pilot and licensed therapist. His course, which includes videos and counseling, starts at $200. Bunn explains to clients how planes fly and why they don't just drop out of the sky. It is one common concern he hears. Bunn also teaches techniques to handle stress and fear. Graduates of SOAR, like Ben Kaminow, say it's worth the price. BEN KAMINOW: It was debilitating to my life. I would not go away with my family. GARSD: Kaminow's phobia started in 1993 when his vacation flight from New York to Mexico hit strong turbulence. He was terrified. When he got back to New York, he vowed never to get on a plane again. And for the next eight years, he didn't. He even canceled his honeymoon to Greece. They drove to North Carolina instead. Eventually, he enrolled in SOAR. KAMINOW: He basically went all about how planes fly, how they get up in the air, what turbulence really is. GARSD: His package even included a letter to give to your flight attendant explaining that you're working on your fear and would love to meet the captain before takeoff. Kaminow could fly again. And then 9/11 happened. He was working at the World Trade Center. KAMINOW: I was in the first tower, plane hit the building. And I could not get - I couldn't get in a plane again. So all these trips I had planned after going through this whole thing to be able to fly again - I shut it down. GARSD: It would take Kaminow another four years to board a plane again. KAMINOW: The fear was just being inside, being uncomfortable and not being able to be in control. You know, when you drive a car, you have the control. Here I didn't have the control. I didn't know how to handle it. GARSD: He says he finally did the SOAR training sessions again - the techniques to handle stress and fear and trusting that pilots are highly trained, and they, too, want to get home safely. I asked Kaminow if the recent Boeing 737 crashes worry him. He says no. In fact, just last week, he took a trip to Los Angeles. For him, being able to even get on that plane without being afraid - that was the real journey. Jasmine Garsd, NPR News, New York.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-19-714890832": {"title": "Mueller Report Draws Attention To Russian Election Interference In Florida : NPR", "url": "https://www.npr.org/2019/04/19/714890832/mueller-report-raises-new-questions-about-russias-hacking-targets-in-2016", "author": "No author found", "published_date": "2019-04-19", "content": "", "section": "Politics", "disclaimer": ""}, "2019-04-20-715499199": {"title": "Helvetica Typeface Gets Makeover After 36 Years : NPR", "url": "https://www.npr.org/2019/04/20/715499199/helvetica-the-iconic-font-both-loved-and-scorned-gets-its-1st-redesign-in-36-yea", "author": "No author found", "published_date": "2019-04-20", "content": "", "section": "Technology", "disclaimer": ""}, "2019-04-21-715616224": {"title": "New Campaign Would Allow Cryptocurrency Donations To Be Distributed To Venezuelans : NPR", "url": "https://www.npr.org/2019/04/21/715616224/new-campaign-would-allow-cryptocurrency-donations-to-be-distributed-to-venezuela", "author": "No author found", "published_date": "2019-04-21", "content": "LULU GARCIA-NAVARRO, HOST: How can you distribute humanitarian aid safely and efficiently, especially to places like Venezuela, where physical cargo, such as food aid and medical supplies, are stopped at the border and the national currency, the bolivar, suffers from devaluation and hyperinflation? Steve Hanke, a professor of applied economics at Johns Hopkins University, is leading a new campaign called Airdrop Venezuela. The campaign would allow cryptocurrency donations to be distributed to Venezuelans that they can use to purchase goods. Professor Hanke joins us now. Welcome. STEVE HANKE: Good to be with you. GARCIA-NAVARRO: So before we begin, we have to say that you served as an economic adviser to former Venezuelan president Rafael Caldera between 1995 and '96. And you're also part of the board of AirTM, an online currency exchange company whose existing platform you're using for this campaign. So tell us, how would it work? How many people receive these donations? And how will they be able to use it? HANKE: Well, our objective is to have 100,000 people verified that would qualify to receive donations. Right now we're at 60,000. We've collected $272,000 to date, and the objective is to get to $1 million. So we'll have $1 million, and we'll have 100,000 Venezuelans receiving equal portions of that. And we'll probably be delivering in August. GARCIA-NAVARRO: Yeah. How does it work? HANKE: You donate a cryptocurrency to the campaign, and that cryptocurrency goes to the AirTM platform and is distributed to wallets of the qualified Venezuelans. GARCIA-NAVARRO: The wallets - you mean their phones. HANKE: Their phones, yes. GARCIA-NAVARRO: Obviously, there are challenges to cryptocurrency. How would this practically work, though? Could people actually use this to buy the things that they need? HANKE: Well, this is what most people probably will do - go to the AirTM platform and exchange the cryptocurrencies that they've received in their wallet for real money that they can use to buy things. And the real money would, in most cases, be U. S. dollars. When the currency in your country is literally melting in your hand and - knowing that, the key is getting people hard currency that they can actually use to purchase something. And so that was the general attraction. And the technology of using this Internet platform is just what the doctor ordered. GARCIA-NAVARRO: AirTM's CEO Ruben Galindo told the Miami Herald he hopes he can work with opposition leader Juan Guaido in the future. So I guess the wider question is, are the intentions of the project purely humanitarian or also partly political? HANKE: No, they're purely humanitarian at this point. And these are private - the private exchanges that are going on. And there's really no particular political motivation. It's just to help people to give them some purchasing power. The money comes from private donations. GARCIA-NAVARRO: And I guess my last question is if this works and this is effective, where do you see this headed? HANKE: This will be a demonstration of how relief agencies all around the world can easily deliver aid and relief to people in need. You won't have to drive a pickup truck around filled with cash that you're giving away or filled with medicine or clothing or food. That's an inefficient and unsafe way to do things. You'll have a very safe way to do it. I think it will be the - a - really, a new thing. GARCIA-NAVARRO: Steve Hanke is a professor of applied economics at Johns Hopkins University. Thank you so much. HANKE: Thank you. LULU GARCIA-NAVARRO, HOST:  How can you distribute humanitarian aid safely and efficiently, especially to places like Venezuela, where physical cargo, such as food aid and medical supplies, are stopped at the border and the national currency, the bolivar, suffers from devaluation and hyperinflation? Steve Hanke, a professor of applied economics at Johns Hopkins University, is leading a new campaign called Airdrop Venezuela. The campaign would allow cryptocurrency donations to be distributed to Venezuelans that they can use to purchase goods. Professor Hanke joins us now. Welcome. STEVE HANKE: Good to be with you. GARCIA-NAVARRO: So before we begin, we have to say that you served as an economic adviser to former Venezuelan president Rafael Caldera between 1995 and '96. And you're also part of the board of AirTM, an online currency exchange company whose existing platform you're using for this campaign. So tell us, how would it work? How many people receive these donations? And how will they be able to use it? HANKE: Well, our objective is to have 100,000 people verified that would qualify to receive donations. Right now we're at 60,000. We've collected $272,000 to date, and the objective is to get to $1 million. So we'll have $1 million, and we'll have 100,000 Venezuelans receiving equal portions of that. And we'll probably be delivering in August. GARCIA-NAVARRO: Yeah. How does it work? HANKE: You donate a cryptocurrency to the campaign, and that cryptocurrency goes to the AirTM platform and is distributed to wallets of the qualified Venezuelans. GARCIA-NAVARRO: The wallets - you mean their phones. HANKE: Their phones, yes. GARCIA-NAVARRO: Obviously, there are challenges to cryptocurrency. How would this practically work, though? Could people actually use this to buy the things that they need? HANKE: Well, this is what most people probably will do - go to the AirTM platform and exchange the cryptocurrencies that they've received in their wallet for real money that they can use to buy things. And the real money would, in most cases, be U. S. dollars. When the currency in your country is literally melting in your hand and - knowing that, the key is getting people hard currency that they can actually use to purchase something. And so that was the general attraction. And the technology of using this Internet platform is just what the doctor ordered. GARCIA-NAVARRO: AirTM's CEO Ruben Galindo told the Miami Herald he hopes he can work with opposition leader Juan Guaido in the future. So I guess the wider question is, are the intentions of the project purely humanitarian or also partly political? HANKE: No, they're purely humanitarian at this point. And these are private - the private exchanges that are going on. And there's really no particular political motivation. It's just to help people to give them some purchasing power. The money comes from private donations. GARCIA-NAVARRO: And I guess my last question is if this works and this is effective, where do you see this headed? HANKE: This will be a demonstration of how relief agencies all around the world can easily deliver aid and relief to people in need. You won't have to drive a pickup truck around filled with cash that you're giving away or filled with medicine or clothing or food. That's an inefficient and unsafe way to do things. You'll have a very safe way to do it. I think it will be the - a - really, a new thing. GARCIA-NAVARRO: Steve Hanke is a professor of applied economics at Johns Hopkins University. Thank you so much. HANKE: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-22-716096498": {"title": "Following Easter Attacks In Sri Lanka, A Social Media Ban Disabled Some Apps : NPR", "url": "https://www.npr.org/2019/04/22/716096498/following-easter-attacks-in-sri-lanka-a-social-media-ban-disabled-some-apps", "author": "No author found", "published_date": "2019-04-22", "content": "ARI SHAPIRO, HOST:  As Sri Lanka investigates who was behind yesterday's terrorist attacks on churches and hotels, the government continues to block access to social media there. Facebook, Instagram and WhatsApp, all owned by Facebook, are among the services under a blackout. This once again brings into focus the fear that Facebook cannot rein in disinformation and calls to violence. NPR's Aarti Shahani is following the situation. Hi, Aarti. AARTI SHAHANI, BYLINE: Hi. SHAPIRO: What explanation did Sri Lanka's government give for shutting down these social media apps? SHAHANI: So in a nutshell, the government doesn't trust Facebook to yank down fake news and calls to violence before they go viral. Keep in mind, Sri Lanka is only a decade out of civil war. That's not a long time. People remember it. Peace feels tenuous to them. Just a year ago, last March, Buddhist extremists torched Muslim homes and businesses and used Facebook to incite violence. In response to that, the government had announced a 72-hour block on social media. And this time around, the government is not putting a time limit on it. It's unclear when the ban will come to an end. SHAPIRO: Of course, there are lots of examples of hoaxes going rampant on Facebook, from Myanmar to the United States. We have to note, they are an NPR sponsor. How has the company responded to what's happening in Sri Lanka? SHAHANI: The response is meek. Company leaders are not defending themselves. They issued a statement in a pretty conciliatory tone saying, hey, we're working to support first responders and law enforcement and to identify and remove harmful content. Facebook has had so many screw-ups. Executives can't give themselves a pat on the back or claim, hey, we have a handle on calls to violence. As recently as the New Zealand massacre, Facebook failed to remove 20% of the video footage of the mass shooting even though, in that instance, people in the company knew what to look out for. SHAPIRO: And tell us how people in Sri Lanka are responding to this. I know you've been reaching out to people affected by the blackout. What are you hearing? SHAHANI: I have. You know, social media has been used time and again to help in crises, right? People turn to Facebook to check in and broadcast that they're safe, to get updates from local officials and hospitals. And especially in Asia, where WhatsApp has replaced regular phone calls for much of the population, the ban really threw people off. I spoke to one woman, an American named Reena Arora, who was in Sri Lanka on vacation. Her family didn't know if she was in Colombo, near a bomb target. And this is her. REENA ARORA: They tried to call me several times, I believe through WhatsApp. And they weren't able to get in contact with me. And so they were very concerned for my safety because all of them knew that I was traveling in Sri Lanka at the time. SHAHANI: She had to worry about her parents worrying that she was injured or even worse than that. And she had no idea WhatsApp was down. When she tried to reach her driver to get to the airport, he didn't respond. And she figured, OK, he's blowing me off. And, you know, he wasn't. He just didn't get the messages. So the both of them were operating in an information vacuum, feeling totally isolated, when what they really needed was to connect in a moment of panic. SHAPIRO: Can we say whether the blackout has actually worked at preventing the spread of hoaxes and conspiracy theories? SHAHANI: Yeah, you know, I actually spoke to a man who lives in Colombo. And he told me that even though Facebook was banned, some people used a backdoor tool - it's called a VPN - to get on anyway. And lo and behold, there were posts online designed to sow fear. One post claimed a bomb went off in a nearby local park. That was not true. Another post claimed that terrorists had poisoned the water supply. That was also a lie. TV and radio journalists had to jump in on that and report that, hey, you can trust the water. You can drink it; it's safe. That was extra work for them. It could have been worse with more people on the platform. SHAPIRO: Wow. So just briefly, how does what's happened in Sri Lanka compare to what you've seen in other parts of the world? SHAHANI: You know, what we've just seen in Sri Lanka is very swift, unilateral action. Other countries, like France and Germany, have gone the regulatory route, right? Germany passed laws to fine Facebook severely for its failure to pull down white supremacist content. Either approach - quick or regulatory methodic - they illustrate that because Facebook hasn't been able to take control, governments have decided they're going to have to act. SHAPIRO: That's NPR's Aarti Shahani. Thanks so much. SHAHANI: Thank you. ARI SHAPIRO, HOST:   As Sri Lanka investigates who was behind yesterday's terrorist attacks on churches and hotels, the government continues to block access to social media there. Facebook, Instagram and WhatsApp, all owned by Facebook, are among the services under a blackout. This once again brings into focus the fear that Facebook cannot rein in disinformation and calls to violence. NPR's Aarti Shahani is following the situation. Hi, Aarti. AARTI SHAHANI, BYLINE: Hi. SHAPIRO: What explanation did Sri Lanka's government give for shutting down these social media apps? SHAHANI: So in a nutshell, the government doesn't trust Facebook to yank down fake news and calls to violence before they go viral. Keep in mind, Sri Lanka is only a decade out of civil war. That's not a long time. People remember it. Peace feels tenuous to them. Just a year ago, last March, Buddhist extremists torched Muslim homes and businesses and used Facebook to incite violence. In response to that, the government had announced a 72-hour block on social media. And this time around, the government is not putting a time limit on it. It's unclear when the ban will come to an end. SHAPIRO: Of course, there are lots of examples of hoaxes going rampant on Facebook, from Myanmar to the United States. We have to note, they are an NPR sponsor. How has the company responded to what's happening in Sri Lanka? SHAHANI: The response is meek. Company leaders are not defending themselves. They issued a statement in a pretty conciliatory tone saying, hey, we're working to support first responders and law enforcement and to identify and remove harmful content. Facebook has had so many screw-ups. Executives can't give themselves a pat on the back or claim, hey, we have a handle on calls to violence. As recently as the New Zealand massacre, Facebook failed to remove 20% of the video footage of the mass shooting even though, in that instance, people in the company knew what to look out for. SHAPIRO: And tell us how people in Sri Lanka are responding to this. I know you've been reaching out to people affected by the blackout. What are you hearing? SHAHANI: I have. You know, social media has been used time and again to help in crises, right? People turn to Facebook to check in and broadcast that they're safe, to get updates from local officials and hospitals. And especially in Asia, where WhatsApp has replaced regular phone calls for much of the population, the ban really threw people off. I spoke to one woman, an American named Reena Arora, who was in Sri Lanka on vacation. Her family didn't know if she was in Colombo, near a bomb target. And this is her. REENA ARORA: They tried to call me several times, I believe through WhatsApp. And they weren't able to get in contact with me. And so they were very concerned for my safety because all of them knew that I was traveling in Sri Lanka at the time. SHAHANI: She had to worry about her parents worrying that she was injured or even worse than that. And she had no idea WhatsApp was down. When she tried to reach her driver to get to the airport, he didn't respond. And she figured, OK, he's blowing me off. And, you know, he wasn't. He just didn't get the messages. So the both of them were operating in an information vacuum, feeling totally isolated, when what they really needed was to connect in a moment of panic. SHAPIRO: Can we say whether the blackout has actually worked at preventing the spread of hoaxes and conspiracy theories? SHAHANI: Yeah, you know, I actually spoke to a man who lives in Colombo. And he told me that even though Facebook was banned, some people used a backdoor tool - it's called a VPN - to get on anyway. And lo and behold, there were posts online designed to sow fear. One post claimed a bomb went off in a nearby local park. That was not true. Another post claimed that terrorists had poisoned the water supply. That was also a lie. TV and radio journalists had to jump in on that and report that, hey, you can trust the water. You can drink it; it's safe. That was extra work for them. It could have been worse with more people on the platform. SHAPIRO: Wow. So just briefly, how does what's happened in Sri Lanka compare to what you've seen in other parts of the world? SHAHANI: You know, what we've just seen in Sri Lanka is very swift, unilateral action. Other countries, like France and Germany, have gone the regulatory route, right? Germany passed laws to fine Facebook severely for its failure to pull down white supremacist content. Either approach - quick or regulatory methodic - they illustrate that because Facebook hasn't been able to take control, governments have decided they're going to have to act. SHAPIRO: That's NPR's Aarti Shahani. Thanks so much. SHAHANI: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-22-716101422": {"title": "Samsung Delays Galaxy Fold Launch After Reviewers Find Broken Screens : NPR", "url": "https://www.npr.org/2019/04/22/716101422/samsung-delays-launch-of-folding-phone-after-reviewers-found-broken-screens", "author": "No author found", "published_date": "2019-04-22", "content": "", "section": "Technology", "disclaimer": ""}, "2019-04-23-716360818": {"title": "Wing Drone Delivery Company Gets FAA OK To Operate As An Airline : NPR", "url": "https://www.npr.org/2019/04/23/716360818/faa-certifies-googles-wing-drone-delivery-company-to-operate-as-an-airline", "author": "No author found", "published_date": "2019-04-23", "content": "", "section": "Business", "disclaimer": ""}, "2019-04-23-715107132": {"title": "Renting Instead Of Owning, And Taking It To The Extreme : NPR", "url": "https://www.npr.org/2019/04/23/715107132/the-affluent-homeless-a-sleeping-pod-a-hired-desk-and-a-handful-of-clothes", "author": "No author found", "published_date": "2019-04-23", "content": "ARI SHAPIRO, HOST: By now, you've probably heard about the so-called rental or sharing economy, where young people own a lot fewer things than their parents did. Instead, they rent and share a whole lot more - houses, cars, music, workspaces. In some places, this rental life has gone to an extreme. NPR's Sam Sanders has one such story. SAM SANDERS, BYLINE: Steven T. Johnson works in social media advertising, and he spends most of his days using things he does not own. STEVEN T JOHNSON: I took an Uber to Equinox to shower before we met and then went to PodShare and then came to WeWork. SANDERS: Steven took a ride-share to get to the gym he uses. He does not own a car. At the gym, he rents a locker. He uses the gym's laundry service because he does not own a washing machine. He doesn't even have an apartment, actually. JOHNSON: Exactly. SANDERS: We were going to meet at PodShare, this co-living space where Steven rents a bed - just a bed - in a big, open room with about a dozen other people. But it was too loud. So we went to his co-working space, a place called WeWork, where Steven rents a desk. WeWork is also an NPR sponsor. How do you keep track? JOHNSON: That's what's great. When you don't own things, you don't have to keep track of them. You just show up. SANDERS: Steven owns so little, he can carry most of his stuff in his hands. JOHNSON: I actually gave up my backpack about - that was the smallest I got down to - and I gave that up two months ago. SANDERS: Steven also only owns two outfits - well, two of the same outfit. JOHNSON: Under Armour brand-less sport shoes, Lululemon pants, Lululemon socks, Lululemon shirt, Lululemon underwear. SANDERS: Steven is part of a newish group of young people - kind of well-off but also, in a way, homeless. Does Steven represent a fundamental shift in American capitalism as we know it? SKYLER WANG: The fact is that we can't afford to sort of hoard anymore. SANDERS: That's Skylar Wang. He's a Ph. D. student at UC, Berkeley. He studies the sociology of the sharing economy. And he thinks one of the biggest factors in this economic shift is younger people buying fewer houses and choosing to live in dense urban areas and rent smaller places. Part of this is houses just being more expensive than they were for our parents. But when you're more OK with renting the place you live in, it's maybe a lot easier to accept the life where you rent and share a whole lot more. Wang does point out even if young people own less, they still have a lot of stuff, stuff that isn't tangible. WANG: I talked to a lot of minimalists. They are the type of people who love to couch-surf, right? They own, like, 30 things, but then the interesting thing is that they hoard digitally. SANDERS: They hoard digitally. WANG: They have tons of photographs. They have thousands and thousands of Instagram posts. SANDERS: It's still an economy of stuff. It's just different stuff. It's experiences. So how do businesses deal with this? For starters, a lot more companies are getting into rentals. Even IKEA is starting to lease its furniture. The outdoor chain REI announced recently that it's vastly expanding its rental program for things like camping gear. Eric Artz is the acting CEO of the company. He says this requires a different kind of outreach - selling experience more than the actual item. ERIC ARTZ: We're selling joy. You know, we're selling inspiration when you get out on a trail or go for a bike ride. You know, we're selling the adrenaline buzz at the end of a run. We're just trying to enable that in any way we possibly can. SANDERS: We should point out REI is an NPR sponsor. Juliet Schor is a sociologist at Boston College. She studies the rental and sharing economies. And she says not everyone's in it for the same reasons. Some are doing it just for joy. Some are doing it to move towards more personal and less corporate transactions. Others are willing to spend more for convenience. But a lot rent and share because they're broke and they need to save money. JULIET SCHOR: So I think it's a mistake to characterize them with one kind of economic orientation or orientation to money. SANDERS: Which makes it really hard to predict whether renting and sharing is our long-term future or just a fad, even for Steven Jones (ph) who is totally plugged in to a rental life. JOHNSON: It's not something that you can do forever because you do need to have a place that you can genuinely point to and say this is my home. SANDERS: I asked him how long he can live the way he's living now - a bed in a large, shared room. He's already done that off and on for more than a year, sometimes for months at a time. Steven tells me he does not know, but he also says he didn't think he'd make it this far. Sam Sanders, NPR News. [POST-BROADCAST CORRECTION: Near the end of the audio version of this report, we mistakenly refer to Steven Johnson as Steven Jones. ] ARI SHAPIRO, HOST:  By now, you've probably heard about the so-called rental or sharing economy, where young people own a lot fewer things than their parents did. Instead, they rent and share a whole lot more - houses, cars, music, workspaces. In some places, this rental life has gone to an extreme. NPR's Sam Sanders has one such story. SAM SANDERS, BYLINE: Steven T. Johnson works in social media advertising, and he spends most of his days using things he does not own. STEVEN T JOHNSON: I took an Uber to Equinox to shower before we met and then went to PodShare and then came to WeWork. SANDERS: Steven took a ride-share to get to the gym he uses. He does not own a car. At the gym, he rents a locker. He uses the gym's laundry service because he does not own a washing machine. He doesn't even have an apartment, actually. JOHNSON: Exactly. SANDERS: We were going to meet at PodShare, this co-living space where Steven rents a bed - just a bed - in a big, open room with about a dozen other people. But it was too loud. So we went to his co-working space, a place called WeWork, where Steven rents a desk. WeWork is also an NPR sponsor. How do you keep track? JOHNSON: That's what's great. When you don't own things, you don't have to keep track of them. You just show up. SANDERS: Steven owns so little, he can carry most of his stuff in his hands. JOHNSON: I actually gave up my backpack about - that was the smallest I got down to - and I gave that up two months ago. SANDERS: Steven also only owns two outfits - well, two of the same outfit. JOHNSON: Under Armour brand-less sport shoes, Lululemon pants, Lululemon socks, Lululemon shirt, Lululemon underwear. SANDERS: Steven is part of a newish group of young people - kind of well-off but also, in a way, homeless. Does Steven represent a fundamental shift in American capitalism as we know it? SKYLER WANG: The fact is that we can't afford to sort of hoard anymore. SANDERS: That's Skylar Wang. He's a Ph. D. student at UC, Berkeley. He studies the sociology of the sharing economy. And he thinks one of the biggest factors in this economic shift is younger people buying fewer houses and choosing to live in dense urban areas and rent smaller places. Part of this is houses just being more expensive than they were for our parents. But when you're more OK with renting the place you live in, it's maybe a lot easier to accept the life where you rent and share a whole lot more. Wang does point out even if young people own less, they still have a lot of stuff, stuff that isn't tangible. WANG: I talked to a lot of minimalists. They are the type of people who love to couch-surf, right? They own, like, 30 things, but then the interesting thing is that they hoard digitally. SANDERS: They hoard digitally. WANG: They have tons of photographs. They have thousands and thousands of Instagram posts. SANDERS: It's still an economy of stuff. It's just different stuff. It's experiences. So how do businesses deal with this? For starters, a lot more companies are getting into rentals. Even IKEA is starting to lease its furniture. The outdoor chain REI announced recently that it's vastly expanding its rental program for things like camping gear. Eric Artz is the acting CEO of the company. He says this requires a different kind of outreach - selling experience more than the actual item. ERIC ARTZ: We're selling joy. You know, we're selling inspiration when you get out on a trail or go for a bike ride. You know, we're selling the adrenaline buzz at the end of a run. We're just trying to enable that in any way we possibly can. SANDERS: We should point out REI is an NPR sponsor. Juliet Schor is a sociologist at Boston College. She studies the rental and sharing economies. And she says not everyone's in it for the same reasons. Some are doing it just for joy. Some are doing it to move towards more personal and less corporate transactions. Others are willing to spend more for convenience. But a lot rent and share because they're broke and they need to save money. JULIET SCHOR: So I think it's a mistake to characterize them with one kind of economic orientation or orientation to money. SANDERS: Which makes it really hard to predict whether renting and sharing is our long-term future or just a fad, even for Steven Jones (ph) who is totally plugged in to a rental life. JOHNSON: It's not something that you can do forever because you do need to have a place that you can genuinely point to and say this is my home. SANDERS: I asked him how long he can live the way he's living now - a bed in a large, shared room. He's already done that off and on for more than a year, sometimes for months at a time. Steven tells me he does not know, but he also says he didn't think he'd make it this far. Sam Sanders, NPR News. [POST-BROADCAST CORRECTION: Near the end of the audio version of this report, we mistakenly refer to Steven Johnson as Steven Jones. ]", "section": "Economy", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-24-716886822": {"title": "Facebook Setting Aside Up To $5 Billion For Privacy Violations : NPR", "url": "https://www.npr.org/2019/04/24/716886822/facebook-could-face-up-to-5-billion-fine-for-privacy-violations", "author": "No author found", "published_date": "2019-04-24", "content": "", "section": "Business", "disclaimer": ""}, "2019-04-28-717970491": {"title": "TikTok, Explained : NPR", "url": "https://www.npr.org/2019/04/28/717970491/tiktok-explained", "author": "No author found", "published_date": "2019-04-28", "content": "LULU GARCIA-NAVARRO, HOST: These are the social media rankings according to my millennial niece Alex Uriarte. Facebook. . . ALEX URIARTE: Where I go to find, like, what my grandma's doing. What is Mema (ph) up to? GARCIA-NAVARRO: Ouch. Snapchat - over it. URIARTE: It's the app that I open, like, every month or so to see if there's anything - and usually nothing anymore. I get a snapchat from, like, Team Snapchat on the holiday. GARCIA-NAVARRO: Yikes. But there is something keeping her tethered to her phone. URIARTE: I have a new app addiction. GARCIA-NAVARRO: OK. Show me. URIARTE: So this is TikTok. And the first thing is this. . . GARCIA-NAVARRO: Oh, my God. What is this? URIARTE: . . . Cosplay - Minzy Dog (ph), a thing I've never heard of. But she's doing what is called the meow dance. And it's this - it's like a. . . GARCIA-NAVARRO: (Laughter) It's ridiculous. I don't even know how to describe this. It is, like, so truly scary. URIARTE: But I find it hysterical. GARCIA-NAVARRO: TikTok is taking the younger generations by storm. I'm not going to have Alex explain TikTok. For that, I'm bringing in Brittany Spanos. She's a writer for Rolling Stone. She spent a week on the app, and she wrote about it. And she joins us now to explain. Hello. BRITTANY SPANOS: Hi. Thank you for having me. GARCIA-NAVARRO: All right. I saw TikTok. I have trouble explaining TikTok. Help me explain TikTok (laughter). SPANOS: Well, TikTok is kind of a culmination of every viral video app that's existed in the last five, maybe even 10 years - kind of going back to early YouTube. And it's comedy. It's music. It's sometimes makeup and sometimes monologuing. And it's a weird hodgepodge of every sort of viral thing that could happen in a short-form video app. GARCIA-NAVARRO: And they're really surreal. They're these psychedelic videos, a lot of cosplay. What's the appeal? SPANOS: It's really a meme incubator. There's a lot of trends happening, a lot of trends around maybe songs or maybe specific styles of comedic video or different forms of making the videos. For example. . . GARCIA-NAVARRO: The meow dance. SPANOS: Yeah. And it's just very easily digestible. I kind of like the weirdness of it. It was just very eclectic. GARCIA-NAVARRO: As we heard from my niece there, social media is going through a massive transformation. Facebook can be used by everyone, even Grandma. But when I showed TikTok to my 20-something producers, they thought that they were too old for it. It feels distinctly for the very young. SPANOS: Yeah. It moves so quickly. And especially with the way that memes move now, the way that Internet content moves now, it moves just as fast as that does - for the user that is high school age that's kind of going through three to four to five different social media platforms at once and absorbing many different viral moments concurrently. And that's what TikTok is. GARCIA-NAVARRO: What does that say about the social media future we will inhabit? I mean, do you think different generations will not be able to even understand each other and will be in these kind of completely different social media universes? SPANOS: Absolutely. I mean, that's been happening for years now. I mean, we're seeing so much stuff that's very aimed for a younger audience that's meant to appeal to them, that's meant to kind of give them this coded language of their own and reflect their certain interests. And so I definitely think it's going to only get bigger. And things like that are going to exist tenfold in the future. GARCIA-NAVARRO: Which was your favorite TikTok? SPANOS: My favorite TikTok was one that was set to Adele's \"Someone Like You. \" There was a goldfish in a bowl. And then as the camera panned out, it was a live version of \"Someone Like You. \" So the entire audience was singing the chorus. And then there was hundreds of goldfish crackers surrounding the alone, live goldfish in a bowl. GARCIA-NAVARRO: (Laughter). SPANOS: And it was great (laughter). GARCIA-NAVARRO: Brittany Spanos, a writer for Rolling Stone, thank you very much. SPANOS: Thank you. (SOUNDBITE OF SONG, \"SOMEONE LIKE YOU\")ADELE: (Singing) Never mind, I'll find someone like you. I wish nothing but the best for you too. Don't forget me. . . LULU GARCIA-NAVARRO, HOST:  These are the social media rankings according to my millennial niece Alex Uriarte. Facebook. . . ALEX URIARTE: Where I go to find, like, what my grandma's doing. What is Mema (ph) up to? GARCIA-NAVARRO: Ouch. Snapchat - over it. URIARTE: It's the app that I open, like, every month or so to see if there's anything - and usually nothing anymore. I get a snapchat from, like, Team Snapchat on the holiday. GARCIA-NAVARRO: Yikes. But there is something keeping her tethered to her phone. URIARTE: I have a new app addiction. GARCIA-NAVARRO: OK. Show me. URIARTE: So this is TikTok. And the first thing is this. . . GARCIA-NAVARRO: Oh, my God. What is this? URIARTE: . . . Cosplay - Minzy Dog (ph), a thing I've never heard of. But she's doing what is called the meow dance. And it's this - it's like a. . . GARCIA-NAVARRO: (Laughter) It's ridiculous. I don't even know how to describe this. It is, like, so truly scary. URIARTE: But I find it hysterical. GARCIA-NAVARRO: TikTok is taking the younger generations by storm. I'm not going to have Alex explain TikTok. For that, I'm bringing in Brittany Spanos. She's a writer for Rolling Stone. She spent a week on the app, and she wrote about it. And she joins us now to explain. Hello. BRITTANY SPANOS: Hi. Thank you for having me. GARCIA-NAVARRO: All right. I saw TikTok. I have trouble explaining TikTok. Help me explain TikTok (laughter). SPANOS: Well, TikTok is kind of a culmination of every viral video app that's existed in the last five, maybe even 10 years - kind of going back to early YouTube. And it's comedy. It's music. It's sometimes makeup and sometimes monologuing. And it's a weird hodgepodge of every sort of viral thing that could happen in a short-form video app. GARCIA-NAVARRO: And they're really surreal. They're these psychedelic videos, a lot of cosplay. What's the appeal? SPANOS: It's really a meme incubator. There's a lot of trends happening, a lot of trends around maybe songs or maybe specific styles of comedic video or different forms of making the videos. For example. . . GARCIA-NAVARRO: The meow dance. SPANOS: Yeah. And it's just very easily digestible. I kind of like the weirdness of it. It was just very eclectic. GARCIA-NAVARRO: As we heard from my niece there, social media is going through a massive transformation. Facebook can be used by everyone, even Grandma. But when I showed TikTok to my 20-something producers, they thought that they were too old for it. It feels distinctly for the very young. SPANOS: Yeah. It moves so quickly. And especially with the way that memes move now, the way that Internet content moves now, it moves just as fast as that does - for the user that is high school age that's kind of going through three to four to five different social media platforms at once and absorbing many different viral moments concurrently. And that's what TikTok is. GARCIA-NAVARRO: What does that say about the social media future we will inhabit? I mean, do you think different generations will not be able to even understand each other and will be in these kind of completely different social media universes? SPANOS: Absolutely. I mean, that's been happening for years now. I mean, we're seeing so much stuff that's very aimed for a younger audience that's meant to appeal to them, that's meant to kind of give them this coded language of their own and reflect their certain interests. And so I definitely think it's going to only get bigger. And things like that are going to exist tenfold in the future. GARCIA-NAVARRO: Which was your favorite TikTok? SPANOS: My favorite TikTok was one that was set to Adele's \"Someone Like You. \" There was a goldfish in a bowl. And then as the camera panned out, it was a live version of \"Someone Like You. \" So the entire audience was singing the chorus. And then there was hundreds of goldfish crackers surrounding the alone, live goldfish in a bowl. GARCIA-NAVARRO: (Laughter). SPANOS: And it was great (laughter). GARCIA-NAVARRO: Brittany Spanos, a writer for Rolling Stone, thank you very much. SPANOS: Thank you. (SOUNDBITE OF SONG, \"SOMEONE LIKE YOU\") ADELE: (Singing) Never mind, I'll find someone like you. I wish nothing but the best for you too. Don't forget me. . .", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-29-718373524": {"title": "8chan's Ties To 2 Shootings Renew Debate Over Internet's Role In Radicalizing : NPR", "url": "https://www.npr.org/2019/04/29/718373524/sites-ties-to-shootings-renews-debate-over-internet-s-role-in-radicalizing-extre", "author": "No author found", "published_date": "2019-04-29", "content": "AUDIE CORNISH, HOST: The manifesto by the suspect of the Chabad Poway shooting was posted on the website 8chan. That's also where the alleged perpetrator of last month's attack in New Zealand announced his attack on the Christchurch mosque. What is 8chan, and why does it attract such extremists? NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: Shortly after the Chabad synagogue shooting, a user posted a joke on the website 8chan. It's a before-and-after picture. The before picture is a goofy-looking white guy. The after picture is presumably the same guy dressed in SWAT gear, busting in through a door with a weapon. Journalist Robert Evans writes for Bellingcat, an online investigative platform. He focuses on how terrorist groups use the Internet to recruit on places like 8chan, a site where you can find communities that discuss Japanese cartoons or \"Game Of Thrones. \" But it's also. . . ROBERT EVANS: A neo-Nazi gathering place on the Internet where young men try to convince each other to commit acts of terrorism. GARSD: 8chan is a website where like-minded users with various interests interact with each other. It particularly appeals to people who feel pushed off of mainstream social media platforms like Facebook and Twitter. Evans says there's this one community whose goal is to radicalize users. They offer tips on weapons, discussions about the best translated version of \"Mein Kampf. \" Also, pictures of mass shooters portrayed as saints. EVANS: There's a lot of these people trying to prepare for what they believe is the inevitable coming race war, and also a lot of these people trying to spark a race war by convincing other people to carry out attacks. GARSD: NPR reached out to 8chan for comment but got no answer. It would be easy to dismiss the site as just another chat room for extremists. But Evans believes the fact that both the alleged Christchurch mosque shooter and the Chabad synagogue shooter in California were on 8chan is no accident. EVANS: You look back in 2006, 2007, when you had neo-Nazis or KKK members gather, 20 people would be a large gathering. And then you get to the point where in 2017, hundreds of them were marching in Charlottesville. That is not a coincidence. The Internet is creating these people. GARSD: Whether the Internet is creating hate groups or just serving as a gathering place, one thing has become clear. What happens online doesn't stay there. Brianna Wu is a software engineer who lives in Massachusetts. In 2014, she was targeted in something called Gamergate, in which men threatened female video game players and developers. The harassment started mainly on 8chan. BRIANNA WU: They threw bricks through my windows. They sent me hundreds upon hundreds of death threats, rape threats. I've had people from 8chan follow me around just to let me know, I'm near you and could hurt you if I wanted to. GARSD: Wu, who is running for Congress, says the solution is simple. WU: We need dedicated FBI agents that understand online culture to look at these kinds of extreme crimes and prosecute them. GARSD: The FBI says on Saturday, it did get several tips that someone was making threatening posts online. Five minutes after the tips came in, the shooting at Chabad synagogue began. Once again, it was too late. Jasmine Garsd, NPR News. AUDIE CORNISH, HOST:  The manifesto by the suspect of the Chabad Poway shooting was posted on the website 8chan. That's also where the alleged perpetrator of last month's attack in New Zealand announced his attack on the Christchurch mosque. What is 8chan, and why does it attract such extremists? NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: Shortly after the Chabad synagogue shooting, a user posted a joke on the website 8chan. It's a before-and-after picture. The before picture is a goofy-looking white guy. The after picture is presumably the same guy dressed in SWAT gear, busting in through a door with a weapon. Journalist Robert Evans writes for Bellingcat, an online investigative platform. He focuses on how terrorist groups use the Internet to recruit on places like 8chan, a site where you can find communities that discuss Japanese cartoons or \"Game Of Thrones. \" But it's also. . . ROBERT EVANS: A neo-Nazi gathering place on the Internet where young men try to convince each other to commit acts of terrorism. GARSD: 8chan is a website where like-minded users with various interests interact with each other. It particularly appeals to people who feel pushed off of mainstream social media platforms like Facebook and Twitter. Evans says there's this one community whose goal is to radicalize users. They offer tips on weapons, discussions about the best translated version of \"Mein Kampf. \" Also, pictures of mass shooters portrayed as saints. EVANS: There's a lot of these people trying to prepare for what they believe is the inevitable coming race war, and also a lot of these people trying to spark a race war by convincing other people to carry out attacks. GARSD: NPR reached out to 8chan for comment but got no answer. It would be easy to dismiss the site as just another chat room for extremists. But Evans believes the fact that both the alleged Christchurch mosque shooter and the Chabad synagogue shooter in California were on 8chan is no accident. EVANS: You look back in 2006, 2007, when you had neo-Nazis or KKK members gather, 20 people would be a large gathering. And then you get to the point where in 2017, hundreds of them were marching in Charlottesville. That is not a coincidence. The Internet is creating these people. GARSD: Whether the Internet is creating hate groups or just serving as a gathering place, one thing has become clear. What happens online doesn't stay there. Brianna Wu is a software engineer who lives in Massachusetts. In 2014, she was targeted in something called Gamergate, in which men threatened female video game players and developers. The harassment started mainly on 8chan. BRIANNA WU: They threw bricks through my windows. They sent me hundreds upon hundreds of death threats, rape threats. I've had people from 8chan follow me around just to let me know, I'm near you and could hurt you if I wanted to. GARSD: Wu, who is running for Congress, says the solution is simple. WU: We need dedicated FBI agents that understand online culture to look at these kinds of extreme crimes and prosecute them. GARSD: The FBI says on Saturday, it did get several tips that someone was making threatening posts online. Five minutes after the tips came in, the shooting at Chabad synagogue began. Once again, it was too late. Jasmine Garsd, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-30-718546541": {"title": "Drone Transports Human Kidney For Transplant  : NPR", "url": "https://www.npr.org/2019/04/30/718546541/drone-transports-human-kidney-for-transplant", "author": "No author found", "published_date": "2019-04-30", "content": "RACHEL MARTIN, HOST: Good morning. I'm Rachel Martin. The University of Maryland Medical Center got a special delivery by drone - a human kidney for transplant. It was a short flight, but the drone was custom-built to be able to monitor the precious payload while in the air. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: Temperature's appropriate. Organ doesn't appear to be injured at all. Looks like a perfectly transplantable organ. MARTIN: It was. The kidney made it into a woman from Baltimore who had spent eight years on dialysis. She said, this whole thing is amazing. Yep. It's MORNING EDITION. RACHEL MARTIN, HOST:  Good morning. I'm Rachel Martin. The University of Maryland Medical Center got a special delivery by drone - a human kidney for transplant. It was a short flight, but the drone was custom-built to be able to monitor the precious payload while in the air. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: Temperature's appropriate. Organ doesn't appear to be injured at all. Looks like a perfectly transplantable organ. MARTIN: It was. The kidney made it into a woman from Baltimore who had spent eight years on dialysis. She said, this whole thing is amazing. Yep. It's MORNING EDITION.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-04-30-717233058": {"title": "Even With Robots, Manufacturers Need The Human Touch : NPR", "url": "https://www.npr.org/2019/04/30/717233058/even-in-the-robot-age-manufacturers-need-the-human-touch", "author": "No author found", "published_date": "2019-04-30", "content": "RACHEL MARTIN, HOST: Robots have revolutionized auto manufacturing, but they've hardly replaced the human touch. NPR's Camila Domonoske reports. CAMILA DOMONOSKE, BYLINE: At Volvo's new plant in Ridgeville, S. C. , a half dozen robot arms move in coordination behind a safety fence. They're spot welding a car's body together. Eventually, this will be an S60 - a luxury sedan. (SOUNDBITE OF MACHINE WHIRRING)DOMONOSKE: A small cluster of sparks flies up. We are at the very start of the production line. . . (SOUNDBITE OF GEARS GRINDING)DOMONOSKE: . . . Where metal components are combined to form the car's body. Here, automation dominates. There are more robots than people in this building. And right here, where the robots are welding roofs together, it's dimly lit. Robots don't need much light to work. Jeff Moore is the head of manufacturing for Volvo in America. He says when you're thinking about what jobs to assign to a robot, you start with work that's repetitive, especially if there are safety concerns. JEFF MOORE: With all the heat and sparks and the high current and things like that associated with welding, that's a natural spot to be looking at where you can more heavily automate. DOMONOSKE: But follow the car body as it moves down the assembly line and soon enough, the lights come up and humans take over. At the other end of this building, people run their hands over the surface of the metal, feeling for imperfections. There are some things robots are better at than people. They're precise and consistent. But there are other things people are better at. Humans are underrated, Tesla CEO Elon Musk tweeted last year after Tesla tried to switch to an extremely automated system. Ultimately, the company gave jobs back to people. And we're good at more than just testing how things look and feel. Here at the Volvo plant in another vast building, a long line of people are preparing engines to go inside the cars. This work involves a lot of fiddly parts with odd shapes, which need to be threaded together or moved around in complex ways. Robots are bad at these fine motor skills. People are great at it. And this line handles different engines - gas or hybrid, all-wheel drive, eventually electric motors. They all take different parts. Humans are good at switching back and forth. Robots - not so much. Jason Dodgins, working on this line, used to work at a plant that made bearings. That was less hands-on, he says. JASON DODGINS: The machine did a lot of the actual labor part. You were basically doing the inspection. This has a lot more manual labor to it. DOMONOSKE: Then there's problem-solving. Skip ahead down the line and workers have put together the engine, transmission, axles - everything to make the car go. The car body is waiting on an elevated conveyor. Now the two need to come together. Trey Yonce helps set up the line where this marriage happens. TREY YONCE: If this radiator is not pushed back far enough, it will crash with the body. DOMONOSKE: At first, workers had to fix the placement again and again. Now, a robot could see that problem, but it won't get annoyed by it. People? YONCE: They just got tired of doing it. And a fellow just came up with an idea, and it worked. DOMONOSKE: He points to a little yellow piece of plastic. It holds the radiator in place to prevent that crash. Jeff Moore says Volvo has already applied for multiple patents based on ideas that came from workers at this new plant. (SOUNDBITE OF MACHINERY)DOMONOSKE: Humans have strengths compared to robots in all sorts of workplaces, not just auto plants. And in general, people and robots work best together, with robots handling dangerous, monotonous jobs and precision work while people switch between tasks and make decisions. And there's a sort of philosophical lesson here. Susan Helper is an economist at Case Western Reserve University. SUSAN HELPER: People often think of manufacturing workers as actually a poor substitute for a robot. DOMONOSKE: People complain. They get tired. HELPER: So, gee, wouldn't robots be better? DOMONOSKE: That's a fundamental misunderstanding, she says. HELPER: But in practice, these things are really difficult. And the assembly line worker is making judgments a lot. And it turns out that when you take that person away, you end up with some problems that are hard to solve. DOMONOSKE: Historically, Helper says, some factories have tried to treat their workers like robots - doing repetitive work without thinking. The best thing robots can do is not replace people but free them up to work like people. Camila Domonoske, NPR News, Ridgeville, S. C. (SOUNDBITE OF EMERALDS' \"GENETIC\") RACHEL MARTIN, HOST:  Robots have revolutionized auto manufacturing, but they've hardly replaced the human touch. NPR's Camila Domonoske reports. CAMILA DOMONOSKE, BYLINE: At Volvo's new plant in Ridgeville, S. C. , a half dozen robot arms move in coordination behind a safety fence. They're spot welding a car's body together. Eventually, this will be an S60 - a luxury sedan. (SOUNDBITE OF MACHINE WHIRRING) DOMONOSKE: A small cluster of sparks flies up. We are at the very start of the production line. . . (SOUNDBITE OF GEARS GRINDING) DOMONOSKE: . . . Where metal components are combined to form the car's body. Here, automation dominates. There are more robots than people in this building. And right here, where the robots are welding roofs together, it's dimly lit. Robots don't need much light to work. Jeff Moore is the head of manufacturing for Volvo in America. He says when you're thinking about what jobs to assign to a robot, you start with work that's repetitive, especially if there are safety concerns. JEFF MOORE: With all the heat and sparks and the high current and things like that associated with welding, that's a natural spot to be looking at where you can more heavily automate. DOMONOSKE: But follow the car body as it moves down the assembly line and soon enough, the lights come up and humans take over. At the other end of this building, people run their hands over the surface of the metal, feeling for imperfections. There are some things robots are better at than people. They're precise and consistent. But there are other things people are better at. Humans are underrated, Tesla CEO Elon Musk tweeted last year after Tesla tried to switch to an extremely automated system. Ultimately, the company gave jobs back to people. And we're good at more than just testing how things look and feel. Here at the Volvo plant in another vast building, a long line of people are preparing engines to go inside the cars. This work involves a lot of fiddly parts with odd shapes, which need to be threaded together or moved around in complex ways. Robots are bad at these fine motor skills. People are great at it. And this line handles different engines - gas or hybrid, all-wheel drive, eventually electric motors. They all take different parts. Humans are good at switching back and forth. Robots - not so much. Jason Dodgins, working on this line, used to work at a plant that made bearings. That was less hands-on, he says. JASON DODGINS: The machine did a lot of the actual labor part. You were basically doing the inspection. This has a lot more manual labor to it. DOMONOSKE: Then there's problem-solving. Skip ahead down the line and workers have put together the engine, transmission, axles - everything to make the car go. The car body is waiting on an elevated conveyor. Now the two need to come together. Trey Yonce helps set up the line where this marriage happens. TREY YONCE: If this radiator is not pushed back far enough, it will crash with the body. DOMONOSKE: At first, workers had to fix the placement again and again. Now, a robot could see that problem, but it won't get annoyed by it. People? YONCE: They just got tired of doing it. And a fellow just came up with an idea, and it worked. DOMONOSKE: He points to a little yellow piece of plastic. It holds the radiator in place to prevent that crash. Jeff Moore says Volvo has already applied for multiple patents based on ideas that came from workers at this new plant. (SOUNDBITE OF MACHINERY) DOMONOSKE: Humans have strengths compared to robots in all sorts of workplaces, not just auto plants. And in general, people and robots work best together, with robots handling dangerous, monotonous jobs and precision work while people switch between tasks and make decisions. And there's a sort of philosophical lesson here. Susan Helper is an economist at Case Western Reserve University. SUSAN HELPER: People often think of manufacturing workers as actually a poor substitute for a robot. DOMONOSKE: People complain. They get tired. HELPER: So, gee, wouldn't robots be better? DOMONOSKE: That's a fundamental misunderstanding, she says. HELPER: But in practice, these things are really difficult. And the assembly line worker is making judgments a lot. And it turns out that when you take that person away, you end up with some problems that are hard to solve. DOMONOSKE: Historically, Helper says, some factories have tried to treat their workers like robots - doing repetitive work without thinking. The best thing robots can do is not replace people but free them up to work like people. Camila Domonoske, NPR News, Ridgeville, S. C. (SOUNDBITE OF EMERALDS' \"GENETIC\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-01-718662899": {"title": "Hotels, Home-Rental Sites Compete For Each Others' Business : NPR", "url": "https://www.npr.org/2019/05/01/718662899/lodging-look-alikes-hotels-airbnb-check-into-each-others-turf", "author": "No author found", "published_date": "2019-05-01", "content": "AILSA CHANG, HOST: When you travel and you need a place to stay, most of us make a choice - a hotel room or, increasingly, a space in someone else's home or apartment. But the line between those two options is blurring, and now hotel chains and home-sharing sites are eyeing each other's turf, as NPR's Yuki Noguchi reports. YUKI NOGUCHI, BYLINE: The array of places for travelers to stay keeps expanding, ranging from a yurt in Utah's Zion backcountry to a mansion overlooking the Amalfi Coast to the standard double-queen room. Lorraine Sileo is senior vice president of research for Phocuswright, which tracks the travel industry. She says the lines between hotels and home stays are changing. LORRAINE SILEO: You really do need to think of it as lodging because that is where this convergence is happening. And I don't think we're going to have these definitions a few years from now. NOGUCHI: Just this week, Marriott and Airbnb offered two stark examples of this. Marriott is expanding its luxury home rental business, and Airbnb said it would develop 10 floors of Rockefeller Center to offer a hotel-like service. Already, search engines like Expedia and Booking. com display both hotel rooms and short-term rentals side by side in their search results. SILEO: Consumers are just getting more and more inventory and more choice at all different price points. NOGUCHI: Marriott's latest move is focused exclusively on high-end properties - lavish glass houses with infinity pools or Caribbean island villas overlooking blue oceanfront. Marriott's rental inventory is 2,000 homes - tiny compared to Airbnb's 6 million rental properties or 2 million on Expedia. But the appeal of home-sharing is that it's growing at twice the rate of hotels. And Marriott isn't alone chasing those dollars; other major hotel brands like Wyndham and Choice Hotels have similar rental offerings. Stephanie Linnartz, Marriott's chief commercial officer, acknowledges her company is changing in response to consumer demand. STEPHANIE LINNARTZ: I think what we're doing here is another example of a 91-year-old company kind of disrupting itself. NOGUCHI: Airbnb's model grew rapidly by offering a variety of cheaper and sometimes offbeat alternatives to hotels - single bedrooms in a converted garage, for example, or a houseboat docked at a marina. Airbnb's properties now outnumber the total number of hotel rooms in the U. S. But Airbnb spokesman Chris Lehane says there's still room for home-sharing to grow. CHRIS LEHANE: We've always said that there is a really big and growing pie for this particular space. NOGUCHI: And the competition goes both ways - in addition to its Rockefeller Center development, Airbnb recently acquired the booking site Hotel Tonight. But competing on each other's turf isn't easy. Neither wants to give up the advantages that made them successful in the first place. For big chains like Marriott, that means they can't afford to alienate their franchises, who don't want to see the company invest too heavily in a rival business line. Dan Wasiolek is a travel industry analyst with Morningstar. DAN WASIOLEK: Marriott does have to be aware, as I'm sure they are, that it potentially, on the margin, could cannibalize their traditional hotel bookings business. NOGUCHI: At the same time, he says Airbnb doesn't want to become just another site offering cookie-cutter rooms; that, he said, would put them at risk of losing their identity as a source of unique and distinctive places to stay. Yuki Noguchi, NPR News, Washington. (SOUNDBITE OF MUSIC) AILSA CHANG, HOST:  When you travel and you need a place to stay, most of us make a choice - a hotel room or, increasingly, a space in someone else's home or apartment. But the line between those two options is blurring, and now hotel chains and home-sharing sites are eyeing each other's turf, as NPR's Yuki Noguchi reports. YUKI NOGUCHI, BYLINE: The array of places for travelers to stay keeps expanding, ranging from a yurt in Utah's Zion backcountry to a mansion overlooking the Amalfi Coast to the standard double-queen room. Lorraine Sileo is senior vice president of research for Phocuswright, which tracks the travel industry. She says the lines between hotels and home stays are changing. LORRAINE SILEO: You really do need to think of it as lodging because that is where this convergence is happening. And I don't think we're going to have these definitions a few years from now. NOGUCHI: Just this week, Marriott and Airbnb offered two stark examples of this. Marriott is expanding its luxury home rental business, and Airbnb said it would develop 10 floors of Rockefeller Center to offer a hotel-like service. Already, search engines like Expedia and Booking. com display both hotel rooms and short-term rentals side by side in their search results. SILEO: Consumers are just getting more and more inventory and more choice at all different price points. NOGUCHI: Marriott's latest move is focused exclusively on high-end properties - lavish glass houses with infinity pools or Caribbean island villas overlooking blue oceanfront. Marriott's rental inventory is 2,000 homes - tiny compared to Airbnb's 6 million rental properties or 2 million on Expedia. But the appeal of home-sharing is that it's growing at twice the rate of hotels. And Marriott isn't alone chasing those dollars; other major hotel brands like Wyndham and Choice Hotels have similar rental offerings. Stephanie Linnartz, Marriott's chief commercial officer, acknowledges her company is changing in response to consumer demand. STEPHANIE LINNARTZ: I think what we're doing here is another example of a 91-year-old company kind of disrupting itself. NOGUCHI: Airbnb's model grew rapidly by offering a variety of cheaper and sometimes offbeat alternatives to hotels - single bedrooms in a converted garage, for example, or a houseboat docked at a marina. Airbnb's properties now outnumber the total number of hotel rooms in the U. S. But Airbnb spokesman Chris Lehane says there's still room for home-sharing to grow. CHRIS LEHANE: We've always said that there is a really big and growing pie for this particular space. NOGUCHI: And the competition goes both ways - in addition to its Rockefeller Center development, Airbnb recently acquired the booking site Hotel Tonight. But competing on each other's turf isn't easy. Neither wants to give up the advantages that made them successful in the first place. For big chains like Marriott, that means they can't afford to alienate their franchises, who don't want to see the company invest too heavily in a rival business line. Dan Wasiolek is a travel industry analyst with Morningstar. DAN WASIOLEK: Marriott does have to be aware, as I'm sure they are, that it potentially, on the margin, could cannibalize their traditional hotel bookings business. NOGUCHI: At the same time, he says Airbnb doesn't want to become just another site offering cookie-cutter rooms; that, he said, would put them at risk of losing their identity as a source of unique and distinctive places to stay. Yuki Noguchi, NPR News, Washington. (SOUNDBITE OF MUSIC)", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-02-718250111": {"title": "'Hacking Darwin' Explores Genetic Engineering \u2014 And What It Means To Be Human : NPR", "url": "https://www.npr.org/2019/05/02/718250111/hacking-darwin-explores-genetic-engineering-and-what-it-means-to-be-human", "author": "No author found", "published_date": "2019-05-02", "content": "", "section": "Book Reviews", "disclaimer": ""}, "2019-05-02-719337356": {"title": "Cellphone Searches By Border Agents Violate Constitution, Lawsuit Says : NPR", "url": "https://www.npr.org/2019/05/02/719337356/aclu-border-agents-violate-constitution-when-they-search-electronic-devices", "author": "No author found", "published_date": "2019-05-02", "content": "", "section": "National", "disclaimer": ""}, "2019-05-02-718270183": {"title": "With Major Contracts Ahead of 2020, Voting Machine Industry Comes Under Scrutiny : NPR", "url": "https://www.npr.org/2019/05/02/718270183/trips-to-vegas-and-chocolate-covered-pretzels-election-vendors-come-under-scruti", "author": "No author found", "published_date": "2019-05-02", "content": "DAVID GREENE, HOST: Election officials are expecting to spend hundreds of millions of dollars on new voting equipment before next year's elections. This is the biggest wave of purchases in over a decade. And it comes at a time of increased concern about election security. It also comes amid questions about the close relationship between voting machine vendors and the officials who are buying their equipment. And to talk about this, I'm joined by NPR's Pam Fessler, who covers the issue. Hi there, Pam. PAM FESSLER, BYLINE: Hi, David. GREENE: Let me start with a basic question. Why now are we seeing so many voting machines being purchased? FESSLER: Well, there are two reasons. First, a lot of the existing equipment that state and local election officials now use was purchased after the 2000 elections. And it's just getting old. It needs to be replaced. The second reason is security. Especially in light of concerns about Russian hacking attempts, there's this nationwide push to replace all paperless voting machines, which are still used in about a dozen states, with ones that produce paper ballots. These paper ballots can always be counted to make sure that the results are accurate in case there are any concerns. So we are seeing entire states like Georgia, South Carolina and Delaware getting all new equipment. And some big states like California, Pennsylvania and Ohio are also buying new machines. GREENE: OK. So why are people raising alarm bells about how these companies - or which companies are selling these machines? FESSLER: It's a really, really small industry. Three vendors dominate the market. We have Election Systems & Software known as ES&S, then Dominion Voting Systems and Hart InterCivic. And it's a really small community. The vendors, the election officials, they all know each other. They've been working together for years. They go to conferences together. Sometimes, the vendors sponsor the conferences. And the companies also hire former election officials to work for them. There was a big controversy last year when it was revealed that a number of election officials were serving on an ES&S customer advisory board. GREENE: Oh, wow. FESSLER: The purpose was to discuss voting trends and to, you know, share information. But it also meant that ES&S was paying thousands of dollars for some of these officials to travel to cities such as Las Vegas for meetings. And that raised some eyebrows, you know, especially at a time when states are spending all this money to buy new equipment. GREENE: I mean, you want to assume that election officials are making decisions based on what's best for voters and for election security in the country. Is there any evidence that these ties have had some sort of negative influence over these decisions? FESSLER: I haven't seen any evidence of, like, a quid pro quo. But I do think it's about appearances. And we're at a time when officials are trying to instill public confidence to make people feel more comfortable in the integrity of our elections, that there's nothing nefarious going on. But we have cases where recently, Pennsylvania's auditor general asked all the county election officials in the state if they'd accepted gifts from vendors, and 18 said that they had. These gifts ranged from, you know, trips paid to Las Vegas but also to things like, you know, boxes of chocolate-covered pretzels. But the auditor general said even small gifts, quote, \"smacks of impropriety. \" And this has become a really, really big issue in Georgia, where reporter Johnny Kauffman of member station WABE has been covering the debate over what new voting machines the state should buy. WILL WESLEY: Once they sign in, they're issued an activation card. The voter would insert that in the machine. JOHNNY KAUFFMAN, BYLINE: Will Wesley and I are standing in a crowded room near the Georgia Statehouse, looking at a table covered with touchscreen computers and some printers. Wesley is with the company ES&S. And he and some reps from other companies are here to show off voting equipment. Wesley chooses his favorite candidates by tapping one of ES&S's touch screens. WESLEY: Now I'm going to go ahead and mark my card. KAUFFMAN: The printer spits out a paper ballot with his selections he calls a card. Wesley picks up the piece of paper that just printed, looks at it and slides it into a scanner. WESLEY: Doesn't matter - upside-down, backwards. It scans it. It takes approximately two to three seconds for it to scan. Thank you for voting. Your ballot's been counted. And it drops down into the ballot box. KAUFFMAN: Earlier this year around the time of this demonstration, the state legislature was in session. And after the contested 2018 midterms in Georgia, election policy debates got especially heated. Republicans were pushing to change the law so the state would buy touch-screen voting machines, the kind ES&S and other companies are competing to sell the state. Republicans say the machines eliminate confusion over who voters intended to cast their ballot for. And they point out that election officials want the machines. But Democrats argued the voting machines are vulnerable to hacks and malfunction. (SOUNDBITE OF ARCHIVED RECORDING)ELENA PARENT: Come on. This is a joke. KAUFFMAN: That's Democratic state senator Elena Parent. Parent and other Democrats unsuccessfully argued for hand-marked paper ballots instead of the machines. During debate in the legislature, they brought up ES&S again and again, alleging corruption between the company and GOP officials. (SOUNDBITE OF ARCHIVED RECORDING)PARENT: I've been given absolutely no good reason why we should buy these things. There's not one good reason. So therefore, it just reeks of corruption that we're prioritizing vendors over voters. KAUFFMAN: There's no evidence of any rule-breaking. But there is a long list of ES&S staff and contractors with relationships to Georgia officials. Among them, the company's VP of government relations used to be the director of elections in the secretary of state's office. And a former ES&S lobbyist is the deputy chief of staff to Gov. Brian Kemp. Jeb Cameron is the regional sales manager in Georgia for ES&S, and he also used to work for the state. JEB CAMERON: Of course we've built relationships in the state. I would say we wouldn't be doing our job if we didn't build those close relationships. KAUFFMAN: Georgia is set to buy new voting machines ahead of 2020. Secretary of State Brad Raffensperger's office will award the contract. Here he is at a press conference earlier this year. (SOUNDBITE OF PRESS CONFERENCE)BRAD RAFFENSPERGER: I want to make sure that Georgians get the best possible value for what we're going to buy that's going to last us for the next 10 to 12 years. KAUFFMAN: Raffensperger himself has few connections to ES&S, but he ran on the same GOP ticket as Gov. Brian Kemp, who does. I wanted to ask Raffensperger how he can guarantee this election process will be fair, but his staff didn't make him available for an interview or answer questions about how it's picking the new machines. GREENE: Johnny Kauffman there giving us a window into this close relationship between election officials and these companies in the state of Georgia. NPR's Pam Fessler - still with me - she covers this around the country. Pam, what can be done to assure voters in the time that election security is such a big issue - you know, reassuring them that officials are making the best decisions here when it comes to buying equipment? FESSLER: I think it's all about transparency. Some members of Congress have called for more federal oversight of vendors. But so far, those proposals haven't gone anywhere. State and local governments, which run elections, they don't want the federal government telling them what to do. But I do think, because there's so much concern about election security, that these deals are going to get a lot more public scrutiny in the coming year. And the vendors and the election officials are well aware of that. For its part, ES&S says it's no longer going to have those advisory board meetings planned because they don't want to put election officials in a compromising position. GREENE: I see - already taking some steps. All right, NPR's Pam Fessler. Pam, we appreciate it. FESSLER: Thanks. DAVID GREENE, HOST:  Election officials are expecting to spend hundreds of millions of dollars on new voting equipment before next year's elections. This is the biggest wave of purchases in over a decade. And it comes at a time of increased concern about election security. It also comes amid questions about the close relationship between voting machine vendors and the officials who are buying their equipment. And to talk about this, I'm joined by NPR's Pam Fessler, who covers the issue. Hi there, Pam. PAM FESSLER, BYLINE: Hi, David. GREENE: Let me start with a basic question. Why now are we seeing so many voting machines being purchased? FESSLER: Well, there are two reasons. First, a lot of the existing equipment that state and local election officials now use was purchased after the 2000 elections. And it's just getting old. It needs to be replaced. The second reason is security. Especially in light of concerns about Russian hacking attempts, there's this nationwide push to replace all paperless voting machines, which are still used in about a dozen states, with ones that produce paper ballots. These paper ballots can always be counted to make sure that the results are accurate in case there are any concerns. So we are seeing entire states like Georgia, South Carolina and Delaware getting all new equipment. And some big states like California, Pennsylvania and Ohio are also buying new machines. GREENE: OK. So why are people raising alarm bells about how these companies - or which companies are selling these machines? FESSLER: It's a really, really small industry. Three vendors dominate the market. We have Election Systems & Software known as ES&S, then Dominion Voting Systems and Hart InterCivic. And it's a really small community. The vendors, the election officials, they all know each other. They've been working together for years. They go to conferences together. Sometimes, the vendors sponsor the conferences. And the companies also hire former election officials to work for them. There was a big controversy last year when it was revealed that a number of election officials were serving on an ES&S customer advisory board. GREENE: Oh, wow. FESSLER: The purpose was to discuss voting trends and to, you know, share information. But it also meant that ES&S was paying thousands of dollars for some of these officials to travel to cities such as Las Vegas for meetings. And that raised some eyebrows, you know, especially at a time when states are spending all this money to buy new equipment. GREENE: I mean, you want to assume that election officials are making decisions based on what's best for voters and for election security in the country. Is there any evidence that these ties have had some sort of negative influence over these decisions? FESSLER: I haven't seen any evidence of, like, a quid pro quo. But I do think it's about appearances. And we're at a time when officials are trying to instill public confidence to make people feel more comfortable in the integrity of our elections, that there's nothing nefarious going on. But we have cases where recently, Pennsylvania's auditor general asked all the county election officials in the state if they'd accepted gifts from vendors, and 18 said that they had. These gifts ranged from, you know, trips paid to Las Vegas but also to things like, you know, boxes of chocolate-covered pretzels. But the auditor general said even small gifts, quote, \"smacks of impropriety. \" And this has become a really, really big issue in Georgia, where reporter Johnny Kauffman of member station WABE has been covering the debate over what new voting machines the state should buy. WILL WESLEY: Once they sign in, they're issued an activation card. The voter would insert that in the machine. JOHNNY KAUFFMAN, BYLINE: Will Wesley and I are standing in a crowded room near the Georgia Statehouse, looking at a table covered with touchscreen computers and some printers. Wesley is with the company ES&S. And he and some reps from other companies are here to show off voting equipment. Wesley chooses his favorite candidates by tapping one of ES&S's touch screens. WESLEY: Now I'm going to go ahead and mark my card. KAUFFMAN: The printer spits out a paper ballot with his selections he calls a card. Wesley picks up the piece of paper that just printed, looks at it and slides it into a scanner. WESLEY: Doesn't matter - upside-down, backwards. It scans it. It takes approximately two to three seconds for it to scan. Thank you for voting. Your ballot's been counted. And it drops down into the ballot box. KAUFFMAN: Earlier this year around the time of this demonstration, the state legislature was in session. And after the contested 2018 midterms in Georgia, election policy debates got especially heated. Republicans were pushing to change the law so the state would buy touch-screen voting machines, the kind ES&S and other companies are competing to sell the state. Republicans say the machines eliminate confusion over who voters intended to cast their ballot for. And they point out that election officials want the machines. But Democrats argued the voting machines are vulnerable to hacks and malfunction. (SOUNDBITE OF ARCHIVED RECORDING) ELENA PARENT: Come on. This is a joke. KAUFFMAN: That's Democratic state senator Elena Parent. Parent and other Democrats unsuccessfully argued for hand-marked paper ballots instead of the machines. During debate in the legislature, they brought up ES&S again and again, alleging corruption between the company and GOP officials. (SOUNDBITE OF ARCHIVED RECORDING) PARENT: I've been given absolutely no good reason why we should buy these things. There's not one good reason. So therefore, it just reeks of corruption that we're prioritizing vendors over voters. KAUFFMAN: There's no evidence of any rule-breaking. But there is a long list of ES&S staff and contractors with relationships to Georgia officials. Among them, the company's VP of government relations used to be the director of elections in the secretary of state's office. And a former ES&S lobbyist is the deputy chief of staff to Gov. Brian Kemp. Jeb Cameron is the regional sales manager in Georgia for ES&S, and he also used to work for the state. JEB CAMERON: Of course we've built relationships in the state. I would say we wouldn't be doing our job if we didn't build those close relationships. KAUFFMAN: Georgia is set to buy new voting machines ahead of 2020. Secretary of State Brad Raffensperger's office will award the contract. Here he is at a press conference earlier this year. (SOUNDBITE OF PRESS CONFERENCE) BRAD RAFFENSPERGER: I want to make sure that Georgians get the best possible value for what we're going to buy that's going to last us for the next 10 to 12 years. KAUFFMAN: Raffensperger himself has few connections to ES&S, but he ran on the same GOP ticket as Gov. Brian Kemp, who does. I wanted to ask Raffensperger how he can guarantee this election process will be fair, but his staff didn't make him available for an interview or answer questions about how it's picking the new machines. GREENE: Johnny Kauffman there giving us a window into this close relationship between election officials and these companies in the state of Georgia. NPR's Pam Fessler - still with me - she covers this around the country. Pam, what can be done to assure voters in the time that election security is such a big issue - you know, reassuring them that officials are making the best decisions here when it comes to buying equipment? FESSLER: I think it's all about transparency. Some members of Congress have called for more federal oversight of vendors. But so far, those proposals haven't gone anywhere. State and local governments, which run elections, they don't want the federal government telling them what to do. But I do think, because there's so much concern about election security, that these deals are going to get a lot more public scrutiny in the coming year. And the vendors and the election officials are well aware of that. For its part, ES&S says it's no longer going to have those advisory board meetings planned because they don't want to put election officials in a compromising position. GREENE: I see - already taking some steps. All right, NPR's Pam Fessler. Pam, we appreciate it. FESSLER: Thanks.", "section": "Politics", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-03-719946622": {"title": "Facebook Bans Several High-Profile Users : NPR", "url": "https://www.npr.org/2019/05/03/719946622/facebook-bans-several-high-profile-users", "author": "No author found", "published_date": "2019-05-03", "content": "RACHEL MARTIN, HOST: A social media juggernaut says it's no longer OK with some of the things its users say. Facebook has banned several high-profile users known for espousing conspiracy theories and inflammatory rhetoric, among them Alex Jones, who created Infowars, and Louis Farrakhan, the leader of the Nation of Islam. With us to explain more is Washington Post technology reporter Tony Romm. Thanks so much for coming in, Tony. TONY ROMM: Thanks for having me. MARTIN: So there are seven users who've been kicked off, right? ROMM: Yeah, multiple users who have been permanently banned from Facebook. Now, the company has been under pressure for some time now to take action against content - posts, videos and otherwise - that users somewhat find repugnant, things like what Alex Jones and Infowars have been saying, conspiracy theories, attacks on the parents of Sandy Hook Elementary School victims from the shooting a few years ago and so forth. And in some cases, Facebook had imposed smaller time-outs on these individuals when they violated their policies. But the decision that Facebook announced yesterday was that folks like Alex Jones have been permanently banned from the platform. They're not going to be allowed back. And it's a sign from Facebook that it's looking to take a new, more aggressive approach to content that folks find to be kind of disgusting. MARTIN: So Facebook, and seemingly all social media sites, have been accused by conservatives as - of having a liberal bias. How is this decision to kick these people off - how is that going to affect that perception? ROMM: Yeah, Facebook, Twitter, Google, all of them sort of face a very difficult conundrum. On one hand, there are new demands for those companies to take more aggressive action when people or individuals or groups say repugnant things online. On the other hand, when they go too far, people on the opposite side of the political spectrum are willing to needle them for censorship. And that's what we've seen in the context of conservative users who have attacked Facebook and Google and others for being biased against right-leaning causes or news sites or whatever the case may be. These criticisms go right to the top of the White House, where President Trump has been regularly critical of Facebook and Twitter for being biased against conservatives. So it puts these companies in a bind. If you talk to experts, it says it makes them less willing sometimes to be moderators of their own platforms because they don't want to deal with the political criticism. MARTIN: I mean, it's also a question of how you make these decisions, right? I mean, what are the criteria to say that someone is too, quote, \"dangerous\" to be on your site? ROMM: You're exactly right. It's what's the criteria. How far is too far? Is Facebook and Twitter applying their policies equally all of the time, to everybody? They've long been faulted for not being equal in handing out justice on their platforms, so to speak. And then it's the people who do that sort of reviewing in the first place. Remember, it's not just artificial intelligence and powerful software that spots and takes down these posts. At the end of the day, sometimes it's real human beings who are doing the review. And so they face a lot of criticism on allegations that human reviewers can be biased. MARTIN: Meanwhile, you're reporting this morning a different Facebook story, that the company has agreed with the government to greater oversight. What can you tell us? ROMM: Yeah, Facebook has been under investigation here in Washington for about a year now for mishandling users' data after promising the government it would do better in 2011. So that settlement is coming to a close. Facebook had said that it's willing to pay a fine into the billions of dollars. This has been under discussion with the Federal Trade Commission. But as part of that settlement, we could see new checks on individual Facebook executives, the decisions they make, the apps that they put out and so forth. So it would be a major new form of privacy oversight for the company. MARTIN: Tony Romm, a technology reporter at The Washington Post. Tony, we appreciate it. ROMM: Thanks for having me. RACHEL MARTIN, HOST:  A social media juggernaut says it's no longer OK with some of the things its users say. Facebook has banned several high-profile users known for espousing conspiracy theories and inflammatory rhetoric, among them Alex Jones, who created Infowars, and Louis Farrakhan, the leader of the Nation of Islam. With us to explain more is Washington Post technology reporter Tony Romm. Thanks so much for coming in, Tony. TONY ROMM: Thanks for having me. MARTIN: So there are seven users who've been kicked off, right? ROMM: Yeah, multiple users who have been permanently banned from Facebook. Now, the company has been under pressure for some time now to take action against content - posts, videos and otherwise - that users somewhat find repugnant, things like what Alex Jones and Infowars have been saying, conspiracy theories, attacks on the parents of Sandy Hook Elementary School victims from the shooting a few years ago and so forth. And in some cases, Facebook had imposed smaller time-outs on these individuals when they violated their policies. But the decision that Facebook announced yesterday was that folks like Alex Jones have been permanently banned from the platform. They're not going to be allowed back. And it's a sign from Facebook that it's looking to take a new, more aggressive approach to content that folks find to be kind of disgusting. MARTIN: So Facebook, and seemingly all social media sites, have been accused by conservatives as - of having a liberal bias. How is this decision to kick these people off - how is that going to affect that perception? ROMM: Yeah, Facebook, Twitter, Google, all of them sort of face a very difficult conundrum. On one hand, there are new demands for those companies to take more aggressive action when people or individuals or groups say repugnant things online. On the other hand, when they go too far, people on the opposite side of the political spectrum are willing to needle them for censorship. And that's what we've seen in the context of conservative users who have attacked Facebook and Google and others for being biased against right-leaning causes or news sites or whatever the case may be. These criticisms go right to the top of the White House, where President Trump has been regularly critical of Facebook and Twitter for being biased against conservatives. So it puts these companies in a bind. If you talk to experts, it says it makes them less willing sometimes to be moderators of their own platforms because they don't want to deal with the political criticism. MARTIN: I mean, it's also a question of how you make these decisions, right? I mean, what are the criteria to say that someone is too, quote, \"dangerous\" to be on your site? ROMM: You're exactly right. It's what's the criteria. How far is too far? Is Facebook and Twitter applying their policies equally all of the time, to everybody? They've long been faulted for not being equal in handing out justice on their platforms, so to speak. And then it's the people who do that sort of reviewing in the first place. Remember, it's not just artificial intelligence and powerful software that spots and takes down these posts. At the end of the day, sometimes it's real human beings who are doing the review. And so they face a lot of criticism on allegations that human reviewers can be biased. MARTIN: Meanwhile, you're reporting this morning a different Facebook story, that the company has agreed with the government to greater oversight. What can you tell us? ROMM: Yeah, Facebook has been under investigation here in Washington for about a year now for mishandling users' data after promising the government it would do better in 2011. So that settlement is coming to a close. Facebook had said that it's willing to pay a fine into the billions of dollars. This has been under discussion with the Federal Trade Commission. But as part of that settlement, we could see new checks on individual Facebook executives, the decisions they make, the apps that they put out and so forth. So it would be a major new form of privacy oversight for the company. MARTIN: Tony Romm, a technology reporter at The Washington Post. Tony, we appreciate it. ROMM: Thanks for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-03-719103958": {"title": "Mystery Skype Helps Some Educators Teach Geography  : NPR", "url": "https://www.npr.org/2019/05/03/719103958/classes-take-trips-around-the-world-through-this-game", "author": "No author found", "published_date": "2019-05-03", "content": "", "section": "Education", "disclaimer": ""}, "2019-05-03-719897599": {"title": "Facebook Bans White Supremacists And Anti-Semites From Platform : NPR", "url": "https://www.npr.org/2019/05/03/719897599/facebook-bans-alex-jones-louis-farrakhan-and-other-dangerous-individuals", "author": "No author found", "published_date": "2019-05-03", "content": "", "section": "Technology", "disclaimer": ""}, "2019-05-03-719897171": {"title": "Facebook Bans Several Controversial Users From Its Platform : NPR", "url": "https://www.npr.org/2019/05/03/719897171/facebook-bans-several-controversial-users-from-its-platform", "author": "No author found", "published_date": "2019-05-03", "content": "RACHEL MARTIN, HOST: The story right now is about Facebook and who does and does not get to be on that platform. According to Facebook, there is a new list of people who cannot. And that includes right-wing conspiracy theorist Alex Jones, Nation of Islam leader Louis Farrakhan. Facebook has banned them because they are, quote, \"dangerous individuals. \" That's according to Facebook. We should mention that Facebook is a financial supporter of NPR. And this move comes as the social media giant and other social media companies face withering criticism for allowing hate speech and false information to spread and influence their users, especially in the run-up, obviously, to the 2016 elections. Paris Martineau covers social media and Internet culture for Wired magazine and joins us from our studios in New York. PARIS MARTINEAU: Hi. Thanks so much for having me. MARTIN: Hi, Paris. So exactly who are we talking about? Who was banned? MARTINEAU: Yeah. So on Thursday afternoon, Facebook banned six high-profile extremists and one conspiracy theorist site. That includes inflammatory figureheads in the far right like Laura Loomer, Infowars' Alex Jones and Paul Joseph Watson, as well as white supremacist Paul Nehlen and radical Muslim preacher Louis Farrakhan. MARTIN: So President Trump and other conservatives have accused various social media companies for censoring right-wing opinions. Alex Jones, Milo Yiannopoulos, Louis Farrakhan - they've been around for a long time. So why is Facebook doing this now? MARTINEAU: So a Facebook spokesperson told me the company has always banned individuals and organizations that promote or engage in violence and hate, regardless of ideology. But the people and organization banned Thursday were just as extreme and peddled just as much misinformation and hate a year ago as they did Thursday. MARTIN: What's been the fallout from this? MARTINEAU: The fallout has largely been the people banned protesting their ban. There was a bit of a snafu in the fact that Facebook kind of ceded a number of media outlets on Thursday afternoon with this story in advance, then struggled to actually take down the accounts at the time the media outlets published the story, allowing these extremist figureheads to publish on their soon-to-be-banned Instagram accounts - follow me on the other platform. MARTIN: So is this - I mean, is this setting a precedent? Are we going to see Facebook continue to ban accounts? And who makes those decisions? MARTINEAU: Those are all great questions. I think that Facebook is, like Facebook often does, going to do whatever it wants. And it'll be interesting to see how it continues to apply this going forward, whether it'll stick with it. MARTIN: OK. Lots of questions still. Paris Martineau, social media reporter at Wired. Thanks, we appreciate it. MARTINEAU: Thanks so much. RACHEL MARTIN, HOST:  The story right now is about Facebook and who does and does not get to be on that platform. According to Facebook, there is a new list of people who cannot. And that includes right-wing conspiracy theorist Alex Jones, Nation of Islam leader Louis Farrakhan. Facebook has banned them because they are, quote, \"dangerous individuals. \" That's according to Facebook. We should mention that Facebook is a financial supporter of NPR. And this move comes as the social media giant and other social media companies face withering criticism for allowing hate speech and false information to spread and influence their users, especially in the run-up, obviously, to the 2016 elections. Paris Martineau covers social media and Internet culture for Wired magazine and joins us from our studios in New York. PARIS MARTINEAU: Hi. Thanks so much for having me. MARTIN: Hi, Paris. So exactly who are we talking about? Who was banned? MARTINEAU: Yeah. So on Thursday afternoon, Facebook banned six high-profile extremists and one conspiracy theorist site. That includes inflammatory figureheads in the far right like Laura Loomer, Infowars' Alex Jones and Paul Joseph Watson, as well as white supremacist Paul Nehlen and radical Muslim preacher Louis Farrakhan. MARTIN: So President Trump and other conservatives have accused various social media companies for censoring right-wing opinions. Alex Jones, Milo Yiannopoulos, Louis Farrakhan - they've been around for a long time. So why is Facebook doing this now? MARTINEAU: So a Facebook spokesperson told me the company has always banned individuals and organizations that promote or engage in violence and hate, regardless of ideology. But the people and organization banned Thursday were just as extreme and peddled just as much misinformation and hate a year ago as they did Thursday. MARTIN: What's been the fallout from this? MARTINEAU: The fallout has largely been the people banned protesting their ban. There was a bit of a snafu in the fact that Facebook kind of ceded a number of media outlets on Thursday afternoon with this story in advance, then struggled to actually take down the accounts at the time the media outlets published the story, allowing these extremist figureheads to publish on their soon-to-be-banned Instagram accounts - follow me on the other platform. MARTIN: So is this - I mean, is this setting a precedent? Are we going to see Facebook continue to ban accounts? And who makes those decisions? MARTINEAU: Those are all great questions. I think that Facebook is, like Facebook often does, going to do whatever it wants. And it'll be interesting to see how it continues to apply this going forward, whether it'll stick with it. MARTIN: OK. Lots of questions still. Paris Martineau, social media reporter at Wired. Thanks, we appreciate it. MARTINEAU: Thanks so much.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-04-720307836": {"title": "Barbershop: Facebook's Purported Pivot To Privacy : NPR", "url": "https://www.npr.org/2019/05/04/720307836/barbershop-facebook-s-purported-pivot-to-privacy", "author": "No author found", "published_date": "2019-05-04", "content": "MICHEL MARTIN, HOST: Finally today, we're going to talk about some big news out of Silicon Valley. This week, Facebook said it is testing a feature to hide the number of likes on Instagram, which Facebook owns, and to ban content posted by people and groups it considers extremist. Both were described as an effort to deal with increasingly sharp criticism that these social media platforms are creating unhealthy social environments by allowing the spread of false, hateful propaganda, in the case of Facebook, or in the case of Instagram, stimulating negative mental health impacts such as depression and anxiety. We were wondering if these changes will really make social media safer and healthier. Or will they just prevent influencers and bloggers from expressing themselves and making a living? We decided to take that to the Barbershop because that's where we talk to interesting people about what's in the news and what's on their minds. And before we begin, let's tell you that Facebook is a financial sponsor of NPR. with that being said, joining me are Alice Marwick. She is a Professor of Communication at the University of North Carolina, Chapel Hill. Welcome. Thanks for joining us. ALICE MARWICK: It's great to be here. MARTIN: Taylor Lorenz is a staff writer for The Atlantic who often writes about social media. Nice to have you back with us. TAYLOR LORENZ: Hi. MARTIN: And Sara Li is a writer and social media strategist - a former influencer. Sara Li, welcome to you. SARA LI: Thank you so much. MARTIN: So let me start with the Instagram news. They're testing this new feature, as we said, to hide the number of likes a post gets. And I just want to ask each of you briefly - as briefly as you can - is this a step in the right direction? And, professor Marwick, I'll start with you. MARWICK: I think it's an interesting experiment. It's certainly going to remove the ability for people to compare individual posts to each other. And since the research seems to show that people who are prone to comparing themselves to others do feel badly when they use sites like Instagram, I think for people like that it could be a good thing. But the entirety of social media is built around gaining status and attention, so I'm not sure how much it's going to change how people participate online. MARTIN: So, Taylor, I don't know if you feel comfortable hazarding your opinion, given that this is your beat. If you do. . . LORENZ: Oh, sure. MARTIN: I'll let you answer however you feel comfortable. LORENZ: Yeah. No, definitely. I share - yeah, I mean, I like to talk about sort of the different things that Instagram tests frequently. I think it's actually sort of a positive step. But, you know, Instagram Stories, which is one of the most rapidly growing features on Instagram, actually does by default hide metrics from users. So you can't - you know, while you yourself can see how many views a story got, it's not outwardly facing. So I think this is just going to be sort of more in line with that. And I think users will get used to it much more quickly than they think. MARTIN: And, Sara, you know, I'm fascinated by - you know, I'm fascinated by you, of course because you actually were an influencer. That means you were paid to post promoted content. You started that as a high school student, and you eventually left that world. So, you know, you have a foot in both sides of it. I mean, on the one hand, you've seen the way it can - what - how would you describe the negatives? Like, sort of create this pressure on you to kind of perform yourself or - but you also made money from it, so. . . LI: Yeah. MARTIN: Tell me. LI: Yeah, absolutely. I was 17 when I started, you know, doing influencer campaigns. And I think the thing that I realized about it is that it's such an empty goal because you're constantly chasing for that high engagement. You're constantly wanting new followers, more likes, etc. And after a certain point, like, I realized there's never going to be a number that's going to be satisfying. You know, you're never going to get enough followers. You're never going to get enough likes. So at that point, you just kind of have to ask yourself, is this worth it? Is it worth putting on this very highlighted persona on this online personality for the money, for the prestige - for the clout as, you know, the kids call it these days? And it's not. Truly, I don't - I really don't think it's worth it because once - the second that you hit a milestone, you're going to want more, and you're going to be constantly, constantly chasing something that you're never going to be happy with. MARTIN: And what about the idea of hiding the likes? What do you think? Is that a step in the right direction? Or you don't think it'll do any - make any difference. LI: So I really hate to be the cynic here, but while I do you think it is a step in the right direction, I'm not entirely sure that's going to last because the idea that Instagram is hiding these likes to promote better mental health - which I, you know, completely agree is a huge issue, especially considering the influence of social media on our kids these days. However, I think it's kind of a Band-Aid solution. I think the second that Instagram starts hiding their likes, there's going to be another social media app that comes out of the blue to kind of fill that gap, to fill that void because the issue isn't that people are getting their validation from Instagram. The issue is that people are looking at social media period for validation. So I think until the day that we address a solution for that bigger issue, I think Instagram hiding the likes - this is going to be a very temporary solution. MARTIN: So, professor Marwick, can you just talk a little bit more about what the research shows? You know, your research focuses on what you call the attention economy. Could you just amplify our understanding? Is there anything else you think we should know about how this works? MARWICK: Yeah. So in the attention economy, likes and followers function as currency. They're worth something. And obviously, they can be converted into actual money, as in an influencer being paid by a brand to put up a sponsored post, as we've talked about. But in other cases, they function as social currency. They give people feedback. They let them know people are watching and paying attention. And in some cases, I do think this is good. I think it allows people to feel socially supported, that they're part of a community and to maybe participate in a trend or a meme that's going around. And these things are good, I think, for our community building and sort of sense of each other. But I think the problem is when this attention gets converted down to this metric, this single number, it provides this very easy point of comparison. And you can very easily compare yourself to your friends, to celebrities, to influencers. And Instagram also uses those like metrics in its product. It uses the like metrics to rank its feed of photos to recommend which photos it shows first to different users. So the likes are still going to matter to the site. They'll still be there under the hood. They just won't necessarily be as visible to the users. MARTIN: Why wouldn't they just monetize that and just say, OK, you can pay extra to see people's likes, get access to that private information? MARWICK: I mean, it remains to be seen, right? Like, this is a test. I think that they are going to have to implement something for people who are making a living off this platform simply because brands and influencers are so dependent on being able to judge the success of a sponsorship or a campaign through the number of likes and views a post gets. So, you know, I could see them down the line rolling out some kind of tools for business and - or for people who want to sort of interact with each other more professionally on the site. MARTIN: That leads to a question I want to save for the end because now I want to - which is the whole question of whether these platforms have gotten to the point where really they should be treated like utilities, for whatever that means? But I'm going to save that for the end because I want to go to Taylor now on the story that you covered for the Atlantic about this move to Facebook and Instagram banning the profiles of public figures like Infowars founder Alex Jones and Nation of Islam leader Louis Farrakhan. Taylor, you covered this for The Atlantic. What exactly does this entail? LORENZ: OK, so a couple things. First of all, I just want to loop back to something - the ways that you guys were just talking about Instagram previously and push back a little bit. That's really. . . MARTIN: Sure. LORENZ: . . . Not how people use Instagram on a broad scale. I mean, Facebook - I'm sorry, Instagram is over a billion users. It's a very - you know, people express themselves in very complex ways on the app. So while, you know, there's this stereotype that people are just posting photos and trying to get likes, like, it's really not like that, especially for younger users who use it - you know, they're very involved in commenting and group chats. And, you know, Stories, its fastest-growing product, is not at all about likes. It's far less performative. So I think sort of in light of that, you see a lot of people using Instagram in, you know, all of these complex, interesting and sometimes very problematic ways. So, you know, Facebook and Instagram's move to ban these extremist accounts was because, you know, some of these extremist figures were using Instagram to actually, you know, spread misinformation, which is a problem that Facebook has had, too. So I think this is a - you know, a strong step for that. But they have a lot more ways to go. You know, Instagram is not just about photos. And there are also a lot of ways to spread misinformation on Facebook, too. MARTIN: Could you talk a little bit more about that briefly? Like, give us another example of that. LORENZ: Of Instagram. MARTIN: Yeah, how it works. LORENZ: Yeah. Well, so. . . MARTIN: Because you've written extensively about that, too - not just the what but the how - how it works that way. LORENZ: Yeah. So Instagram's this big, you know, messy social network. Obviously, you have the main feed of photos, which people less and less use to just post straight pictures, like the aspirational photos you were talking about earlier. Especially younger users engage in memes. They engage in video content from other platforms, things like that. There's also IGTV, which is essentially a YouTube competitor. There's Stories, which are the short-form video, and GIF and photo formats. So - and then there's comments and group chats and all of these different things - live-streaming. And so, you know, extremist figures can kind of play in all of those different formats, you know, whether it's an Infowars video uploaded to a meme page on IGTV to reach, you know, Gen Z kids or, you know, problematic messages told in visual format through the feed or, you know, an extremist going live. MARTIN: So to that end, though - and I also want to hear from the others on this - but, Taylor, do you - again, to the degree you feel comfortable because you cover this. . . LORENZ: Sure. MARTIN: Is this focusing on these individuals who have very long track records, right, of public utterances that most people I think know - do you think that's the right approach? LORENZ: Well, yeah. I mean, it's the right approach to crack down on extremism as we've seen, you know, the consequences of letting that kind of speech flourish. But, I mean, they're not going far enough. I mean, this is literally, like, four people or five people. So there's a really long way to go on both platforms. MARTIN: Professor Marwick, what do you think about that because I know you study this as well? LORENZ: Well, I think it's a difficult position for a platform like Facebook. I mean, Taylor is completely right. There are - these are enormous, multi-feature platforms with many different ways for people to engage. And, as a result, it's not just coming from the top, right? Like, it's not just coming from Alex Jones. It's coming in private Facebook groups. It's coming in WhatsApp chats. It's coming in all of these different ways that people engage with the platform. So I do think it's one head of the many-headed hydra that you can cut off. But at some point, there - you're going to have to take some kind of stance on what type of content is acceptable and not on the platform. If they choose to do that, I think that's going to be very, very difficult for them because there's going to be an accusation of partisanship. And, as a result, I think they've tried to portray themselves as sort of this, like, neutral third-party because they want to be seen as a platform, right, on which people interact and on which people perform. But when you are drawing a line in the sand and saying, you know, this is the type of content that isn't acceptable, that's going to encompass a range of people, some who are very fringy and some who are a little bit more mainstream. And I think that's a political battle that Facebook is going to have to face as it moves forward with some of these decisions. MARTIN: So that's the question I wanted to sort of conclude with. Does this point to the idea that these platforms - that we should think about them as they're not kind of just neutral entities that are just there, and people just kind of do what - that they're so woven into the fabric of our lives that perhaps we should think about them in a different way. I know that there's a lot of resistance to that for all kinds of reasons. But, you know, are we at the point where they should be thought about like utilities, where the public gets to participate in how these entities interface with us? There are public service commissions. You know, there are boards. There are - do you see my point? It's not just a private matter how they conduct themselves. What do you think about that, as briefly - I know it's a big question - as briefly as you can? So, Taylor, you want to start? LORENZ: Yeah, sure. I mean, I think it's so hard. I mean, if you're going to talk about them becoming public utilities and regulation, you have to really understand how these platforms work and how people communicate on them. And I think that's evolving so rapidly that it's really hard for, you know, lawmakers to even regulate that type of stuff. I'd point to some of the influencer regulations that the FTC has put in place. I mean, that really hasn't done anything to, you know, like, really rein in a lot of this bad spammy influencer marketing. So I think - yeah, I mean, I think it's worth noting, you know, the role that these people - or the role that these platforms play in our lives. But I don't know. MARTIN: It's big. LORENZ: It's so big and so complicated (laughter). MARTIN: Yeah. Sara, what do you think? LI: I'm just really thinking about how that would even work because, like Taylor said, Instagram really isn't just for the sole purpose of just posting a picture and getting likes anymore. We've seen it branch out so much bigger than that. Like I said, it's a method for communication. And now we're seeing media publications use it as a means to communicate with their audience. So just the idea of it becoming kind of a public service - I can see that maybe going into effect if there was a more prominent health safety issue such as, like Taylor mentioned, the spammy influencer marketing where you have people promoting, like, these detox teas that truly are harmful for your body. So I can see that being part of that huge conversation. MARTIN: Professor Marwick, I only gave you 30 seconds to answer this terribly big question, so you'll have to come back and tell us more. But what do you think? MARWICK: It's a - these are global platforms, so even if the U. S. decided to implement regulations, we still have millions - hundreds of millions of users outside the U. S. who are under different regulatory regimes. And when you're talking about regulation, you are talking about something that gets extremely complicated and requires an intimate knowledge of the platform which lawmakers don't necessarily always have. MARTIN: Well, thanks for at least getting us started in trying to understand it. MARWICK: (Laughter). MARTIN: That's Alice Marwick, professor of communication at UNC Chapel Hill, author of \"Status Update: Celebrity, Publicity, And Branding In The Social Media Age,\" Sara Li, social media strategist, and Taylor Lorenz, staff writer for The Atlantic. Thank you all so much for joining us. LI: Thank you. LORENZ: Thank you. MARWICK: Thank you. MICHEL MARTIN, HOST:  Finally today, we're going to talk about some big news out of Silicon Valley. This week, Facebook said it is testing a feature to hide the number of likes on Instagram, which Facebook owns, and to ban content posted by people and groups it considers extremist. Both were described as an effort to deal with increasingly sharp criticism that these social media platforms are creating unhealthy social environments by allowing the spread of false, hateful propaganda, in the case of Facebook, or in the case of Instagram, stimulating negative mental health impacts such as depression and anxiety. We were wondering if these changes will really make social media safer and healthier. Or will they just prevent influencers and bloggers from expressing themselves and making a living? We decided to take that to the Barbershop because that's where we talk to interesting people about what's in the news and what's on their minds. And before we begin, let's tell you that Facebook is a financial sponsor of NPR. with that being said, joining me are Alice Marwick. She is a Professor of Communication at the University of North Carolina, Chapel Hill. Welcome. Thanks for joining us. ALICE MARWICK: It's great to be here. MARTIN: Taylor Lorenz is a staff writer for The Atlantic who often writes about social media. Nice to have you back with us. TAYLOR LORENZ: Hi. MARTIN: And Sara Li is a writer and social media strategist - a former influencer. Sara Li, welcome to you. SARA LI: Thank you so much. MARTIN: So let me start with the Instagram news. They're testing this new feature, as we said, to hide the number of likes a post gets. And I just want to ask each of you briefly - as briefly as you can - is this a step in the right direction? And, professor Marwick, I'll start with you. MARWICK: I think it's an interesting experiment. It's certainly going to remove the ability for people to compare individual posts to each other. And since the research seems to show that people who are prone to comparing themselves to others do feel badly when they use sites like Instagram, I think for people like that it could be a good thing. But the entirety of social media is built around gaining status and attention, so I'm not sure how much it's going to change how people participate online. MARTIN: So, Taylor, I don't know if you feel comfortable hazarding your opinion, given that this is your beat. If you do. . . LORENZ: Oh, sure. MARTIN: I'll let you answer however you feel comfortable. LORENZ: Yeah. No, definitely. I share - yeah, I mean, I like to talk about sort of the different things that Instagram tests frequently. I think it's actually sort of a positive step. But, you know, Instagram Stories, which is one of the most rapidly growing features on Instagram, actually does by default hide metrics from users. So you can't - you know, while you yourself can see how many views a story got, it's not outwardly facing. So I think this is just going to be sort of more in line with that. And I think users will get used to it much more quickly than they think. MARTIN: And, Sara, you know, I'm fascinated by - you know, I'm fascinated by you, of course because you actually were an influencer. That means you were paid to post promoted content. You started that as a high school student, and you eventually left that world. So, you know, you have a foot in both sides of it. I mean, on the one hand, you've seen the way it can - what - how would you describe the negatives? Like, sort of create this pressure on you to kind of perform yourself or - but you also made money from it, so. . . LI: Yeah. MARTIN: Tell me. LI: Yeah, absolutely. I was 17 when I started, you know, doing influencer campaigns. And I think the thing that I realized about it is that it's such an empty goal because you're constantly chasing for that high engagement. You're constantly wanting new followers, more likes, etc. And after a certain point, like, I realized there's never going to be a number that's going to be satisfying. You know, you're never going to get enough followers. You're never going to get enough likes. So at that point, you just kind of have to ask yourself, is this worth it? Is it worth putting on this very highlighted persona on this online personality for the money, for the prestige - for the clout as, you know, the kids call it these days? And it's not. Truly, I don't - I really don't think it's worth it because once - the second that you hit a milestone, you're going to want more, and you're going to be constantly, constantly chasing something that you're never going to be happy with. MARTIN: And what about the idea of hiding the likes? What do you think? Is that a step in the right direction? Or you don't think it'll do any - make any difference. LI: So I really hate to be the cynic here, but while I do you think it is a step in the right direction, I'm not entirely sure that's going to last because the idea that Instagram is hiding these likes to promote better mental health - which I, you know, completely agree is a huge issue, especially considering the influence of social media on our kids these days. However, I think it's kind of a Band-Aid solution. I think the second that Instagram starts hiding their likes, there's going to be another social media app that comes out of the blue to kind of fill that gap, to fill that void because the issue isn't that people are getting their validation from Instagram. The issue is that people are looking at social media period for validation. So I think until the day that we address a solution for that bigger issue, I think Instagram hiding the likes - this is going to be a very temporary solution. MARTIN: So, professor Marwick, can you just talk a little bit more about what the research shows? You know, your research focuses on what you call the attention economy. Could you just amplify our understanding? Is there anything else you think we should know about how this works? MARWICK: Yeah. So in the attention economy, likes and followers function as currency. They're worth something. And obviously, they can be converted into actual money, as in an influencer being paid by a brand to put up a sponsored post, as we've talked about. But in other cases, they function as social currency. They give people feedback. They let them know people are watching and paying attention. And in some cases, I do think this is good. I think it allows people to feel socially supported, that they're part of a community and to maybe participate in a trend or a meme that's going around. And these things are good, I think, for our community building and sort of sense of each other. But I think the problem is when this attention gets converted down to this metric, this single number, it provides this very easy point of comparison. And you can very easily compare yourself to your friends, to celebrities, to influencers. And Instagram also uses those like metrics in its product. It uses the like metrics to rank its feed of photos to recommend which photos it shows first to different users. So the likes are still going to matter to the site. They'll still be there under the hood. They just won't necessarily be as visible to the users. MARTIN: Why wouldn't they just monetize that and just say, OK, you can pay extra to see people's likes, get access to that private information? MARWICK: I mean, it remains to be seen, right? Like, this is a test. I think that they are going to have to implement something for people who are making a living off this platform simply because brands and influencers are so dependent on being able to judge the success of a sponsorship or a campaign through the number of likes and views a post gets. So, you know, I could see them down the line rolling out some kind of tools for business and - or for people who want to sort of interact with each other more professionally on the site. MARTIN: That leads to a question I want to save for the end because now I want to - which is the whole question of whether these platforms have gotten to the point where really they should be treated like utilities, for whatever that means? But I'm going to save that for the end because I want to go to Taylor now on the story that you covered for the Atlantic about this move to Facebook and Instagram banning the profiles of public figures like Infowars founder Alex Jones and Nation of Islam leader Louis Farrakhan. Taylor, you covered this for The Atlantic. What exactly does this entail? LORENZ: OK, so a couple things. First of all, I just want to loop back to something - the ways that you guys were just talking about Instagram previously and push back a little bit. That's really. . . MARTIN: Sure. LORENZ: . . . Not how people use Instagram on a broad scale. I mean, Facebook - I'm sorry, Instagram is over a billion users. It's a very - you know, people express themselves in very complex ways on the app. So while, you know, there's this stereotype that people are just posting photos and trying to get likes, like, it's really not like that, especially for younger users who use it - you know, they're very involved in commenting and group chats. And, you know, Stories, its fastest-growing product, is not at all about likes. It's far less performative. So I think sort of in light of that, you see a lot of people using Instagram in, you know, all of these complex, interesting and sometimes very problematic ways. So, you know, Facebook and Instagram's move to ban these extremist accounts was because, you know, some of these extremist figures were using Instagram to actually, you know, spread misinformation, which is a problem that Facebook has had, too. So I think this is a - you know, a strong step for that. But they have a lot more ways to go. You know, Instagram is not just about photos. And there are also a lot of ways to spread misinformation on Facebook, too. MARTIN: Could you talk a little bit more about that briefly? Like, give us another example of that. LORENZ: Of Instagram. MARTIN: Yeah, how it works. LORENZ: Yeah. Well, so. . . MARTIN: Because you've written extensively about that, too - not just the what but the how - how it works that way. LORENZ: Yeah. So Instagram's this big, you know, messy social network. Obviously, you have the main feed of photos, which people less and less use to just post straight pictures, like the aspirational photos you were talking about earlier. Especially younger users engage in memes. They engage in video content from other platforms, things like that. There's also IGTV, which is essentially a YouTube competitor. There's Stories, which are the short-form video, and GIF and photo formats. So - and then there's comments and group chats and all of these different things - live-streaming. And so, you know, extremist figures can kind of play in all of those different formats, you know, whether it's an Infowars video uploaded to a meme page on IGTV to reach, you know, Gen Z kids or, you know, problematic messages told in visual format through the feed or, you know, an extremist going live. MARTIN: So to that end, though - and I also want to hear from the others on this - but, Taylor, do you - again, to the degree you feel comfortable because you cover this. . . LORENZ: Sure. MARTIN: Is this focusing on these individuals who have very long track records, right, of public utterances that most people I think know - do you think that's the right approach? LORENZ: Well, yeah. I mean, it's the right approach to crack down on extremism as we've seen, you know, the consequences of letting that kind of speech flourish. But, I mean, they're not going far enough. I mean, this is literally, like, four people or five people. So there's a really long way to go on both platforms. MARTIN: Professor Marwick, what do you think about that because I know you study this as well? LORENZ: Well, I think it's a difficult position for a platform like Facebook. I mean, Taylor is completely right. There are - these are enormous, multi-feature platforms with many different ways for people to engage. And, as a result, it's not just coming from the top, right? Like, it's not just coming from Alex Jones. It's coming in private Facebook groups. It's coming in WhatsApp chats. It's coming in all of these different ways that people engage with the platform. So I do think it's one head of the many-headed hydra that you can cut off. But at some point, there - you're going to have to take some kind of stance on what type of content is acceptable and not on the platform. If they choose to do that, I think that's going to be very, very difficult for them because there's going to be an accusation of partisanship. And, as a result, I think they've tried to portray themselves as sort of this, like, neutral third-party because they want to be seen as a platform, right, on which people interact and on which people perform. But when you are drawing a line in the sand and saying, you know, this is the type of content that isn't acceptable, that's going to encompass a range of people, some who are very fringy and some who are a little bit more mainstream. And I think that's a political battle that Facebook is going to have to face as it moves forward with some of these decisions. MARTIN: So that's the question I wanted to sort of conclude with. Does this point to the idea that these platforms - that we should think about them as they're not kind of just neutral entities that are just there, and people just kind of do what - that they're so woven into the fabric of our lives that perhaps we should think about them in a different way. I know that there's a lot of resistance to that for all kinds of reasons. But, you know, are we at the point where they should be thought about like utilities, where the public gets to participate in how these entities interface with us? There are public service commissions. You know, there are boards. There are - do you see my point? It's not just a private matter how they conduct themselves. What do you think about that, as briefly - I know it's a big question - as briefly as you can? So, Taylor, you want to start? LORENZ: Yeah, sure. I mean, I think it's so hard. I mean, if you're going to talk about them becoming public utilities and regulation, you have to really understand how these platforms work and how people communicate on them. And I think that's evolving so rapidly that it's really hard for, you know, lawmakers to even regulate that type of stuff. I'd point to some of the influencer regulations that the FTC has put in place. I mean, that really hasn't done anything to, you know, like, really rein in a lot of this bad spammy influencer marketing. So I think - yeah, I mean, I think it's worth noting, you know, the role that these people - or the role that these platforms play in our lives. But I don't know. MARTIN: It's big. LORENZ: It's so big and so complicated (laughter). MARTIN: Yeah. Sara, what do you think? LI: I'm just really thinking about how that would even work because, like Taylor said, Instagram really isn't just for the sole purpose of just posting a picture and getting likes anymore. We've seen it branch out so much bigger than that. Like I said, it's a method for communication. And now we're seeing media publications use it as a means to communicate with their audience. So just the idea of it becoming kind of a public service - I can see that maybe going into effect if there was a more prominent health safety issue such as, like Taylor mentioned, the spammy influencer marketing where you have people promoting, like, these detox teas that truly are harmful for your body. So I can see that being part of that huge conversation. MARTIN: Professor Marwick, I only gave you 30 seconds to answer this terribly big question, so you'll have to come back and tell us more. But what do you think? MARWICK: It's a - these are global platforms, so even if the U. S. decided to implement regulations, we still have millions - hundreds of millions of users outside the U. S. who are under different regulatory regimes. And when you're talking about regulation, you are talking about something that gets extremely complicated and requires an intimate knowledge of the platform which lawmakers don't necessarily always have. MARTIN: Well, thanks for at least getting us started in trying to understand it. MARWICK: (Laughter). MARTIN: That's Alice Marwick, professor of communication at UNC Chapel Hill, author of \"Status Update: Celebrity, Publicity, And Branding In The Social Media Age,\" Sara Li, social media strategist, and Taylor Lorenz, staff writer for The Atlantic. Thank you all so much for joining us. LI: Thank you. LORENZ: Thank you. MARWICK: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-04-720221912": {"title": "'Cyber Disruption' Affected Parts Of U.S. Energy Grid : NPR", "url": "https://www.npr.org/2019/05/04/720221912/cyber-disruption-affected-parts-of-u-s-energy-grid", "author": "No author found", "published_date": "2019-05-04", "content": "SCOTT SIMON, HOST:  There's been a disruption in part of the U. S. energy grid, and it appears to have been a first-of-its-kind cyberattack. It happened in March. We know about it because the utility reported it to the Department of Energy, and that report caught the attention of Blake Sobczak of E&E News. He joins us in our studios. Thanks so much for being with us. BLAKE SOBCZAK: Thank you for having me on the program. SIMON: There was a disruption - right? - in Utah, Wyoming and Southern California but not a power outage or anything that customers noticed. What effect did it have? SOBCZAK: That's correct. There was a disruption, but it did not lead to any blackouts or really, as far as we know, any halt in the flow of electricity there. What likely happened here was what's called a loss of visibility. There was a denial-of-service attack against some part of the utilities network infrastructure, and that basically led operators to not be able to see what was going on in the grid. So it's sort of like driving with blinders on. As long as nothing crazy happens, you should be fine, but it certainly constitutes a disruption and a reportable event here to the Department of Energy. SIMON: And a possible danger, obviously. SOBCZAK: It does pose a hazard, and that's why the Department of Energy actually requires utilities to report if they experience a cyberattack within one hour of the event itself. And so this is really the first time that we've seen a utility tell regulators at the DOE, at the Department of Energy, hey, hackers disrupted some part of our operations. And in this case, again, it appears that that was related to visibility as to what was happening on the grid there. SIMON: Do you know the utility? SOBCZAK: We do not know the utility. And what's interesting is this affected a pretty wide geographic area - Utah, Wyoming and California - Southern California. And so there aren't too many utilities with that kind of geographic footprint. And I reached out to several, and they all basically denied filing this report. SIMON: And obviously no way of knowing if it were - if the hackers were freelancers or agents of a government or a terrorist group. SOBCZAK: It's really hard to say much about the hackers at this point, that's correct. Presumably, the utility that was affected by this is investigating and is really, you know, calling in perhaps the FBI to look at this. What we do know is that the type of attack that this was was what's called a denial of service. And it can be a little bit of a simpler attack, so we don't know whether, for instance, these were Russian hackers or sophisticated nation state-backed spies doing this. It really could have been somebody with a fairly rudimentary understanding of how to launch this type of attack. Now, a denial-of-service attack works by flooding target networks with traffic so that you can't tell what's legitimate from what's not. So in the case of a public website, a hacker might just bombard it with requests to visit that page until normal users can't actually access it. So if you're ever having trouble logging onto a site or accessing it, you know, maybe that site's under a denial-of-service attack. In this case, the denial of service exploited a particular vulnerability, so it was a little bit more targeted than that. The hacker or hackers knew what they were doing and were able to actually find a particular flaw in this network equipment and send a certain type of packet or string of data to really make it stop working. SIMON: There was, of course, reportedly in, I guess, 2015 an attack in Ukraine where the lights went out - reportedly mounted by Russia. How susceptible is the U. S. energy infrastructure and grid? SOBCZAK: So what can be said about the U. S. grid is it's an enormously complicated, huge machine. A lot of people consider it to be the most complex machine ever built by humankind. And so that means there's a lot of resiliency built into it. It'd be very difficult to disrupt the grid in a big way from a cyberattack. However, we actually heard earlier this year - U. S. Director of National Intelligence Dan Coats warned that foreign hackers backed by perhaps nation states like China and Russia could cause localized temporary disruptions in U. S. critical infrastructure to include the power grid. So I think what intelligence officials are worried about and what utilities are potentially seeing is a small-scale hack causing perhaps a temporary power outage but nothing more, which, in and of itself, is still certainly noteworthy and alarming that our level of connectivity is such that hackers can have this power. SIMON: Blake Sobczak, a reporter for E&E News, thanks so much for being with us. SOBCZAK: Thanks for having me on. SCOTT SIMON, HOST:   There's been a disruption in part of the U. S. energy grid, and it appears to have been a first-of-its-kind cyberattack. It happened in March. We know about it because the utility reported it to the Department of Energy, and that report caught the attention of Blake Sobczak of E&E News. He joins us in our studios. Thanks so much for being with us. BLAKE SOBCZAK: Thank you for having me on the program. SIMON: There was a disruption - right? - in Utah, Wyoming and Southern California but not a power outage or anything that customers noticed. What effect did it have? SOBCZAK: That's correct. There was a disruption, but it did not lead to any blackouts or really, as far as we know, any halt in the flow of electricity there. What likely happened here was what's called a loss of visibility. There was a denial-of-service attack against some part of the utilities network infrastructure, and that basically led operators to not be able to see what was going on in the grid. So it's sort of like driving with blinders on. As long as nothing crazy happens, you should be fine, but it certainly constitutes a disruption and a reportable event here to the Department of Energy. SIMON: And a possible danger, obviously. SOBCZAK: It does pose a hazard, and that's why the Department of Energy actually requires utilities to report if they experience a cyberattack within one hour of the event itself. And so this is really the first time that we've seen a utility tell regulators at the DOE, at the Department of Energy, hey, hackers disrupted some part of our operations. And in this case, again, it appears that that was related to visibility as to what was happening on the grid there. SIMON: Do you know the utility? SOBCZAK: We do not know the utility. And what's interesting is this affected a pretty wide geographic area - Utah, Wyoming and California - Southern California. And so there aren't too many utilities with that kind of geographic footprint. And I reached out to several, and they all basically denied filing this report. SIMON: And obviously no way of knowing if it were - if the hackers were freelancers or agents of a government or a terrorist group. SOBCZAK: It's really hard to say much about the hackers at this point, that's correct. Presumably, the utility that was affected by this is investigating and is really, you know, calling in perhaps the FBI to look at this. What we do know is that the type of attack that this was was what's called a denial of service. And it can be a little bit of a simpler attack, so we don't know whether, for instance, these were Russian hackers or sophisticated nation state-backed spies doing this. It really could have been somebody with a fairly rudimentary understanding of how to launch this type of attack. Now, a denial-of-service attack works by flooding target networks with traffic so that you can't tell what's legitimate from what's not. So in the case of a public website, a hacker might just bombard it with requests to visit that page until normal users can't actually access it. So if you're ever having trouble logging onto a site or accessing it, you know, maybe that site's under a denial-of-service attack. In this case, the denial of service exploited a particular vulnerability, so it was a little bit more targeted than that. The hacker or hackers knew what they were doing and were able to actually find a particular flaw in this network equipment and send a certain type of packet or string of data to really make it stop working. SIMON: There was, of course, reportedly in, I guess, 2015 an attack in Ukraine where the lights went out - reportedly mounted by Russia. How susceptible is the U. S. energy infrastructure and grid? SOBCZAK: So what can be said about the U. S. grid is it's an enormously complicated, huge machine. A lot of people consider it to be the most complex machine ever built by humankind. And so that means there's a lot of resiliency built into it. It'd be very difficult to disrupt the grid in a big way from a cyberattack. However, we actually heard earlier this year - U. S. Director of National Intelligence Dan Coats warned that foreign hackers backed by perhaps nation states like China and Russia could cause localized temporary disruptions in U. S. critical infrastructure to include the power grid. So I think what intelligence officials are worried about and what utilities are potentially seeing is a small-scale hack causing perhaps a temporary power outage but nothing more, which, in and of itself, is still certainly noteworthy and alarming that our level of connectivity is such that hackers can have this power. SIMON: Blake Sobczak, a reporter for E&E News, thanks so much for being with us. SOBCZAK: Thanks for having me on.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-05-720490357": {"title": "Tech Columnist Farhad Manjoo Laments Uber's Rise As Company Goes Public : NPR", "url": "https://www.npr.org/2019/05/05/720490357/tech-columnist-farhad-manjoo-laments-uber-s-rise-as-company-goes-public", "author": "No author found", "published_date": "2019-05-05", "content": "AARTI SHAHANI, HOST: And now to another story we'll hear more about this week. It starts with a number - $90 billion. That's about how much Uber is expected to be worth when it goes public - that despite the fact the company has never actually turned a profit. It's made plenty of money but never more than it spends. Its move to go public and consolidate power comes at a time when Americans are questioning the power of Silicon Valley and leading politicians are calling for the breakup of tech giants. New York Times opinion columnist Farhad Manjoo recently called the Uber IPO a moral stain on Silicon Valley. He admitted his once-bright-eyed take on the startup has dimmed dramatically. He's here to talk with us about why. And he joins us from Mountain View, Calif. Welcome. FARHAD MANJOO: Hi. Good to be here. SHAHANI: You say you used to be a naive baby tech pundit who bought into the Uber hype, fanboy of sorts. These are your words. But this past week in your column, you lamented, what's become of Uber? What exactly are you lamenting? MANJOO: Yeah, I mean, I once thought that Uber could be a win for everyone. You know, we used to - in cities, we used to sort of have these taxi cartels which limited supply and didn't really offer a great service. And Uber seemed like a better version of the taxi. And it also kind of offered these environmental promises. We'd have, you know, people using cars more efficiently. Like, Uber promised that it could get us - you know, more people in cars, get fewer cars on the road. And also, it promised that it could give all of its drivers a living - and even a better than living - wage. And those promises really haven't materialized. Instead, we have this company that had this really reckless history. And that is - you know, with the IPO, rewarding many of the people who pushed its kind of lawlessness and its recklessness until now. SHAHANI: And so the lament is that it's not a beacon for environmentalism, it's not a beacon for greater labor standards or labor protections, but it's something else? MANJOO: Yeah. I mean, it has turned into this dystopian vision of tech, where, you know, a few people prosper, and everyone else gets kind of poverty wages. Users of the service get pretty good service but kind of at the cost of all of these drivers getting very little from the service, getting very little equity. I think it is a betrayal of what Silicon Valley, you know, said that it stood for, which is creating new ideas that are good for the world at large, not just, you know, a few hundred people on the West Coast. SHAHANI: You know, many would herald Uber as a great American success story, a testament to what capitalism can create - right? - jobs for millions of people in the U. S. and around the world. And its IPO will make a handful of people millionaires and even billionaires. Isn't that a good thing? MANJOO: I mean, it's perhaps better than not having it, but I don't think it's fundamentally or sort of objectively a good thing. What would be better is if Uber was a smaller and less valuable company but that less value was sort of more equitably distributed. SHAHANI: Is your concern about how Uber has operated, or is it also how the media, how journalists in business and technology, have covered Uber's growth? You know, it was not a secret in earlier years that Uber was running circles around regulators, flouting laws, for example, not having insurance to cover the rides, refusing to pay livery taxes. These were not secrets. And even as they were happening, there was still a celebration of this - you know, this startup. MANJOO: Yeah. I mean, I think that criticism of the tech press kind of applies to the Uber case, and it also applies to Facebook and Google and Amazon and even Apple. Like, one of the things that we've learned over the past, you know, two or three years is that, you know, for the preceding decade, we kind of - we and the press and just the media and society generally gave tech companies a pass because they were new, and they were celebrated. And they were - they seemed to be giving us kind of the best of the future in America, you know, building the American dream. And, you know, in some cases, they were. And I value and appreciate many of their services. But I think that their kind of effect on the world wasn't, you know, looked at skeptically enough. SHAHANI: When you lament where the company is, do you feel there are steps that can be taken to bring it back to the loftier values that, you know, you first believed were true? MANJOO: Yeah, I think there are steps, but I don't think they're kind of what Uber should do. I think they're what regulators and lawmakers and sort of society should do. I mean, sort of a very easy step would be to, you know, re-examine this independent contractor status of their drivers and see if there's some way to have them be employees or gain some equity in the company. That would be one way. SHAHANI: So basically you're saying, as we're about to hoo and ha over the massive figure of possibly a hundred billion dollars in worth - regulators, pay attention and protect the drivers and the little people in the game? MANJOO: Yeah. And I think actually, you know, Uber going public may prompt that - which we are seeing strikes. We are seeing protests from drivers. And we're in this moment where lawmakers, presidential candidates, regulators are paying more attention to this industry and its effects on the world. So I don't think Uber's IPO is going to be the end of the story for its drivers and for its culture. I think it's, you know, maybe the beginning of a new, more restrictive, you know, oversight over the company. SHAHANI: That was Farhad Manjoo, New York Times tech columnist. Thanks for coming on. It's been great talking with you. MANJOO: Hey, thanks a lot. (SOUNDBITE OF SEAN HAYES SONG, \u201cNAKED AS THE SUN\u201d) AARTI SHAHANI, HOST:  And now to another story we'll hear more about this week. It starts with a number - $90 billion. That's about how much Uber is expected to be worth when it goes public - that despite the fact the company has never actually turned a profit. It's made plenty of money but never more than it spends. Its move to go public and consolidate power comes at a time when Americans are questioning the power of Silicon Valley and leading politicians are calling for the breakup of tech giants. New York Times opinion columnist Farhad Manjoo recently called the Uber IPO a moral stain on Silicon Valley. He admitted his once-bright-eyed take on the startup has dimmed dramatically. He's here to talk with us about why. And he joins us from Mountain View, Calif. Welcome. FARHAD MANJOO: Hi. Good to be here. SHAHANI: You say you used to be a naive baby tech pundit who bought into the Uber hype, fanboy of sorts. These are your words. But this past week in your column, you lamented, what's become of Uber? What exactly are you lamenting? MANJOO: Yeah, I mean, I once thought that Uber could be a win for everyone. You know, we used to - in cities, we used to sort of have these taxi cartels which limited supply and didn't really offer a great service. And Uber seemed like a better version of the taxi. And it also kind of offered these environmental promises. We'd have, you know, people using cars more efficiently. Like, Uber promised that it could get us - you know, more people in cars, get fewer cars on the road. And also, it promised that it could give all of its drivers a living - and even a better than living - wage. And those promises really haven't materialized. Instead, we have this company that had this really reckless history. And that is - you know, with the IPO, rewarding many of the people who pushed its kind of lawlessness and its recklessness until now. SHAHANI: And so the lament is that it's not a beacon for environmentalism, it's not a beacon for greater labor standards or labor protections, but it's something else? MANJOO: Yeah. I mean, it has turned into this dystopian vision of tech, where, you know, a few people prosper, and everyone else gets kind of poverty wages. Users of the service get pretty good service but kind of at the cost of all of these drivers getting very little from the service, getting very little equity. I think it is a betrayal of what Silicon Valley, you know, said that it stood for, which is creating new ideas that are good for the world at large, not just, you know, a few hundred people on the West Coast. SHAHANI: You know, many would herald Uber as a great American success story, a testament to what capitalism can create - right? - jobs for millions of people in the U. S. and around the world. And its IPO will make a handful of people millionaires and even billionaires. Isn't that a good thing? MANJOO: I mean, it's perhaps better than not having it, but I don't think it's fundamentally or sort of objectively a good thing. What would be better is if Uber was a smaller and less valuable company but that less value was sort of more equitably distributed. SHAHANI: Is your concern about how Uber has operated, or is it also how the media, how journalists in business and technology, have covered Uber's growth? You know, it was not a secret in earlier years that Uber was running circles around regulators, flouting laws, for example, not having insurance to cover the rides, refusing to pay livery taxes. These were not secrets. And even as they were happening, there was still a celebration of this - you know, this startup. MANJOO: Yeah. I mean, I think that criticism of the tech press kind of applies to the Uber case, and it also applies to Facebook and Google and Amazon and even Apple. Like, one of the things that we've learned over the past, you know, two or three years is that, you know, for the preceding decade, we kind of - we and the press and just the media and society generally gave tech companies a pass because they were new, and they were celebrated. And they were - they seemed to be giving us kind of the best of the future in America, you know, building the American dream. And, you know, in some cases, they were. And I value and appreciate many of their services. But I think that their kind of effect on the world wasn't, you know, looked at skeptically enough. SHAHANI: When you lament where the company is, do you feel there are steps that can be taken to bring it back to the loftier values that, you know, you first believed were true? MANJOO: Yeah, I think there are steps, but I don't think they're kind of what Uber should do. I think they're what regulators and lawmakers and sort of society should do. I mean, sort of a very easy step would be to, you know, re-examine this independent contractor status of their drivers and see if there's some way to have them be employees or gain some equity in the company. That would be one way. SHAHANI: So basically you're saying, as we're about to hoo and ha over the massive figure of possibly a hundred billion dollars in worth - regulators, pay attention and protect the drivers and the little people in the game? MANJOO: Yeah. And I think actually, you know, Uber going public may prompt that - which we are seeing strikes. We are seeing protests from drivers. And we're in this moment where lawmakers, presidential candidates, regulators are paying more attention to this industry and its effects on the world. So I don't think Uber's IPO is going to be the end of the story for its drivers and for its culture. I think it's, you know, maybe the beginning of a new, more restrictive, you know, oversight over the company. SHAHANI: That was Farhad Manjoo, New York Times tech columnist. Thanks for coming on. It's been great talking with you. MANJOO: Hey, thanks a lot. (SOUNDBITE OF SEAN HAYES SONG, \u201cNAKED AS THE SUN\u201d)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-06-720800666": {"title": "Artificial Intelligence Can Make Our Lives Easier, But It Can Also Go Awry : NPR", "url": "https://www.npr.org/2019/05/06/720800666/artificial-intelligence-can-make-our-lives-easier-but-it-can-also-go-awry", "author": "No author found", "published_date": "2019-05-06", "content": "AILSA CHANG, HOST: Even if you can advance technology - say, create the next greatest app or build a robot that fights wars - the question is, should you? In this month's All Tech Considered, we're looking at some of the ethical dilemmas raised by technological innovation. If technology is going to be used for some of the most sensitive purposes, what are the guardrails to make sure that technology does no harm? (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CHANG: There is a group thinking about this very issue as it relates to the spread of artificial intelligence. It's called AI Now Institute. It's dedicated to understanding how artificial intelligence can reshape our everyday lives. One of the group's co-founders is Kate Crawford. Welcome. KATE CRAWFORD: Hi, Ailsa. Lovely to talk with you. CHANG: Lovely to talk to you. So what are some real-world examples of how AI has gone badly wrong? CRAWFORD: I mean, most people know, of course, the issues around Facebook, of course, with race, gender and age discrimination, particularly, in terms of everything from job ads to housing ads. But it even goes deeper than that. We've seen it in facial recognition systems that work less well for darker-skinned women. We've seen it in voice recognition systems that have been trained to detect male-sounding voices better than female-sounding voices. And there are even emotion detection tools now that assign more negative emotions to black men's faces than white men's faces, for example. So there are these real quite substantial problems across the systems that we sort of fit under this loose heading of AI. CHANG: And how are companies addressing these concerns now? I mean, do you think companies are doing enough? CRAWFORD: Well, it's a really hard problem. What's interesting is to see that it just - in the last couple of years, companies are really starting to take it seriously in terms of the scale of these issues. We're starting to see groups focused on responsibility in AI sometimes. These groups are very much focused on technical solutions. So can they tweak the data sets? Can they tweak the algorithms to try and produce less biased results? But in some cases, that just isn't enough. I mean, we have a really chilling example, of course, from Amazon when they tried to create an AI hiring tool that was designed to scan resumes to decide who should get an interview. And what they found was that, ultimately, even if you had the word women on your CV, let alone if you were actually a woman, your CV was getting downrated. CHANG: No way. CRAWFORD: This is exactly true. It has been widely reported. And, of course, engineers tried to fix this, of course, by sort of tweaking the algorithm and the data set. Of course, the data set itself was very skewed towards male engineers. Just have a look at Amazon's workforce and you'll see how bad that skew is. So in the end, they found that they could not fix this problem. And they ended up devolving the tool. They do not use it. So these problems, in some ways, are very hard to fix technically. And that's because we have to look at them much more broadly. CHANG: So do you think the government should step in to try to help companies avoid these biases, help them avoid injecting AI with these biases? CRAWFORD: So I think certainly there are some technologies that really do need very stringent regulation. In the AI Now Report for 2018, which came out in December, we specifically looked at facial recognition and so-called affect recognition. That's kind of like a subclass of facial recognition that claims to detect things like your true personality, your inner feelings and even your mental health based on images or video of your face. Now, I would say that some of these claims are just not backed by very robust scientific evidence in some cases. You know, the science has really been questioned. And so I think linking these types of emotion and affect recognition tools to things like hiring or access to insurance or policing, for example, creates really concerning risks. CHANG: You know, for years, the government, at least here in the United States, has kind of taken this hands-off approach towards Silicon Valley because there was this fear that overregulation would suppress innovation. So could you take government regulation too far, where you end up tipping the balance towards suppressing innovation? CRAWFORD: In some ways, I think it's a false choice. We can actually have innovation and safety at the same time. And certainly we don't want to be rolling out systems that discriminate. The philosophy of moving fast and breaking things has really had its day. I mean, all of these systems are so profound that I think it's really legitimate that people want to know that they're safe and non-discriminatory. CHANG: Kate Crawford is the co-founder of the AI Now Institute at New York University. Thank you very much for joining us. CRAWFORD: It's a pleasure. AILSA CHANG, HOST:  Even if you can advance technology - say, create the next greatest app or build a robot that fights wars - the question is, should you? In this month's All Tech Considered, we're looking at some of the ethical dilemmas raised by technological innovation. If technology is going to be used for some of the most sensitive purposes, what are the guardrails to make sure that technology does no harm? (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CHANG: There is a group thinking about this very issue as it relates to the spread of artificial intelligence. It's called AI Now Institute. It's dedicated to understanding how artificial intelligence can reshape our everyday lives. One of the group's co-founders is Kate Crawford. Welcome. KATE CRAWFORD: Hi, Ailsa. Lovely to talk with you. CHANG: Lovely to talk to you. So what are some real-world examples of how AI has gone badly wrong? CRAWFORD: I mean, most people know, of course, the issues around Facebook, of course, with race, gender and age discrimination, particularly, in terms of everything from job ads to housing ads. But it even goes deeper than that. We've seen it in facial recognition systems that work less well for darker-skinned women. We've seen it in voice recognition systems that have been trained to detect male-sounding voices better than female-sounding voices. And there are even emotion detection tools now that assign more negative emotions to black men's faces than white men's faces, for example. So there are these real quite substantial problems across the systems that we sort of fit under this loose heading of AI. CHANG: And how are companies addressing these concerns now? I mean, do you think companies are doing enough? CRAWFORD: Well, it's a really hard problem. What's interesting is to see that it just - in the last couple of years, companies are really starting to take it seriously in terms of the scale of these issues. We're starting to see groups focused on responsibility in AI sometimes. These groups are very much focused on technical solutions. So can they tweak the data sets? Can they tweak the algorithms to try and produce less biased results? But in some cases, that just isn't enough. I mean, we have a really chilling example, of course, from Amazon when they tried to create an AI hiring tool that was designed to scan resumes to decide who should get an interview. And what they found was that, ultimately, even if you had the word women on your CV, let alone if you were actually a woman, your CV was getting downrated. CHANG: No way. CRAWFORD: This is exactly true. It has been widely reported. And, of course, engineers tried to fix this, of course, by sort of tweaking the algorithm and the data set. Of course, the data set itself was very skewed towards male engineers. Just have a look at Amazon's workforce and you'll see how bad that skew is. So in the end, they found that they could not fix this problem. And they ended up devolving the tool. They do not use it. So these problems, in some ways, are very hard to fix technically. And that's because we have to look at them much more broadly. CHANG: So do you think the government should step in to try to help companies avoid these biases, help them avoid injecting AI with these biases? CRAWFORD: So I think certainly there are some technologies that really do need very stringent regulation. In the AI Now Report for 2018, which came out in December, we specifically looked at facial recognition and so-called affect recognition. That's kind of like a subclass of facial recognition that claims to detect things like your true personality, your inner feelings and even your mental health based on images or video of your face. Now, I would say that some of these claims are just not backed by very robust scientific evidence in some cases. You know, the science has really been questioned. And so I think linking these types of emotion and affect recognition tools to things like hiring or access to insurance or policing, for example, creates really concerning risks. CHANG: You know, for years, the government, at least here in the United States, has kind of taken this hands-off approach towards Silicon Valley because there was this fear that overregulation would suppress innovation. So could you take government regulation too far, where you end up tipping the balance towards suppressing innovation? CRAWFORD: In some ways, I think it's a false choice. We can actually have innovation and safety at the same time. And certainly we don't want to be rolling out systems that discriminate. The philosophy of moving fast and breaking things has really had its day. I mean, all of these systems are so profound that I think it's really legitimate that people want to know that they're safe and non-discriminatory. CHANG: Kate Crawford is the co-founder of the AI Now Institute at New York University. Thank you very much for joining us. CRAWFORD: It's a pleasure.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-06-720096005": {"title": "Drones Might Work Longer With Some Bird-Inspired Modifications : NPR", "url": "https://www.npr.org/2019/05/06/720096005/if-drones-had-claws-they-might-be-able-to-fly-for-longer", "author": "No author found", "published_date": "2019-05-06", "content": "ARI SHAPIRO, HOST: Small drones have lots of practical applications. They can help firefighters find hot spots. They can let environmental monitors pinpoint the source of hazardous chemical leaks. But small drones have a problem. They can't fly for very long. NPR's Joe Palca reports on a new solution. JOE PALCA, BYLINE: Small quad copters have become extremely popular. You see them at sporting events or outdoor concerts, maybe around your neighborhood. But it takes a lot of power to spin four helicopter blades fast enough to keep a quadcopter in the air. KAIYU HANG: If you fly it with full battery, you can use it for at most, like, 25 minutes to half an hour. PALCA: Kaiyu Hang and his colleagues at Yale University are trying to extend the time a drone can stay on the job. And for inspiration, they turn to birds. HANG: Birds usually fly somewhere. They stay at the top of the roof or some tree branches, and then they look for their prey or they just stay there without, you know, flapping their wings all the time. And then they can still observe what is happening around them. PALCA: And observing takes a lot less energy than flying. Hang and his colleagues developed a claw-like landing system that lets a drone grab on to a branch or pole and turn off its engines. And Hang says their new system allows a drone to save energy even when no suitable branch is available. HANG: We have developed, like, modularized landing gear framework that allows the drone to not only perch on some structures, but also the drone can rest on some structures when perching is not possible. PALCA: By resting, Hang means the quadcopter can essentially lean against the ledge and stay there using just two of its four rotors, saving energy. Hang describes his work in the journal Science Robotics. Mark Cutkosky also works on drone landing systems. He's at Stanford University. He says the ability to perch can be handy for a number of reasons. MARK CUTKOSKY: If you perch, for example, underneath a bridge or underneath the eaves of a building, you can ride out storms or bad weather that would make it hard to fly. PALCA: Cutkosky says the Yale team's new landing system does have some drawbacks. For one thing, it adds weight to the drone. That means it takes more energy to keep it aloft. CUTKOSKY: So there had better be a good tradeoff in terms of really improving mission time in order to pay for that added weight. PALCA: Another drawback is, for now, a human has to fly the drone to the perch site. To make the landing system really useful, the drone should be able to land on its own. So the question is. . . CUTKOSKY: What sorts of strategies can we use that would allow this system to discover perchable sites? PALCA: Cutkosky says the Yale team has already begun integrating the drone's computer with its onboard camera. So he thinks before too long, drones will be able to decide on their own where to settle down for a rest. Joe Palca, NPR News. (SOUNDBITE OF TORO Y MOI'S \"SAY THAT\") ARI SHAPIRO, HOST:  Small drones have lots of practical applications. They can help firefighters find hot spots. They can let environmental monitors pinpoint the source of hazardous chemical leaks. But small drones have a problem. They can't fly for very long. NPR's Joe Palca reports on a new solution. JOE PALCA, BYLINE: Small quad copters have become extremely popular. You see them at sporting events or outdoor concerts, maybe around your neighborhood. But it takes a lot of power to spin four helicopter blades fast enough to keep a quadcopter in the air. KAIYU HANG: If you fly it with full battery, you can use it for at most, like, 25 minutes to half an hour. PALCA: Kaiyu Hang and his colleagues at Yale University are trying to extend the time a drone can stay on the job. And for inspiration, they turn to birds. HANG: Birds usually fly somewhere. They stay at the top of the roof or some tree branches, and then they look for their prey or they just stay there without, you know, flapping their wings all the time. And then they can still observe what is happening around them. PALCA: And observing takes a lot less energy than flying. Hang and his colleagues developed a claw-like landing system that lets a drone grab on to a branch or pole and turn off its engines. And Hang says their new system allows a drone to save energy even when no suitable branch is available. HANG: We have developed, like, modularized landing gear framework that allows the drone to not only perch on some structures, but also the drone can rest on some structures when perching is not possible. PALCA: By resting, Hang means the quadcopter can essentially lean against the ledge and stay there using just two of its four rotors, saving energy. Hang describes his work in the journal Science Robotics. Mark Cutkosky also works on drone landing systems. He's at Stanford University. He says the ability to perch can be handy for a number of reasons. MARK CUTKOSKY: If you perch, for example, underneath a bridge or underneath the eaves of a building, you can ride out storms or bad weather that would make it hard to fly. PALCA: Cutkosky says the Yale team's new landing system does have some drawbacks. For one thing, it adds weight to the drone. That means it takes more energy to keep it aloft. CUTKOSKY: So there had better be a good tradeoff in terms of really improving mission time in order to pay for that added weight. PALCA: Another drawback is, for now, a human has to fly the drone to the perch site. To make the landing system really useful, the drone should be able to land on its own. So the question is. . . CUTKOSKY: What sorts of strategies can we use that would allow this system to discover perchable sites? PALCA: Cutkosky says the Yale team has already begun integrating the drone's computer with its onboard camera. So he thinks before too long, drones will be able to decide on their own where to settle down for a rest. Joe Palca, NPR News. (SOUNDBITE OF TORO Y MOI'S \"SAY THAT\")", "section": "Joe's Big Idea", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-06-716414780": {"title": "WATCH: Future You With Elise Hu  : NPR", "url": "https://www.npr.org/2019/05/06/716414780/videos-future-you", "author": "No author found", "published_date": "2019-05-06", "content": "", "section": "NPR Video", "disclaimer": ""}, "2019-05-06-719590408": {"title": "As Saudi Arabia Builds A Nuclear Reactor, Some Worry About Its Motives : NPR", "url": "https://www.npr.org/2019/05/06/719590408/as-saudi-arabia-builds-a-nuclear-reactor-some-worry-about-its-motives", "author": "No author found", "published_date": "2019-05-06", "content": "AILSA CHANG, HOST: Saudi Arabia is building its first nuclear reactor. It's small, and nuclear-powered electricity is an important part of Saudi Arabia's plans for its future. But as NPR's Geoff Brumfiel reports, there may be an ulterior reason for the interest in nukes. GEOFF BRUMFIEL, BYLINE: Right now Saudi Arabia generates its electricity with fossil fuels. SHARON SQUASSONI: They're one of the few countries that actually uses oil for electricity. Most countries stopped doing that in the '70s. BRUMFIEL: Sharon Squassoni is at George Washington University. It's totally obvious why Saudi Arabia does this. They have lots of oil and natural gas. But looking to the future, the Saudi government predicts that oil will be more valuable as an export. So starting in the late 2000s, Saudi Arabia began pursuing an ambitious plan to start a nuclear energy program. Even after the 2011 nuclear disaster in Japan, Squassoni says Saudi Arabia kept at it. SQUASSONI: Most countries were walking away from nuclear. But they decided, look; this is our long-term plan. BRUMFIEL: Which she finds a little puzzling given that the country is perfectly suited for other kinds of less costly electricity production, particularly renewables. SQUASSONI: They have these vast deserts that'd be pretty easy, I would think, to put out big solar farmsBRUMFIEL: Squassoni's background is an arms control, and she's worried that Saudi Arabia might be interested in nuclear technology for a different reason, nuclear weapons - why? - because of their chief rival in the region. SQUASSONI: The big thing is Iran. BRUMFIEL: Its nuclear program has had military dimensions in the past according to the International Atomic Energy Agency. Speaking last year on CBS' \"60 Minutes,\" Saudi Crown Prince Mohammed bin Salman said, if Iran got a nuke, Saudi Arabia would, too. (SOUNDBITE OF TV SHOW, \"60 MINUTES\")CROWN PRINCE MOHAMMED BIN SALMAN: (Through interpreter) Saudi Arabia does not want to acquire any nuclear bomb. But without a doubt, if Iran developed a nuclear bomb, we will follow suit as soon as possible. BRUMFIEL: Bin Salman's words matter because Saudi Arabia's nuclear plans are moving from paper to reality. Recent satellite images show construction is underway on its first research reactor on the outskirts of Riyadh. Aaron Stein is director of the Middle East Program at the Foreign Policy Research Institute. He says this new reactor is too small and too low-power to be of any use in bomb-making. AARON STEIN: This is not something that a country would engage upon for a weapons program. BRUMFIEL: In fact, even large civilian nuclear power plants can't be used easily to make bombs. But there are other parts of a civilian nuclear program that can. In particular, if Saudi Arabia decides it wants to make its own fuel for its nuclear reactors instead of buying it on the open market, that would require enriching uranium, which uses the same technologies that can be used to enrich uranium for bombs. STEIN: I think that would send alarm bells throughout the region. I think that there would be interpreted as a move to hedge and to consider building nuclear weapons down the line. BRUMFIEL: Saudi Arabia hasn't said whether it wants fuel-making technology, but there's plenty of reason to worry. Iran already has it, thousands of uranium centrifuges which for now remain under heavy international monitoring. The Trump administration is reportedly negotiating a nuclear cooperation deal with Saudi Arabia. Squassoni says that deal should be carefully crafted. SQUASSONI: The big, big question in the background for the U. S. and for all suppliers of nuclear technology to the Saudis is - do we think we have enough controls in place that we can trust them since they've been pretty clear about their intentions should things go bad with Iran? BRUMFIEL: She hopes the U. S. will seek assurances that Saudi Arabia will not pursue civilian technologies that could allow it to make a bomb. Geoff Brumfiel, NPR News, Washington. AILSA CHANG, HOST:  Saudi Arabia is building its first nuclear reactor. It's small, and nuclear-powered electricity is an important part of Saudi Arabia's plans for its future. But as NPR's Geoff Brumfiel reports, there may be an ulterior reason for the interest in nukes. GEOFF BRUMFIEL, BYLINE: Right now Saudi Arabia generates its electricity with fossil fuels. SHARON SQUASSONI: They're one of the few countries that actually uses oil for electricity. Most countries stopped doing that in the '70s. BRUMFIEL: Sharon Squassoni is at George Washington University. It's totally obvious why Saudi Arabia does this. They have lots of oil and natural gas. But looking to the future, the Saudi government predicts that oil will be more valuable as an export. So starting in the late 2000s, Saudi Arabia began pursuing an ambitious plan to start a nuclear energy program. Even after the 2011 nuclear disaster in Japan, Squassoni says Saudi Arabia kept at it. SQUASSONI: Most countries were walking away from nuclear. But they decided, look; this is our long-term plan. BRUMFIEL: Which she finds a little puzzling given that the country is perfectly suited for other kinds of less costly electricity production, particularly renewables. SQUASSONI: They have these vast deserts that'd be pretty easy, I would think, to put out big solar farms BRUMFIEL: Squassoni's background is an arms control, and she's worried that Saudi Arabia might be interested in nuclear technology for a different reason, nuclear weapons - why? - because of their chief rival in the region. SQUASSONI: The big thing is Iran. BRUMFIEL: Its nuclear program has had military dimensions in the past according to the International Atomic Energy Agency. Speaking last year on CBS' \"60 Minutes,\" Saudi Crown Prince Mohammed bin Salman said, if Iran got a nuke, Saudi Arabia would, too. (SOUNDBITE OF TV SHOW, \"60 MINUTES\") CROWN PRINCE MOHAMMED BIN SALMAN: (Through interpreter) Saudi Arabia does not want to acquire any nuclear bomb. But without a doubt, if Iran developed a nuclear bomb, we will follow suit as soon as possible. BRUMFIEL: Bin Salman's words matter because Saudi Arabia's nuclear plans are moving from paper to reality. Recent satellite images show construction is underway on its first research reactor on the outskirts of Riyadh. Aaron Stein is director of the Middle East Program at the Foreign Policy Research Institute. He says this new reactor is too small and too low-power to be of any use in bomb-making. AARON STEIN: This is not something that a country would engage upon for a weapons program. BRUMFIEL: In fact, even large civilian nuclear power plants can't be used easily to make bombs. But there are other parts of a civilian nuclear program that can. In particular, if Saudi Arabia decides it wants to make its own fuel for its nuclear reactors instead of buying it on the open market, that would require enriching uranium, which uses the same technologies that can be used to enrich uranium for bombs. STEIN: I think that would send alarm bells throughout the region. I think that there would be interpreted as a move to hedge and to consider building nuclear weapons down the line. BRUMFIEL: Saudi Arabia hasn't said whether it wants fuel-making technology, but there's plenty of reason to worry. Iran already has it, thousands of uranium centrifuges which for now remain under heavy international monitoring. The Trump administration is reportedly negotiating a nuclear cooperation deal with Saudi Arabia. Squassoni says that deal should be carefully crafted. SQUASSONI: The big, big question in the background for the U. S. and for all suppliers of nuclear technology to the Saudis is - do we think we have enough controls in place that we can trust them since they've been pretty clear about their intentions should things go bad with Iran? BRUMFIEL: She hopes the U. S. will seek assurances that Saudi Arabia will not pursue civilian technologies that could allow it to make a bomb. Geoff Brumfiel, NPR News, Washington.", "section": "World", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-06-720071488": {"title": "Amid Worries About Election Security, Microsoft Unveils Voting Machine Software : NPR", "url": "https://www.npr.org/2019/05/06/720071488/ahead-of-2020-microsoft-unveils-tool-to-allow-voters-to-track-their-ballots", "author": "No author found", "published_date": "2019-05-06", "content": "", "section": "Politics", "disclaimer": ""}, "2019-05-07-716410633": {"title": "VIDEO: How Computer-Assisted Telepathy Helps Humans Communicate : NPR", "url": "https://www.npr.org/2019/05/07/716410633/mind-machine-meld-how-computer-assisted-telepathy-helps-humans-communicate", "author": "No author found", "published_date": "2019-05-07", "content": "NOEL KING, HOST: And now, our co-host David Greene is going to tell us about a brand new series on NPR. DAVID GREENE, HOST: Yeah, that's right. It's a new beat here at NPR West in Culver City, Calif. And the beat belongs to our colleague Elise Hu. She is covering the future, among other things, in a new monthly video series. It's called Future You. And Elise is with me. Elise, how do you cover the future when it hasn't happened yet? ELISE HU, BYLINE: Well, it means I can't get anything wrong. . . GREENE: That's true. That's good. HU: . . . Or you can't fact-check me until way later. GREENE: Yeah, that's a perk. But seriously, what have you been looking at? HU: Well, what we're really focusing on is how human intelligence and artificial intelligence are melding more and more. GREENE: Sounds like stuff out of a science fiction movie, but it - that's really happening. HU: Yeah, and there's a lot more happening than I thought was possible. Elon Musk started a company called Neuralink that's working on this. Facebook began working on a way for us to type straight from our brains. And the Venmo billionaire Bryan Johnson started a company called Kernel, and he says the goal is improving human cognition. So all these companies and researchers are saying that connecting the most sophisticated computer on the planet, which is our brains, to outside machines could change who we are as humans. GREENE: So you sort of immersed yourself in this world recently. And just take us there. Tell me what you've been discovering. HU: Well, there have been a number of developments just within the past few years. People can now just wear a cap that's plugged into what's called a brain machine interface and learn how to move things like robot arms or send thoughts. Computer-assisted telepathy - I tried it. GREENE: This is crazy. HU: And while we can't see what private tech firms are doing, we are able to see some of the published research. So I went to the University of Washington Center for Neurotechnology and met the head of the center there, Rajesh Rao. RAJESH RAO: All right. Here we are. HU: OK. This is everybody. RAO: So we have a brain-to-brain interface called BrainNet, which allows two people to communicate with a third person directly using brain signals. HU: So the experiment lets three players, one in each room, simultaneously play a video game like Tetris. Remember Tetris, David? GREENE: Of course - moving blocks, rotating them to make sure they fall into place. Yeah. HU: Right. So in this situation, there were three of us, each in separate rooms, looking at screens. And we're all wearing these caps with electrodes on them called EEG caps, hooked up to computers. Two of us are the senders of thoughts. So we could see the block in the bottom row. . . GREENE: Which is critical to make sure that it fits in if you have to rotate it. HU: Exactly. GREENE: Yeah. HU: But the third player - the one who actually had to decide whether to rotate the block to clear the row - he couldn't see that bottom row. So he completely relied on us to tell him whether to rotate the block through our minds. And all we did was look at flashing strobe lights that flashed at different rates for yes or for no when we decided to send the thought. UNIDENTIFIED PERSON: All you got to do is just sit back and relax. HU: Let's do - sit back and relax in your brain bonnet while strobing lights come at you. . . UNIDENTIFIED PERSON: Exactly. HU: . . . At different frequencies. UNIDENTIFIED PERSON: Yep. HU: So the computer can pick up on my brain reacting to the flashing light at yes or at a different rate of pulsing on no. So then it would send that signal to the player in the other room. He would see a kind of glow in his field of vision that, yes, he should rotate the block. GREENE: Did it work? HU: Oh, yeah. We totally did it. This experiment has been run a number of times now, and it's a well-known test case for computer-assisted telepathy. GREENE: OK. So this is an early experiment, but what are researchers saying? Where could this go? HU: Well, I asked Rajesh Rao, the head of UW's neurotech center, about this and what he imagines. RAO: Transfer of knowledge and skills definitely is a possibility. If you watched the movie \"The Matrix\" - learning kung fu, for example, just by downloading it. So how'd you do in calculus? Were you great in calculus? Well. . . HU: No. RAO: (Laughter) No. OK. HU: Absolutely not. RAO: So that's an example where just downloading it might not be sufficient 'cause you might have questions. So you might want to have interactive tutoring - you know, brain tutoring. GREENE: I could've just tutored my brain directly and not studied calculus. HU: Yeah. Maybe direct knowledge transfer is a possibility for the future. GREENE: Is it - am I wrong? This is kind of creepy, too. HU: Well, all technology can have malicious uses and be used for ill, right? But in this case, it gets even thornier. And Rao addressed some of that, too. RAO: You could have brain tapping - you know, somebody reading your thoughts. You could have computer viruses. Imagine, you know, somebody doing that with a - like, a mind virus. HU: So malware for the brain or something. RAO: Exactly. HU: Yeah, mind malware. GREENE: That's crazy. Like, you could be thinking that you wanted to infect my brain somehow and make me have different thoughts. HU: Well, worse, it would be used by businesses or governments. GREENE: So this is important stuff you're covering. HU: Yeah, and it's exactly why I thought that it was an important frontier for us to be looking at. GREENE: All right. And you can look at it much more by watching the premiere episode of Elise's new video series Future You, With Elise Hu. I like the rhyme. Just go to npr. org/futureyou. Elise, thanks. HU: You're welcome. NOEL KING, HOST:  And now, our co-host David Greene is going to tell us about a brand new series on NPR. DAVID GREENE, HOST:  Yeah, that's right. It's a new beat here at NPR West in Culver City, Calif. And the beat belongs to our colleague Elise Hu. She is covering the future, among other things, in a new monthly video series. It's called Future You. And Elise is with me. Elise, how do you cover the future when it hasn't happened yet? ELISE HU, BYLINE: Well, it means I can't get anything wrong. . . GREENE: That's true. That's good. HU: . . . Or you can't fact-check me until way later. GREENE: Yeah, that's a perk. But seriously, what have you been looking at? HU: Well, what we're really focusing on is how human intelligence and artificial intelligence are melding more and more. GREENE: Sounds like stuff out of a science fiction movie, but it - that's really happening. HU: Yeah, and there's a lot more happening than I thought was possible. Elon Musk started a company called Neuralink that's working on this. Facebook began working on a way for us to type straight from our brains. And the Venmo billionaire Bryan Johnson started a company called Kernel, and he says the goal is improving human cognition. So all these companies and researchers are saying that connecting the most sophisticated computer on the planet, which is our brains, to outside machines could change who we are as humans. GREENE: So you sort of immersed yourself in this world recently. And just take us there. Tell me what you've been discovering. HU: Well, there have been a number of developments just within the past few years. People can now just wear a cap that's plugged into what's called a brain machine interface and learn how to move things like robot arms or send thoughts. Computer-assisted telepathy - I tried it. GREENE: This is crazy. HU: And while we can't see what private tech firms are doing, we are able to see some of the published research. So I went to the University of Washington Center for Neurotechnology and met the head of the center there, Rajesh Rao. RAJESH RAO: All right. Here we are. HU: OK. This is everybody. RAO: So we have a brain-to-brain interface called BrainNet, which allows two people to communicate with a third person directly using brain signals. HU: So the experiment lets three players, one in each room, simultaneously play a video game like Tetris. Remember Tetris, David? GREENE: Of course - moving blocks, rotating them to make sure they fall into place. Yeah. HU: Right. So in this situation, there were three of us, each in separate rooms, looking at screens. And we're all wearing these caps with electrodes on them called EEG caps, hooked up to computers. Two of us are the senders of thoughts. So we could see the block in the bottom row. . . GREENE: Which is critical to make sure that it fits in if you have to rotate it. HU: Exactly. GREENE: Yeah. HU: But the third player - the one who actually had to decide whether to rotate the block to clear the row - he couldn't see that bottom row. So he completely relied on us to tell him whether to rotate the block through our minds. And all we did was look at flashing strobe lights that flashed at different rates for yes or for no when we decided to send the thought. UNIDENTIFIED PERSON: All you got to do is just sit back and relax. HU: Let's do - sit back and relax in your brain bonnet while strobing lights come at you. . . UNIDENTIFIED PERSON: Exactly. HU: . . . At different frequencies. UNIDENTIFIED PERSON: Yep. HU: So the computer can pick up on my brain reacting to the flashing light at yes or at a different rate of pulsing on no. So then it would send that signal to the player in the other room. He would see a kind of glow in his field of vision that, yes, he should rotate the block. GREENE: Did it work? HU: Oh, yeah. We totally did it. This experiment has been run a number of times now, and it's a well-known test case for computer-assisted telepathy. GREENE: OK. So this is an early experiment, but what are researchers saying? Where could this go? HU: Well, I asked Rajesh Rao, the head of UW's neurotech center, about this and what he imagines. RAO: Transfer of knowledge and skills definitely is a possibility. If you watched the movie \"The Matrix\" - learning kung fu, for example, just by downloading it. So how'd you do in calculus? Were you great in calculus? Well. . . HU: No. RAO: (Laughter) No. OK. HU: Absolutely not. RAO: So that's an example where just downloading it might not be sufficient 'cause you might have questions. So you might want to have interactive tutoring - you know, brain tutoring. GREENE: I could've just tutored my brain directly and not studied calculus. HU: Yeah. Maybe direct knowledge transfer is a possibility for the future. GREENE: Is it - am I wrong? This is kind of creepy, too. HU: Well, all technology can have malicious uses and be used for ill, right? But in this case, it gets even thornier. And Rao addressed some of that, too. RAO: You could have brain tapping - you know, somebody reading your thoughts. You could have computer viruses. Imagine, you know, somebody doing that with a - like, a mind virus. HU: So malware for the brain or something. RAO: Exactly. HU: Yeah, mind malware. GREENE: That's crazy. Like, you could be thinking that you wanted to infect my brain somehow and make me have different thoughts. HU: Well, worse, it would be used by businesses or governments. GREENE: So this is important stuff you're covering. HU: Yeah, and it's exactly why I thought that it was an important frontier for us to be looking at. GREENE: All right. And you can look at it much more by watching the premiere episode of Elise's new video series Future You, With Elise Hu. I like the rhyme. Just go to npr. org/futureyou. Elise, thanks. HU: You're welcome.", "section": "Video: Future You, With Elise Hu", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-07-716412296": {"title": "VIDEO: How Brain-Controlled Exoskeletons Could Help People Walk : NPR", "url": "https://www.npr.org/2019/05/07/716412296/how-mind-controlled-robot-suits-could-enhance-our-limbs", "author": "No author found", "published_date": "2019-05-07", "content": "DAVID GREENE, HOST: All right, picture yourself inside a robotic suit, one that lets you pick up, say, huge heavy objects, just like a superhero. This is really a thing. Robotic exoskeletons, as they're called, are being used on the floor of hardware stores, also other places like that, so workers can lift heavier loads, and they're controlled by those workers' movements. But the more advanced exoskeletons, well, they're controlled by the mind, and they can help paralyzed people, even children, walk. NPR's Elise Hu has been checking all this out for her video series Future You, about how emerging technologies could change life as we know it. And Elise is back with us. Hey, Elise. ELISE HU, BYLINE: Good morning. GREENE: Just to be clear - this kind of stuff I'm talking about, this exists today? HU: Yeah. And the most promising ones of these exoskeletons are powered with thought control, as you mentioned, which means folks who are paralyzed from the waist down or even the neck down have been fitted for exoskeletons that let them use their minds to move robot arms or legs. We've actually all seen this. We saw a paralyzed man walk on the field in a robot suit to kick the opening ball at the 2014 World Cup. GREENE: Wow. That's incredible. HU: Yeah. The suits we're talking about here are powered by something I've been studying - brain-machine interfaces. They are computers wired to our brains, and they can read neurological signals to tell the robot limbs what to do. Research on this has been going on worldwide. And I went to the University of Houston in Texas to try out the exoskeletons there. Here we are. ATILLA KILICARSLAN: OK. So I'm going to ask you to lean forward a little bit, OK? HU: What's so cool about the work going on there in Houston is the scientists are focused on pediatric exoskeletons. So I am not who this experiment would be aimed at; it's actually aimed at helping paralyzed kids stand up and walk. GREENE: What a cool thing. HU: Yeah. GREENE: I mean, you got an idea of what it's like for them to be in a robotic suit or what the experience is like to be wearing something that's actually reading your brain. HU: Right. And if you've seen some of my videos testing this kind of technology, you've seen me wear kind of a swim cap with sensors all over it. GREENE: It did look like that, yeah. HU: (Laughter) And that's what this uses. GREENE: So what is the science here? How is this actually working? HU: The University of Houston researcher there, Atilla Kilicarslan, he worked with me. And what he did is he fit my body into a heavy, metallic robot suit. Whoa. And then my head's connected to electrodes, and the wires are connected to a computer. So I'm not exactly very nimble in this suit. (LAUGHTER)HU: But from there, I had to learn to stop when I heard there was a beep or then walk when I heard the next beep. (SOUNDBITE OF BEEP)HU: I did it. (LAUGHTER)HU: Feels like a superpower. GREENE: I mean, it must feel like a superpower if this thing is reading your brain impulses. I mean, where does this technology go? HU: Lots of possibilities for human augmentation. And what I mean by this is, it all started on soldiers, trying to help soldiers in the field carry larger loads, move faster. And now the technology behind it is being applied to medicine, to rehab, to construction. The researcher there in Houston, Kilicarslan, put it to me this way. KILICARSLAN: If you're a firefighter and if you need to lift something really heavy and save lives, then why not? By all means, go ahead and, you know, augment the human capability there. HU: This idea of super strength isn't something that the University of Houston team is working on; they are working on those pediatric exoskeletons. But this has been looked at by the Defense Department and its research arm, DARPA. GREENE: I mean, I'm not joking here. It sounds like we could all start becoming, like, cyborgs if we use this kind of thing. And does that start to raise questions about where the line is? HU: It absolutely does, and it's actually something that philosophers and ethicists are debating right now. And one of the big worries when I report on this is lethality. One of the fathers of brain-machine interfaces talked with me and said he doesn't want to see military uses of these mind-controlled robot suits because he's worried about how it could be used to help humans kill each other. GREENE: And you and I have talked about this in your series, I mean, when technology goes too far. It's like, it's great if it's helping people who are paralyzed walk, but what if private businesses, what if the government gets control of stuff like this and uses it for the wrong reasons? HU: Right, and these are a lot of the questions that are being worked out and debated now. Right now, though, the near-term promises to get paralyzed patients back on their feet again, that is keeping scientists funded and hopeful. And with all of this human augmentation technology we've been exploring, it is worthwhile, though, to consider the implications that you're talking about. GREENE: Elise, it's always interesting stuff. Elise Hu is in Culver City, Calif. , at NPR West. And we really appreciate it. HU: Thank you. GREENE: And that is part of NPR's monthly original video series, Future You with Elise Hu. And you can see those robot legs in action on NPR's YouTube channel or by going to npr. org/futureyou. (SOUNDBITE OF SKINNY WILLIAMS & STEPHEN GOODSON'S \"POP STAR EXPLOSION\") DAVID GREENE, HOST:  All right, picture yourself inside a robotic suit, one that lets you pick up, say, huge heavy objects, just like a superhero. This is really a thing. Robotic exoskeletons, as they're called, are being used on the floor of hardware stores, also other places like that, so workers can lift heavier loads, and they're controlled by those workers' movements. But the more advanced exoskeletons, well, they're controlled by the mind, and they can help paralyzed people, even children, walk. NPR's Elise Hu has been checking all this out for her video series Future You, about how emerging technologies could change life as we know it. And Elise is back with us. Hey, Elise. ELISE HU, BYLINE: Good morning. GREENE: Just to be clear - this kind of stuff I'm talking about, this exists today? HU: Yeah. And the most promising ones of these exoskeletons are powered with thought control, as you mentioned, which means folks who are paralyzed from the waist down or even the neck down have been fitted for exoskeletons that let them use their minds to move robot arms or legs. We've actually all seen this. We saw a paralyzed man walk on the field in a robot suit to kick the opening ball at the 2014 World Cup. GREENE: Wow. That's incredible. HU: Yeah. The suits we're talking about here are powered by something I've been studying - brain-machine interfaces. They are computers wired to our brains, and they can read neurological signals to tell the robot limbs what to do. Research on this has been going on worldwide. And I went to the University of Houston in Texas to try out the exoskeletons there. Here we are. ATILLA KILICARSLAN: OK. So I'm going to ask you to lean forward a little bit, OK? HU: What's so cool about the work going on there in Houston is the scientists are focused on pediatric exoskeletons. So I am not who this experiment would be aimed at; it's actually aimed at helping paralyzed kids stand up and walk. GREENE: What a cool thing. HU: Yeah. GREENE: I mean, you got an idea of what it's like for them to be in a robotic suit or what the experience is like to be wearing something that's actually reading your brain. HU: Right. And if you've seen some of my videos testing this kind of technology, you've seen me wear kind of a swim cap with sensors all over it. GREENE: It did look like that, yeah. HU: (Laughter) And that's what this uses. GREENE: So what is the science here? How is this actually working? HU: The University of Houston researcher there, Atilla Kilicarslan, he worked with me. And what he did is he fit my body into a heavy, metallic robot suit. Whoa. And then my head's connected to electrodes, and the wires are connected to a computer. So I'm not exactly very nimble in this suit. (LAUGHTER) HU: But from there, I had to learn to stop when I heard there was a beep or then walk when I heard the next beep. (SOUNDBITE OF BEEP) HU: I did it. (LAUGHTER) HU: Feels like a superpower. GREENE: I mean, it must feel like a superpower if this thing is reading your brain impulses. I mean, where does this technology go? HU: Lots of possibilities for human augmentation. And what I mean by this is, it all started on soldiers, trying to help soldiers in the field carry larger loads, move faster. And now the technology behind it is being applied to medicine, to rehab, to construction. The researcher there in Houston, Kilicarslan, put it to me this way. KILICARSLAN: If you're a firefighter and if you need to lift something really heavy and save lives, then why not? By all means, go ahead and, you know, augment the human capability there. HU: This idea of super strength isn't something that the University of Houston team is working on; they are working on those pediatric exoskeletons. But this has been looked at by the Defense Department and its research arm, DARPA. GREENE: I mean, I'm not joking here. It sounds like we could all start becoming, like, cyborgs if we use this kind of thing. And does that start to raise questions about where the line is? HU: It absolutely does, and it's actually something that philosophers and ethicists are debating right now. And one of the big worries when I report on this is lethality. One of the fathers of brain-machine interfaces talked with me and said he doesn't want to see military uses of these mind-controlled robot suits because he's worried about how it could be used to help humans kill each other. GREENE: And you and I have talked about this in your series, I mean, when technology goes too far. It's like, it's great if it's helping people who are paralyzed walk, but what if private businesses, what if the government gets control of stuff like this and uses it for the wrong reasons? HU: Right, and these are a lot of the questions that are being worked out and debated now. Right now, though, the near-term promises to get paralyzed patients back on their feet again, that is keeping scientists funded and hopeful. And with all of this human augmentation technology we've been exploring, it is worthwhile, though, to consider the implications that you're talking about. GREENE: Elise, it's always interesting stuff. Elise Hu is in Culver City, Calif. , at NPR West. And we really appreciate it. HU: Thank you. GREENE: And that is part of NPR's monthly original video series, Future You with Elise Hu. And you can see those robot legs in action on NPR's YouTube channel or by going to npr. org/futureyou. (SOUNDBITE OF SKINNY WILLIAMS & STEPHEN GOODSON'S \"POP STAR EXPLOSION\")", "section": "Video: Future You, With Elise Hu", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-08-720728055": {"title": "Making Nuclear Energy Smaller, Cheaper And Safer : NPR", "url": "https://www.npr.org/2019/05/08/720728055/this-company-says-the-future-of-nuclear-energy-is-smaller-cheaper-and-safer", "author": "No author found", "published_date": "2019-05-08", "content": "LULU GARCIA-NAVARRO, HOST:  Nuclear power plants are so big, complicated and expensive to build that more are shutting down than opening up. An Oregon company wants to build a new kind of nuclear power plant that many see as the future of the industry. NPR's Jeff Brady reports. JEFF BRADY, BYLINE: Power plants are so big because they're designed to take advantage of economies of scale. JOSE REYES: What we've done is we've developed economies of small. BRADY: Jose Reyes is co-founder of NuScale Power and says his company's reactors are more adaptable for a world moving away from fossil fuels to cleaner forms of energy. REYES: We haven't just taken a large plant and shrunk it down. We've completely simplified it and changed how we operate those plants. BRADY: And, Reyes says, the design cuts expensive construction time by about half. NuScale hasn't actually built a plant yet, though it does have models of its design about an hour and a half south of Portland, Ore. One is a simulated control room. UNIDENTIFIED PERSON #1: The room we're in is the same size as the room we expect the control room to be in. BRADY: Outside, there's a mockup of one of the modules that would hold a NuScale reactor. It's a big, metal tank several stories tall but still much smaller than a typical reactor. And this one has a very squeaky door. (SOUNDBITE OF DOOR SQUEAKING)UNIDENTIFIED PERSON #2: This is kind of the main access port for when we want to. . . BRADY: Inside, it's painted industrial gray, and only a few people can fit at a time. Instead of one big reactor powering a plant, NuScale plans a series of up to 12 much smaller reactors like this one. They'd be built in a factory and transported by truck. Karin Feldman is a vice president with the company and says that manufacturing will get more efficient over time. KARIN FELDMAN: We know that the first one's going to take longer than the second one. But we anticipate by the time you get to, say, the third one or the fourth one, you've learned everything that you need to learn about that manufacturing process. And you can be very predictable. BRADY: Feldman says this design is safer, too. The Fukushima disaster in 2011 happened when a tsunami knocked offline the emergency generators that cooled the reactors and spent fuel, leading to a meltdown. Feldman says NuScale's design is cooled passively. The reactors sit underground in a huge pool of water that can absorb heat. FELDMAN: The reactor will fail to a safe position. It doesn't require additional water, doesn't require AC or DC power, doesn't require any operator action. And it can stay in that safe configuration for as long as it's needed. BRADY: NuScale plans to build its first nuclear power plant at the Idaho National Lab. The electricity will head across the state border to a group of utilities called the Utah Associated Municipal Power System - or UAMPS. The group was looking for a carbon-free source of electricity to generate power when solar panels and wind turbines can't. While big nuclear reactors run all the time, this collection of smaller reactors can be ramped up and down relatively quickly to meet demand. Batteries can do that too, but UAMPS CEO Doug Hunter says NuScale's reactors are cheaper. DOUG HUNTER: Each module will be - have enough fuel in it for two years of operation, so it's like we're a battery that has a two-year charge to it. BRADY: NuScale still must convince regulators the plant is safe. That's a challenge because the design is so different that existing regulations have to be changed. That worries Edwin Lyman with the Union of Concerned Scientists. EDWIN LYMAN: My concern about NuScale is that they believe so deeply that the reactor is safe and doesn't need to meet the same criteria as the large reactors that it's pushing for lots of exemptions and exceptions. BRADY: So Lyman will be among those watching regulators closely as NuScale pushes to have its first power plant built and operating in 2026. Jeff Brady, NPR News. (SOUNDBITE OF THE VINES SONG, \"MARY JANE\") LULU GARCIA-NAVARRO, HOST:   Nuclear power plants are so big, complicated and expensive to build that more are shutting down than opening up. An Oregon company wants to build a new kind of nuclear power plant that many see as the future of the industry. NPR's Jeff Brady reports. JEFF BRADY, BYLINE: Power plants are so big because they're designed to take advantage of economies of scale. JOSE REYES: What we've done is we've developed economies of small. BRADY: Jose Reyes is co-founder of NuScale Power and says his company's reactors are more adaptable for a world moving away from fossil fuels to cleaner forms of energy. REYES: We haven't just taken a large plant and shrunk it down. We've completely simplified it and changed how we operate those plants. BRADY: And, Reyes says, the design cuts expensive construction time by about half. NuScale hasn't actually built a plant yet, though it does have models of its design about an hour and a half south of Portland, Ore. One is a simulated control room. UNIDENTIFIED PERSON #1: The room we're in is the same size as the room we expect the control room to be in. BRADY: Outside, there's a mockup of one of the modules that would hold a NuScale reactor. It's a big, metal tank several stories tall but still much smaller than a typical reactor. And this one has a very squeaky door. (SOUNDBITE OF DOOR SQUEAKING) UNIDENTIFIED PERSON #2: This is kind of the main access port for when we want to. . . BRADY: Inside, it's painted industrial gray, and only a few people can fit at a time. Instead of one big reactor powering a plant, NuScale plans a series of up to 12 much smaller reactors like this one. They'd be built in a factory and transported by truck. Karin Feldman is a vice president with the company and says that manufacturing will get more efficient over time. KARIN FELDMAN: We know that the first one's going to take longer than the second one. But we anticipate by the time you get to, say, the third one or the fourth one, you've learned everything that you need to learn about that manufacturing process. And you can be very predictable. BRADY: Feldman says this design is safer, too. The Fukushima disaster in 2011 happened when a tsunami knocked offline the emergency generators that cooled the reactors and spent fuel, leading to a meltdown. Feldman says NuScale's design is cooled passively. The reactors sit underground in a huge pool of water that can absorb heat. FELDMAN: The reactor will fail to a safe position. It doesn't require additional water, doesn't require AC or DC power, doesn't require any operator action. And it can stay in that safe configuration for as long as it's needed. BRADY: NuScale plans to build its first nuclear power plant at the Idaho National Lab. The electricity will head across the state border to a group of utilities called the Utah Associated Municipal Power System - or UAMPS. The group was looking for a carbon-free source of electricity to generate power when solar panels and wind turbines can't. While big nuclear reactors run all the time, this collection of smaller reactors can be ramped up and down relatively quickly to meet demand. Batteries can do that too, but UAMPS CEO Doug Hunter says NuScale's reactors are cheaper. DOUG HUNTER: Each module will be - have enough fuel in it for two years of operation, so it's like we're a battery that has a two-year charge to it. BRADY: NuScale still must convince regulators the plant is safe. That's a challenge because the design is so different that existing regulations have to be changed. That worries Edwin Lyman with the Union of Concerned Scientists. EDWIN LYMAN: My concern about NuScale is that they believe so deeply that the reactor is safe and doesn't need to meet the same criteria as the large reactors that it's pushing for lots of exemptions and exceptions. BRADY: So Lyman will be among those watching regulators closely as NuScale pushes to have its first power plant built and operating in 2026. Jeff Brady, NPR News. (SOUNDBITE OF THE VINES SONG, \"MARY JANE\")", "section": "Environment And Energy Collaborative", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-09-721772856": {"title": "China Mobile USA Blocked From Providing International Phone Services By The FCC : NPR", "url": "https://www.npr.org/2019/05/09/721772856/fcc-blocks-chinese-companys-bid-for-international-phone-services-in-the-u-s", "author": "No author found", "published_date": "2019-05-09", "content": "", "section": "Technology", "disclaimer": ""}, "2019-05-12-722647774": {"title": "'Code For Venezuela' Aims To Find Solutions For The Embattled Country : NPR", "url": "https://www.npr.org/2019/05/12/722647774/code-for-venezuela-aims-to-find-solutions-for-the-embattled-country", "author": "No author found", "published_date": "2019-05-12", "content": "MICHEL MARTIN, HOST: We've been talking for some time now about the chaos in Venezuela. The power struggle between the Maduro regime and the opposition continues. The economy is in shambles, infrastructure's breaking down. All that, not to mention the shortages of food, medicine and other critical supplies, have caused millions of people to leave the country in recent years. Among those who have left are tech workers who've landed in California's Silicon Valley and other tech hubs. And some of these expatriates have started a group called Code For Venezuela, with the goal of trying to help solve some of the problems back in Venezuela using technology. One of the group's founders, Jose Montes de Oca, is joining us now from San Francisco. Jose, thanks so much for talking with us. JOSE MONTES DE OCA: No, thank you, Michel. It's our pleasure. MARTIN: So tell me a little bit about your background, and how long have you been living outside of Venezuela, and what caused you to leave? MONTES DE OCA: Well, I am born and raised in Barquisimeto, Venezuela. I study computer science. Once I started finishing my school in 2010, I started to think, OK, what's next in my career? Started applying to big tech companies like Google, Facebook, Microsoft. And I got an offer from Google and came to the Valley in 2011. MARTIN: So how did the idea for Code For Venezuela come about? What was the goal of the group? MONTES DE OCA: It's a group of friends that - we've known each other for three or four years now here in the Bay Area. And right about late last year, we started, like, really brainstorm - hey, you know, what can we do that's not only financial that could really impact the country? MARTIN: Can you give me a sense of what some of the projects are that you've tried to work on so far, that you're trying to innovate now? MONTES DE OCA: Yes, yes. So to give you some context, as we were doing this brainstorm, we set a goal. OK, we had, like, three hypotheses that we wanted to prove. The first one being, you know, can we unite the Venezuelan expats? And then the second one, can we find organizations that are working in Venezuela on the ground that need help with technology? And then the third is like, as, like, Code For Venezuela, can we connect the two? So, you know, with that in mind, we thought, hey, let's put a goal of a hackathon. Let's come up with interesting challenges and put it out there for the community. So, for example, we have the pleasure to be working with Dr. Julio Castro, who is a very prominent doctor in Venezuela expert in Malaysia. And the work that he has been doing in Venezuela is remarkable. He has a group called Medicos Por La Salud. They have been tracking data points around everything that you can imagine in the health system. So think about it sort of like a parallel minister of Health. So he's tracking medicine, supply, the number of beds or what's needed on the hospital. All of this based on like groundwork, crowd sourcing via Twitter and WHY NOT. And he has been gathering this data but very rudimentary. So when he came to us with the problem and what he was doing, we were like, wow, you know, we can definitely be your technical arm and elevate your work. MARTIN: Have you been able to solve any specific problems in real time yet? I mean, you can see where, for expatriates, this is probably relieving to have something that you can do to help to try to help because it has to be frustrating to live outside the country and have your countrymen and probably your relatives going through all this. And you probably feel like, you know, what can I do? MONTES DE OCA: Yes. So in the hackathon, we had 17 submissions. Three of them were completed, and four our ongoing. So like, you know, one of the ones that, you know, we're pretty excited is related to this Twitter data where people are, like, requesting medicines or offering medicine. A team built a Twitter bot that replies to those requests and try to match - do matchmaking between offer and the match. MARTIN: That's exciting. I mean, I'm guessing this has been a fulfilling project for you. I mean, how has it made you feel to work on this and to see your work starting to grow? MONTES DE OCA: Amazing. I think this is the best I've felt in a long time - to actively solve and engage with, you know, challenges in the country has been very gratifying for us as a team. MARTIN: That's Jose Montes de Oca. He's one of the founders of Code For Venezuela. And we reached him in San Francisco. Jose, thanks so much for talking to us. MONTES DE OCA: No, thank you for the time. MICHEL MARTIN, HOST:  We've been talking for some time now about the chaos in Venezuela. The power struggle between the Maduro regime and the opposition continues. The economy is in shambles, infrastructure's breaking down. All that, not to mention the shortages of food, medicine and other critical supplies, have caused millions of people to leave the country in recent years. Among those who have left are tech workers who've landed in California's Silicon Valley and other tech hubs. And some of these expatriates have started a group called Code For Venezuela, with the goal of trying to help solve some of the problems back in Venezuela using technology. One of the group's founders, Jose Montes de Oca, is joining us now from San Francisco. Jose, thanks so much for talking with us. JOSE MONTES DE OCA: No, thank you, Michel. It's our pleasure. MARTIN: So tell me a little bit about your background, and how long have you been living outside of Venezuela, and what caused you to leave? MONTES DE OCA: Well, I am born and raised in Barquisimeto, Venezuela. I study computer science. Once I started finishing my school in 2010, I started to think, OK, what's next in my career? Started applying to big tech companies like Google, Facebook, Microsoft. And I got an offer from Google and came to the Valley in 2011. MARTIN: So how did the idea for Code For Venezuela come about? What was the goal of the group? MONTES DE OCA: It's a group of friends that - we've known each other for three or four years now here in the Bay Area. And right about late last year, we started, like, really brainstorm - hey, you know, what can we do that's not only financial that could really impact the country? MARTIN: Can you give me a sense of what some of the projects are that you've tried to work on so far, that you're trying to innovate now? MONTES DE OCA: Yes, yes. So to give you some context, as we were doing this brainstorm, we set a goal. OK, we had, like, three hypotheses that we wanted to prove. The first one being, you know, can we unite the Venezuelan expats? And then the second one, can we find organizations that are working in Venezuela on the ground that need help with technology? And then the third is like, as, like, Code For Venezuela, can we connect the two? So, you know, with that in mind, we thought, hey, let's put a goal of a hackathon. Let's come up with interesting challenges and put it out there for the community. So, for example, we have the pleasure to be working with Dr. Julio Castro, who is a very prominent doctor in Venezuela expert in Malaysia. And the work that he has been doing in Venezuela is remarkable. He has a group called Medicos Por La Salud. They have been tracking data points around everything that you can imagine in the health system. So think about it sort of like a parallel minister of Health. So he's tracking medicine, supply, the number of beds or what's needed on the hospital. All of this based on like groundwork, crowd sourcing via Twitter and WHY NOT. And he has been gathering this data but very rudimentary. So when he came to us with the problem and what he was doing, we were like, wow, you know, we can definitely be your technical arm and elevate your work. MARTIN: Have you been able to solve any specific problems in real time yet? I mean, you can see where, for expatriates, this is probably relieving to have something that you can do to help to try to help because it has to be frustrating to live outside the country and have your countrymen and probably your relatives going through all this. And you probably feel like, you know, what can I do? MONTES DE OCA: Yes. So in the hackathon, we had 17 submissions. Three of them were completed, and four our ongoing. So like, you know, one of the ones that, you know, we're pretty excited is related to this Twitter data where people are, like, requesting medicines or offering medicine. A team built a Twitter bot that replies to those requests and try to match - do matchmaking between offer and the match. MARTIN: That's exciting. I mean, I'm guessing this has been a fulfilling project for you. I mean, how has it made you feel to work on this and to see your work starting to grow? MONTES DE OCA: Amazing. I think this is the best I've felt in a long time - to actively solve and engage with, you know, challenges in the country has been very gratifying for us as a team. MARTIN: That's Jose Montes de Oca. He's one of the founders of Code For Venezuela. And we reached him in San Francisco. Jose, thanks so much for talking to us. MONTES DE OCA: No, thank you for the time.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-13-722909218": {"title": "This Tech Worker Objected To Company's Work On Military Project : NPR", "url": "https://www.npr.org/2019/05/13/722909218/when-technology-can-be-used-to-build-weapons-some-workers-take-a-stand", "author": "No author found", "published_date": "2019-05-13", "content": "ARI SHAPIRO, HOST: Even if you can advance technology, create the next great app or a robot that fights wars, should you? We're exploring that question on this month's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")SHAPIRO: Here in the U. S. , there is little government oversight of the tech industry. So more and more it is the tech workers themselves who are raising ethical concerns. NPR's Jasmine Garsd reports on one company and an employee who says she'd had enough. JASMINE GARSD, BYLINE: Earlier this year, on the night of January 16, Liz O'Sullivan hit send on a letter she'd been working on for weeks. It was directed at her boss, Matt Zeiler, the founder and CEO of Clarifai, a tech company. LIZ O'SULLIVAN: The moment before I hit send, I mean, and then afterwards, my heart - I could just feel it racing. GARSD: The letter asked the question, is our technology going to be used to build weapons? O'Sullivan is 34. She's from the generation that saw the birth of high-speed Internet, Facebook, Venmo, Uber. She often describes technology as magic. O'SULLIVAN: There are companies out there doing things that really look like magic. They feel like magic. GARSD: O'Sullivan's story begins two years ago, when she started working at Clarifai. She says one of her jobs was to explain the company's product to customers. It's visual recognition technology. It's used by websites to identify nudity and inappropriate content. Doctors use it to spot disease. It was a startup. But shortly after O'Sullivan joined, Clarifai got a big break - a government contract reportedly for millions of dollars. It was all very secretive. At first, the people assigned to work on that government project were in a windowless room with the glass doors covered. O'Sullivan would walk by and wonder, what are they doing in there? Matt Zeiler, CEO of Clarifai, says the contract required secrecy. But everyone working directly on the project knew what it was about. Here's Zeiler. (SOUNDBITE OF ARCHIVED RECORDING)MATT ZEILER: We got briefed before even writing a single line of code. And I also briefed everybody I asked to participate on this project. GARSD: NPR spoke to one employee who did work directly on the project. That person, who requested anonymity for fear of retaliation, said many of the workers in that room were not entirely clear what this was going to be used for. The technology they were putting together, it's the same that they had been working on for other projects. In the months that followed, former employees say information started trickling down. They were working with the Department of Defense. Then people working on the project got an email that outlined some details. In the text, a blink-and-you'll-miss-it reference to something called Project Maven. The Pentagon told NPR that Project Maven was created in April 2017. It's also called algorithmic warfare. Its first task was to use computer vision technology for drones in the campaign against ISIS. BEN SHNEIDERMAN: This could be more effective than humans, who might miss something or misunderstand something, that the computer vision could be more accurate. GARSD: That's professor Ben Shneiderman, a computer scientist at the University of Maryland, talking on Skype. He had serious ethical concerns about the project. He wasn't alone. Many people in the tech world were starting to wonder, what is this technology we're building going to be used for down the road? Liz O'Sullivan says this question began to haunt her, too. The big fear among tech activists is, will this be used toward building autonomous weapons? That's weapons that are programmed to find targets and kill people without human intervention. The Department of Defense's current policy requires that autonomous weapons, quote, \"allow commanders and operators to exercise appropriate levels of human judgment. \" It's a definition many find murky. And in 2018, tech workers began to ask a lot of questions. Here's professor Shneiderman again. SHNEIDERMAN: It's a historic moment of the employees rising up in a principled way, an ethical way and saying, we won't do this. GARSD: Microsoft employees protested their company's work with Immigration and Customs Enforcement. And several thousand employees demanded that Google stop working on Project Maven. Google did not renew its contract with the project. In June of last year, Clarifai CEO Matt Zeiler also weighed in. In a blog post, he explained why the company was working on a military project. Liz O'Sullivan read that with interest. O'SULLIVAN: You know, the people running these companies are sort of techno utopians. And they believe that tech is going to save the world and that we really just have to build everything that we can and then figure out where the cards fall. But there are a lot of us out here saying, should we be building this at all? GARSD: Former Clarifai employees told NPR that at the office, the mood got tense. There were plenty of people who felt comfortable working on Project Maven. Others resented that it had been so secretive. And some just found it morally troubling. As the months went by, O'Sullivan says she realized she couldn't change the direction of the company. So at the beginning of this year, she wrote that letter to CEO Matt Zeiler and sent it to the whole staff. Here she is reading an excerpt. O'SULLIVAN: (Reading) We have serious concerns about recent events and are beginning to worry about what we're all working so hard to build. GARSD: She goes on to ask a bunch of questions. Many of them are the same questions being asked across the tech world today. Like, are you going to let us know who we're selling our stuff to? Are you going to vet how it's used? Do we care if this is used to hurt people? A week after she sent that letter, there was a staff meeting where Zeiler spoke. O'SULLIVAN: He did say that our technology was likely to be used for weapons - and autonomous weapons at that. GARSD: Clarifai CEO Matt Zeiler does not deny this. In fact, he says, countries like China, they're already doing it. The U. S. needs to step it up. (SOUNDBITE OF ARCHIVED RECORDING)ZEILER: We're not going to be building missiles or any kind of stuff like that at Clarifai. But the technology, like I was saying, is going to be useful for those. And through partnerships with the DOD and other contractors, I do think it will make its way into autonomous weapons. GARSD: Here's where he and O'Sullivan disagree. Should companies like Clarifai, Google and Amazon be involved in military projects? Zeiler says Clarifai's technology is going to help save American soldiers. (SOUNDBITE OF ARCHIVED RECORDING)ZEILER: At the end of the day, they're out there to do a mission. And if we can provide the best technology so that they can accurately do their mission, you know, in the worst case, there might be a human life at the other end that they're targeting. But in many cases, it might be a weapons cache that's not - any humans around or a bridge to slow down an enemy threat. GARSD: And Zeiler says also it's going to help minimize civilian casualties by improving the accuracy of weapons. O'Sullivan wasn't buying that. She quit the day after the staff meeting. She describes herself as a conscientious tech objector. She went on to join a startup that advises companies on how to make trustworthy artificial intelligence. She says she still thinks tech can be really wonderful or really dangerous, like playing with magic. Jasmine Garsd, NPR News, New York. ARI SHAPIRO, HOST:  Even if you can advance technology, create the next great app or a robot that fights wars, should you? We're exploring that question on this month's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") SHAPIRO: Here in the U. S. , there is little government oversight of the tech industry. So more and more it is the tech workers themselves who are raising ethical concerns. NPR's Jasmine Garsd reports on one company and an employee who says she'd had enough. JASMINE GARSD, BYLINE: Earlier this year, on the night of January 16, Liz O'Sullivan hit send on a letter she'd been working on for weeks. It was directed at her boss, Matt Zeiler, the founder and CEO of Clarifai, a tech company. LIZ O'SULLIVAN: The moment before I hit send, I mean, and then afterwards, my heart - I could just feel it racing. GARSD: The letter asked the question, is our technology going to be used to build weapons? O'Sullivan is 34. She's from the generation that saw the birth of high-speed Internet, Facebook, Venmo, Uber. She often describes technology as magic. O'SULLIVAN: There are companies out there doing things that really look like magic. They feel like magic. GARSD: O'Sullivan's story begins two years ago, when she started working at Clarifai. She says one of her jobs was to explain the company's product to customers. It's visual recognition technology. It's used by websites to identify nudity and inappropriate content. Doctors use it to spot disease. It was a startup. But shortly after O'Sullivan joined, Clarifai got a big break - a government contract reportedly for millions of dollars. It was all very secretive. At first, the people assigned to work on that government project were in a windowless room with the glass doors covered. O'Sullivan would walk by and wonder, what are they doing in there? Matt Zeiler, CEO of Clarifai, says the contract required secrecy. But everyone working directly on the project knew what it was about. Here's Zeiler. (SOUNDBITE OF ARCHIVED RECORDING) MATT ZEILER: We got briefed before even writing a single line of code. And I also briefed everybody I asked to participate on this project. GARSD: NPR spoke to one employee who did work directly on the project. That person, who requested anonymity for fear of retaliation, said many of the workers in that room were not entirely clear what this was going to be used for. The technology they were putting together, it's the same that they had been working on for other projects. In the months that followed, former employees say information started trickling down. They were working with the Department of Defense. Then people working on the project got an email that outlined some details. In the text, a blink-and-you'll-miss-it reference to something called Project Maven. The Pentagon told NPR that Project Maven was created in April 2017. It's also called algorithmic warfare. Its first task was to use computer vision technology for drones in the campaign against ISIS. BEN SHNEIDERMAN: This could be more effective than humans, who might miss something or misunderstand something, that the computer vision could be more accurate. GARSD: That's professor Ben Shneiderman, a computer scientist at the University of Maryland, talking on Skype. He had serious ethical concerns about the project. He wasn't alone. Many people in the tech world were starting to wonder, what is this technology we're building going to be used for down the road? Liz O'Sullivan says this question began to haunt her, too. The big fear among tech activists is, will this be used toward building autonomous weapons? That's weapons that are programmed to find targets and kill people without human intervention. The Department of Defense's current policy requires that autonomous weapons, quote, \"allow commanders and operators to exercise appropriate levels of human judgment. \" It's a definition many find murky. And in 2018, tech workers began to ask a lot of questions. Here's professor Shneiderman again. SHNEIDERMAN: It's a historic moment of the employees rising up in a principled way, an ethical way and saying, we won't do this. GARSD: Microsoft employees protested their company's work with Immigration and Customs Enforcement. And several thousand employees demanded that Google stop working on Project Maven. Google did not renew its contract with the project. In June of last year, Clarifai CEO Matt Zeiler also weighed in. In a blog post, he explained why the company was working on a military project. Liz O'Sullivan read that with interest. O'SULLIVAN: You know, the people running these companies are sort of techno utopians. And they believe that tech is going to save the world and that we really just have to build everything that we can and then figure out where the cards fall. But there are a lot of us out here saying, should we be building this at all? GARSD: Former Clarifai employees told NPR that at the office, the mood got tense. There were plenty of people who felt comfortable working on Project Maven. Others resented that it had been so secretive. And some just found it morally troubling. As the months went by, O'Sullivan says she realized she couldn't change the direction of the company. So at the beginning of this year, she wrote that letter to CEO Matt Zeiler and sent it to the whole staff. Here she is reading an excerpt. O'SULLIVAN: (Reading) We have serious concerns about recent events and are beginning to worry about what we're all working so hard to build. GARSD: She goes on to ask a bunch of questions. Many of them are the same questions being asked across the tech world today. Like, are you going to let us know who we're selling our stuff to? Are you going to vet how it's used? Do we care if this is used to hurt people? A week after she sent that letter, there was a staff meeting where Zeiler spoke. O'SULLIVAN: He did say that our technology was likely to be used for weapons - and autonomous weapons at that. GARSD: Clarifai CEO Matt Zeiler does not deny this. In fact, he says, countries like China, they're already doing it. The U. S. needs to step it up. (SOUNDBITE OF ARCHIVED RECORDING) ZEILER: We're not going to be building missiles or any kind of stuff like that at Clarifai. But the technology, like I was saying, is going to be useful for those. And through partnerships with the DOD and other contractors, I do think it will make its way into autonomous weapons. GARSD: Here's where he and O'Sullivan disagree. Should companies like Clarifai, Google and Amazon be involved in military projects? Zeiler says Clarifai's technology is going to help save American soldiers. (SOUNDBITE OF ARCHIVED RECORDING) ZEILER: At the end of the day, they're out there to do a mission. And if we can provide the best technology so that they can accurately do their mission, you know, in the worst case, there might be a human life at the other end that they're targeting. But in many cases, it might be a weapons cache that's not - any humans around or a bridge to slow down an enemy threat. GARSD: And Zeiler says also it's going to help minimize civilian casualties by improving the accuracy of weapons. O'Sullivan wasn't buying that. She quit the day after the staff meeting. She describes herself as a conscientious tech objector. She went on to join a startup that advises companies on how to make trustworthy artificial intelligence. She says she still thinks tech can be really wonderful or really dangerous, like playing with magic. Jasmine Garsd, NPR News, New York.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-13-722831702": {"title": "Supreme Court Rules Against Apple, As Kavanaugh Sides With Liberal Justices : NPR", "url": "https://www.npr.org/2019/05/13/722831702/supreme-court-rules-against-apple-as-kavanaugh-sides-with-liberal-justices", "author": "No author found", "published_date": "2019-05-13", "content": "", "section": "Technology", "disclaimer": ""}, "2019-05-14-723215498": {"title": "Russian Hackers Broke Into Election Systems In 2 Florida Counties In 2016 : NPR", "url": "https://www.npr.org/2019/05/14/723215498/florida-governor-says-russian-hackers-breached-two-florida-counties-in-2016", "author": "No author found", "published_date": "2019-05-14", "content": "", "section": "Politics", "disclaimer": ""}, "2019-05-14-723193785": {"title": "San Francisco Bans Use Of Facial Recognition Technology By Municipal Government : NPR", "url": "https://www.npr.org/2019/05/14/723193785/san-francisco-considers-ban-on-governments-use-of-facial-recognition-technology", "author": "No author found", "published_date": "2019-05-14", "content": "", "section": "Technology", "disclaimer": ""}, "2019-05-14-722904731": {"title": "Is Your Family Fighting Over Screens? We Want To Help : NPR", "url": "https://www.npr.org/2019/05/14/722904731/is-your-family-fighting-over-screens-we-want-to-help", "author": "No author found", "published_date": "2019-05-14", "content": "", "section": "Education", "disclaimer": ""}, "2019-05-14-722933448": {"title": "Tech Export Controls Aimed At China Could Backfire, Researchers And Firms Say : NPR", "url": "https://www.npr.org/2019/05/14/722933448/stopping-key-tech-exports-to-china-could-backfire-researchers-and-firms-say", "author": "No author found", "published_date": "2019-05-14", "content": "STEVE INSKEEP, HOST: One of the most raw parts of the U. S. rivalry with China involves the way that China has obtained U. S. technology. And this year, the United States will likely add new restrictions on the export of certain technologies, a move that is mostly aimed at China. It is meant to help American companies, but it turns out it could also limit the ability of American firms to compete. Here's NPR's Emily Feng. EMILY FENG, BYLINE: The cybersecurity firm McAfee, known for its antivirus software, is constantly monitoring the Internet, searching for the next big threat. The company's chief technology officer, Steve Grobman, explains. STEVE GROBMAN: We actually have a billion sensors deployed around the world that allows us to see new threats, track where they go. FENG: Now McAfee is one of thousands of American technology companies at the frontlines of a global tech rivalry between the U. S. and China. Grobman says the Trump administration's plans to update export controls, which would restrict U. S. companies from exporting sensitive technologies, could give the bad guys the upper hand. GROBMAN: They're going to use the best technology that they can get their hands on in order to create the most malicious capabilities that would potentially impact our customers. FENG: Those controls are the most ambitious measures thus far, aimed at shutting down Chinese hackers and companies stealing technology to get ahead. That theft is a key sticking point in ongoing trade talks between Beijing and Washington to resolve a nearly one-year trade war. In November, the administration sought comment on possible new export controls for emerging and foundational technologies, technologies core to cutting-edge applications. That concerns researchers such as Joanne Kamens. JOANNE KAMENS: It just slows down science in general for all the people doing good and important discovery research. FENG: Kamens is executive director of Addgene, a biomedical nonprofit that supports research by sharing material like chromosomal DNA worldwide. She warns export controls could hurt this kind of cooperation, cutting the U. S. off from a global research community. That would open the door to China to race ahead. KAMENS: DNA is a linear sequence of letters. Once the letters are known, anyone in the world could, for money and time, make any one of these things. FENG: The new proposed controls could even restrict who universities and companies hire. Erin Ennis, the senior vice president of the U. S. -China Business Council, explains how companies might have to be licensed to pass knowledge of a technology to a foreign citizen. ERIN ENNIS: Not only is the product that they make covered by an export control, but the employees that they have would be covered by them, as well. FENG: That's worrying for many tech companies, such as those writing code and working on artificial intelligence. Addgene's Joanne Kamens stresses that modern research almost always crosses international boundaries. KAMENS: Basic research, which has given rise, of course, to so many important discoveries that have driven all of the cures in the clinic - you know, all of that work is done collaboratively with scientists between countries. FENG: Instead, cybersecurity analysts like New America's Samm Sacks advocate a small yard, high fence approach - restricting the sale of specific technologies with military applications but leaving everything else accessible. SAMM SACKS: Because tools like export controls, these kinds of things were meant to be used as scalpels but not as blunt instruments. FENG: Sometime this summer, the U. S. Commerce Department will release a second proposed export controls list for foundational technologies. American tech companies who submitted comments hope the Commerce Department will narrow down what is restricted. So far, they haven't heard back. Emily Feng, NPR News. (SOUNDBITE OF EL JAZZY CHAVO'S \"ONE FOR THE WAITRESS\") STEVE INSKEEP, HOST:  One of the most raw parts of the U. S. rivalry with China involves the way that China has obtained U. S. technology. And this year, the United States will likely add new restrictions on the export of certain technologies, a move that is mostly aimed at China. It is meant to help American companies, but it turns out it could also limit the ability of American firms to compete. Here's NPR's Emily Feng. EMILY FENG, BYLINE: The cybersecurity firm McAfee, known for its antivirus software, is constantly monitoring the Internet, searching for the next big threat. The company's chief technology officer, Steve Grobman, explains. STEVE GROBMAN: We actually have a billion sensors deployed around the world that allows us to see new threats, track where they go. FENG: Now McAfee is one of thousands of American technology companies at the frontlines of a global tech rivalry between the U. S. and China. Grobman says the Trump administration's plans to update export controls, which would restrict U. S. companies from exporting sensitive technologies, could give the bad guys the upper hand. GROBMAN: They're going to use the best technology that they can get their hands on in order to create the most malicious capabilities that would potentially impact our customers. FENG: Those controls are the most ambitious measures thus far, aimed at shutting down Chinese hackers and companies stealing technology to get ahead. That theft is a key sticking point in ongoing trade talks between Beijing and Washington to resolve a nearly one-year trade war. In November, the administration sought comment on possible new export controls for emerging and foundational technologies, technologies core to cutting-edge applications. That concerns researchers such as Joanne Kamens. JOANNE KAMENS: It just slows down science in general for all the people doing good and important discovery research. FENG: Kamens is executive director of Addgene, a biomedical nonprofit that supports research by sharing material like chromosomal DNA worldwide. She warns export controls could hurt this kind of cooperation, cutting the U. S. off from a global research community. That would open the door to China to race ahead. KAMENS: DNA is a linear sequence of letters. Once the letters are known, anyone in the world could, for money and time, make any one of these things. FENG: The new proposed controls could even restrict who universities and companies hire. Erin Ennis, the senior vice president of the U. S. -China Business Council, explains how companies might have to be licensed to pass knowledge of a technology to a foreign citizen. ERIN ENNIS: Not only is the product that they make covered by an export control, but the employees that they have would be covered by them, as well. FENG: That's worrying for many tech companies, such as those writing code and working on artificial intelligence. Addgene's Joanne Kamens stresses that modern research almost always crosses international boundaries. KAMENS: Basic research, which has given rise, of course, to so many important discoveries that have driven all of the cures in the clinic - you know, all of that work is done collaboratively with scientists between countries. FENG: Instead, cybersecurity analysts like New America's Samm Sacks advocate a small yard, high fence approach - restricting the sale of specific technologies with military applications but leaving everything else accessible. SAMM SACKS: Because tools like export controls, these kinds of things were meant to be used as scalpels but not as blunt instruments. FENG: Sometime this summer, the U. S. Commerce Department will release a second proposed export controls list for foundational technologies. American tech companies who submitted comments hope the Commerce Department will narrow down what is restricted. So far, they haven't heard back. Emily Feng, NPR News. (SOUNDBITE OF EL JAZZY CHAVO'S \"ONE FOR THE WAITRESS\")", "section": "World", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-15-723457869": {"title": "New Zealand PM Calls On Companies To Prevent Streaming Of Terrorist Attacks : NPR", "url": "https://www.npr.org/2019/05/15/723457869/new-zealands-ardern-calls-on-social-media-companies-to-stem-terrorist-content", "author": "No author found", "published_date": "2019-05-15", "content": "", "section": "Technology", "disclaimer": ""}, "2019-05-15-723466522": {"title": "Facebook Announces Restrictions To Its Live Feature : NPR", "url": "https://www.npr.org/2019/05/15/723466522/facebook-announces-restrictions-to-its-live-feature", "author": "No author found", "published_date": "2019-05-15", "content": "NOEL KING, HOST: Facebook has just announced a new restriction for its livestreaming features. RACHEL MARTIN, HOST: Yeah, they announced this change yesterday. And essentially it says that users of Facebook would be banned from posting on Facebook Live - just for a period of time - if they had violated the company's user rules in some way. And this is coming, of course, after the horrendous attack in New Zealand - the attack on the mosques in Christchurch a couple of months ago. The gunman had livestreamed that massacre. New Zealand's prime minister, Jacinda Ardern, at the time, issued a call to address the issue. (SOUNDBITE OF ARCHIVED RECORDING)PRIME MINISTER JACINDA ARDERN: The security risks that we've experienced means that it is time for companies and governments to come together to address the issues we experienced on the 15 of March. MARTIN: Jacinda Ardern is in Paris today, where she is expected to push governments, and also these tech companies, to commit to combating the spread of extremism on social media. KING: Heather Kelly is CNN, San Francisco, technology editor. She's been following this story carefully. Good morning, Heather. HEATHER KELLY: Good morning. Thanks for having me. KING: So this is one change that Facebook is making, but it seems as though it may be significant. Walk us through what exactly is changing about their policy. KELLY: Sure thing. So it's actually a very narrow and specific new rule that they're kind of enacting here. It is going to only apply to Facebook Live. And they're also being a little vague about it. But anybody who violates certain policies will get kind of a one-strike rule, and immediately they will be banned from Facebook Live for a set period of time. It sounds like they're going to start with 30 days, a second strike could be longer. And eventually, they could be kicked off of Facebook Live or even Facebook itself. And what's interesting is Facebook isn't actually saying which rules will kind of fall into this. It does say that anybody violating its dangerous organizations and individuals policy could be banned from Facebook Live. And that's essentially anybody that is basically supporting a terrorist organization or posting a link to a terrorist organization, perhaps without enough context to make it really clear where they stand. KING: Facebook - as Rachel pointed out - says they're doing this because of this massacre in Christchurch, New Zealand. The man who committed those attacks livestreamed them on Facebook. Would these changes have stopped him or someone like him from being able to post? Do we know that? KELLY: Facebook told me yesterday that this would have actually applied to the shooter. If these rules had been enacted before - he had apparently violated some Facebook rules, we don't know which, previously and he would have had a ban on his Facebook livestreaming. What's interesting, though, is, you know, hypothetically, if that did happen, he would have known that he was banned. He could have started a new account. He could have found a way around it. We don't really know how it would play out in that hypothetical. KING: Facebook is often slammed for not taking responsibility when things like this happen on its platforms. Does this policy seem to point to a shift in taking responsibility? KELLY: Again, it's such a narrow policy, it's really hard to read too much into it. It does address one kind of way that somebody might broadcast hate speech on the platform. But there are tons of other ways that people are kind of using Facebook to spread hate speech, conspiracy theories - all sorts of what it kind of calls hateful speech. And it's not really going to address all those or fix all those. It's just fixing this one situation that happened exactly two months ago. KING: CNN San Francisco technology editor Heather Kelly joining us via Skype. Heather, thank you so much. KELLY: Sure thing. KING: And we should note that Facebook is a sponsor of NPR. NOEL KING, HOST:  Facebook has just announced a new restriction for its livestreaming features. RACHEL MARTIN, HOST:  Yeah, they announced this change yesterday. And essentially it says that users of Facebook would be banned from posting on Facebook Live - just for a period of time - if they had violated the company's user rules in some way. And this is coming, of course, after the horrendous attack in New Zealand - the attack on the mosques in Christchurch a couple of months ago. The gunman had livestreamed that massacre. New Zealand's prime minister, Jacinda Ardern, at the time, issued a call to address the issue. (SOUNDBITE OF ARCHIVED RECORDING) PRIME MINISTER JACINDA ARDERN: The security risks that we've experienced means that it is time for companies and governments to come together to address the issues we experienced on the 15 of March. MARTIN: Jacinda Ardern is in Paris today, where she is expected to push governments, and also these tech companies, to commit to combating the spread of extremism on social media. KING: Heather Kelly is CNN, San Francisco, technology editor. She's been following this story carefully. Good morning, Heather. HEATHER KELLY: Good morning. Thanks for having me. KING: So this is one change that Facebook is making, but it seems as though it may be significant. Walk us through what exactly is changing about their policy. KELLY: Sure thing. So it's actually a very narrow and specific new rule that they're kind of enacting here. It is going to only apply to Facebook Live. And they're also being a little vague about it. But anybody who violates certain policies will get kind of a one-strike rule, and immediately they will be banned from Facebook Live for a set period of time. It sounds like they're going to start with 30 days, a second strike could be longer. And eventually, they could be kicked off of Facebook Live or even Facebook itself. And what's interesting is Facebook isn't actually saying which rules will kind of fall into this. It does say that anybody violating its dangerous organizations and individuals policy could be banned from Facebook Live. And that's essentially anybody that is basically supporting a terrorist organization or posting a link to a terrorist organization, perhaps without enough context to make it really clear where they stand. KING: Facebook - as Rachel pointed out - says they're doing this because of this massacre in Christchurch, New Zealand. The man who committed those attacks livestreamed them on Facebook. Would these changes have stopped him or someone like him from being able to post? Do we know that? KELLY: Facebook told me yesterday that this would have actually applied to the shooter. If these rules had been enacted before - he had apparently violated some Facebook rules, we don't know which, previously and he would have had a ban on his Facebook livestreaming. What's interesting, though, is, you know, hypothetically, if that did happen, he would have known that he was banned. He could have started a new account. He could have found a way around it. We don't really know how it would play out in that hypothetical. KING: Facebook is often slammed for not taking responsibility when things like this happen on its platforms. Does this policy seem to point to a shift in taking responsibility? KELLY: Again, it's such a narrow policy, it's really hard to read too much into it. It does address one kind of way that somebody might broadcast hate speech on the platform. But there are tons of other ways that people are kind of using Facebook to spread hate speech, conspiracy theories - all sorts of what it kind of calls hateful speech. And it's not really going to address all those or fix all those. It's just fixing this one situation that happened exactly two months ago. KING: CNN San Francisco technology editor Heather Kelly joining us via Skype. Heather, thank you so much. KELLY: Sure thing. KING: And we should note that Facebook is a sponsor of NPR.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-16-723983442": {"title": "International Cybercrime Network That Tried To Steal $100 Million Is Dismantled : NPR", "url": "https://www.npr.org/2019/05/16/723983442/authorities-dismantle-transnational-cybercrime-group", "author": "No author found", "published_date": "2019-05-16", "content": "", "section": "Technology", "disclaimer": ""}, "2019-05-16-723983055": {"title": "Trump's Isolation Of China's Huawei Could Affect Global Supply Chain : NPR", "url": "https://www.npr.org/2019/05/16/723983055/u-s-move-to-isolate-huawei-sends-ripples-through-global-supply-chain", "author": "No author found", "published_date": "2019-05-16", "content": "", "section": "World", "disclaimer": ""}, "2019-05-16-723996207": {"title": "'Possible' More Counties Than Now Known Were Hacked In 2016, Fla. Officials Say : NPR", "url": "https://www.npr.org/2019/05/16/723996207/possible-more-counties-than-now-known-were-hacked-in-2016-fla-delegation-says", "author": "No author found", "published_date": "2019-05-16", "content": "NOEL KING, HOST: Russian cyberattacks compromised the voting systems of two counties in Florida in 2016. But the state's members of Congress didn't learn which counties until late last week during a closed-door briefing by the FBI. Now lawmakers are frustrated that it took so long for them to find out. NPR's Miles Parks covers election security. He's in studio this morning. Hi, Miles. MILES PARKS, BYLINE: Good morning. KING: So we have been talking about interference in the 2016 election for so long. There have been so many investigations. Why are we learning about these cyberattacks now? PARKS: So law enforcement says it's to protect the sources and methods that it used to gather information about the attacks. As for the counties that were actually attacked, we have to think back to 2016. And they may have been worried about confidence had they disclosed the breaches publicly right before this very polarized election. Now, though, it's been three years. And the lawmakers, the election officials I've talked to seem to think that secrecy is doing more harm than good. Here's Lori Edwards. She's the election supervisor in Polk County, Fla. LORI EDWARDS: I just honestly cannot make sense of why people are trying to keep a secret from three years ago. And as a matter of fact, I think they're making it worse by doing so. It's kind of like a noise in the dark. It's a lot worse than if you can see what really happened. PARKS: So this is someone who is trying to prep for the 2020 election. She's in charge of half a million voters in Florida - in the same state where these hacks occurred. And she doesn't have information. This kind of gets back to this idea that there hasn't been an exhaustive, all-inclusive public report about what happened in terms of Russian interference in 2016. We thought the Mueller report - special counsel Robert Mueller's report might be that source of information. But it's clear there's more - there was more that we still have yet to learn. KING: Including in Florida, where there's - these hackers did something. Do we know what they did? Did they change anyone's vote? PARKS: No. So lawmakers and election officials have been very clear - there is no evidence any votes were affected by this. They were able - the hackers were able to breach registration systems, which are not connected to the systems that actually count and tally votes. They were able to break into the systems. They could've changed information about the level of access they gained. But there's no evidence that they did so. The Washington Post reports that one of the counties that was breached was Washington County, a small county in Northern Florida. Still unclear on what the second county was. What's also unclear is what we still don't know. I pressed the lawmakers last week about whether they thought it was possible that other counties in the United States saw similar breaches in 2016. They said it is possible and that we still may not know about other breaches. KING: Wow. So that information may come forward. I mean, you talked about the importance of having confidence in the election in 2020 - not a small deal. What is Congress doing? What are states doing to stop this from happening again categorically? PARKS: So Congress allocated almost $400 million last year to the states to improve election security. That's probably it from the money front. There's probably not going to be an influx of cash from the federal level to improve other levels of hardware. The biggest thing, though, and what's really interesting is that the thing the federal government has really touted as improved looking ahead to 2020 as opposed to 2016 is communication - that they're sharing information about threats from the local level to the state level, all the way up to the federal level. But here we see there is a lot of work still left to be done on the communication front, especially when it comes to letting the public know about breaches, so they can have confidence that if something has gone wrong in an election they voted in, they're going to know about it. And they're not going to know about it three years later. KING: Likely to be a very big story as we head into 2020. NPR's Miles Parks is covering it. Miles, thanks so much. PARKS: Thank you. NOEL KING, HOST:  Russian cyberattacks compromised the voting systems of two counties in Florida in 2016. But the state's members of Congress didn't learn which counties until late last week during a closed-door briefing by the FBI. Now lawmakers are frustrated that it took so long for them to find out. NPR's Miles Parks covers election security. He's in studio this morning. Hi, Miles. MILES PARKS, BYLINE: Good morning. KING: So we have been talking about interference in the 2016 election for so long. There have been so many investigations. Why are we learning about these cyberattacks now? PARKS: So law enforcement says it's to protect the sources and methods that it used to gather information about the attacks. As for the counties that were actually attacked, we have to think back to 2016. And they may have been worried about confidence had they disclosed the breaches publicly right before this very polarized election. Now, though, it's been three years. And the lawmakers, the election officials I've talked to seem to think that secrecy is doing more harm than good. Here's Lori Edwards. She's the election supervisor in Polk County, Fla. LORI EDWARDS: I just honestly cannot make sense of why people are trying to keep a secret from three years ago. And as a matter of fact, I think they're making it worse by doing so. It's kind of like a noise in the dark. It's a lot worse than if you can see what really happened. PARKS: So this is someone who is trying to prep for the 2020 election. She's in charge of half a million voters in Florida - in the same state where these hacks occurred. And she doesn't have information. This kind of gets back to this idea that there hasn't been an exhaustive, all-inclusive public report about what happened in terms of Russian interference in 2016. We thought the Mueller report - special counsel Robert Mueller's report might be that source of information. But it's clear there's more - there was more that we still have yet to learn. KING: Including in Florida, where there's - these hackers did something. Do we know what they did? Did they change anyone's vote? PARKS: No. So lawmakers and election officials have been very clear - there is no evidence any votes were affected by this. They were able - the hackers were able to breach registration systems, which are not connected to the systems that actually count and tally votes. They were able to break into the systems. They could've changed information about the level of access they gained. But there's no evidence that they did so. The Washington Post reports that one of the counties that was breached was Washington County, a small county in Northern Florida. Still unclear on what the second county was. What's also unclear is what we still don't know. I pressed the lawmakers last week about whether they thought it was possible that other counties in the United States saw similar breaches in 2016. They said it is possible and that we still may not know about other breaches. KING: Wow. So that information may come forward. I mean, you talked about the importance of having confidence in the election in 2020 - not a small deal. What is Congress doing? What are states doing to stop this from happening again categorically? PARKS: So Congress allocated almost $400 million last year to the states to improve election security. That's probably it from the money front. There's probably not going to be an influx of cash from the federal level to improve other levels of hardware. The biggest thing, though, and what's really interesting is that the thing the federal government has really touted as improved looking ahead to 2020 as opposed to 2016 is communication - that they're sharing information about threats from the local level to the state level, all the way up to the federal level. But here we see there is a lot of work still left to be done on the communication front, especially when it comes to letting the public know about breaches, so they can have confidence that if something has gone wrong in an election they voted in, they're going to know about it. And they're not going to know about it three years later. KING: Likely to be a very big story as we head into 2020. NPR's Miles Parks is covering it. Miles, thanks so much. PARKS: Thank you.", "section": "Politics", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-16-723824048": {"title": "Trump Takes Aim At Huawei, Paves Way For Ban Of Foreign Telecom Equipment : NPR", "url": "https://www.npr.org/2019/05/16/723824048/trump-takes-aim-at-huawei-paves-way-for-ban-of-foreign-telecom-equipment", "author": "No author found", "published_date": "2019-05-16", "content": "", "section": "Business", "disclaimer": ""}, "2019-05-17-724433238": {"title": "Experts Talk Best Practices For Facial Recognition Technology : NPR", "url": "https://www.npr.org/2019/05/17/724433238/experts-talk-best-practices-for-facial-recognition-technology", "author": "No author found", "published_date": "2019-05-17", "content": "ARI SHAPIRO, HOST:  San Francisco's Board of Supervisors voted earlier this week to ban the city's use of facial recognition software. That's jump-started a debate about what rules might make the technology more acceptable to people. NPR's Martin Kaste has more. MARTIN KASTE, BYLINE: Go somewhere that's already bristling with cameras - say, a neighborhood around a big marijuana store here in central Seattle - and ask people what they'd think if these cameras were hooked up to facial recognition software. You're mostly going to get reactions like this. CARA BRAUN: I don't want my face to be recognized on the streets. I feel like it's my right to privacy. And if I want someone to know where I'm at and when I'm there, then I'll tell them. KASTE: That's Cara Braun, definitely someone who likes the idea of a complete ban. Does that kind of attitude worry the industry? BENJI HUTCHINSON: It does. It does. It worries us because, from the all-out simplistic ban perspective of the technology, it doesn't really seem like there's sufficient justification. KASTE: Benji Hutchinson is VP of federal operations for NCE Corporation of America. It's one of the biggest sellers of facial recognition to law enforcement. And after this vote in San Francisco, he wants people to take a moment. HUTCHINSON: There needs to be a discussion around the use cases of the technology and what specific instances are appropriate for using the technology. It needs to start there. KASTE: He's right that facial recognition can be used in very different ways, and people tend to see some uses as worse than others. On one extreme is the blanket surveillance that China is trying to build - cameras everywhere capable of live tracking crowds of people. That use has few defenders here in America, at least in public. On the other end of the spectrum, though, is crime investigations, after the fact. JAMES HART: Burglaries where a homeowner's surveillance camera captured a suspect coming up and breaking into their house. KASTE: That's Detective James Hart with the Sacramento Sheriff's Department. He uses facial recognition in the way that's most typical in America right now - taking an image from a crime scene and trying to find a match. But right there is a big question for the reformers - whom should police be matching these images to? Just the people in their mug shot files, or everybody who's ever had a driver's license photo? Also, the reformers say cops should get a warrant first. Hart says if that became the rule, it would definitely change things for him. HART: If it came down to the point where we were having to write search warrants on each one of these cases in order to use facial recognition, I mean, it would severely delay us in investigating additional crimes. KASTE: Claire Garvie tracks law enforcement use of facial recognition for Georgetown Law's Center on Privacy and Technology. She'd like to see a complete moratorium. But if police are going to use this, she says it should be reserved for serious crimes. CLAIRE GARVIE: And here's why - if the threshold is set too low, we could very quickly imagine a world where face recognition, like what's been trialled in China, is used to identify jaywalkers and send you a jaywalking traffic ticket, if you will. KASTE: As to the other end of that spectrum of uses, the China style real-time street surveillance of crowds, she says there definitely have to be legal limits on that. GARVIE: A limited location, limited time, a limited number of people that the system can identify, so that this is not a system that, just because there's somebody on FBI's most wanted list, the FBI gets to run face surveillance on every single camera across the country. KASTE: But so far, limiting law enforcement's use of facial recognition is not something that Congress seems very interested in. There is a bill to regulate how companies use it. Brian Schatz, the senator from Hawaii, is one of the authors. BRIAN SCHATZ: I think this issue is all brand new to most members of Congress. And so we're starting with the commercial sector because we think there's a better chance to get consensus. KASTE: At the state level, too, lawmakers have been paying more attention to commercial users than law enforcement. And this suspicion about the private sector is echoed by Detective Hart in Sacramento. Even though he uses facial recognition in his work, he doesn't like the idea of stores using it on him. HART: I don't want to be tracked myself, specifically also to give some sort of edge to a business who's trying to make more money. KASTE: As to expanding law enforcement's uses, Hart says he can see using live facial recognition at, say, an airport. But he thinks the average person should still be able to, in his words, walk around and have his own personal life be his own personal life. Martin Kaste, NPR News. (SOUNDBITE OF ANGEL OLSEN'S \"THOSE WERE THE DAYS\") ARI SHAPIRO, HOST:   San Francisco's Board of Supervisors voted earlier this week to ban the city's use of facial recognition software. That's jump-started a debate about what rules might make the technology more acceptable to people. NPR's Martin Kaste has more. MARTIN KASTE, BYLINE: Go somewhere that's already bristling with cameras - say, a neighborhood around a big marijuana store here in central Seattle - and ask people what they'd think if these cameras were hooked up to facial recognition software. You're mostly going to get reactions like this. CARA BRAUN: I don't want my face to be recognized on the streets. I feel like it's my right to privacy. And if I want someone to know where I'm at and when I'm there, then I'll tell them. KASTE: That's Cara Braun, definitely someone who likes the idea of a complete ban. Does that kind of attitude worry the industry? BENJI HUTCHINSON: It does. It does. It worries us because, from the all-out simplistic ban perspective of the technology, it doesn't really seem like there's sufficient justification. KASTE: Benji Hutchinson is VP of federal operations for NCE Corporation of America. It's one of the biggest sellers of facial recognition to law enforcement. And after this vote in San Francisco, he wants people to take a moment. HUTCHINSON: There needs to be a discussion around the use cases of the technology and what specific instances are appropriate for using the technology. It needs to start there. KASTE: He's right that facial recognition can be used in very different ways, and people tend to see some uses as worse than others. On one extreme is the blanket surveillance that China is trying to build - cameras everywhere capable of live tracking crowds of people. That use has few defenders here in America, at least in public. On the other end of the spectrum, though, is crime investigations, after the fact. JAMES HART: Burglaries where a homeowner's surveillance camera captured a suspect coming up and breaking into their house. KASTE: That's Detective James Hart with the Sacramento Sheriff's Department. He uses facial recognition in the way that's most typical in America right now - taking an image from a crime scene and trying to find a match. But right there is a big question for the reformers - whom should police be matching these images to? Just the people in their mug shot files, or everybody who's ever had a driver's license photo? Also, the reformers say cops should get a warrant first. Hart says if that became the rule, it would definitely change things for him. HART: If it came down to the point where we were having to write search warrants on each one of these cases in order to use facial recognition, I mean, it would severely delay us in investigating additional crimes. KASTE: Claire Garvie tracks law enforcement use of facial recognition for Georgetown Law's Center on Privacy and Technology. She'd like to see a complete moratorium. But if police are going to use this, she says it should be reserved for serious crimes. CLAIRE GARVIE: And here's why - if the threshold is set too low, we could very quickly imagine a world where face recognition, like what's been trialled in China, is used to identify jaywalkers and send you a jaywalking traffic ticket, if you will. KASTE: As to the other end of that spectrum of uses, the China style real-time street surveillance of crowds, she says there definitely have to be legal limits on that. GARVIE: A limited location, limited time, a limited number of people that the system can identify, so that this is not a system that, just because there's somebody on FBI's most wanted list, the FBI gets to run face surveillance on every single camera across the country. KASTE: But so far, limiting law enforcement's use of facial recognition is not something that Congress seems very interested in. There is a bill to regulate how companies use it. Brian Schatz, the senator from Hawaii, is one of the authors. BRIAN SCHATZ: I think this issue is all brand new to most members of Congress. And so we're starting with the commercial sector because we think there's a better chance to get consensus. KASTE: At the state level, too, lawmakers have been paying more attention to commercial users than law enforcement. And this suspicion about the private sector is echoed by Detective Hart in Sacramento. Even though he uses facial recognition in his work, he doesn't like the idea of stores using it on him. HART: I don't want to be tracked myself, specifically also to give some sort of edge to a business who's trying to make more money. KASTE: As to expanding law enforcement's uses, Hart says he can see using live facial recognition at, say, an airport. But he thinks the average person should still be able to, in his words, walk around and have his own personal life be his own personal life. Martin Kaste, NPR News. (SOUNDBITE OF ANGEL OLSEN'S \"THOSE WERE THE DAYS\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-18-724569289": {"title": "Microsoft Updates Old System To Respond To New Threat : NPR", "url": "https://www.npr.org/2019/05/18/724569289/microsoft-updates-old-system-to-respond-to-new-threat", "author": "No author found", "published_date": "2019-05-18", "content": "SCOTT SIMON, HOST: Microsoft has had to update an old system to meet a new threat. Years ago, they stopped supporting the Windows XP operating system, but millions of computers around the world still use it. This week, Microsoft released a security patch for Windows XP to prevent a major attack on vulnerable computers. Brian Barrett, a news editor at WIRED magazine, joins us from member station WBHM in Birmingham. Thanks so much for being with us. BRIAN BARRETT: Thanks for having me. SIMON: Microsoft stopped updating Windows XP in 2014, but a lot of places still use it, don't they? BARRETT: That's right. When you think of Windows XP, you think of maybe people who aren't tech savvy who haven't update their computers in a long time, and that's part of it. But I think the more concerning thing here is that it's also a still popular operating system for industrial control systems, for health care systems, for a lot of places that, for one reason or another, still rely on it and now are potentially exposed. SIMON: What's Microsoft fixing? BARRETT: Well, they are saying it has something to do with remote desktop services, which is - you know, if you work in an office, an administrator can sometimes take over your computer to fix a problem, so it has something to do with that. We're not sure exactly what. But the bigger deal is that it's a - it's what's known as a worm, which means that if a hacker takes advantage of it, they can sort of hop from one computer to the next without the person who runs that computer doing anything. So, you know, normally, you think of clicking a link and you get malware or opening an attachment and it happens. This just kind of spreads on its own. SIMON: Microsoft had to make a fix kind of like this in 2017, didn't they? BARRETT: They did, and I think that also shows why this is alarming. So I don't know for the listeners who remember - WannaCry, which was a ransomware attack in 2017, spread very quickly across the globe and was only stopped by sort of a piece of chance. A malware researcher figured out a very clever way to stop it before it could spread further. But Microsoft had released a patch for WannaCry a month or two before it hit. And I think that shows you two things - one, that Microsoft thinks that this is that level of urgency, which is pretty big, and, two, even when Microsoft released that patch, it didn't do much good to stop WannaCry because it is hard for people to get around to actually installing it. So, you know, you don't want to overhype these things, but Microsoft says there's a viable threat. And a lot of, you know, computers could be in trouble in the next, weeks, months - who can say. SIMON: To be plain, we don't have any reason to think anybody is trying to take advantage of the system, do we? BARRETT: That's right. So Microsoft has said that they hadn't seen anyone attacking this previously. The concern, though, is now that they've said that it's out there and they've pointed to the general area where it is, that's when you start to get a little bit worried because now hackers know where to look. They know what it can do. And there's a big incentive there for them to take advantage of it. SIMON: How difficult - maybe I should say expensive - is it for companies to change an old operating system? BARRETT: Difficult and expensive are both the right words, especially when you think about a company - like, say, it's a chemical refining plant. They're operating 24 hours a day, seven days a week, and it's very hard for them to come offline because not only do they have to just sort of update with this patch, they have to test it to make sure that it doesn't interfere with what they're working on already. And especially when you have these sensitive issues like health care, it's hard for a hospital to go offline for a while because if they have parents (ph) relying on critical care, you know, then you don't want interrupt that. SIMON: Is there a lesson here, whether we have Windows XP or not? BARRETT: You know, we sometimes forget that there are tens of millions of devices that are vulnerable and that they're very hard to fix, even if the fix is available. So I think the lesson is going to play out over the next weeks and months if we see what if anything actually comes of this. But in the meantime, it's that just being aware of what your vulnerabilities are and taking as many steps as you can to sort of prevent a broad attack. SIMON: And when you get an update, install it. BARRETT: I would say so, and at the very least, you get rid of that little pop-up. SIMON: (Laughter) Brian Barrett, news editor at WIRED magazine, thanks so much for being with us. BARRETT: Thanks again for having me. SCOTT SIMON, HOST:  Microsoft has had to update an old system to meet a new threat. Years ago, they stopped supporting the Windows XP operating system, but millions of computers around the world still use it. This week, Microsoft released a security patch for Windows XP to prevent a major attack on vulnerable computers. Brian Barrett, a news editor at WIRED magazine, joins us from member station WBHM in Birmingham. Thanks so much for being with us. BRIAN BARRETT: Thanks for having me. SIMON: Microsoft stopped updating Windows XP in 2014, but a lot of places still use it, don't they? BARRETT: That's right. When you think of Windows XP, you think of maybe people who aren't tech savvy who haven't update their computers in a long time, and that's part of it. But I think the more concerning thing here is that it's also a still popular operating system for industrial control systems, for health care systems, for a lot of places that, for one reason or another, still rely on it and now are potentially exposed. SIMON: What's Microsoft fixing? BARRETT: Well, they are saying it has something to do with remote desktop services, which is - you know, if you work in an office, an administrator can sometimes take over your computer to fix a problem, so it has something to do with that. We're not sure exactly what. But the bigger deal is that it's a - it's what's known as a worm, which means that if a hacker takes advantage of it, they can sort of hop from one computer to the next without the person who runs that computer doing anything. So, you know, normally, you think of clicking a link and you get malware or opening an attachment and it happens. This just kind of spreads on its own. SIMON: Microsoft had to make a fix kind of like this in 2017, didn't they? BARRETT: They did, and I think that also shows why this is alarming. So I don't know for the listeners who remember - WannaCry, which was a ransomware attack in 2017, spread very quickly across the globe and was only stopped by sort of a piece of chance. A malware researcher figured out a very clever way to stop it before it could spread further. But Microsoft had released a patch for WannaCry a month or two before it hit. And I think that shows you two things - one, that Microsoft thinks that this is that level of urgency, which is pretty big, and, two, even when Microsoft released that patch, it didn't do much good to stop WannaCry because it is hard for people to get around to actually installing it. So, you know, you don't want to overhype these things, but Microsoft says there's a viable threat. And a lot of, you know, computers could be in trouble in the next, weeks, months - who can say. SIMON: To be plain, we don't have any reason to think anybody is trying to take advantage of the system, do we? BARRETT: That's right. So Microsoft has said that they hadn't seen anyone attacking this previously. The concern, though, is now that they've said that it's out there and they've pointed to the general area where it is, that's when you start to get a little bit worried because now hackers know where to look. They know what it can do. And there's a big incentive there for them to take advantage of it. SIMON: How difficult - maybe I should say expensive - is it for companies to change an old operating system? BARRETT: Difficult and expensive are both the right words, especially when you think about a company - like, say, it's a chemical refining plant. They're operating 24 hours a day, seven days a week, and it's very hard for them to come offline because not only do they have to just sort of update with this patch, they have to test it to make sure that it doesn't interfere with what they're working on already. And especially when you have these sensitive issues like health care, it's hard for a hospital to go offline for a while because if they have parents (ph) relying on critical care, you know, then you don't want interrupt that. SIMON: Is there a lesson here, whether we have Windows XP or not? BARRETT: You know, we sometimes forget that there are tens of millions of devices that are vulnerable and that they're very hard to fix, even if the fix is available. So I think the lesson is going to play out over the next weeks and months if we see what if anything actually comes of this. But in the meantime, it's that just being aware of what your vulnerabilities are and taking as many steps as you can to sort of prevent a broad attack. SIMON: And when you get an update, install it. BARRETT: I would say so, and at the very least, you get rid of that little pop-up. SIMON: (Laughter) Brian Barrett, news editor at WIRED magazine, thanks so much for being with us. BARRETT: Thanks again for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-19-723326933": {"title": "Billion-Dollar Gamble: How A 'Singular Hero' Helped Start A New Field In Physics : NPR", "url": "https://www.npr.org/2019/05/19/723326933/billion-dollar-gamble-how-a-singular-hero-helped-start-a-new-field-in-physics", "author": "No author found", "published_date": "2019-05-19", "content": "LULU GARCIA-NAVARRO, HOST: Imagine spending 40 years and a billion dollars on a gamble, a scientific gamble. That's what one government agency did thanks, in large part, to a staffer there who saw a chance to cultivate some stunning research. As NPR's Nell Greenfieldboyce reports, it was a huge risk. And it's paying off big time. NELL GREENFIELDBOYCE, BYLINE: Richard Isaacson and I are wearing the same socks - black with gold toes. I know this because our shoes are off so we can walk around on one of his fabulous antique carpets. He likes ones with geometric designs. Collecting textiles from Central Asia is what he does now. But he used to do physics. RICHARD ISAACSON: Since modern physics is highly geometrical, it's not all that different except that physicists work in somewhere between four or 10 dimensions usually. So for a retirement career, working in two dimensions is a piece of cake. GREENFIELDBOYCE: He unrolls a red carpet made by an Uzbek tribe in the mid-19th century. He thinks the forgotten weavers who made this were a hundred years ahead of famous modern art celebrities. ISAACSON: They were anonymous women. And they were completely ignored. But they were doing beautiful things. GREENFIELDBOYCE: The reason I'm here is that Isaacson is also kind of anonymous. And he also did a lot of painstaking work that resulted in something beautiful. It all started back in the 1960s, when he was a Ph. D. student. He got interested in a prediction made by Albert Einstein. Einstein said anytime two massive objects crash together, shock waves should move through the very fabric of the universe. These gravitational waves are like the ripples you see in water when you toss in a pebble. ISAACSON: For my thesis, I showed how gravitational waves behave like other kinds of waves, like light and radar and X-rays. GREENFIELDBOYCE: Now, Einstein thought that gravitational waves would most likely never be detected. They're just too tiny. Isaacson was more optimistic. ISAACSON: So I imagined that sometime in my career we would see it. GREENFIELDBOYCE: He was right. The first detection was made just a few years ago. This has opened up a whole new way to find and study some of the most powerful extreme events in the universe. It was such a huge deal that the work almost immediately got a Nobel Prize. Rai Weiss is a physicist at MIT, 1 of the 3 people who won the prize. He says lots of people helped pull off this historic feat. RAI WEISS: Why should I have a Nobel Prize when there were at least 20 other people who have had equivalent input into this thing, too? GREENFIELDBOYCE: Still, he thinks one person does deserve special recognition. He thinks it's Rich Isaacson. WEISS: Rich was in a singular place, fighting on a singular war that nobody else could have fought. I think he's the hero. You know, he is the singular hero. We just don't have a good way of recognizing people like that. GREENFIELDBOYCE: He says in the early 1970s, Isaacson was working at the National Science Foundation. And Weiss wanted this fledgling agency to fund a kind of crazy idea. Weiss thought it could be possible to detect gravitational waves using lasers. Lasers could, in theory, measure very, very small distortions in space, like changes that were a thousandth of the width of an atomic nucleus. WEISS: Most people have said, holy mackerel, that - he must be nuts. You can't do that. GREENFIELDBOYCE: The technology was just too hard. Plus, no one even knew what in the universe could spew out gravitational waves strong enough to be measured like that. WEISS: With those two things, you would have normally - if that was now the situation in the NSF, that proposal would have been dead on arrival. But it wasn't that way with Rich. GREENFIELDBOYCE: Because Rich Isaacson had previously studied gravitational waves, he saw the potential. So Weiss says Isaacson personally shepherded this research for almost 30 years. It became the biggest project the National Science Foundation had ever funded. WEISS: He sat in the NSF and single-handedly - I mean, single-handedly - convinced everybody in the NSF this was the right thing for the NSF to support. And the science was going to be spectacular if it should succeed. And he made the argument stick. GREENFIELDBOYCE: He made it stick through prototype tests and expert review panels and feasibility studies and management nightmares. Isaacson says lots of people thought it was crazy to spend hundreds of millions of dollars to build giant detectors that might never detect anything. ISAACSON: There is always a danger that the project can get stopped. And like all of the big projects in science, it's a roller coaster ride. GREENFIELDBOYCE: Officially, Isaacson never worked on this more than half time. In reality, it was all-consuming. The long workdays took a toll. At one point, his blood pressure went sky high, and his doctor became alarmed. Isaacson told me he felt lucky to have been in a position to help change history. ISAACSON: But history demands you pay a price for that privilege, you know, in terms of all the stress and agony and lifestyles, you know, family events. And if you're willing to pay the price, OK, then you've got this chance. And you can go ahead, and maybe it'll work - maybe it won't. GREENFIELDBOYCE: In 2002, Isaacson retired. That was also the year the agency started searching with its brand new Laser Interferometer Gravitational-Wave Observatory, two massive detectors - one in Washington state and one in Louisiana. Each has lasers that travel down pipes 2 1/2 miles long. As Isaacson focused on his beloved textiles, these detectors hunted for gravitational waves and found nothing. For years, scientists stuck with it. They improved the detectors' instruments. And in 2015, Isaacson traveled to Maine for a vacation getaway with Weiss and another colleague who opened up a laptop to reveal measurements that were made just a couple days before - the first-ever detection of gravitational waves from two black holes that collided over a billion light years away. ISAACSON: It was absolutely clear that this fantastic thing had just happened. GREENFIELDBOYCE: Did you feel, like, different in any way? ISAACSON: Well, a little warm glow. I guess now that I'm a few years away from it, I'm beginning to feel it more. GREENFIELDBOYCE: I asked him what he thinks the chances are that a project like this could happen today. ISAACSON: Zero. We live in a very different time. Nobody can, I think, take such large-scale, high-risk, long-term research. GREENFIELDBOYCE: Just last month, researchers started up the massive detectors after another major hardware upgrade. They've already detected at least five more gravitational wave events. Nell Greenfieldboyce, NPR News. (SOUNDBITE OF SPUNKSHINE'S \"JAMBIC REEL\") LULU GARCIA-NAVARRO, HOST:  Imagine spending 40 years and a billion dollars on a gamble, a scientific gamble. That's what one government agency did thanks, in large part, to a staffer there who saw a chance to cultivate some stunning research. As NPR's Nell Greenfieldboyce reports, it was a huge risk. And it's paying off big time. NELL GREENFIELDBOYCE, BYLINE: Richard Isaacson and I are wearing the same socks - black with gold toes. I know this because our shoes are off so we can walk around on one of his fabulous antique carpets. He likes ones with geometric designs. Collecting textiles from Central Asia is what he does now. But he used to do physics. RICHARD ISAACSON: Since modern physics is highly geometrical, it's not all that different except that physicists work in somewhere between four or 10 dimensions usually. So for a retirement career, working in two dimensions is a piece of cake. GREENFIELDBOYCE: He unrolls a red carpet made by an Uzbek tribe in the mid-19th century. He thinks the forgotten weavers who made this were a hundred years ahead of famous modern art celebrities. ISAACSON: They were anonymous women. And they were completely ignored. But they were doing beautiful things. GREENFIELDBOYCE: The reason I'm here is that Isaacson is also kind of anonymous. And he also did a lot of painstaking work that resulted in something beautiful. It all started back in the 1960s, when he was a Ph. D. student. He got interested in a prediction made by Albert Einstein. Einstein said anytime two massive objects crash together, shock waves should move through the very fabric of the universe. These gravitational waves are like the ripples you see in water when you toss in a pebble. ISAACSON: For my thesis, I showed how gravitational waves behave like other kinds of waves, like light and radar and X-rays. GREENFIELDBOYCE: Now, Einstein thought that gravitational waves would most likely never be detected. They're just too tiny. Isaacson was more optimistic. ISAACSON: So I imagined that sometime in my career we would see it. GREENFIELDBOYCE: He was right. The first detection was made just a few years ago. This has opened up a whole new way to find and study some of the most powerful extreme events in the universe. It was such a huge deal that the work almost immediately got a Nobel Prize. Rai Weiss is a physicist at MIT, 1 of the 3 people who won the prize. He says lots of people helped pull off this historic feat. RAI WEISS: Why should I have a Nobel Prize when there were at least 20 other people who have had equivalent input into this thing, too? GREENFIELDBOYCE: Still, he thinks one person does deserve special recognition. He thinks it's Rich Isaacson. WEISS: Rich was in a singular place, fighting on a singular war that nobody else could have fought. I think he's the hero. You know, he is the singular hero. We just don't have a good way of recognizing people like that. GREENFIELDBOYCE: He says in the early 1970s, Isaacson was working at the National Science Foundation. And Weiss wanted this fledgling agency to fund a kind of crazy idea. Weiss thought it could be possible to detect gravitational waves using lasers. Lasers could, in theory, measure very, very small distortions in space, like changes that were a thousandth of the width of an atomic nucleus. WEISS: Most people have said, holy mackerel, that - he must be nuts. You can't do that. GREENFIELDBOYCE: The technology was just too hard. Plus, no one even knew what in the universe could spew out gravitational waves strong enough to be measured like that. WEISS: With those two things, you would have normally - if that was now the situation in the NSF, that proposal would have been dead on arrival. But it wasn't that way with Rich. GREENFIELDBOYCE: Because Rich Isaacson had previously studied gravitational waves, he saw the potential. So Weiss says Isaacson personally shepherded this research for almost 30 years. It became the biggest project the National Science Foundation had ever funded. WEISS: He sat in the NSF and single-handedly - I mean, single-handedly - convinced everybody in the NSF this was the right thing for the NSF to support. And the science was going to be spectacular if it should succeed. And he made the argument stick. GREENFIELDBOYCE: He made it stick through prototype tests and expert review panels and feasibility studies and management nightmares. Isaacson says lots of people thought it was crazy to spend hundreds of millions of dollars to build giant detectors that might never detect anything. ISAACSON: There is always a danger that the project can get stopped. And like all of the big projects in science, it's a roller coaster ride. GREENFIELDBOYCE: Officially, Isaacson never worked on this more than half time. In reality, it was all-consuming. The long workdays took a toll. At one point, his blood pressure went sky high, and his doctor became alarmed. Isaacson told me he felt lucky to have been in a position to help change history. ISAACSON: But history demands you pay a price for that privilege, you know, in terms of all the stress and agony and lifestyles, you know, family events. And if you're willing to pay the price, OK, then you've got this chance. And you can go ahead, and maybe it'll work - maybe it won't. GREENFIELDBOYCE: In 2002, Isaacson retired. That was also the year the agency started searching with its brand new Laser Interferometer Gravitational-Wave Observatory, two massive detectors - one in Washington state and one in Louisiana. Each has lasers that travel down pipes 2 1/2 miles long. As Isaacson focused on his beloved textiles, these detectors hunted for gravitational waves and found nothing. For years, scientists stuck with it. They improved the detectors' instruments. And in 2015, Isaacson traveled to Maine for a vacation getaway with Weiss and another colleague who opened up a laptop to reveal measurements that were made just a couple days before - the first-ever detection of gravitational waves from two black holes that collided over a billion light years away. ISAACSON: It was absolutely clear that this fantastic thing had just happened. GREENFIELDBOYCE: Did you feel, like, different in any way? ISAACSON: Well, a little warm glow. I guess now that I'm a few years away from it, I'm beginning to feel it more. GREENFIELDBOYCE: I asked him what he thinks the chances are that a project like this could happen today. ISAACSON: Zero. We live in a very different time. Nobody can, I think, take such large-scale, high-risk, long-term research. GREENFIELDBOYCE: Just last month, researchers started up the massive detectors after another major hardware upgrade. They've already detected at least five more gravitational wave events. Nell Greenfieldboyce, NPR News. (SOUNDBITE OF SPUNKSHINE'S \"JAMBIC REEL\")", "section": "Science", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-20-725139657": {"title": "Google Restricts Huawei's Access To Android Systems After Trump Ban : NPR", "url": "https://www.npr.org/2019/05/20/725139657/google-restricts-huawei-s-access-to-android-systems-after-trump-ban", "author": "No author found", "published_date": "2019-05-20", "content": "MARY LOUISE KELLY, HOST:  The Trump administration gave a little ground today in its dispute with Chinese tech giant Huawei. It is backing off on restrictions placed on the company less than a week ago. The Commerce Department had barred U. S. companies from doing business with Huawei unless they received a special license. The idea was to hurt Huawei, but U. S. tech company stocks were caught in the crossfire. NPR's Jackie Northam has been following developments. She's back in the studio. Hello again, Jackie. JACKIE NORTHAM, BYLINE: Hi, Mary Louise. KELLY: All right. So why is the Commerce Department backing off these restrictions so quickly, just put in place in the last week? NORTHAM: Right. Well, they are backing off. It's a 90-day reprieve. So in other words, Huawei can continue to buy components from U. S. companies, whether it be computer chips or software, anything like that. Analysts told me that it was the U. S. tech companies that were going to take a beating here because they have so much business with Huawei and that they've been passing on that message to President Trump that they were going to get hit hard. Stocks in two major computer chip companies - Qualcomm and Broadcom, two major ones - were down 6% today. Google dropped 2%. And this was after Google said it would stop supporting Android devices for phones made by Huawei because the company was on the Commerce Department's blacklist. Google is a major software provider for Huawei. And if these restrictions went on for months, Google could take a beating. KELLY: Correct me if I'm wrong here, Jackie, but didn't the U. S. back off last year in a somewhat similar way shortly after blacklisting another Chinese telecom company? NORTHAM: That's right. It was a company called ZTE, and they were on the blacklist. And it went on for a while. And they were on the verge of bankruptcy. And this is a major Chinese corporation - almost went bust until the Trump administration reversed that decision and allowed U. S. companies to supply it again. So it's the same sort of scenario. But, Mary Louise, the Trump administration has been tightening the screws on Huawei for months now in part because it has links to the Chinese government. And U. S. intelligence agencies believe Huawei could use its equipment to spy on the U. S. and its allies. The administration asked Canada to arrest and extradite one of Huawei's senior officials. And administration officials have been fanning out across the world asking allies, pressuring allies not to use Huawei equipment. So up until now, it's been a full-court press by the administration to contain the company. KELLY: Yeah. It sounds like a fascinating back-and-forth, how to hurt Chinese companies without harming U. S. companies in the balance. I mean, you could have described this until a few hours ago, as it sounded like almost all-out war between the U. S. and Huawei. Are we now at a 90-day truce? NORTHAM: Maybe a cease-fire. The decision to blacklist Huawei was seen by many - in the beginning, anyway - as a bargaining ploy in the overall trade war between China and the U. S. , where there are increasing tariffs on each other's products. And it was seen as an effort to gain leverage over Beijing. One analyst I talked to just in the past couple hours says easing the restrictions on Huawei now may be an opportunity for both sides to save face and get back to the negotiating table. But we'll see if that works. KELLY: Yeah. And again, 90-day truce. What reason do we have to think that the U. S. will be in a stronger position 90 days from now? NORTHAM: It's hard to say. Huawei is an enormously powerful company. You know, it's the second-largest supplier of smartphones in the world. It passed Apple last year, and it was on its way to pass Samsung as well. So again, this is a very powerful company. So, you know, it has its own bargaining chips. Let's put it that way. KELLY: NPR's Jackie Northam reporting. Thank you, Jackie. NORTHAM: Thank you, Mary Louise. MARY LOUISE KELLY, HOST:   The Trump administration gave a little ground today in its dispute with Chinese tech giant Huawei. It is backing off on restrictions placed on the company less than a week ago. The Commerce Department had barred U. S. companies from doing business with Huawei unless they received a special license. The idea was to hurt Huawei, but U. S. tech company stocks were caught in the crossfire. NPR's Jackie Northam has been following developments. She's back in the studio. Hello again, Jackie. JACKIE NORTHAM, BYLINE: Hi, Mary Louise. KELLY: All right. So why is the Commerce Department backing off these restrictions so quickly, just put in place in the last week? NORTHAM: Right. Well, they are backing off. It's a 90-day reprieve. So in other words, Huawei can continue to buy components from U. S. companies, whether it be computer chips or software, anything like that. Analysts told me that it was the U. S. tech companies that were going to take a beating here because they have so much business with Huawei and that they've been passing on that message to President Trump that they were going to get hit hard. Stocks in two major computer chip companies - Qualcomm and Broadcom, two major ones - were down 6% today. Google dropped 2%. And this was after Google said it would stop supporting Android devices for phones made by Huawei because the company was on the Commerce Department's blacklist. Google is a major software provider for Huawei. And if these restrictions went on for months, Google could take a beating. KELLY: Correct me if I'm wrong here, Jackie, but didn't the U. S. back off last year in a somewhat similar way shortly after blacklisting another Chinese telecom company? NORTHAM: That's right. It was a company called ZTE, and they were on the blacklist. And it went on for a while. And they were on the verge of bankruptcy. And this is a major Chinese corporation - almost went bust until the Trump administration reversed that decision and allowed U. S. companies to supply it again. So it's the same sort of scenario. But, Mary Louise, the Trump administration has been tightening the screws on Huawei for months now in part because it has links to the Chinese government. And U. S. intelligence agencies believe Huawei could use its equipment to spy on the U. S. and its allies. The administration asked Canada to arrest and extradite one of Huawei's senior officials. And administration officials have been fanning out across the world asking allies, pressuring allies not to use Huawei equipment. So up until now, it's been a full-court press by the administration to contain the company. KELLY: Yeah. It sounds like a fascinating back-and-forth, how to hurt Chinese companies without harming U. S. companies in the balance. I mean, you could have described this until a few hours ago, as it sounded like almost all-out war between the U. S. and Huawei. Are we now at a 90-day truce? NORTHAM: Maybe a cease-fire. The decision to blacklist Huawei was seen by many - in the beginning, anyway - as a bargaining ploy in the overall trade war between China and the U. S. , where there are increasing tariffs on each other's products. And it was seen as an effort to gain leverage over Beijing. One analyst I talked to just in the past couple hours says easing the restrictions on Huawei now may be an opportunity for both sides to save face and get back to the negotiating table. But we'll see if that works. KELLY: Yeah. And again, 90-day truce. What reason do we have to think that the U. S. will be in a stronger position 90 days from now? NORTHAM: It's hard to say. Huawei is an enormously powerful company. You know, it's the second-largest supplier of smartphones in the world. It passed Apple last year, and it was on its way to pass Samsung as well. So again, this is a very powerful company. So, you know, it has its own bargaining chips. Let's put it that way. KELLY: NPR's Jackie Northam reporting. Thank you, Jackie. NORTHAM: Thank you, Mary Louise.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-20-725139678": {"title": "Microsoft President Brad Smith Discusses The Ethics Of Artificial Intelligence : NPR", "url": "https://www.npr.org/2019/05/20/725139678/microsoft-president-brad-smith-discusses-the-ethics-of-artificial-intelligence", "author": "No author found", "published_date": "2019-05-20", "content": "AUDIE CORNISH, HOST:  Just because we can use it, should we? That's the question more and more people are asking about face recognition technology, software that's already in our phones and our social media feeds and many security systems. San Francisco leaders have voted to ban the police from using it, and even some in the tech industry say there should be limits. BRAD SMITH: It's the kind of technology that can do a lot of good for a lot of people, but it can be misused. It can be abused. It can be used in ways that lead to discrimination and bias. It can really make possible the kind of mass surveillance that in the past has always just been the realm of science fiction. CORNISH: That's Brad Smith. He's president of Microsoft. The company makes facial recognition software and other tools that raise some ethical dilemmas, and he's our guest on this week's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CORNISH: Brad Smith told me Microsoft has turned down deals to sell its facial recognition software to buyers it didn't trust to use it properly. SMITH: What facial recognition services enables somebody to do is look at your image on a camera and run it against a database and identify who you are. One deal we turned down was with a law enforcement division in California. They wanted to put this in all their police cars so that if you were pulled over for any purpose, they would look for a match. And if you matched a suspect's face, you would be taken downtown for further questioning. Our concern is that the technology, regardless of where you buy it from, is just not ready for that. It will lead to bias. It will lead to false identifications. People will be put in the back of police cars when they've done nothing wrong. It seemed to us to be a step too far. CORNISH: Now, on the other hand, Microsoft has provided the technology to a prison. Microsoft researchers have worked with a Chinese military-run university on AI research that some people fear could be used against China's minorities. So to you, where is the line when it comes to helping governments gain and use these kinds of tools? SMITH: We need to look constantly at the technology that's at issue and how it's going to be used. We were comfortable providing facial recognition within a prison because the sample size of people involved is actually relatively small. We could be confident that people would be identified correctly. And there was a societally beneficial goal, namely to actually help keep prisoners safe by knowing who was where and at what time. More broadly, we've supported basic research, advances in the fundamental frontiers of knowledge. You know, we're not working, for example, with authorities in China to deploy facial recognition services for surveillance. But when you get to the bottom foundation for all artificial intelligence, advances in machine learning and the like, we believe that that's where there are real benefits for people able to work on advancing scientific knowledge more generally. CORNISH: Now, something that bothered some Microsoft employees earlier this year enough to write an open letter about it was Microsoft's contract with the U. S. Army. And you're making augmented reality headsets for soldiers. These employees said, look; we didn't sign up to develop weapons. You still have that contract. How did you square that? I mean, did the resistance by employees change your thinking or affect your thinking? SMITH: First, we believe we have a responsibility - even a patriotic responsibility - to provide our technology to the people who defend our country and keep us safe. So we said we'll provide artificial intelligence and our other technology products to the U. S. military. Second, we recognize that certain employees may not want to work on these projects. And we've said that we'll work to enable people to work on other projects. That's one of the benefits of being a large company. And so far we've been able to accommodate everybody's interests and respect their views. But third, we've said we'll focus on the issues. We do want the U. S. military - every military and every country - to really address in a thoughtful way the new ethical and broader public policy issues that artificial intelligence is creating. CORNISH: The motto for tech companies for so long was move fast, and break things. Are you all hitting up against the reality of what that could mean? SMITH: Well, I've never been a great fan myself of that motto. I think our motto needs to be, don't move faster than the speed of thought. Put guardrails in place, and think about society. There's still lots of room to move fast. We will need to move fast. But we actually need to be more thoughtful. And what we also need is we need governments. CORNISH: But do you get pushback from that? I mean, that seems like that is completely at odds not just with the motto but with the tech industry saying, look; we are first in the world in the U. S. because we are aggressive, because we push boundaries. SMITH: There are lots of different voices in the tech sector. There are days when I get pushback. There are more days in recent months when I see heads nod. It doesn't mean that the whole industry has moved to a different place. But I do think that we've all learned that technology plays such a pervasive role in the world that it just can't afford to break everything around it. There's just too much that will end up broken. CORNISH: You are, finally, calling for government regulation. What is the role for government in this situation? And I ask because having covered Congress, they don't always seem like they're all that caught up (laughter) on the technology based on some of the questioning. What would tougher rules or regulations look like? SMITH: We live in a country where the government has learned generally well how to manage and regulate complicated products, whether it's an automobile or an airplane or a pharmaceutical product or even a drug. CORNISH: Although when you listen to industry, there's lots of complaints about regulation. It doesn't always sound like, you know, people feel that way. SMITH: Yeah, I - and the complaints are often valid. And to some degree, I think that people who work in business complain about regulation the way college students complain about the food in the cafeteria. It just is part of everyday life. And you can understand it. But a world in which important products are subject to the rule of law and rules of public safety, in my view, is better than a world where there are no rules in place. Rather than complaining about what regulation may bring or even ridiculing people who may ask the wrong question or may ask the right question the wrong way - I think we're all far too quick to criticize even our legislators if they don't ask a question precisely right. That's not going to get us where we need to go. CORNISH: Brad Smith, Microsoft's president, thank you so much for speaking with ALL THINGS CONSIDERED. SMITH: Thank you. AUDIE CORNISH, HOST:   Just because we can use it, should we? That's the question more and more people are asking about face recognition technology, software that's already in our phones and our social media feeds and many security systems. San Francisco leaders have voted to ban the police from using it, and even some in the tech industry say there should be limits. BRAD SMITH: It's the kind of technology that can do a lot of good for a lot of people, but it can be misused. It can be abused. It can be used in ways that lead to discrimination and bias. It can really make possible the kind of mass surveillance that in the past has always just been the realm of science fiction. CORNISH: That's Brad Smith. He's president of Microsoft. The company makes facial recognition software and other tools that raise some ethical dilemmas, and he's our guest on this week's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CORNISH: Brad Smith told me Microsoft has turned down deals to sell its facial recognition software to buyers it didn't trust to use it properly. SMITH: What facial recognition services enables somebody to do is look at your image on a camera and run it against a database and identify who you are. One deal we turned down was with a law enforcement division in California. They wanted to put this in all their police cars so that if you were pulled over for any purpose, they would look for a match. And if you matched a suspect's face, you would be taken downtown for further questioning. Our concern is that the technology, regardless of where you buy it from, is just not ready for that. It will lead to bias. It will lead to false identifications. People will be put in the back of police cars when they've done nothing wrong. It seemed to us to be a step too far. CORNISH: Now, on the other hand, Microsoft has provided the technology to a prison. Microsoft researchers have worked with a Chinese military-run university on AI research that some people fear could be used against China's minorities. So to you, where is the line when it comes to helping governments gain and use these kinds of tools? SMITH: We need to look constantly at the technology that's at issue and how it's going to be used. We were comfortable providing facial recognition within a prison because the sample size of people involved is actually relatively small. We could be confident that people would be identified correctly. And there was a societally beneficial goal, namely to actually help keep prisoners safe by knowing who was where and at what time. More broadly, we've supported basic research, advances in the fundamental frontiers of knowledge. You know, we're not working, for example, with authorities in China to deploy facial recognition services for surveillance. But when you get to the bottom foundation for all artificial intelligence, advances in machine learning and the like, we believe that that's where there are real benefits for people able to work on advancing scientific knowledge more generally. CORNISH: Now, something that bothered some Microsoft employees earlier this year enough to write an open letter about it was Microsoft's contract with the U. S. Army. And you're making augmented reality headsets for soldiers. These employees said, look; we didn't sign up to develop weapons. You still have that contract. How did you square that? I mean, did the resistance by employees change your thinking or affect your thinking? SMITH: First, we believe we have a responsibility - even a patriotic responsibility - to provide our technology to the people who defend our country and keep us safe. So we said we'll provide artificial intelligence and our other technology products to the U. S. military. Second, we recognize that certain employees may not want to work on these projects. And we've said that we'll work to enable people to work on other projects. That's one of the benefits of being a large company. And so far we've been able to accommodate everybody's interests and respect their views. But third, we've said we'll focus on the issues. We do want the U. S. military - every military and every country - to really address in a thoughtful way the new ethical and broader public policy issues that artificial intelligence is creating. CORNISH: The motto for tech companies for so long was move fast, and break things. Are you all hitting up against the reality of what that could mean? SMITH: Well, I've never been a great fan myself of that motto. I think our motto needs to be, don't move faster than the speed of thought. Put guardrails in place, and think about society. There's still lots of room to move fast. We will need to move fast. But we actually need to be more thoughtful. And what we also need is we need governments. CORNISH: But do you get pushback from that? I mean, that seems like that is completely at odds not just with the motto but with the tech industry saying, look; we are first in the world in the U. S. because we are aggressive, because we push boundaries. SMITH: There are lots of different voices in the tech sector. There are days when I get pushback. There are more days in recent months when I see heads nod. It doesn't mean that the whole industry has moved to a different place. But I do think that we've all learned that technology plays such a pervasive role in the world that it just can't afford to break everything around it. There's just too much that will end up broken. CORNISH: You are, finally, calling for government regulation. What is the role for government in this situation? And I ask because having covered Congress, they don't always seem like they're all that caught up (laughter) on the technology based on some of the questioning. What would tougher rules or regulations look like? SMITH: We live in a country where the government has learned generally well how to manage and regulate complicated products, whether it's an automobile or an airplane or a pharmaceutical product or even a drug. CORNISH: Although when you listen to industry, there's lots of complaints about regulation. It doesn't always sound like, you know, people feel that way. SMITH: Yeah, I - and the complaints are often valid. And to some degree, I think that people who work in business complain about regulation the way college students complain about the food in the cafeteria. It just is part of everyday life. And you can understand it. But a world in which important products are subject to the rule of law and rules of public safety, in my view, is better than a world where there are no rules in place. Rather than complaining about what regulation may bring or even ridiculing people who may ask the wrong question or may ask the right question the wrong way - I think we're all far too quick to criticize even our legislators if they don't ask a question precisely right. That's not going to get us where we need to go. CORNISH: Brad Smith, Microsoft's president, thank you so much for speaking with ALL THINGS CONSIDERED. SMITH: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-20-725038799": {"title": "T-Mobile, Sprint Merger Blessed By FCC Chairman : NPR", "url": "https://www.npr.org/2019/05/20/725038799/fcc-chairman-endorses-t-mobile-merger-with-sprint", "author": "No author found", "published_date": "2019-05-20", "content": "", "section": "Business", "disclaimer": ""}, "2019-05-20-724910121": {"title": "U.S. Delays Ban; Huawei Phones Will Get Android Updates : NPR", "url": "https://www.npr.org/2019/05/20/724910121/after-trump-ban-huawei-phones-will-lose-access-to-google-software", "author": "No author found", "published_date": "2019-05-20", "content": "", "section": "Technology", "disclaimer": ""}, "2019-05-21-725524334": {"title": "United States Postal Service Teams Up With TuSimple For Driveless Truck Pilot : NPR", "url": "https://www.npr.org/2019/05/21/725524334/u-s-postal-service-is-testing-self-driving-trucks", "author": "No author found", "published_date": "2019-05-21", "content": "", "section": "Business", "disclaimer": ""}, "2019-05-21-725118702": {"title": "Baltimore City Ransomware Attack Knocks City Services Offline : NPR", "url": "https://www.npr.org/2019/05/21/725118702/ransomware-cyberattacks-on-baltimore-put-city-services-offline", "author": "No author found", "published_date": "2019-05-21", "content": "STEVE INSKEEP, HOST: A hostage situation continues today in Baltimore. To be clear, the hostage is not a person. It's data. Two weeks ago today, hackers breached city servers, and many digital city services are no longer accessible as a result. Experts say restoring those services could take months because the city is not willing to pay the ransom. From member station WYPR, Emily Sullivan reports. EMILY SULLIVAN, BYLINE: Hackers used an extremely malicious type of ransomware, called RobinHood, to pull off the heist. They're demanding about $100,000 in bitcoin to unlock their grip on city servers by giving up a digital key to all that data. That data ranges from legislative bills to online payments for water and parking tickets. Even the city's lien system is frozen, meaning no real estate sales can happen. AVI RUBIN: Imagine if somebody would sneak into a government building at night, load up a bunch of boxes with all the paperwork for all the pending permits, and all the pending house closings, and all the pending business that the city was conducting, put it all in a truck and drive away and demand some money in order to bring that truck back to give back all the papers. SULLIVAN: That's Avi Rubin, a Johns Hopkins professor and a cybersecurity expert. Baltimore Mayor Jack Young has said the city won't pay the ransom demand. Rubin says there's no way any federal agency could replicate the key needed to unlock the data. RUBIN: I don't even think that the NSA would be able to break this. SULLIVAN: And that means Baltimore will essentially have to painstakingly rebuild its online systems. And as expected, residents here are not happy about it. Many think the city should have been prepared for a cyberattack, especially after Atlanta was hit by a malware last year and made national news. Local reports say it cost Atlanta $17 million to recover from the attack. Baltimore was using older hardware and software, which are more vulnerable. Avi Rubin doesn't blame the cash-strapped city for falling victim. That kind of attitude appears commonplace. RUBIN: It costs a lot of money to prepare for something like this. And if it's never happened, it's only natural to say, well, this type of thing has never happened before so why should we spend a lot of money on it? SULLIVAN: At least 20 municipalities and Cleveland's airport were hit by similar malware attacks in recent months. Private companies are targeted all the time, and ransoms do get paid, providing hackers motivation to keep up attacks. Years ago, it was hospitals being attacked, and most bolstered security. But it's harder for a city like Baltimore to spend a lot of money on what can feel like an abstract threat. Rubin says hopefully this latest cyberattack will change that. RUBIN: It should be an early warning sign to a lot of other cities that they need to beef up their security, and they need to beef up their IT. They need to get more modern computer systems, use the cloud. SULLIVAN: Ashley Merson (ph) has been under contract for a two-bedroom house for over a month now. She's frustrated that the real estate system didn't have a paper backup in place. ASHLEY MERSON: The fact that you have a completely unsustainable computer system with no plan in place for when something like this happens after watching it happen to countless other cities, it's frustrating and disappointing. SULLIVAN: Yesterday, nearly two weeks after the attacks, officials introduced a non-digital workaround for home buying, involving lots of paper. Meanwhile, as the cyberattack continues, officials say they'll try to come up with paper workarounds for other city services. For NPR News, I'm Emily Sullivan in Baltimore. STEVE INSKEEP, HOST:  A hostage situation continues today in Baltimore. To be clear, the hostage is not a person. It's data. Two weeks ago today, hackers breached city servers, and many digital city services are no longer accessible as a result. Experts say restoring those services could take months because the city is not willing to pay the ransom. From member station WYPR, Emily Sullivan reports. EMILY SULLIVAN, BYLINE: Hackers used an extremely malicious type of ransomware, called RobinHood, to pull off the heist. They're demanding about $100,000 in bitcoin to unlock their grip on city servers by giving up a digital key to all that data. That data ranges from legislative bills to online payments for water and parking tickets. Even the city's lien system is frozen, meaning no real estate sales can happen. AVI RUBIN: Imagine if somebody would sneak into a government building at night, load up a bunch of boxes with all the paperwork for all the pending permits, and all the pending house closings, and all the pending business that the city was conducting, put it all in a truck and drive away and demand some money in order to bring that truck back to give back all the papers. SULLIVAN: That's Avi Rubin, a Johns Hopkins professor and a cybersecurity expert. Baltimore Mayor Jack Young has said the city won't pay the ransom demand. Rubin says there's no way any federal agency could replicate the key needed to unlock the data. RUBIN: I don't even think that the NSA would be able to break this. SULLIVAN: And that means Baltimore will essentially have to painstakingly rebuild its online systems. And as expected, residents here are not happy about it. Many think the city should have been prepared for a cyberattack, especially after Atlanta was hit by a malware last year and made national news. Local reports say it cost Atlanta $17 million to recover from the attack. Baltimore was using older hardware and software, which are more vulnerable. Avi Rubin doesn't blame the cash-strapped city for falling victim. That kind of attitude appears commonplace. RUBIN: It costs a lot of money to prepare for something like this. And if it's never happened, it's only natural to say, well, this type of thing has never happened before so why should we spend a lot of money on it? SULLIVAN: At least 20 municipalities and Cleveland's airport were hit by similar malware attacks in recent months. Private companies are targeted all the time, and ransoms do get paid, providing hackers motivation to keep up attacks. Years ago, it was hospitals being attacked, and most bolstered security. But it's harder for a city like Baltimore to spend a lot of money on what can feel like an abstract threat. Rubin says hopefully this latest cyberattack will change that. RUBIN: It should be an early warning sign to a lot of other cities that they need to beef up their security, and they need to beef up their IT. They need to get more modern computer systems, use the cloud. SULLIVAN: Ashley Merson (ph) has been under contract for a two-bedroom house for over a month now. She's frustrated that the real estate system didn't have a paper backup in place. ASHLEY MERSON: The fact that you have a completely unsustainable computer system with no plan in place for when something like this happens after watching it happen to countless other cities, it's frustrating and disappointing. SULLIVAN: Yesterday, nearly two weeks after the attacks, officials introduced a non-digital workaround for home buying, involving lots of paper. Meanwhile, as the cyberattack continues, officials say they'll try to come up with paper workarounds for other city services. For NPR News, I'm Emily Sullivan in Baltimore.", "section": "National", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-23-726353723": {"title": "Fake Facebook Account Removal Doubled Since October, New Report Says  : NPR", "url": "https://www.npr.org/2019/05/23/726353723/facebook-removed-nearly-3-2-billion-fake-accounts-in-last-six-months", "author": "No author found", "published_date": "2019-05-23", "content": "", "section": "Business", "disclaimer": ""}, "2019-05-23-726173840": {"title": "Global Aviation Regulators Meet To Consider Timetable For Boeing's 737 Max  : NPR", "url": "https://www.npr.org/2019/05/23/726173840/global-aviation-regulators-meet-to-consider-timetable-for-boeings-737-max", "author": "No author found", "published_date": "2019-05-23", "content": "", "section": "National", "disclaimer": ""}, "2019-05-24-726784477": {"title": "Digital Forensics Expert Weighs In On Doctored Video Of House Speaker Nancy Pelosi : NPR", "url": "https://www.npr.org/2019/05/24/726784477/digital-forensics-expert-weighs-in-on-doctored-video-of-house-speaker-nancy-pelo", "author": "No author found", "published_date": "2019-05-24", "content": "AUDIE CORNISH, HOST: Now for some digital forensics. Here's House Speaker Nancy Pelosi speaking about Donald Trump. She was at the left-leaning Center for American Progress. (SOUNDBITE OF ARCHIVED RECORDING)NANCY PELOSI: Basically he's saying back to me, why would I work with you if you're investigating me? But the fact is something happened there. CORNISH: Now here's the version that spread on social media - sounds a bit different. (SOUNDBITE OF ARCHIVED RECORDING)PELOSI: Basically he's saying back to me, why would I work with you if you're investigating me? But the fact is something happened there. CORNISH: All right. One more time. Here's what Pelosi actually said. (SOUNDBITE OF ARCHIVED RECORDING)PELOSI: Basically he's saying back to me, why would I work with you if you're investigating me? But the fact is something happened there. CORNISH: And here's what got around the Web. (SOUNDBITE OF ARCHIVED RECORDING)PELOSI: Basically he's saying back to me, why would I work with you if you're investigating me? But the fact is something happened there. CORNISH: The real audio is about eight seconds long. The doctored version shared widely in some social media conservative circles is 11 seconds long. We want to bring in Hany Farid to talk about what's going on. He's a digital forensics expert at the University of California Berkeley. Welcome back to the program. HANY FARID: It's good to be with you again. CORNISH: So let's talk about the difference between these two audio clips. What do you hear? FARID: This is a pretty low-tech fake. Essentially what happened is the video was slowed down a little bit, and the audio was tweaked to get the tone right. The result, though, is quite effective. It makes it sound like Speaker Pelosi is slurring her words. CORNISH: But talk about how you're able to tell. I mean, how hard is it technically to manipulate a video in this way? FARID: It's not that hard at all. It's actually relatively easy to do, to essentially slow down a video. So the average person would be able to do this. The detection is interesting. In this case, it was actually relatively easy because you can go to C-SPAN and you can look at the original. And you can see that it's not slowed down by a small amount. It's about 75% - as you said, eight seconds to around 11 or 12 seconds. When I first saw the video, it seemed to me obviously fake because it just was so exaggerated in the slurring of the words. And it looked to me like the video had been slowed down. Her facial expressions weren't moving properly. So I was pretty sure it was a fake. But once you saw the original C-SPAN video, then it's very easy to determine what happened. CORNISH: At the same time, you had high-profile supporters of the president - his personal lawyer, Rudy Giuliani, tweeted a link to the video with the message, what is wrong with Nancy Pelosi? He later deleted that tweet. What does it mean when we see people spreading it like this? FARID: Yeah. So there's the sort of the rub in all of this is that the fake content is not in and of itself perhaps the biggest problem. I think the biggest problem is the ability to amplify that and spread that around the world almost instantaneously through social media and then, of course, have that being amplified by the president, the White House and the people who work for him. And so therein lies, I think, sort of the danger here is that it's not just a technological problem from the creation of the fake. It's a technological problem from the distribution. And then, of course, it's a very human problem - all of us consuming that, believing it too readily and then spreading it even further. CORNISH: YouTube took down the altered video of Pelosi, Facebook has not. What role should these companies have in approaching this problem? FARID: I think this is going to be one of the defining issues of the next few years for us is, how do you take these private companies and have them take on huge, massive societal responsibilities to the tune of, by the way, 500 hours of YouTube video uploaded a minute? And this video is also incredibly challenging. So when we were dealing with the New Zealand video of the horrific violence, that was a clear-cut case. There was no place for that video online. This video is more interesting. It's not a fundamentally bad video. Satire is fine. Comedy is fine. But when it is put in the context of fake news and trying to disparage our politicians or individuals, well, then it's a more complex issue that we have to deal with. So it's not just a technical issue. It's also a policy issue in how we think about balancing speech and an open and free Internet. CORNISH: Hany Farid is a computer science professor and digital forensics expert at the University of California Berkeley. Thank you for speaking with us. FARID: Good to be here. Thank you. AUDIE CORNISH, HOST:  Now for some digital forensics. Here's House Speaker Nancy Pelosi speaking about Donald Trump. She was at the left-leaning Center for American Progress. (SOUNDBITE OF ARCHIVED RECORDING) NANCY PELOSI: Basically he's saying back to me, why would I work with you if you're investigating me? But the fact is something happened there. CORNISH: Now here's the version that spread on social media - sounds a bit different. (SOUNDBITE OF ARCHIVED RECORDING) PELOSI: Basically he's saying back to me, why would I work with you if you're investigating me? But the fact is something happened there. CORNISH: All right. One more time. Here's what Pelosi actually said. (SOUNDBITE OF ARCHIVED RECORDING) PELOSI: Basically he's saying back to me, why would I work with you if you're investigating me? But the fact is something happened there. CORNISH: And here's what got around the Web. (SOUNDBITE OF ARCHIVED RECORDING) PELOSI: Basically he's saying back to me, why would I work with you if you're investigating me? But the fact is something happened there. CORNISH: The real audio is about eight seconds long. The doctored version shared widely in some social media conservative circles is 11 seconds long. We want to bring in Hany Farid to talk about what's going on. He's a digital forensics expert at the University of California Berkeley. Welcome back to the program. HANY FARID: It's good to be with you again. CORNISH: So let's talk about the difference between these two audio clips. What do you hear? FARID: This is a pretty low-tech fake. Essentially what happened is the video was slowed down a little bit, and the audio was tweaked to get the tone right. The result, though, is quite effective. It makes it sound like Speaker Pelosi is slurring her words. CORNISH: But talk about how you're able to tell. I mean, how hard is it technically to manipulate a video in this way? FARID: It's not that hard at all. It's actually relatively easy to do, to essentially slow down a video. So the average person would be able to do this. The detection is interesting. In this case, it was actually relatively easy because you can go to C-SPAN and you can look at the original. And you can see that it's not slowed down by a small amount. It's about 75% - as you said, eight seconds to around 11 or 12 seconds. When I first saw the video, it seemed to me obviously fake because it just was so exaggerated in the slurring of the words. And it looked to me like the video had been slowed down. Her facial expressions weren't moving properly. So I was pretty sure it was a fake. But once you saw the original C-SPAN video, then it's very easy to determine what happened. CORNISH: At the same time, you had high-profile supporters of the president - his personal lawyer, Rudy Giuliani, tweeted a link to the video with the message, what is wrong with Nancy Pelosi? He later deleted that tweet. What does it mean when we see people spreading it like this? FARID: Yeah. So there's the sort of the rub in all of this is that the fake content is not in and of itself perhaps the biggest problem. I think the biggest problem is the ability to amplify that and spread that around the world almost instantaneously through social media and then, of course, have that being amplified by the president, the White House and the people who work for him. And so therein lies, I think, sort of the danger here is that it's not just a technological problem from the creation of the fake. It's a technological problem from the distribution. And then, of course, it's a very human problem - all of us consuming that, believing it too readily and then spreading it even further. CORNISH: YouTube took down the altered video of Pelosi, Facebook has not. What role should these companies have in approaching this problem? FARID: I think this is going to be one of the defining issues of the next few years for us is, how do you take these private companies and have them take on huge, massive societal responsibilities to the tune of, by the way, 500 hours of YouTube video uploaded a minute? And this video is also incredibly challenging. So when we were dealing with the New Zealand video of the horrific violence, that was a clear-cut case. There was no place for that video online. This video is more interesting. It's not a fundamentally bad video. Satire is fine. Comedy is fine. But when it is put in the context of fake news and trying to disparage our politicians or individuals, well, then it's a more complex issue that we have to deal with. So it's not just a technical issue. It's also a policy issue in how we think about balancing speech and an open and free Internet. CORNISH: Hany Farid is a computer science professor and digital forensics expert at the University of California Berkeley. Thank you for speaking with us. FARID: Good to be here. Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-24-726680414": {"title": "Trump Intends To Deploy 1,500 Troops To The Middle East : NPR", "url": "https://www.npr.org/2019/05/24/726680414/trump-orders-an-additional-1-500-troops-to-the-middle-east", "author": "No author found", "published_date": "2019-05-24", "content": "", "section": "National Security", "disclaimer": ""}, "2019-05-25-722811953": {"title": "Women Make Up The Majority Of The Workforce At Food Delivery Apps : NPR", "url": "https://www.npr.org/2019/05/25/722811953/why-suburban-moms-are-delivering-your-groceries", "author": "No author found", "published_date": "2019-05-25", "content": "MICHEL MARTIN, HOST:  Some of the newest jobs of this economy have been created by apps like Uber and Instacart. And it turns out that grocery delivery gigs are particularly attractive to women, who make up the majority of workers. Two apps confirmed this to NPR's Alina Selyukh. As NPR's look into the full employment economy, she spent a day with one suburban mom in Sacramento to find out why. HILARY GORDON: You want apple? UNIDENTIFIED PERSON #1: Just apple and. . . UNIDENTIFIED PERSON #2: We'll be going to bed. ALINA SELYUKH, BYLINE: At 6:30 a. m. , 4 out of 5 Gordon family members are up - if you only count the humans, because there are also dogs, chickens, goats, a bunny, a tortoise and a not-so-miniature miniature pig. UNIDENTIFIED PERSON #2: Squiggy, come here. Good boy. (SOUNDBITE OF PIG SNORTING)GORDON: This is what I have in my kitchen every morning. SELYUKH: That's Hilary Gordon - she's 47 - doing the morning mom hustle. GORDON: OK. Your breakfast sandwich, Jane's check for graduation. Drew, soccer ball - really? SELYUKH: Her daughters are 14 and 17. Her son is 11. UNIDENTIFIED PERSON #1: Oh, by the way. I forgot. I have homework. SELYUKH: Having time like this with her family is a major reason why Gordon works as a shopper for Instacart. It's one of the delivery apps that hires tens of thousands of people like her to deliver groceries from supermarkets - think Safeway or Sam's Club. It's the epitome of gig work. Like with Uber and Lyft, you can decide when to work. But with deliveries, you don't invite strangers into your car. This draws women, often in their 40s and 50s, who now make up more than half of the contractors working for major food delivery apps. GORDON: This is what I tend to do. I'll sit and wait. SELYUKH: This Subaru SUV is Gordon's office. Her Instacart shift today is 8 p. m. to 6 p. m. She'll spend much of it in her car in parking lots with her shopping sneakers on, watching her phone, waiting for grocery orders. GORDON: There we go. We've got a batch - $8, there's no tip, for seven items to take 4 miles. SELYUKH: Plus, the store is another 8 miles away. GORDON: It's really hard to say yes to that because you feel like then they're thinking, oh, well, see, they'll work for that. And I don't want to work for that. SELYUKH: But you can't skip too many orders. The app will think you stopped working. Eventually, we get a good one. GORDON: It's 9. 87 for them and a $6 tip, but it's half a mile. SELYUKH: We're close. The order is eight items. Gordon can knock it out in about 20 minutes. Shopping for other people is a bit like a scavenger hunt, except the app is timing you. You rush through the whole store, do your best quality checks on produce, some Hail Marys on hard versus soft avocados. . . GORDON: Is it for tomorrow? Is it for today? SELYUKH: . . . And a surprising amount of chatting up store workers. GORDON: Hi, Belle (ph). Do you know if they're going to stock more in here? SELYUKH: Gordon says this is the part she loves the most - meeting other people at the store or at their homes, especially if they have friendly pets. But, like a lot of gig workers, this is not a career she pictured for herself. GORDON: So I've got two master's degrees - one in counseling and one in - master of family therapy. SELYUKH: Gordon worked as a therapist in other states, but a move to California required more classes and exams right as her oldest was born. She became a social worker until she made the calculation familiar to many working women. Her job took up mornings and evenings. Her salary - not great. GORDON: When you're going to pay the majority of it to let somebody else raise your kids, just didn't make any sense. SELYUKH: A few years later, the recession hit. Her husband lost his job in finance. Their house value dropped. They're still paying down the debt that stacked up. But now, there's also. . . GORDON: Swim team, soccer - Drew's cleats fell apart on the field - trips to look at colleges, AP exams, junior prom dresses. . . GORDON: This is what I heard a lot as I interviewed more than a half-dozen women working for Instacart and other delivery apps like DoorDash, Shipt and Postmates. They said the apps help them cover some expenses of child and health care and opportunity for side income on top of another steadier job - their own or their partner's. Researchers say this supplemental work is the fastest-growing part of the gig economy. STACIE BALLARD: If you're a mom or a caregiver, something that's really flexible. MARTIN: That Stacie Ballard from Atlanta, who's worked for at least seven apps while building her own business. She keeps a stack of work T-shirts in her trunk - a green one to walk dogs for Wag, a gray one to deliver packages for Shipt. BALLARD: Run to the bathroom, put the shirt on, go through the grocery thing (laughter). And then, if it was the next one, swap the shirt out. And yeah, it was kind of funny. SELYUKH: Ballard works for apps part-time, like many women in the gig economy. Hustling like this full-time is exhausting. But also, hours are unreliable. Take Gordon, who after a year and a half is a super user at Instacart. She's one of the regulars who gets early access to all hours for the week. But to qualify for that access, she has to commit to this gig basically full-time. And on Sunday mornings, it's a moment of madness signing up for shifts. GORDON: So it's really anxiety-producing. SELYUKH: And you're just clicking. GORDON: Click, click, click, click, click and then save. And then you hit the next day. Click, click, click, click, click, click, click. SELYUKH: After all that, as any gig worker knows, having more hours does not assure a huge paycheck bump. Remember all that math Gordon does before deciding if an order is worth taking? Gas expenses are a huge factor. Some days, Gordon might drive a total of 100 miles. Instacart pays some of the delivery mileage, but not for the drive to the store in the first place. Next to consider is what she'll be buying. Is it complicated or heavy - like this job shopping for two customers at one store for 32 bucks before tips? GORDON: It looks decent if you don't scroll all the way through to see the last - 13 cases of water. SELYUKH: This lesson she learned early on when she accidentally signed up for a batch with 81 cases of water from Costco. She had to return and give up the whole order. One final thing she always checks is whether she knows the delivery address. GORDON: There's occasionally places that I will not go. SELYUKH: Shoppers said they like to keep mental track of houses with creepy dudes, apartments with no elevators and bad tippers. Orders from businesses are notorious for not tipping. So if the batch does not check all these boxes with enough money, Gordon will probably skip it with a sassy message for Instacart. GORDON: Pay too low - I will not work for free. SELYUKH: Today, she delivered eight orders in 10 hours and made $133. Today was OK. Her best day was $255 in 12 hours. As another woman told me, working for an app is a great job as long as you don't really need the money. Alina Selyukh, NPR News, Sacramento, Calif. MICHEL MARTIN, HOST:   Some of the newest jobs of this economy have been created by apps like Uber and Instacart. And it turns out that grocery delivery gigs are particularly attractive to women, who make up the majority of workers. Two apps confirmed this to NPR's Alina Selyukh. As NPR's look into the full employment economy, she spent a day with one suburban mom in Sacramento to find out why. HILARY GORDON: You want apple? UNIDENTIFIED PERSON #1: Just apple and. . . UNIDENTIFIED PERSON #2: We'll be going to bed. ALINA SELYUKH, BYLINE: At 6:30 a. m. , 4 out of 5 Gordon family members are up - if you only count the humans, because there are also dogs, chickens, goats, a bunny, a tortoise and a not-so-miniature miniature pig. UNIDENTIFIED PERSON #2: Squiggy, come here. Good boy. (SOUNDBITE OF PIG SNORTING) GORDON: This is what I have in my kitchen every morning. SELYUKH: That's Hilary Gordon - she's 47 - doing the morning mom hustle. GORDON: OK. Your breakfast sandwich, Jane's check for graduation. Drew, soccer ball - really? SELYUKH: Her daughters are 14 and 17. Her son is 11. UNIDENTIFIED PERSON #1: Oh, by the way. I forgot. I have homework. SELYUKH: Having time like this with her family is a major reason why Gordon works as a shopper for Instacart. It's one of the delivery apps that hires tens of thousands of people like her to deliver groceries from supermarkets - think Safeway or Sam's Club. It's the epitome of gig work. Like with Uber and Lyft, you can decide when to work. But with deliveries, you don't invite strangers into your car. This draws women, often in their 40s and 50s, who now make up more than half of the contractors working for major food delivery apps. GORDON: This is what I tend to do. I'll sit and wait. SELYUKH: This Subaru SUV is Gordon's office. Her Instacart shift today is 8 p. m. to 6 p. m. She'll spend much of it in her car in parking lots with her shopping sneakers on, watching her phone, waiting for grocery orders. GORDON: There we go. We've got a batch - $8, there's no tip, for seven items to take 4 miles. SELYUKH: Plus, the store is another 8 miles away. GORDON: It's really hard to say yes to that because you feel like then they're thinking, oh, well, see, they'll work for that. And I don't want to work for that. SELYUKH: But you can't skip too many orders. The app will think you stopped working. Eventually, we get a good one. GORDON: It's 9. 87 for them and a $6 tip, but it's half a mile. SELYUKH: We're close. The order is eight items. Gordon can knock it out in about 20 minutes. Shopping for other people is a bit like a scavenger hunt, except the app is timing you. You rush through the whole store, do your best quality checks on produce, some Hail Marys on hard versus soft avocados. . . GORDON: Is it for tomorrow? Is it for today? SELYUKH: . . . And a surprising amount of chatting up store workers. GORDON: Hi, Belle (ph). Do you know if they're going to stock more in here? SELYUKH: Gordon says this is the part she loves the most - meeting other people at the store or at their homes, especially if they have friendly pets. But, like a lot of gig workers, this is not a career she pictured for herself. GORDON: So I've got two master's degrees - one in counseling and one in - master of family therapy. SELYUKH: Gordon worked as a therapist in other states, but a move to California required more classes and exams right as her oldest was born. She became a social worker until she made the calculation familiar to many working women. Her job took up mornings and evenings. Her salary - not great. GORDON: When you're going to pay the majority of it to let somebody else raise your kids, just didn't make any sense. SELYUKH: A few years later, the recession hit. Her husband lost his job in finance. Their house value dropped. They're still paying down the debt that stacked up. But now, there's also. . . GORDON: Swim team, soccer - Drew's cleats fell apart on the field - trips to look at colleges, AP exams, junior prom dresses. . . GORDON: This is what I heard a lot as I interviewed more than a half-dozen women working for Instacart and other delivery apps like DoorDash, Shipt and Postmates. They said the apps help them cover some expenses of child and health care and opportunity for side income on top of another steadier job - their own or their partner's. Researchers say this supplemental work is the fastest-growing part of the gig economy. STACIE BALLARD: If you're a mom or a caregiver, something that's really flexible. MARTIN: That Stacie Ballard from Atlanta, who's worked for at least seven apps while building her own business. She keeps a stack of work T-shirts in her trunk - a green one to walk dogs for Wag, a gray one to deliver packages for Shipt. BALLARD: Run to the bathroom, put the shirt on, go through the grocery thing (laughter). And then, if it was the next one, swap the shirt out. And yeah, it was kind of funny. SELYUKH: Ballard works for apps part-time, like many women in the gig economy. Hustling like this full-time is exhausting. But also, hours are unreliable. Take Gordon, who after a year and a half is a super user at Instacart. She's one of the regulars who gets early access to all hours for the week. But to qualify for that access, she has to commit to this gig basically full-time. And on Sunday mornings, it's a moment of madness signing up for shifts. GORDON: So it's really anxiety-producing. SELYUKH: And you're just clicking. GORDON: Click, click, click, click, click and then save. And then you hit the next day. Click, click, click, click, click, click, click. SELYUKH: After all that, as any gig worker knows, having more hours does not assure a huge paycheck bump. Remember all that math Gordon does before deciding if an order is worth taking? Gas expenses are a huge factor. Some days, Gordon might drive a total of 100 miles. Instacart pays some of the delivery mileage, but not for the drive to the store in the first place. Next to consider is what she'll be buying. Is it complicated or heavy - like this job shopping for two customers at one store for 32 bucks before tips? GORDON: It looks decent if you don't scroll all the way through to see the last - 13 cases of water. SELYUKH: This lesson she learned early on when she accidentally signed up for a batch with 81 cases of water from Costco. She had to return and give up the whole order. One final thing she always checks is whether she knows the delivery address. GORDON: There's occasionally places that I will not go. SELYUKH: Shoppers said they like to keep mental track of houses with creepy dudes, apartments with no elevators and bad tippers. Orders from businesses are notorious for not tipping. So if the batch does not check all these boxes with enough money, Gordon will probably skip it with a sassy message for Instacart. GORDON: Pay too low - I will not work for free. SELYUKH: Today, she delivered eight orders in 10 hours and made $133. Today was OK. Her best day was $255 in 12 hours. As another woman told me, working for an app is a great job as long as you don't really need the money. Alina Selyukh, NPR News, Sacramento, Calif.", "section": "Profiles Of America In Full Employment", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-25-726941833": {"title": "Doctored Political Videos And Social Media : NPR", "url": "https://www.npr.org/2019/05/25/726941833/doctored-political-videos-and-social-media", "author": "No author found", "published_date": "2019-05-25", "content": "SCOTT SIMON, HOST: Is it real or altered? Rudolph Giuliani, President Trump's personal lawyer and former mayor of New York, posted a video of Nancy Pelosi online that was manipulated to alter her speech in a way that made her appear to slur. She had not. He later removed that video. President Trump posted a different video that isolated a few stumbles in her speech. It is still online. Amanda Carpenter joins us. She's a former communications adviser to Texas Senator Ted Cruz and author of the book \"Gaslighting America. \"Ms. Carpenter, thanks so much for being with us. AMANDA CARPENTER: Thanks for having me on. SIMON: Is this a difference without a distinction - the doctored video versus one that just isolates a few moments? CARPENTER: Well, no as long as it has the same effect. I mean, we talk about whether Trump cares about whether something is altered or real when he's trying to make a point. No, he does not. We've seen this happen time and time and time again. This goes back to the birtherism conspiracy. This time, it just happens to be a video. And the problem, especially with these videos, is that many people can't tell it was manipulated. And in this case, with Nancy Pelosi, it wasn't changed that much. She does stutter a little bit when she talks, but it was exaggerated in a way so that she looked foolish. And once it's out and about and elevated by a president or prominent members of his team, you can't unring that bell. SIMON: Are we in a new age where videos and, for that matter, audio can be doctored? CARPENTER: We're not in a new age where video and audio can be doctored. That has happened. What's new now is that the Internet, as we now know, is the No. 1 way that people get information. And the problem is that the Internet doesn't have an editor. The Internet doesn't care about whether something is credible or not. The people who are in charge of powerful networks - Facebook, Twitter, Instagram - only care about engagement - click, click, click, click. And so there's no incentive to take it down because no one is held responsible for content that is false. SIMON: Explain. I guess, I'm not holding you responsible for it, but we certainly have interviewed people who have cited a questionable video or another and said to them, look; that's not true. They often refuse to believe that. CARPENTER: Ultimately, I think we will get to a place where we will have to authenticate videos. Someone will have to independently verify things. We're not there yet right now. And the only reason that we know the Nancy Pelosi video was fake is because she is so high profile that someone independently looked at it, right? You know, it would be great if Facebook made people signify that something is parody or manipulated in some way from the original when they posted it. But they're so reluctant to do anything that would put any burden on anyone from posting content that it's going to take a lot of pressure. SIMON: I have to ask you about your time with Senator Cruz's campaign as his communications adviser. Donald Trump, then the - the candidate notably said I'm going to read a quote. \"His father\" - meaning Senator Cruz's father - \"was with Lee Harvey Oswald prior to Oswald's being, you know, shot. I mean, what was he doing with Lee Harvey Oswald shortly before the death, before the shooting? It's horrible. \" Senator Cruz now supports President Trump. Is this conciliation just enabling the president to float nonsense in the public mind? CARPENTER: At this point, the Republican Party is Trump's party. And, you know, Senator Cruz is in a very tough position. Donald Trump used smears in the National Enquirer to damage Cruz's candidacy. But that's not the first time that Trump made things up. SIMON: Who do you want to hold responsible - the campaigns, the politicians themselves, the platforms over which this stuff is generated? CARPENTER: I think that social networks - Facebook, Instagram, Twitter - absolutely have an obligation. But ultimately, it is our fault that someone like Donald Trump gets the highest office in the land by peddling rumors, conspiracies and hoaxes. That is on voters to hold accountable. And a lot of people stayed home in 2016 thinking Hillary Clinton would win. That didn't happen. And so if it happens again in 2020, it is our fault. SIMON: Amanda Carpenter - her book \"Gaslighting America: Why We Love It When Trump Lies To Us\" - thanks so much for being with us. CARPENTER: Thank you so much. SCOTT SIMON, HOST:  Is it real or altered? Rudolph Giuliani, President Trump's personal lawyer and former mayor of New York, posted a video of Nancy Pelosi online that was manipulated to alter her speech in a way that made her appear to slur. She had not. He later removed that video. President Trump posted a different video that isolated a few stumbles in her speech. It is still online. Amanda Carpenter joins us. She's a former communications adviser to Texas Senator Ted Cruz and author of the book \"Gaslighting America. \" Ms. Carpenter, thanks so much for being with us. AMANDA CARPENTER: Thanks for having me on. SIMON: Is this a difference without a distinction - the doctored video versus one that just isolates a few moments? CARPENTER: Well, no as long as it has the same effect. I mean, we talk about whether Trump cares about whether something is altered or real when he's trying to make a point. No, he does not. We've seen this happen time and time and time again. This goes back to the birtherism conspiracy. This time, it just happens to be a video. And the problem, especially with these videos, is that many people can't tell it was manipulated. And in this case, with Nancy Pelosi, it wasn't changed that much. She does stutter a little bit when she talks, but it was exaggerated in a way so that she looked foolish. And once it's out and about and elevated by a president or prominent members of his team, you can't unring that bell. SIMON: Are we in a new age where videos and, for that matter, audio can be doctored? CARPENTER: We're not in a new age where video and audio can be doctored. That has happened. What's new now is that the Internet, as we now know, is the No. 1 way that people get information. And the problem is that the Internet doesn't have an editor. The Internet doesn't care about whether something is credible or not. The people who are in charge of powerful networks - Facebook, Twitter, Instagram - only care about engagement - click, click, click, click. And so there's no incentive to take it down because no one is held responsible for content that is false. SIMON: Explain. I guess, I'm not holding you responsible for it, but we certainly have interviewed people who have cited a questionable video or another and said to them, look; that's not true. They often refuse to believe that. CARPENTER: Ultimately, I think we will get to a place where we will have to authenticate videos. Someone will have to independently verify things. We're not there yet right now. And the only reason that we know the Nancy Pelosi video was fake is because she is so high profile that someone independently looked at it, right? You know, it would be great if Facebook made people signify that something is parody or manipulated in some way from the original when they posted it. But they're so reluctant to do anything that would put any burden on anyone from posting content that it's going to take a lot of pressure. SIMON: I have to ask you about your time with Senator Cruz's campaign as his communications adviser. Donald Trump, then the - the candidate notably said I'm going to read a quote. \"His father\" - meaning Senator Cruz's father - \"was with Lee Harvey Oswald prior to Oswald's being, you know, shot. I mean, what was he doing with Lee Harvey Oswald shortly before the death, before the shooting? It's horrible. \" Senator Cruz now supports President Trump. Is this conciliation just enabling the president to float nonsense in the public mind? CARPENTER: At this point, the Republican Party is Trump's party. And, you know, Senator Cruz is in a very tough position. Donald Trump used smears in the National Enquirer to damage Cruz's candidacy. But that's not the first time that Trump made things up. SIMON: Who do you want to hold responsible - the campaigns, the politicians themselves, the platforms over which this stuff is generated? CARPENTER: I think that social networks - Facebook, Instagram, Twitter - absolutely have an obligation. But ultimately, it is our fault that someone like Donald Trump gets the highest office in the land by peddling rumors, conspiracies and hoaxes. That is on voters to hold accountable. And a lot of people stayed home in 2016 thinking Hillary Clinton would win. That didn't happen. And so if it happens again in 2020, it is our fault. SIMON: Amanda Carpenter - her book \"Gaslighting America: Why We Love It When Trump Lies To Us\" - thanks so much for being with us. CARPENTER: Thank you so much.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-26-727107542": {"title": "Need Help Deciphering That Vague Text Message? AI Wants To Help : NPR", "url": "https://www.npr.org/2019/05/26/727107542/how-ai-can-change-texting", "author": "No author found", "published_date": "2019-05-26", "content": "SUSAN DAVIS, HOST: Texting is the go-to way to communicate these days. It keeps us connected, but it also has some gray areas. What does that upside-down smiley face mean? Why don't people use periods at the ends of their sentences anymore? A number of tech companies are using artificial intelligence, or AI, to help improve texting relationships. (SOUNDBITE OF AD)UNIDENTIFIED PERSON: Mei can understand you and the people you chat with just based on texts that are already on your phone. Finally, an app that uses AI to empower you. DAVIS: Rainesford Stauffer is a journalist who has written on this issue for The New York Times. She joins us now. Good morning. RAINESFORD STAUFFER: Hi, there. Thanks so much for having me on. DAVIS: So how are apps like Mei designed to help with texting? STAUFFER: I think the idea behind an app like Mei is that based on text messages and call logs that come in through your phone, it is capable of detecting and understanding everything from personality and mood to how we interact and when we interact with our contacts. As it builds this personality profile, it gives you feedback on your texts. And the examples of that were really interesting. It was everything from, your mom loves you very much to, you seem like more of an introvert than an extrovert or detecting abnormal behavior if you're not texting like your usual self. DAVIS: One thing I thought of is I thought about downloading Mei onto my phone. But then I also feel a little freaked out by the privacy component here. I mean, it really does have access to all of your past text messaging, all of your contacts. It seems like that might make some users a little squeamish about using this kind of technology, even if they like the idea of it. STAUFFER: Absolutely. That was one of the things that I really got to speak to the founder about. And he did reassure me that, though they collect communication with every contact to build an accurate profile of the user, the only personally identifiable information they have is your telephone number. So they don't have your name. They couldn't pick you out of a crowd. But the person on the other end who you're corresponding with is not aware that a personality profile is being built on them or that their information is being analyzed in this way. I think the argument to that is, you know, you could turn around and show a friend that you're sitting next to someone's text and say, hey, what do you think this means? Do you think this is a little odd? But the idea of information being harvested with the intention of analyzing mood and personality adds a little bit to that gray area of who has access to what and what we're reading into. DAVIS: Why do people need help texting? Is it really that hard? STAUFFER: I think that now more than ever, our communications are loaded in the sense that it's possible to read into a lot. As you said, does putting a period at the end of a sentence mean anything? Is someone's tone different? I think that we have a desire for answers about what the person on the other end of the line is thinking. And I think apps like Mei work to fill that void. I think the most interesting element of this is the emphasis on trying to make communication and technology more human through increased technology. I think that we're really yearning for human-to-human understanding and trying to figure out how to relate to each other through all of these different forms of communication. DAVIS: Journalist Rainesford Stauffer, thank you. STAUFFER: Thanks so much. SUSAN DAVIS, HOST:  Texting is the go-to way to communicate these days. It keeps us connected, but it also has some gray areas. What does that upside-down smiley face mean? Why don't people use periods at the ends of their sentences anymore? A number of tech companies are using artificial intelligence, or AI, to help improve texting relationships. (SOUNDBITE OF AD) UNIDENTIFIED PERSON: Mei can understand you and the people you chat with just based on texts that are already on your phone. Finally, an app that uses AI to empower you. DAVIS: Rainesford Stauffer is a journalist who has written on this issue for The New York Times. She joins us now. Good morning. RAINESFORD STAUFFER: Hi, there. Thanks so much for having me on. DAVIS: So how are apps like Mei designed to help with texting? STAUFFER: I think the idea behind an app like Mei is that based on text messages and call logs that come in through your phone, it is capable of detecting and understanding everything from personality and mood to how we interact and when we interact with our contacts. As it builds this personality profile, it gives you feedback on your texts. And the examples of that were really interesting. It was everything from, your mom loves you very much to, you seem like more of an introvert than an extrovert or detecting abnormal behavior if you're not texting like your usual self. DAVIS: One thing I thought of is I thought about downloading Mei onto my phone. But then I also feel a little freaked out by the privacy component here. I mean, it really does have access to all of your past text messaging, all of your contacts. It seems like that might make some users a little squeamish about using this kind of technology, even if they like the idea of it. STAUFFER: Absolutely. That was one of the things that I really got to speak to the founder about. And he did reassure me that, though they collect communication with every contact to build an accurate profile of the user, the only personally identifiable information they have is your telephone number. So they don't have your name. They couldn't pick you out of a crowd. But the person on the other end who you're corresponding with is not aware that a personality profile is being built on them or that their information is being analyzed in this way. I think the argument to that is, you know, you could turn around and show a friend that you're sitting next to someone's text and say, hey, what do you think this means? Do you think this is a little odd? But the idea of information being harvested with the intention of analyzing mood and personality adds a little bit to that gray area of who has access to what and what we're reading into. DAVIS: Why do people need help texting? Is it really that hard? STAUFFER: I think that now more than ever, our communications are loaded in the sense that it's possible to read into a lot. As you said, does putting a period at the end of a sentence mean anything? Is someone's tone different? I think that we have a desire for answers about what the person on the other end of the line is thinking. And I think apps like Mei work to fill that void. I think the most interesting element of this is the emphasis on trying to make communication and technology more human through increased technology. I think that we're really yearning for human-to-human understanding and trying to figure out how to relate to each other through all of these different forms of communication. DAVIS: Journalist Rainesford Stauffer, thank you. STAUFFER: Thanks so much.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-28-727585904": {"title": "World Health Organization (WHO) List Video Game Addiction As An Illness  : NPR", "url": "https://www.npr.org/2019/05/28/727585904/is-gaming-disorder-an-illness-the-who-says-yes-adding-it-to-its-list-of-diseases", "author": "No author found", "published_date": "2019-05-28", "content": "", "section": "Education", "disclaimer": ""}, "2019-05-28-727612204": {"title": "MacKenzie Bezos Pledges To Give More Than Half Of Her Billions To Charity : NPR", "url": "https://www.npr.org/2019/05/28/727612204/mackenzie-bezos-pledges-to-give-more-than-half-of-her-billions-to-charity", "author": "No author found", "published_date": "2019-05-28", "content": "", "section": "Your Money", "disclaimer": ""}, "2019-05-28-726658317": {"title": "Instagramming Crowds Pack National Parks : NPR", "url": "https://www.npr.org/2019/05/28/726658317/instagramming-crowds-pack-national-parks", "author": "No author found", "published_date": "2019-05-28", "content": "RACHEL MARTIN, HOST: If you've ever spent time at Old Faithful in Yellowstone Park or the rim of the Grand Canyon, you've no doubt seen a bunch of people jockeying for position to capture the perfect selfie for that perfect Instagram moment. Nate Hegyi of the Mountain West News Bureau explores how social media is changing the way that we experience wild places. NATE HEGYI, BYLINE: It's 5:30 in the morning at Canyonlands National Park in southern Utah. The predawn sky is just beginning to show color. Jonathan Zhang is frantically setting up his professional-looking camera and tripod. He wants to get the perfect shot of the sun rising over deep canyons and rock spires, all framed by a glowing, orange arch. JONATHAN ZHANG: Once the sun comes up, the color of the arch is really incredible. HEGYI: Zhang says it's a stunning view and one you've probably seen before. (SOUNDBITE OF WINDOWS 7 STARTUP SOUND)HEGYI: See. A few years ago, Microsoft used a picture of the Sunrise at Mesa Arch as one of its backgrounds for Windows 7. The landmark has since blown up on social media, making this once-hidden gem Instagram famous. Today, Zhang has to squeeze between dozens of other people all vying to get the same shot. Zhang says this busy scene was not how it looked when he first visited Mesa Arch a decade ago. ZHANG: There were only maybe two or three guys. But now, you know, it's like every morning, it's crowded. HEGYI: I watch as a couple of tourists pose for selfies near the edge of the cliff before a nearby photographer waves them away. And that points to another side effect of all this obsession with documenting your life on social media. People sometimes do stupid things to get cool shots. They climb arches, hang off the edge of cliff sides or stand right next to a bison at Yellowstone. Now, no one has fallen off the steep cliff near Mesa Arch yet, but hiker Renee Gardner says the growing popularity is a double-edged sword. RENEE GARDNER: Parks need money and contributions. But it's also when you come to, like, view something like this, and there's a million people taking pictures, and you can't really enjoy the view. HEGYI: Across the Mountain West, visitation to national parks has increased by more than a third since Instagram was founded in 2010. But is social media - people tagging their exact location and sharing the photos - really to blame for all the new crowds? The evidence is anecdotal. Gas prices and a strong U. S. economy may play a bigger role. Ashley D'Antonio is a recreation ecologist at Oregon State University. She argues that sharing locations on social media can actually be a good thing for our national parks. ASHLEY D'ANTONIO: So most of the visitors to our parks and protected areas are kind of upper-middle class, white, slightly older. But there's been a lot of groups that use social media to kind of get more people of color or people that aren't traditionally seen in our protected areas out recreating and promoting that this is a space for you, too. HEGYI: Still, bigger crowds are leading to tensions between smartphone-wielding tourists and serious photographers who used to have these places more to themselves. Photographer Drew Armstrong and a couple of his buddies watch as a young woman strikes a yoga pose in front of the arch. DREW ARMSTRONG: It's frustrating when they want to get a shot in their Lululemon pants because they're wearing Lululemon pants. I want them to be here and say, you know, this is important. This is - this needs to be protected. HEGYI: Armstrong and his two buddies believe that in the rush to take selfies and post them to the Internet, people are missing out on what makes the parks and public lands so amazing in the first place. NEALE ZINGLE: Solitude, peaceful moments, nature. HEGYI: That's Armstrong's buddy Neale Zingle. But as the sun rises above Mesa Arch, the big crowds begin to dwindle. And eventually, they're all gone. And that's what I'm left with - my own Instagram moment, alone and peaceful. For NPR News, I'm Nate Hegyi at Canyonlands National Park. (SOUNDBITE OF KORESMA'S \"TURQUOISE [EDAPOLLO REMIX]\")MARTIN: That story comes to us from the Mountain West News Bureau. RACHEL MARTIN, HOST:  If you've ever spent time at Old Faithful in Yellowstone Park or the rim of the Grand Canyon, you've no doubt seen a bunch of people jockeying for position to capture the perfect selfie for that perfect Instagram moment. Nate Hegyi of the Mountain West News Bureau explores how social media is changing the way that we experience wild places. NATE HEGYI, BYLINE: It's 5:30 in the morning at Canyonlands National Park in southern Utah. The predawn sky is just beginning to show color. Jonathan Zhang is frantically setting up his professional-looking camera and tripod. He wants to get the perfect shot of the sun rising over deep canyons and rock spires, all framed by a glowing, orange arch. JONATHAN ZHANG: Once the sun comes up, the color of the arch is really incredible. HEGYI: Zhang says it's a stunning view and one you've probably seen before. (SOUNDBITE OF WINDOWS 7 STARTUP SOUND) HEGYI: See. A few years ago, Microsoft used a picture of the Sunrise at Mesa Arch as one of its backgrounds for Windows 7. The landmark has since blown up on social media, making this once-hidden gem Instagram famous. Today, Zhang has to squeeze between dozens of other people all vying to get the same shot. Zhang says this busy scene was not how it looked when he first visited Mesa Arch a decade ago. ZHANG: There were only maybe two or three guys. But now, you know, it's like every morning, it's crowded. HEGYI: I watch as a couple of tourists pose for selfies near the edge of the cliff before a nearby photographer waves them away. And that points to another side effect of all this obsession with documenting your life on social media. People sometimes do stupid things to get cool shots. They climb arches, hang off the edge of cliff sides or stand right next to a bison at Yellowstone. Now, no one has fallen off the steep cliff near Mesa Arch yet, but hiker Renee Gardner says the growing popularity is a double-edged sword. RENEE GARDNER: Parks need money and contributions. But it's also when you come to, like, view something like this, and there's a million people taking pictures, and you can't really enjoy the view. HEGYI: Across the Mountain West, visitation to national parks has increased by more than a third since Instagram was founded in 2010. But is social media - people tagging their exact location and sharing the photos - really to blame for all the new crowds? The evidence is anecdotal. Gas prices and a strong U. S. economy may play a bigger role. Ashley D'Antonio is a recreation ecologist at Oregon State University. She argues that sharing locations on social media can actually be a good thing for our national parks. ASHLEY D'ANTONIO: So most of the visitors to our parks and protected areas are kind of upper-middle class, white, slightly older. But there's been a lot of groups that use social media to kind of get more people of color or people that aren't traditionally seen in our protected areas out recreating and promoting that this is a space for you, too. HEGYI: Still, bigger crowds are leading to tensions between smartphone-wielding tourists and serious photographers who used to have these places more to themselves. Photographer Drew Armstrong and a couple of his buddies watch as a young woman strikes a yoga pose in front of the arch. DREW ARMSTRONG: It's frustrating when they want to get a shot in their Lululemon pants because they're wearing Lululemon pants. I want them to be here and say, you know, this is important. This is - this needs to be protected. HEGYI: Armstrong and his two buddies believe that in the rush to take selfies and post them to the Internet, people are missing out on what makes the parks and public lands so amazing in the first place. NEALE ZINGLE: Solitude, peaceful moments, nature. HEGYI: That's Armstrong's buddy Neale Zingle. But as the sun rises above Mesa Arch, the big crowds begin to dwindle. And eventually, they're all gone. And that's what I'm left with - my own Instagram moment, alone and peaceful. For NPR News, I'm Nate Hegyi at Canyonlands National Park. (SOUNDBITE OF KORESMA'S \"TURQUOISE [EDAPOLLO REMIX]\") MARTIN: That story comes to us from the Mountain West News Bureau.", "section": "National", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-29-728005817": {"title": "Uber To Start Banning Riders With Low Ratings : NPR", "url": "https://www.npr.org/2019/05/29/728005817/uber-to-start-banning-passengers-with-low-ratings", "author": "No author found", "published_date": "2019-05-29", "content": "", "section": "Technology", "disclaimer": ""}, "2019-05-29-727850485": {"title": "'Plenty Of Cards To Play': Chinese Media Suggest Cutting Rare Earth Exports To U.S. : NPR", "url": "https://www.npr.org/2019/05/29/727850485/plenty-of-cards-to-play-chinese-media-suggest-cutting-rare-earth-exports-to-u-s", "author": "No author found", "published_date": "2019-05-29", "content": "", "section": "World", "disclaimer": ""}, "2019-05-29-726760128": {"title": "How Iran And Its Proxies Use Drone Warfare : NPR", "url": "https://www.npr.org/2019/05/29/726760128/in-yemen-conflict-some-see-a-new-age-of-drone-warfare", "author": "No author found", "published_date": "2019-05-29", "content": "AUDIE CORNISH, HOST: As tensions rise between the U. S. and Iran, we turn to a related story. Drone strikes have been hitting oil pipelines and airfields in the Gulf region. These drones are relatively low tech, yet as NPR's Geoff Brumfiel reports, they are the clearest sign yet that a new era of warfare has begun. GEOFF BRUMFIEL, BYLINE: Back in January, a group of high-level military commanders gathered for an event at an airbase in Yemen. It was far from the front lines of the ongoing civil war. But then as the cameras were rolling, something suddenly appeared out of the sky. And a warning - what you're about to hear is loud. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: (Foreign language spoken). (SOUNDBITE OF EXPLOSION)NICK WATERS: It's pretty scary because it's clear that these guys had absolutely no idea what had just happened. BRUMFIEL: Nick Waters is with Bellingcat, a group of open-source researchers. What had happened was a small drone had flown over the dais and detonated, peppering the ground with shrapnel. According to press reports, the blast killed several, including the head of military intelligence. This weapon was launched by Houthi rebels over 18 miles away. Waters says it was a glorified model airplane with an explosive on the front and a propeller on the back. It flew a pre-programmed route using GPS. WATERS: Although it's, like, relatively simple, it can be very effective. BRUMFIEL: In recent weeks, Houthi rebels have sent similar drones to targets inside Saudi Arabia, including oil pumping stations and airfields. Experts say these drones likely come from the Houthis' main sponsor, Iran. ARIANE TABATABAI: So Iran started to develop drones in the 1980s. BRUMFIEL: Ariane Tabatabai is a political scientist at the RAND Corporation. TABATABAI: That was in the context of the Iran-Iraq War, and it was largely trying to make up for its lack of conventional capabilities. BRUMFIEL: Iran has been under various arms embargoes for decades, so Iranian engineers developed drones on their own. Iran has kept the technology to itself until a few years ago when the rival Islamic State, or ISIS, started to gain territory nearby. TABATABAI: The rise of ISIS in Syria and Iraq was really the main driver behind Iran starting to deploy its drones outside of its borders. BRUMFIEL: In fact, the Syrian civil war has been a drone testing ground for all sides. Israel has sent drones to hunt Syrian air defenses. Russia has tried out its newest technology. Even ISIS used commercial drones to drop small explosives on the enemy. Many of Iran's drones occupy a sort of middle ground between the advanced weapons of major powers and the off-the-shelf technology used by ISIS. Ali Vaez is with the international Crisis Group. He says taking cheap drones to the next level is exactly the kind of thing Iran is good at. ALI VAEZ: It is very much in line with Iranians' modus operandi. BRUMFIEL: Vaez says these drones fit well with the nation's overall defense strategy, using asymmetric warfare and proxy groups to take on its enemies far from its own borders. He's also not surprised by recent evidence that Iran is sharing not just drones but drone technologies so that partner groups like the Houthis in Yemen can build their own drones. VAEZ: The Iranian mentality is generally that instead of giving fish to your partners and proxies in the region, you should teach them how to do fishing. BRUMFIEL: Nick Waters, who tracks weapons systems in Yemen and elsewhere, says there is evidence that the Houthi rebels are building their own drones now. And he says small drones are showing up in conflict zones from Ukraine to the Philippines. WATERS: These things are going to be around simply because the capability they give is really, really useful, and they're not that expensive. BRUMFIEL: Cheap drones are rapidly becoming just another weapon of war. Geoff Brumfiel, NPR News. AUDIE CORNISH, HOST:  As tensions rise between the U. S. and Iran, we turn to a related story. Drone strikes have been hitting oil pipelines and airfields in the Gulf region. These drones are relatively low tech, yet as NPR's Geoff Brumfiel reports, they are the clearest sign yet that a new era of warfare has begun. GEOFF BRUMFIEL, BYLINE: Back in January, a group of high-level military commanders gathered for an event at an airbase in Yemen. It was far from the front lines of the ongoing civil war. But then as the cameras were rolling, something suddenly appeared out of the sky. And a warning - what you're about to hear is loud. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: (Foreign language spoken). (SOUNDBITE OF EXPLOSION) NICK WATERS: It's pretty scary because it's clear that these guys had absolutely no idea what had just happened. BRUMFIEL: Nick Waters is with Bellingcat, a group of open-source researchers. What had happened was a small drone had flown over the dais and detonated, peppering the ground with shrapnel. According to press reports, the blast killed several, including the head of military intelligence. This weapon was launched by Houthi rebels over 18 miles away. Waters says it was a glorified model airplane with an explosive on the front and a propeller on the back. It flew a pre-programmed route using GPS. WATERS: Although it's, like, relatively simple, it can be very effective. BRUMFIEL: In recent weeks, Houthi rebels have sent similar drones to targets inside Saudi Arabia, including oil pumping stations and airfields. Experts say these drones likely come from the Houthis' main sponsor, Iran. ARIANE TABATABAI: So Iran started to develop drones in the 1980s. BRUMFIEL: Ariane Tabatabai is a political scientist at the RAND Corporation. TABATABAI: That was in the context of the Iran-Iraq War, and it was largely trying to make up for its lack of conventional capabilities. BRUMFIEL: Iran has been under various arms embargoes for decades, so Iranian engineers developed drones on their own. Iran has kept the technology to itself until a few years ago when the rival Islamic State, or ISIS, started to gain territory nearby. TABATABAI: The rise of ISIS in Syria and Iraq was really the main driver behind Iran starting to deploy its drones outside of its borders. BRUMFIEL: In fact, the Syrian civil war has been a drone testing ground for all sides. Israel has sent drones to hunt Syrian air defenses. Russia has tried out its newest technology. Even ISIS used commercial drones to drop small explosives on the enemy. Many of Iran's drones occupy a sort of middle ground between the advanced weapons of major powers and the off-the-shelf technology used by ISIS. Ali Vaez is with the international Crisis Group. He says taking cheap drones to the next level is exactly the kind of thing Iran is good at. ALI VAEZ: It is very much in line with Iranians' modus operandi. BRUMFIEL: Vaez says these drones fit well with the nation's overall defense strategy, using asymmetric warfare and proxy groups to take on its enemies far from its own borders. He's also not surprised by recent evidence that Iran is sharing not just drones but drone technologies so that partner groups like the Houthis in Yemen can build their own drones. VAEZ: The Iranian mentality is generally that instead of giving fish to your partners and proxies in the region, you should teach them how to do fishing. BRUMFIEL: Nick Waters, who tracks weapons systems in Yemen and elsewhere, says there is evidence that the Houthi rebels are building their own drones now. And he says small drones are showing up in conflict zones from Ukraine to the Philippines. WATERS: These things are going to be around simply because the capability they give is really, really useful, and they're not that expensive. BRUMFIEL: Cheap drones are rapidly becoming just another weapon of war. Geoff Brumfiel, NPR News.", "section": "World", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-05-29-727612692": {"title": "Chinese-Made Drones Spark Concerns And Warnings From U.S. Government : NPR", "url": "https://www.npr.org/2019/05/29/727612692/we-re-not-being-paranoid-u-s-warns-of-spy-dangers-of-chinese-made-drones", "author": "No author found", "published_date": "2019-05-29", "content": "", "section": "Politics", "disclaimer": ""}, "2019-05-30-727529802": {"title": "Electronic Poll Books Speed Up Voting Lines, Raise Election Security Concerns : NPR", "url": "https://www.npr.org/2019/05/30/727529802/technology-has-made-voting-lines-move-faster-but-also-made-elections-less-secure", "author": "No author found", "published_date": "2019-05-30", "content": "", "section": "Politics", "disclaimer": ""}, "2019-05-31-727945689": {"title": "Solving Tech's Ethics Problem Could Start In The Classroom : NPR", "url": "https://www.npr.org/2019/05/31/727945689/solving-the-tech-industrys-ethics-problem-could-start-in-the-classroom", "author": "No author found", "published_date": "2019-05-31", "content": "AUDIE CORNISH, HOST: We've been asking this question on All Tech Considered - just because you can develop more advanced technology, should you? A Massachusetts Institute of Technology professor is trying to get future tech workers to ask that question. From member station WBUR, Zeninjor Enwemeka reports. ZENINJOR ENWEMEKA, BYLINE: MIT philosophy professor Abby Everett Jaques thinks a lot about ethics. ABBY EVERETT JAQUES: I'm an ethicist. I'm especially interested in ethics of things we make. ENWEMEKA: Ethics is something the world's largest tech companies have been forced to reckon with because of problems with their products. Facebook has been criticized for failing to quickly remove toxic content, like the livestream of the New Zealand mosque shooting. YouTube had to disable comments on videos of minors after pedophiles flocked to its platform. And as artificial intelligence continues to creep into our daily lives, Professor Jaques worries about privacy. She's really freaked out by facial recognition. JAQUES: Tracking us continuously and pervasively. ENWEMEKA: Studies have already shown facial recognition misidentifies minorities. Some companies have hired ethicists to help them spot some of these issues. But Professor Jaques wants future engineers and computer scientists to understand the pitfalls of tech before they enter the industry. So she created a new class at MIT called Ethics of Technology. One exercise she came up with is to have her class of 30 students play a game designed to make them think about how to achieve fairness. (SOUNDBITE OF CLASSROOM CHATTER)ENWEMEKA: Jaques puts a large paper bag at the front of the room. The students don't know what's inside, except that there are treats and they have to figure out how best to share them with the class. JAQUES: All right. Let's hear some ideas. ENWEMEKA: One student thinks they should just dump everything out of the bag and figure things out from there. But freshman Benjamin Spector thinks they should first put someone in charge. BENJAMIN SPECTOR: Random benevolent dictator. Entirely empowered. (LAUGHTER)ENWEMEKA: The class comes up with a dozen other ideas. Then they vote. The chosen idea, each student is randomly assigned a number and allowed to pick something based on their number once the bag is opened. (SOUNDBITE OF CLASSROOM CHATTER)ENWEMEKA: Professor Jaques empties the bag. It's assorted baked goods. JAQUES: Rice crispy treats, chocolate chip cookies, ginger cookies and oatmeal cookies. ENWEMEKA: But then one student says. . . UNIDENTIFIED MIT STUDENT #1: Sorry. Can we, like, determine who's vegan here? And, like. . . UNIDENTIFIED MIT STUDENT #2: No. No. (LAUGHTER)ENWEMEKA: The class didn't account for different dietary needs. And that's exactly what Professor Jaques wants the students to think about. JAQUES: Our system didn't protect a certain important minority. ENWEMEKA: That resonates with Cel Skeggs, a senior in computer science. CEL SKEGGS: I've been the person throughout the semester beating the dead horse of the how does this technology affect LGBTQ people? To the extent that some people have, like, suggested solutions to things, and then when that question's imposed, they're like, I didn't actually think about that thing at all. ENWEMEKA: This comes into play in real life, too. For instance, some transgender Uber drivers were kicked off the app when a security feature couldn't recognize them. The feature required drivers to take a selfie to verify their identity but didn't account for people who are transitioning. Srinivas Kaza is a computer science major. He wants to work with image technology but says he's really concerned about doctored photos and the spread of misinformation. SRINIVAS KAZA: I think it's just important to not contribute to the problem. ENWEMEKA: And that's exactly why Professor Jaques created this class, for these students to understand that ethics is essential to their work as engineers and computer scientists. For NPR News, I'm Zeninjor Enwemeka in Boston. AUDIE CORNISH, HOST:  We've been asking this question on All Tech Considered - just because you can develop more advanced technology, should you? A Massachusetts Institute of Technology professor is trying to get future tech workers to ask that question. From member station WBUR, Zeninjor Enwemeka reports. ZENINJOR ENWEMEKA, BYLINE: MIT philosophy professor Abby Everett Jaques thinks a lot about ethics. ABBY EVERETT JAQUES: I'm an ethicist. I'm especially interested in ethics of things we make. ENWEMEKA: Ethics is something the world's largest tech companies have been forced to reckon with because of problems with their products. Facebook has been criticized for failing to quickly remove toxic content, like the livestream of the New Zealand mosque shooting. YouTube had to disable comments on videos of minors after pedophiles flocked to its platform. And as artificial intelligence continues to creep into our daily lives, Professor Jaques worries about privacy. She's really freaked out by facial recognition. JAQUES: Tracking us continuously and pervasively. ENWEMEKA: Studies have already shown facial recognition misidentifies minorities. Some companies have hired ethicists to help them spot some of these issues. But Professor Jaques wants future engineers and computer scientists to understand the pitfalls of tech before they enter the industry. So she created a new class at MIT called Ethics of Technology. One exercise she came up with is to have her class of 30 students play a game designed to make them think about how to achieve fairness. (SOUNDBITE OF CLASSROOM CHATTER) ENWEMEKA: Jaques puts a large paper bag at the front of the room. The students don't know what's inside, except that there are treats and they have to figure out how best to share them with the class. JAQUES: All right. Let's hear some ideas. ENWEMEKA: One student thinks they should just dump everything out of the bag and figure things out from there. But freshman Benjamin Spector thinks they should first put someone in charge. BENJAMIN SPECTOR: Random benevolent dictator. Entirely empowered. (LAUGHTER) ENWEMEKA: The class comes up with a dozen other ideas. Then they vote. The chosen idea, each student is randomly assigned a number and allowed to pick something based on their number once the bag is opened. (SOUNDBITE OF CLASSROOM CHATTER) ENWEMEKA: Professor Jaques empties the bag. It's assorted baked goods. JAQUES: Rice crispy treats, chocolate chip cookies, ginger cookies and oatmeal cookies. ENWEMEKA: But then one student says. . . UNIDENTIFIED MIT STUDENT #1: Sorry. Can we, like, determine who's vegan here? And, like. . . UNIDENTIFIED MIT STUDENT #2: No. No. (LAUGHTER) ENWEMEKA: The class didn't account for different dietary needs. And that's exactly what Professor Jaques wants the students to think about. JAQUES: Our system didn't protect a certain important minority. ENWEMEKA: That resonates with Cel Skeggs, a senior in computer science. CEL SKEGGS: I've been the person throughout the semester beating the dead horse of the how does this technology affect LGBTQ people? To the extent that some people have, like, suggested solutions to things, and then when that question's imposed, they're like, I didn't actually think about that thing at all. ENWEMEKA: This comes into play in real life, too. For instance, some transgender Uber drivers were kicked off the app when a security feature couldn't recognize them. The feature required drivers to take a selfie to verify their identity but didn't account for people who are transitioning. Srinivas Kaza is a computer science major. He wants to work with image technology but says he's really concerned about doctored photos and the spread of misinformation. SRINIVAS KAZA: I think it's just important to not contribute to the problem. ENWEMEKA: And that's exactly why Professor Jaques created this class, for these students to understand that ethics is essential to their work as engineers and computer scientists. For NPR News, I'm Zeninjor Enwemeka in Boston.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-03-729390558": {"title": "iTunes Fans Share Their Fondest Memories : NPR", "url": "https://www.npr.org/2019/06/03/729390558/apple-will-not-offer-itunes-as-part-of-its-new-operating-system", "author": "No author found", "published_date": "2019-06-03", "content": "ARI SHAPIRO, HOST: Nearly two decades ago, Apple announced a new jukebox software. It was called iTunes. And today Apple has announced that in its new operating system, iTunes is going away to be replaced by another music app. In Rolling Stone, Amy Wang writes a \"Farewell To A Clunky But World-Shattering Icon. \" And she joins us now for a remembrance. Hi, Amy. AMY WANG: Hey, Ari. SHAPIRO: World-shattering is a very strong phrase. So when iTunes debuted in 2001, how did it revolutionize the music industry? WANG: It completely changed the way that people buy and listen to music, right? And of course you have to factor in that before iTunes, there was also a brief era where the music industry was terrified that people were just going to download things illegally and pirate music. . . SHAPIRO: The Napster era. WANG: . . . For the rest of their lives. SHAPIRO: Right. WANG: Exactly. And so iTunes came in and sort of did two things at once. It moved the model from retail stores onto the Web, and it also sort of helped ease those fears that Napster would take over. SHAPIRO: OK, so I asked people on Twitter for their iTunes memories. And someone named Monica Bisha said, that first time I realized I no longer needed to put 10 CDs in my backpack to get work done at the library. It was amazing. WANG: Yeah, I mean, to kids today the image of fitting 10 CDs into one thing is just unspeakable. SHAPIRO: (Laughter) Right. WANG: That's, like, so uncool. SHAPIRO: Do you remember the first music you bought on iTunes? WANG: I think it was probably something, you know, embarrassing now, like The Killers' single or something. SHAPIRO: Oh, of course - coming out of my. . . (SOUNDBITE OF THE KILLERS SONG, \"MR. BRIGHTSIDE\")SHAPIRO: That one? WANG: Yeah, definitely. (SOUNDBITE OF SONG, \"MR. BRIGHTSIDE\")THE KILLERS: (Singing) . . . And I've been doing just fine. Gotta gotta be down because I want it all. It started out with a kiss. . . SHAPIRO: OK, when I asked on Twitter for memories, somebody named Byungho Park brought up a big moment in 2010 when, finally, the Beatles' catalog was available on iTunes, after they had held out for almost a decade. (SOUNDBITE OF SONG, \"HERE COMES THE SUN\")THE BEATLES: (Singing) Here comes the sun, do do do do. . . SHAPIRO: It seemed to acknowledge the inevitability of this platform, right? WANG: Yeah, absolutely. If you think of the Beatles as the sort of, like, cultural indicator of anything, whatever they say is kind of, like, the gold standard for the industry. (SOUNDBITE OF SONG, \"HERE COMES THE SUN\")THE BEATLES: (Singing) Here comes the sun. And I say it's all right. SHAPIRO: Someone named Luke Vargas wrote on Twitter, iTunes debuted when I was in sixth grade. Macs sold that year featured some preloaded songs to showcase iTunes, including \"Love Shack\" by the B-52's. (SOUNDBITE OF SONG, \"LOVE SHACK\")B-52'S: (Singing) . . . Says 15 miles to the. . . Love shack. SHAPIRO: He says, the song still grates on me, but it will forever be my first digital audio experience. WANG: (Laughter). iTunes came out when the iPod came out, too. Those two things were in tandem. And to this day, I know people who still say, you know, that's the iPod song - because they remember it so vividly. (SOUNDBITE OF SONG, \"LOVE SHACK\")B-52'S: (Singing) The love shack is a little old place where we can get together. SHAPIRO: It feels to me like as we go from purchasing physical things, like CDs, to purchasing MP3s on iTunes to now just streaming, the connection to the music we listen to has become weaker. WANG: That's so true. You essentially lease music instead of buying it. So that's the hypothesis for why live events are growing so much. And people want to go to concerts and festivals more than ever because if you can stream things so easily and so openly every day, then you want to crave that emotional connection to an artist that you can get maybe in person or via some other means rather than just buying their music. SHAPIRO: Is there a song from the iTunes era you'd like us to go out on? WANG: I think what comes to mind is that Feist song, \"1234. \"SHAPIRO: Yes. WANG: You know what I'm talking about? Yeah. SHAPIRO: (Laughter) Yes, I totally do. WANG: The iconic, like, person bobbing along to an iPod. (SOUNDBITE OF SONG, \"1234\")FEIST: (Singing) One, two, three, four, tell me that you love me more. Three. . . SHAPIRO: Amy Wang of Rolling Stone, thanks for this reminiscence. WANG: Thanks so much, Ari. (SOUNDBITE OF SONG, \"1234\")FEIST: (Singing) Old teenage hopes are alive at your door. Left you with nothing. . . ARI SHAPIRO, HOST:  Nearly two decades ago, Apple announced a new jukebox software. It was called iTunes. And today Apple has announced that in its new operating system, iTunes is going away to be replaced by another music app. In Rolling Stone, Amy Wang writes a \"Farewell To A Clunky But World-Shattering Icon. \" And she joins us now for a remembrance. Hi, Amy. AMY WANG: Hey, Ari. SHAPIRO: World-shattering is a very strong phrase. So when iTunes debuted in 2001, how did it revolutionize the music industry? WANG: It completely changed the way that people buy and listen to music, right? And of course you have to factor in that before iTunes, there was also a brief era where the music industry was terrified that people were just going to download things illegally and pirate music. . . SHAPIRO: The Napster era. WANG: . . . For the rest of their lives. SHAPIRO: Right. WANG: Exactly. And so iTunes came in and sort of did two things at once. It moved the model from retail stores onto the Web, and it also sort of helped ease those fears that Napster would take over. SHAPIRO: OK, so I asked people on Twitter for their iTunes memories. And someone named Monica Bisha said, that first time I realized I no longer needed to put 10 CDs in my backpack to get work done at the library. It was amazing. WANG: Yeah, I mean, to kids today the image of fitting 10 CDs into one thing is just unspeakable. SHAPIRO: (Laughter) Right. WANG: That's, like, so uncool. SHAPIRO: Do you remember the first music you bought on iTunes? WANG: I think it was probably something, you know, embarrassing now, like The Killers' single or something. SHAPIRO: Oh, of course - coming out of my. . . (SOUNDBITE OF THE KILLERS SONG, \"MR. BRIGHTSIDE\") SHAPIRO: That one? WANG: Yeah, definitely. (SOUNDBITE OF SONG, \"MR. BRIGHTSIDE\") THE KILLERS: (Singing) . . . And I've been doing just fine. Gotta gotta be down because I want it all. It started out with a kiss. . . SHAPIRO: OK, when I asked on Twitter for memories, somebody named Byungho Park brought up a big moment in 2010 when, finally, the Beatles' catalog was available on iTunes, after they had held out for almost a decade. (SOUNDBITE OF SONG, \"HERE COMES THE SUN\") THE BEATLES: (Singing) Here comes the sun, do do do do. . . SHAPIRO: It seemed to acknowledge the inevitability of this platform, right? WANG: Yeah, absolutely. If you think of the Beatles as the sort of, like, cultural indicator of anything, whatever they say is kind of, like, the gold standard for the industry. (SOUNDBITE OF SONG, \"HERE COMES THE SUN\") THE BEATLES: (Singing) Here comes the sun. And I say it's all right. SHAPIRO: Someone named Luke Vargas wrote on Twitter, iTunes debuted when I was in sixth grade. Macs sold that year featured some preloaded songs to showcase iTunes, including \"Love Shack\" by the B-52's. (SOUNDBITE OF SONG, \"LOVE SHACK\") B-52'S: (Singing) . . . Says 15 miles to the. . . Love shack. SHAPIRO: He says, the song still grates on me, but it will forever be my first digital audio experience. WANG: (Laughter). iTunes came out when the iPod came out, too. Those two things were in tandem. And to this day, I know people who still say, you know, that's the iPod song - because they remember it so vividly. (SOUNDBITE OF SONG, \"LOVE SHACK\") B-52'S: (Singing) The love shack is a little old place where we can get together. SHAPIRO: It feels to me like as we go from purchasing physical things, like CDs, to purchasing MP3s on iTunes to now just streaming, the connection to the music we listen to has become weaker. WANG: That's so true. You essentially lease music instead of buying it. So that's the hypothesis for why live events are growing so much. And people want to go to concerts and festivals more than ever because if you can stream things so easily and so openly every day, then you want to crave that emotional connection to an artist that you can get maybe in person or via some other means rather than just buying their music. SHAPIRO: Is there a song from the iTunes era you'd like us to go out on? WANG: I think what comes to mind is that Feist song, \"1234. \" SHAPIRO: Yes. WANG: You know what I'm talking about? Yeah. SHAPIRO: (Laughter) Yes, I totally do. WANG: The iconic, like, person bobbing along to an iPod. (SOUNDBITE OF SONG, \"1234\") FEIST: (Singing) One, two, three, four, tell me that you love me more. Three. . . SHAPIRO: Amy Wang of Rolling Stone, thanks for this reminiscence. WANG: Thanks so much, Ari. (SOUNDBITE OF SONG, \"1234\") FEIST: (Singing) Old teenage hopes are alive at your door. Left you with nothing. . .", "section": "Music News", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-03-729290123": {"title": "Apple Is Killing iTunes, As Music Streaming Exceeds Downloads : NPR", "url": "https://www.npr.org/2019/06/03/729290123/itunes-death-is-all-about-how-we-listen-to-music-today", "author": "No author found", "published_date": "2019-06-03", "content": "", "section": "Business", "disclaimer": ""}, "2019-06-03-729275206": {"title": "Space Is About To Get Much More Crowded, Which Could Ruin The View : NPR", "url": "https://www.npr.org/2019/06/03/729275206/astronomers-worry-that-elon-musks-new-satellites-will-ruin-the-view", "author": "No author found", "published_date": "2019-06-03", "content": "ARI SHAPIRO, HOST: In late May, the commercial spaceflight company SpaceX launched a rocket. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: Three, two, one, zero. Ignition. Liftoff. SHAPIRO: About an hour later, that rocket released 60 satellites. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: They really are just slowly fanning out like a deck of cards into space. SHAPIRO: The satellites are the first part of a massive constellation that will bring the Internet to every corner of the planet, and as NPR's Geoff Brumfiel reports, it may forever change our view of the heavens. GEOFF BRUMFIEL, BYLINE: Victoria Girgis does public education at the Lowell Observatory in Flagstaff, Ariz. And a couple of nights after SpaceX launched all those satellites, she saw them - a bunch of little dots moving into line across the sky. VICTORIA GIRGIS: And they actually just crossed right in front of where the telescope was pointed. BRUMFIEL: The telescope was a small one - Girgis uses it to show celestial objects to the observatory's guests. That night, she was trying to photograph some faint galaxies. The bright satellites created over two dozen streaks across the image. GIRGIS: My first immediate reaction was that's visually kind of cool. But my second reaction was, man, you can't see a single galaxy. BRUMFIEL: The picture was basically ruined. Now, Girgis wasn't the only one to notice this. In the days after the launch, images and videos began to pop up on social media. Jessie Christiansen is an astronomer at Caltech. JESSIE CHRISTIANSEN: All of those videos and pictures delighted the public, but it horrified the astronomy community because it was like, wait, that's bad. BRUMFIEL: Bad because professional astronomers are trying to take lots of pictures of really faint things far out in space. And believe it or not, none of them thought these satellites were going to be a problem. CHRISTIANSEN: Nobody really realized until after launch that they were going to be so bright. BRUMFIEL: But the satellites are bright. And here's the thing - SpaceX is just getting started. The company plans on launching a total of nearly 12,000 satellites to provide global Internet. Other companies, including Amazon, are planning similar constellations. Space is about to get much more crowded, and Christiansen says there's not a lot astronomers can do about it. CHRISTIANSEN: Space is still a little bit of the Wild West, right? We're still working out who owns it and who gets to make the rules. BRUMFIEL: In a statement, SpaceX says it expects the satellites to grow dimmer as they reach their final orbits. And it's looking into other ways to minimize the glare problem. That will help, Christiansen says, but she also thinks the night sky is about to undergo a very big change. CHRISTIANSEN: I think with, you know, 12,000 low-Earth orbit bright satellites, now it'll be, like, hard to find the things that stay still. You're not going to see the same sky anymore. It'll be really interesting. It'll, you know, it'll be a cultural shift. BRUMFIEL: On the bright side, we'll have good Internet. CHRISTIANSEN: We will. And, you know, honestly, there's 10,000 astronomers in the world. We shouldn't stack up against the total sum of humanity, right? There are 7 billion people in the world. And the Internet is an incredible gift. It can be used for so much good. BRUMFIEL: If SpaceX succeeds, that good will soon include cat videos anywhere on earth. Geoff Brumfiel, NPR News. ARI SHAPIRO, HOST:  In late May, the commercial spaceflight company SpaceX launched a rocket. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: Three, two, one, zero. Ignition. Liftoff. SHAPIRO: About an hour later, that rocket released 60 satellites. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #2: They really are just slowly fanning out like a deck of cards into space. SHAPIRO: The satellites are the first part of a massive constellation that will bring the Internet to every corner of the planet, and as NPR's Geoff Brumfiel reports, it may forever change our view of the heavens. GEOFF BRUMFIEL, BYLINE: Victoria Girgis does public education at the Lowell Observatory in Flagstaff, Ariz. And a couple of nights after SpaceX launched all those satellites, she saw them - a bunch of little dots moving into line across the sky. VICTORIA GIRGIS: And they actually just crossed right in front of where the telescope was pointed. BRUMFIEL: The telescope was a small one - Girgis uses it to show celestial objects to the observatory's guests. That night, she was trying to photograph some faint galaxies. The bright satellites created over two dozen streaks across the image. GIRGIS: My first immediate reaction was that's visually kind of cool. But my second reaction was, man, you can't see a single galaxy. BRUMFIEL: The picture was basically ruined. Now, Girgis wasn't the only one to notice this. In the days after the launch, images and videos began to pop up on social media. Jessie Christiansen is an astronomer at Caltech. JESSIE CHRISTIANSEN: All of those videos and pictures delighted the public, but it horrified the astronomy community because it was like, wait, that's bad. BRUMFIEL: Bad because professional astronomers are trying to take lots of pictures of really faint things far out in space. And believe it or not, none of them thought these satellites were going to be a problem. CHRISTIANSEN: Nobody really realized until after launch that they were going to be so bright. BRUMFIEL: But the satellites are bright. And here's the thing - SpaceX is just getting started. The company plans on launching a total of nearly 12,000 satellites to provide global Internet. Other companies, including Amazon, are planning similar constellations. Space is about to get much more crowded, and Christiansen says there's not a lot astronomers can do about it. CHRISTIANSEN: Space is still a little bit of the Wild West, right? We're still working out who owns it and who gets to make the rules. BRUMFIEL: In a statement, SpaceX says it expects the satellites to grow dimmer as they reach their final orbits. And it's looking into other ways to minimize the glare problem. That will help, Christiansen says, but she also thinks the night sky is about to undergo a very big change. CHRISTIANSEN: I think with, you know, 12,000 low-Earth orbit bright satellites, now it'll be, like, hard to find the things that stay still. You're not going to see the same sky anymore. It'll be really interesting. It'll, you know, it'll be a cultural shift. BRUMFIEL: On the bright side, we'll have good Internet. CHRISTIANSEN: We will. And, you know, honestly, there's 10,000 astronomers in the world. We shouldn't stack up against the total sum of humanity, right? There are 7 billion people in the world. And the Internet is an incredible gift. It can be used for so much good. BRUMFIEL: If SpaceX succeeds, that good will soon include cat videos anywhere on earth. Geoff Brumfiel, NPR News.", "section": "Space", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-03-729390504": {"title": "How Influencing As A Career Has Impacted Today's Economy : NPR", "url": "https://www.npr.org/2019/06/03/729390504/how-influencing-as-a-career-has-impacted-todays-economy", "author": "No author found", "published_date": "2019-06-03", "content": "AUDIE CORNISH, HOST: A generation ago, kids didn't dream of being social media influencers. But last year, companies spent around $1. 3 billion on marketing through Instagram. So this month's All Tech Considered is all about influencers, their careers and their impact on our economy. (SOUNDBITE OF MUSIC)CORNISH: Today, we hear from two people on opposite ends of this career path. First, Cosette Rinab. She's 19. She has thousands of followers on Instagram and TikTok. That's a platform for short-form videos. And honestly, she's not wild about the term influencer. COSETTE RINAB: You think of the basic Instagram lifestyle influencer, fashion influencer promoting skinny tea, you know? I think most people gear towards the term creator. CORNISH: Rinab got started back in high school with a fashion blog. Soon, she was reaching out to different brands, asking to collaborate. RINAB: I would go on Etsy and find small boutique shops and send them a message. Hi, my name's Cosette. I'm from New York City. I model and act. I have a fashion blog. Would you like to send me some products to try out, and I'll write a dedicated blog post about it? One of the first companies was a shoe company that bought shoes and bedazzled them. That was one of my first deals that really struck me as, oh, wow, this is actually something that I could be making money off of. CORNISH: Fast forward a few years. Rinab is now a rising junior at the University of Southern California, and she runs a club for content creators. Hundreds of students apply each semester. Just 10% get in. RINAB: I actually give a presentation every semester on collaborating with brands and how exactly to approach that because for some people, it's a very foreign idea, and they're scared to reach out to companies. I'm like, how do you expect people to just find your Instagram and reach out to you? You have to make the initiative to actually reach out to them. CORNISH: You took over this club from its founder, who dropped out of college to do this kind of work full-time. So is college even necessary for someone on this path? RINAB: That's a good question. I think it depends. You know, he was at his maximum earning potential. He saw - he felt that school was holding him back. And there are a lot of people who, you know, are in such a good place and have such a good positioning in the industry right now that if they don't pursue all of their time, it might not be there forever. So yeah, I mean, I think college is not necessary for the social media industry, but a lot of it is experience over education in the sense that if you go out and you do the things yourself, you'll most likely learn more than you could learn in a classroom. CORNISH: What are some things that you think some older people, people who are not maybe as familiar with this world, don't understand about the kind of work you're doing? RINAB: I think it's really difficult for people of older generations to really grasp that people are making more money off of YouTube than they could be if they were going to medical school and studying - you know, pursuing a career in medicine. So I think it's such a new idea that people just need a little bit more time to get accustomed to the idea. CORNISH: One thing about this business in particular is it's all about the numbers, I mean, the same way TV's about ratings. But the follower count for people who do social media work is so important. Do you look at yourselves as kind of like athletes? Like, you're in this business to go hard and fast for a long time, and then you'll get out and do something else? Or do you see yourself doing something like this long-term? RINAB: I see myself doing something like this long-term. I think the goal for me is not to hustle, hustle, hustle, get the big numbers fast and then move on to the next thing. I'm very natural in front of the camera. I love sharing parts of my life with my following. And while I don't think that I'm at a point where I can fully support myself by just doing that, I would love to one day. CORNISH: Cosette Rinab is a student at the University of Southern California. She runs the club Reach, a club for influencers. I guess I should say creators, right? RINAB: Yeah. CORNISH: Cosette, thanks so much. RINAB: Thank you so much. CORNISH: Like Cosette Rinab, Sara Li discovered Instagram and its power to persuade when she was a teenager. But she started out an activist. She posted about sexual assault prevention, and it went viral. SARA LI: The number of people who saw my tiny, tiny account from Topeka, Kan. , was mind-blowing. I mean, like, I had actresses from The CW emailing me that said, hey, I saw your project on Instagram. It's really cool. Like, I think it's great that you're speaking out against sexual assault. How can I get involved? CORNISH: Li is now a social media strategist. She works with influencers, but she says she didn't want to be one anymore. LI: It just really wasn't for me. I mean, it was definitely a few years ago when it was less regulated. CORNISH: What do you mean by regulated? LI: Well, so nowadays, you know, there's a lot more transparency on influencers and kind of their brand deals. We have, like, the megainfluencers, like Danielle Bernstein, talking about how much she makes in a post. We have FTC guidelines. And, you know, five years ago, there really wasn't that kind of transparency. I was also 17, 18 at the time. And I remember being part of this Facebook group for influencers, so kind of a digital influencer club very unlike the one that she had mentioned. But it was essentially more along the lines of, how can I edit this photo to make myself as skinny as possible? How can I get this brand deal with this company that's really not great? And I just kind of remember sitting there, being like, I don't want to promote skinny teas. I don't want to promote, you know, like, these faulty products. You know, we're so used to Instagram being this highlight reel that I think that as more and more people talk about influencer culture, we're getting this need for transparency and kind of a more genuine storytelling rather than, here are the best parts of my life, and that's all there is. CORNISH: It sounds like you're saying that this business has come around in a way that magazines once were, right? Like, now we're just looking at people - beautiful people with a lot of resources doing what they do. LI: Yes. Yeah, completely. And, you know, I'm not going to say all influencers are bad because that's not true. I know a lot of influencers and content creators who really do put, like, a genuine, authentic version of themselves out there. But on that note, I think it's also important to kind of realize that there is a huge, huge portion of influencers who do - who kind of are in it for the wrong reason - kind of like that glossy, magazine-style life. CORNISH: How realistic is it to have a sustained career as an influencer? LI: Very realistic. You know, whether you like influencers or not, it's a very profitable career. And I would say it's not unrealistic to quit your full-time job to become an influencer. I would say if you're going to do that, have an actual business plan in mind because it's not as easy as, you know, I have a huge following on Instagram. You really have to know the business. You really need to know how to adapt to every single social media change. And you just really need to have a plan in mind. CORNISH: Is there a danger of burnout? Like, when you talk to young people getting into this business, is that something that they're aware of? LI: Oh, absolutely. But that's the same for any entrepreneur - right? - any self-starter who may be starting their own business at home. I just think as a content creator, there's just more pressure because it's you that you're selling, not just the product. I know for me, sometimes when I post a lot of stories, I'm just like, I don't even know if I'm living my life or I'm just posting this to make it seem like I have a life. You know, you're watching your own reality directed back at you, and then you're also putting it out there for other people to consume. And it's really intense. It's a lot. And for a, like, full-time content creator to deal with that constantly, it can't be easy. CORNISH: Sara Li is a writer and social media strategist. Thank you so much for speaking with us. LI: Yeah, of course. Thank you so much for having me. (SOUNDBITE OF MUSIC) AUDIE CORNISH, HOST:  A generation ago, kids didn't dream of being social media influencers. But last year, companies spent around $1. 3 billion on marketing through Instagram. So this month's All Tech Considered is all about influencers, their careers and their impact on our economy. (SOUNDBITE OF MUSIC) CORNISH: Today, we hear from two people on opposite ends of this career path. First, Cosette Rinab. She's 19. She has thousands of followers on Instagram and TikTok. That's a platform for short-form videos. And honestly, she's not wild about the term influencer. COSETTE RINAB: You think of the basic Instagram lifestyle influencer, fashion influencer promoting skinny tea, you know? I think most people gear towards the term creator. CORNISH: Rinab got started back in high school with a fashion blog. Soon, she was reaching out to different brands, asking to collaborate. RINAB: I would go on Etsy and find small boutique shops and send them a message. Hi, my name's Cosette. I'm from New York City. I model and act. I have a fashion blog. Would you like to send me some products to try out, and I'll write a dedicated blog post about it? One of the first companies was a shoe company that bought shoes and bedazzled them. That was one of my first deals that really struck me as, oh, wow, this is actually something that I could be making money off of. CORNISH: Fast forward a few years. Rinab is now a rising junior at the University of Southern California, and she runs a club for content creators. Hundreds of students apply each semester. Just 10% get in. RINAB: I actually give a presentation every semester on collaborating with brands and how exactly to approach that because for some people, it's a very foreign idea, and they're scared to reach out to companies. I'm like, how do you expect people to just find your Instagram and reach out to you? You have to make the initiative to actually reach out to them. CORNISH: You took over this club from its founder, who dropped out of college to do this kind of work full-time. So is college even necessary for someone on this path? RINAB: That's a good question. I think it depends. You know, he was at his maximum earning potential. He saw - he felt that school was holding him back. And there are a lot of people who, you know, are in such a good place and have such a good positioning in the industry right now that if they don't pursue all of their time, it might not be there forever. So yeah, I mean, I think college is not necessary for the social media industry, but a lot of it is experience over education in the sense that if you go out and you do the things yourself, you'll most likely learn more than you could learn in a classroom. CORNISH: What are some things that you think some older people, people who are not maybe as familiar with this world, don't understand about the kind of work you're doing? RINAB: I think it's really difficult for people of older generations to really grasp that people are making more money off of YouTube than they could be if they were going to medical school and studying - you know, pursuing a career in medicine. So I think it's such a new idea that people just need a little bit more time to get accustomed to the idea. CORNISH: One thing about this business in particular is it's all about the numbers, I mean, the same way TV's about ratings. But the follower count for people who do social media work is so important. Do you look at yourselves as kind of like athletes? Like, you're in this business to go hard and fast for a long time, and then you'll get out and do something else? Or do you see yourself doing something like this long-term? RINAB: I see myself doing something like this long-term. I think the goal for me is not to hustle, hustle, hustle, get the big numbers fast and then move on to the next thing. I'm very natural in front of the camera. I love sharing parts of my life with my following. And while I don't think that I'm at a point where I can fully support myself by just doing that, I would love to one day. CORNISH: Cosette Rinab is a student at the University of Southern California. She runs the club Reach, a club for influencers. I guess I should say creators, right? RINAB: Yeah. CORNISH: Cosette, thanks so much. RINAB: Thank you so much. CORNISH: Like Cosette Rinab, Sara Li discovered Instagram and its power to persuade when she was a teenager. But she started out an activist. She posted about sexual assault prevention, and it went viral. SARA LI: The number of people who saw my tiny, tiny account from Topeka, Kan. , was mind-blowing. I mean, like, I had actresses from The CW emailing me that said, hey, I saw your project on Instagram. It's really cool. Like, I think it's great that you're speaking out against sexual assault. How can I get involved? CORNISH: Li is now a social media strategist. She works with influencers, but she says she didn't want to be one anymore. LI: It just really wasn't for me. I mean, it was definitely a few years ago when it was less regulated. CORNISH: What do you mean by regulated? LI: Well, so nowadays, you know, there's a lot more transparency on influencers and kind of their brand deals. We have, like, the megainfluencers, like Danielle Bernstein, talking about how much she makes in a post. We have FTC guidelines. And, you know, five years ago, there really wasn't that kind of transparency. I was also 17, 18 at the time. And I remember being part of this Facebook group for influencers, so kind of a digital influencer club very unlike the one that she had mentioned. But it was essentially more along the lines of, how can I edit this photo to make myself as skinny as possible? How can I get this brand deal with this company that's really not great? And I just kind of remember sitting there, being like, I don't want to promote skinny teas. I don't want to promote, you know, like, these faulty products. You know, we're so used to Instagram being this highlight reel that I think that as more and more people talk about influencer culture, we're getting this need for transparency and kind of a more genuine storytelling rather than, here are the best parts of my life, and that's all there is. CORNISH: It sounds like you're saying that this business has come around in a way that magazines once were, right? Like, now we're just looking at people - beautiful people with a lot of resources doing what they do. LI: Yes. Yeah, completely. And, you know, I'm not going to say all influencers are bad because that's not true. I know a lot of influencers and content creators who really do put, like, a genuine, authentic version of themselves out there. But on that note, I think it's also important to kind of realize that there is a huge, huge portion of influencers who do - who kind of are in it for the wrong reason - kind of like that glossy, magazine-style life. CORNISH: How realistic is it to have a sustained career as an influencer? LI: Very realistic. You know, whether you like influencers or not, it's a very profitable career. And I would say it's not unrealistic to quit your full-time job to become an influencer. I would say if you're going to do that, have an actual business plan in mind because it's not as easy as, you know, I have a huge following on Instagram. You really have to know the business. You really need to know how to adapt to every single social media change. And you just really need to have a plan in mind. CORNISH: Is there a danger of burnout? Like, when you talk to young people getting into this business, is that something that they're aware of? LI: Oh, absolutely. But that's the same for any entrepreneur - right? - any self-starter who may be starting their own business at home. I just think as a content creator, there's just more pressure because it's you that you're selling, not just the product. I know for me, sometimes when I post a lot of stories, I'm just like, I don't even know if I'm living my life or I'm just posting this to make it seem like I have a life. You know, you're watching your own reality directed back at you, and then you're also putting it out there for other people to consume. And it's really intense. It's a lot. And for a, like, full-time content creator to deal with that constantly, it can't be easy. CORNISH: Sara Li is a writer and social media strategist. Thank you so much for speaking with us. LI: Yeah, of course. Thank you so much for having me. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-04-729547912": {"title": "Lead Prosecutor In Navy SEAL Case Is Removed For Potential Conflict Of Interest : NPR", "url": "https://www.npr.org/2019/06/04/729547912/judge-removes-lead-prosecutor-in-navy-seal-war-crime-case", "author": "No author found", "published_date": "2019-06-04", "content": "", "section": "Law", "disclaimer": ""}, "2019-06-04-729539997": {"title": "Huawei Chairman Proposes A  'No-Spy' Deal With United States : NPR", "url": "https://www.npr.org/2019/06/04/729539997/huawei-chairman-willing-to-sign-a-no-spy-deal-with-the-united-states", "author": "No author found", "published_date": "2019-06-04", "content": "AUDIE CORNISH, HOST: The Chinese company Huawei sits right in the middle of the escalating trade war between the U. S. and China. Huawei is one of the biggest tech companies in the world, and it's poised to extend its global reach. The U. S. doesn't want to see that happen. NPR's chief business editor Pallavi Gogoi visited the company's headquarters. She spoke with its chairman, and she joins us now. And Pallavi, what did you hear? PALLAVI GOGOI, BYLINE: So today, as part of a small group of visiting U. S. journalists, I was at Huawei's beautiful leafy campus. We met with Chairman Liang Hua, who had a surprising message for American leaders who worry that Huawei technology would be used for spying by the Chinese government. In fact, I asked him whether the company would be willing to sign a no-spy agreement, which is basically like a promise that the Huawei technology would not be used for spying. And he said yes. He'd be willing to do that with any country. (SOUNDBITE OF ARCHIVED RECORDING)LIANG HUA: (Speaking Chinese). GOGOI: What he's saying is that he is skeptical that such a deal might not necessarily be possible, specifically with the U. S. And he said the U. S. has not bought anything from Huawei, is not buying anything. And that's the reason why he doesn't see an opportunity to sign such an agreement, specifically with the U. S. CORNISH: Remind us what this is all in response to. GOGOI: So a lot of things, right, Audie? In recent weeks, Huawei has come under intense attack from the U. S. government. Just last month, the U. S. added Huawei to its, quote, unquote, \"entity list,\" which bars American businesses from selling technology to Huawei without government approval. That hurts Huawei at its core. For instance, Google won't be able to supply Huawei with its Android operating system, which you use to get, you know, for Google search or to get Gmail. But really, when you think about it, all of this began last year when the U. S. and China were clashing over trade. In the midst of all that, the U. S. issued an arrest warrant for Huawei chief financial officer Meng Wanzhou on charges that Huawei's breaching American sanctions against Iran, among other allegations. Now, Meng is not just any, you know, CFO. She's also the daughter of Huawei's founder and CEO and is now under house arrest in Canada. Huawei has maintained consistently that the arrest is all about politics and stymieing Huawei's ambitions. CORNISH: Why is this company at the heart of this trade war? GOGOI: So Huawei is one of China's most successful tech companies. It is the world's second-largest smartphone maker, behind Samsung, even ahead of Apple. But more significantly, Huawei is expected to be a world leader in 5G. That's a revolutionary technology that will fuel the future of, you know, things like driverless cars, smart cities, smart traffic lights and what have you. The Trump administration believe that trade is about dominance, and China is showing - China and Huawei is showing to be ahead in this key area. CORNISH: In the meantime, what's next for this company? GOGOI: There is no doubt that if the U. S. ban is imposed, it will hurt Huawei in the short-term. However, there are some experts who say that it might strengthen Huawei's hand over the long-term. Huawei has a plan B to make its own computer chips and operating system in the coming years. Now, there was another development today that could signal an easing of tensions between the U. S. government and Huawei. President Trump was in London, and he met with Theresa May earlier today. In that press conference, he was asked specifically about the U. S. warning European countries just this week of dire consequences, you know, when it comes to sharing intelligence if those countries don't ban Huawei from their 5G networks. President Trump sounded conciliatory, actually. (SOUNDBITE OF ARCHIVED RECORDING)PRESIDENT DONALD TRUMP: No, because we're going to have, absolutely, an agreement on Huawei and everything else. We have an incredible intelligence relationship, and we will be able to work out any differences. GOGOI: But as we have learned, the devil really is in the details. We'll have to find out, you know, what such an agreement would look like. So stay tuned. CORNISH: That's NPR's Pallavi Gogoi reporting on Huawei in China. Thank you for speaking with us. GOGOI: Thank you, Audie. AUDIE CORNISH, HOST:  The Chinese company Huawei sits right in the middle of the escalating trade war between the U. S. and China. Huawei is one of the biggest tech companies in the world, and it's poised to extend its global reach. The U. S. doesn't want to see that happen. NPR's chief business editor Pallavi Gogoi visited the company's headquarters. She spoke with its chairman, and she joins us now. And Pallavi, what did you hear? PALLAVI GOGOI, BYLINE: So today, as part of a small group of visiting U. S. journalists, I was at Huawei's beautiful leafy campus. We met with Chairman Liang Hua, who had a surprising message for American leaders who worry that Huawei technology would be used for spying by the Chinese government. In fact, I asked him whether the company would be willing to sign a no-spy agreement, which is basically like a promise that the Huawei technology would not be used for spying. And he said yes. He'd be willing to do that with any country. (SOUNDBITE OF ARCHIVED RECORDING) LIANG HUA: (Speaking Chinese). GOGOI: What he's saying is that he is skeptical that such a deal might not necessarily be possible, specifically with the U. S. And he said the U. S. has not bought anything from Huawei, is not buying anything. And that's the reason why he doesn't see an opportunity to sign such an agreement, specifically with the U. S. CORNISH: Remind us what this is all in response to. GOGOI: So a lot of things, right, Audie? In recent weeks, Huawei has come under intense attack from the U. S. government. Just last month, the U. S. added Huawei to its, quote, unquote, \"entity list,\" which bars American businesses from selling technology to Huawei without government approval. That hurts Huawei at its core. For instance, Google won't be able to supply Huawei with its Android operating system, which you use to get, you know, for Google search or to get Gmail. But really, when you think about it, all of this began last year when the U. S. and China were clashing over trade. In the midst of all that, the U. S. issued an arrest warrant for Huawei chief financial officer Meng Wanzhou on charges that Huawei's breaching American sanctions against Iran, among other allegations. Now, Meng is not just any, you know, CFO. She's also the daughter of Huawei's founder and CEO and is now under house arrest in Canada. Huawei has maintained consistently that the arrest is all about politics and stymieing Huawei's ambitions. CORNISH: Why is this company at the heart of this trade war? GOGOI: So Huawei is one of China's most successful tech companies. It is the world's second-largest smartphone maker, behind Samsung, even ahead of Apple. But more significantly, Huawei is expected to be a world leader in 5G. That's a revolutionary technology that will fuel the future of, you know, things like driverless cars, smart cities, smart traffic lights and what have you. The Trump administration believe that trade is about dominance, and China is showing - China and Huawei is showing to be ahead in this key area. CORNISH: In the meantime, what's next for this company? GOGOI: There is no doubt that if the U. S. ban is imposed, it will hurt Huawei in the short-term. However, there are some experts who say that it might strengthen Huawei's hand over the long-term. Huawei has a plan B to make its own computer chips and operating system in the coming years. Now, there was another development today that could signal an easing of tensions between the U. S. government and Huawei. President Trump was in London, and he met with Theresa May earlier today. In that press conference, he was asked specifically about the U. S. warning European countries just this week of dire consequences, you know, when it comes to sharing intelligence if those countries don't ban Huawei from their 5G networks. President Trump sounded conciliatory, actually. (SOUNDBITE OF ARCHIVED RECORDING) PRESIDENT DONALD TRUMP: No, because we're going to have, absolutely, an agreement on Huawei and everything else. We have an incredible intelligence relationship, and we will be able to work out any differences. GOGOI: But as we have learned, the devil really is in the details. We'll have to find out, you know, what such an agreement would look like. So stay tuned. CORNISH: That's NPR's Pallavi Gogoi reporting on Huawei in China. Thank you for speaking with us. GOGOI: Thank you, Audie.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-04-729510937": {"title": "Have Tech Companies Become Too Powerful? Congress Will Investigate : NPR", "url": "https://www.npr.org/2019/06/04/729510937/have-tech-companies-become-too-powerful-congress-will-investigate", "author": "No author found", "published_date": "2019-06-04", "content": "RACHEL MARTIN, HOST: How important have Facebook, Google, Apple and Amazon become to your daily life? Some in Congress think that these companies have gotten way too big and have way too much power to the point that they are snuffing out competition and actually harming consumers. This comes as the Trump administration has also suggested ramping up its antitrust oversight. Tony Romm with The Washington Post has done extensive reporting on this and is here with us in the studio. Thanks for coming in. TONY ROMM: Hey. Thanks for having me. MARTIN: So Congress is holding hearings this week on this very topic. What, realistically, is going to come from them? ROMM: Yeah. This is about the walls really closing in on big tech companies here in Washington. We've heard lots of theoretical concerns for a long time that companies like Facebook and Google and Amazon are too big and, as a result of that bigness, are misusing your data or stifling competition. But we're now beginning to see lawmakers of both political parties putting that into action. And what we had this week was an announcement from David Cicilline, the top congressman who leads the House Judiciary Committee's competition focus panel, saying that they're going to embark on this very lengthy, top-to-bottom review of big tech companies to see exactly if they're stifling competition and then to figure out at the end of the line here whether something has to be done to fix the country's antitrust laws. And so we could see a lot here. We could see public hearings. We could see the grilling of major tech executives once again. We could even see subpoenas that force these companies to turn over documents. So it could be pretty uncomfortable for tech. MARTIN: I mean, we did see the Facebook CEO, Mark Zuckerberg, testifying in front of Congress last year. I want to play a little clip of that. Let's listen. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: It overlaps with a portion of what we do. LINDSEY GRAHAM: You don't think you have a monopoly? ZUCKERBERG: It certainly doesn't feel like that to me. GRAHAM: OK. (LAUGHTER)MARTIN: So Facebook, as you point out, has long been accused of privacy breaches, of spreading disinformation. And Washington kind of just turned a blind eye. Now there is this change. Is this all because of the Russia investigation? ROMM: It feels like they're playing a bit of catch-up here in Washington. It's not all because of the Russia investigation. But I feel - and if you talk to experts, they say the same thing - there's this overwhelming feeling that perhaps Washington just wasn't paying enough attention to the ills of the tech industry, that companies like Facebook had gotten a pretty decent political ride here in Washington. But whether it's what happened with Russia and the spread of disinformation or the privacy violations we've seen at Facebook, there's now this recognition that perhaps regulation hasn't caught up with these companies. And so this sort of translates into the antitrust conversation with a company like Facebook because lawmakers say, well, if you're concerned with the things that a company like Facebook is doing, where do you turn? MARTIN: Yeah. ROMM: What's the other social network that you use if you find that Facebook is acting in some objectionable way? They think the choices are too few. MARTIN: Right. Because unlike, you know, Apple, which produces the iPhone, there are other alternatives. Something like Facebook, there really isn't something distinct that could provide an alternative for consumers. ROMM: Yeah. And Facebook bought up some of its competitors, right? So Facebook also owns WhatsApp, a messaging service. It owns Instagram, which is a photo sharing site that many folks use. And so the concern is that Washington allowed this sort of consolidation to happen and that perhaps there needs to be a second look at a company like Facebook. MARTIN: So separately, the Trump administration is taking these steps - at least, talking about them - to increase antitrust oversight. How is that going to play out? ROMM: Yeah. We're sort of seeing the reckoning translate to the Trump administration as well as the Federal Trade Commission and the Department of Justice. The two major antitrust agencies here in Washington are beginning to divvy up their territory, deciding which agency is going to look at Facebook or Google or Apple or Amazon. And so it's still early days. This is not to say that there's some immediate antitrust investigation hanging over these companies or that they're going to be broken up or something. But this starts the process that could lead to the sorts of things that change the way these companies behave. MARTIN: I mean, Donald Trump and Democratic contender for the White House Elizabeth Warren don't agree on much. But at least they're both talking about the fact that these companies have gotten too large. I mean, she's just calling out for the all-out disassembly of these companies. Facebook's co-founder, Chris Hughes, published an op-ed in The New York Times and talked to us on our show last month calling for Facebook to be broken up. Let's play that. (SOUNDBITE OF ARCHIVED BROADCAST)CHRIS HUGHES: What used to be a little startup, the story of American entrepreneurship, has become a leviathan. And most importantly, Mark Zuckerberg is unaccountable. And I think government should step up, break up the company and regulate it. MARTIN: Can you foresee that happening? ROMM: We're a long way from that. There is a rare political alignment right now between folks like Warren and President Trump who all feel for slightly different reasons that we need to take a much tougher look at these big tech companies. But a breakup of Facebook or any of its peers, for that matter, is something that wouldn't just happen overnight. I mean, remember, when the U. S. government looked at Microsoft and embarked on an antitrust investigation then, it took over a decade for that to even be resolved. So we're talking about a long process, and the conversation started really this week. MARTIN: Tony Romm covers technology policy for The Washington Post, and he was in our studios. We should add Facebook, Amazon and Google are all NPR sponsors. Tony, thank you. ROMM: Thanks for having me. RACHEL MARTIN, HOST:  How important have Facebook, Google, Apple and Amazon become to your daily life? Some in Congress think that these companies have gotten way too big and have way too much power to the point that they are snuffing out competition and actually harming consumers. This comes as the Trump administration has also suggested ramping up its antitrust oversight. Tony Romm with The Washington Post has done extensive reporting on this and is here with us in the studio. Thanks for coming in. TONY ROMM: Hey. Thanks for having me. MARTIN: So Congress is holding hearings this week on this very topic. What, realistically, is going to come from them? ROMM: Yeah. This is about the walls really closing in on big tech companies here in Washington. We've heard lots of theoretical concerns for a long time that companies like Facebook and Google and Amazon are too big and, as a result of that bigness, are misusing your data or stifling competition. But we're now beginning to see lawmakers of both political parties putting that into action. And what we had this week was an announcement from David Cicilline, the top congressman who leads the House Judiciary Committee's competition focus panel, saying that they're going to embark on this very lengthy, top-to-bottom review of big tech companies to see exactly if they're stifling competition and then to figure out at the end of the line here whether something has to be done to fix the country's antitrust laws. And so we could see a lot here. We could see public hearings. We could see the grilling of major tech executives once again. We could even see subpoenas that force these companies to turn over documents. So it could be pretty uncomfortable for tech. MARTIN: I mean, we did see the Facebook CEO, Mark Zuckerberg, testifying in front of Congress last year. I want to play a little clip of that. Let's listen. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: It overlaps with a portion of what we do. LINDSEY GRAHAM: You don't think you have a monopoly? ZUCKERBERG: It certainly doesn't feel like that to me. GRAHAM: OK. (LAUGHTER) MARTIN: So Facebook, as you point out, has long been accused of privacy breaches, of spreading disinformation. And Washington kind of just turned a blind eye. Now there is this change. Is this all because of the Russia investigation? ROMM: It feels like they're playing a bit of catch-up here in Washington. It's not all because of the Russia investigation. But I feel - and if you talk to experts, they say the same thing - there's this overwhelming feeling that perhaps Washington just wasn't paying enough attention to the ills of the tech industry, that companies like Facebook had gotten a pretty decent political ride here in Washington. But whether it's what happened with Russia and the spread of disinformation or the privacy violations we've seen at Facebook, there's now this recognition that perhaps regulation hasn't caught up with these companies. And so this sort of translates into the antitrust conversation with a company like Facebook because lawmakers say, well, if you're concerned with the things that a company like Facebook is doing, where do you turn? MARTIN: Yeah. ROMM: What's the other social network that you use if you find that Facebook is acting in some objectionable way? They think the choices are too few. MARTIN: Right. Because unlike, you know, Apple, which produces the iPhone, there are other alternatives. Something like Facebook, there really isn't something distinct that could provide an alternative for consumers. ROMM: Yeah. And Facebook bought up some of its competitors, right? So Facebook also owns WhatsApp, a messaging service. It owns Instagram, which is a photo sharing site that many folks use. And so the concern is that Washington allowed this sort of consolidation to happen and that perhaps there needs to be a second look at a company like Facebook. MARTIN: So separately, the Trump administration is taking these steps - at least, talking about them - to increase antitrust oversight. How is that going to play out? ROMM: Yeah. We're sort of seeing the reckoning translate to the Trump administration as well as the Federal Trade Commission and the Department of Justice. The two major antitrust agencies here in Washington are beginning to divvy up their territory, deciding which agency is going to look at Facebook or Google or Apple or Amazon. And so it's still early days. This is not to say that there's some immediate antitrust investigation hanging over these companies or that they're going to be broken up or something. But this starts the process that could lead to the sorts of things that change the way these companies behave. MARTIN: I mean, Donald Trump and Democratic contender for the White House Elizabeth Warren don't agree on much. But at least they're both talking about the fact that these companies have gotten too large. I mean, she's just calling out for the all-out disassembly of these companies. Facebook's co-founder, Chris Hughes, published an op-ed in The New York Times and talked to us on our show last month calling for Facebook to be broken up. Let's play that. (SOUNDBITE OF ARCHIVED BROADCAST) CHRIS HUGHES: What used to be a little startup, the story of American entrepreneurship, has become a leviathan. And most importantly, Mark Zuckerberg is unaccountable. And I think government should step up, break up the company and regulate it. MARTIN: Can you foresee that happening? ROMM: We're a long way from that. There is a rare political alignment right now between folks like Warren and President Trump who all feel for slightly different reasons that we need to take a much tougher look at these big tech companies. But a breakup of Facebook or any of its peers, for that matter, is something that wouldn't just happen overnight. I mean, remember, when the U. S. government looked at Microsoft and embarked on an antitrust investigation then, it took over a decade for that to even be resolved. So we're talking about a long process, and the conversation started really this week. MARTIN: Tony Romm covers technology policy for The Washington Post, and he was in our studios. We should add Facebook, Amazon and Google are all NPR sponsors. Tony, thank you. ROMM: Thanks for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-05-730057453": {"title": "YouTube Announces It Will Ban White Supremacist Content, Other Hateful Material : NPR", "url": "https://www.npr.org/2019/06/05/730057453/youtube-announces-it-will-ban-white-supremacist-content-other-hateful-material", "author": "No author found", "published_date": "2019-06-05", "content": "MARY LOUISE KELLY, HOST: YouTube says it is removing thousands of white supremacist and extremist videos from its platform. Now, this does not happen in a vacuum. YouTube was under intense pressure, and its move comes as a broader debate unfolds over whether social media companies are responsible for hate speech posted to their platforms. NPR's Hannah Allam is here to talk about YouTube's new policy and the wider challenge of trying to police hate online. Hi, there. HANNAH ALLAM, BYLINE: Hi. KELLY: So what exactly has YouTube announced it's going to do? ALLAM: In a company blog post today, YouTube said it was going to ban what it called supremacist content - so videos that YouTube says justify discrimination, segregation, exclusion of specific groups of people. And this goes beyond a 2017 policy YouTube introduced that made it harder to recommend or comment on or share videos with hateful content. And so this move today - it really goes beyond that to thousands of videos by white supremacists, neo-Nazis and other types of extremists. And YouTube says that's also going to apply to videos that deny well-documented violent events, so that means Holocaust deniers and deniers of the Sandy Hook Elementary shooting, for example. KELLY: All of that will be banned going forward - kind of mind-blowing to think that all of that was allowed on YouTube until now. So what prompted this? Why now? ALLAM: The context is this bigger debate going on about the responsibility of tech companies like YouTube, Facebook and Twitter to police hate speech on their platforms. There's political and public pressure to clean up those sites. Facebook and Twitter have announced similar efforts to remove hateful content. But YouTube was seen as especially resistant. KELLY: And why? What was going on specifically at YouTube? ALLAM: Well, we had one clear example this week in the experience of Carlos Maza, a journalist for Vox who'd been targeted for two years by a right-wing YouTube creator who has a following of more than 3 million subscribers. Maza was referred to by slurs about his Cuban heritage, his sexual orientation. And that creator's fans even published his cellphone number at one point. Maza reported the harassment to YouTube, which said the slurs did not violate its policies. And here's what Maza had to say about that today in an interview with BuzzFeed News. (SOUNDBITE OF ARCHIVED RECORDING)CARLOS MAZA: It should not be a policy that you're allowed to get away with harassment and hate speech on YouTube as long as you're popular enough to make them uncomfortable about shutting you down. ALLAM: There was this big social media backlash echoing Maza's outrage. And examples like his show how when tech companies fail to act on this quickly, it can really spiral into a PR problem for companies like YouTube and its parent company Google. KELLY: I'm also imagining the challenge of, how do they enforce this new policy with the - I don't know what the number is - but the zillions of new videos getting posted on YouTube every day? ALLAM: And that's the big question. YouTube was already under fire for not doing more to enforce the old measures it had, which were supposed to make it harder for people to find and profit from these kinds of videos. So the blog post didn't really go into enforcement, into much detail. And critics like Maza, the Vox journalist, have said already that they're skeptical this is going to make a difference. KELLY: NPR's Hannah Allam reporting on the new YouTube policy announced today. Thank you, Hannah. ALLAM: Thank you. MARY LOUISE KELLY, HOST:  YouTube says it is removing thousands of white supremacist and extremist videos from its platform. Now, this does not happen in a vacuum. YouTube was under intense pressure, and its move comes as a broader debate unfolds over whether social media companies are responsible for hate speech posted to their platforms. NPR's Hannah Allam is here to talk about YouTube's new policy and the wider challenge of trying to police hate online. Hi, there. HANNAH ALLAM, BYLINE: Hi. KELLY: So what exactly has YouTube announced it's going to do? ALLAM: In a company blog post today, YouTube said it was going to ban what it called supremacist content - so videos that YouTube says justify discrimination, segregation, exclusion of specific groups of people. And this goes beyond a 2017 policy YouTube introduced that made it harder to recommend or comment on or share videos with hateful content. And so this move today - it really goes beyond that to thousands of videos by white supremacists, neo-Nazis and other types of extremists. And YouTube says that's also going to apply to videos that deny well-documented violent events, so that means Holocaust deniers and deniers of the Sandy Hook Elementary shooting, for example. KELLY: All of that will be banned going forward - kind of mind-blowing to think that all of that was allowed on YouTube until now. So what prompted this? Why now? ALLAM: The context is this bigger debate going on about the responsibility of tech companies like YouTube, Facebook and Twitter to police hate speech on their platforms. There's political and public pressure to clean up those sites. Facebook and Twitter have announced similar efforts to remove hateful content. But YouTube was seen as especially resistant. KELLY: And why? What was going on specifically at YouTube? ALLAM: Well, we had one clear example this week in the experience of Carlos Maza, a journalist for Vox who'd been targeted for two years by a right-wing YouTube creator who has a following of more than 3 million subscribers. Maza was referred to by slurs about his Cuban heritage, his sexual orientation. And that creator's fans even published his cellphone number at one point. Maza reported the harassment to YouTube, which said the slurs did not violate its policies. And here's what Maza had to say about that today in an interview with BuzzFeed News. (SOUNDBITE OF ARCHIVED RECORDING) CARLOS MAZA: It should not be a policy that you're allowed to get away with harassment and hate speech on YouTube as long as you're popular enough to make them uncomfortable about shutting you down. ALLAM: There was this big social media backlash echoing Maza's outrage. And examples like his show how when tech companies fail to act on this quickly, it can really spiral into a PR problem for companies like YouTube and its parent company Google. KELLY: I'm also imagining the challenge of, how do they enforce this new policy with the - I don't know what the number is - but the zillions of new videos getting posted on YouTube every day? ALLAM: And that's the big question. YouTube was already under fire for not doing more to enforce the old measures it had, which were supposed to make it harder for people to find and profit from these kinds of videos. So the blog post didn't really go into enforcement, into much detail. And critics like Maza, the Vox journalist, have said already that they're skeptical this is going to make a difference. KELLY: NPR's Hannah Allam reporting on the new YouTube policy announced today. Thank you, Hannah. ALLAM: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-05-729920147": {"title": "Federal Government To Check North Carolina Election Equipment Over Hacking Fears : NPR", "url": "https://www.npr.org/2019/06/05/729920147/federal-government-to-inspect-north-carolina-election-equipment-over-hacking-fea", "author": "No author found", "published_date": "2019-06-05", "content": "", "section": "Politics", "disclaimer": ""}, "2019-06-06-730408514": {"title": "Tetris, Addictive Video Game, Turns 35 : NPR", "url": "https://www.npr.org/2019/06/06/730408514/happy-birthday-tetris-35-years-later-youre-as-addictive-and-tetromino-y-as-ever", "author": "No author found", "published_date": "2019-06-06", "content": "MARY LOUISE KELLY, HOST:  If you had happened to peek into my college dorm room freshman year early '90s, you would have witnessed my roommates and me glued to our computer screens completely obsessed not with overdue term papers and problem sets but with a game called Tetris. Turns out we were not alone. Today Tetris is recognized as one of the most popular video games of all time. Thirty-five years ago, it was a side project for a Russian software developer working in Moscow. That software developer was Alexey Pajitnov, and he is here now. Welcome. ALEXEY PAJITNOV: Hello. KELLY: Hello. I suppose I should start by saying happy anniversary. Tetris is celebrating its 35th birthday today. PAJITNOV: Could you believe it? It was like yesterday (laughter). KELLY: Well, I want to hear the story of how you came up with it, but I guess I should start by explaining for people who, unlike me and my college roommates, were not completely addicted to it at some point in their life that Tetris is - it's a very simple game. There's no, you know, asteroids to dodge or aliens chasing you. It's blocks that fall from the top of the screen, and you manipulate them, spin them around to create lines. And if you do it fast enough, then you clear your screen. And if you don't, then game over. Have I basically explained that accurately? PAJITNOV: More or less yes. KELLY: Where were you when you came up with this idea? PAJITNOV: I was in Moscow. I was young developer, young programmer. It was a very fascinating time for me because I got something which looks like a personal computer now. KELLY: Yeah. What kind of computer were you working on? PAJITNOV: It was kind of ugly Russian clone called Electronika 60. So my friends just made it up out of spare parts or something. KELLY: (Laughter) OK. PAJITNOV: The prototype of Tetris was the board game called Pentomino. KELLY: Pentomino - go on. PAJITNOV: Yeah. I love those puzzle all my life. And once I decide to put on computer some to play our game based on it. And when I start to program it, the idea of real-time game with those pieces came to my mind, and that's how Tetris was born. KELLY: As I understand it - course you came up with this game. This is 1980s in the Soviet Union. You eventually - you lost the rights for a while and then eventually regained the rights after the Cold War ended. Is that right? PAJITNOV: Well, I granted my rights for 10 years to the Soviets, to my job place. That's the only way the foreign agreement could be done. KELLY: I should mention you in the intervening years have left the Soviet Union you live now in the United States. Do you own the rights to Tetris now? Or who does? PAJITNOV: Yes, I do. It was my partner. Finally in '95, '96, the original right came back to me, and we maintain Tetris brand since. KELLY: What do you think explains the ongoing popularity of this game given that now people can play games with incredibly complicated, you know, computer-generated graphics and yet people still love just spinning blocks around? PAJITNOV: The software and hardware are changing dramatically, but our brains do not. So we still love what we used to love many, many centuries ago. KELLY: We still like playing with blocks as little kids, I suppose. So why not do it on the screen? PAJITNOV: So Tetris is very attractive because it has a constructed spirit. You feel that you create something rather than destroy, you know? KELLY: Yeah. PAJITNOV: Yeah. KELLY: Do you still play? PAJITNOV: Oh, yes, I do. KELLY: How many hours would you wager you have spent on Tetris? PAJITNOV: Not that much. KELLY: No? PAJITNOV: I have to catch up with the other games as well. (LAUGHTER)KELLY: Alexey Pajitnov, thank you so much. PAJITNOV: Thank you. Have a nice day. KELLY: He joined us via Skype. He is the creator of Tetris, which, as you heard, turns 35 years old today. (SOUNDBITE OF SONG, \"KOROBEINIKI\") MARY LOUISE KELLY, HOST:   If you had happened to peek into my college dorm room freshman year early '90s, you would have witnessed my roommates and me glued to our computer screens completely obsessed not with overdue term papers and problem sets but with a game called Tetris. Turns out we were not alone. Today Tetris is recognized as one of the most popular video games of all time. Thirty-five years ago, it was a side project for a Russian software developer working in Moscow. That software developer was Alexey Pajitnov, and he is here now. Welcome. ALEXEY PAJITNOV: Hello. KELLY: Hello. I suppose I should start by saying happy anniversary. Tetris is celebrating its 35th birthday today. PAJITNOV: Could you believe it? It was like yesterday (laughter). KELLY: Well, I want to hear the story of how you came up with it, but I guess I should start by explaining for people who, unlike me and my college roommates, were not completely addicted to it at some point in their life that Tetris is - it's a very simple game. There's no, you know, asteroids to dodge or aliens chasing you. It's blocks that fall from the top of the screen, and you manipulate them, spin them around to create lines. And if you do it fast enough, then you clear your screen. And if you don't, then game over. Have I basically explained that accurately? PAJITNOV: More or less yes. KELLY: Where were you when you came up with this idea? PAJITNOV: I was in Moscow. I was young developer, young programmer. It was a very fascinating time for me because I got something which looks like a personal computer now. KELLY: Yeah. What kind of computer were you working on? PAJITNOV: It was kind of ugly Russian clone called Electronika 60. So my friends just made it up out of spare parts or something. KELLY: (Laughter) OK. PAJITNOV: The prototype of Tetris was the board game called Pentomino. KELLY: Pentomino - go on. PAJITNOV: Yeah. I love those puzzle all my life. And once I decide to put on computer some to play our game based on it. And when I start to program it, the idea of real-time game with those pieces came to my mind, and that's how Tetris was born. KELLY: As I understand it - course you came up with this game. This is 1980s in the Soviet Union. You eventually - you lost the rights for a while and then eventually regained the rights after the Cold War ended. Is that right? PAJITNOV: Well, I granted my rights for 10 years to the Soviets, to my job place. That's the only way the foreign agreement could be done. KELLY: I should mention you in the intervening years have left the Soviet Union you live now in the United States. Do you own the rights to Tetris now? Or who does? PAJITNOV: Yes, I do. It was my partner. Finally in '95, '96, the original right came back to me, and we maintain Tetris brand since. KELLY: What do you think explains the ongoing popularity of this game given that now people can play games with incredibly complicated, you know, computer-generated graphics and yet people still love just spinning blocks around? PAJITNOV: The software and hardware are changing dramatically, but our brains do not. So we still love what we used to love many, many centuries ago. KELLY: We still like playing with blocks as little kids, I suppose. So why not do it on the screen? PAJITNOV: So Tetris is very attractive because it has a constructed spirit. You feel that you create something rather than destroy, you know? KELLY: Yeah. PAJITNOV: Yeah. KELLY: Do you still play? PAJITNOV: Oh, yes, I do. KELLY: How many hours would you wager you have spent on Tetris? PAJITNOV: Not that much. KELLY: No? PAJITNOV: I have to catch up with the other games as well. (LAUGHTER) KELLY: Alexey Pajitnov, thank you so much. PAJITNOV: Thank you. Have a nice day. KELLY: He joined us via Skype. He is the creator of Tetris, which, as you heard, turns 35 years old today. (SOUNDBITE OF SONG, \"KOROBEINIKI\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-06-730415950": {"title": "FCC Encourages Companies To Block Robocalls By Default : NPR", "url": "https://www.npr.org/2019/06/06/730415950/heres-why-you-may-start-receiving-fewer-robocalls", "author": "No author found", "published_date": "2019-06-06", "content": "", "section": "Business", "disclaimer": ""}, "2019-06-06-727711432": {"title": "Never-Ending Phone Spam Is Turning Off Consumers : NPR", "url": "https://www.npr.org/2019/06/06/727711432/do-i-know-you-and-other-spam-phone-calls-we-can-t-get-rid-of", "author": "No author found", "published_date": "2019-06-06", "content": "NOEL KING, HOST: All right. Cellphones have made our lives easier for sure, but lately, when you pick up the phone, you are likely to hear something like this. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: This message is concerning your unsecured credit debt. KING: These spam calls make up at least a quarter of all phone calls in the United States, but there is hope. The FCC is set to vote today on rules clarifying that phone companies can step in to block these unwanted calls. NPR's Yuki Noguchi has that story. YUKI NOGUCHI, BYLINE: By now, this is a familiar drill. The phone rings. It's not a familiar number. Is it important or yet another spam call? The problem of phone spam is so pervasive it's creating related nuisances for people like Dakota Hill. He says he gets 100 junk calls every month and also gets calls from people who think he's spamming them. They call saying. . . DAKOTA HILL: Do I know you? NOGUCHI: Or. . . HILL: Why did you wake me up? And that was definitely an angry one. NOGUCHI: In fact, Hill hadn't placed any of those calls. His number had been spoofed. That is, fraudsters used software to trick the caller ID system to make it appear as though calls were coming from Hill's phone. He explains this over and over to the people calling him, and not every caller is understanding. One woman chastised him. HILL: She went on and on about how I was letting people use my phone and not controlling them (laughter). NOGUCHI: There is an irony here. The cellphone has become our everything - our wallet, photo archives, computer and music library. But it's also becoming less useful as a phone. Consumer Reports found 70% of people no longer answer calls they don't recognize. Regulators and industry are combating junk calls but at least so far haven't succeeded. In fact, the Federal Communications Commission, which regulates phone companies, had its own spam problem. Patrick Webre heads the agency's consumer bureau. PATRICK WEBRE: We've seen recently scammers using our number, spoofing our number, to try to convince consumers that they're from the FCC and in some way get money out of them. NOGUCHI: He says spam calls are the No. 1 consumer complaint and the agency's top priority. The FCC is demanding all U. S. phone carriers install technology to verify calls and flag potential spam. The deadline is the end of this year. Jonathan Nelson is on the front lines of this battle. Nelson is director of product management at Hiya, a Seattle technology startup that's designing ways to block spam. He tracks phone calls across the U. S. on giant computer monitors. JONATHAN NELSON: You see this huge, vast area of green, which is good, all the good calls. But then there's this little red area that just bounces along, you know. It's the scammers. NOGUCHI: Nelson says they devise clever, new ways of bilking people - the latest being the one-ring scam, which emerged May 3. That day, Nelson's monitors turned a flurry of red. NELSON: It was explosion of calls. We'd never seen that level of volume before. NOGUCHI: In this case, robo callers hang up after one ring, hoping to trick the victim into calling back on an expensive, international toll line, likely to West Africa. Scammers profit by taking a portion of the added fees. Many scams prey on fear of arrest or investigation by a government agency. Targets include immigrants, taxpayers, debtors or retirees. Scams cost Americans an estimated $10 billion a year. Their success, Nelson says, is making people skeptical about answering calls. NELSON: We're kind of seeing the death of the phone call. NOGUCHI: Most cellphone carriers recognize they need to step up. Chris Oatway is associate general counsel for Verizon Wireless. He says this year, the company's investing more than ever in technologies to detect, identify and trace junk calls. CHRIS OATWAY: There is an arms race where they are looking to evolve to get around some of the protections we have in place. NOGUCHI: I would say that the carriers are not winning that arms race. OATWAY: I think that's true. The key here is to restore trust in voice calls. NOGUCHI: Doing so, Oatway says, won't be easy because telephone networks are so interconnected. If another wireless carrier doesn't flag a spam call, Verizon's network might not recognize it's a problem. That's just one way he says spammers might still get through. Yuki Noguchi, NPR News, Washington. NOEL KING, HOST:  All right. Cellphones have made our lives easier for sure, but lately, when you pick up the phone, you are likely to hear something like this. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: This message is concerning your unsecured credit debt. KING: These spam calls make up at least a quarter of all phone calls in the United States, but there is hope. The FCC is set to vote today on rules clarifying that phone companies can step in to block these unwanted calls. NPR's Yuki Noguchi has that story. YUKI NOGUCHI, BYLINE: By now, this is a familiar drill. The phone rings. It's not a familiar number. Is it important or yet another spam call? The problem of phone spam is so pervasive it's creating related nuisances for people like Dakota Hill. He says he gets 100 junk calls every month and also gets calls from people who think he's spamming them. They call saying. . . DAKOTA HILL: Do I know you? NOGUCHI: Or. . . HILL: Why did you wake me up? And that was definitely an angry one. NOGUCHI: In fact, Hill hadn't placed any of those calls. His number had been spoofed. That is, fraudsters used software to trick the caller ID system to make it appear as though calls were coming from Hill's phone. He explains this over and over to the people calling him, and not every caller is understanding. One woman chastised him. HILL: She went on and on about how I was letting people use my phone and not controlling them (laughter). NOGUCHI: There is an irony here. The cellphone has become our everything - our wallet, photo archives, computer and music library. But it's also becoming less useful as a phone. Consumer Reports found 70% of people no longer answer calls they don't recognize. Regulators and industry are combating junk calls but at least so far haven't succeeded. In fact, the Federal Communications Commission, which regulates phone companies, had its own spam problem. Patrick Webre heads the agency's consumer bureau. PATRICK WEBRE: We've seen recently scammers using our number, spoofing our number, to try to convince consumers that they're from the FCC and in some way get money out of them. NOGUCHI: He says spam calls are the No. 1 consumer complaint and the agency's top priority. The FCC is demanding all U. S. phone carriers install technology to verify calls and flag potential spam. The deadline is the end of this year. Jonathan Nelson is on the front lines of this battle. Nelson is director of product management at Hiya, a Seattle technology startup that's designing ways to block spam. He tracks phone calls across the U. S. on giant computer monitors. JONATHAN NELSON: You see this huge, vast area of green, which is good, all the good calls. But then there's this little red area that just bounces along, you know. It's the scammers. NOGUCHI: Nelson says they devise clever, new ways of bilking people - the latest being the one-ring scam, which emerged May 3. That day, Nelson's monitors turned a flurry of red. NELSON: It was explosion of calls. We'd never seen that level of volume before. NOGUCHI: In this case, robo callers hang up after one ring, hoping to trick the victim into calling back on an expensive, international toll line, likely to West Africa. Scammers profit by taking a portion of the added fees. Many scams prey on fear of arrest or investigation by a government agency. Targets include immigrants, taxpayers, debtors or retirees. Scams cost Americans an estimated $10 billion a year. Their success, Nelson says, is making people skeptical about answering calls. NELSON: We're kind of seeing the death of the phone call. NOGUCHI: Most cellphone carriers recognize they need to step up. Chris Oatway is associate general counsel for Verizon Wireless. He says this year, the company's investing more than ever in technologies to detect, identify and trace junk calls. CHRIS OATWAY: There is an arms race where they are looking to evolve to get around some of the protections we have in place. NOGUCHI: I would say that the carriers are not winning that arms race. OATWAY: I think that's true. The key here is to restore trust in voice calls. NOGUCHI: Doing so, Oatway says, won't be easy because telephone networks are so interconnected. If another wireless carrier doesn't flag a spam call, Verizon's network might not recognize it's a problem. That's just one way he says spammers might still get through. Yuki Noguchi, NPR News, Washington.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-06-717486969": {"title": "Video: How Transcranial Direct Current Stimulation (TDCS) Might Help Humans : NPR", "url": "https://www.npr.org/2019/06/06/717486969/higher-better-stronger-faster-brain-science-is-trying-to-get-there", "author": "No author found", "published_date": "2019-06-06", "content": "RACHEL MARTIN, HOST: If you want to get good at something, whether it's piano or golf, we know what works - practice. Do something over and over and over, and your brain eventually masters that activity. But what if your brain could be so ready to learn that you didn't have to practice something so many times to master it? Some top athletes are training with headsets that are supposed to stimulate their brain cells, make their brains more ready to learn so practice is more effective. NPR's Elise Hu has been trying it out as part of our video series on how emerging technologies may shape our future. It is called Future You With Elise Hu. And she spoke with Steve. STEVE INSKEEP, BYLINE: Hi there. ELISE HU, BYLINE: Hey. Good morning. INSKEEP: OK, so I watched your video, and you've got this thing on. And it looks like one of those old-timey, like, 1990s big, old stereo headsets. What is it really? HU: That's right. It's a headset that looks like headphones like Beats by Dre, but it's actually a brain-stimulating device that can zap you with tiny currents, so you don't feel anything. The electrical charge is super low. . . INSKEEP: But you're getting shocked. HU: Yes (laughter). You are supposed to wear it for about 20 minutes before you practice this golf swing or squats or lunges, any repetitive activity that you could get better at over time. And what it's doing is it's priming your brain before you do that activity. So to try it out here at NPR West, I decided to learn something I've never done before - vertical jump. OK, so then. . . UNIDENTIFIED PERSON #1: Get comfortable and then just jump straight up. Boom, there it is. NATE ROTT, BYLINE: Nice. HU: OK. UNIDENTIFIED PERSON #1: So you're looking at 14 inches on the dot. ROTT: Wow. HU: OK, 14 inches. ROTT: That's more than a foot. HU: So I needed to actually train on jumping over four to six weeks while wearing this headset. And then NPR reporter Nate Rott did not get a headset. And we trained together. (SOUNDBITE OF ARCHIVED RECORDING)HU: Oh, that was much longer than mine. Jeez. UNIDENTIFIED PERSON #1: Seventeen inches. ROTT: Got three inches on you. HU: Oh, my gosh. So he's already. . . So this isn't. . . INSKEEP: He's the control group, like the placebo. OK, fine. HU: This isn't exactly a perfect science experiment, but we wanted to try this to see if I could improve better than he did. So for another month after this, I used this headset before working out. Nate then worked out with me, but he didn't get a headset. INSKEEP: OK, so what's the big thing you're trying to get at by testing your vertical jump? HU: So the entire video series Future You is about exploring where new technologies are going to take us by 2050. And in this case, what we're looking at is electrical stimulation for your brain. The technical term is called transcranial direct-current stimulation. We are still waiting for more of a body of clinical data on this. We do know that one DARPA-funded study showed as much as a 40% improvement in performance in primates. INSKEEP: DARPA - that's the Pentagon science guys. HU: That's right. They are testing this on the military. The one I used actually does have a contract with the DoD. It's from a company called Halo. And I asked the founder of the company where he thinks this might lead us a few decades into the future. He's a Stanford trained MD named Dr. Dan Chao. DAN CHAO: Like, could we be more attentive and focused as a society? You know, advance ourselves, advance humanity - that's a good thing. HU: So you're expecting kind of a super cognition - right? - like advanced memory capabilities, advanced focus capabilities. CHAO: Advanced focus and - like, I would use it when I needed to get meaningful work done. If I could have had a neuro stimulator to help me with focus, I would've been a better student. HU: If humans can be upgraded this much, then are we creating different classes of humans? Or even with athletes, there's asterisks by those who used steroids. CHAO: There's legal performance enhancement that's all around us, like drinking coffee. It's been identified by the World Anti-Doping Association as performance enhancing, yet it's legal. What WADA considers illegal is things that are unsafe to the health of the athlete and against the spirit of the sport. I would argue it isn't. INSKEEP: OK, it's said for the moment it's cool for athletes to use this as they train, which brings us back to your vertical jump, Elise. How'd you do? HU: So not scientific but after about a month of training with the headset. . . (Laughter). . . . I was able to increase my vertical jump by 11%. INSKEEP: Is that good? I don't know. HU: Well, it was pretty surprising, even for me. UNIDENTIFIED PERSON #1: You got some ups. ROTT: Serious ups. INSKEEP: How'd Nate do? - the control group. HU: Nate Rott only increased his vertical jump by 8%. So I like to think I won. INSKEEP: Well, Elise, thanks very much for the insights - really appreciate it. HU: You're welcome. INSKEEP: Now all of this is on video. You can see it for yourself. The series is called Future You With Elise Hu. It rhymes. HU: It rhymes. INSKEEP: And you can find it at npr. org/futureyou. (SOUNDBITE OF SKINNY WILLIAMS AND STEPHEN GOODSON'S \"POP STAR EXPLOSION\") RACHEL MARTIN, HOST:  If you want to get good at something, whether it's piano or golf, we know what works - practice. Do something over and over and over, and your brain eventually masters that activity. But what if your brain could be so ready to learn that you didn't have to practice something so many times to master it? Some top athletes are training with headsets that are supposed to stimulate their brain cells, make their brains more ready to learn so practice is more effective. NPR's Elise Hu has been trying it out as part of our video series on how emerging technologies may shape our future. It is called Future You With Elise Hu. And she spoke with Steve. STEVE INSKEEP, BYLINE: Hi there. ELISE HU, BYLINE: Hey. Good morning. INSKEEP: OK, so I watched your video, and you've got this thing on. And it looks like one of those old-timey, like, 1990s big, old stereo headsets. What is it really? HU: That's right. It's a headset that looks like headphones like Beats by Dre, but it's actually a brain-stimulating device that can zap you with tiny currents, so you don't feel anything. The electrical charge is super low. . . INSKEEP: But you're getting shocked. HU: Yes (laughter). You are supposed to wear it for about 20 minutes before you practice this golf swing or squats or lunges, any repetitive activity that you could get better at over time. And what it's doing is it's priming your brain before you do that activity. So to try it out here at NPR West, I decided to learn something I've never done before - vertical jump. OK, so then. . . UNIDENTIFIED PERSON #1: Get comfortable and then just jump straight up. Boom, there it is. NATE ROTT, BYLINE: Nice. HU: OK. UNIDENTIFIED PERSON #1: So you're looking at 14 inches on the dot. ROTT: Wow. HU: OK, 14 inches. ROTT: That's more than a foot. HU: So I needed to actually train on jumping over four to six weeks while wearing this headset. And then NPR reporter Nate Rott did not get a headset. And we trained together. (SOUNDBITE OF ARCHIVED RECORDING) HU: Oh, that was much longer than mine. Jeez. UNIDENTIFIED PERSON #1: Seventeen inches. ROTT: Got three inches on you. HU: Oh, my gosh. So he's already. . . So this isn't. . . INSKEEP: He's the control group, like the placebo. OK, fine. HU: This isn't exactly a perfect science experiment, but we wanted to try this to see if I could improve better than he did. So for another month after this, I used this headset before working out. Nate then worked out with me, but he didn't get a headset. INSKEEP: OK, so what's the big thing you're trying to get at by testing your vertical jump? HU: So the entire video series Future You is about exploring where new technologies are going to take us by 2050. And in this case, what we're looking at is electrical stimulation for your brain. The technical term is called transcranial direct-current stimulation. We are still waiting for more of a body of clinical data on this. We do know that one DARPA-funded study showed as much as a 40% improvement in performance in primates. INSKEEP: DARPA - that's the Pentagon science guys. HU: That's right. They are testing this on the military. The one I used actually does have a contract with the DoD. It's from a company called Halo. And I asked the founder of the company where he thinks this might lead us a few decades into the future. He's a Stanford trained MD named Dr. Dan Chao. DAN CHAO: Like, could we be more attentive and focused as a society? You know, advance ourselves, advance humanity - that's a good thing. HU: So you're expecting kind of a super cognition - right? - like advanced memory capabilities, advanced focus capabilities. CHAO: Advanced focus and - like, I would use it when I needed to get meaningful work done. If I could have had a neuro stimulator to help me with focus, I would've been a better student. HU: If humans can be upgraded this much, then are we creating different classes of humans? Or even with athletes, there's asterisks by those who used steroids. CHAO: There's legal performance enhancement that's all around us, like drinking coffee. It's been identified by the World Anti-Doping Association as performance enhancing, yet it's legal. What WADA considers illegal is things that are unsafe to the health of the athlete and against the spirit of the sport. I would argue it isn't. INSKEEP: OK, it's said for the moment it's cool for athletes to use this as they train, which brings us back to your vertical jump, Elise. How'd you do? HU: So not scientific but after about a month of training with the headset. . . (Laughter). . . . I was able to increase my vertical jump by 11%. INSKEEP: Is that good? I don't know. HU: Well, it was pretty surprising, even for me. UNIDENTIFIED PERSON #1: You got some ups. ROTT: Serious ups. INSKEEP: How'd Nate do? - the control group. HU: Nate Rott only increased his vertical jump by 8%. So I like to think I won. INSKEEP: Well, Elise, thanks very much for the insights - really appreciate it. HU: You're welcome. INSKEEP: Now all of this is on video. You can see it for yourself. The series is called Future You With Elise Hu. It rhymes. HU: It rhymes. INSKEEP: And you can find it at npr. org/futureyou. (SOUNDBITE OF SKINNY WILLIAMS AND STEPHEN GOODSON'S \"POP STAR EXPLOSION\")", "section": "Video: Future You, With Elise Hu", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-07-730536125": {"title": "Google Talking To U.S. On Huawei's Behalf, Official Of Chinese Firm Says : NPR", "url": "https://www.npr.org/2019/06/07/730536125/as-google-advances-its-interests-it-serves-as-huawei-emissary-to-u-s", "author": "No author found", "published_date": "2019-06-07", "content": "NOEL KING, HOST: Turning to some news here in the U. S. on Huawei. Google has been quietly assuming the role of emissary for Huawei after the Chinese telecom company was blacklisted by the Trump administration. Google is, in effect, negotiating with the Commerce Department on Huawei's behalf. That's according to a senior official at Huawei. NPR's Aarti Shahani has that story. AARTI SHAHANI, BYLINE: The fates of Huawei and Google are intertwined. Huawei is a leader in creating next-generation wireless networks, and it's the world's No. 2 maker of smartphones. Google provides support for Android, the popular operating system. The U. S. government ban against Huawei also blocks Google from giving security updates to millions of existing Huawei phones and from issuing Android licenses in the future. In an interview this week with Huawei chairman Liang Hua, NPR asked him how his company would resolve the problem of losing access to Google software. LIANG HUA: (Through interpreter) Google is a very responsible company. We have maintained a very good cooperation with each other. SHAHANI: He's speaking through a translator at Huawei headquarters in Shenzhen. When the ban first went into effect and Google announced it would cut ties, there was an outcry. Days later, the Trump administration said it would postpone parts of the ban until August. The outsized power of American tech giants is well-understood the world over. Huawei's Liang is now leaning on Google to influence the Department of Commerce on his company's behalf. HUA: (Through interpreter) We really look forward to productive results from the communication that Google is currently having with the Commerce Department. SHAHANI: Last month, citing national security concerns, the Trump administration added Huawei to a list of banned entities. American companies, from mobile providers to chipmakers like Intel and Qualcomm, will not be allowed to do business with Huawei. That's because, according to U. S. officials, the company's technology could be used for surveillance. If a resolution isn't reached, Liang says, Huawei will have to build its own software, which would be difficult. HUA: (Through interpreter) So we really hope that there were possible remedies coming out of the communication between Google and the Commerce Department. And we think that it is in the benefit of the consumers if they could work out a solution. SHAHANI: Liang says he does not know the details of the talks. In an email, a Google spokesperson says the company is engaging with the Department of Commerce to ensure Google is in full compliance with the new rules. The company declined to say if its talks with the government had included directly or indirectly advocating for the ability to support future Huawei devices. NPR interviewed several former senior officials at Commerce and the White House who are concerned that a private company, governed by its own self-interest, is advocating at this juncture, where the foreign partner has been officially blacklisted for security concerns. Eric Hirschhorn says turning Huawei into a bargaining chip in the U. S. -China trade war was a strategic mistake. ERIC HIRSCHHORN: I spent a lot of time trying to make sure that national security and trade were kept separate. SHAHANI: Hirschhorn served in the Department of Commerce during the Obama administration as under secretary for Industry and Security. According to former Commerce officials, it's standard for companies to reach out to the department about their ability to do business abroad. But the foreign partner is often at the table too, able to talk and be questioned. Hirschhorn says that changes once the U. S. government decides to take enforcement action, as it did last month when it banned Huawei. Company financials should not be considered alongside national security decisions, he says. If Huawei gets what it wants vis a vis Google's effort, he says, that sends a very, very bad message to people who break American rules. HIRSCHHORN: If I know that my government or my powerful business partner can basically fix a ticket if I get one, I won't worry about speeding. SHAHANI: The Commerce Department says it routinely responds to inquiries from companies about regulatory requirements. This is not new to this administration, nor do these discussions influence law enforcement actions. Aarti Shahani, NPR News. (SOUNDBITE OF SOULAR ORDER'S \"KEYFRAMES\")KING: And we need to note that Google is a financial sponsor of NPR. NOEL KING, HOST:  Turning to some news here in the U. S. on Huawei. Google has been quietly assuming the role of emissary for Huawei after the Chinese telecom company was blacklisted by the Trump administration. Google is, in effect, negotiating with the Commerce Department on Huawei's behalf. That's according to a senior official at Huawei. NPR's Aarti Shahani has that story. AARTI SHAHANI, BYLINE: The fates of Huawei and Google are intertwined. Huawei is a leader in creating next-generation wireless networks, and it's the world's No. 2 maker of smartphones. Google provides support for Android, the popular operating system. The U. S. government ban against Huawei also blocks Google from giving security updates to millions of existing Huawei phones and from issuing Android licenses in the future. In an interview this week with Huawei chairman Liang Hua, NPR asked him how his company would resolve the problem of losing access to Google software. LIANG HUA: (Through interpreter) Google is a very responsible company. We have maintained a very good cooperation with each other. SHAHANI: He's speaking through a translator at Huawei headquarters in Shenzhen. When the ban first went into effect and Google announced it would cut ties, there was an outcry. Days later, the Trump administration said it would postpone parts of the ban until August. The outsized power of American tech giants is well-understood the world over. Huawei's Liang is now leaning on Google to influence the Department of Commerce on his company's behalf. HUA: (Through interpreter) We really look forward to productive results from the communication that Google is currently having with the Commerce Department. SHAHANI: Last month, citing national security concerns, the Trump administration added Huawei to a list of banned entities. American companies, from mobile providers to chipmakers like Intel and Qualcomm, will not be allowed to do business with Huawei. That's because, according to U. S. officials, the company's technology could be used for surveillance. If a resolution isn't reached, Liang says, Huawei will have to build its own software, which would be difficult. HUA: (Through interpreter) So we really hope that there were possible remedies coming out of the communication between Google and the Commerce Department. And we think that it is in the benefit of the consumers if they could work out a solution. SHAHANI: Liang says he does not know the details of the talks. In an email, a Google spokesperson says the company is engaging with the Department of Commerce to ensure Google is in full compliance with the new rules. The company declined to say if its talks with the government had included directly or indirectly advocating for the ability to support future Huawei devices. NPR interviewed several former senior officials at Commerce and the White House who are concerned that a private company, governed by its own self-interest, is advocating at this juncture, where the foreign partner has been officially blacklisted for security concerns. Eric Hirschhorn says turning Huawei into a bargaining chip in the U. S. -China trade war was a strategic mistake. ERIC HIRSCHHORN: I spent a lot of time trying to make sure that national security and trade were kept separate. SHAHANI: Hirschhorn served in the Department of Commerce during the Obama administration as under secretary for Industry and Security. According to former Commerce officials, it's standard for companies to reach out to the department about their ability to do business abroad. But the foreign partner is often at the table too, able to talk and be questioned. Hirschhorn says that changes once the U. S. government decides to take enforcement action, as it did last month when it banned Huawei. Company financials should not be considered alongside national security decisions, he says. If Huawei gets what it wants vis a vis Google's effort, he says, that sends a very, very bad message to people who break American rules. HIRSCHHORN: If I know that my government or my powerful business partner can basically fix a ticket if I get one, I won't worry about speeding. SHAHANI: The Commerce Department says it routinely responds to inquiries from companies about regulatory requirements. This is not new to this administration, nor do these discussions influence law enforcement actions. Aarti Shahani, NPR News. (SOUNDBITE OF SOULAR ORDER'S \"KEYFRAMES\") KING: And we need to note that Google is a financial sponsor of NPR.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-08-730608664": {"title": "Amid Debate Over Carlos Maza, Steven Crowder, Some Ask: Is YouTube Doing Enough? : NPR", "url": "https://www.npr.org/2019/06/08/730608664/is-youtube-doing-enough-to-stop-harassment-of-lgbtq-content-creators", "author": "No author found", "published_date": "2019-06-08", "content": "", "section": "Technology", "disclaimer": ""}, "2019-06-08-730898415": {"title": "Understanding AirDrop 'Crossfire' : NPR", "url": "https://www.npr.org/2019/06/08/730898415/understanding-airdrop-crossfire", "author": "No author found", "published_date": "2019-06-08", "content": "SCOTT SIMON, HOST: Maybe you've been a victim. Riding the subway, passing a passel of teenagers on the street - suddenly, your phone flashes a request for you to accept wacky emojis and photos. Welcome to AirDrop crossfire. Taylor Lorenz writes about this sign of the times in The Atlantic. She joins us on the line. Thanks so much for being with us. TAYLOR LORENZ: Yeah. Thank you so much for having me. SIMON: I have the scholarly advantage of living with two teenagers. For those who don't know what AirDrop crossfire is, what is it? LORENZ: Sure. So AirDrop is a feature on iPhones where you can easily send images, photos and different types of media via Bluetooth. So you don't have to be on Wi-Fi. And you don't have to have the person in your contacts. So it's a popular way for kids to kind of just blast out memes and joke pictures to people around them. SIMON: Yeah - whether they know them or not. LORENZ: Exactly. Yeah. I mean, the fun is doing it with people you don't know. So if you're, you know, in a crowded subway, waiting for a concert to begin, at a school assembly, just start kind of blasting out funny memes as a joke. SIMON: Yeah. So you mean they're not blasting out articles from The Atlantic or summaries of the news from NPR or. . . LORENZ: I would love that if they were. It's usually more SpongeBob memes and just sort of silly images they find on the Internet, things like that. SIMON: I gather it can also be used for more ugly things, too - right? - bullying and cheating. LORENZ: Yeah, definitely. So I spoke to a couple teachers and school administrators at different districts. And, you know, AirDrop cheating is an issue. A lot of schools have Wi-Fi off. People have to put their phones away. But kids will snap a picture of the test and then AirDrop it to classmates who have the class later in the day. It can also be used for bullying. You can AirDrop a really unflattering picture or, you know, illicit photos of other people to tons of people in the school really easily. So it can be really used for more negative purposes, too, in that kind of environment. SIMON: Is AirDropping becoming what amounts to its own social network that doesn't rely on major platforms? LORENZ: Sort of. It's a way to get around these platforms. I mean, on all social platforms and text messaging, you have to have people's numbers in your phone. There's - AirDrop is sort of like this pop-up, location-based, anonymous message board where everyone can just, you know, spam out different things to the people around them. And, you know, if you want to leave, you can just turn your AirDrop off or walk away. So yeah. It's a little bit more fluid than actual, structured social networks. SIMON: If somebody doesn't want to be part of the AirDrop experience, what can they do? LORENZ: They can just turn their settings on AirDrop to contacts only or receiving off. It's pretty easy if you just search AirDrop in your settings. SIMON: Taylor Lorenz - her article in The Atlantic is called \"When Grown-Ups Get Caught In Teens' AirDrop Crossfire. \" Thanks so much for coming in and out of the crossfire and speaking with us. LORENZ: Yeah. Thank you so much for having me. SCOTT SIMON, HOST:  Maybe you've been a victim. Riding the subway, passing a passel of teenagers on the street - suddenly, your phone flashes a request for you to accept wacky emojis and photos. Welcome to AirDrop crossfire. Taylor Lorenz writes about this sign of the times in The Atlantic. She joins us on the line. Thanks so much for being with us. TAYLOR LORENZ: Yeah. Thank you so much for having me. SIMON: I have the scholarly advantage of living with two teenagers. For those who don't know what AirDrop crossfire is, what is it? LORENZ: Sure. So AirDrop is a feature on iPhones where you can easily send images, photos and different types of media via Bluetooth. So you don't have to be on Wi-Fi. And you don't have to have the person in your contacts. So it's a popular way for kids to kind of just blast out memes and joke pictures to people around them. SIMON: Yeah - whether they know them or not. LORENZ: Exactly. Yeah. I mean, the fun is doing it with people you don't know. So if you're, you know, in a crowded subway, waiting for a concert to begin, at a school assembly, just start kind of blasting out funny memes as a joke. SIMON: Yeah. So you mean they're not blasting out articles from The Atlantic or summaries of the news from NPR or. . . LORENZ: I would love that if they were. It's usually more SpongeBob memes and just sort of silly images they find on the Internet, things like that. SIMON: I gather it can also be used for more ugly things, too - right? - bullying and cheating. LORENZ: Yeah, definitely. So I spoke to a couple teachers and school administrators at different districts. And, you know, AirDrop cheating is an issue. A lot of schools have Wi-Fi off. People have to put their phones away. But kids will snap a picture of the test and then AirDrop it to classmates who have the class later in the day. It can also be used for bullying. You can AirDrop a really unflattering picture or, you know, illicit photos of other people to tons of people in the school really easily. So it can be really used for more negative purposes, too, in that kind of environment. SIMON: Is AirDropping becoming what amounts to its own social network that doesn't rely on major platforms? LORENZ: Sort of. It's a way to get around these platforms. I mean, on all social platforms and text messaging, you have to have people's numbers in your phone. There's - AirDrop is sort of like this pop-up, location-based, anonymous message board where everyone can just, you know, spam out different things to the people around them. And, you know, if you want to leave, you can just turn your AirDrop off or walk away. So yeah. It's a little bit more fluid than actual, structured social networks. SIMON: If somebody doesn't want to be part of the AirDrop experience, what can they do? LORENZ: They can just turn their settings on AirDrop to contacts only or receiving off. It's pretty easy if you just search AirDrop in your settings. SIMON: Taylor Lorenz - her article in The Atlantic is called \"When Grown-Ups Get Caught In Teens' AirDrop Crossfire. \" Thanks so much for coming in and out of the crossfire and speaking with us. LORENZ: Yeah. Thank you so much for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-09-731044416": {"title": "YouTube Removes White Supremacist Content : NPR", "url": "https://www.npr.org/2019/06/09/731044416/youtube-removes-white-supremacist-content", "author": "No author found", "published_date": "2019-06-09", "content": "LULU GARCIA-NAVARRO, HOST: YouTube is removing thousands of videos with white supremacist and other extremist content. It's the latest big tech company to tighten restrictions in response to public pressure to clean up the hate speech that permeates social media. NPR national security correspondent Hannah Allam is here with us to talk about the purge. Hey. HANNAH ALLAM, BYLINE: Hi. GARCIA-NAVARRO: So it has been a couple of days since YouTube began taking down videos. What have you seen? What kind of effect has it had? ALLAM: Well, there was no waiting around. This policy kicked in immediately. YouTube videos with extremist content started vanishing - videos that promoted white supremacy, neo-Nazi videos. Some civil rights groups and people who've been targeted for harassment online say it's a step in the right direction, although they also have concerns that it doesn't go far enough or it's impossible to enforce. And on the flipside, there are people who say it goes too far. GARCIA-NAVARRO: Well, tell me. Give me an example of that. Who's saying it's going too far? ALLAM: One big example is Steven Crowder. He's a right-wing commentator. His YouTube channel has 3 million or more subscribers. He has a history of offensive language, including repeatedly insulting a Vox journalist's race and sexual orientation. After reviews, YouTube ultimately decided not to take down his videos. They said it wasn't a violation. Got some pushback, but then they decided to demonetize them, meaning he can't profit from them. Of course, he wasn't happy with this, and neither were his fans. And his fans include Senator Ted Cruz, the Texas Republican who, you know, went on Twitter demanding that YouTube, quote, \"stop playing God and silencing those voices you disagree with. \"GARCIA-NAVARRO: In YouTube's effort to get rid of hate speech and bigotry, it also apparently swept up some unintended victims? ALLAM: That's right. That was a concern going into this, and it has played out. We've already seen several examples of historical and educational material being removed, things like an educational video used to teach about Hitler in Nazi Germany. Even the Southern Poverty Law Center, which is one of the nation's best-known trackers of extremism, they had one of their videos removed because it included an interview with a British holocaust denier. GARCIA-NAVARRO: So I guess that brings us to the question of who or what is flagging these videos? ALLAM: Yes. And the question of enforcement and, you know, weeding out these videos is difficult when YouTube uses a variety of methods. They have automated systems, human monitors. YouTube users themselves can report and flag violations. But like other platforms, YouTube's in a bind. On one hand, it wants to be an open forum for a broad spectrum of ideas. On the other, it doesn't want to be accused of helping to spread extremism and hate that we've seen lead to violence in some cases. GARCIA-NAVARRO: And, of course, it wants to make money. ALLAM: Definitely, it wants to make money. Advertisers love YouTube because it's unmatched in its reach. And so, yes, there's definitely the financial angle, as well. GARCIA-NAVARRO: Some free speech advocates say this restricting of content smacks of censorship. ALLAM: That's right. It's typically framed as a kind of a free speech issue. But there's also an argument that it's a public safety issue, and that we're in a new era with new and evolving technology that's made it incredibly simple and instantaneous to spread hate and extremism. And when those ideas turn to violence, as we've seen happen in places like Christchurch, New Zealand, it becomes as much about public safety as it is about free speech. And so for now, this debate is in the private sector. But the concern is if these companies fail to do something about it, does the government then come in and start regulating? And, you know, that raises a bunch of thorny First Amendment questions. And so that's the tension we're seeing, who gets to decide the new rules for a new era. GARCIA-NAVARRO: NPR's national security correspondent Hannah Allam. Thank you so much. ALLAM: Thank you. LULU GARCIA-NAVARRO, HOST:  YouTube is removing thousands of videos with white supremacist and other extremist content. It's the latest big tech company to tighten restrictions in response to public pressure to clean up the hate speech that permeates social media. NPR national security correspondent Hannah Allam is here with us to talk about the purge. Hey. HANNAH ALLAM, BYLINE: Hi. GARCIA-NAVARRO: So it has been a couple of days since YouTube began taking down videos. What have you seen? What kind of effect has it had? ALLAM: Well, there was no waiting around. This policy kicked in immediately. YouTube videos with extremist content started vanishing - videos that promoted white supremacy, neo-Nazi videos. Some civil rights groups and people who've been targeted for harassment online say it's a step in the right direction, although they also have concerns that it doesn't go far enough or it's impossible to enforce. And on the flipside, there are people who say it goes too far. GARCIA-NAVARRO: Well, tell me. Give me an example of that. Who's saying it's going too far? ALLAM: One big example is Steven Crowder. He's a right-wing commentator. His YouTube channel has 3 million or more subscribers. He has a history of offensive language, including repeatedly insulting a Vox journalist's race and sexual orientation. After reviews, YouTube ultimately decided not to take down his videos. They said it wasn't a violation. Got some pushback, but then they decided to demonetize them, meaning he can't profit from them. Of course, he wasn't happy with this, and neither were his fans. And his fans include Senator Ted Cruz, the Texas Republican who, you know, went on Twitter demanding that YouTube, quote, \"stop playing God and silencing those voices you disagree with. \" GARCIA-NAVARRO: In YouTube's effort to get rid of hate speech and bigotry, it also apparently swept up some unintended victims? ALLAM: That's right. That was a concern going into this, and it has played out. We've already seen several examples of historical and educational material being removed, things like an educational video used to teach about Hitler in Nazi Germany. Even the Southern Poverty Law Center, which is one of the nation's best-known trackers of extremism, they had one of their videos removed because it included an interview with a British holocaust denier. GARCIA-NAVARRO: So I guess that brings us to the question of who or what is flagging these videos? ALLAM: Yes. And the question of enforcement and, you know, weeding out these videos is difficult when YouTube uses a variety of methods. They have automated systems, human monitors. YouTube users themselves can report and flag violations. But like other platforms, YouTube's in a bind. On one hand, it wants to be an open forum for a broad spectrum of ideas. On the other, it doesn't want to be accused of helping to spread extremism and hate that we've seen lead to violence in some cases. GARCIA-NAVARRO: And, of course, it wants to make money. ALLAM: Definitely, it wants to make money. Advertisers love YouTube because it's unmatched in its reach. And so, yes, there's definitely the financial angle, as well. GARCIA-NAVARRO: Some free speech advocates say this restricting of content smacks of censorship. ALLAM: That's right. It's typically framed as a kind of a free speech issue. But there's also an argument that it's a public safety issue, and that we're in a new era with new and evolving technology that's made it incredibly simple and instantaneous to spread hate and extremism. And when those ideas turn to violence, as we've seen happen in places like Christchurch, New Zealand, it becomes as much about public safety as it is about free speech. And so for now, this debate is in the private sector. But the concern is if these companies fail to do something about it, does the government then come in and start regulating? And, you know, that raises a bunch of thorny First Amendment questions. And so that's the tension we're seeing, who gets to decide the new rules for a new era. GARCIA-NAVARRO: NPR's national security correspondent Hannah Allam. Thank you so much. ALLAM: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-10-731385508": {"title": "The Money And Trade-Offs Involved In The Influencer Economy  : NPR", "url": "https://www.npr.org/2019/06/10/731385508/the-money-and-trade-offs-involved-in-the-influencer-economy", "author": "No author found", "published_date": "2019-06-10", "content": "AUDIE CORNISH, HOST:  Time now for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CORNISH: This month, All Tech is all about social media influencers. We heard last week from college student Cosette Rinab. She posts content to Instagram and TikTok, and she hopes to turn that into a full-time career. (SOUNDBITE OF ARCHIVED BROADCAST)COSETTE RINAB: People are making more money off of YouTube than they could be if they were going to medical school and studying, you know, pursuing a career in medicine. CORNISH: We're going to talk now about the influencer economy, the money involved and the tradeoffs required. Taylor Lorenz writes for The Atlantic. She's covered the business of influencers for a decade. Welcome to the program. TAYLOR LORENZ: Thanks for having me. CORNISH: So how much money changes hands? What are we talking about? LORENZ: Billions and billions of dollars. It's estimated that brands are going to spend upwards of $10 billion on influencer marketing efforts by 2020, which is just next year. CORNISH: Can we talk about the companies for a second? Why would a company the size of a Walmart or a major clothing chain even need to advertise in this way, through social media, right? Like, having some person pose next to something or wear the clothes. What's the payoff for them? LORENZ: Well, influencer marketing is very powerful because, you know, people have very close relationships with these influencers. You know, they often follow people who they aspire to be like, or people that seem like them and resonate with their lifestyle. So, you know, Walmart, for instance, paying a mom in Kentucky to speak to other moms in the South is much more powerful than them just running spammy banner ads, most of which get ignored. CORNISH: Right. I mean, part of the thing that makes it appealing is it doesn't always feel like an ad. LORENZ: Exactly. It doesn't feel like an ad. And often (laughter), because there's not a lot of regulation around this, it's often not even disclosed as an ad. CORNISH: Who has the upper hand, the brands or the influencers? LORENZ: The brands, absolutely. I mean, they're ultimately the ones that control the purse strings. So they're often the ones dictating the terms of these agreements and sometimes screwing over, (laughter), the younger, you know, smaller influencers who often don't even have management or any power to negotiate. CORNISH: What do you mean by that? What kind of contract can be structured in a way that would be at a disadvantage to a young person trying to come up in the business? LORENZ: Well, most of these people don't even have formal contracts. That's the first problem. I mean, brand deals are negotiated over direct messages on different platforms. It's very fluid and very casual. You know, if you're the top 1% of YouTubers or Instagram stars, you're likely to have management or maybe you're repped by a talent agency. But for the vast majority of so-called influencers, they're doing it all themselves. CORNISH: Right. Basically, they're independent contractors. And what are the pitfalls for that in this particular business? LORENZ: I mean, a lot of these influencers will work with middlemen, which are essentially influencer management agencies. So Walmart will work with one of these social media influencer agencies and say, we want to run a social campaign with, you know, 10 influencers between these ages that have these qualifications. And the agency will then pull from this massive pool and, you know, pull influencers to work with the brand. And the brand will pay the agency directly, and the agency doesn't always pay the influencer. CORNISH: Wow. So they'll just take the money (laughter)? LORENZ: Yeah. I mean, we saw Speakr, which is actually the biggest and oldest social media influencer agency, actually, you know, fail to pay hundreds of influencers just last year. These are people with millions of followers in some cases, and they were owed tens of thousands of dollars in others' cases - so yeah. CORNISH: You're describing an industry and players that are basically regulating themselves, right? And you've even described some abuse. Is there any effort by the social media companies or the government to have any kind of regulatory influence here? LORENZ: Well, the FTC did attempt to kind of regulate it by, you know, saying that if you post an ad, you have to disclose that it's an ad. The thing is, is that all of these ad deals are structured very fluidly. You know, you can get a lot of free product and become a brand ambassador and post about that product all the time, but maybe you're not receiving direct payment. You know? And so I think that regulators need a much better understanding of this industry in general in order to regulate it. On the platform side, actually, Instagram and YouTube and all of these social media platforms definitely realize that there's a lot of commerce happening on their platforms. But I think their stance on it is to kind of just try and get a piece of that pie rather than help the influencers themselves. CORNISH: Taylor Lorenz is a staff writer at The Atlantic. She covers Internet culture. Thanks so much. LORENZ: Yeah, thank you for having me. AUDIE CORNISH, HOST:   Time now for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CORNISH: This month, All Tech is all about social media influencers. We heard last week from college student Cosette Rinab. She posts content to Instagram and TikTok, and she hopes to turn that into a full-time career. (SOUNDBITE OF ARCHIVED BROADCAST) COSETTE RINAB: People are making more money off of YouTube than they could be if they were going to medical school and studying, you know, pursuing a career in medicine. CORNISH: We're going to talk now about the influencer economy, the money involved and the tradeoffs required. Taylor Lorenz writes for The Atlantic. She's covered the business of influencers for a decade. Welcome to the program. TAYLOR LORENZ: Thanks for having me. CORNISH: So how much money changes hands? What are we talking about? LORENZ: Billions and billions of dollars. It's estimated that brands are going to spend upwards of $10 billion on influencer marketing efforts by 2020, which is just next year. CORNISH: Can we talk about the companies for a second? Why would a company the size of a Walmart or a major clothing chain even need to advertise in this way, through social media, right? Like, having some person pose next to something or wear the clothes. What's the payoff for them? LORENZ: Well, influencer marketing is very powerful because, you know, people have very close relationships with these influencers. You know, they often follow people who they aspire to be like, or people that seem like them and resonate with their lifestyle. So, you know, Walmart, for instance, paying a mom in Kentucky to speak to other moms in the South is much more powerful than them just running spammy banner ads, most of which get ignored. CORNISH: Right. I mean, part of the thing that makes it appealing is it doesn't always feel like an ad. LORENZ: Exactly. It doesn't feel like an ad. And often (laughter), because there's not a lot of regulation around this, it's often not even disclosed as an ad. CORNISH: Who has the upper hand, the brands or the influencers? LORENZ: The brands, absolutely. I mean, they're ultimately the ones that control the purse strings. So they're often the ones dictating the terms of these agreements and sometimes screwing over, (laughter), the younger, you know, smaller influencers who often don't even have management or any power to negotiate. CORNISH: What do you mean by that? What kind of contract can be structured in a way that would be at a disadvantage to a young person trying to come up in the business? LORENZ: Well, most of these people don't even have formal contracts. That's the first problem. I mean, brand deals are negotiated over direct messages on different platforms. It's very fluid and very casual. You know, if you're the top 1% of YouTubers or Instagram stars, you're likely to have management or maybe you're repped by a talent agency. But for the vast majority of so-called influencers, they're doing it all themselves. CORNISH: Right. Basically, they're independent contractors. And what are the pitfalls for that in this particular business? LORENZ: I mean, a lot of these influencers will work with middlemen, which are essentially influencer management agencies. So Walmart will work with one of these social media influencer agencies and say, we want to run a social campaign with, you know, 10 influencers between these ages that have these qualifications. And the agency will then pull from this massive pool and, you know, pull influencers to work with the brand. And the brand will pay the agency directly, and the agency doesn't always pay the influencer. CORNISH: Wow. So they'll just take the money (laughter)? LORENZ: Yeah. I mean, we saw Speakr, which is actually the biggest and oldest social media influencer agency, actually, you know, fail to pay hundreds of influencers just last year. These are people with millions of followers in some cases, and they were owed tens of thousands of dollars in others' cases - so yeah. CORNISH: You're describing an industry and players that are basically regulating themselves, right? And you've even described some abuse. Is there any effort by the social media companies or the government to have any kind of regulatory influence here? LORENZ: Well, the FTC did attempt to kind of regulate it by, you know, saying that if you post an ad, you have to disclose that it's an ad. The thing is, is that all of these ad deals are structured very fluidly. You know, you can get a lot of free product and become a brand ambassador and post about that product all the time, but maybe you're not receiving direct payment. You know? And so I think that regulators need a much better understanding of this industry in general in order to regulate it. On the platform side, actually, Instagram and YouTube and all of these social media platforms definitely realize that there's a lot of commerce happening on their platforms. But I think their stance on it is to kind of just try and get a piece of that pie rather than help the influencers themselves. CORNISH: Taylor Lorenz is a staff writer at The Atlantic. She covers Internet culture. Thanks so much. LORENZ: Yeah, thank you for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-11-731703544": {"title": "News Groups Say Big Tech Is Potentially Existential Threat To Publishers : NPR", "url": "https://www.npr.org/2019/06/11/731703544/news-publishers-say-tech-industry-poses-potentially-existential-threat-to-media", "author": "No author found", "published_date": "2019-06-11", "content": "", "section": "Politics", "disclaimer": ""}, "2019-06-11-731607445": {"title": "Uber CEO Says Tech Regulation Is Deserved : NPR", "url": "https://www.npr.org/2019/06/11/731607445/uber-ceo-some-of-the-increased-scrutiny-on-tech-is-deserved", "author": "No author found", "published_date": "2019-06-11", "content": "", "section": "Technology", "disclaimer": ""}, "2019-06-11-731551123": {"title": "Border Patrol Say Hackers Nabbed Photos Of License Plate And People : NPR", "url": "https://www.npr.org/2019/06/11/731551123/hackers-grabbed-security-camera-images-taken-at-border-crossing-cbp-says", "author": "No author found", "published_date": "2019-06-11", "content": "", "section": "Technology", "disclaimer": ""}, "2019-06-12-732157537": {"title": "La Liga Phone App Spied On Fans To Catch Bars Showing Unlicensed Game Broadcasts : NPR", "url": "https://www.npr.org/2019/06/12/732157537/spains-soccer-league-fined-for-using-app-to-spy-on-fans-in-fight-to-curb-piracy", "author": "No author found", "published_date": "2019-06-12", "content": "", "section": "Technology", "disclaimer": ""}, "2019-06-13-732320853": {"title": "Hackers Demanding Ransoms Paralyze City Computer Systems In The U.S. : NPR", "url": "https://www.npr.org/2019/06/13/732320853/hackers-demanding-ransoms-paralyze-city-computer-systems-in-the-u-s", "author": "No author found", "published_date": "2019-06-13", "content": "DAVE DAVIES, HOST: This is FRESH AIR. I'm Dave Davies in for Terry Gross, who's off this week. As we become increasingly dependent on sprawling computer networks, we're increasingly vulnerable to hackers who exploit weaknesses in them. A recent trend is cyberattacks on American cities. Last year, hackers in Dallas gained the ability to turn on tornado sirens at will. And for weeks, the city of Baltimore has struggled to revive computer systems paralyzed by hackers demanding money. Our guest, New York Times cybersecurity correspondent Nicole Perlroth, says even more troubling is the fact that the Baltimore hackers used stolen cyberweapons originally developed by the U. S. National Security Agency. Perlroth has reported on the proliferation of cyberweapons used by countries against each other, by hackers against governments and corporations and by private security firms willing to give clients digital espionage capabilities for the right price. Perlroth has also reported on concerns about interference in the 2020 presidential campaign and evidence that voting technology may have been hacked in one swing state in the 2016 election. I spoke to her yesterday. Nicole Perlroth, welcome to FRESH AIR. We've seen cases where cities have suffered cyberattacks. One of the best known as Baltimore. Let's take that as an example. What happened? NICOLE PERLROTH: Well, about a month ago, cybercriminals infected Baltimore with a type of ransomware, which is just malware that locks up your systems. And the attackers will demand a ransom to unlock them. And in Baltimore's case, they demanded bitcoin payment in the form of, I think, something like $100,000. And for the last month, Baltimore has been trying to unravel this thing. They have not paid their attackers. And as a result, it looks like they're up against something like $18 million in lost revenue and damages. So, effectively, what's happened is people have not been able to pay parking tickets. They haven't been able to pay their water bills. The real estate agents have not been able to close deals because there's a process by which they need to check existing liens on Baltimore City network, so all of these systems have been paralyzed using this ransomware. Now, on the back end, essentially what happened is Baltimore looks like they hadn't patched for some of the malware that was able to take over their systems. And one particular aspect of the attack that caught our attention was that contractors on site - and we know there are at least four contractors on site - picked up, in one case, a hacking tool on Baltimore's network called EternalBlue that was stolen from the National Security Agency a couple of years ago. Now, that tool has been used in ransomware attacks all over the country recently. So some of the other places it's popped up were an attack in San Antonio, Texas, and an attack in Allentown, Pa. , which were very different from Baltimore's but enough that they caused significant damage, at least in Allentown's case as well. DAVIES: When this happens, it seems the cities don't say very much about this. Is this PR damage control or is there a good operational reason for being so reticent to talk about it? PERLROTH: I think there's been quite a bit of victim shaming for a long time that if you get hit by one of these attacks, it's your fault, that there's something you did to not adequately protect your systems. And I think a lot of the victims come at this from a defensive crouch, basically. But the reality is we're getting to a point where most cities are facing a million of these attacks every week - not successful in many cases, but this is now what local municipalities are up against. And in Baltimore's case, this became particularly sensitive because one of the hacking tools that was found on its network was a tool that was stolen from the NSA for which there was a patch made available almost two years ago. So there was some culpability on Baltimore's part that they had not kept their software up to date or patched their systems for the vulnerability that was exploited on its network. And I think that's where you start to see some of this defensiveness come into play. DAVIES: A patch being a software update from, you know, Microsoft or one of the software companies they were using - you said cities are suffering millions of attacks. Does that mean millions of individual probes, like phishing emails, that kind of thing? PERLROTH: Exactly, millions of probes or incidents or types of malware trying to hit their systems. Now, many of these are not successful or they don't get past the firewall or they try to exploit a vulnerability in software that's been patched already, but this is what cities are up against. And in many cases, cities just don't have the big budgets for security that, say, J. P. Morgan or Bank of America do, which get hit with even more attacks but not too many more than local cities these days. And the reason that cybercriminals have targeted American cities - and I should say this is happening all over the world - is that so many critical systems are now connected to the Internet. And cities are in a place where they're managing these sprawling networks. In most cases, they don't even know what's on their network. And when they get hit by a ransomware attack, the immediate instinct is just pay them. Just pay the attackers so that we can go back to our business, when, in reality, cities are tied up in red tape. They can't pay these attacks or if they did, they would have to do so very quietly. And in a city like Baltimore, which is struggling with gun violence and monitoring street drugs, it's a really hard sell to say, we're going to allocate some of the budget to pay off a group of cybercriminals so we can get our data back. In the case of Baltimore, it's got into a place where the attack has been so public, city officials were left in a position where they felt like they couldn't pay. And what's interesting is the ransom that was demanded was something like $100,000, but the losses from lost revenue and the cost of remediation cleaning up from that cyberattack is now nearing $20 million. DAVIES: Are cities or, you know, municipal authorities paying ransoms? Do we know? PERLROTH: Many quietly are. And the reason we know that is because a lot of these ransoms come in the form of bitcoin demands. And if you look at some of the bitcoin wallets that belong to the attackers, you are seeing quite a few transactions. So we know in some cases, police departments have come out and said, we had to pay this ransom. Hospitals, companies have been paying these ransoms. And the fact is, sometimes, it's just a lot cheaper to pay the ransom than to deal with the fallout of a huge attack that wipes all your records and all your data clean and paralyzes your networks. DAVIES: And I don't know if you can give a general answer to this question, but what are the stakes if a city refuses to pay the ransom and simply decides it's going to repair the damage, restore the files? I mean, do they have to rebuild records by doing paper entries or are things lost forever? What happens? PERLROTH: In some cases, cities have been able to recover their files through backups or other channels. But the fact remains a lot of data gets lost. And in many cases, it's not just that the data gets lost. It's the lost revenue from these attacks where people have not been able to close deals; people have not been able to collect parking ticket payments. They haven't been able to collect water bills. And the city essentially, in some cases, loses its tally of who has paid what. DAVIES: And do we know who hit Baltimore or Atlanta or Dallas? I mean, do we know if it's one person or group of people that are - you know, have these tools and are going from town to town? PERLROTH: Well, what's interesting is, in Atlanta's case, there were indictments that named a very famous group of ransomware criminals that was called the SamSam group. And the SamSam group was known for demanding pretty high ransom demands and also being pretty meticulous about the data that they locked up on systems. They really, in many cases, left victims no choice but to pay or lose all their data. And what we didn't know until these indictments came out last year was that the SamSam group was actually a team of hackers in Iran. Now, there was no clear nexus between this group of Iranian cybercriminals and the state. But this is something they were doing from Iran for profit. Now, in Baltimore, we still don't know. There's someone online who's taken credit for the attack. And I've heard from some people who track the dark Web that they believe it's an individual in Turkey. But we just don't know yet. In some of these other cities we may never know. And that's sort of the danger of the Internet is that criminals and nation-states route these attacks through servers all around the globe. And attributing them to one group or one nation-state can take a very long time. And in some cases, it's just impossible. DAVIES: When you pay the ransom, can you be sure that you'll get the files back? What does experience tell us? PERLROTH: The experience tells us no. There are some groups who reliably unscramble your data. We know that. We know that the SamSam group, the group I just referred to from Iran, that they did unlock data in their ransomware attacks when victims did pay. But there are many, many other groups that do not. And that's part of the decision-making process that these victims have to deal with is, if they pay and - especially in cities - if they get through that red tape where they can approve that payment and they can budget for that payment - and in a city like Baltimore that's already struggling with problems far bigger than their security budgets, it's a real dilemma. If they pay, will the criminals on the other end unlock their systems? In some cases, these ransomware groups have a reputation to keep up, and they will reliably decrypt your systems, but in many cases, they won't. DAVIES: Nicole Perlroth is a cybersecurity correspondent for The New York Times. We'll continue our conversation after a short break. This is FRESH AIR. (SOUNDBITE OF PATTI SMITH'S \"SMELLS LIKE TEEN SPIRIT\")DAVIES: This is FRESH AIR, and we're speaking with Nicole Perlroth. She is a cybersecurity correspondent for The New York Times. You have reported that the attack on Baltimore and others were done using a cyberweapon that was developed by the NSA, the National Security Agency. Tell us a little about this. What is the weapon, and how did it get in the wrong hands? PERLROTH: Well, for decades now, the National Security Agency has been essentially looking for vulnerabilities in software - often in Windows software, just because it gets used so much around the world - but obviously in firewalls and in apps and in your iPhone software. And the reason they're looking for those vulnerabilities is that if they find one and they can write code to exploit that vulnerability, it essentially gives them a backdoor into that system. And often what they're using it for is just espionage - to collect information off those systems. So for years, the NSA has been looking for these vulnerabilities. They've been writing the code to exploit them. And they've been holding onto them in what we describe as sort of a stockpile of of hacking tools. Now, the problem with this is that, as opposed to, say, 30 years ago when we were just trying to get in to, say, the systems that Russia used at the Russian Embassy, that's no longer the case that Russia is using some different technology and we're using different technology here in the United States and China is using different technology. For the most part, we're all using the same technology. So if the NSA is finding a vulnerability in that technology, they're not just finding it in Russia's systems or China's systems, they're finding it in technology that gets used by a lot of Americans. So over the last 10 years, the White House has tried to set up a process for discussing which of these vulnerabilities the NSA should turn over and which should be kept because they're so valuable to the NSA's intelligence-gathering mission. DAVIES: And when you say whether they should turn them over, you mean what? PERLROTH: Turn them over to companies like Microsoft, whose software was vulnerable in the first place, so that Microsoft can then patch that vulnerability and roll it out to their customers all over the world so they're no longer vulnerable to attacks that can make use of that vulnerability. DAVIES: So Microsoft could send out an update so that, heaven - you know, you've got this vulnerability you didn't know about, it can be fixed provided you upload the update. PERLROTH: Exactly. And over the last decade or so, the White House set up a process for this - to decide, OK, which of these vulnerabilities are so valuable we're going to keep them for intelligence-gathering? And which of these vulnerabilities, if discovered by one of our adversaries or by cybercriminals, could be exploited and cause so much damage or potential damage to American interests that it's far more logical that we would turn it over to a Microsoft, an Apple or Facebook or Cisco to patch just because the potential damage to American interests could be so great. And what's interesting is that as we've all sort of connected everything we possibly can to the Internet, more and more United States agencies have sent representatives into this process. So whereas in the beginning it might just be representatives for the intelligence agencies and the Department of Homeland Security, who were a part of these discussions, now we know that there needs to be representatives from the Treasury, representatives from the Department of Health because so many - so much of the software touches hospitals and banks and trading systems. And if there is a vulnerability in that software that could be exploited to harm those interests, well, then those agencies need to be a part of that debate. DAVIES: So it seems in this case, the NSA decided it had discovered this vulnerability. And it was in a Microsoft operating system, right? And they decided to keep it a secret so that they could use it. What happened? PERLROTH: Well, we know that there was actually quite a bit of work that went into not only finding this vulnerability, which was just in a Microsoft Windows protocol. There was actually quite a bit of work - a team that spent many man hours making the code to exploit this in a way that wouldn't crash computer screens on the other end. And that was not easy. That took quite a bit of work by the NSA's engineers and algorithms to develop and exploit that could reliably hack into these Microsoft systems. So part of what we learned in our reporting was that once the NSA had honed this, they considered it to be one of the most-valuable tools in their arsenal. They said it netted some of the best counter intelligence that they had and was - played a role in some of the biggest counterterrorism investigations that the NSA was participating in. So they actually said at no point did they consider turning this over to Microsoft for patching because it just played such a big role in their mission. So fast-forward to 2016, a new group comes online. They call themselves The Shadow Brokers. We still have no idea who they are. And the group essentially starts threatening the NSA online and says, we have some of your hacking tools. They allude to some of the tools that they've stolen. And they begin to auction them for sale online. Now, no one comes forward to pay for these things, in part for obvious reasons. But one year later, in early 2017, the group just goes ahead and dumps these tools online. DAVIES: The program was called EternalBlue. PERLROTH: Yeah. So EternalBlue was, like I said, one of the most-important tools in the NSA's arsenal. And to be fair, the NSA was using this tool for espionage. They weren't using it necessarily to destroy computers or to send out cyberattacks that were going to destroy data or paralyze networks. But that same tool could be picked up and used by nation-states and cybercriminals and built onto their own ransomware or malware for whatever they sought to do. So what happened was - the first thing we saw was an about May of 2017. North Korean hackers took the NSA's stolen tool, EternalBlue, and put it on to some ransomware that that researchers called WannaCry. And they sent it around the world. And within about 24 hours, they had paralyzed the British health system and hit some 200,000 other victims - just in the first 24 hours. DAVIES: What was their purpose? PERLROTH: Well, in that case, the malware that they were sending around was ransomware. Now, unfortunately, the North Koreans hadn't put enough thought into this ransomware, and it wasn't that effective. In fact, if you paid for - to unlock your systems - and some did - the bitcoin wallet wasn't functioning. And no matter what, you wouldn't have gotten your data back. But clearly there was a profit incentive here. And it was pretty interesting to see a nation-state send out ransomware like that. And it's not - had not been unprecedented. We'd seen this from hackers in Iran before. But it was such a wide-scale attack, and it paralyzed so many systems that it was a huge embarrassment to the National Security Agency. DAVIES: Nicole Perlroth is a cybersecurity correspondent at The New York Times. After a break, we'll hear more about the cyberweapons stolen from the NSA and how they're being used around the world. And she'll talk about evidence that Russian hacking may have impaired voting systems in one North Carolina county in the 2016 presidential election. Also, Justin Chang reviews the new film \"The Last Black Man In San Francisco. \" I'm Dave Davies, and this is FRESH AIR. (SOUNDBITE OF FAREED HAQUE AND KAIA STRING QUARTET'S \"QUINTET FOR GUITAR AND STRINGS - ALLEGRO VIVACE\")DAVIES: This is FRESH AIR. I'm Dave Davies in for Terry Gross, who's off this week. Let's get back to my interview with New York Times cybersecurity correspondent Nicole Perlroth. She's written recently about ransomware attacks, which have paralyzed computer networks in several American cities. When we left off, we were talking about how some hacking tools developed by the NSA have been stolen by outside groups and used in cyberattacks, including one on the city of Baltimore. One of those NSA tools, which was stolen and posted on the Internet by an anonymous group called The Shadow Brokers, is called EternalBlue. EternalBlue was, essentially, a way to get into a lot of computer systems. And once you had that tool, there are different kinds of mischief you could do once you penetrated it, right? - could be ransomware. PERLROTH: Right. DAVIES: It could be destroying files, whatever. PERLROTH: Yeah, and it's not just a way to get into these systems. It's a way of spreading within these systems, so you can think of it almost like a way to supercharge your ransomware so it makes it out to as many systems as possible. And what happened a month later, after North Koreans put EternalBlue onto their ransomware, is Russia picked up EternalBlue. And this time, they attached it to ransomware that they aimed at Ukraine. And it was a hugely successful attack. In fact, we now know that this was, perhaps, the most destructive, costliest cyberattack in history. We're talking about tens of billions of dollars of damages. And what was interesting that time - and the attack against Ukraine was called NotPetya but by some researchers who reverse engineered the code. What was interesting in that case was it didn't just hit Ukraine. It hit any business that had a remote employee in Ukraine or had an office in Ukraine or worked with a contractor there. So we saw Mondelez - giant snack maker, makes Oreo cookies. They got hit by the NotPetya attack and have now reported something like $600 million in damages, so this was not something that was just an attack by Russia on Ukraine. This was an attack that boomeranged back to American companies and companies all over the world, where the damage was caused, in large part, by an American-made hacking tool - EternalBlue. DAVIES: Wow. Now, last month, you and a couple of other reporters - David Sanger and Scott Shane - reported that the Chinese had acquired some NSA hacking tools and repurposed them to attack U. S. allies and private companies. Is this EternalBlue or is this something different? PERLROTH: It's something different, but the exploit that they used was also among the tools that The Shadow Brokers dumped online in 2017. And what was interesting in this case is that it turns out that a group we had been tracking in China - very sophisticated Chinese contractor group that, in the past, had hacked a lot of aerospace, satellite space technology, even nuclear propulsion technology - had somehow discovered the NSA's hacking tool, we think, in an attack on China's own systems. And they took it, and they repurposed it for their own attacks on American interests and allies all over the globe. So in that case, what was interesting was it's not like they took the tool after it was dumped online by The Shadow Brokers. They actually discovered it in the course of an NSA operation, picked it up, reverse engineered it and bolted it onto their own malware for their own espionage operations. DAVIES: So a weapon aimed at them, they effectively captured and turned it around and used it against their attacker. PERLROTH: Exactly. And what's interesting about that case is it raises questions about, how safe can the NSA keep its most valuable cyberweapons? So clearly, in the case of Shadow Brokers, we still don't know how Shadow Brokers got the NSA's tools. In fact, we still don't know, two years later, who The Shadow Brokers are. Initially, we thought maybe it was Russia that had hacked the NSA somehow. We've heard that the investigation has started to focus on a potential NSA mole, but we still don't have answers there. And that's something that, I think, we need to see more accountability from the NSA on. For right now, the NSA has yet to even acknowledge that the tools that were dumped online belonged to the agency. But what was interesting about the Chinese case is it's not as if the Chinese took this tool after it had been dumped online. They just took it in the course of the NSA's routine operations, which begs the question is - once you use these tools, can you ever really control them? DAVIES: The Shadow Brokers have posted on social media, right? What are their posts like? Do they give us any clues as to who they might be? PERLROTH: So The Shadow Brokers - they write these posts and these demands in very poorly broken English. And it honestly sounds like someone who is not Russian trying to sound Russian (laughter), so we have no idea who they are. But, essentially, they're very menacing. They've actually picked out individuals who used to work at the NSA and made fun of them alongside some of their dumps of the NSA hacking tools. And some of the posts that they made have made other NSA analysts or, I should say, former NSA analysts start to think that this is someone who had very deep operational knowledge of the agency and the way it worked and knew things about the agency that could not have been gleaned just out there on the Internet. It's stuff that they would have had to have had some deep knowledge of the inner workings of the agency. And I think, in part, that's why this investigation has gone from assuming that this was a Russian operation to potentially an insider. DAVIES: You know, as we see a circumstance here where this really important National Security Agency of the United States has developed these weapons, which have fallen into the wrong hands, I mean, it really does raise policy questions, right? I mean, they say war is an instrument of policy. Cyberwar is another instrument of policy. Are people at the top asking fundamental questions like, should we be developing weapons like this? Or is there a way for international regulation of weapons like this, as we've seen with, you know, nuclear treaties? PERLROTH: There are some voices in the wilderness calling for a sort of digital version of the Geneva Convention. And the leading voice out there is actually Brad Smith, the president of Microsoft. Now, Microsoft has a stake in this, which is that it's Microsoft software that's being infiltrated in so many cases by nation-states for foreign espionage simply because Microsoft has such a great market share that it's the leading target for a lot of these nation-state espionage operations. And what Microsoft has said is that, essentially, what happened with EternalBlue and The Shadow Brokers leak is that the NSA left a missile out there for anyone to pick up, and they didn't adequately protect these missiles. And now American companies and American citizens are paying for that situation. DAVIES: Nicole Perlroth is a cybersecurity correspondent for The New York Times. We will continue our conversation after this short break. This is FRESH AIR. (SOUNDBITE OF MUSIC)DAVIES: This is FRESH AIR, and we're speaking with Nicole Perlroth. She is a cybersecurity correspondent for The New York Times. I want to talk a bit about election security. You know, there's a general feeling, I think, that there was Russian interference in the 2016 presidential campaign in the form of hacked emails and disinformation campaigns but that the voting process itself was not tampered with. You've been on this beat a while and have been looking into this. What did you find? PERLROTH: Well, we found that there is a dearth of serious forensic investigation that investigated problems from the 2016 election. And there were issues, particularly in North Carolina, that suggest that there actually were quite a few problems tied to issues with electronic pollbook systems, the systems that check you in when you go to check in at the voting booth. And in many cases, these pollbooks were telling people that they had already voted when they hadn't, that they were not registered to vote when they were. And some of those pollbooks were managed by a company called VR Systems that, we know from leaked NSA documents, was in fact hacked by Russia prior to the 2016 election. DAVIES: And was there any pattern to which communities or precincts these problems occurred? PERLROTH: There was. Durham County, in particular, had a lot of problems with its e-poll book systems. Now, if you were going to try to disenfranchise a large number of Democratic voters in North Carolina, you'd probably go right to Durham County. This is a blue county in a largely red state. And when people went to go vote in Durham County, they were finding a lot of irregularities with the e-poll book systems. So over a year ago, we wrote about those problems. And what was really disturbing is that when we tried to find whether there had been an in-depth forensic investigation of the e-poll book issues in Durham County, I found a report that was conducted that was unlike any other cyberforensics report I had ever seen. Usually, when you look at these forensic reports, they tell you, you know, we did an analysis of this computer. We found this vulnerability. We found this malware or we didn't find this malware or we found this hacking technique or we didn't find this hacking technique. This read very differently. It read like a police report, where whoever was conducting the investigation was a local detective, former police officer, who said, at 3:15 p. m. , I interviewed Suzy (ph), who was working at the voting booth, and she said all was normal. I mean, I've never seen a cybersecurity investigation report look like that. And when we asked North Carolina to sort of account for this or to take a deeper look, they were pretty defensive about the issues that had happened in Durham County. And only now, a couple of years later, have we found out that, in fact, VR Systems - the company that was hacked by China - did remotely access the e-poll book systems in Durham the night before the 2016 election to try and diagnose some problems it was seeing. And that remote access could have very well been exploited by nation-state hackers. We just don't know. DAVIES: Right. And so, again, we're talking about the electronic pollbooks. That's essentially the registry of electors in a particular polling place. And the company that managed them, VR Systems - you say that we know that it actually was penetrated by Russian hackers. What exactly do we know about that? PERLROTH: We know this from leaked NSA documents that VR Systems was compromised in some kind of spear phishing-attack - so when employees open a malicious email attachment or click on a malicious link that allows malware into their systems. And we know that VR Systems maintained remote access to the e-poll books in Durham and many other counties all over the country - in Florida and elsewhere. And what we don't know is, was that access exploited by Russian hackers to disenfranchise voters? We still don't know. And only now do we know that DHS, the Department of Homeland Security, is conducting a forensic examination of those e-poll book issues in Durham County. DAVIES: And what does VR Systems say? PERLROTH: VR Systems hasn't said much. I think the last time we spoke to them, they denied that they had been phished. They have sort of resisted what was leaked in the NSA documents that suggested it was successfully phished. And they've said they're cooperating with investigators. But beyond that, we really don't know what actually happened there. DAVIES: Authorities actually identified the person in the NSA who leaked this report that VR Systems had been hacked. You want to tell us that story? What became of her? PERLROTH: Right. So we may have never known about this if not for a young NSA employee by the name of Reality Winner, and that is her actual name. She leaked NSA documents that confirmed VR Systems had been hacked in a Russian cyberattack to The Intercept - a digital publication run by Glenn Greenwald. And The Intercept actually published the leaked documents and did it in a way that the NSA was able to trace the leak pretty easily back to Reality Winner. Now, she's since been sentenced to more than five years in prison under the Espionage Act for leaking those documents. DAVIES: You've also written that there's evidence of Russian hacking in the 2018 midterm elections. Are the FBI and American security officials putting more resources into dealing with foreign interference in 2020? Is Congress doing anything? PERLROTH: I wish I could say yes. The reality is that there's been a lot of red tape and a lot of politics around securing the next election. Now, that's not to say nothing's been done. We know that U. S. Cyber Command, that U. S. military hackers, going into the 2018 election, conducted a cyberattack that shut down servers that belonged to Russia's Internet Research Agency to sort of preemptively shut down any kind of Russian interference. We also know Claire McCaskill and other Democratic senators were targeted by spear phishing attacks ahead of the 2018 midterm elections, although they say that the attacks weren't successful. Now, the problem going into 2020 is the same phishing attacks that targeted John Podesta and others in the 2016 election are probably happening right now. And what has happened is a lot of candidates have reached out to cybersecurity firms and services and said, please help us defend against these phishing attacks. We know they're coming. We know their targets. We need help. And what has been incredibly frustrating, I think, for these candidates and for the cybersecurity community and anyone whose eyes are open enough to see that 2020 is going to be huge battleground for cyberattacks and disinformation is that campaigns have not been able to accept help from many cybersecurity firms that defend against phishing attacks and other forms of cyberattacks because those services are considered an in-kind donation and illegal because of federal campaign finance laws. And so we just had a case where there was a Silicon Valley company that asked the Federal Election Commission to make an exemption so that it could help campaigns defend against phishing attacks. And so far, the Federal Election Commission has said that they would not actually make an exemption, although we're still waiting on a final ruling. DAVIES: That's so remarkable. The Federal Election Commission seems to never make a decision. (LAUGHTER)DAVIES: But they'd made a decision about this. Anything happening in Congress on this issue? PERLROTH: No, and we have one man to thank for that, and his name is Mitch McConnell. Mitch McConnell has said he does not plan to bring any election security bills to the Senate floor, period. We know that the House is trying to come up with some work arounds for this, but many of the bills that would just establish basic security norms and things like paper backups of votes have been proposed by Senator Ron Wyden and others are really stalled in the Senate right now because Mitch McConnell has said he refuses to bring them to a Senate vote. DAVIES: What's McConnell saying about why he's doing this? PERLROTH: Well, in general, McConnell has said that there's no reason to bring these measures to the Senate floor, that states should play the biggest role in securing elections and that the federal government should not get involved. But behind the scenes, we're told that a lot of this just has to do with the optics, that the president doesn't want to even hear about election security because, apparently, according to people we've spoken with, he thinks any focus on election security or the Russian interference in the 2016 election takes away from the legitimacy of his victory in 2016. DAVIES: Well, Nicole Perlroth, thanks so much for speaking with us. PERLROTH: Thanks so much for having me, Dave. DAVIES: Nicole Perlroth is a cybersecurity correspondent at The New York Times. Coming up, Justin Chang reviews the new film \"The Last Black Man In San Francisco. \" This is FRESH AIR. (SOUNDBITE OF RAY CHARLES' \"DOODLIN'\") DAVE DAVIES, HOST:  This is FRESH AIR. I'm Dave Davies in for Terry Gross, who's off this week. As we become increasingly dependent on sprawling computer networks, we're increasingly vulnerable to hackers who exploit weaknesses in them. A recent trend is cyberattacks on American cities. Last year, hackers in Dallas gained the ability to turn on tornado sirens at will. And for weeks, the city of Baltimore has struggled to revive computer systems paralyzed by hackers demanding money. Our guest, New York Times cybersecurity correspondent Nicole Perlroth, says even more troubling is the fact that the Baltimore hackers used stolen cyberweapons originally developed by the U. S. National Security Agency. Perlroth has reported on the proliferation of cyberweapons used by countries against each other, by hackers against governments and corporations and by private security firms willing to give clients digital espionage capabilities for the right price. Perlroth has also reported on concerns about interference in the 2020 presidential campaign and evidence that voting technology may have been hacked in one swing state in the 2016 election. I spoke to her yesterday. Nicole Perlroth, welcome to FRESH AIR. We've seen cases where cities have suffered cyberattacks. One of the best known as Baltimore. Let's take that as an example. What happened? NICOLE PERLROTH: Well, about a month ago, cybercriminals infected Baltimore with a type of ransomware, which is just malware that locks up your systems. And the attackers will demand a ransom to unlock them. And in Baltimore's case, they demanded bitcoin payment in the form of, I think, something like $100,000. And for the last month, Baltimore has been trying to unravel this thing. They have not paid their attackers. And as a result, it looks like they're up against something like $18 million in lost revenue and damages. So, effectively, what's happened is people have not been able to pay parking tickets. They haven't been able to pay their water bills. The real estate agents have not been able to close deals because there's a process by which they need to check existing liens on Baltimore City network, so all of these systems have been paralyzed using this ransomware. Now, on the back end, essentially what happened is Baltimore looks like they hadn't patched for some of the malware that was able to take over their systems. And one particular aspect of the attack that caught our attention was that contractors on site - and we know there are at least four contractors on site - picked up, in one case, a hacking tool on Baltimore's network called EternalBlue that was stolen from the National Security Agency a couple of years ago. Now, that tool has been used in ransomware attacks all over the country recently. So some of the other places it's popped up were an attack in San Antonio, Texas, and an attack in Allentown, Pa. , which were very different from Baltimore's but enough that they caused significant damage, at least in Allentown's case as well. DAVIES: When this happens, it seems the cities don't say very much about this. Is this PR damage control or is there a good operational reason for being so reticent to talk about it? PERLROTH: I think there's been quite a bit of victim shaming for a long time that if you get hit by one of these attacks, it's your fault, that there's something you did to not adequately protect your systems. And I think a lot of the victims come at this from a defensive crouch, basically. But the reality is we're getting to a point where most cities are facing a million of these attacks every week - not successful in many cases, but this is now what local municipalities are up against. And in Baltimore's case, this became particularly sensitive because one of the hacking tools that was found on its network was a tool that was stolen from the NSA for which there was a patch made available almost two years ago. So there was some culpability on Baltimore's part that they had not kept their software up to date or patched their systems for the vulnerability that was exploited on its network. And I think that's where you start to see some of this defensiveness come into play. DAVIES: A patch being a software update from, you know, Microsoft or one of the software companies they were using - you said cities are suffering millions of attacks. Does that mean millions of individual probes, like phishing emails, that kind of thing? PERLROTH: Exactly, millions of probes or incidents or types of malware trying to hit their systems. Now, many of these are not successful or they don't get past the firewall or they try to exploit a vulnerability in software that's been patched already, but this is what cities are up against. And in many cases, cities just don't have the big budgets for security that, say, J. P. Morgan or Bank of America do, which get hit with even more attacks but not too many more than local cities these days. And the reason that cybercriminals have targeted American cities - and I should say this is happening all over the world - is that so many critical systems are now connected to the Internet. And cities are in a place where they're managing these sprawling networks. In most cases, they don't even know what's on their network. And when they get hit by a ransomware attack, the immediate instinct is just pay them. Just pay the attackers so that we can go back to our business, when, in reality, cities are tied up in red tape. They can't pay these attacks or if they did, they would have to do so very quietly. And in a city like Baltimore, which is struggling with gun violence and monitoring street drugs, it's a really hard sell to say, we're going to allocate some of the budget to pay off a group of cybercriminals so we can get our data back. In the case of Baltimore, it's got into a place where the attack has been so public, city officials were left in a position where they felt like they couldn't pay. And what's interesting is the ransom that was demanded was something like $100,000, but the losses from lost revenue and the cost of remediation cleaning up from that cyberattack is now nearing $20 million. DAVIES: Are cities or, you know, municipal authorities paying ransoms? Do we know? PERLROTH: Many quietly are. And the reason we know that is because a lot of these ransoms come in the form of bitcoin demands. And if you look at some of the bitcoin wallets that belong to the attackers, you are seeing quite a few transactions. So we know in some cases, police departments have come out and said, we had to pay this ransom. Hospitals, companies have been paying these ransoms. And the fact is, sometimes, it's just a lot cheaper to pay the ransom than to deal with the fallout of a huge attack that wipes all your records and all your data clean and paralyzes your networks. DAVIES: And I don't know if you can give a general answer to this question, but what are the stakes if a city refuses to pay the ransom and simply decides it's going to repair the damage, restore the files? I mean, do they have to rebuild records by doing paper entries or are things lost forever? What happens? PERLROTH: In some cases, cities have been able to recover their files through backups or other channels. But the fact remains a lot of data gets lost. And in many cases, it's not just that the data gets lost. It's the lost revenue from these attacks where people have not been able to close deals; people have not been able to collect parking ticket payments. They haven't been able to collect water bills. And the city essentially, in some cases, loses its tally of who has paid what. DAVIES: And do we know who hit Baltimore or Atlanta or Dallas? I mean, do we know if it's one person or group of people that are - you know, have these tools and are going from town to town? PERLROTH: Well, what's interesting is, in Atlanta's case, there were indictments that named a very famous group of ransomware criminals that was called the SamSam group. And the SamSam group was known for demanding pretty high ransom demands and also being pretty meticulous about the data that they locked up on systems. They really, in many cases, left victims no choice but to pay or lose all their data. And what we didn't know until these indictments came out last year was that the SamSam group was actually a team of hackers in Iran. Now, there was no clear nexus between this group of Iranian cybercriminals and the state. But this is something they were doing from Iran for profit. Now, in Baltimore, we still don't know. There's someone online who's taken credit for the attack. And I've heard from some people who track the dark Web that they believe it's an individual in Turkey. But we just don't know yet. In some of these other cities we may never know. And that's sort of the danger of the Internet is that criminals and nation-states route these attacks through servers all around the globe. And attributing them to one group or one nation-state can take a very long time. And in some cases, it's just impossible. DAVIES: When you pay the ransom, can you be sure that you'll get the files back? What does experience tell us? PERLROTH: The experience tells us no. There are some groups who reliably unscramble your data. We know that. We know that the SamSam group, the group I just referred to from Iran, that they did unlock data in their ransomware attacks when victims did pay. But there are many, many other groups that do not. And that's part of the decision-making process that these victims have to deal with is, if they pay and - especially in cities - if they get through that red tape where they can approve that payment and they can budget for that payment - and in a city like Baltimore that's already struggling with problems far bigger than their security budgets, it's a real dilemma. If they pay, will the criminals on the other end unlock their systems? In some cases, these ransomware groups have a reputation to keep up, and they will reliably decrypt your systems, but in many cases, they won't. DAVIES: Nicole Perlroth is a cybersecurity correspondent for The New York Times. We'll continue our conversation after a short break. This is FRESH AIR. (SOUNDBITE OF PATTI SMITH'S \"SMELLS LIKE TEEN SPIRIT\") DAVIES: This is FRESH AIR, and we're speaking with Nicole Perlroth. She is a cybersecurity correspondent for The New York Times. You have reported that the attack on Baltimore and others were done using a cyberweapon that was developed by the NSA, the National Security Agency. Tell us a little about this. What is the weapon, and how did it get in the wrong hands? PERLROTH: Well, for decades now, the National Security Agency has been essentially looking for vulnerabilities in software - often in Windows software, just because it gets used so much around the world - but obviously in firewalls and in apps and in your iPhone software. And the reason they're looking for those vulnerabilities is that if they find one and they can write code to exploit that vulnerability, it essentially gives them a backdoor into that system. And often what they're using it for is just espionage - to collect information off those systems. So for years, the NSA has been looking for these vulnerabilities. They've been writing the code to exploit them. And they've been holding onto them in what we describe as sort of a stockpile of of hacking tools. Now, the problem with this is that, as opposed to, say, 30 years ago when we were just trying to get in to, say, the systems that Russia used at the Russian Embassy, that's no longer the case that Russia is using some different technology and we're using different technology here in the United States and China is using different technology. For the most part, we're all using the same technology. So if the NSA is finding a vulnerability in that technology, they're not just finding it in Russia's systems or China's systems, they're finding it in technology that gets used by a lot of Americans. So over the last 10 years, the White House has tried to set up a process for discussing which of these vulnerabilities the NSA should turn over and which should be kept because they're so valuable to the NSA's intelligence-gathering mission. DAVIES: And when you say whether they should turn them over, you mean what? PERLROTH: Turn them over to companies like Microsoft, whose software was vulnerable in the first place, so that Microsoft can then patch that vulnerability and roll it out to their customers all over the world so they're no longer vulnerable to attacks that can make use of that vulnerability. DAVIES: So Microsoft could send out an update so that, heaven - you know, you've got this vulnerability you didn't know about, it can be fixed provided you upload the update. PERLROTH: Exactly. And over the last decade or so, the White House set up a process for this - to decide, OK, which of these vulnerabilities are so valuable we're going to keep them for intelligence-gathering? And which of these vulnerabilities, if discovered by one of our adversaries or by cybercriminals, could be exploited and cause so much damage or potential damage to American interests that it's far more logical that we would turn it over to a Microsoft, an Apple or Facebook or Cisco to patch just because the potential damage to American interests could be so great. And what's interesting is that as we've all sort of connected everything we possibly can to the Internet, more and more United States agencies have sent representatives into this process. So whereas in the beginning it might just be representatives for the intelligence agencies and the Department of Homeland Security, who were a part of these discussions, now we know that there needs to be representatives from the Treasury, representatives from the Department of Health because so many - so much of the software touches hospitals and banks and trading systems. And if there is a vulnerability in that software that could be exploited to harm those interests, well, then those agencies need to be a part of that debate. DAVIES: So it seems in this case, the NSA decided it had discovered this vulnerability. And it was in a Microsoft operating system, right? And they decided to keep it a secret so that they could use it. What happened? PERLROTH: Well, we know that there was actually quite a bit of work that went into not only finding this vulnerability, which was just in a Microsoft Windows protocol. There was actually quite a bit of work - a team that spent many man hours making the code to exploit this in a way that wouldn't crash computer screens on the other end. And that was not easy. That took quite a bit of work by the NSA's engineers and algorithms to develop and exploit that could reliably hack into these Microsoft systems. So part of what we learned in our reporting was that once the NSA had honed this, they considered it to be one of the most-valuable tools in their arsenal. They said it netted some of the best counter intelligence that they had and was - played a role in some of the biggest counterterrorism investigations that the NSA was participating in. So they actually said at no point did they consider turning this over to Microsoft for patching because it just played such a big role in their mission. So fast-forward to 2016, a new group comes online. They call themselves The Shadow Brokers. We still have no idea who they are. And the group essentially starts threatening the NSA online and says, we have some of your hacking tools. They allude to some of the tools that they've stolen. And they begin to auction them for sale online. Now, no one comes forward to pay for these things, in part for obvious reasons. But one year later, in early 2017, the group just goes ahead and dumps these tools online. DAVIES: The program was called EternalBlue. PERLROTH: Yeah. So EternalBlue was, like I said, one of the most-important tools in the NSA's arsenal. And to be fair, the NSA was using this tool for espionage. They weren't using it necessarily to destroy computers or to send out cyberattacks that were going to destroy data or paralyze networks. But that same tool could be picked up and used by nation-states and cybercriminals and built onto their own ransomware or malware for whatever they sought to do. So what happened was - the first thing we saw was an about May of 2017. North Korean hackers took the NSA's stolen tool, EternalBlue, and put it on to some ransomware that that researchers called WannaCry. And they sent it around the world. And within about 24 hours, they had paralyzed the British health system and hit some 200,000 other victims - just in the first 24 hours. DAVIES: What was their purpose? PERLROTH: Well, in that case, the malware that they were sending around was ransomware. Now, unfortunately, the North Koreans hadn't put enough thought into this ransomware, and it wasn't that effective. In fact, if you paid for - to unlock your systems - and some did - the bitcoin wallet wasn't functioning. And no matter what, you wouldn't have gotten your data back. But clearly there was a profit incentive here. And it was pretty interesting to see a nation-state send out ransomware like that. And it's not - had not been unprecedented. We'd seen this from hackers in Iran before. But it was such a wide-scale attack, and it paralyzed so many systems that it was a huge embarrassment to the National Security Agency. DAVIES: Nicole Perlroth is a cybersecurity correspondent at The New York Times. After a break, we'll hear more about the cyberweapons stolen from the NSA and how they're being used around the world. And she'll talk about evidence that Russian hacking may have impaired voting systems in one North Carolina county in the 2016 presidential election. Also, Justin Chang reviews the new film \"The Last Black Man In San Francisco. \" I'm Dave Davies, and this is FRESH AIR. (SOUNDBITE OF FAREED HAQUE AND KAIA STRING QUARTET'S \"QUINTET FOR GUITAR AND STRINGS - ALLEGRO VIVACE\") DAVIES: This is FRESH AIR. I'm Dave Davies in for Terry Gross, who's off this week. Let's get back to my interview with New York Times cybersecurity correspondent Nicole Perlroth. She's written recently about ransomware attacks, which have paralyzed computer networks in several American cities. When we left off, we were talking about how some hacking tools developed by the NSA have been stolen by outside groups and used in cyberattacks, including one on the city of Baltimore. One of those NSA tools, which was stolen and posted on the Internet by an anonymous group called The Shadow Brokers, is called EternalBlue. EternalBlue was, essentially, a way to get into a lot of computer systems. And once you had that tool, there are different kinds of mischief you could do once you penetrated it, right? - could be ransomware. PERLROTH: Right. DAVIES: It could be destroying files, whatever. PERLROTH: Yeah, and it's not just a way to get into these systems. It's a way of spreading within these systems, so you can think of it almost like a way to supercharge your ransomware so it makes it out to as many systems as possible. And what happened a month later, after North Koreans put EternalBlue onto their ransomware, is Russia picked up EternalBlue. And this time, they attached it to ransomware that they aimed at Ukraine. And it was a hugely successful attack. In fact, we now know that this was, perhaps, the most destructive, costliest cyberattack in history. We're talking about tens of billions of dollars of damages. And what was interesting that time - and the attack against Ukraine was called NotPetya but by some researchers who reverse engineered the code. What was interesting in that case was it didn't just hit Ukraine. It hit any business that had a remote employee in Ukraine or had an office in Ukraine or worked with a contractor there. So we saw Mondelez - giant snack maker, makes Oreo cookies. They got hit by the NotPetya attack and have now reported something like $600 million in damages, so this was not something that was just an attack by Russia on Ukraine. This was an attack that boomeranged back to American companies and companies all over the world, where the damage was caused, in large part, by an American-made hacking tool - EternalBlue. DAVIES: Wow. Now, last month, you and a couple of other reporters - David Sanger and Scott Shane - reported that the Chinese had acquired some NSA hacking tools and repurposed them to attack U. S. allies and private companies. Is this EternalBlue or is this something different? PERLROTH: It's something different, but the exploit that they used was also among the tools that The Shadow Brokers dumped online in 2017. And what was interesting in this case is that it turns out that a group we had been tracking in China - very sophisticated Chinese contractor group that, in the past, had hacked a lot of aerospace, satellite space technology, even nuclear propulsion technology - had somehow discovered the NSA's hacking tool, we think, in an attack on China's own systems. And they took it, and they repurposed it for their own attacks on American interests and allies all over the globe. So in that case, what was interesting was it's not like they took the tool after it was dumped online by The Shadow Brokers. They actually discovered it in the course of an NSA operation, picked it up, reverse engineered it and bolted it onto their own malware for their own espionage operations. DAVIES: So a weapon aimed at them, they effectively captured and turned it around and used it against their attacker. PERLROTH: Exactly. And what's interesting about that case is it raises questions about, how safe can the NSA keep its most valuable cyberweapons? So clearly, in the case of Shadow Brokers, we still don't know how Shadow Brokers got the NSA's tools. In fact, we still don't know, two years later, who The Shadow Brokers are. Initially, we thought maybe it was Russia that had hacked the NSA somehow. We've heard that the investigation has started to focus on a potential NSA mole, but we still don't have answers there. And that's something that, I think, we need to see more accountability from the NSA on. For right now, the NSA has yet to even acknowledge that the tools that were dumped online belonged to the agency. But what was interesting about the Chinese case is it's not as if the Chinese took this tool after it had been dumped online. They just took it in the course of the NSA's routine operations, which begs the question is - once you use these tools, can you ever really control them? DAVIES: The Shadow Brokers have posted on social media, right? What are their posts like? Do they give us any clues as to who they might be? PERLROTH: So The Shadow Brokers - they write these posts and these demands in very poorly broken English. And it honestly sounds like someone who is not Russian trying to sound Russian (laughter), so we have no idea who they are. But, essentially, they're very menacing. They've actually picked out individuals who used to work at the NSA and made fun of them alongside some of their dumps of the NSA hacking tools. And some of the posts that they made have made other NSA analysts or, I should say, former NSA analysts start to think that this is someone who had very deep operational knowledge of the agency and the way it worked and knew things about the agency that could not have been gleaned just out there on the Internet. It's stuff that they would have had to have had some deep knowledge of the inner workings of the agency. And I think, in part, that's why this investigation has gone from assuming that this was a Russian operation to potentially an insider. DAVIES: You know, as we see a circumstance here where this really important National Security Agency of the United States has developed these weapons, which have fallen into the wrong hands, I mean, it really does raise policy questions, right? I mean, they say war is an instrument of policy. Cyberwar is another instrument of policy. Are people at the top asking fundamental questions like, should we be developing weapons like this? Or is there a way for international regulation of weapons like this, as we've seen with, you know, nuclear treaties? PERLROTH: There are some voices in the wilderness calling for a sort of digital version of the Geneva Convention. And the leading voice out there is actually Brad Smith, the president of Microsoft. Now, Microsoft has a stake in this, which is that it's Microsoft software that's being infiltrated in so many cases by nation-states for foreign espionage simply because Microsoft has such a great market share that it's the leading target for a lot of these nation-state espionage operations. And what Microsoft has said is that, essentially, what happened with EternalBlue and The Shadow Brokers leak is that the NSA left a missile out there for anyone to pick up, and they didn't adequately protect these missiles. And now American companies and American citizens are paying for that situation. DAVIES: Nicole Perlroth is a cybersecurity correspondent for The New York Times. We will continue our conversation after this short break. This is FRESH AIR. (SOUNDBITE OF MUSIC) DAVIES: This is FRESH AIR, and we're speaking with Nicole Perlroth. She is a cybersecurity correspondent for The New York Times. I want to talk a bit about election security. You know, there's a general feeling, I think, that there was Russian interference in the 2016 presidential campaign in the form of hacked emails and disinformation campaigns but that the voting process itself was not tampered with. You've been on this beat a while and have been looking into this. What did you find? PERLROTH: Well, we found that there is a dearth of serious forensic investigation that investigated problems from the 2016 election. And there were issues, particularly in North Carolina, that suggest that there actually were quite a few problems tied to issues with electronic pollbook systems, the systems that check you in when you go to check in at the voting booth. And in many cases, these pollbooks were telling people that they had already voted when they hadn't, that they were not registered to vote when they were. And some of those pollbooks were managed by a company called VR Systems that, we know from leaked NSA documents, was in fact hacked by Russia prior to the 2016 election. DAVIES: And was there any pattern to which communities or precincts these problems occurred? PERLROTH: There was. Durham County, in particular, had a lot of problems with its e-poll book systems. Now, if you were going to try to disenfranchise a large number of Democratic voters in North Carolina, you'd probably go right to Durham County. This is a blue county in a largely red state. And when people went to go vote in Durham County, they were finding a lot of irregularities with the e-poll book systems. So over a year ago, we wrote about those problems. And what was really disturbing is that when we tried to find whether there had been an in-depth forensic investigation of the e-poll book issues in Durham County, I found a report that was conducted that was unlike any other cyberforensics report I had ever seen. Usually, when you look at these forensic reports, they tell you, you know, we did an analysis of this computer. We found this vulnerability. We found this malware or we didn't find this malware or we found this hacking technique or we didn't find this hacking technique. This read very differently. It read like a police report, where whoever was conducting the investigation was a local detective, former police officer, who said, at 3:15 p. m. , I interviewed Suzy (ph), who was working at the voting booth, and she said all was normal. I mean, I've never seen a cybersecurity investigation report look like that. And when we asked North Carolina to sort of account for this or to take a deeper look, they were pretty defensive about the issues that had happened in Durham County. And only now, a couple of years later, have we found out that, in fact, VR Systems - the company that was hacked by China - did remotely access the e-poll book systems in Durham the night before the 2016 election to try and diagnose some problems it was seeing. And that remote access could have very well been exploited by nation-state hackers. We just don't know. DAVIES: Right. And so, again, we're talking about the electronic pollbooks. That's essentially the registry of electors in a particular polling place. And the company that managed them, VR Systems - you say that we know that it actually was penetrated by Russian hackers. What exactly do we know about that? PERLROTH: We know this from leaked NSA documents that VR Systems was compromised in some kind of spear phishing-attack - so when employees open a malicious email attachment or click on a malicious link that allows malware into their systems. And we know that VR Systems maintained remote access to the e-poll books in Durham and many other counties all over the country - in Florida and elsewhere. And what we don't know is, was that access exploited by Russian hackers to disenfranchise voters? We still don't know. And only now do we know that DHS, the Department of Homeland Security, is conducting a forensic examination of those e-poll book issues in Durham County. DAVIES: And what does VR Systems say? PERLROTH: VR Systems hasn't said much. I think the last time we spoke to them, they denied that they had been phished. They have sort of resisted what was leaked in the NSA documents that suggested it was successfully phished. And they've said they're cooperating with investigators. But beyond that, we really don't know what actually happened there. DAVIES: Authorities actually identified the person in the NSA who leaked this report that VR Systems had been hacked. You want to tell us that story? What became of her? PERLROTH: Right. So we may have never known about this if not for a young NSA employee by the name of Reality Winner, and that is her actual name. She leaked NSA documents that confirmed VR Systems had been hacked in a Russian cyberattack to The Intercept - a digital publication run by Glenn Greenwald. And The Intercept actually published the leaked documents and did it in a way that the NSA was able to trace the leak pretty easily back to Reality Winner. Now, she's since been sentenced to more than five years in prison under the Espionage Act for leaking those documents. DAVIES: You've also written that there's evidence of Russian hacking in the 2018 midterm elections. Are the FBI and American security officials putting more resources into dealing with foreign interference in 2020? Is Congress doing anything? PERLROTH: I wish I could say yes. The reality is that there's been a lot of red tape and a lot of politics around securing the next election. Now, that's not to say nothing's been done. We know that U. S. Cyber Command, that U. S. military hackers, going into the 2018 election, conducted a cyberattack that shut down servers that belonged to Russia's Internet Research Agency to sort of preemptively shut down any kind of Russian interference. We also know Claire McCaskill and other Democratic senators were targeted by spear phishing attacks ahead of the 2018 midterm elections, although they say that the attacks weren't successful. Now, the problem going into 2020 is the same phishing attacks that targeted John Podesta and others in the 2016 election are probably happening right now. And what has happened is a lot of candidates have reached out to cybersecurity firms and services and said, please help us defend against these phishing attacks. We know they're coming. We know their targets. We need help. And what has been incredibly frustrating, I think, for these candidates and for the cybersecurity community and anyone whose eyes are open enough to see that 2020 is going to be huge battleground for cyberattacks and disinformation is that campaigns have not been able to accept help from many cybersecurity firms that defend against phishing attacks and other forms of cyberattacks because those services are considered an in-kind donation and illegal because of federal campaign finance laws. And so we just had a case where there was a Silicon Valley company that asked the Federal Election Commission to make an exemption so that it could help campaigns defend against phishing attacks. And so far, the Federal Election Commission has said that they would not actually make an exemption, although we're still waiting on a final ruling. DAVIES: That's so remarkable. The Federal Election Commission seems to never make a decision. (LAUGHTER) DAVIES: But they'd made a decision about this. Anything happening in Congress on this issue? PERLROTH: No, and we have one man to thank for that, and his name is Mitch McConnell. Mitch McConnell has said he does not plan to bring any election security bills to the Senate floor, period. We know that the House is trying to come up with some work arounds for this, but many of the bills that would just establish basic security norms and things like paper backups of votes have been proposed by Senator Ron Wyden and others are really stalled in the Senate right now because Mitch McConnell has said he refuses to bring them to a Senate vote. DAVIES: What's McConnell saying about why he's doing this? PERLROTH: Well, in general, McConnell has said that there's no reason to bring these measures to the Senate floor, that states should play the biggest role in securing elections and that the federal government should not get involved. But behind the scenes, we're told that a lot of this just has to do with the optics, that the president doesn't want to even hear about election security because, apparently, according to people we've spoken with, he thinks any focus on election security or the Russian interference in the 2016 election takes away from the legitimacy of his victory in 2016. DAVIES: Well, Nicole Perlroth, thanks so much for speaking with us. PERLROTH: Thanks so much for having me, Dave. DAVIES: Nicole Perlroth is a cybersecurity correspondent at The New York Times. Coming up, Justin Chang reviews the new film \"The Last Black Man In San Francisco. \" This is FRESH AIR. (SOUNDBITE OF RAY CHARLES' \"DOODLIN'\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-15-728256381": {"title": "New Auto Safety Features Can Make Car Insurance More Expensive : NPR", "url": "https://www.npr.org/2019/06/15/728256381/why-safer-cars-dont-lead-to-cheaper-car-insurance-yet", "author": "No author found", "published_date": "2019-06-15", "content": "SCOTT SIMON, HOST: If you're in the market for a new car, you might have noticed there are a lot of safety features available these days. (SOUNDBITE OF MONTAGE)UNIDENTIFIED PERSON #1: It can even help you avoid a collision. UNIDENTIFIED PERSON #2: . . . Keep you in your lane. UNIDENTIFIED PERSON #3: Automatic high beam is. . . UNIDENTIFIED PERSON #4: Automatic emergency braking. . . SIMON: All these features reduce the risk of crashes, so you might assume that would lead to cheaper car insurance. Well, no. In fact, all that safety tech can make your premiums more expensive. NPR's Camila Domonoske explains. CAMILA DOMONOSKE, BYLINE: Some new cars can automatically hit the brakes, steer around obstacles or peak into blind spots. These features prevent accidents. But they also make accidents more expensive. Consider your headlights. SCOTT WALLISCH: So a lot of vehicles are moving towards adaptive headlights that'll, you know, kind of look around the corner or, you know, are LED and are very bright. DOMONOSKE: Scott Wallisch is an auto pricing director with American Family Insurance. He says the benefit of these headlights is obvious; it makes it easier to see. So you're less likely to hit something, but. . . WALLISCH: You know, if a headlight gets in an accident, it used to be $200 to replace it. Now it's $2,000 to replace that same headlight because it is adaptive, and it is, you know, LED. DOMONOSKE: It's the same story for other safety features. If your car is watching your blind spot, let's just say the technology on your side mirrors will be pricier than it appears. And sensors that help your vehicle detect pedestrians - they bump up the cost of your bumper. So a safer car will get in fewer crashes, but each one costs more. MICHAEL KLEIN: And at least thus far, the improvements in safety and accident avoidance hasn't been significant enough to overtake the increase in costs to repair vehicles. DOMONOSKE: Michael Klein is the president of personal insurance at Travelers. He says, yes, that means premiums go up. But he says that shouldn't dissuade anyone from choosing safer vehicles. KLEIN: Not all the incentives are economic, right? I mean, if you're a person in the market for a vehicle and you have the opportunity to buy a vehicle that has features that should make it safer and make it less likely you're going to get into an accident, that ought to be worth something to you. DOMONOSKE: It's also important to note that policies vary. Carmen Balber of Consumer Watchdog says it's crucial to shop around. CARMEN BALBER: Our research has shown that some auto insurance companies do give consumers discounts for having these safety features. DOMONOSKE: You might find a deal. BALBER: But you may have to look around. And they vary state by state. DOMONOSKE: Automatic emergency braking, where the car hits the brakes if it predicts a crash, is more likely to get you a discount, but you still can't count on it. This could change in the future. The fancy, new technology could get cheaper; it often does. The features could get more popular and reduce accidents more dramatically. Or insurers might simply need more time to understand just how effective the new safety features are. Amy Bach runs United Policyholders, a nonprofit representing consumers. She says insurers are slow to change course. AMY BACH: Insurance, you know, because it's about risk, insurers tend to be cautious. DOMONOSKE: Insurance companies make decisions about rates now based on what they know about the past. That's how insurance works. Tom Karol, who works at a group representing mutual insurance companies, says some of this technology is brand new. TOM KAROL: Not only new but evolving. DOMONOSKE: A feature might work one way this year then get updated next year. And carmakers don't always like to share details about their proprietary tech. KAROL: It's very difficult to get data on a moving target like that. DOMONOSKE: But he says insurers are working on it. Camila Domonoske, NPR News. (SOUNDBITE OF PRINCE'S \"CINNAMON GIRL\") SCOTT SIMON, HOST:  If you're in the market for a new car, you might have noticed there are a lot of safety features available these days. (SOUNDBITE OF MONTAGE) UNIDENTIFIED PERSON #1: It can even help you avoid a collision. UNIDENTIFIED PERSON #2: . . . Keep you in your lane. UNIDENTIFIED PERSON #3: Automatic high beam is. . . UNIDENTIFIED PERSON #4: Automatic emergency braking. . . SIMON: All these features reduce the risk of crashes, so you might assume that would lead to cheaper car insurance. Well, no. In fact, all that safety tech can make your premiums more expensive. NPR's Camila Domonoske explains. CAMILA DOMONOSKE, BYLINE: Some new cars can automatically hit the brakes, steer around obstacles or peak into blind spots. These features prevent accidents. But they also make accidents more expensive. Consider your headlights. SCOTT WALLISCH: So a lot of vehicles are moving towards adaptive headlights that'll, you know, kind of look around the corner or, you know, are LED and are very bright. DOMONOSKE: Scott Wallisch is an auto pricing director with American Family Insurance. He says the benefit of these headlights is obvious; it makes it easier to see. So you're less likely to hit something, but. . . WALLISCH: You know, if a headlight gets in an accident, it used to be $200 to replace it. Now it's $2,000 to replace that same headlight because it is adaptive, and it is, you know, LED. DOMONOSKE: It's the same story for other safety features. If your car is watching your blind spot, let's just say the technology on your side mirrors will be pricier than it appears. And sensors that help your vehicle detect pedestrians - they bump up the cost of your bumper. So a safer car will get in fewer crashes, but each one costs more. MICHAEL KLEIN: And at least thus far, the improvements in safety and accident avoidance hasn't been significant enough to overtake the increase in costs to repair vehicles. DOMONOSKE: Michael Klein is the president of personal insurance at Travelers. He says, yes, that means premiums go up. But he says that shouldn't dissuade anyone from choosing safer vehicles. KLEIN: Not all the incentives are economic, right? I mean, if you're a person in the market for a vehicle and you have the opportunity to buy a vehicle that has features that should make it safer and make it less likely you're going to get into an accident, that ought to be worth something to you. DOMONOSKE: It's also important to note that policies vary. Carmen Balber of Consumer Watchdog says it's crucial to shop around. CARMEN BALBER: Our research has shown that some auto insurance companies do give consumers discounts for having these safety features. DOMONOSKE: You might find a deal. BALBER: But you may have to look around. And they vary state by state. DOMONOSKE: Automatic emergency braking, where the car hits the brakes if it predicts a crash, is more likely to get you a discount, but you still can't count on it. This could change in the future. The fancy, new technology could get cheaper; it often does. The features could get more popular and reduce accidents more dramatically. Or insurers might simply need more time to understand just how effective the new safety features are. Amy Bach runs United Policyholders, a nonprofit representing consumers. She says insurers are slow to change course. AMY BACH: Insurance, you know, because it's about risk, insurers tend to be cautious. DOMONOSKE: Insurance companies make decisions about rates now based on what they know about the past. That's how insurance works. Tom Karol, who works at a group representing mutual insurance companies, says some of this technology is brand new. TOM KAROL: Not only new but evolving. DOMONOSKE: A feature might work one way this year then get updated next year. And carmakers don't always like to share details about their proprietary tech. KAROL: It's very difficult to get data on a moving target like that. DOMONOSKE: But he says insurers are working on it. Camila Domonoske, NPR News. (SOUNDBITE OF PRINCE'S \"CINNAMON GIRL\")", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-17-733497829": {"title": "Brands Push Back On Partnering With Social Media Influencers : NPR", "url": "https://www.npr.org/2019/06/17/733497829/brands-push-back-on-partnering-with-social-media-influencers", "author": "No author found", "published_date": "2019-06-17", "content": "AUDIE CORNISH, HOST:  Now for All Tech Considered. All this month we're looking at the growing role of social media influencers in our economy. Brands are expected to spend up to $10 billion on influencer-related marketing by next year. There is, though, a growing backlash. NPR's Julie McCarthy takes us to the eye of the storm, the White Banana bar on an idyllic island in the Philippines. JULIE MCCARTHY, BYLINE: With its gin-lined shelves, beanbags and barefoot surfers, the White Banana bar overlooks the Pacific. Italian Gianluca Casaccia, owner and manager of this hipster watering-hole-cum-guesthouse made a splash when he denounced proposals he'd received like this one. GIANLUCA CASACCIA: Hi, I'm coming with my girlfriend and a friend the 23 to the 27 of July. We will need three rooms, food, accommodation for three days, and I put you in two stories and two posts. MCCARTHY: Such entitled-sounding inquiries landing in his email drove Casaccia to Facebook to post, no more collaborations with self-proclaimed influencers. It went on. UNIDENTIFIED PERSON: And we would like to suggest for them to try another way to eat, drink or sleep for free - or try to actually work. UNIDENTIFIED ANNOUNCER: That was heat No. 2 of the airborne competition. Coming up next, heat No. 3. MCCARTHY: Casaccia touched a nerve on the teardrop-shaped island of Siargao, fabled for its surfing. Mayan Benedicto leans on the railing of the Cloud Nine viewing pier and soaks up a recent Sunday competition. UNIDENTIFIED ANNOUNCER: Surfers in three, two, one. . . (SOUNDBITE OF SIREN)MCCARTHY: A travel and food blogger with more than 11,000 Instagram followers, Benedicto says she's in the industry, not for a glamorous lifestyle but to share her love of travel. MAYAN BENEDICTO: Because I want to encourage Filipinos to travel, I post a lot on Facebook. Both Instagram and Facebook, I have people messaging me, being inspired by my photos and inspired by my travels. And that's already enough for me. MCCARTHY: Unlike the people who emailed Casaccia, she says her policy is no freebies. BENEDICTO: Because then there's this idea that they have control of what you say, or you're pressured to say what they want you to say. And I don't believe in that. (SOUNDBITE OF HAMMERING)MCCARTHY: As repairmen hammer a new roof onto a lodging along a cafe and resort-lined strip of Siargao, Mark Roa, part owner, waves us in. A travel blogger and island tour operator, Roa says he's not surprised that bar owner Casaccia's Facebook post elicited 14,000 likes. He liked it too but says the influencer community was stung by the bar's approach. MARK ROA: They were trying to generalize that influencers are, like, the cancer of social media. They're, like, the - (laughter) - that's how it sounded. But the influencers and business entities can actually work harmoniously. MCCARTHY: Forty-five-year-old Anton Diaz is among the leading social media authorities on food and travel in the Philippines. He rejects the label influencer, mostly because of the bad connotations. Diaz pioneered travel blogging here 14 years ago. And his site, Our Awesome Planet, is comprehensive - chock-a-block with photos and information. ANTON DIAZ: It's now considered as a long-form narrative. MCCARTHY: And today international brands come to him. National Geographic commissioned Diaz last year to provide a content campaign for their cruise through the Galapagos. The secret to his success. . . DIAZ: The main secret really, here, is you have to know your audience. MCCARTHY: Diaz produces content for four different audiences - the Gen X, millennials, the youngest Z Generation and active seniors. Gianluca Casaccia says he'd consider collaborating with influencers who produce quality content. He's even memorialized the industry at his White Banana bar. CASACCIA: We made a cocktail (laughter), we call it Influencer. So now it's one of the best-seller. It's very good, actually. MCCARTHY: An ounce of bourbon, a splash of Cynar, a bitter digestif and wild honey, a best-seller. Julie McCarthy, NPR News, Siargao. AUDIE CORNISH, HOST:   Now for All Tech Considered. All this month we're looking at the growing role of social media influencers in our economy. Brands are expected to spend up to $10 billion on influencer-related marketing by next year. There is, though, a growing backlash. NPR's Julie McCarthy takes us to the eye of the storm, the White Banana bar on an idyllic island in the Philippines. JULIE MCCARTHY, BYLINE: With its gin-lined shelves, beanbags and barefoot surfers, the White Banana bar overlooks the Pacific. Italian Gianluca Casaccia, owner and manager of this hipster watering-hole-cum-guesthouse made a splash when he denounced proposals he'd received like this one. GIANLUCA CASACCIA: Hi, I'm coming with my girlfriend and a friend the 23 to the 27 of July. We will need three rooms, food, accommodation for three days, and I put you in two stories and two posts. MCCARTHY: Such entitled-sounding inquiries landing in his email drove Casaccia to Facebook to post, no more collaborations with self-proclaimed influencers. It went on. UNIDENTIFIED PERSON: And we would like to suggest for them to try another way to eat, drink or sleep for free - or try to actually work. UNIDENTIFIED ANNOUNCER: That was heat No. 2 of the airborne competition. Coming up next, heat No. 3. MCCARTHY: Casaccia touched a nerve on the teardrop-shaped island of Siargao, fabled for its surfing. Mayan Benedicto leans on the railing of the Cloud Nine viewing pier and soaks up a recent Sunday competition. UNIDENTIFIED ANNOUNCER: Surfers in three, two, one. . . (SOUNDBITE OF SIREN) MCCARTHY: A travel and food blogger with more than 11,000 Instagram followers, Benedicto says she's in the industry, not for a glamorous lifestyle but to share her love of travel. MAYAN BENEDICTO: Because I want to encourage Filipinos to travel, I post a lot on Facebook. Both Instagram and Facebook, I have people messaging me, being inspired by my photos and inspired by my travels. And that's already enough for me. MCCARTHY: Unlike the people who emailed Casaccia, she says her policy is no freebies. BENEDICTO: Because then there's this idea that they have control of what you say, or you're pressured to say what they want you to say. And I don't believe in that. (SOUNDBITE OF HAMMERING) MCCARTHY: As repairmen hammer a new roof onto a lodging along a cafe and resort-lined strip of Siargao, Mark Roa, part owner, waves us in. A travel blogger and island tour operator, Roa says he's not surprised that bar owner Casaccia's Facebook post elicited 14,000 likes. He liked it too but says the influencer community was stung by the bar's approach. MARK ROA: They were trying to generalize that influencers are, like, the cancer of social media. They're, like, the - (laughter) - that's how it sounded. But the influencers and business entities can actually work harmoniously. MCCARTHY: Forty-five-year-old Anton Diaz is among the leading social media authorities on food and travel in the Philippines. He rejects the label influencer, mostly because of the bad connotations. Diaz pioneered travel blogging here 14 years ago. And his site, Our Awesome Planet, is comprehensive - chock-a-block with photos and information. ANTON DIAZ: It's now considered as a long-form narrative. MCCARTHY: And today international brands come to him. National Geographic commissioned Diaz last year to provide a content campaign for their cruise through the Galapagos. The secret to his success. . . DIAZ: The main secret really, here, is you have to know your audience. MCCARTHY: Diaz produces content for four different audiences - the Gen X, millennials, the youngest Z Generation and active seniors. Gianluca Casaccia says he'd consider collaborating with influencers who produce quality content. He's even memorialized the industry at his White Banana bar. CASACCIA: We made a cocktail (laughter), we call it Influencer. So now it's one of the best-seller. It's very good, actually. MCCARTHY: An ounce of bourbon, a splash of Cynar, a bitter digestif and wild honey, a best-seller. Julie McCarthy, NPR News, Siargao.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-18-733809352": {"title": "Facebook Announces Plans For Libra, Its Own Cryptocurrency : NPR", "url": "https://www.npr.org/2019/06/18/733809352/facebook-announces-plans-for-libra-its-own-cryptocurrency", "author": "No author found", "published_date": "2019-06-18", "content": "AUDIE CORNISH, HOST: Facebook announced today it's creating a currency that could be available next year to its 2 billion-plus users worldwide. People who use Facebook apps like WhatsApp and Messenger could make and receive payments with the new currency. NPR's Aarti Shahani has more. AARTI SHAHANI, BYLINE: An estimated 1. 7 billion adults on Earth do not have a bank account. When members of this global underclass try to send money to family or pay a bill, they can get slapped with huge fees. Facebook could have solved that problem by saying, hey, whatever country you're in, we'll let you have a wallet on Facebook based on that country's currency - say, the dollar, the Indian rupee, the Brazilian real. And we'll let you basically text money to other people for very cheap. That is not what Facebook did. DARRELL DUFFIE: They're creating a cryptocurrency. Nothing like this has been tried on this scale. SHAHANI: That's Darrell Duffie, finance professor at Stanford Graduate School of Business. Cryptocurrency is digital money not issued by a central bank or any country's government. The most popular one to date is bitcoin. The value of cryptocurrencies can fluctuate wildly by thousands of dollars in the span of months. DUFFIE: It is true that bitcoin is a wild ride for anybody that's bought or sold it in the past. But this is not at all like bitcoin. SHAHANI: The Facebook currency, which is called Libra, is being created through a non-profit association based in Switzerland. Partners include Lyft and Uber, Visa and MasterCard. The Libra is different from bitcoin in one key respect. The Libra will be backed by real assets - a basket of bank deposits and short-term securities from different governments, not just the U. S. DUFFIE: But having said that, this basket of currencies will be relatively stable. SHAHANI: That's what Duffie believes. Mark Williams is less optimistic. He's a finance professor at Boston University. In the U. S. , the dollar is backed by the full faith and credit of the U. S. government, and the currency is widely trusted. Williams says Facebook has been exposed repeatedly for failing to meet its promises - for example, with privacy. MARK WILLIAMS: Can this entity really perform the service of a government, of a central bank and maintain the trust? SHAHANI: The Libra is an ambitious experiment to make cryptocurrency mainstream. No other cryptocreator has started with such a massive user base. Facebook critics and even the company's CEO, Mark Zuckerberg, have expressed concern about how much power Facebook has over speech, online expression, information flow. If Libra succeeds, the tech giant amasses a new kind of power - financial. Here's Darrell Duffie. DUFFIE: It keeps them independent of the banks. That's probably another reason to do it. They don't need to rely on the banks to run this. SHAHANI: Facebook has also left the door open to data tracking. Company statements indicate that if customers give consent, Facebook and other parties can collect and track Libra transactions, see how people spend, use that business intelligence against competitors and to target ads - Williams. WILLIAMS: Again Facebook wins. So is privacy maintained - no. Is a new currency created - yes. Does it benefit Facebook - yes. So that doesn't sound very altruistic to me. SHAHANI: A senior U. S. lawmaker, Representative Maxine Waters, who chairs the House Financial Services Committee, says Facebook should halt development of this new cryptocurrency until Congress and regulators are able to conduct a review. Aarti Shahani, NPR News. AUDIE CORNISH, HOST:  Facebook announced today it's creating a currency that could be available next year to its 2 billion-plus users worldwide. People who use Facebook apps like WhatsApp and Messenger could make and receive payments with the new currency. NPR's Aarti Shahani has more. AARTI SHAHANI, BYLINE: An estimated 1. 7 billion adults on Earth do not have a bank account. When members of this global underclass try to send money to family or pay a bill, they can get slapped with huge fees. Facebook could have solved that problem by saying, hey, whatever country you're in, we'll let you have a wallet on Facebook based on that country's currency - say, the dollar, the Indian rupee, the Brazilian real. And we'll let you basically text money to other people for very cheap. That is not what Facebook did. DARRELL DUFFIE: They're creating a cryptocurrency. Nothing like this has been tried on this scale. SHAHANI: That's Darrell Duffie, finance professor at Stanford Graduate School of Business. Cryptocurrency is digital money not issued by a central bank or any country's government. The most popular one to date is bitcoin. The value of cryptocurrencies can fluctuate wildly by thousands of dollars in the span of months. DUFFIE: It is true that bitcoin is a wild ride for anybody that's bought or sold it in the past. But this is not at all like bitcoin. SHAHANI: The Facebook currency, which is called Libra, is being created through a non-profit association based in Switzerland. Partners include Lyft and Uber, Visa and MasterCard. The Libra is different from bitcoin in one key respect. The Libra will be backed by real assets - a basket of bank deposits and short-term securities from different governments, not just the U. S. DUFFIE: But having said that, this basket of currencies will be relatively stable. SHAHANI: That's what Duffie believes. Mark Williams is less optimistic. He's a finance professor at Boston University. In the U. S. , the dollar is backed by the full faith and credit of the U. S. government, and the currency is widely trusted. Williams says Facebook has been exposed repeatedly for failing to meet its promises - for example, with privacy. MARK WILLIAMS: Can this entity really perform the service of a government, of a central bank and maintain the trust? SHAHANI: The Libra is an ambitious experiment to make cryptocurrency mainstream. No other cryptocreator has started with such a massive user base. Facebook critics and even the company's CEO, Mark Zuckerberg, have expressed concern about how much power Facebook has over speech, online expression, information flow. If Libra succeeds, the tech giant amasses a new kind of power - financial. Here's Darrell Duffie. DUFFIE: It keeps them independent of the banks. That's probably another reason to do it. They don't need to rely on the banks to run this. SHAHANI: Facebook has also left the door open to data tracking. Company statements indicate that if customers give consent, Facebook and other parties can collect and track Libra transactions, see how people spend, use that business intelligence against competitors and to target ads - Williams. WILLIAMS: Again Facebook wins. So is privacy maintained - no. Is a new currency created - yes. Does it benefit Facebook - yes. So that doesn't sound very altruistic to me. SHAHANI: A senior U. S. lawmaker, Representative Maxine Waters, who chairs the House Financial Services Committee, says Facebook should halt development of this new cryptocurrency until Congress and regulators are able to conduct a review. Aarti Shahani, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-18-733728082": {"title": "Google Will Devote $1 Billion To Try To Tame Housing Costs In SF Bay Area : NPR", "url": "https://www.npr.org/2019/06/18/733728082/google-will-devote-1-billion-trying-to-tame-housing-costs-in-bay-area", "author": "No author found", "published_date": "2019-06-18", "content": "", "section": "National", "disclaimer": ""}, "2019-06-18-733701971": {"title": "Facebook Unveils Libra Cryptocurrency, Sets Launch For 2020 : NPR", "url": "https://www.npr.org/2019/06/18/733701971/facebook-unveils-libra-cryptocurrency-sets-launch-for-2020", "author": "No author found", "published_date": "2019-06-18", "content": "", "section": "Economy", "disclaimer": ""}, "2019-06-19-734095998": {"title": "Slack Stock Jumps On First Trading Day, Company Value Tops $20 Billion : NPR", "url": "https://www.npr.org/2019/06/19/734095998/putting-a-price-on-chat-slack-is-going-public-at-16-billion-value", "author": "No author found", "published_date": "2019-06-19", "content": "", "section": "Business", "disclaimer": ""}, "2019-06-19-731268823": {"title": "Bringing Shoe Manufacturing Back To The United States Poses Challenges : NPR", "url": "https://www.npr.org/2019/06/19/731268823/why-the-american-shoe-disappeared-and-why-its-so-hard-to-bring-it-back", "author": "No author found", "published_date": "2019-06-19", "content": "NOEL KING, HOST:  If President Trump follows through on his threat to impose more tariffs on imports from China, one industry that has a lot to lose is U. S. footwear companies. That's because most of the shoes sold here are made in China. Even more of those shoes have parts from China. NPR's Alina Selyukh has been wondering could America ever make its own shoes again? ALINA SELYUKH, BYLINE: Douglas Clark has been asking this question for decades. In the '80s, he worked for Nike as factories closed all around him in the Northeast. DOUGLAS CLARK: I'll tell you, one of the darkest parts of being at Nike was when I watched domestic manufacturing evaporate and everything go to Asia. SELYUKH: Where labor was much cheaper. What followed was a mass exodus. If you chart jobs in footwear manufacturing in the U. S. , the graph has the curve of a stiletto, a steep slope through the '90s before it levels out around fewer than 13,000 jobs in the past decade. CLARK: As a true Yankee, (laughter), and my father was the colonial historian, you know, it was heartbreaking. SELYUKH: Clark would go on to a long career in footwear - at Converse, Reebok, Timberland then his own line of shoes at New England Footwear - and eight years devoted to one mission. CLARK: Not only can we create a model for how footwear can be made back in the U. S. in a profitable way, but that that model could transfer to all the other lost industries that have left the U. S. SELYUKH: This was a tall order. At a time when the president speaks of rebuilding American manufacturing, footwear is a telling example of how hard it is to turn back time. These days, 99% of shoes sold in the U. S. are imported from countries like China, Vietnam, Indonesia. And that's the number that Clark was going up against when he went after the main reason why shoemakers left in the first place - labor costs. Here's Mike Jeppesen, head of global operations at Wolverine Worldwide, which owns brands like Merrill, Sperry and Keds. MIKE JEPPESEN: Our actual cost price in making the shoes is somewhere around $16 an hour. When we are sourcing from China today, that price is about $3 an hour. SELYUKH: More than five times more, a cost that ends up quadrupling after wholesale and retail markups. JEPPESEN: So that's a $50 price difference between a pair of shoes made in the U. S. and a pair of shoes made in China. Simple as that. SELYUKH: That's why the mass market companies, especially athletic shoes and women's fashion shoes, have focused their U. S. operations more on design and marketing, leaving all the cutting and gluing and stitching to manufacturers overseas. So Clark focused on automating and simplifying this labor-intensive work. CLARK: Instead of a shoe being made of 55 parts, we had a shoe that was made of 10, or 11 or 12 parts. SELYUKH: To be clear, shoe-making never entirely left America. But it's down to some 200 factories. The majority of them employ fewer than 10 people. For many, the U. S. military is the main customer, which has to buy made-in-the-U. S. A. But outside of that, large shoe companies will say there's little commercial reason for them to manufacture here. Smaller ones say they stay because they found a unique niche, or want to carry on the tradition or take a stand against the environmental impact of trans-Pacific shipping. To make it work, these firms rely on their shoppers choosing to pay more for bespoke quality and the made-in-America brand. NANCY RICHARDSON: We know that we can't make a $19 shoe to be sold at Target or Walmart. That's just not going to be possible for us. SELYUKH: Nancy Richardson is CEO of SAS. (SOUNDBITE OF FACTORY MACHINERY NOISE)SELYUKH: It's a mid-sized company that's been making shoes in San Antonio since the '70s, still starting the traditional way, carving the shape of a foot into wooden block called the last. (SOUNDBITE OF HAMMERING ON WOOD)SELYUKH: SAS shoes cost around $150 to $200. And the biggest challenge for SAS is finding workers. RICHARDSON: At one time in the U. S. , you might have put an ad and gotten 200 resumes. Today, you might get a handful of people. SELYUKH: U. S. factories rarely get applicants under 40. Arthritis is a common struggle. As shoe-making jobs disappeared, so did the support system. Suppliers of things like metal parts or colorful leather followed the industry overseas. Doug Clark, on his mission to return mainstream manufacturing to America, knew all this. But he also knew that history was already starting to repeat itself in China, too. Wages have been going up. Footwear companies have been moving, again chasing lower costs. This could be the opportunity for America's comeback, Clark thought. But for it to work, you need robots. A few years back, he got a contract with a big brand and a grant to make shoe manufacturing less manual starting with the top parts. CLARK: The first thing we did was we basically developed ways to make uppers that didn't involve a lot of labor. SELYUKH: Footwear manufacturing has long included machines cutting or gluing soles. But higher-level innovation? Ironically, factory owners I spoke with said that's happening where the industry is, overseas. Major brands like Nike and Adidas have been developing new technologies, including in the U. S. , but they still rely heavily on factory workers abroad. Because unlike humans, robots aren't nimble. They can't notice imperfections or quickly switch to a new fashion style. CLARK: Robots are not forgiving. SELYUKH: For Clark, the story had a frustrating end. Developing automation got very expensive and slower than expected. He ran out of money and sold his factory to a technology company which knew a lot about robots. The factory is now closed. Clark hoped his legacy would be reviving American shoe manufacturing. Instead, he's now in real estate. Alina Selyukh, NPR News. NOEL KING, HOST:   If President Trump follows through on his threat to impose more tariffs on imports from China, one industry that has a lot to lose is U. S. footwear companies. That's because most of the shoes sold here are made in China. Even more of those shoes have parts from China. NPR's Alina Selyukh has been wondering could America ever make its own shoes again? ALINA SELYUKH, BYLINE: Douglas Clark has been asking this question for decades. In the '80s, he worked for Nike as factories closed all around him in the Northeast. DOUGLAS CLARK: I'll tell you, one of the darkest parts of being at Nike was when I watched domestic manufacturing evaporate and everything go to Asia. SELYUKH: Where labor was much cheaper. What followed was a mass exodus. If you chart jobs in footwear manufacturing in the U. S. , the graph has the curve of a stiletto, a steep slope through the '90s before it levels out around fewer than 13,000 jobs in the past decade. CLARK: As a true Yankee, (laughter), and my father was the colonial historian, you know, it was heartbreaking. SELYUKH: Clark would go on to a long career in footwear - at Converse, Reebok, Timberland then his own line of shoes at New England Footwear - and eight years devoted to one mission. CLARK: Not only can we create a model for how footwear can be made back in the U. S. in a profitable way, but that that model could transfer to all the other lost industries that have left the U. S. SELYUKH: This was a tall order. At a time when the president speaks of rebuilding American manufacturing, footwear is a telling example of how hard it is to turn back time. These days, 99% of shoes sold in the U. S. are imported from countries like China, Vietnam, Indonesia. And that's the number that Clark was going up against when he went after the main reason why shoemakers left in the first place - labor costs. Here's Mike Jeppesen, head of global operations at Wolverine Worldwide, which owns brands like Merrill, Sperry and Keds. MIKE JEPPESEN: Our actual cost price in making the shoes is somewhere around $16 an hour. When we are sourcing from China today, that price is about $3 an hour. SELYUKH: More than five times more, a cost that ends up quadrupling after wholesale and retail markups. JEPPESEN: So that's a $50 price difference between a pair of shoes made in the U. S. and a pair of shoes made in China. Simple as that. SELYUKH: That's why the mass market companies, especially athletic shoes and women's fashion shoes, have focused their U. S. operations more on design and marketing, leaving all the cutting and gluing and stitching to manufacturers overseas. So Clark focused on automating and simplifying this labor-intensive work. CLARK: Instead of a shoe being made of 55 parts, we had a shoe that was made of 10, or 11 or 12 parts. SELYUKH: To be clear, shoe-making never entirely left America. But it's down to some 200 factories. The majority of them employ fewer than 10 people. For many, the U. S. military is the main customer, which has to buy made-in-the-U. S. A. But outside of that, large shoe companies will say there's little commercial reason for them to manufacture here. Smaller ones say they stay because they found a unique niche, or want to carry on the tradition or take a stand against the environmental impact of trans-Pacific shipping. To make it work, these firms rely on their shoppers choosing to pay more for bespoke quality and the made-in-America brand. NANCY RICHARDSON: We know that we can't make a $19 shoe to be sold at Target or Walmart. That's just not going to be possible for us. SELYUKH: Nancy Richardson is CEO of SAS. (SOUNDBITE OF FACTORY MACHINERY NOISE) SELYUKH: It's a mid-sized company that's been making shoes in San Antonio since the '70s, still starting the traditional way, carving the shape of a foot into wooden block called the last. (SOUNDBITE OF HAMMERING ON WOOD) SELYUKH: SAS shoes cost around $150 to $200. And the biggest challenge for SAS is finding workers. RICHARDSON: At one time in the U. S. , you might have put an ad and gotten 200 resumes. Today, you might get a handful of people. SELYUKH: U. S. factories rarely get applicants under 40. Arthritis is a common struggle. As shoe-making jobs disappeared, so did the support system. Suppliers of things like metal parts or colorful leather followed the industry overseas. Doug Clark, on his mission to return mainstream manufacturing to America, knew all this. But he also knew that history was already starting to repeat itself in China, too. Wages have been going up. Footwear companies have been moving, again chasing lower costs. This could be the opportunity for America's comeback, Clark thought. But for it to work, you need robots. A few years back, he got a contract with a big brand and a grant to make shoe manufacturing less manual starting with the top parts. CLARK: The first thing we did was we basically developed ways to make uppers that didn't involve a lot of labor. SELYUKH: Footwear manufacturing has long included machines cutting or gluing soles. But higher-level innovation? Ironically, factory owners I spoke with said that's happening where the industry is, overseas. Major brands like Nike and Adidas have been developing new technologies, including in the U. S. , but they still rely heavily on factory workers abroad. Because unlike humans, robots aren't nimble. They can't notice imperfections or quickly switch to a new fashion style. CLARK: Robots are not forgiving. SELYUKH: For Clark, the story had a frustrating end. Developing automation got very expensive and slower than expected. He ran out of money and sold his factory to a technology company which knew a lot about robots. The factory is now closed. Clark hoped his legacy would be reviving American shoe manufacturing. Instead, he's now in real estate. Alina Selyukh, NPR News.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-20-734496829": {"title": "YouTube Under Fire For Collecting Data On Young Children Without Parental Consent : NPR", "url": "https://www.npr.org/2019/06/20/734496829/youtube-under-fire-for-collecting-data-on-young-children-without-parental-consen", "author": "No author found", "published_date": "2019-06-20", "content": "ARI SHAPIRO, HOST: YouTube is something of a modern-day babysitter, and it is under fire for collecting data on young children without asking their parents first. The company may face a big fine by the Federal Trade Commission as NPR's Aarti Shahani reports. AARTI SHAHANI, BYLINE: Here's a poorly kept secret. Little kids watch YouTube a lot. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: Hey, everyone. Today we're unrolling Poopsie toilet rolls. SHAHANI: So many little kids are on, it's prompted the quirkiest cultural phenomenon - unboxing toys; taking toys out of boxes and bags one after the other. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: What do we have here? Unicorn food - such a cute, pearly bag. SHAHANI: Here is the issue. Under a federal privacy law passed in the 1990s, websites directed at kids under 13 have to get parents' consent before collecting or sharing the kids personal information. YouTube doesn't do that. The company owned by Google has an official line. You have to be 13 or older to have an account, so that kid's privacy law doesn't apply. The Washington Post has reported the FTC is in the late stages of an investigation into YouTube for possible child privacy violations and may impose a fine on the company. This comes at a time when regulators, the Justice Department and Congress are investigating the growing power of big tech. Ashkan Soltani, former FTC chief technologist, says the regulator may be responding to political pressure. ASHKAN SOLTANI: I know it's not supposed to - right? - in the same way it's not supposed to affect the Supreme Court. But I do know that there's interest in signaling they're reining in big tech. SHAHANI: Google and YouTube have been accused of leaving children vulnerable to pedophiles. Privacy and watchdog groups have, for years, charged that the companies are illegally gathering data on children and that they're targeting videos at children that would never have passed muster in the TV days. These videos are commercials paid for by toy makers, like many of the unboxing videos. Angela Campbell, a law professor at Georgetown University who's filed numerous complaints to the FTC about Google and its partners, points out they are not the only ones. ANGELA CAMPBELL: This problem has gone on for a long time, and it's just sort of been, you know, ignored. And I think, you know, it's now just gotten too big to ignore. SHAHANI: Google, an NPR sponsor, declined to comment on potential FTC action. Campbell says that after years of inaction, she's seeing a kind of bipartisan consensus forming, with Democrats concerned about privacy violations and Republicans about inappropriate content landing in front of kids' eyeballs, though some content is just silly. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: And here we actually have unicorn poop (laughter). SHAHANI: Aarti Shahani, NPR News. ARI SHAPIRO, HOST:  YouTube is something of a modern-day babysitter, and it is under fire for collecting data on young children without asking their parents first. The company may face a big fine by the Federal Trade Commission as NPR's Aarti Shahani reports. AARTI SHAHANI, BYLINE: Here's a poorly kept secret. Little kids watch YouTube a lot. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: Hey, everyone. Today we're unrolling Poopsie toilet rolls. SHAHANI: So many little kids are on, it's prompted the quirkiest cultural phenomenon - unboxing toys; taking toys out of boxes and bags one after the other. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: What do we have here? Unicorn food - such a cute, pearly bag. SHAHANI: Here is the issue. Under a federal privacy law passed in the 1990s, websites directed at kids under 13 have to get parents' consent before collecting or sharing the kids personal information. YouTube doesn't do that. The company owned by Google has an official line. You have to be 13 or older to have an account, so that kid's privacy law doesn't apply. The Washington Post has reported the FTC is in the late stages of an investigation into YouTube for possible child privacy violations and may impose a fine on the company. This comes at a time when regulators, the Justice Department and Congress are investigating the growing power of big tech. Ashkan Soltani, former FTC chief technologist, says the regulator may be responding to political pressure. ASHKAN SOLTANI: I know it's not supposed to - right? - in the same way it's not supposed to affect the Supreme Court. But I do know that there's interest in signaling they're reining in big tech. SHAHANI: Google and YouTube have been accused of leaving children vulnerable to pedophiles. Privacy and watchdog groups have, for years, charged that the companies are illegally gathering data on children and that they're targeting videos at children that would never have passed muster in the TV days. These videos are commercials paid for by toy makers, like many of the unboxing videos. Angela Campbell, a law professor at Georgetown University who's filed numerous complaints to the FTC about Google and its partners, points out they are not the only ones. ANGELA CAMPBELL: This problem has gone on for a long time, and it's just sort of been, you know, ignored. And I think, you know, it's now just gotten too big to ignore. SHAHANI: Google, an NPR sponsor, declined to comment on potential FTC action. Campbell says that after years of inaction, she's seeing a kind of bipartisan consensus forming, with Democrats concerned about privacy violations and Republicans about inappropriate content landing in front of kids' eyeballs, though some content is just silly. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: And here we actually have unicorn poop (laughter). SHAHANI: Aarti Shahani, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-21-734451600": {"title": "Ever Plugged A USB In Wrong? Of Course You Have. Here's Why : NPR", "url": "https://www.npr.org/2019/06/21/734451600/ever-plugged-a-usb-in-wrong-of-course-you-have-heres-why", "author": "No author found", "published_date": "2019-06-21", "content": "", "section": "Technology", "disclaimer": ""}, "2019-06-22-735076936": {"title": "Amazon Wins Patent Approval For Drone 'Home Surveillance' Plan : NPR", "url": "https://www.npr.org/2019/06/22/735076936/amazon-explores-having-its-drones-provide-home-surveillance-for-customers", "author": "No author found", "published_date": "2019-06-22", "content": "", "section": "Business", "disclaimer": ""}, "2019-06-22-735005802": {"title": "Hackers Use Ransomware To Attack Cities : NPR", "url": "https://www.npr.org/2019/06/22/735005802/hackers-use-ransomware-to-attack-cities", "author": "No author found", "published_date": "2019-06-22", "content": "SCOTT SIMON, HOST: American cities are being taken hostage - not by bandits in ski masks, but cybercriminals who use so-called ransomware to shut down critical systems until the cities pay up. Baltimore has spent more than a month and $18 million trying to recover from that kind of attack. And Riviera Beach, Fla. , reportedly agreed to pay almost $600,000 in ransom this week. Kate Fazzini is a cybersecurity reporter at CNBC. Thanks so much for being with us. KATE FAZZINI: Thank you very much, Scott. SIMON: What do we know? What happened in these two cities? FAZZINI: So it seems like some pretty standard ransomware was used. It's usually sent to some unsuspecting people via email. They click on a link, and it spreads very, very quickly through their entire network so that their systems are locked up, data is locked up. So there's nothing they can do until either they pay or replace all the equipment. SIMON: And how common have these attacks become? FAZZINI: Oh, they're very common. In fact, they're much more common than you might expect. The cities get highlighted a lot because, obviously, if you can't pay your utility bill, it becomes a public issue. But these are just sweeping through many different companies and organizations. They don't always have to report - in fact, they very rarely have to report that they've happened, so what you're seeing in the media is just the tip of the iceberg. SIMON: Any idea who's responsible, or does that encompass a lot of people? FAZZINI: It definitely encompasses a lot of people, a lot of nation-states. You see some of these groups sort of doing both. So we've had issues with ransomware being deployed by criminals who were also doing some work for the Iranian government or the North Korean government. It's almost impossible to tell right away, and even after a lengthy investigation, it's still very hard to tell. SIMON: So Riviera Beach, Fla. - what makes it worth their while to pay hundreds of thousands of dollars? FAZZINI: So there have been a couple of different big ransomware attacks. Obviously, you see Baltimore, Atlanta had to pay several million dollars to fix and replace their equipment and get back online. A lot of these cities lose hours and hours of productivity. It just makes financial sense for some organizations to pay the ransom. They do have to have some reassurances, though, that the information they're going to get back from the criminals who did this will actually work to unlock their computers. Also, it appears in Florida that their insurer is actually going to be covering that cost, so that's probably another incentive for them to go forward with it. SIMON: I wonder about this. If Riviera Beach pays to avoid being attacked, doesn't it make it more likely that criminal organization just attacks - and I'll make up something - Delray Beach, Fla. , the next week? FAZZINI: I think you maybe missed your calling as a cybercriminal, Scott. Criminals are always looking to, what is the formula that makes paying the ransom worth it to the person who I'm attacking? So I'm absolutely certain that you're right that you will see an uptick in at least attempts to do this in other cities. SIMON: Cities have computer experts, right? Shouldn't they be avoiding these attacks somehow? FAZZINI: I think it's a pretty sensitive issue. In the case of Baltimore, the mayor came out and publicly blamed the National Security Agency for having lost some exploits several years ago that had resulted in the attack that they sustained. But at the same time, there had been a patch available that the city hadn't put in place. You know, nationwide, there was an enormous lack of people with cybersecurity skills. City governments aren't known for their enormously high salaries. So I think you'll see the competition at tech firms, at big banks making it very hard for these cities to have enough people in place. SIMON: Kate Fazzini reports on cybersecurity at CNBC, and her new book is \"Kingdom Of Lies: Unnerving Adventures In The World Of Cybercrime. \" Thanks so much for being with us. FAZZINI: Thank you, Scott, for having me. SCOTT SIMON, HOST:  American cities are being taken hostage - not by bandits in ski masks, but cybercriminals who use so-called ransomware to shut down critical systems until the cities pay up. Baltimore has spent more than a month and $18 million trying to recover from that kind of attack. And Riviera Beach, Fla. , reportedly agreed to pay almost $600,000 in ransom this week. Kate Fazzini is a cybersecurity reporter at CNBC. Thanks so much for being with us. KATE FAZZINI: Thank you very much, Scott. SIMON: What do we know? What happened in these two cities? FAZZINI: So it seems like some pretty standard ransomware was used. It's usually sent to some unsuspecting people via email. They click on a link, and it spreads very, very quickly through their entire network so that their systems are locked up, data is locked up. So there's nothing they can do until either they pay or replace all the equipment. SIMON: And how common have these attacks become? FAZZINI: Oh, they're very common. In fact, they're much more common than you might expect. The cities get highlighted a lot because, obviously, if you can't pay your utility bill, it becomes a public issue. But these are just sweeping through many different companies and organizations. They don't always have to report - in fact, they very rarely have to report that they've happened, so what you're seeing in the media is just the tip of the iceberg. SIMON: Any idea who's responsible, or does that encompass a lot of people? FAZZINI: It definitely encompasses a lot of people, a lot of nation-states. You see some of these groups sort of doing both. So we've had issues with ransomware being deployed by criminals who were also doing some work for the Iranian government or the North Korean government. It's almost impossible to tell right away, and even after a lengthy investigation, it's still very hard to tell. SIMON: So Riviera Beach, Fla. - what makes it worth their while to pay hundreds of thousands of dollars? FAZZINI: So there have been a couple of different big ransomware attacks. Obviously, you see Baltimore, Atlanta had to pay several million dollars to fix and replace their equipment and get back online. A lot of these cities lose hours and hours of productivity. It just makes financial sense for some organizations to pay the ransom. They do have to have some reassurances, though, that the information they're going to get back from the criminals who did this will actually work to unlock their computers. Also, it appears in Florida that their insurer is actually going to be covering that cost, so that's probably another incentive for them to go forward with it. SIMON: I wonder about this. If Riviera Beach pays to avoid being attacked, doesn't it make it more likely that criminal organization just attacks - and I'll make up something - Delray Beach, Fla. , the next week? FAZZINI: I think you maybe missed your calling as a cybercriminal, Scott. Criminals are always looking to, what is the formula that makes paying the ransom worth it to the person who I'm attacking? So I'm absolutely certain that you're right that you will see an uptick in at least attempts to do this in other cities. SIMON: Cities have computer experts, right? Shouldn't they be avoiding these attacks somehow? FAZZINI: I think it's a pretty sensitive issue. In the case of Baltimore, the mayor came out and publicly blamed the National Security Agency for having lost some exploits several years ago that had resulted in the attack that they sustained. But at the same time, there had been a patch available that the city hadn't put in place. You know, nationwide, there was an enormous lack of people with cybersecurity skills. City governments aren't known for their enormously high salaries. So I think you'll see the competition at tech firms, at big banks making it very hard for these cities to have enough people in place. SIMON: Kate Fazzini reports on cybersecurity at CNBC, and her new book is \"Kingdom Of Lies: Unnerving Adventures In The World Of Cybercrime. \" Thanks so much for being with us. FAZZINI: Thank you, Scott, for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-23-735273669": {"title": "Troll Watch: Robocalls : NPR", "url": "https://www.npr.org/2019/06/23/735273669/troll-watch-robocalls", "author": "No author found", "published_date": "2019-06-23", "content": "MICHEL MARTIN, HOST: Sorry to do this to you, but we need to talk robocalls. Could they be any more annoying? AUTOMATED VOICE #1: New message - decided to suspend your social security number and file a lawsuit against you. AUTOMATED VOICE #2: You're now eligible to qualify for a zero percent interest rate. AUTOMATED VOICE #3: There are some legal enforcement actions filed on your Social Security number for fraudulent activities. AUTOMATED VOICE #4: If you wish to opt out, press 2 now. AUTOMATED VOICE #5: Message marked for deletion. There are no more messages. MARTIN: Ignore them, you've still been interrupted. Answer them, thinking it's your spouse or child or student loan lender on the line, and there's somebody trying to entice you or intimidate you into giving them money or personal information. And now the problem is moving from infuriating to potentially dangerous as robocallers have started targeting hospitals and other health facilities and patients. So we're taking this to our regular segment, Troll Watch. (SOUNDBITE OF MUSIC)MARTIN: Joining me to talk about this is Dave Summitt. He is the chief information security officer at the Moffitt Cancer Center, which is one of the health care providers that has been targeted by robocallers. He's with us now from WUSF in Tampa, Fla. Dave Summitt, thanks so much for joining us. DAVE SUMMITT: Thank you for having me. MARTIN: So when did you first realize that robocalls had moved from just being - not just, but irritating to a serious crisis that was affecting your facilities and your patients? SUMMITT: About a year and a half ago, we noticed an uptick in phone calls coming into the organization made to appear like it's coming on behalf of Moffitt, to Moffitt. So therefore, people inside of our organization is going to obviously pick up the phone because they think it's someone from inside the organization. And then, when they do, it's not someone from inside the organization. It's someone from the outside trying to either grab additional information or just scam us. MARTIN: You actually have testified to Congress about this. And you shared that in the past 90 days, Moffitt got over 6,600 external calls identified as a Moffitt internal phone number, and it took your employees 65 hours to respond to these, and that you said, over a 30-day period, there were more than 300 calls made to the center coming from Washington, D. C. And what do these people want? Like, what are they - they're trying to, what, induce people to give out other patients' personal information, or they're trying to trick patients into thinking that they're from Moffitt? What are they trying to do? SUMMITT: So there's a wide variety in these calls that are being placed. The obvious part of most of these calls are to get a person to pick up the phone in the first place. So the purpose of the spoofing from the bad guy's perspective is, the more I can make it appear like it's a legitimate phone call, the more apt the person is going to pick up the phone and start answering questions. And in our case, we were having people calling local people outside of our facility in our local area and across the state imitating that they're coming from Moffitt and attempting to get additional health insurance information or personal information. And then they would potentially use that information against that person. The other types of calls that were coming into our organization were a wide range. But the most damaging one would be the kind that is asking for physician information that they could potentially use in fraudulently taking our physician information and potentially making fraudulent Medicare claims. MARTIN: Have you reached out to other healthcare providers about this? And what have they told you? SUMMITT: So we have. Before my testimony, we did reach out to approximately 18-plus other health care organizations across the nation. And basically, 18 of them signed up with us to help bring this awareness to Congress. And our biggest concern in the health care industry is not only is it potentially disruptive to health care operations itself, but it's also possible that during these calls that there could be what's called a denial of service against our telecommunication systems. And what that would mean would be a complete shutdown of our communications. And that's one of our worst fears. And, in fact, there was a hospital within the last 12 months that had over 65,000 calls launched against their facility, which did do a denial of service. And what we're afraid of is that's going to continue. MARTIN: Do you have any insight into why this is happening? SUMMITT: For the most part, it's an easy thing to do from a bad actor standpoint, and they're making money off of it. And that's really the bottom line. MARTIN: Before we let you go, can you offer some advice for people who are dealing with this? I mean, if you think you've gotten a call from the Moffitt Cancer Center, then it just doesn't seem like a great idea to ignore it. Like, what do you suggest that people do? SUMMITT: So what we're advising and what I advise even friends when they're asking me, what do I do about these calls, really, is to know who the caller is. There's no need to answer the call immediately. Even if it's an emergency situation, typically, the person who's trying to got ahold of you will call you right back. Robocallers don't do that. If you don't answer the call the first, they move on to the next. So if you get back-to-back phone calls from the same number, it's most likely someone really trying to get in touch with you. And the second thing I really advise is that when you've answered the call, don't immediately say hello, and don't give a hello, this is and your name. That helps the other people on the other end to know even more about you. So the least amount of information you give over one of these calls the better. And the other option that you actually have is to let it go to voicemail. If it's legitimate, they're going to leave a voicemail. What I don't really recommend, which is what a lot of people are doing, are putting those numbers in their block system. Keep in mind these are spammed, spoofed calls which, means that who's calling you is not who is showing up on the caller ID, and all you're doing now is blocking the caller ID number, which could potentially be someone that really needs to get in touch with you. MARTIN: That is Dave Summitt, chief information security officer from Moffitt Cancer Center in Tampa, Fla. Moffitt provides cancer care to more than 60,000 patients each year. It's the third-busiest standalone cancer hospital in the United States. We reached him in Tampa, Fla. Dave Summitt, thanks so much for talking with us, and thanks for that advice. SUMMITT: Thank you for helping us raise this awareness. MICHEL MARTIN, HOST:  Sorry to do this to you, but we need to talk robocalls. Could they be any more annoying? AUTOMATED VOICE #1: New message - decided to suspend your social security number and file a lawsuit against you. AUTOMATED VOICE #2: You're now eligible to qualify for a zero percent interest rate. AUTOMATED VOICE #3: There are some legal enforcement actions filed on your Social Security number for fraudulent activities. AUTOMATED VOICE #4: If you wish to opt out, press 2 now. AUTOMATED VOICE #5: Message marked for deletion. There are no more messages. MARTIN: Ignore them, you've still been interrupted. Answer them, thinking it's your spouse or child or student loan lender on the line, and there's somebody trying to entice you or intimidate you into giving them money or personal information. And now the problem is moving from infuriating to potentially dangerous as robocallers have started targeting hospitals and other health facilities and patients. So we're taking this to our regular segment, Troll Watch. (SOUNDBITE OF MUSIC) MARTIN: Joining me to talk about this is Dave Summitt. He is the chief information security officer at the Moffitt Cancer Center, which is one of the health care providers that has been targeted by robocallers. He's with us now from WUSF in Tampa, Fla. Dave Summitt, thanks so much for joining us. DAVE SUMMITT: Thank you for having me. MARTIN: So when did you first realize that robocalls had moved from just being - not just, but irritating to a serious crisis that was affecting your facilities and your patients? SUMMITT: About a year and a half ago, we noticed an uptick in phone calls coming into the organization made to appear like it's coming on behalf of Moffitt, to Moffitt. So therefore, people inside of our organization is going to obviously pick up the phone because they think it's someone from inside the organization. And then, when they do, it's not someone from inside the organization. It's someone from the outside trying to either grab additional information or just scam us. MARTIN: You actually have testified to Congress about this. And you shared that in the past 90 days, Moffitt got over 6,600 external calls identified as a Moffitt internal phone number, and it took your employees 65 hours to respond to these, and that you said, over a 30-day period, there were more than 300 calls made to the center coming from Washington, D. C. And what do these people want? Like, what are they - they're trying to, what, induce people to give out other patients' personal information, or they're trying to trick patients into thinking that they're from Moffitt? What are they trying to do? SUMMITT: So there's a wide variety in these calls that are being placed. The obvious part of most of these calls are to get a person to pick up the phone in the first place. So the purpose of the spoofing from the bad guy's perspective is, the more I can make it appear like it's a legitimate phone call, the more apt the person is going to pick up the phone and start answering questions. And in our case, we were having people calling local people outside of our facility in our local area and across the state imitating that they're coming from Moffitt and attempting to get additional health insurance information or personal information. And then they would potentially use that information against that person. The other types of calls that were coming into our organization were a wide range. But the most damaging one would be the kind that is asking for physician information that they could potentially use in fraudulently taking our physician information and potentially making fraudulent Medicare claims. MARTIN: Have you reached out to other healthcare providers about this? And what have they told you? SUMMITT: So we have. Before my testimony, we did reach out to approximately 18-plus other health care organizations across the nation. And basically, 18 of them signed up with us to help bring this awareness to Congress. And our biggest concern in the health care industry is not only is it potentially disruptive to health care operations itself, but it's also possible that during these calls that there could be what's called a denial of service against our telecommunication systems. And what that would mean would be a complete shutdown of our communications. And that's one of our worst fears. And, in fact, there was a hospital within the last 12 months that had over 65,000 calls launched against their facility, which did do a denial of service. And what we're afraid of is that's going to continue. MARTIN: Do you have any insight into why this is happening? SUMMITT: For the most part, it's an easy thing to do from a bad actor standpoint, and they're making money off of it. And that's really the bottom line. MARTIN: Before we let you go, can you offer some advice for people who are dealing with this? I mean, if you think you've gotten a call from the Moffitt Cancer Center, then it just doesn't seem like a great idea to ignore it. Like, what do you suggest that people do? SUMMITT: So what we're advising and what I advise even friends when they're asking me, what do I do about these calls, really, is to know who the caller is. There's no need to answer the call immediately. Even if it's an emergency situation, typically, the person who's trying to got ahold of you will call you right back. Robocallers don't do that. If you don't answer the call the first, they move on to the next. So if you get back-to-back phone calls from the same number, it's most likely someone really trying to get in touch with you. And the second thing I really advise is that when you've answered the call, don't immediately say hello, and don't give a hello, this is and your name. That helps the other people on the other end to know even more about you. So the least amount of information you give over one of these calls the better. And the other option that you actually have is to let it go to voicemail. If it's legitimate, they're going to leave a voicemail. What I don't really recommend, which is what a lot of people are doing, are putting those numbers in their block system. Keep in mind these are spammed, spoofed calls which, means that who's calling you is not who is showing up on the caller ID, and all you're doing now is blocking the caller ID number, which could potentially be someone that really needs to get in touch with you. MARTIN: That is Dave Summitt, chief information security officer from Moffitt Cancer Center in Tampa, Fla. Moffitt provides cancer care to more than 60,000 patients each year. It's the third-busiest standalone cancer hospital in the United States. We reached him in Tampa, Fla. Dave Summitt, thanks so much for talking with us, and thanks for that advice. SUMMITT: Thank you for helping us raise this awareness.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-23-735216904": {"title": "Training Better Robotic Surgeons In Virtual Reality : NPR", "url": "https://www.npr.org/2019/06/23/735216904/doctors-learn-the-nuts-and-bolts-of-robotic-surgery", "author": "No author found", "published_date": "2019-06-23", "content": "", "section": "Health", "disclaimer": ""}, "2019-06-23-735191310": {"title": "Training Better Robotic Surgeons : NPR", "url": "https://www.npr.org/2019/06/23/735191310/training-better-robotic-surgeons", "author": "No author found", "published_date": "2019-06-23", "content": "LULU GARCIA-NAVARRO, HOST:  When you go into your next surgery, your doctor may have some help from robots - yes, robots. Institutions across the U. S. have rapidly adopted this technology. But one very big problem - many doctors don't know how to use them. Mary Scott Hodgin of member station WBHM reports. MARY SCOTT HODGIN, BYLINE: Robot-assisted surgery is minimally invasive, and recovery time is often shorter. Those are a few reasons patients and doctors like it. But the technology is expensive. And studies show it can sometimes lead to worse long-term outcomes than other types of surgery. Still, the device has become common practice in some specialties, which means more surgeons are learning to use it. KENNETH KIM: It's not necessarily, is robot better? Robot is just another tool that they need to master just like any other surgical tool. SCOTT HODGIN: That's Dr. Kenneth Kim. He directs the robotic training program at UAB Hospital in Birmingham, Ala. Kim says the first step to learning robotic surgery is understanding how to use the robot. But that's not easy. KIM: It never was an issue because open surgery, like scissors - like, everyone learns how to use scissors in kindergarten. Everyone knows, functionally, how to use a knife. But with the robot, it's a totally different, new tool, and it's more complex. So now that has a separate learning curve. SCOTT HODGIN: To be clear, the robot isn't self-operating, at least not yet. The way it works is the surgeon sits at a console, sort of like a big video game, and uses hand and feet controls to manipulate a separate surgical part attached to the patient. Kim says one way students get comfortable with the device is virtual reality. KIM: Having trouble? THERESA BOITANO: That one's always the trouble one, especially if they're small. KIM: Yeah. SCOTT HODGIN: Surgical residents gather with Kim at UAB Hospital. They're practicing on a new robotic simulator. Theresa Boitano is an OB-GYN resident at UAB. She's maneuvering the robotic arms with precision to lift colorful rings and place them onto spikes, almost like a kid's game. BOITANO: And so I'm going now to grab this first ring. And at the same time, I'm thinking, OK. Now where do I need to go to the next one? You're always trying to stay ahead of the game but then also making sure you're not doing any errors at the same time. SCOTT HODGIN: The simulator records everything - how accurately she moves the robot arms, how fast she completes the exercise. It provides objective data about how well a surgeon performs. Dr. Khurshid Guru says this helps standardize the training process. Guru directs robotic surgery at Roswell Park Comprehensive Cancer Center in New York. KHURSHID GURU: The analogy is that now you don't have to worry about learning how to drive a car because everybody could get one on the street. They are taught the basic principles of driving a car. The million-dollar question now is, when would you allow them to get onto the expressway? SCOTT HODGIN: Guru says that's the next step - when surgeons specialize in different procedures. Dr. Monica Hagan Vetter of Ohio State University has studied robotic training programs across the country. She says using a simulator to measure surgical ability helps ensure surgeons have a certain level of skill before they actually operate on people. MONICA HAGAN VETTER: You can learn the steps of the procedure. But if you don't know how the robot works, if you don't know how to troubleshoot the robot or what to do in an emergency, then even if you can perform the world's best hysterectomy and you know all the steps and all the instruments, you are not safe to do that. SCOTT HODGIN: Dr. Kenneth Kim says simulators and the data they provide are great for that first step - learning to use the robot as a tool. But he says surgeons still have to watch and learn. KIM: The simulator's good, but it can only simulate so much. SCOTT HODGIN: In the real world, Kim says robot-assisted surgery isn't right for every patient. A surgeon needs to know when to use it and when not to. And those decisions can change as researchers continue to study patient outcomes from robotic surgery. For NPR News, I'm Mary Scott Hodgin in Birmingham. (SOUNDBITE OF SONG, \"MR. ROBOTO\")STYX: (Singing) Domo arigato, Mr. Roboto. (Singing in Japanese). LULU GARCIA-NAVARRO, HOST:   When you go into your next surgery, your doctor may have some help from robots - yes, robots. Institutions across the U. S. have rapidly adopted this technology. But one very big problem - many doctors don't know how to use them. Mary Scott Hodgin of member station WBHM reports. MARY SCOTT HODGIN, BYLINE: Robot-assisted surgery is minimally invasive, and recovery time is often shorter. Those are a few reasons patients and doctors like it. But the technology is expensive. And studies show it can sometimes lead to worse long-term outcomes than other types of surgery. Still, the device has become common practice in some specialties, which means more surgeons are learning to use it. KENNETH KIM: It's not necessarily, is robot better? Robot is just another tool that they need to master just like any other surgical tool. SCOTT HODGIN: That's Dr. Kenneth Kim. He directs the robotic training program at UAB Hospital in Birmingham, Ala. Kim says the first step to learning robotic surgery is understanding how to use the robot. But that's not easy. KIM: It never was an issue because open surgery, like scissors - like, everyone learns how to use scissors in kindergarten. Everyone knows, functionally, how to use a knife. But with the robot, it's a totally different, new tool, and it's more complex. So now that has a separate learning curve. SCOTT HODGIN: To be clear, the robot isn't self-operating, at least not yet. The way it works is the surgeon sits at a console, sort of like a big video game, and uses hand and feet controls to manipulate a separate surgical part attached to the patient. Kim says one way students get comfortable with the device is virtual reality. KIM: Having trouble? THERESA BOITANO: That one's always the trouble one, especially if they're small. KIM: Yeah. SCOTT HODGIN: Surgical residents gather with Kim at UAB Hospital. They're practicing on a new robotic simulator. Theresa Boitano is an OB-GYN resident at UAB. She's maneuvering the robotic arms with precision to lift colorful rings and place them onto spikes, almost like a kid's game. BOITANO: And so I'm going now to grab this first ring. And at the same time, I'm thinking, OK. Now where do I need to go to the next one? You're always trying to stay ahead of the game but then also making sure you're not doing any errors at the same time. SCOTT HODGIN: The simulator records everything - how accurately she moves the robot arms, how fast she completes the exercise. It provides objective data about how well a surgeon performs. Dr. Khurshid Guru says this helps standardize the training process. Guru directs robotic surgery at Roswell Park Comprehensive Cancer Center in New York. KHURSHID GURU: The analogy is that now you don't have to worry about learning how to drive a car because everybody could get one on the street. They are taught the basic principles of driving a car. The million-dollar question now is, when would you allow them to get onto the expressway? SCOTT HODGIN: Guru says that's the next step - when surgeons specialize in different procedures. Dr. Monica Hagan Vetter of Ohio State University has studied robotic training programs across the country. She says using a simulator to measure surgical ability helps ensure surgeons have a certain level of skill before they actually operate on people. MONICA HAGAN VETTER: You can learn the steps of the procedure. But if you don't know how the robot works, if you don't know how to troubleshoot the robot or what to do in an emergency, then even if you can perform the world's best hysterectomy and you know all the steps and all the instruments, you are not safe to do that. SCOTT HODGIN: Dr. Kenneth Kim says simulators and the data they provide are great for that first step - learning to use the robot as a tool. But he says surgeons still have to watch and learn. KIM: The simulator's good, but it can only simulate so much. SCOTT HODGIN: In the real world, Kim says robot-assisted surgery isn't right for every patient. A surgeon needs to know when to use it and when not to. And those decisions can change as researchers continue to study patient outcomes from robotic surgery. For NPR News, I'm Mary Scott Hodgin in Birmingham. (SOUNDBITE OF SONG, \"MR. ROBOTO\") STYX: (Singing) Domo arigato, Mr. Roboto. (Singing in Japanese).", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-24-734747462": {"title": "Instagram Advertising By Micro-Influencers: Do You Know It, When You See It? : NPR", "url": "https://www.npr.org/2019/06/24/734747462/instagram-advertising-do-you-know-it-when-you-see-it", "author": "No author found", "published_date": "2019-06-24", "content": "MARY LOUISE KELLY, HOST:  Today on All Tech Considered, living in a world where anyone can be a social media influencer. (SOUNDBITE OF MUSIC)KELLY: Influencing took off years ago, with reality stars such as the Kardashians leading the way. Now companies are hiring regular people to post about themselves on platforms such as Instagram and Snapchat. This is raising alarm amongst consumer rights groups, as NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: In the photograph, Gretchen Altman is smiling, leaning back casually, a cup of coffee in hand - Hills Bros. Coffee, to be precise. It looks like a candid shot, but if you hit like, leave a comment and tag a friend, you can get three different blends of brew for free. GRETCHEN ALTMAN: They approached me, and I sent them my rate sheet. I sent them an invoice. And we agreed on a set number of posts. GARSD: Altman's is going rate is $300 to $800 to promote something. She does some posts in exchange for free goods, she says, as long as it's stuff she believes in. Altman doesn't have the 140 million Instagram followers of Kim Kardashian. She has around 6,000. That makes her a micro-influencer. ALTMAN: I'm just living a normal life, and people relate to that. They just feel like I'm a friend of theirs. GARSD: Bonnie Patten, the executive director of Truth In Advertising, says. . . BONNIE PATTEN: It works. Consumers are very apt to buy things that they see being promoted on social media, especially by people they feel they have some sort of authentic, natural connection with. GARSD: This worries Patten and consumer rights groups. Several recent studies have found that young audiences are largely unable to understand when something is sponsored content. (SOUNDBITE OF AD)JONATHAN VAN NESS: Do you know a woman who needs a brand-new wardrobe all year long? GARSD: In this Instagram post, Jonathan Van Ness, star of Netflix's \"Queer Eye,\" is excited about a clothing rental service. (SOUNDBITE OF AD)VAN NESS: We're going to try on clothes. We're doing the most. We're doing everything. Love you, guys. Happy Holidays. Yas (ph). GARSD: In this case, Van Ness' post is clearly labeled as an advertisement. But what happens when it's an everyday person with just a couple thousand followers? PATTEN: The problem with a lot of these social media posts is that you don't know whether it's an ad or not. GARSD: What Patten wants is transparency in social media advertising. That nutritional shake, that tooth whitener that will make you look like a Cheshire cat - Patten wants influencers to be clear that they're getting paid to recommend it, which means including the hashtag #advertisement or #sponsoredcontent. Ultimately, consumer advocates say the buck stops with the Federal Trade Commission, the FTC, which several watchdog groups say has done little in terms of enforcement. PATTEN: Unfortunately, the FTC does not have the resources to police social media platforms to the extent necessary. GARSD: NPR reached out to the FTC, who referred us to their guidelines. If you're getting paid to promote, quote, \"then a disclosure is appropriate,\" end quote. To be clear, Gretchen Altman is diligent about using those hashtags. She loves what she does and sees it as a business. She doesn't necessarily want to be a social media celebrity. ALTMAN: With social media being so integrated into our everyday lives, we have this unique opportunity that I don't think anyone has ever had before, where we can each be our own brand. GARSD: For many, the very idea of everyday people becoming brands sounds like a nightmare capitalist dystopia. Professor Saleem Alhabash teaches public relations and social media at Michigan State University. He says there are bigger implications to this. When the lines between what is real life and what is marketing get blurred, it changes people's behaviors. SALEEM ALHABASH: You always need to be doing something exciting - taking pictures of your food, taking pictures of your feet by the beach. It becomes so important for people to be liked and appreciated that they always have to almost live another person's life. GARSD: He, like many, wonders, what are we buying into when we're all trying to sell something? Jasmine Garsd, NPR News, New York. (SOUNDBITE OF '80'S CHILD'S \"I CANT GO 4 THAT\") MARY LOUISE KELLY, HOST:   Today on All Tech Considered, living in a world where anyone can be a social media influencer. (SOUNDBITE OF MUSIC) KELLY: Influencing took off years ago, with reality stars such as the Kardashians leading the way. Now companies are hiring regular people to post about themselves on platforms such as Instagram and Snapchat. This is raising alarm amongst consumer rights groups, as NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: In the photograph, Gretchen Altman is smiling, leaning back casually, a cup of coffee in hand - Hills Bros. Coffee, to be precise. It looks like a candid shot, but if you hit like, leave a comment and tag a friend, you can get three different blends of brew for free. GRETCHEN ALTMAN: They approached me, and I sent them my rate sheet. I sent them an invoice. And we agreed on a set number of posts. GARSD: Altman's is going rate is $300 to $800 to promote something. She does some posts in exchange for free goods, she says, as long as it's stuff she believes in. Altman doesn't have the 140 million Instagram followers of Kim Kardashian. She has around 6,000. That makes her a micro-influencer. ALTMAN: I'm just living a normal life, and people relate to that. They just feel like I'm a friend of theirs. GARSD: Bonnie Patten, the executive director of Truth In Advertising, says. . . BONNIE PATTEN: It works. Consumers are very apt to buy things that they see being promoted on social media, especially by people they feel they have some sort of authentic, natural connection with. GARSD: This worries Patten and consumer rights groups. Several recent studies have found that young audiences are largely unable to understand when something is sponsored content. (SOUNDBITE OF AD) JONATHAN VAN NESS: Do you know a woman who needs a brand-new wardrobe all year long? GARSD: In this Instagram post, Jonathan Van Ness, star of Netflix's \"Queer Eye,\" is excited about a clothing rental service. (SOUNDBITE OF AD) VAN NESS: We're going to try on clothes. We're doing the most. We're doing everything. Love you, guys. Happy Holidays. Yas (ph). GARSD: In this case, Van Ness' post is clearly labeled as an advertisement. But what happens when it's an everyday person with just a couple thousand followers? PATTEN: The problem with a lot of these social media posts is that you don't know whether it's an ad or not. GARSD: What Patten wants is transparency in social media advertising. That nutritional shake, that tooth whitener that will make you look like a Cheshire cat - Patten wants influencers to be clear that they're getting paid to recommend it, which means including the hashtag #advertisement or #sponsoredcontent. Ultimately, consumer advocates say the buck stops with the Federal Trade Commission, the FTC, which several watchdog groups say has done little in terms of enforcement. PATTEN: Unfortunately, the FTC does not have the resources to police social media platforms to the extent necessary. GARSD: NPR reached out to the FTC, who referred us to their guidelines. If you're getting paid to promote, quote, \"then a disclosure is appropriate,\" end quote. To be clear, Gretchen Altman is diligent about using those hashtags. She loves what she does and sees it as a business. She doesn't necessarily want to be a social media celebrity. ALTMAN: With social media being so integrated into our everyday lives, we have this unique opportunity that I don't think anyone has ever had before, where we can each be our own brand. GARSD: For many, the very idea of everyday people becoming brands sounds like a nightmare capitalist dystopia. Professor Saleem Alhabash teaches public relations and social media at Michigan State University. He says there are bigger implications to this. When the lines between what is real life and what is marketing get blurred, it changes people's behaviors. SALEEM ALHABASH: You always need to be doing something exciting - taking pictures of your food, taking pictures of your feet by the beach. It becomes so important for people to be liked and appreciated that they always have to almost live another person's life. GARSD: He, like many, wonders, what are we buying into when we're all trying to sell something? Jasmine Garsd, NPR News, New York. (SOUNDBITE OF '80'S CHILD'S \"I CANT GO 4 THAT\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-25-735681134": {"title": "FedEx Sues U.S. Commerce Department Over Export Controls In Huawei Dispute : NPR", "url": "https://www.npr.org/2019/06/25/735681134/fedex-sues-u-s-commerce-department-over-export-controls-in-huawei-dispute", "author": "No author found", "published_date": "2019-06-25", "content": "", "section": "Business", "disclaimer": ""}, "2019-06-26-736351237": {"title": "Reddit Restricts Pro-Trump Community Forum Over Violent Threats : NPR", "url": "https://www.npr.org/2019/06/26/736351237/reddit-has-quarantined-popular-pro-trump-thread-over-violent-threats", "author": "No author found", "published_date": "2019-06-26", "content": "", "section": "Technology", "disclaimer": ""}, "2019-06-26-735883899": {"title": "Facebook Is Challenged To Ban Military Leader Accused Of Killings : NPR", "url": "https://www.npr.org/2019/06/26/735883899/why-facebook-wont-kick-off-a-warlord", "author": "No author found", "published_date": "2019-06-26", "content": "ARI SHAPIRO, HOST: A Sudanese warlord who led an attack last month on protesters that left more than a hundred people dead is, it turns out, a Facebook personality. He is using the platform to promote himself as a strong-yet-kind leader. Pro-democracy activists want him booted off the site. And so far, Facebook says no. Here to talk with us about it is NPR's Aarti Shahani. Welcome back to the studio. AARTI SHAHANI, BYLINE: Hi. SHAPIRO: Tell us about who this warlord is and what the relationship is with Facebook. SHAHANI: Yes. So his name is Lieutenant General Mohamed Hamdan Dagalo, better known as Hemeti. He's got a long history of violence. He was a member of Janjaweed, the militia considered responsible for the genocide in Darfur. He was a senior aide to the former dictator and most recently, as you mentioned, the massacre. This is the man who's using Facebook to whitewash his image. He's got a bunch of pages dedicated to making him look good. In one post, he promises to raise teacher salaries in Sudan. In another, which went up soon after the recent killings, there's a lovely video of him standing on his Jeep with throngs of men, women and children dancing around him. The message there and in many others is Hemeti is the protector of the nation, and the protesters are the unpatriotic ones. Sudanese activists find this sickening. They've petitioned Facebook saying, hey, you booted off American right-wingers like Alex Jones; please boot off this warlord who is way, way worse. Ahmed el-Gaili, a Sudanese international law expert who's based in Dubai - he says Facebook is letting itself be a propaganda machine for Hemeti and his paramilitary group. AHMED EL-GAILI: This is giving a pulpit to what is essentially a terrorist organization. Even if older posting are pictures of cats and dogs, they do not belong in Facebook or in any other forum. SHAPIRO: Is Facebook responding to this? SHAHANI: Yeah. The company has a response, and it's fascinating. I had a long talk on the phone with a man at the company who leads their work on dangerous organizations. Facebook was not comfortable with recording, so basically to recap what he said, he said, look; there is a big difference between an Alex Jones and this warlord. Jones is not a governmental official; neither is ISIS, which Facebook has built artificial intelligence to root out. But the warlord could be considered a representative of the state. Hemeti started at the fringes of Sudanese society. He's since climbed into the mainstream. He was appointed interim vice president, second in command in Sudan. So Facebook is hesitant to intervene. Many governments are worried that the company is too powerful already. The Facebook official says if we go ahead and ban someone who it could be argued is the representative of a sovereign state, that could make many governments even more wary. SHAPIRO: So Facebook is choosing not to get involved in this particular issue. Do they have an ethical responsibility, though? SHAHANI: So plenty of people who've tracked extremism online say yes and that Facebook should know better. And they give the example of Myanmar, OK? As early as 2014, human rights groups there were telling Facebook extremist anti-Muslim leaders are exploiting your platform. They're building followings. They are dangerous people. Facebook ignored the warnings until, fast-forward, these extremists activated their followers online and called for attacks against Rohingya Muslims. Last year, Facebook acknowledged it was slow to respond to what became a genocide in Myanmar. So people watching Sudan say, Facebook, you should know better. Violent leaders in volatile regions start soft, flip the switch. That is a familiar playbook. SHAPIRO: That's NPR's Aarti Shahani. Thanks a lot. SHAHANI: Thank you. AUDIE CORNISH, HOST: And we should note Facebook is a sponsor of NPR. (SOUNDBITE OF TRENTEMOLLER'S \"MISS YOU\") ARI SHAPIRO, HOST:  A Sudanese warlord who led an attack last month on protesters that left more than a hundred people dead is, it turns out, a Facebook personality. He is using the platform to promote himself as a strong-yet-kind leader. Pro-democracy activists want him booted off the site. And so far, Facebook says no. Here to talk with us about it is NPR's Aarti Shahani. Welcome back to the studio. AARTI SHAHANI, BYLINE: Hi. SHAPIRO: Tell us about who this warlord is and what the relationship is with Facebook. SHAHANI: Yes. So his name is Lieutenant General Mohamed Hamdan Dagalo, better known as Hemeti. He's got a long history of violence. He was a member of Janjaweed, the militia considered responsible for the genocide in Darfur. He was a senior aide to the former dictator and most recently, as you mentioned, the massacre. This is the man who's using Facebook to whitewash his image. He's got a bunch of pages dedicated to making him look good. In one post, he promises to raise teacher salaries in Sudan. In another, which went up soon after the recent killings, there's a lovely video of him standing on his Jeep with throngs of men, women and children dancing around him. The message there and in many others is Hemeti is the protector of the nation, and the protesters are the unpatriotic ones. Sudanese activists find this sickening. They've petitioned Facebook saying, hey, you booted off American right-wingers like Alex Jones; please boot off this warlord who is way, way worse. Ahmed el-Gaili, a Sudanese international law expert who's based in Dubai - he says Facebook is letting itself be a propaganda machine for Hemeti and his paramilitary group. AHMED EL-GAILI: This is giving a pulpit to what is essentially a terrorist organization. Even if older posting are pictures of cats and dogs, they do not belong in Facebook or in any other forum. SHAPIRO: Is Facebook responding to this? SHAHANI: Yeah. The company has a response, and it's fascinating. I had a long talk on the phone with a man at the company who leads their work on dangerous organizations. Facebook was not comfortable with recording, so basically to recap what he said, he said, look; there is a big difference between an Alex Jones and this warlord. Jones is not a governmental official; neither is ISIS, which Facebook has built artificial intelligence to root out. But the warlord could be considered a representative of the state. Hemeti started at the fringes of Sudanese society. He's since climbed into the mainstream. He was appointed interim vice president, second in command in Sudan. So Facebook is hesitant to intervene. Many governments are worried that the company is too powerful already. The Facebook official says if we go ahead and ban someone who it could be argued is the representative of a sovereign state, that could make many governments even more wary. SHAPIRO: So Facebook is choosing not to get involved in this particular issue. Do they have an ethical responsibility, though? SHAHANI: So plenty of people who've tracked extremism online say yes and that Facebook should know better. And they give the example of Myanmar, OK? As early as 2014, human rights groups there were telling Facebook extremist anti-Muslim leaders are exploiting your platform. They're building followings. They are dangerous people. Facebook ignored the warnings until, fast-forward, these extremists activated their followers online and called for attacks against Rohingya Muslims. Last year, Facebook acknowledged it was slow to respond to what became a genocide in Myanmar. So people watching Sudan say, Facebook, you should know better. Violent leaders in volatile regions start soft, flip the switch. That is a familiar playbook. SHAPIRO: That's NPR's Aarti Shahani. Thanks a lot. SHAHANI: Thank you. AUDIE CORNISH, HOST:  And we should note Facebook is a sponsor of NPR. (SOUNDBITE OF TRENTEMOLLER'S \"MISS YOU\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-26-735785005": {"title": "For U.S. And China, Stakes Are High In Trump-Xi Trade Talks At G-20 : NPR", "url": "https://www.npr.org/2019/06/26/735785005/deal-or-no-deal-the-stakes-are-high-for-trump-xi-trade-talks", "author": "No author found", "published_date": "2019-06-26", "content": "NOEL KING, HOST: President Trump heads to Japan this afternoon for the G-20 summit. But much of the focus will be on the G-2, the United States and China. President Trump and Chinese President Xi Jinping have a chance to diffuse their ongoing trade war, or possibly to escalate it. NPR's Scott Horsley has the story. SCOTT HORSLEY, BYLINE: Summits like the G-20 are often a chance for geopolitical speed dating. President Trump has meetings scheduled with at least eight world leaders over the next three days. But none is more consequential than his sit-down Saturday with Chinese President Xi Jinping. Two big, very different personalities jockeying for global power and economic might with hundreds of billions of dollars' worth of trade on the line. Matthew Goodman, who served in both the George W. Bush and Obama administrations, says the talks could go either way. MATTHEW GOODMAN: They may have an incentive to do a deal and shake hands. I think that's the least likely outcome. The other extreme is that they have a breakdown and they decide they're going to escalate further. That's possible, but I think also not the most likely. HORSLEY: Like many observers, Goodman thinks the likeliest course is a temporary truce that leaves existing tariffs in place but postpones additional taxes on Chinese imports while the two sides go back to the bargaining table. David Dollar, who focused on China at the Treasury Department and the World Bank, agrees. Although Trump has threatened to impose tariffs on another $300 billion worth of Chinese goods, Dollar says that would be costly for both countries. DAVID DOLLAR: At the minimum, I think it's in the U. S. interest to hold off on the next round of tariffs 'cause I think that's going to have a fairly serious effect on the U. S. economy and a bad effect on markets. HORSLEY: The stock market breathed a sigh of relief last week when Trump said he had a very good telephone conversation with Xi. Still, Dollar says, it's doubtful the two countries can quickly nail down an agreement that would address all the U. S. concerns around intellectual property protection and China's forced transfer of American know-how. DOLLAR: I just don't see how that can be negotiated in the next couple of days. HORSLEY: The White House also downplayed expectations for the Xi meeting, saying Trump is comfortable with any outcome. Ordinarily, the U. S. could expect some support from allies around the G-20 table, many of whom share Washington's concerns about China. But Michael Green of the Center for Strategic and International Studies says Trump has pursued a go-it-alone strategy while simultaneously picking fights with Europe, Japan and neighbors in North America. MICHAEL GREEN: What we're doing is we're saying loudly that we're the sheriff, and there's a bad guy at the end of the street and we're going to get him. You know, we're calling the posse together, which would be Japan and Europe, and then we're shooting at them. HORSLEY: Far from rallying the international community, Trump often thumbs his nose at it. In fact, it's not clear the president will even go along with the traditional joint statement at the end of the G-20 summit if other countries insist on including tough warnings about protectionism or the dangers of manmade climate change. That could be awkward for Japan, which is hosting the summit. Prime Minister Shinzo Abe has frequently courted Trump both on and off the golf course. Mireya Solis of the Brookings Institution says Abe will try to preserve a veneer of international cooperation. MIREYA SOLIS: He wants to have a functional G-20. But with a very unconventional president and outspoken critic, I think that's going to be an uphill battle. HORSLEY: Abe will hold his own one-on-one meeting with Trump where they'll discuss a possible U. S. -Japan trade agreement. Trump's withdrawal from a big Asia-Pacific trade pact has left U. S. exporters at a disadvantage in Japan. Trump wants to strike a new bargain. If that fails, he's held out the threat of steep tariffs on Japanese cars. Scott Horsley, NPR News, Washington. NOEL KING, HOST:  President Trump heads to Japan this afternoon for the G-20 summit. But much of the focus will be on the G-2, the United States and China. President Trump and Chinese President Xi Jinping have a chance to diffuse their ongoing trade war, or possibly to escalate it. NPR's Scott Horsley has the story. SCOTT HORSLEY, BYLINE: Summits like the G-20 are often a chance for geopolitical speed dating. President Trump has meetings scheduled with at least eight world leaders over the next three days. But none is more consequential than his sit-down Saturday with Chinese President Xi Jinping. Two big, very different personalities jockeying for global power and economic might with hundreds of billions of dollars' worth of trade on the line. Matthew Goodman, who served in both the George W. Bush and Obama administrations, says the talks could go either way. MATTHEW GOODMAN: They may have an incentive to do a deal and shake hands. I think that's the least likely outcome. The other extreme is that they have a breakdown and they decide they're going to escalate further. That's possible, but I think also not the most likely. HORSLEY: Like many observers, Goodman thinks the likeliest course is a temporary truce that leaves existing tariffs in place but postpones additional taxes on Chinese imports while the two sides go back to the bargaining table. David Dollar, who focused on China at the Treasury Department and the World Bank, agrees. Although Trump has threatened to impose tariffs on another $300 billion worth of Chinese goods, Dollar says that would be costly for both countries. DAVID DOLLAR: At the minimum, I think it's in the U. S. interest to hold off on the next round of tariffs 'cause I think that's going to have a fairly serious effect on the U. S. economy and a bad effect on markets. HORSLEY: The stock market breathed a sigh of relief last week when Trump said he had a very good telephone conversation with Xi. Still, Dollar says, it's doubtful the two countries can quickly nail down an agreement that would address all the U. S. concerns around intellectual property protection and China's forced transfer of American know-how. DOLLAR: I just don't see how that can be negotiated in the next couple of days. HORSLEY: The White House also downplayed expectations for the Xi meeting, saying Trump is comfortable with any outcome. Ordinarily, the U. S. could expect some support from allies around the G-20 table, many of whom share Washington's concerns about China. But Michael Green of the Center for Strategic and International Studies says Trump has pursued a go-it-alone strategy while simultaneously picking fights with Europe, Japan and neighbors in North America. MICHAEL GREEN: What we're doing is we're saying loudly that we're the sheriff, and there's a bad guy at the end of the street and we're going to get him. You know, we're calling the posse together, which would be Japan and Europe, and then we're shooting at them. HORSLEY: Far from rallying the international community, Trump often thumbs his nose at it. In fact, it's not clear the president will even go along with the traditional joint statement at the end of the G-20 summit if other countries insist on including tough warnings about protectionism or the dangers of manmade climate change. That could be awkward for Japan, which is hosting the summit. Prime Minister Shinzo Abe has frequently courted Trump both on and off the golf course. Mireya Solis of the Brookings Institution says Abe will try to preserve a veneer of international cooperation. MIREYA SOLIS: He wants to have a functional G-20. But with a very unconventional president and outspoken critic, I think that's going to be an uphill battle. HORSLEY: Abe will hold his own one-on-one meeting with Trump where they'll discuss a possible U. S. -Japan trade agreement. Trump's withdrawal from a big Asia-Pacific trade pact has left U. S. exporters at a disadvantage in Japan. Trump wants to strike a new bargain. If that fails, he's held out the threat of steep tariffs on Japanese cars. Scott Horsley, NPR News, Washington.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-06-27-736644485": {"title": "Body Camera Manufacturer Will Not Use Facial Recognition Software : NPR", "url": "https://www.npr.org/2019/06/27/736644485/major-police-body-camera-manufacturer-rejects-facial-recognition-software", "author": "No author found", "published_date": "2019-06-27", "content": "", "section": "National", "disclaimer": ""}, "2019-06-27-736668003": {"title": "Twitter Adds Warning Label For Offensive Political Tweets : NPR", "url": "https://www.npr.org/2019/06/27/736668003/twitter-adds-warning-label-for-offensive-political-tweets", "author": "No author found", "published_date": "2019-06-27", "content": "", "section": "Technology", "disclaimer": ""}, "2019-06-27-736288434": {"title": "Tell Us: Why Do You Share Your Location? : NPR", "url": "https://www.npr.org/2019/06/27/736288434/tell-us-why-do-you-share-your-location", "author": "No author found", "published_date": "2019-06-27", "content": "", "section": "National", "disclaimer": ""}, "2019-06-27-736572732": {"title": "Google Maps Leads About 100 Drivers Into A 'Muddy Mess' In Colorado : NPR", "url": "https://www.npr.org/2019/06/27/736572732/google-maps-leads-about-100-drivers-into-a-muddy-mess-in-colorado", "author": "No author found", "published_date": "2019-06-27", "content": "", "section": "Technology", "disclaimer": ""}, "2019-06-28-734034327": {"title": "Algorithmic Intelligence Has Gotten So Good, It's Easy To Forget It's Artificial : NPR", "url": "https://www.npr.org/2019/06/28/734034327/algorithmic-intelligence-has-gotten-so-smart-its-easy-to-forget-it-s-artificial", "author": "No author found", "published_date": "2019-06-28", "content": "DAVID BIANCULLI, HOST: This is FRESH AIR. Algorithms, that's the headline word for all the decision-making we've handed over to computers, from assigning credit scores to recommending YouTube videos to diagnosing cancer. The more we rely on them, the more of a hash they seem to make of things. Our linguist Geoff Nunberg has these thoughts on a word that has come to stand in for the power technology wields in our lives. GEOFF NUNBERG, BYLINE: Algorithms were around for a very long time before the public paid them any notice. The word itself is derived from the name of a 9th-century Persian mathematician. And the notion is simple enough. An algorithm's just any step-by-step procedure for accomplishing some task, from making the morning coffee to performing cardiac surgery. Computers use algorithms for pretty much everything they do, adding up a column of figures, resizing a window, saving a file to a disk. But all those things usually just happen the way they're supposed to. We don't have to think about what's going on under the hood. But algorithms got harder to ignore when they started taking over tasks that used to require human judgment - deciding which criminal defendants get bail, winnowing job applications, prioritizing stories in a news feed. All at once, the media are full of disquieting headlines like, \"How To Manage Our Algorithmic Overlords\" and \"The Algorithmification Of The Human Experience. \" Ordinary muggles may not know exactly how an algorithm works its magic, and a lot of people use the word just as a tech-inflected abracadabra. But we're reminded every day how unreliable these algorithms can be. Ads for vitamin supplements show up in our mail feed, while wedding invitations are buried in the junk file. An app sends us off a crowded highway and lands us bumper to bumper in local streets. OK, these are mostly just inconveniences. But they shake our confidence in the algorithms that are doing more important work. How can I trust Facebook's algorithms to get hate speech right when they've got other algorithms telling advertisers that my interests include \"The Celebrity Apprentice,\" beauty pageants and the World Wrestling Entertainment Hall of Fame? It's hard to resist anthropomorphizing these algorithms. We endow them with insight and intellect or with human frailties, like bad taste and bias. Disney actually personified the algorithm literally in their 2018 animated movie, \"Ralph Breaks The Internet,\" in the form of a character who has the title of head algorithm at a video-sharing site. She's an imperious fashionista who recalls Meryl Streep in \"The Devil Wears Prada\" as she sits at a desk swiping through cat videos and saying, no, no, yes. Tech companies tend to foster that anthropomorphic illusion when they tout their algorithms as artificial intelligence, or just AI. To most people, that term evokes the efforts to create self-aware beings capable of reasoning and explaining themselves, like Commander Data of \"Star Trek\" or HAL in \"2001. \" That was the aim of what computer scientists call good old-fashioned AI. But AI now connotes what's called second-wave AI or narrow AI. That's a very different project focused on machine learning. The idea is to build systems that can mimic human behavior without having to understand it. You train an algorithm in something like the way psychologists have trained pigeons to distinguish pictures of Charlie Brown from pictures of Lucy. You give it a pile of data, posts that Facebook users have engaged with, comments that human reviewers have classified as toxic or benign, messages tagged as spam or not spam and so on. The algorithm chews over thousands or millions of factors until it can figure out for itself how to tell the categories apart or predict which posts or videos somebody will click on. At that point, you can set it loose in the world. These algorithms can be quite adept at specific tasks. Take a very simple system I built with two colleagues some years ago that could sort out texts according to their genre. We trained an algorithm on a set of texts that were tagged as news articles, editorials, fiction and so on. And it masticated their words and punctuation until it was pretty good at telling them apart. For instance, it figured out for itself that when a text contained an exclamation point or question mark, it was more likely to be an editorial than a news story. But it didn't understand the text it was processing or have any concept of the difference between an opinion and a news story - no more than those pigeons know who Charlie Brown and Lucy are. The University of Toronto computer scientist Brian Cantwell Smith makes this point very crisply in a forthcoming book called \"The Promise Of Artificial Intelligence. \" However impressive they may be, he says, all existing AI systems do not know what they're talking about. By that he means that the systems have no concept of spam or porn or extremism or even of a game. Those are just elements of the narratives we tell about them. The algorithms are really triumphs of intelligent artifice, ingenious systems that can mindlessly simulate human judgment. Sometimes they do that all too well when they reproduce the errors in judgment they were trained on. If you train a credit rating algorithm on historical lending data that's infected with racial or gender bias, the algorithm's going to inherit that bias, and it won't be easy to tell. But they can also fail in alien ways that betray an unhuman weirdness. You think of the porn filters that block flesh-colored pictures of pigs and puddings or those notorious image-recognition algorithms that were identifying black faces as gorillas. So it's natural to be wary of our new algorithmic overlords. They've gotten so good at faking intelligent behavior that it's easy to forget that there's really nobody home. BIANCULLI: Geoff Nunberg is a linguist at the University of California Berkeley School of Information. Coming up, I review the new Showtime miniseries \"The Loudest Voice,\" about TV executive Roger Ailes and the birth and rise of the Fox News Channel. This is FRESH AIR. (SOUNDBITE OF FRED KATZ'S \"OLD PAINT\") DAVID BIANCULLI, HOST:  This is FRESH AIR. Algorithms, that's the headline word for all the decision-making we've handed over to computers, from assigning credit scores to recommending YouTube videos to diagnosing cancer. The more we rely on them, the more of a hash they seem to make of things. Our linguist Geoff Nunberg has these thoughts on a word that has come to stand in for the power technology wields in our lives. GEOFF NUNBERG, BYLINE: Algorithms were around for a very long time before the public paid them any notice. The word itself is derived from the name of a 9th-century Persian mathematician. And the notion is simple enough. An algorithm's just any step-by-step procedure for accomplishing some task, from making the morning coffee to performing cardiac surgery. Computers use algorithms for pretty much everything they do, adding up a column of figures, resizing a window, saving a file to a disk. But all those things usually just happen the way they're supposed to. We don't have to think about what's going on under the hood. But algorithms got harder to ignore when they started taking over tasks that used to require human judgment - deciding which criminal defendants get bail, winnowing job applications, prioritizing stories in a news feed. All at once, the media are full of disquieting headlines like, \"How To Manage Our Algorithmic Overlords\" and \"The Algorithmification Of The Human Experience. \" Ordinary muggles may not know exactly how an algorithm works its magic, and a lot of people use the word just as a tech-inflected abracadabra. But we're reminded every day how unreliable these algorithms can be. Ads for vitamin supplements show up in our mail feed, while wedding invitations are buried in the junk file. An app sends us off a crowded highway and lands us bumper to bumper in local streets. OK, these are mostly just inconveniences. But they shake our confidence in the algorithms that are doing more important work. How can I trust Facebook's algorithms to get hate speech right when they've got other algorithms telling advertisers that my interests include \"The Celebrity Apprentice,\" beauty pageants and the World Wrestling Entertainment Hall of Fame? It's hard to resist anthropomorphizing these algorithms. We endow them with insight and intellect or with human frailties, like bad taste and bias. Disney actually personified the algorithm literally in their 2018 animated movie, \"Ralph Breaks The Internet,\" in the form of a character who has the title of head algorithm at a video-sharing site. She's an imperious fashionista who recalls Meryl Streep in \"The Devil Wears Prada\" as she sits at a desk swiping through cat videos and saying, no, no, yes. Tech companies tend to foster that anthropomorphic illusion when they tout their algorithms as artificial intelligence, or just AI. To most people, that term evokes the efforts to create self-aware beings capable of reasoning and explaining themselves, like Commander Data of \"Star Trek\" or HAL in \"2001. \" That was the aim of what computer scientists call good old-fashioned AI. But AI now connotes what's called second-wave AI or narrow AI. That's a very different project focused on machine learning. The idea is to build systems that can mimic human behavior without having to understand it. You train an algorithm in something like the way psychologists have trained pigeons to distinguish pictures of Charlie Brown from pictures of Lucy. You give it a pile of data, posts that Facebook users have engaged with, comments that human reviewers have classified as toxic or benign, messages tagged as spam or not spam and so on. The algorithm chews over thousands or millions of factors until it can figure out for itself how to tell the categories apart or predict which posts or videos somebody will click on. At that point, you can set it loose in the world. These algorithms can be quite adept at specific tasks. Take a very simple system I built with two colleagues some years ago that could sort out texts according to their genre. We trained an algorithm on a set of texts that were tagged as news articles, editorials, fiction and so on. And it masticated their words and punctuation until it was pretty good at telling them apart. For instance, it figured out for itself that when a text contained an exclamation point or question mark, it was more likely to be an editorial than a news story. But it didn't understand the text it was processing or have any concept of the difference between an opinion and a news story - no more than those pigeons know who Charlie Brown and Lucy are. The University of Toronto computer scientist Brian Cantwell Smith makes this point very crisply in a forthcoming book called \"The Promise Of Artificial Intelligence. \" However impressive they may be, he says, all existing AI systems do not know what they're talking about. By that he means that the systems have no concept of spam or porn or extremism or even of a game. Those are just elements of the narratives we tell about them. The algorithms are really triumphs of intelligent artifice, ingenious systems that can mindlessly simulate human judgment. Sometimes they do that all too well when they reproduce the errors in judgment they were trained on. If you train a credit rating algorithm on historical lending data that's infected with racial or gender bias, the algorithm's going to inherit that bias, and it won't be easy to tell. But they can also fail in alien ways that betray an unhuman weirdness. You think of the porn filters that block flesh-colored pictures of pigs and puddings or those notorious image-recognition algorithms that were identifying black faces as gorillas. So it's natural to be wary of our new algorithmic overlords. They've gotten so good at faking intelligent behavior that it's easy to forget that there's really nobody home. BIANCULLI: Geoff Nunberg is a linguist at the University of California Berkeley School of Information. Coming up, I review the new Showtime miniseries \"The Loudest Voice,\" about TV executive Roger Ailes and the birth and rise of the Fox News Channel. This is FRESH AIR. (SOUNDBITE OF FRED KATZ'S \"OLD PAINT\")", "section": "Opinion", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-01-737498507": {"title": "For Facebook Content Moderators, Traumatizing Material Is A Job Hazard : NPR", "url": "https://www.npr.org/2019/07/01/737498507/for-facebook-content-moderators-traumatizing-material-is-a-job-hazard", "author": "No author found", "published_date": "2019-07-01", "content": "TERRY GROSS, HOST: This is FRESH AIR. I'm Terry Gross. In order to prevent you from being exposed to graphic violence, pornography and hate speech on your Facebook feed, there are thousands of people who are paid to view objectionable posts and decide which need to be removed. These content moderators are described by my guest, journalist Casey Newton, as first responders performing a critical function on a platform with billions of users. He's been investigating the working conditions of these content moderators. He's interviewed current and former employees. And for the first time, three former employees have broken their nondisclosure agreements and spoken on the record. Newton reports that repeated exposure to these traumatizing posts has left some content moderators with mental health problems. As we'll hear, there are other problems in the workplace, too. Facebook contracts with other companies to hire the content moderators and provide office space. Originally, these sites were in other countries, but the operations have expanded to the U. S. , including sites in California, Arizona, Texas and Florida. Later, we'll talk about how Facebook is preparing to launch what's been described as a Supreme Court to rule on contested content decisions. And we'll hear what Facebook's been doing in the lead-up to the 2020 election to try to protect users from disinformation and propaganda from Russia and other countries. Casey Newton covers the Silicon Valley for the tech site The Verge and writes the daily newsletter The Interface about the intersection of social media and democracy. Casey Newton, welcome to FRESH AIR. Give us a sense of what the content moderators have to look at in one typical day at work. CASEY NEWTON: Sure. So they are sitting at a computer workstation. They click a button that indicates they're ready to review content, and then they see a steady stream of text posts, images, videos. And they can be absolutely anything. Much of it will be benign - just words on a screen that someone didn't like. But some of it will be incredibly disturbing, some of the worst things imaginable. So moderators have described to me seeing, on a regular basis, beheadings, murders, animal abuse, child exploitation. And often, it arrives without warning. And so over the course of a long day, you sort of never know when it might be that you see the thing that is just going to ruin your day and might haunt you for a long time after. GROSS: So you spoke to a lot of content moderators. Some of them broke their nondisclosure agreements to speak with you. What did they tell you about the psychological and emotional impact of watching some of the brutality and pornography and reading the hate that they have to look at on a daily level to do their job? NEWTON: Well, as you might imagine, it's just really hard for them. The folks who I've spoken to, the ones who have reached out to me, have told me that this is work that has continued to haunt them even months or years after they have left the job. There is something traumatizing about viewing these images. We're empathetic people. We see other people or we see animals suffering - we're going to absorb some of that. And I've talked to some moderators in particular who were placed in queues of content where they were seeing much more graphic violence than other folks were. So, you know, maybe the average moderator might only see a handful of things a day that an average person would find really upsetting. I've talked to some people who are seeing dozens or even more than a hundred of those images. And as a result, I've talked to folks who will wake up in the middle of the night in a cold sweat. They will have nightmares about the content that they saw. And eventually, many of them get diagnosed with post-traumatic stress disorder, you know, almost a diagnosis that you might expect from somebody who had, you know, been into a war zone. There are - there's one other effect that I would say caught me by surprise in my reporting, which is I've talked to moderators who said that once they looked at so many fringe views and conspiracy theories, they started to believe those views themselves. And so I've talked to moderators who worked in workplaces where managers will stalk the floor, kind of promoting the idea that the earth is flat. I talked to somebody who said that he no longer believed that 9/11 was a terrorist attack. And so this content - just kind of that repeated daily exposure - it can change the way that these moderators see the world in really unexpected ways. GROSS: I'm really surprised to hear that because, you know, you'd almost think it would be the other way around because they're supposed to be tutored in how to spot a conspiracy theory. And you'd think that they'd want to debunk it and not start subscribing to it. NEWTON: Sure. Although, in the training that these moderators received, they don't get trained on resiliency to fringe ideas, right? Like, what they get trained on is, should this stay up? Or should this come down? And they are generally not prepared for the idea that they're going to be subjected to a lot of fringe and conspiracy views. I remember one sort of chilling story where in the immediate aftermath of the Parkland shooting - which is right when Facebook was starting to kind of ramp up its use of moderators in America. In the immediate aftermath, moderators were very upset by all of the videos that people were uploading of the violence. But then conspiracy sites started to create videos saying, well, that shooting never happened. These kids are crisis actors. And the discussion on the floor became, oh, well, gosh. I guess it didn't happen. I guess it was all a big hoax. And I actually am empathetic to those moderators because that was the only information they were getting about this shooting. They were spending eight hours a day at work, and every time they clicked, you know, to a new post, they were seeing something saying that the Parkland shooting was all made up. So, you know, I think in ways that we're only beginning to understand, we really are shaped by our media environment and repeated exposure to the same idea. And so if all you're seeing is fringe views, I think, eventually, some of that is going to seep into your beliefs. GROSS: Is there no attempt to put some of these posts in a larger social, political, gender, racial, ethnic, cultural context for the content moderators? NEWTON: Well - so one of the ideas behind bringing these moderators to America was exactly that - that they would have a kind of natural context for understanding posts that would make them more adept at moderating content that had been created by Americans. You'd sort of know who the politicians are around here. Or what are the big social issues that people are discussing? But in practice, that happens less than you might expect. Lot of the folks who got hired to do these jobs don't have a secondary education. A lot of them struggle with even basic questions of citizenship, I would argue. So moderators will tell me that their colleagues don't know who Senator Ted Cruz is, for example. Or they don't know what a refugee is. And so even though the idea of bringing these moderators to America was that they would have a better natural understanding of these issues, it turns out that they really don't. And also, Facebook doesn't provide a kind of ongoing cultural education for them to bring them up to speed with one exception, and that's when a particular issue kind of goes viral on Facebook, and there's a sudden need to catch everybody up in real time. But, you know, trying to explain the entire culture to a global army of 15,000 people is a really tall order. And honestly, I just think Facebook hasn't even started to figure out how it might try to do that. GROSS: So getting back to the, like, emotional and psychological impact of watching all these, like, violent and pornographic and hate-filled posts, you heard several stories about people who ended up buying guns for various reasons but for reasons that seemed related to the work. Can you give us an example? NEWTON: Yeah. I remember talking to one man who himself had started to embrace some of these conspiracy views. He was the one who said that he no longer believed that 9/11 had happened. But he had a job at one of these sites that is known internally as a QA or a quality analyst. And it's the job of that person to be a check on the moderators. They'll sort of look at a subset of those moderators' decisions and kind of check through them and see if they're right or wrong. And at these sites, accuracy is the single most important way that the moderators are evaluated. And if they miss even a handful of questions in a week, they can be put on a performance improvement plan. And they can eventually lose their job. So the moderators are under a great deal of stress to get everything right. And so this person, who I call Randy in my story, was a QA and would sometimes mark people wrong because he thought that they had made the wrong decision. And those folks would then come up to him in the parking lot, and they would threaten him and try to intimidate him into changing his ruling. And so it was after that that he started bringing a gun to work because he was afraid for his life. Outside of that, moderators have told me at both of the sites that I visited that when moderators do get fired or sometimes before they get fired, they will create posts either on Facebook itself or in an internal forum in which they threaten the lives of their co-workers. And they say, you know, I'm thinking about coming back and shooting this place up. So there's just always this undercurrent of anxiety in these offices. And it does inspire a small number of people to start bringing guns to work to protect themselves. GROSS: You know, when you mentioned the pressure to be accurate - and people are supposed to be 98% accurate - accuracy is what? It's making the right decision on whether to delete a post or not? NEWTON: Yes. The literal way that Facebook defines accuracy is, did you make the right decision? Should you - you know, did you take down the things that should be taken down? And did you leave up what ought to be left up? But in practice, the question is really more about, did the QA agree with the moderator when they did the weekly audit? And this is where some moderators I've spoken with have said that the system itself is somewhat suspect because, you know, as you can imagine, many, many of these decisions involve making subjective judgments. There is no policy that can account for every imaginable variation. And so you'll have a lot of really hard-working, well-meaning moderators making the decision that they think is right based on their, you know, mostly objective view. And then their boss will essentially come in and say, well, I don't think you're right. And it just leads to all kinds of anxiety. But what a lot of the moderators I've spoken with really want to get across is how subjective some of these decisions can be. GROSS: Let's take a short break here. And then we'll talk some more. If you're just joining us my guest is Casey Newton. And he covers Silicon Valley for the tech site The Verge. We'll be right back after a break. This is FRESH AIR. (SOUNDBITE OF JOAN JEANRENAUD'S \"AXIS\")GROSS: This is FRESH AIR. And if you're just joining us, my guest is Casey Newton. He covers Silicon Valley for the tech site The Verge and writes the daily newsletter The Interface about the intersection of social media and democracy. He's written two recent articles for The Verge, reporting on the working conditions of Facebook content moderators who purge the violent, pornographic and hate-speech posts from Facebook. So we've been talking about, like, the emotional and psychological effects on the content moderators of seeing all these, like, you know, hateful and violent and pornographic videos. But the physical working conditions are difficult, too. Tell us about, like, what they're allowed in terms of, you know, breaks and lunches and things like that. NEWTON: Yeah. To me, it is a huge part of this story because, as hard as the work itself is, I believe it's made exponentially harder by the physical environments of the job and the - sort of the way that it's structured. So, you know, in terms of breaks, they get two 15-minute breaks, a 30-minute lunch and nine minutes of something called wellness time. And the idea of wellness time is that if you see something that really overwhelms you, you're going to want to probably get up and walk away from your desk. And so Facebook will have these contractors offer various services. Like, maybe there's a yoga class or a ping-pong table. They also have counselors who are onsite for at least some of the shifts that workers can talk to. But as you might imagine, nine minutes isn't often enough to discuss the full weight of the issues that you're dealing with. And so that nine-minute limit has confused a lot of the moderators that I've spoken with. Maybe of equal concern, at least for some of the folks I've spoken with, is just the physical environment itself. And at least at these two sites I visited - one in Phoenix, one in Tampa - I've been told that they are very dirty, that the bathrooms are disgusting, that there's sort of human waste everywhere on a regular basis. People are finding bodily waste at their workstations because they sort of share workstations, and they're not always kept very clean. And so people coming to work will just kind of say that they don't like being in these offices because they don't feel like professional work environments. People will tell me that they see fights, both physical and verbal, between associates. People will be smoking weed in the parking lot. People at every office I've talked to say that their colleagues have sex in the break rooms, in the rooms that are reserved for mothers to lactate. And so it's just a very chaotic environment. And, you know, you're trying to make hundreds of these often very nuanced decisions a day about content moderation. But just something as simple as, am I going to have a clean bathroom to use? - really weighs on these moderators' minds. And so all of it adds up to a situation that feels untenable for some of the moderators I've spoken with. GROSS: What about the pay people receive? NEWTON: So in the United States, a moderator will be paid $15 an hour, which is $28,800 a year. That compares with a median pay package at Facebook that last year was $240,000 if you include salary, stock and bonuses. So there is a huge discrepancy there. But I think that tells you the value that Facebook places on this work, relatively speaking. GROSS: Now, the conditions that you've described and the salary you've described, really, it doesn't sound like what people expect that Facebook employees are faced with. But these people, the content moderators, aren't literally Facebook employees because Facebook contracts this work to other companies. Can you describe the arrangement Facebook has with the companies that actually hire and house the content moderators? NEWTON: Yes. So the model for this work is a call center, right? So let's say maybe you, you know, buy an appliance somewhere, and you're having a delivery issue, and you need to call someone to get it straightened out; the call goes to a call center. The exact same model is used for Facebook content moderation. Call center work in the United States is typically low-wage work. You get a bunch of bodies. You put them in a big office. And your goal is to just keep them as busy as possible, answering as many tickets as come in as possible. One of the things I've tried to highlight in my stories is that while we pay these folks as if the work is low-skill labor, in many cases, in my opinion, it is very high-skilled labor because they're making these very nuanced judgments about the boundaries of speech on the Internet. So if you accept that Facebook and Instagram, these big services that these folks are moderating, represent an essential component of political speech in this day and age, my guess is you might want them to be paid more than $28,000 a year. GROSS: And are they full-time employees or are they, like, part-time? NEWTON: They are full-time employees, and Facebook will take pains to tell you that it treats them far better than it treats the average employee of the call center industry. So they get full health benefits, for example. They get mental health benefits that go beyond what you would see at a call center. So there is, like, a 24-hour hotline that folks can call. So it definitely does provide more resources to employees than you would find in the average call center. But one of the points that I've tried to raise in my reporting is just maybe we should not be using a call center model for this kind of work. GROSS: What did Facebook and the subcontractor who runs two of the sites in America, the two that you visited - and that company's name is Cognizant - what did they have to say in response to your report on working conditions for the content moderators? NEWTON: So when I wrote the first story, Facebook did a big internal blog post where they sort of outlined changes that they were going to make in the future. And I want to highlight a few of them because I actually think they're positive and important, and some of them are somewhat theoretical (laughter). But the first one is, Facebook has said it's going to give a $3 an hour raise to these contractors, so they would make a minimum of $18 an hour, which I think comes out to more like $36,000 a year. That's a really good start. I don't think it's enough. I think it's a good start. Two - they said they were going to do a better job screening their candidates going forward. This gets tricky with employment law. But Facebook believes there is probably some way to inquire about people's mental health or willingness to do a job that might prevent them from hiring folks who are more likely to get PTSD from doing this kind of work. And then the third and final thing - which, you know, is sort of in the earliest discussions but I think would be so important - is after people leave this job, either because they're fired or because they quit, somehow making available for them some kind of counseling so that there will be someone to talk to, and Facebook would cover the cost. You know, if you get PTSD looking at Facebook, I think there is, you know, some thought that maybe Facebook should be the one paying for your counselor. So that has kind of been the word from Facebook on how they would like to handle these things in the future. At the earliest, though, that pay increase, it's not expected to go into effect until next summer. So we have a long way to go, right? There is sort of a year to go before that one major change is going to take place. When it comes to Cognizant, I think they've sort of presented the stories that I've found as, well, like, what can you do? We've got a lot of people working in an office. Things are going to break. Bad things are going to happen. And we try to take care of them, you know, when we find out about them. But the workers that, you know, continue to text me and email me every day, tell me that they have not found that response sufficient. GROSS: My guest is Casey Newton. He covers the Silicon Valley for the tech site The Verge. After a break, we'll talk about the so-called Supreme Court that Facebook is trying to create for appealing controversial content decisions, and we'll discuss what Facebook has been doing to prepare for the 2020 election. And Ken Tucker will review country hits with a hip-hop sensibility by Lil Nas X and Blanco Brown. I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF DOMINIC MILLER'S \"CHAOS THEORY\")GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to my interview with Casey Newton about Facebook and its content moderators. Those are the people who view posts that have been flagged as objectionable, including violent videos, pornography and hate speech, and decide which posts to remove. Newton has written two articles reporting on the content moderators' working conditions and the psychological impact of being repeatedly exposed to traumatizing content. Facebook contracts with other companies to employ, supervise and provide the office space for the moderators. These sites are located in several other countries as well as in the U. S. Casey Newton covers the Silicon Valley for the tech site The Verge and writes the newsletter The Interface about the intersection of social media and democracy. Facebook has had, I think, a big internal debate, and there's been a big national debate or international debate around Facebook about whether it's more like, say, the phone company that is a communications network that doesn't monitor the signals that are sent across it - it's not the job of any of the phone companies to listen into what you're saying and decide whether it passes certain standards or not - or whether Facebook is more like a newspaper or a magazine, a content provider where Facebook has some responsibility for what is posted on the site that it owns and profits from. So has Facebook been shifting in its perception of what it is and what its responsibility is to its users in terms of the content that Facebook is exposing them to? NEWTON: I think so. You know, there's a - there are a lot of journalists over the years who have pressed Facebook to admit it's a media company rather than a technology company, which I think kind of gets to the heart of the debate that you just described. And Mark Zuckerberg, I think, kind of ended that argument when he said that he actually believes Facebook has more responsibilities than a media company, or it has more responsibilities than, you know, a newspaper or a magazine precisely because of its reach. And so that's why starting in 2017 they announced that they would hire tens of thousands of more people to work on safety and security. So there are now 30,000 people at Facebook who work on safety and security, and about half of those are content moderators. So I do think that Facebook is taking its role in public affairs much more seriously than it was, say, five years ago. GROSS: Was there a turning point, like a specific post or posts, that led them to decide, no, we need more content moderators; we need to take more responsibility for what is on Facebook? NEWTON: Absolutely. It was the - the aftermath of the 2016 election completely changed public perception of Facebook, and in time, it changed Facebook's conception of itself. So in the 2016 election, Russians attacked us, right? They attacked our democracy. They used Facebook to spread various inflammatory ideas, and Facebook was caught flat-footed. They didn't realize there were so many people creating fake accounts. They didn't realize that those fake accounts were creating fake events that real Americans were actually showing up to. And they didn't realize how fake news, hoaxes were spreading on the site and, in many cases, receiving more views than legitimate, credible information. And so you just had this maelstrom, and Mark Zuckerberg and Facebook were under enormous pressure to do something about it. And so the story of Facebook since the end of 2016 has been making up for all of the failures that led up to the 2016 presidential election and trying to ensure that a similar set of circumstances doesn't unfold again. GROSS: So Facebook is in the process of creating a content oversight board that's being described by a lot of people as, like, a Facebook Supreme Court for final judgment calls on content decisions and whether content should be purged or left on the site. And last Thursday, Facebook released a report summarizing the input that it got from a series of public listening meetings around the world. So describe what we know about what Facebook thinks the role of the content oversight board will be once it's created. And I know this is a work in progress, and Facebook doesn't really know yet. And it's still processing all the feedback and input it's gotten, but what do we know so far? NEWTON: Yeah, I mean, I think the first thing to say is just what an absolutely wild idea this is, right? So you have this private company that hosts a bunch of user posts. And they have decided that they're going to create some kind of body that is independent from the private company, and that body will make the decision about what can stay on the site and what needs to come down. And so to kind of imagine what this might look like, you may recall a few weeks back there was a big controversy because a video had gone viral on Facebook that appeared to show House Speaker Nancy Pelosi slurring her words. She looks sort of drunk. And it turned out that somebody had just altered the video to make her look drunk when she was not. And so then there was this ferocious debate about whether Facebook should leave the video up on the site, whether they should take it down or whether they should, you know, take some other measures. Ultimately they decided that they would leave it on the site but sort of label it as false. In the future, this oversight board that Facebook is now creating would be the one that would make that decision. And so the question has been, how do you set up such a board? And the result has been kind of this pseudo-constitutional convention that has been happening around the world for the past several months. And I think it's really fascinating. GROSS: And last week at the Aspen Ideas Festival, Mark Zuckerberg was interviewed by Cass Sunstein. And Zuckerberg said in the interview that he was hoping more of the industry would join in this project. NEWTON: Yeah. And I'm actually sympathetic to that because one problem with our tech companies being so large now is that while they have increasing amounts of power over our lives, they are not really accountable to us in any way. And so if you might think about a similar situation on YouTube where I post a video; it gets taken down; maybe my channel gets removed, and maybe that channel was actually my livelihood, there are thousands of people around this world, if not tens of thousands of people, for whom this is the case. Well, if your channel gets deleted, you have no Supreme Court that you can appeal to. There is no system of jurisprudence. There is no case law. There is no precedent. And so everything just winds up being this ad hoc decision. And so as a result, you have this enormous frustration in that community. There are, you know, constant death threats against YouTube executives. My hope, if I want to be an optimist about all of this - it's that by devolving some of the power that these tech companies have back to the people, that these systems will be more accountable to their everyday users, that they'll sort of seem legitimate and that we can bring some semblance of democracy to what, you know, will always remain private companies. GROSS: So Facebook does have a booklet of community standards that's available for the public to read. I have never read it. I imagine most Facebook users haven't read it, but I imagine you have. Can you give a sense of, like, what some of the standards are and in guiding people as, like, what is violent? Like, what is too violent? Like, what is too sexual? What is too hateful and hate speech and has to be taken down? NEWTON: Well, I think nudity offers a useful place to start. Often these guidelines that get written by companies start as a single page, and somebody at the company - typically not the most senior executive - will just write down a list of things that they do not think should be on their network, right? And it'll usually start with, like, no nudity. And then as the network goes along, a mother will post a photo of herself breastfeeding, and then the company will have to decide whether it wants to continue to host breastfeeding content. And then somebody will say, well, you know, we decided to permit breastfeeding but maybe not in this particular case, right? Or maybe we don't think that a nipple should be visible at this distance. And so over time, these guidelines just grow and grow and grow. And also over time, as a society, our expectations change, and things that we once thought should never be public all of a sudden we become OK with. So it's a never-ending series of infinitely branching debates, and I think it speaks to the - again, to the difficulty of the job and moderating it but also of writing the rules in the first place, right? You know, traditionally we have had systems of law to make these decisions about what is allowed and not allowed. You know, in the United States, obviously we have free speech protections. Facebook's, I think, content moderation guidelines were written in the spirit of the First Amendment to maximize the amount of speech. But it's a global network, and most countries do not have the First Amendment. And so it's just been a really chaotic time for the company as it tries to apply this one global standard in places that are not always very receptive to it. GROSS: Can you think of an example when the rules changed, like, in the course of one day about what was acceptable and what wasn't for posting? NEWTON: So there was this phenomenon that Facebook was seeing of young girls saying to each other something like, I love this ho, short for hooker. And Facebook's guidelines have traditionally said that you can't call somebody a hooker, but then they realized after getting a lot of angry feedback that these girls were actually speaking positively of their friends and that somehow what had been an insult was now a compliment. And so one day a switch was flipped, and all of a sudden, it was now OK to say, I love you, ho. GROSS: Was it context - like, did you have to figure out what the context was if you were a content moderator, whether this was a term of affection or whether it was intended as an insult? NEWTON: That's exactly right. And so, you know, think about this. You're a moderator. You don't know these people. You're looking at one sentence and maybe a couple of comments underneath it, and now it's your job to decide whether, you know, Suzy (ph) likes Jenny (ph) or not. GROSS: Right (laughter). NEWTON: It's very difficult. GROSS: (Laughter) OK. So an example that I read about in terms of content moderation is that the - a post that said autistic people should be sterilized was not taken down because autistic people aren't a protected group. What does that mean? NEWTON: So Facebook uses a standard similar to a U. S. legal standard where certain classes of people are protected. So, you know, race is a great example - a classic example of a protected class. You're not allowed to say, you know, all members of this race are bad. Autism, though, was not considered a protected class, and so if you wanted to say something that most of us would find really offensive like all autistic people should be sterilized, Facebook would just leave that up in the name of free speech. Interestingly enough, a moderator texted me about that two days ago and said that autism is now considered a protected characteristic, and so you can no longer say that. GROSS: Let's take a short break here, and then we'll talk some more. If you're just joining us, my guest is Casey Newton. He covers Silicon Valley for the tech site The Verge. We'll be right back. This is FRESH AIR. (SOUNDBITE OF MUSIC)GROSS: This is FRESH AIR, and if you're just joining us, my guest is Casey Newton. He covers Silicon Valley for the tech site The Verge, and he writes the daily newsletter The Interface about the intersection of social media and democracy. He's written two recent articles for The Verge reporting on the working conditions of Facebook's content moderators. These are the people who purge the violent, pornographic and hate speech posts. Social media, including Facebook, was used as a tool of the Russians to subvert our 2016 presidential election, and many Americans are very concerned about what the Russians or the Chinese or the Iranians or other actors might do to try to use social media to subvert the next presidential election. What is Facebook doing to prepare for the election? NEWTON: So it turns out that one pretty good, easy thing that you can do to reduce the risk of that kind of thing happening is to catch fake accounts. So Facebook has this ironclad rule that every account - you know, you're only allowed to have one account, and it has to be your real name, and that's that - no exceptions. And so if you're, say, a Russian agent, you probably are not going to be able to create a bunch of posts that go viral in America criticizing an American political candidate because it will just sort of be clear that, you know, you don't belong in this political discussion. But if you can create an account that says, you know, Joe Smith and you live on Main Street, USA, maybe you can create a page that gets a million followers and all of a sudden is able to start posting a lot of inflammatory memes and rhetoric. So that's a major area where Facebook has invested - is just cracking down on fake accounts. They now say that they detect millions of them every week. And of course it's a cat-and-mouse game where the Russians are always getting better, but that is one place where they've been able to focus. They've also placed a lot of new requirements around ads. So for example, there is now an archive that you can visit to see every single ad that a political candidate bought or even just a political ad that was created by a political action committee. You know, in the 2016 election, we didn't really know what ads people were seeing because the ads could be targeted to, you know, down to the level of one person, and it would just sort of never be publicly searchable. That's not the case anymore. Now you can actually dive in. Researchers are, you know, putting together collections of these ads and studying them in real time. So those are kind of two things that I would highlight as ways that Facebook is trying to detect these, you know, networks of bad actors and disrupt them. GROSS: So, you know, one of the things social media is trying to contend with now is extremists. For example, Facebook initially banned Alex Jones' network from the site, and then it banned Alex Jones himself. What are the guidelines that Facebook uses to decide who's too extreme to post? NEWTON: It's a great question and one where the answer has changed pretty significantly over the past year. Last summer was when attention really started to turn on this question of Alex Jones and how he was using social networks not just to reach an audience but to build an audience because all of these networks have recommendation algorithms and search algorithms that can sort of point more and more people at you. And I think Facebook started to look at the effect that Alex Jones was having on the world. Sort of most famously, there was a family of a victim of the Sandy Hook shooting who had moved seven times because Alex Jones' followers had been moved by him to harass the family of the dead little boy because Alex Jones had released a number of videos in which he suggested that the Sandy Hook shooting had all been faked and was a pretext for the government to take away people's guns. And so at a certain point, Facebook has to ask itself, why do these rules exist, and why is it that the rules right now seem to be doing a better job of protecting Alex Jones' right to speak his mind than they do the victim of a shooting, you know, whose family can no longer regularly visit their son's grave? And I think as that thinking started to reverberate through the social network, they made the decision that Alex Jones was what they call a hate figure, which means that, you know, not only is he not allowed on the site, but no one can post a statement or a video in which they say, Alex Jones is great, or, you know, sort of express support for his ideas, you know? But what I think you're touching on is that these are really subjective calls. These are political calls. These are calls that a lot of folks are going to disagree with, and so I think we're really kind of having one of the great reckonings over free speech globally that we've had in a long time. And there isn't one great answer. It's always a question of, how are you going to manage all of the tradeoffs? GROSS: Since you cover the Silicon Valley and social media on a regular basis and you have a daily newsletter, what are some of the things you're looking to in the near future, like issues that are emerging now or changes that you expect will emerge very soon? NEWTON: I think we are having a reckoning over the Internet globally because tech companies have gotten so big and so consequential. And so we are reckoning with this power, and you see it in a hundred different ways, whether it's the debates that we're having about facial recognition technology or deepfakes or independent oversight boards. I think you're just seeing this sense that there is not a system of justice that can adequately protect us from how rapid technology is changing the world around us. And so what I am trying to do is just understand that reckoning a little bit better, every day try to anticipate some of the changes, try to help to frame some of those debates. You know, I don't think you can really overstate how dramatically the Internet has reshaped our lives already but how much it's going to reshape them in the future. And so, you know, I will never have a shortage of things to write about. GROSS: Casey Newton, thank you so much for your reporting, and thank you so much for talking with us. NEWTON: Oh, Terry, it was an absolute pleasure. GROSS: Casey Newton covers the Silicon Valley for the tech site The Verge and writes the newsletter The Interface about the intersection of social media and democracy. After we take a short break, Ken Tucker will review country hits with a hip-hop sensibility by Lil Nas X and Blanco Brown. This is FRESH AIR. (SOUNDBITE OF CALEXICO'S \"PRASKOVIA\") TERRY GROSS, HOST:  This is FRESH AIR. I'm Terry Gross. In order to prevent you from being exposed to graphic violence, pornography and hate speech on your Facebook feed, there are thousands of people who are paid to view objectionable posts and decide which need to be removed. These content moderators are described by my guest, journalist Casey Newton, as first responders performing a critical function on a platform with billions of users. He's been investigating the working conditions of these content moderators. He's interviewed current and former employees. And for the first time, three former employees have broken their nondisclosure agreements and spoken on the record. Newton reports that repeated exposure to these traumatizing posts has left some content moderators with mental health problems. As we'll hear, there are other problems in the workplace, too. Facebook contracts with other companies to hire the content moderators and provide office space. Originally, these sites were in other countries, but the operations have expanded to the U. S. , including sites in California, Arizona, Texas and Florida. Later, we'll talk about how Facebook is preparing to launch what's been described as a Supreme Court to rule on contested content decisions. And we'll hear what Facebook's been doing in the lead-up to the 2020 election to try to protect users from disinformation and propaganda from Russia and other countries. Casey Newton covers the Silicon Valley for the tech site The Verge and writes the daily newsletter The Interface about the intersection of social media and democracy. Casey Newton, welcome to FRESH AIR. Give us a sense of what the content moderators have to look at in one typical day at work. CASEY NEWTON: Sure. So they are sitting at a computer workstation. They click a button that indicates they're ready to review content, and then they see a steady stream of text posts, images, videos. And they can be absolutely anything. Much of it will be benign - just words on a screen that someone didn't like. But some of it will be incredibly disturbing, some of the worst things imaginable. So moderators have described to me seeing, on a regular basis, beheadings, murders, animal abuse, child exploitation. And often, it arrives without warning. And so over the course of a long day, you sort of never know when it might be that you see the thing that is just going to ruin your day and might haunt you for a long time after. GROSS: So you spoke to a lot of content moderators. Some of them broke their nondisclosure agreements to speak with you. What did they tell you about the psychological and emotional impact of watching some of the brutality and pornography and reading the hate that they have to look at on a daily level to do their job? NEWTON: Well, as you might imagine, it's just really hard for them. The folks who I've spoken to, the ones who have reached out to me, have told me that this is work that has continued to haunt them even months or years after they have left the job. There is something traumatizing about viewing these images. We're empathetic people. We see other people or we see animals suffering - we're going to absorb some of that. And I've talked to some moderators in particular who were placed in queues of content where they were seeing much more graphic violence than other folks were. So, you know, maybe the average moderator might only see a handful of things a day that an average person would find really upsetting. I've talked to some people who are seeing dozens or even more than a hundred of those images. And as a result, I've talked to folks who will wake up in the middle of the night in a cold sweat. They will have nightmares about the content that they saw. And eventually, many of them get diagnosed with post-traumatic stress disorder, you know, almost a diagnosis that you might expect from somebody who had, you know, been into a war zone. There are - there's one other effect that I would say caught me by surprise in my reporting, which is I've talked to moderators who said that once they looked at so many fringe views and conspiracy theories, they started to believe those views themselves. And so I've talked to moderators who worked in workplaces where managers will stalk the floor, kind of promoting the idea that the earth is flat. I talked to somebody who said that he no longer believed that 9/11 was a terrorist attack. And so this content - just kind of that repeated daily exposure - it can change the way that these moderators see the world in really unexpected ways. GROSS: I'm really surprised to hear that because, you know, you'd almost think it would be the other way around because they're supposed to be tutored in how to spot a conspiracy theory. And you'd think that they'd want to debunk it and not start subscribing to it. NEWTON: Sure. Although, in the training that these moderators received, they don't get trained on resiliency to fringe ideas, right? Like, what they get trained on is, should this stay up? Or should this come down? And they are generally not prepared for the idea that they're going to be subjected to a lot of fringe and conspiracy views. I remember one sort of chilling story where in the immediate aftermath of the Parkland shooting - which is right when Facebook was starting to kind of ramp up its use of moderators in America. In the immediate aftermath, moderators were very upset by all of the videos that people were uploading of the violence. But then conspiracy sites started to create videos saying, well, that shooting never happened. These kids are crisis actors. And the discussion on the floor became, oh, well, gosh. I guess it didn't happen. I guess it was all a big hoax. And I actually am empathetic to those moderators because that was the only information they were getting about this shooting. They were spending eight hours a day at work, and every time they clicked, you know, to a new post, they were seeing something saying that the Parkland shooting was all made up. So, you know, I think in ways that we're only beginning to understand, we really are shaped by our media environment and repeated exposure to the same idea. And so if all you're seeing is fringe views, I think, eventually, some of that is going to seep into your beliefs. GROSS: Is there no attempt to put some of these posts in a larger social, political, gender, racial, ethnic, cultural context for the content moderators? NEWTON: Well - so one of the ideas behind bringing these moderators to America was exactly that - that they would have a kind of natural context for understanding posts that would make them more adept at moderating content that had been created by Americans. You'd sort of know who the politicians are around here. Or what are the big social issues that people are discussing? But in practice, that happens less than you might expect. Lot of the folks who got hired to do these jobs don't have a secondary education. A lot of them struggle with even basic questions of citizenship, I would argue. So moderators will tell me that their colleagues don't know who Senator Ted Cruz is, for example. Or they don't know what a refugee is. And so even though the idea of bringing these moderators to America was that they would have a better natural understanding of these issues, it turns out that they really don't. And also, Facebook doesn't provide a kind of ongoing cultural education for them to bring them up to speed with one exception, and that's when a particular issue kind of goes viral on Facebook, and there's a sudden need to catch everybody up in real time. But, you know, trying to explain the entire culture to a global army of 15,000 people is a really tall order. And honestly, I just think Facebook hasn't even started to figure out how it might try to do that. GROSS: So getting back to the, like, emotional and psychological impact of watching all these, like, violent and pornographic and hate-filled posts, you heard several stories about people who ended up buying guns for various reasons but for reasons that seemed related to the work. Can you give us an example? NEWTON: Yeah. I remember talking to one man who himself had started to embrace some of these conspiracy views. He was the one who said that he no longer believed that 9/11 had happened. But he had a job at one of these sites that is known internally as a QA or a quality analyst. And it's the job of that person to be a check on the moderators. They'll sort of look at a subset of those moderators' decisions and kind of check through them and see if they're right or wrong. And at these sites, accuracy is the single most important way that the moderators are evaluated. And if they miss even a handful of questions in a week, they can be put on a performance improvement plan. And they can eventually lose their job. So the moderators are under a great deal of stress to get everything right. And so this person, who I call Randy in my story, was a QA and would sometimes mark people wrong because he thought that they had made the wrong decision. And those folks would then come up to him in the parking lot, and they would threaten him and try to intimidate him into changing his ruling. And so it was after that that he started bringing a gun to work because he was afraid for his life. Outside of that, moderators have told me at both of the sites that I visited that when moderators do get fired or sometimes before they get fired, they will create posts either on Facebook itself or in an internal forum in which they threaten the lives of their co-workers. And they say, you know, I'm thinking about coming back and shooting this place up. So there's just always this undercurrent of anxiety in these offices. And it does inspire a small number of people to start bringing guns to work to protect themselves. GROSS: You know, when you mentioned the pressure to be accurate - and people are supposed to be 98% accurate - accuracy is what? It's making the right decision on whether to delete a post or not? NEWTON: Yes. The literal way that Facebook defines accuracy is, did you make the right decision? Should you - you know, did you take down the things that should be taken down? And did you leave up what ought to be left up? But in practice, the question is really more about, did the QA agree with the moderator when they did the weekly audit? And this is where some moderators I've spoken with have said that the system itself is somewhat suspect because, you know, as you can imagine, many, many of these decisions involve making subjective judgments. There is no policy that can account for every imaginable variation. And so you'll have a lot of really hard-working, well-meaning moderators making the decision that they think is right based on their, you know, mostly objective view. And then their boss will essentially come in and say, well, I don't think you're right. And it just leads to all kinds of anxiety. But what a lot of the moderators I've spoken with really want to get across is how subjective some of these decisions can be. GROSS: Let's take a short break here. And then we'll talk some more. If you're just joining us my guest is Casey Newton. And he covers Silicon Valley for the tech site The Verge. We'll be right back after a break. This is FRESH AIR. (SOUNDBITE OF JOAN JEANRENAUD'S \"AXIS\") GROSS: This is FRESH AIR. And if you're just joining us, my guest is Casey Newton. He covers Silicon Valley for the tech site The Verge and writes the daily newsletter The Interface about the intersection of social media and democracy. He's written two recent articles for The Verge, reporting on the working conditions of Facebook content moderators who purge the violent, pornographic and hate-speech posts from Facebook. So we've been talking about, like, the emotional and psychological effects on the content moderators of seeing all these, like, you know, hateful and violent and pornographic videos. But the physical working conditions are difficult, too. Tell us about, like, what they're allowed in terms of, you know, breaks and lunches and things like that. NEWTON: Yeah. To me, it is a huge part of this story because, as hard as the work itself is, I believe it's made exponentially harder by the physical environments of the job and the - sort of the way that it's structured. So, you know, in terms of breaks, they get two 15-minute breaks, a 30-minute lunch and nine minutes of something called wellness time. And the idea of wellness time is that if you see something that really overwhelms you, you're going to want to probably get up and walk away from your desk. And so Facebook will have these contractors offer various services. Like, maybe there's a yoga class or a ping-pong table. They also have counselors who are onsite for at least some of the shifts that workers can talk to. But as you might imagine, nine minutes isn't often enough to discuss the full weight of the issues that you're dealing with. And so that nine-minute limit has confused a lot of the moderators that I've spoken with. Maybe of equal concern, at least for some of the folks I've spoken with, is just the physical environment itself. And at least at these two sites I visited - one in Phoenix, one in Tampa - I've been told that they are very dirty, that the bathrooms are disgusting, that there's sort of human waste everywhere on a regular basis. People are finding bodily waste at their workstations because they sort of share workstations, and they're not always kept very clean. And so people coming to work will just kind of say that they don't like being in these offices because they don't feel like professional work environments. People will tell me that they see fights, both physical and verbal, between associates. People will be smoking weed in the parking lot. People at every office I've talked to say that their colleagues have sex in the break rooms, in the rooms that are reserved for mothers to lactate. And so it's just a very chaotic environment. And, you know, you're trying to make hundreds of these often very nuanced decisions a day about content moderation. But just something as simple as, am I going to have a clean bathroom to use? - really weighs on these moderators' minds. And so all of it adds up to a situation that feels untenable for some of the moderators I've spoken with. GROSS: What about the pay people receive? NEWTON: So in the United States, a moderator will be paid $15 an hour, which is $28,800 a year. That compares with a median pay package at Facebook that last year was $240,000 if you include salary, stock and bonuses. So there is a huge discrepancy there. But I think that tells you the value that Facebook places on this work, relatively speaking. GROSS: Now, the conditions that you've described and the salary you've described, really, it doesn't sound like what people expect that Facebook employees are faced with. But these people, the content moderators, aren't literally Facebook employees because Facebook contracts this work to other companies. Can you describe the arrangement Facebook has with the companies that actually hire and house the content moderators? NEWTON: Yes. So the model for this work is a call center, right? So let's say maybe you, you know, buy an appliance somewhere, and you're having a delivery issue, and you need to call someone to get it straightened out; the call goes to a call center. The exact same model is used for Facebook content moderation. Call center work in the United States is typically low-wage work. You get a bunch of bodies. You put them in a big office. And your goal is to just keep them as busy as possible, answering as many tickets as come in as possible. One of the things I've tried to highlight in my stories is that while we pay these folks as if the work is low-skill labor, in many cases, in my opinion, it is very high-skilled labor because they're making these very nuanced judgments about the boundaries of speech on the Internet. So if you accept that Facebook and Instagram, these big services that these folks are moderating, represent an essential component of political speech in this day and age, my guess is you might want them to be paid more than $28,000 a year. GROSS: And are they full-time employees or are they, like, part-time? NEWTON: They are full-time employees, and Facebook will take pains to tell you that it treats them far better than it treats the average employee of the call center industry. So they get full health benefits, for example. They get mental health benefits that go beyond what you would see at a call center. So there is, like, a 24-hour hotline that folks can call. So it definitely does provide more resources to employees than you would find in the average call center. But one of the points that I've tried to raise in my reporting is just maybe we should not be using a call center model for this kind of work. GROSS: What did Facebook and the subcontractor who runs two of the sites in America, the two that you visited - and that company's name is Cognizant - what did they have to say in response to your report on working conditions for the content moderators? NEWTON: So when I wrote the first story, Facebook did a big internal blog post where they sort of outlined changes that they were going to make in the future. And I want to highlight a few of them because I actually think they're positive and important, and some of them are somewhat theoretical (laughter). But the first one is, Facebook has said it's going to give a $3 an hour raise to these contractors, so they would make a minimum of $18 an hour, which I think comes out to more like $36,000 a year. That's a really good start. I don't think it's enough. I think it's a good start. Two - they said they were going to do a better job screening their candidates going forward. This gets tricky with employment law. But Facebook believes there is probably some way to inquire about people's mental health or willingness to do a job that might prevent them from hiring folks who are more likely to get PTSD from doing this kind of work. And then the third and final thing - which, you know, is sort of in the earliest discussions but I think would be so important - is after people leave this job, either because they're fired or because they quit, somehow making available for them some kind of counseling so that there will be someone to talk to, and Facebook would cover the cost. You know, if you get PTSD looking at Facebook, I think there is, you know, some thought that maybe Facebook should be the one paying for your counselor. So that has kind of been the word from Facebook on how they would like to handle these things in the future. At the earliest, though, that pay increase, it's not expected to go into effect until next summer. So we have a long way to go, right? There is sort of a year to go before that one major change is going to take place. When it comes to Cognizant, I think they've sort of presented the stories that I've found as, well, like, what can you do? We've got a lot of people working in an office. Things are going to break. Bad things are going to happen. And we try to take care of them, you know, when we find out about them. But the workers that, you know, continue to text me and email me every day, tell me that they have not found that response sufficient. GROSS: My guest is Casey Newton. He covers the Silicon Valley for the tech site The Verge. After a break, we'll talk about the so-called Supreme Court that Facebook is trying to create for appealing controversial content decisions, and we'll discuss what Facebook has been doing to prepare for the 2020 election. And Ken Tucker will review country hits with a hip-hop sensibility by Lil Nas X and Blanco Brown. I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF DOMINIC MILLER'S \"CHAOS THEORY\") GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to my interview with Casey Newton about Facebook and its content moderators. Those are the people who view posts that have been flagged as objectionable, including violent videos, pornography and hate speech, and decide which posts to remove. Newton has written two articles reporting on the content moderators' working conditions and the psychological impact of being repeatedly exposed to traumatizing content. Facebook contracts with other companies to employ, supervise and provide the office space for the moderators. These sites are located in several other countries as well as in the U. S. Casey Newton covers the Silicon Valley for the tech site The Verge and writes the newsletter The Interface about the intersection of social media and democracy. Facebook has had, I think, a big internal debate, and there's been a big national debate or international debate around Facebook about whether it's more like, say, the phone company that is a communications network that doesn't monitor the signals that are sent across it - it's not the job of any of the phone companies to listen into what you're saying and decide whether it passes certain standards or not - or whether Facebook is more like a newspaper or a magazine, a content provider where Facebook has some responsibility for what is posted on the site that it owns and profits from. So has Facebook been shifting in its perception of what it is and what its responsibility is to its users in terms of the content that Facebook is exposing them to? NEWTON: I think so. You know, there's a - there are a lot of journalists over the years who have pressed Facebook to admit it's a media company rather than a technology company, which I think kind of gets to the heart of the debate that you just described. And Mark Zuckerberg, I think, kind of ended that argument when he said that he actually believes Facebook has more responsibilities than a media company, or it has more responsibilities than, you know, a newspaper or a magazine precisely because of its reach. And so that's why starting in 2017 they announced that they would hire tens of thousands of more people to work on safety and security. So there are now 30,000 people at Facebook who work on safety and security, and about half of those are content moderators. So I do think that Facebook is taking its role in public affairs much more seriously than it was, say, five years ago. GROSS: Was there a turning point, like a specific post or posts, that led them to decide, no, we need more content moderators; we need to take more responsibility for what is on Facebook? NEWTON: Absolutely. It was the - the aftermath of the 2016 election completely changed public perception of Facebook, and in time, it changed Facebook's conception of itself. So in the 2016 election, Russians attacked us, right? They attacked our democracy. They used Facebook to spread various inflammatory ideas, and Facebook was caught flat-footed. They didn't realize there were so many people creating fake accounts. They didn't realize that those fake accounts were creating fake events that real Americans were actually showing up to. And they didn't realize how fake news, hoaxes were spreading on the site and, in many cases, receiving more views than legitimate, credible information. And so you just had this maelstrom, and Mark Zuckerberg and Facebook were under enormous pressure to do something about it. And so the story of Facebook since the end of 2016 has been making up for all of the failures that led up to the 2016 presidential election and trying to ensure that a similar set of circumstances doesn't unfold again. GROSS: So Facebook is in the process of creating a content oversight board that's being described by a lot of people as, like, a Facebook Supreme Court for final judgment calls on content decisions and whether content should be purged or left on the site. And last Thursday, Facebook released a report summarizing the input that it got from a series of public listening meetings around the world. So describe what we know about what Facebook thinks the role of the content oversight board will be once it's created. And I know this is a work in progress, and Facebook doesn't really know yet. And it's still processing all the feedback and input it's gotten, but what do we know so far? NEWTON: Yeah, I mean, I think the first thing to say is just what an absolutely wild idea this is, right? So you have this private company that hosts a bunch of user posts. And they have decided that they're going to create some kind of body that is independent from the private company, and that body will make the decision about what can stay on the site and what needs to come down. And so to kind of imagine what this might look like, you may recall a few weeks back there was a big controversy because a video had gone viral on Facebook that appeared to show House Speaker Nancy Pelosi slurring her words. She looks sort of drunk. And it turned out that somebody had just altered the video to make her look drunk when she was not. And so then there was this ferocious debate about whether Facebook should leave the video up on the site, whether they should take it down or whether they should, you know, take some other measures. Ultimately they decided that they would leave it on the site but sort of label it as false. In the future, this oversight board that Facebook is now creating would be the one that would make that decision. And so the question has been, how do you set up such a board? And the result has been kind of this pseudo-constitutional convention that has been happening around the world for the past several months. And I think it's really fascinating. GROSS: And last week at the Aspen Ideas Festival, Mark Zuckerberg was interviewed by Cass Sunstein. And Zuckerberg said in the interview that he was hoping more of the industry would join in this project. NEWTON: Yeah. And I'm actually sympathetic to that because one problem with our tech companies being so large now is that while they have increasing amounts of power over our lives, they are not really accountable to us in any way. And so if you might think about a similar situation on YouTube where I post a video; it gets taken down; maybe my channel gets removed, and maybe that channel was actually my livelihood, there are thousands of people around this world, if not tens of thousands of people, for whom this is the case. Well, if your channel gets deleted, you have no Supreme Court that you can appeal to. There is no system of jurisprudence. There is no case law. There is no precedent. And so everything just winds up being this ad hoc decision. And so as a result, you have this enormous frustration in that community. There are, you know, constant death threats against YouTube executives. My hope, if I want to be an optimist about all of this - it's that by devolving some of the power that these tech companies have back to the people, that these systems will be more accountable to their everyday users, that they'll sort of seem legitimate and that we can bring some semblance of democracy to what, you know, will always remain private companies. GROSS: So Facebook does have a booklet of community standards that's available for the public to read. I have never read it. I imagine most Facebook users haven't read it, but I imagine you have. Can you give a sense of, like, what some of the standards are and in guiding people as, like, what is violent? Like, what is too violent? Like, what is too sexual? What is too hateful and hate speech and has to be taken down? NEWTON: Well, I think nudity offers a useful place to start. Often these guidelines that get written by companies start as a single page, and somebody at the company - typically not the most senior executive - will just write down a list of things that they do not think should be on their network, right? And it'll usually start with, like, no nudity. And then as the network goes along, a mother will post a photo of herself breastfeeding, and then the company will have to decide whether it wants to continue to host breastfeeding content. And then somebody will say, well, you know, we decided to permit breastfeeding but maybe not in this particular case, right? Or maybe we don't think that a nipple should be visible at this distance. And so over time, these guidelines just grow and grow and grow. And also over time, as a society, our expectations change, and things that we once thought should never be public all of a sudden we become OK with. So it's a never-ending series of infinitely branching debates, and I think it speaks to the - again, to the difficulty of the job and moderating it but also of writing the rules in the first place, right? You know, traditionally we have had systems of law to make these decisions about what is allowed and not allowed. You know, in the United States, obviously we have free speech protections. Facebook's, I think, content moderation guidelines were written in the spirit of the First Amendment to maximize the amount of speech. But it's a global network, and most countries do not have the First Amendment. And so it's just been a really chaotic time for the company as it tries to apply this one global standard in places that are not always very receptive to it. GROSS: Can you think of an example when the rules changed, like, in the course of one day about what was acceptable and what wasn't for posting? NEWTON: So there was this phenomenon that Facebook was seeing of young girls saying to each other something like, I love this ho, short for hooker. And Facebook's guidelines have traditionally said that you can't call somebody a hooker, but then they realized after getting a lot of angry feedback that these girls were actually speaking positively of their friends and that somehow what had been an insult was now a compliment. And so one day a switch was flipped, and all of a sudden, it was now OK to say, I love you, ho. GROSS: Was it context - like, did you have to figure out what the context was if you were a content moderator, whether this was a term of affection or whether it was intended as an insult? NEWTON: That's exactly right. And so, you know, think about this. You're a moderator. You don't know these people. You're looking at one sentence and maybe a couple of comments underneath it, and now it's your job to decide whether, you know, Suzy (ph) likes Jenny (ph) or not. GROSS: Right (laughter). NEWTON: It's very difficult. GROSS: (Laughter) OK. So an example that I read about in terms of content moderation is that the - a post that said autistic people should be sterilized was not taken down because autistic people aren't a protected group. What does that mean? NEWTON: So Facebook uses a standard similar to a U. S. legal standard where certain classes of people are protected. So, you know, race is a great example - a classic example of a protected class. You're not allowed to say, you know, all members of this race are bad. Autism, though, was not considered a protected class, and so if you wanted to say something that most of us would find really offensive like all autistic people should be sterilized, Facebook would just leave that up in the name of free speech. Interestingly enough, a moderator texted me about that two days ago and said that autism is now considered a protected characteristic, and so you can no longer say that. GROSS: Let's take a short break here, and then we'll talk some more. If you're just joining us, my guest is Casey Newton. He covers Silicon Valley for the tech site The Verge. We'll be right back. This is FRESH AIR. (SOUNDBITE OF MUSIC) GROSS: This is FRESH AIR, and if you're just joining us, my guest is Casey Newton. He covers Silicon Valley for the tech site The Verge, and he writes the daily newsletter The Interface about the intersection of social media and democracy. He's written two recent articles for The Verge reporting on the working conditions of Facebook's content moderators. These are the people who purge the violent, pornographic and hate speech posts. Social media, including Facebook, was used as a tool of the Russians to subvert our 2016 presidential election, and many Americans are very concerned about what the Russians or the Chinese or the Iranians or other actors might do to try to use social media to subvert the next presidential election. What is Facebook doing to prepare for the election? NEWTON: So it turns out that one pretty good, easy thing that you can do to reduce the risk of that kind of thing happening is to catch fake accounts. So Facebook has this ironclad rule that every account - you know, you're only allowed to have one account, and it has to be your real name, and that's that - no exceptions. And so if you're, say, a Russian agent, you probably are not going to be able to create a bunch of posts that go viral in America criticizing an American political candidate because it will just sort of be clear that, you know, you don't belong in this political discussion. But if you can create an account that says, you know, Joe Smith and you live on Main Street, USA, maybe you can create a page that gets a million followers and all of a sudden is able to start posting a lot of inflammatory memes and rhetoric. So that's a major area where Facebook has invested - is just cracking down on fake accounts. They now say that they detect millions of them every week. And of course it's a cat-and-mouse game where the Russians are always getting better, but that is one place where they've been able to focus. They've also placed a lot of new requirements around ads. So for example, there is now an archive that you can visit to see every single ad that a political candidate bought or even just a political ad that was created by a political action committee. You know, in the 2016 election, we didn't really know what ads people were seeing because the ads could be targeted to, you know, down to the level of one person, and it would just sort of never be publicly searchable. That's not the case anymore. Now you can actually dive in. Researchers are, you know, putting together collections of these ads and studying them in real time. So those are kind of two things that I would highlight as ways that Facebook is trying to detect these, you know, networks of bad actors and disrupt them. GROSS: So, you know, one of the things social media is trying to contend with now is extremists. For example, Facebook initially banned Alex Jones' network from the site, and then it banned Alex Jones himself. What are the guidelines that Facebook uses to decide who's too extreme to post? NEWTON: It's a great question and one where the answer has changed pretty significantly over the past year. Last summer was when attention really started to turn on this question of Alex Jones and how he was using social networks not just to reach an audience but to build an audience because all of these networks have recommendation algorithms and search algorithms that can sort of point more and more people at you. And I think Facebook started to look at the effect that Alex Jones was having on the world. Sort of most famously, there was a family of a victim of the Sandy Hook shooting who had moved seven times because Alex Jones' followers had been moved by him to harass the family of the dead little boy because Alex Jones had released a number of videos in which he suggested that the Sandy Hook shooting had all been faked and was a pretext for the government to take away people's guns. And so at a certain point, Facebook has to ask itself, why do these rules exist, and why is it that the rules right now seem to be doing a better job of protecting Alex Jones' right to speak his mind than they do the victim of a shooting, you know, whose family can no longer regularly visit their son's grave? And I think as that thinking started to reverberate through the social network, they made the decision that Alex Jones was what they call a hate figure, which means that, you know, not only is he not allowed on the site, but no one can post a statement or a video in which they say, Alex Jones is great, or, you know, sort of express support for his ideas, you know? But what I think you're touching on is that these are really subjective calls. These are political calls. These are calls that a lot of folks are going to disagree with, and so I think we're really kind of having one of the great reckonings over free speech globally that we've had in a long time. And there isn't one great answer. It's always a question of, how are you going to manage all of the tradeoffs? GROSS: Since you cover the Silicon Valley and social media on a regular basis and you have a daily newsletter, what are some of the things you're looking to in the near future, like issues that are emerging now or changes that you expect will emerge very soon? NEWTON: I think we are having a reckoning over the Internet globally because tech companies have gotten so big and so consequential. And so we are reckoning with this power, and you see it in a hundred different ways, whether it's the debates that we're having about facial recognition technology or deepfakes or independent oversight boards. I think you're just seeing this sense that there is not a system of justice that can adequately protect us from how rapid technology is changing the world around us. And so what I am trying to do is just understand that reckoning a little bit better, every day try to anticipate some of the changes, try to help to frame some of those debates. You know, I don't think you can really overstate how dramatically the Internet has reshaped our lives already but how much it's going to reshape them in the future. And so, you know, I will never have a shortage of things to write about. GROSS: Casey Newton, thank you so much for your reporting, and thank you so much for talking with us. NEWTON: Oh, Terry, it was an absolute pleasure. GROSS: Casey Newton covers the Silicon Valley for the tech site The Verge and writes the newsletter The Interface about the intersection of social media and democracy. After we take a short break, Ken Tucker will review country hits with a hip-hop sensibility by Lil Nas X and Blanco Brown. This is FRESH AIR. (SOUNDBITE OF CALEXICO'S \"PRASKOVIA\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-01-736990455": {"title": "GOP Hopes New Small-Donor Platform WinRed Catches Up With Democrats' ActBlue : NPR", "url": "https://www.npr.org/2019/07/01/736990455/red-shift-how-republicans-plan-to-catch-democrats-in-online-fundraising", "author": "No author found", "published_date": "2019-07-01", "content": "", "section": "Politics", "disclaimer": ""}, "2019-07-07-739316746": {"title": "Microsoft Erases E-Library And Digital Rights Management Server : NPR", "url": "https://www.npr.org/2019/07/07/739316746/microsoft-closes-the-book-on-its-e-library-erasing-all-user-content", "author": "No author found", "published_date": "2019-07-07", "content": "", "section": "Technology", "disclaimer": ""}, "2019-07-07-739288262": {"title": "Microsoft To Erase E-Books : NPR", "url": "https://www.npr.org/2019/07/07/739288262/microsoft-to-erase-e-books", "author": "No author found", "published_date": "2019-07-07", "content": "LULU GARCIA-NAVARRO, HOST:  In this era of streaming and downloading, what do we really own? That song on Spotify; maybe not. That e-book you're reading at the beach right now; think again. You may have access, but you don't always have ownership. This is known as DRM - digital rights management. Now Microsoft has closed its e-book department and is scheduled to start removing all electronic books from users' libraries. Joining us now to discuss this is Aaron Perzanowski. He is the author of \"The End Of Ownership: Personal Property In The Digital Economy. \"Welcome. AARON PERZANOWSKI: Great to be here. GARCIA-NAVARRO: So if I'm in the middle of reading \"Anna Karenina\" and I fire up my Kindle to continue and it might just be gone, I never find out what happens. PERZANOWSKI: That's certainly a possibility. This is a problem that stretches back about a decade, when Amazon deleted a number of books from the collections of its customers, including, quite ironically, George Orwell's \"1984. \"GARCIA-NAVARRO: So do you get a refund? Do you get reimbursed for your loss? PERZANOWSKI: In most of these instances, the companies have provided refunds. That's not always the case. I think there's also a question of whether or not a refund really makes the consumer whole. In many of these platforms, you're able to make annotations or take notes or highlight passages. And a refund might not make you whole for the investment that you've made in researching and reading a particular work. GARCIA-NAVARRO: Well, yeah. I read that Microsoft will throw in an extra 25 bucks if you added markups to your e-book, but maybe some of those notes were crucial - right? - if you're, say, a teacher or a lawyer. And now they're gone. PERZANOWSKI: Yeah. I can say as an academic researcher, $25 is not going to be enough to satisfy me if I lose, you know, months or years of research that I've invested in one of these e-book platforms. GARCIA-NAVARRO: What other goods, though, have DRMs? PERZANOWSKI: One of the things that I think people don't realize that's crucially important is that DRM and related software tools are embedded in all sorts of devices that we buy - your car, your smart home appliances, your home security system, right? All of these systems have software that allows for this kind of control over how the devices are used. And I think we're going to see these same sorts of situations crop up in the context of physical devices that are being used in people's homes. GARCIA-NAVARRO: DRM was originally meant as an anti-piracy measure, but it's seemingly become more of a way for companies to lock consumers into their ecosystems. PERZANOWSKI: I think that's absolutely right. The initial vision for DRM was that it was going to allow for the sale of digital goods online in a way that reduced the risk of copyright infringement. As this technology has been deployed, what we've seen is that the big beneficiaries of DRM have not been copyright holders. They have been technology companies like Amazon, like Microsoft, who are able to control these ecosystems to make it harder for consumers to switch over to new platforms. GARCIA-NAVARRO: So what is a consumer to do? Can I buy a non-DRM song or a book title or movie? PERZANOWSKI: It depends on what sort of content you're interested in. Interestingly, the digital download music market moved away from DRM quite a long time ago. In the context of movies, e-books, video games, DRM is still very common, even though they present real downsides for consumers. GARCIA-NAVARRO: I guess there's another alternative, which is to actually physically buy things like a printed book and (laughter) a DVD that we can have and hold. PERZANOWSKI: So I think that's absolutely true in the media context. When it comes to physical devices, though, you know, you can go out and buy a car. And you think you own the car because it's parked in your garage. But in reality, how it functions, who can repair it, what replacement parts are compatible with it - all of that is controlled through software code. And so I think that line between the physical and the digital is getting increasingly blurry. GARCIA-NAVARRO: Aaron Perzanowski is a law professor at Case Western Reserve University. Thank you so much for speaking with us. PERZANOWSKI: Thank you. LULU GARCIA-NAVARRO, HOST:   In this era of streaming and downloading, what do we really own? That song on Spotify; maybe not. That e-book you're reading at the beach right now; think again. You may have access, but you don't always have ownership. This is known as DRM - digital rights management. Now Microsoft has closed its e-book department and is scheduled to start removing all electronic books from users' libraries. Joining us now to discuss this is Aaron Perzanowski. He is the author of \"The End Of Ownership: Personal Property In The Digital Economy. \" Welcome. AARON PERZANOWSKI: Great to be here. GARCIA-NAVARRO: So if I'm in the middle of reading \"Anna Karenina\" and I fire up my Kindle to continue and it might just be gone, I never find out what happens. PERZANOWSKI: That's certainly a possibility. This is a problem that stretches back about a decade, when Amazon deleted a number of books from the collections of its customers, including, quite ironically, George Orwell's \"1984. \" GARCIA-NAVARRO: So do you get a refund? Do you get reimbursed for your loss? PERZANOWSKI: In most of these instances, the companies have provided refunds. That's not always the case. I think there's also a question of whether or not a refund really makes the consumer whole. In many of these platforms, you're able to make annotations or take notes or highlight passages. And a refund might not make you whole for the investment that you've made in researching and reading a particular work. GARCIA-NAVARRO: Well, yeah. I read that Microsoft will throw in an extra 25 bucks if you added markups to your e-book, but maybe some of those notes were crucial - right? - if you're, say, a teacher or a lawyer. And now they're gone. PERZANOWSKI: Yeah. I can say as an academic researcher, $25 is not going to be enough to satisfy me if I lose, you know, months or years of research that I've invested in one of these e-book platforms. GARCIA-NAVARRO: What other goods, though, have DRMs? PERZANOWSKI: One of the things that I think people don't realize that's crucially important is that DRM and related software tools are embedded in all sorts of devices that we buy - your car, your smart home appliances, your home security system, right? All of these systems have software that allows for this kind of control over how the devices are used. And I think we're going to see these same sorts of situations crop up in the context of physical devices that are being used in people's homes. GARCIA-NAVARRO: DRM was originally meant as an anti-piracy measure, but it's seemingly become more of a way for companies to lock consumers into their ecosystems. PERZANOWSKI: I think that's absolutely right. The initial vision for DRM was that it was going to allow for the sale of digital goods online in a way that reduced the risk of copyright infringement. As this technology has been deployed, what we've seen is that the big beneficiaries of DRM have not been copyright holders. They have been technology companies like Amazon, like Microsoft, who are able to control these ecosystems to make it harder for consumers to switch over to new platforms. GARCIA-NAVARRO: So what is a consumer to do? Can I buy a non-DRM song or a book title or movie? PERZANOWSKI: It depends on what sort of content you're interested in. Interestingly, the digital download music market moved away from DRM quite a long time ago. In the context of movies, e-books, video games, DRM is still very common, even though they present real downsides for consumers. GARCIA-NAVARRO: I guess there's another alternative, which is to actually physically buy things like a printed book and (laughter) a DVD that we can have and hold. PERZANOWSKI: So I think that's absolutely true in the media context. When it comes to physical devices, though, you know, you can go out and buy a car. And you think you own the car because it's parked in your garage. But in reality, how it functions, who can repair it, what replacement parts are compatible with it - all of that is controlled through software code. And so I think that line between the physical and the digital is getting increasingly blurry. GARCIA-NAVARRO: Aaron Perzanowski is a law professor at Case Western Reserve University. Thank you so much for speaking with us. PERZANOWSKI: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-08-739643786": {"title": "ICE Turned To DMV Driver's License Databases For Help With Facial Recognition : NPR", "url": "https://www.npr.org/2019/07/08/739643786/ice-turned-to-dmv-drivers-license-databases-for-help-with-facial-recognition", "author": "No author found", "published_date": "2019-07-08", "content": "AUDIE CORNISH, HOST: Now we're going to look more broadly at what's been revealed today about ICE turning to DMV offices for help with facial recognition - that is, using driver's license photographs and algorithms to identify people suspected of being in the country illegally. Now, this collaboration was unearthed by a team at Georgetown University, and here to brief us is NPR's Aarti Shahani. Hey there, Aarti. AARTI SHAHANI, BYLINE: Hi. CORNISH: I understand that in the past, ICE has gone to DMV offices and just asked for records on immigrants. We just heard about the case in Vermont that alleges that much. What exactly is new here? SHAHANI: So what was uncovered is that ICE agents, while looking for undocumented people, ended up having extraordinary access to the state records of American citizens. Lawyers at Georgetown's Center on Privacy and Technology have been submitting Freedom of Information lawsuits to DMVs around the country, trying to learn what they can about how each state does or doesn't collaborate with ICE. Three states - Utah, Washington and Vermont - handed over documents showing that ICE was not just reaching out to them with targeted searches. ICE agents were not just saying, hey, here's a specific person we want, their full name and date of birth; can you share what you've got on them? Instead, ICE was saying, hey, we've got a high-resolution picture of somebody who entered the U. S. on a visa. We believe this person overstayed. Can you take this picture, run it through your database which includes many, if not mostly, U. S. citizens and give us the faces that match this one? Basically, help us pair of visa photos to license photos. These requests happen from 2015 to 2017. CORNISH: And I understand according to the Georgetown findings, these three states did do it - right? - hand over facial recognition matches to ICE. SHAHANI: Well, according to the FOIA documents, Utah and Vermont did, and with Washington, it's unclear. The agency told The Washington Post, which had first reported this, that they just respond to court orders. As you'd mentioned, NPR also reached out to ICE. And I'd add that ICE - the ICE spokesperson said that what they're doing is consistent with what other law enforcement agencies do. Now, it is important to point out facial recognition has done plenty of good in this world. It's helped find missing children and reunite with them with their families. But in this instance, activists have raised concerns. They say there is a bait-and-switch going on. Not every state lets undocumented immigrants get a license. These three states are among those that do. They're signaling to undocumented people it is safe to come here and apply for your driver's license. But then the DMVs are turning around and handing files over to deportation officers. CORNISH: What about the reliability of the technology itself? Is facial recognition far enough along that we can be counting on it in this way? SHAHANI: Well, last year, the MIT Media Lab did a study. It found that leading software was accurate 99% of the time when it came to identifying the gender of white males. So failure in only 1 out of a hundred. But with darker-skinned women, it failed to identify them as women 1 out of 3 times. So that is a huge disparity. Alvaro Bedoya, one of the Georgetown lawyers - he called the ICE-DMV tag-teaming a dragnet. And he says citizens of color are particularly vulnerable here. This is him. ALVARO BEDOYA: The question that people need to ask themselves in these states is not, am I my undocumented, but rather, does a flawed face recognition algorithm think that I look like someone who is undocumented? SHAHANI: And he is not alone in his concerns. San Francisco recently banned the use of facial recognition by police and city agencies. The company that's the largest maker of police body cameras says it's not going to sell facial recognition tech for now because it's just not reliable enough. Experts from some of the biggest tech companies like Google and Microsoft have petitioned Amazon to stop selling facial recognition tech for that same reason, though Amazon is still selling it, including to government agencies. So we're seeing very powerful entities at odds. CORNISH: That's NPR's Aarti Shahani. Aarti, thank you. SHAHANI: Thank you. AUDIE CORNISH, HOST:  Now we're going to look more broadly at what's been revealed today about ICE turning to DMV offices for help with facial recognition - that is, using driver's license photographs and algorithms to identify people suspected of being in the country illegally. Now, this collaboration was unearthed by a team at Georgetown University, and here to brief us is NPR's Aarti Shahani. Hey there, Aarti. AARTI SHAHANI, BYLINE: Hi. CORNISH: I understand that in the past, ICE has gone to DMV offices and just asked for records on immigrants. We just heard about the case in Vermont that alleges that much. What exactly is new here? SHAHANI: So what was uncovered is that ICE agents, while looking for undocumented people, ended up having extraordinary access to the state records of American citizens. Lawyers at Georgetown's Center on Privacy and Technology have been submitting Freedom of Information lawsuits to DMVs around the country, trying to learn what they can about how each state does or doesn't collaborate with ICE. Three states - Utah, Washington and Vermont - handed over documents showing that ICE was not just reaching out to them with targeted searches. ICE agents were not just saying, hey, here's a specific person we want, their full name and date of birth; can you share what you've got on them? Instead, ICE was saying, hey, we've got a high-resolution picture of somebody who entered the U. S. on a visa. We believe this person overstayed. Can you take this picture, run it through your database which includes many, if not mostly, U. S. citizens and give us the faces that match this one? Basically, help us pair of visa photos to license photos. These requests happen from 2015 to 2017. CORNISH: And I understand according to the Georgetown findings, these three states did do it - right? - hand over facial recognition matches to ICE. SHAHANI: Well, according to the FOIA documents, Utah and Vermont did, and with Washington, it's unclear. The agency told The Washington Post, which had first reported this, that they just respond to court orders. As you'd mentioned, NPR also reached out to ICE. And I'd add that ICE - the ICE spokesperson said that what they're doing is consistent with what other law enforcement agencies do. Now, it is important to point out facial recognition has done plenty of good in this world. It's helped find missing children and reunite with them with their families. But in this instance, activists have raised concerns. They say there is a bait-and-switch going on. Not every state lets undocumented immigrants get a license. These three states are among those that do. They're signaling to undocumented people it is safe to come here and apply for your driver's license. But then the DMVs are turning around and handing files over to deportation officers. CORNISH: What about the reliability of the technology itself? Is facial recognition far enough along that we can be counting on it in this way? SHAHANI: Well, last year, the MIT Media Lab did a study. It found that leading software was accurate 99% of the time when it came to identifying the gender of white males. So failure in only 1 out of a hundred. But with darker-skinned women, it failed to identify them as women 1 out of 3 times. So that is a huge disparity. Alvaro Bedoya, one of the Georgetown lawyers - he called the ICE-DMV tag-teaming a dragnet. And he says citizens of color are particularly vulnerable here. This is him. ALVARO BEDOYA: The question that people need to ask themselves in these states is not, am I my undocumented, but rather, does a flawed face recognition algorithm think that I look like someone who is undocumented? SHAHANI: And he is not alone in his concerns. San Francisco recently banned the use of facial recognition by police and city agencies. The company that's the largest maker of police body cameras says it's not going to sell facial recognition tech for now because it's just not reliable enough. Experts from some of the biggest tech companies like Google and Microsoft have petitioned Amazon to stop selling facial recognition tech for that same reason, though Amazon is still selling it, including to government agencies. So we're seeing very powerful entities at odds. CORNISH: That's NPR's Aarti Shahani. Aarti, thank you. SHAHANI: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-08-739491857": {"title": "ICE Uses Facial Recognition To Sift State Driver's License Records, Researchers Say : NPR", "url": "https://www.npr.org/2019/07/08/739491857/ice-uses-facial-recognition-to-sift-state-drivers-license-records-researchers-sa", "author": "No author found", "published_date": "2019-07-08", "content": "", "section": "Law", "disclaimer": ""}, "2019-07-08-739464205": {"title": "FBI, ICE Use State Driver's License Databases To Scan Photos : NPR", "url": "https://www.npr.org/2019/07/08/739464205/fbi-ice-use-state-drivers-license-databases-to-scan-photos", "author": "No author found", "published_date": "2019-07-08", "content": "RACHEL MARTIN, HOST:  The FBI and ICE are using state driver's license databases as a sort of gold mine for facial recognition. That information is coming to light because of documents obtained by Georgetown Law's Center on Privacy and Technology and reported by The Washington Post. The documents show that federal agencies are looking through millions of Americans' photos without their consent or their knowledge. Alvaro Bedoya is the founding director of the Center on Privacy and Technology and joins us now. Thank you so much for being with us. ALVARO BEDOYA: Thanks for having me. MARTIN: Can you explain what these records are? Where did you get them and what do they show? BEDOYA: Sure. For several years now, we've been filing Freedom of Information Act requests with police departments and DMVs across the country. And in this most recent batch, we found that while states across the country, over a dozen states, are actually urging undocumented people to come out of the shadows and to get a driver's license, in at least three of them - in Washington, Utah and Vermont - ICE is actually taking advantage of that to secretly find and deport those people using face recognition technology. And in our view, this is a scandal and a huge betrayal of undocumented people. MARTIN: Do you have evidence of that intention? BEDOYA: I'm sorry - of what intention? MARTIN: Of the intention of ICE to use the software recognition technology to deport people who are in this country illegally. BEDOYA: Sure. So we do have written requests from ICE in all three states, requesting that Departments of Motor Vehicle find people who are undocumented for the purposes of investigation and arrest. We do have that evidence, yes. MARTIN: And the Department of Motor Vehicles, I mean, what's their role in this? They're just granting access to ICE? BEDOYA: Exactly right - often without the citizens of the state knowing about it. What happens is that ICE takes advantage of a two-decade-old law that says that, in general, DMV should cooperate with law enforcement. And this law was written before face recognition existed, and ICE uses it to ask that the DMVs comply. And the DMVs do so usually in secret and without telling the people in the state. And I think it's really important for folks to realize that, even if you're not undocumented, this does affect you because the software is biased and doesn't really detect or find people of color, women or young people really well. The question isn't whether you're undocumented, but rather whether a flawed algorithm thinks you look like someone who's undocumented. MARTIN: Literally, in The Washington Post's reporting of this, it highlights, as you just did, that the technology itself has a racial bias, that it actually generates more false positives or just isn't as accurate when it comes to people of color. BEDOYA: Exactly right. Peer-reviewed study after peer-reviewed study - most recently out of MIT, Joy Buolamwini and Timnit Gebru - have shown that the technology underperforms on people who are women, on people with darker skin tones. And despite all of this, the federal government, ICE, FBI and police departments across the country continue to use it as if it were neutral, and that's not, in fact, the case. MARTIN: Has the FBI - because the FBI is using this as well, has the FBI or ICE explained their justification for using this technology? BEDOYA: Sure. They typically say that it's an investigative tool. The FBI has been slightly more forthcoming than ICE. ICE has essentially hid behind a, you know, we-don't-talk-about-investigative-techniques response. But the FBI said that it's been an investigative tool in their arsenal. The issue is that, unlike a large majority of law enforcement technology, it is inherently biased against certain groups of people - people who tend to be overrepresented in the criminal justice system. And so they talk about it as if it's, you know, just one more tool in their toolkit, when reality - it's a biased tool, a secret tool (ph) and a problem. MARTIN: Is there any sort of congressional authorization for something like this? BEDOYA: We think there clearly is not. In fact, in the most recent two oversight hearings before the House Oversight Committee, Republicans and Democrats alike said that it was an outrage that the federal government was searching driver's licenses in this way. This is not authorized on a bipartisan basis. MARTIN: Alvaro Bedoya with the Center on Privacy and Technology at Georgetown. Thank you so much for your time. BEDOYA: Thank you for having me. RACHEL MARTIN, HOST:   The FBI and ICE are using state driver's license databases as a sort of gold mine for facial recognition. That information is coming to light because of documents obtained by Georgetown Law's Center on Privacy and Technology and reported by The Washington Post. The documents show that federal agencies are looking through millions of Americans' photos without their consent or their knowledge. Alvaro Bedoya is the founding director of the Center on Privacy and Technology and joins us now. Thank you so much for being with us. ALVARO BEDOYA: Thanks for having me. MARTIN: Can you explain what these records are? Where did you get them and what do they show? BEDOYA: Sure. For several years now, we've been filing Freedom of Information Act requests with police departments and DMVs across the country. And in this most recent batch, we found that while states across the country, over a dozen states, are actually urging undocumented people to come out of the shadows and to get a driver's license, in at least three of them - in Washington, Utah and Vermont - ICE is actually taking advantage of that to secretly find and deport those people using face recognition technology. And in our view, this is a scandal and a huge betrayal of undocumented people. MARTIN: Do you have evidence of that intention? BEDOYA: I'm sorry - of what intention? MARTIN: Of the intention of ICE to use the software recognition technology to deport people who are in this country illegally. BEDOYA: Sure. So we do have written requests from ICE in all three states, requesting that Departments of Motor Vehicle find people who are undocumented for the purposes of investigation and arrest. We do have that evidence, yes. MARTIN: And the Department of Motor Vehicles, I mean, what's their role in this? They're just granting access to ICE? BEDOYA: Exactly right - often without the citizens of the state knowing about it. What happens is that ICE takes advantage of a two-decade-old law that says that, in general, DMV should cooperate with law enforcement. And this law was written before face recognition existed, and ICE uses it to ask that the DMVs comply. And the DMVs do so usually in secret and without telling the people in the state. And I think it's really important for folks to realize that, even if you're not undocumented, this does affect you because the software is biased and doesn't really detect or find people of color, women or young people really well. The question isn't whether you're undocumented, but rather whether a flawed algorithm thinks you look like someone who's undocumented. MARTIN: Literally, in The Washington Post's reporting of this, it highlights, as you just did, that the technology itself has a racial bias, that it actually generates more false positives or just isn't as accurate when it comes to people of color. BEDOYA: Exactly right. Peer-reviewed study after peer-reviewed study - most recently out of MIT, Joy Buolamwini and Timnit Gebru - have shown that the technology underperforms on people who are women, on people with darker skin tones. And despite all of this, the federal government, ICE, FBI and police departments across the country continue to use it as if it were neutral, and that's not, in fact, the case. MARTIN: Has the FBI - because the FBI is using this as well, has the FBI or ICE explained their justification for using this technology? BEDOYA: Sure. They typically say that it's an investigative tool. The FBI has been slightly more forthcoming than ICE. ICE has essentially hid behind a, you know, we-don't-talk-about-investigative-techniques response. But the FBI said that it's been an investigative tool in their arsenal. The issue is that, unlike a large majority of law enforcement technology, it is inherently biased against certain groups of people - people who tend to be overrepresented in the criminal justice system. And so they talk about it as if it's, you know, just one more tool in their toolkit, when reality - it's a biased tool, a secret tool (ph) and a problem. MARTIN: Is there any sort of congressional authorization for something like this? BEDOYA: We think there clearly is not. In fact, in the most recent two oversight hearings before the House Oversight Committee, Republicans and Democrats alike said that it was an outrage that the federal government was searching driver's licenses in this way. This is not authorized on a bipartisan basis. MARTIN: Alvaro Bedoya with the Center on Privacy and Technology at Georgetown. Thank you so much for your time. BEDOYA: Thank you for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-08-733714689": {"title": "Robots, Not Humans, Are The New Space Explorers : NPR", "url": "https://www.npr.org/2019/07/08/733714689/robots-not-humans-are-the-new-space-explorers", "author": "No author found", "published_date": "2019-07-08", "content": "RACHEL MARTIN, HOST:  This month we're marking the 50th anniversary of an achievement that still inspires - putting astronauts on the moon. But as NPR's Joe Palca reports, another kind of explorer is responsible for much of the modern enthusiasm for space exploration. JOE PALCA, BYLINE: Look, humans do still explore space. The International Space Station is a remarkable piece of hardware. But as amazing as the space station is, excitement about it is relatively modest. EMILY LAKDAWALLA: Well, you know, since the days of Apollo, the greatest adventures in space have been these robots that have gone all over the solar system. PALCA: Emily Lakdawalla works for The Planetary Society. By these robots, Lakdawalla is referring to the various probes that have flown past planets, moons and asteroids, orbiting some, landing on others. Lakdawalla says, sure, it would be inspiring to see an astronaut land on Mars, but in the meantime, inspiration is coming from NASA's Mars rovers. LAKDAWALLA: It's very easy to anthropomorphize them and imagine ourselves rolling across the surface of Mars. I think it's really not a stretch to imagine ourselves in the place of these robots exploring across the solar system. PALCA: Millions of people around the world have watched with delight as the six-wheeled rovers have trundled across the Martian surface snapping pictures and taking selfies. (SOUNDBITE OF MUSEUM AMBIENCE)PALCA: At the National Air and Space Museum, Matthew Shindell says visitors make a point of coming to the planetary exhibit. MATTHEW SHINDELL: This gallery is kind of tucked into the west end of the museum. PALCA: Shindell is the museum's planetary science curator. A prominent feature of the gallery is a large glass case containing versions of all the rovers NASA sent to Mars, starting with the tiny Sojourner rover that landed in 1997, right up to the 2,000-pound Curiosity rover that arrived in 2012. SHINDELL: People love to come and see the rovers and really get a sense of what they look like up close and their actual scale. PALCA: Now, Mars has a mystique all its own so perhaps it's not surprising that people find exploring the Red Planet enticing. But the New Horizons' flyby of Pluto, the Messenger's close encounter with Mercury and Juno buzzing over Jupiter's poles have all generated great public interest. And Lindy Elkins-Tanton of Arizona State University says the public's appetite for space exploration goes beyond the familiar bits of the solar system. Elkins-Tanton is lead scientist on a mission to a weird asteroid known as Psyche. It's weird because Psyche is made entirely of metal, the only asteroid like it in our solar system. True, most people haven't heard of Psyche. But Elkins-Tanton says when they learn about it, they're hooked. LINDY ELKINS-TANTON: We're sending this probe to a place that is absolutely uninhabitable in an Earth kind of sense, and yet the engagement we're getting from the public already, two years before launch, is profound. PALCA: Elkins-Tanton says she and her team are inviting the public to explore Psyche with them. ELKINS-TANTON: We're going to be sending the images that we get out onto the Internet for everyone in the world to see within a half-hour of our receiving them. So everyone in the world is going to see this crazy world at the same time, and we can all scratch our heads together. PALCA: The idea of sharing the experience of space missions is something NASA has embraced. Emily Lakdawalla says the public has been able to see the passion and commitment of the scientists who work on these robotic explorers. She points to the day NASA decided to end the Cassini mission to Saturn by sending the probe into the planet's atmosphere, where it burned up. LAKDAWALLA: There were cameras on the scientists and engineers who were weeping openly at the end of this spacecraft. It really humanized the mission. And so it made that human connection that maybe in the past you only got through astronauts. Now you recognize that robotic exploration is human exploration. It's just that the humans are back on Earth, and the robots are going where humans can't currently go. PALCA: Joe Palca, NPR News. RACHEL MARTIN, HOST:   This month we're marking the 50th anniversary of an achievement that still inspires - putting astronauts on the moon. But as NPR's Joe Palca reports, another kind of explorer is responsible for much of the modern enthusiasm for space exploration. JOE PALCA, BYLINE: Look, humans do still explore space. The International Space Station is a remarkable piece of hardware. But as amazing as the space station is, excitement about it is relatively modest. EMILY LAKDAWALLA: Well, you know, since the days of Apollo, the greatest adventures in space have been these robots that have gone all over the solar system. PALCA: Emily Lakdawalla works for The Planetary Society. By these robots, Lakdawalla is referring to the various probes that have flown past planets, moons and asteroids, orbiting some, landing on others. Lakdawalla says, sure, it would be inspiring to see an astronaut land on Mars, but in the meantime, inspiration is coming from NASA's Mars rovers. LAKDAWALLA: It's very easy to anthropomorphize them and imagine ourselves rolling across the surface of Mars. I think it's really not a stretch to imagine ourselves in the place of these robots exploring across the solar system. PALCA: Millions of people around the world have watched with delight as the six-wheeled rovers have trundled across the Martian surface snapping pictures and taking selfies. (SOUNDBITE OF MUSEUM AMBIENCE) PALCA: At the National Air and Space Museum, Matthew Shindell says visitors make a point of coming to the planetary exhibit. MATTHEW SHINDELL: This gallery is kind of tucked into the west end of the museum. PALCA: Shindell is the museum's planetary science curator. A prominent feature of the gallery is a large glass case containing versions of all the rovers NASA sent to Mars, starting with the tiny Sojourner rover that landed in 1997, right up to the 2,000-pound Curiosity rover that arrived in 2012. SHINDELL: People love to come and see the rovers and really get a sense of what they look like up close and their actual scale. PALCA: Now, Mars has a mystique all its own so perhaps it's not surprising that people find exploring the Red Planet enticing. But the New Horizons' flyby of Pluto, the Messenger's close encounter with Mercury and Juno buzzing over Jupiter's poles have all generated great public interest. And Lindy Elkins-Tanton of Arizona State University says the public's appetite for space exploration goes beyond the familiar bits of the solar system. Elkins-Tanton is lead scientist on a mission to a weird asteroid known as Psyche. It's weird because Psyche is made entirely of metal, the only asteroid like it in our solar system. True, most people haven't heard of Psyche. But Elkins-Tanton says when they learn about it, they're hooked. LINDY ELKINS-TANTON: We're sending this probe to a place that is absolutely uninhabitable in an Earth kind of sense, and yet the engagement we're getting from the public already, two years before launch, is profound. PALCA: Elkins-Tanton says she and her team are inviting the public to explore Psyche with them. ELKINS-TANTON: We're going to be sending the images that we get out onto the Internet for everyone in the world to see within a half-hour of our receiving them. So everyone in the world is going to see this crazy world at the same time, and we can all scratch our heads together. PALCA: The idea of sharing the experience of space missions is something NASA has embraced. Emily Lakdawalla says the public has been able to see the passion and commitment of the scientists who work on these robotic explorers. She points to the day NASA decided to end the Cassini mission to Saturn by sending the probe into the planet's atmosphere, where it burned up. LAKDAWALLA: There were cameras on the scientists and engineers who were weeping openly at the end of this spacecraft. It really humanized the mission. And so it made that human connection that maybe in the past you only got through astronauts. Now you recognize that robotic exploration is human exploration. It's just that the humans are back on Earth, and the robots are going where humans can't currently go. PALCA: Joe Palca, NPR News.", "section": "The Apollo 11 Moon Landing, 50 Years Later", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-09-739923528": {"title": "Federal Appeals Court Rules President Trump Can't Block Twitter Followers : NPR", "url": "https://www.npr.org/2019/07/09/739923528/trump-cant-block-twitter-followers-federal-appeals-court-rules", "author": "No author found", "published_date": "2019-07-09", "content": "ARI SHAPIRO, HOST: A federal appeals court has upheld a ruling that President Trump cannot block critics from his Twitter account. The decision comes at a time when many government officials are using social media platforms to communicate with the public. Here to talk about this case is NPR's Aarti Shahani. Hey there. AARTI SHAHANI, BYLINE: Hi. SHAPIRO: So this comes from an appellate court in New York. Tell us about the case. SHAHANI: Yes. It was a unanimous decision. All three judges agreed to uphold the lower court ruling. The case started two years ago. A few people joined a lawsuit against the president. They included a doctor, a university professor, a comedian and a police officer. Trump had blocked each of them from his Twitter account after they criticized him. And they said, hey, you can't do that. I've got freedom of speech. Blocking means a user who's logged in can't tweet at the president, read his tweets or join in and respond to other people commenting. Twitter created this feature after people complained about being harassed, say, by bullies or exes. Blocking lets you control who you hear from. Press some buttons and, bam, you create an echo chamber of just your adoring fans. Trump arguably did a version of that when he blocked his critics. And the problem with that, according to the court, is that the president uses Twitter in his official capacity as a public servant. It doesn't matter that he started his account in 2009 when he was a private citizen. That's not how he uses it anymore. @realDonaldTrump isn't where the president talks about his golf game. It's where he announces major decisions, like firing his chief of staff or banning transgender people in the military. As my esteemed colleague David Folkenflik puts it, the president's Twitter is his mood ring. SHAPIRO: What has the response to this ruling been from the president or the administration? SHAHANI: Well, the president has not tweeted about it yet. A Justice Department spokesperson said that the DOJ is disappointed with the court's decision and is exploring next steps. The DOJ reiterated its argument from court that the president's account is not a public forum. Blocking users is private conduct. Now, that argument didn't fly because the administration had said so much to the contrary. As the judges pointed out, Trump's former press secretary, Sean Spicer, called the tweets official statements. SHAPIRO: So beyond the president, what does this ruling mean for other politicians or public figures who are not elected politicians - I mean, movie stars, for example? SHAHANI: Well, it really focuses on public officials serving in a public capacity, OK, and that's key. Trump is the most high-profile blocker, but he's by no means the only official public servant doing so. I spoke with Jameel Jaffer. He is the director of the Knight First Amendment Institute at Columbia University. He represented a group that filed the lawsuit against Trump. He also sued a Democrat, a county official in Virginia, for the same kind of behavior. Jaffer said the purpose of suing Trump is to bring the free speech rights that exist in the physical world into digital life. JAMEEL JAFFER: The whole point of the case was to take this body of law that exists off line - and that applies to spaces like town hall and city council meetings - and ensure that that same body of law would be applied to these new digital spaces that are increasingly important to our democracy. SHAHANI: And so what he's saying there is that in the real world at a town hall, the president can cover his ears if he doesn't like what someone has to say, but he can't make them shut up. So Jaffer says that's what Trump was effectively doing by blocking his critics on Twitter and the kind of activity he's trying to reverse. SHAPIRO: OK. So the president or a county official in Virginia can't block people on Twitter. What about when platforms delete accounts? Just briefly, does this ruling apply there? SHAHANI: Yeah. So the judges were careful to spell out the limits of this ruling. They said it's about public officials working in their official capacity. The judges are not deciding whether the social media companies are bound by the First Amendment. They were clear about that constraint. SHAPIRO: That's NPR's Aarti Shahani. Thanks so much. SHAHANI: Thanks very much. ARI SHAPIRO, HOST:  A federal appeals court has upheld a ruling that President Trump cannot block critics from his Twitter account. The decision comes at a time when many government officials are using social media platforms to communicate with the public. Here to talk about this case is NPR's Aarti Shahani. Hey there. AARTI SHAHANI, BYLINE: Hi. SHAPIRO: So this comes from an appellate court in New York. Tell us about the case. SHAHANI: Yes. It was a unanimous decision. All three judges agreed to uphold the lower court ruling. The case started two years ago. A few people joined a lawsuit against the president. They included a doctor, a university professor, a comedian and a police officer. Trump had blocked each of them from his Twitter account after they criticized him. And they said, hey, you can't do that. I've got freedom of speech. Blocking means a user who's logged in can't tweet at the president, read his tweets or join in and respond to other people commenting. Twitter created this feature after people complained about being harassed, say, by bullies or exes. Blocking lets you control who you hear from. Press some buttons and, bam, you create an echo chamber of just your adoring fans. Trump arguably did a version of that when he blocked his critics. And the problem with that, according to the court, is that the president uses Twitter in his official capacity as a public servant. It doesn't matter that he started his account in 2009 when he was a private citizen. That's not how he uses it anymore. @realDonaldTrump isn't where the president talks about his golf game. It's where he announces major decisions, like firing his chief of staff or banning transgender people in the military. As my esteemed colleague David Folkenflik puts it, the president's Twitter is his mood ring. SHAPIRO: What has the response to this ruling been from the president or the administration? SHAHANI: Well, the president has not tweeted about it yet. A Justice Department spokesperson said that the DOJ is disappointed with the court's decision and is exploring next steps. The DOJ reiterated its argument from court that the president's account is not a public forum. Blocking users is private conduct. Now, that argument didn't fly because the administration had said so much to the contrary. As the judges pointed out, Trump's former press secretary, Sean Spicer, called the tweets official statements. SHAPIRO: So beyond the president, what does this ruling mean for other politicians or public figures who are not elected politicians - I mean, movie stars, for example? SHAHANI: Well, it really focuses on public officials serving in a public capacity, OK, and that's key. Trump is the most high-profile blocker, but he's by no means the only official public servant doing so. I spoke with Jameel Jaffer. He is the director of the Knight First Amendment Institute at Columbia University. He represented a group that filed the lawsuit against Trump. He also sued a Democrat, a county official in Virginia, for the same kind of behavior. Jaffer said the purpose of suing Trump is to bring the free speech rights that exist in the physical world into digital life. JAMEEL JAFFER: The whole point of the case was to take this body of law that exists off line - and that applies to spaces like town hall and city council meetings - and ensure that that same body of law would be applied to these new digital spaces that are increasingly important to our democracy. SHAHANI: And so what he's saying there is that in the real world at a town hall, the president can cover his ears if he doesn't like what someone has to say, but he can't make them shut up. So Jaffer says that's what Trump was effectively doing by blocking his critics on Twitter and the kind of activity he's trying to reverse. SHAPIRO: OK. So the president or a county official in Virginia can't block people on Twitter. What about when platforms delete accounts? Just briefly, does this ruling apply there? SHAHANI: Yeah. So the judges were careful to spell out the limits of this ruling. They said it's about public officials working in their official capacity. The judges are not deciding whether the social media companies are bound by the First Amendment. They were clear about that constraint. SHAPIRO: That's NPR's Aarti Shahani. Thanks so much. SHAHANI: Thanks very much.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-09-739999739": {"title": "YouTube Creators Are Trying To Fight Radicalization Online : NPR", "url": "https://www.npr.org/2019/07/09/739999739/youtube-creators-are-trying-to-fight-radicalization-online", "author": "No author found", "published_date": "2019-07-09", "content": "AUDIE CORNISH, HOST: YouTube is constantly tinkering with its recommendation system. It's designed to keep viewers watching for as long as possible. Outside tech experts, however, have shown how YouTube's algorithms can lead viewers quickly down a rabbit hole of increasingly fringe videos, in some cases ending up with extremist, mostly right-wing content. A growing number of content creators are trying to get in front of that audience and stop radicalization. NPR's Andrew Limbong introduces us to two of them. ANDREW LIMBONG, BYLINE: Here's how one teenager got caught up watching far-right videos. His name's Alex, and he didn't want us to use his last name for fear of retaliation. When he was 16, he spent a lot of time on YouTube watching angry videos. ALEX: And I started to believe that feminism was a bunch of whining about nothing and that third-wave feminism was cancer. That's what I was basically told. LIMBONG: From there, YouTube recommended more increasingly extreme right-wing creators. ALEX: And then further down, I started watching people like Stefan Molyneux. . . LIMBONG: Who made a long movie about protecting Poland as a white ethno-state. ALEX: . . . And Lauren Southern. LIMBONG: A far-right advocate who made this video about a racist conspiracy called the great replacement. (SOUNDBITE OF ARCHIVED RECORDING)LAUREN SOUTHERN: And with the disparities in birth rates, the non-Muslim population will continue to decline as the Muslim population multiplies in France well above the replacement rate. LIMBONG: This isn't just rambling hate speech. It has real-world consequences. For instance, the great replacement inspired the Christchurch, New Zealand, shooting. ALEX: I would hear all these stats and be like, well, damn, she must know what she's talking about. LIMBONG: Alex says Lauren Southern didn't sound hateful to him; she sounded pragmatic. But listen to how her arguments on immigration fall apart when she gets into it with a guy named Steven Bonnell in an interview on YouTube. (SOUNDBITE OF ARCHIVED RECORDING)SOUTHERN: But you saying that there is no proof that immigrants are a net drain on the country, I think, is ridiculous. STEVEN BONNELL: I mean, most of the reading that I did to prepare for this was of the economist that you cited in your book who agrees that immigration has been a net gain for the Western world in every. . . LIMBONG: That was Steven Bonnell, one of the few creators on YouTube who are actively challenging this deluge of misinformation. It wasn't always part of his gig. Bonnell's career began playing video games online and cracking jokes. But then he noticed others in his field becoming more political, getting real popular making videos attacking feminism, racial justice, immigration. BONNELL: Basically what I saw was - especially around the 2016 time period, especially listening to Trump - was there's just so many bad arguments. And it drove me crazy to see how many people are just unwilling to engage with the reality of what they're talking about. LIMBONG: So Bonnell began holding hours-long debates with them. BONNELL: At the end of the day, I want my person that I'm talking to to feel like they're floundering or to feel like they don't know what they're talking about or that their arguments, you know, fell apart under more scrutiny. LIMBONG: Another YouTuber giving these arguments more scrutiny is Natalie Wynn. She runs the channel ContraPoints. And instead of debating these personalities directly, she'll break down their arguments in video essays like this. (SOUNDBITE OF ARCHIVED RECORDING)NATALIE WYNN: They'll also use euphemisms for core components of their beliefs. If talking about preserving a homeland for white people sounds too fascist, they'll talk about preserving Western civilization or Western culture instead. LIMBONG: Wynn says she started this channel because she'd been watching this corner of hate and misinformation grow for years. And she's learned a few things. You have to take trolls seriously. You have to use their language, and you have to confront their prejudices head-on. And as a trans woman, for Wynn, that meant making jokes at her own expense. WYNN: I know I look weird. I know that you have an idea of me as this, like, degenerate. But, like, I am capable of seeing things the way you're seeing things. I'm capable of making jokes from your perspective. Now just listen to me. LIMBONG: And it worked. Her videos questioning misogynist involuntary celibates and other pop philosophers got millions of views. Now she is expanding her scope, tackling topics more directed at her growing female and queer audiences because edgy boys aren't the only ones affected by toxic online communities. WYNN: I'm not worried about being out of work because humanity is suddenly going to become happy and functional. LIMBONG: It's an uphill battle. Overall, five hours of content is uploaded to YouTube every minute, and even the company itself struggles to find a fair and effective way to police extremism on its platform. But Wynn and Bonnell are inspiring others, like Alex, who's 19 now, to start their own channels - a small but growing cohort of creators hoping to make YouTube a little less hateful. Andrew Limbong, NPR News. AUDIE CORNISH, HOST:  YouTube is constantly tinkering with its recommendation system. It's designed to keep viewers watching for as long as possible. Outside tech experts, however, have shown how YouTube's algorithms can lead viewers quickly down a rabbit hole of increasingly fringe videos, in some cases ending up with extremist, mostly right-wing content. A growing number of content creators are trying to get in front of that audience and stop radicalization. NPR's Andrew Limbong introduces us to two of them. ANDREW LIMBONG, BYLINE: Here's how one teenager got caught up watching far-right videos. His name's Alex, and he didn't want us to use his last name for fear of retaliation. When he was 16, he spent a lot of time on YouTube watching angry videos. ALEX: And I started to believe that feminism was a bunch of whining about nothing and that third-wave feminism was cancer. That's what I was basically told. LIMBONG: From there, YouTube recommended more increasingly extreme right-wing creators. ALEX: And then further down, I started watching people like Stefan Molyneux. . . LIMBONG: Who made a long movie about protecting Poland as a white ethno-state. ALEX: . . . And Lauren Southern. LIMBONG: A far-right advocate who made this video about a racist conspiracy called the great replacement. (SOUNDBITE OF ARCHIVED RECORDING) LAUREN SOUTHERN: And with the disparities in birth rates, the non-Muslim population will continue to decline as the Muslim population multiplies in France well above the replacement rate. LIMBONG: This isn't just rambling hate speech. It has real-world consequences. For instance, the great replacement inspired the Christchurch, New Zealand, shooting. ALEX: I would hear all these stats and be like, well, damn, she must know what she's talking about. LIMBONG: Alex says Lauren Southern didn't sound hateful to him; she sounded pragmatic. But listen to how her arguments on immigration fall apart when she gets into it with a guy named Steven Bonnell in an interview on YouTube. (SOUNDBITE OF ARCHIVED RECORDING) SOUTHERN: But you saying that there is no proof that immigrants are a net drain on the country, I think, is ridiculous. STEVEN BONNELL: I mean, most of the reading that I did to prepare for this was of the economist that you cited in your book who agrees that immigration has been a net gain for the Western world in every. . . LIMBONG: That was Steven Bonnell, one of the few creators on YouTube who are actively challenging this deluge of misinformation. It wasn't always part of his gig. Bonnell's career began playing video games online and cracking jokes. But then he noticed others in his field becoming more political, getting real popular making videos attacking feminism, racial justice, immigration. BONNELL: Basically what I saw was - especially around the 2016 time period, especially listening to Trump - was there's just so many bad arguments. And it drove me crazy to see how many people are just unwilling to engage with the reality of what they're talking about. LIMBONG: So Bonnell began holding hours-long debates with them. BONNELL: At the end of the day, I want my person that I'm talking to to feel like they're floundering or to feel like they don't know what they're talking about or that their arguments, you know, fell apart under more scrutiny. LIMBONG: Another YouTuber giving these arguments more scrutiny is Natalie Wynn. She runs the channel ContraPoints. And instead of debating these personalities directly, she'll break down their arguments in video essays like this. (SOUNDBITE OF ARCHIVED RECORDING) NATALIE WYNN: They'll also use euphemisms for core components of their beliefs. If talking about preserving a homeland for white people sounds too fascist, they'll talk about preserving Western civilization or Western culture instead. LIMBONG: Wynn says she started this channel because she'd been watching this corner of hate and misinformation grow for years. And she's learned a few things. You have to take trolls seriously. You have to use their language, and you have to confront their prejudices head-on. And as a trans woman, for Wynn, that meant making jokes at her own expense. WYNN: I know I look weird. I know that you have an idea of me as this, like, degenerate. But, like, I am capable of seeing things the way you're seeing things. I'm capable of making jokes from your perspective. Now just listen to me. LIMBONG: And it worked. Her videos questioning misogynist involuntary celibates and other pop philosophers got millions of views. Now she is expanding her scope, tackling topics more directed at her growing female and queer audiences because edgy boys aren't the only ones affected by toxic online communities. WYNN: I'm not worried about being out of work because humanity is suddenly going to become happy and functional. LIMBONG: It's an uphill battle. Overall, five hours of content is uploaded to YouTube every minute, and even the company itself struggles to find a fair and effective way to police extremism on its platform. But Wynn and Bonnell are inspiring others, like Alex, who's 19 now, to start their own channels - a small but growing cohort of creators hoping to make YouTube a little less hateful. Andrew Limbong, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-09-739999730": {"title": "Ransomware Attacks Create Dilemma For Cities : NPR", "url": "https://www.npr.org/2019/07/09/739999730/what-happens-when-hackers-hold-cities-hostage-with-ransomware-attacks", "author": "No author found", "published_date": "2019-07-09", "content": "ARI SHAPIRO, HOST: It's been a bad few months for government IT departments across the U. S. Hackers have used ransomware to attack the data networks of Baltimore, the Georgia court system and Lake City, Fla. , to name just a few places. As NPR's Wade Goodwyn reports, these governments are struggling to decide whether to pay up. WADE GOODWYN, BYLINE: It started out as a nice, normal Monday morning in June at Lake City, Fla. , City Hall. But then someone in IT noticed something was wrong with the network - something so, so wrong. MIKE LEE: They immediately brought everything offline. They turned off the servers. They literally went, like, room to room through city hall, like, unplugging people's network cables and turning off all computers. GOODWYN: Mike Lee is a sergeant with the Lake City Police Department. Lee says after everything was disconnected, there was a tiny bit of hope that maybe they caught it before everything was encrypted. Lee says in Lake City's case, that hope was forlorn. LEE: And in hindsight, yeah, the riot (ph) ransomware attack quietly makes its way through the entire system. And then it encrypts everything at once and sends you a ransom. So we kind of cut it off partway through, but, you know, a lot of the damage had already been done. GOODWYN: Business at city hall didn't so much grind to a halt. It was more like a finger snap. And how much did the crooks want for the decryption key that would restore Lake City's information systems? LEE: Their payment request was for 42 bitcoins. At the time of the purchase, it was roughly $460,000. GOODWYN: Lake City officials notified state and federal law enforcement and then called their insurance company - the Florida League of Cities. ERIC HARTWELL: We put them in touch with a cybersecurity firm that would, essentially, pick up the reins and walk them through the process. GOODWYN: Eric Hartwell is the insurance counsel at the 500-plus member Florida League of Cities. HARTWELL: Every city is kind of like a business. They've got to evaluate, what data is missing? What kind of backup information do we have? Is it reliable? - whether or not to cooperate with what the demand has been or whether or not to stand pat. GOODWYN: Not paying often means replacing equipment and, basically, starting over. That's a lot more costly than paying the ransom. The city of Baltimore decided not to pay the 13 bitcoin ransom demand - roughly $75,000 - when their systems were hacked with RobbinHood ransomware. The cost of Mayor Jack Young's principled stand has topped $18 million. Back in Florida, Lake City Police Sgt. Mike Lee said they were advised to pay the hackers. LEE: Yes, we have received the decryption key. And we are slowly making our way through our system a little at a time. And at this point, that key has proven successful where we've used it. GOODWYN: The Lake City taxpayers had to pick up the $10,000 deductible, but the rest was paid by insurance. Ransomware crime is many times more lucrative than, say, bank robbery, with the advantage of no weapons, disguises, getaway cars, police chases - in fact, practically no risk of getting caught at all. AMANDA VIDELL: We see these types of attacks happen every day all across the country. GOODWYN: Amanda Videll is with the FBI, which is investigating Lake City's attack. Videll says even though ransomware hacks are much more common than is generally understood, the official numbers are nevertheless an underrepresentation. That's because businesses sometimes decide not to report they were targeted because getting hacked carries a stigma, which can be bad for business. VIDELL: We are trying to encourage any victim of ransomware, whether it be a business or an individual or a city agency or a government agency, to report that to the FBI directly before they decide to take any action; basically, whether or not to pay. GOODWYN: From the FBI's point of view, paying ransom encourages more hacking. And when a private business doesn't report a ransomware attack, it's an added boon for the extortionists. The FBI says it's not unsympathetic toward the victim's plight and dilemma, but paying data-hostage-takers has to stop or the attacks never will. BRYAN GARDNER: OK. So this is the data - our data center. We actually have some cloud presence too. So, like, all of our major systems reside here. GOODWYN: Dr. Bryan Gardner is a chief information security officer for the city of Dallas. Gardner has watched with concern as cities, hospitals, court systems and other vital public institutions' information systems have been hacked and encrypted. Dallas follows the best security practices outlined by the National Institute of Standards and Technology, known as NIST. But Gardner says municipal information security officers know their system could be next. GARDNER: Right now it's 197 days before a breach is detected, normally. That's the average. So you're talking 200 days that they've been in, looking around. They know your system probably better than you do. GOODWYN: Last week, the administrative office of the Georgia courts became the latest victim to have its data encrypted by ransomware. That follows on the heels of last year's attack, when the city of Atlanta's computer network was hacked and $51,000 in ransom demanded. To the FBI's satisfaction, Atlanta refused to pay. But the resulting damage has been estimated to cost around $17 million. Wade Goodwyn, NPR News, Dallas. (SOUNDBITE OF EL PERRO DEL MAR'S \"DARK NIGHT\") ARI SHAPIRO, HOST:  It's been a bad few months for government IT departments across the U. S. Hackers have used ransomware to attack the data networks of Baltimore, the Georgia court system and Lake City, Fla. , to name just a few places. As NPR's Wade Goodwyn reports, these governments are struggling to decide whether to pay up. WADE GOODWYN, BYLINE: It started out as a nice, normal Monday morning in June at Lake City, Fla. , City Hall. But then someone in IT noticed something was wrong with the network - something so, so wrong. MIKE LEE: They immediately brought everything offline. They turned off the servers. They literally went, like, room to room through city hall, like, unplugging people's network cables and turning off all computers. GOODWYN: Mike Lee is a sergeant with the Lake City Police Department. Lee says after everything was disconnected, there was a tiny bit of hope that maybe they caught it before everything was encrypted. Lee says in Lake City's case, that hope was forlorn. LEE: And in hindsight, yeah, the riot (ph) ransomware attack quietly makes its way through the entire system. And then it encrypts everything at once and sends you a ransom. So we kind of cut it off partway through, but, you know, a lot of the damage had already been done. GOODWYN: Business at city hall didn't so much grind to a halt. It was more like a finger snap. And how much did the crooks want for the decryption key that would restore Lake City's information systems? LEE: Their payment request was for 42 bitcoins. At the time of the purchase, it was roughly $460,000. GOODWYN: Lake City officials notified state and federal law enforcement and then called their insurance company - the Florida League of Cities. ERIC HARTWELL: We put them in touch with a cybersecurity firm that would, essentially, pick up the reins and walk them through the process. GOODWYN: Eric Hartwell is the insurance counsel at the 500-plus member Florida League of Cities. HARTWELL: Every city is kind of like a business. They've got to evaluate, what data is missing? What kind of backup information do we have? Is it reliable? - whether or not to cooperate with what the demand has been or whether or not to stand pat. GOODWYN: Not paying often means replacing equipment and, basically, starting over. That's a lot more costly than paying the ransom. The city of Baltimore decided not to pay the 13 bitcoin ransom demand - roughly $75,000 - when their systems were hacked with RobbinHood ransomware. The cost of Mayor Jack Young's principled stand has topped $18 million. Back in Florida, Lake City Police Sgt. Mike Lee said they were advised to pay the hackers. LEE: Yes, we have received the decryption key. And we are slowly making our way through our system a little at a time. And at this point, that key has proven successful where we've used it. GOODWYN: The Lake City taxpayers had to pick up the $10,000 deductible, but the rest was paid by insurance. Ransomware crime is many times more lucrative than, say, bank robbery, with the advantage of no weapons, disguises, getaway cars, police chases - in fact, practically no risk of getting caught at all. AMANDA VIDELL: We see these types of attacks happen every day all across the country. GOODWYN: Amanda Videll is with the FBI, which is investigating Lake City's attack. Videll says even though ransomware hacks are much more common than is generally understood, the official numbers are nevertheless an underrepresentation. That's because businesses sometimes decide not to report they were targeted because getting hacked carries a stigma, which can be bad for business. VIDELL: We are trying to encourage any victim of ransomware, whether it be a business or an individual or a city agency or a government agency, to report that to the FBI directly before they decide to take any action; basically, whether or not to pay. GOODWYN: From the FBI's point of view, paying ransom encourages more hacking. And when a private business doesn't report a ransomware attack, it's an added boon for the extortionists. The FBI says it's not unsympathetic toward the victim's plight and dilemma, but paying data-hostage-takers has to stop or the attacks never will. BRYAN GARDNER: OK. So this is the data - our data center. We actually have some cloud presence too. So, like, all of our major systems reside here. GOODWYN: Dr. Bryan Gardner is a chief information security officer for the city of Dallas. Gardner has watched with concern as cities, hospitals, court systems and other vital public institutions' information systems have been hacked and encrypted. Dallas follows the best security practices outlined by the National Institute of Standards and Technology, known as NIST. But Gardner says municipal information security officers know their system could be next. GARDNER: Right now it's 197 days before a breach is detected, normally. That's the average. So you're talking 200 days that they've been in, looking around. They know your system probably better than you do. GOODWYN: Last week, the administrative office of the Georgia courts became the latest victim to have its data encrypted by ransomware. That follows on the heels of last year's attack, when the city of Atlanta's computer network was hacked and $51,000 in ransom demanded. To the FBI's satisfaction, Atlanta refused to pay. But the resulting damage has been estimated to cost around $17 million. Wade Goodwyn, NPR News, Dallas. (SOUNDBITE OF EL PERRO DEL MAR'S \"DARK NIGHT\")", "section": "National", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-09-739906562": {"title": "Appeals Court Upholds Ruling Saying Trump Can't Block Critics On Twitter : NPR", "url": "https://www.npr.org/2019/07/09/739906562/u-s-appeals-court-rules-trump-violated-first-amendment-by-blocking-twitter-follo", "author": "No author found", "published_date": "2019-07-09", "content": "", "section": "Law", "disclaimer": ""}, "2019-07-09-739784008": {"title": "Wimbledon, Steeped In Tradition, Embraces Artificial Intelligence : NPR", "url": "https://www.npr.org/2019/07/09/739784008/wimbledon-steeped-in-tradition-embraces-artificial-intelligence", "author": "No author found", "published_date": "2019-07-09", "content": "STEVE INSKEEP, HOST: Wimbledon is in its second week, a tennis tournament steeped in tradition and also embracing artificial intelligence. We should note that a company that makes AI, IBM, is a financial supporter of NPR on our way to letting you know that Wimbledon is using AI to produce highlights of the most exciting moments much more quickly than a human producer. But how does a computer program know what makes for good tennis? At Wimbledon, Brenda Salinas explains. BRENDA SALINAS, BYLINE: Fans from all over the world are gathered around 18 grass tennis courts cheering on their favorite players. (CHEERING)SALINAS: In this match, Russian Daniil Medvedev faces off against the Italian Paolo Lorenzi. (CHEERING)UNIDENTIFIED ANNOUNCER: Twenty, 15. SALINAS: But I'm not watching the tennis drinking a traditional Pimm's cocktail. I'm in a basement where engineers use that very sound you're hearing to power an artificially intelligent program. ALEXANDRA WILLIS: We are in the room affectionately known as The Bunker. SALINAS: Alexandra Willis heads up digital marketing at Wimbledon. She shows me the IBM dashboard that can automatically determine what parts of a tennis match are the most exciting for fans to watch. Every match is automatically clipped and ranked according to three categories. WILLIS: The first is the noise of the crowd. So how they react to that particular point. SALINAS: The computer program knows what point in the match it is. WILLIS: So was it break point? Was it an ace? What kind of point was it? SALINAS: And lastly, the tricky bit, what emotions the human tennis player is feeling. WILLIS: So are they fist pumping? Are they actually looking in complete despair? SALINAS: That's right, the computer can tell whether a tennis player is celebrating or wincing in despair. WILLIS: For a while, player gestures, it was picking up this movement - wiping your face - and thinking, is that some kind of celebration? Actually, it was the player saying, I want my towel. So that's the whole beauty of this, is that we have to test it and learn it constantly. SALINAS: Wimbledon has been using this technology for three years, but this year, it says it's smarter than ever. The highlight reels get distributed all over Wimbledon's digital properties, including on the Jumbotrons and its YouTube channel. IBM engineer Dave Provan shows me how it works. DAVE PROVAN: Good crowd reaction on the volley. Looks like a set point. So the highlight will automatically do the set points, match points and other points like that. SALINAS: The polished highlights reel comes together just two minutes after a match has ended. That's about nine times faster than a human video editor. Plus the program can analyze matches across 18 courts. No human can do that. So did this just come to you like this? PROVAN: Yeah. It comes fully edited like this together. SALINAS: No human input at all? PROVAN: There's human review to make sure that it looks good, but yeah, it's basically an automatic system. SALINAS: Tennis always follows the same structure, but the story of every match is different. That's why Courtney Nguyen is skeptical that robots can capture the most important parts of the game. COURTNEY NGUYEN: Yeah. That's nuts. (Laughter). SALINAS: Nguyen hosts a podcast for the Women's Tennis Association where she analyzes the texture of the game. NGUYEN: I think that when you're actually cutting a highlight package that tells the true story of the match, there could be, you know, in oftentimes, in those situations, something very different happening that maybe even a crowd completely misses, or even a player doesn't even notice is happening, could turn a match. SALINAS: Computer programs are sophisticated enough to capture the emotions of a crowd or on a player's face, but they're not smart enough to capture the tiny moments that can make a match. At least, not yet. (APPLAUSE)SALINAS: At Wimbledon, I'm Brenda Salinas. STEVE INSKEEP, HOST:  Wimbledon is in its second week, a tennis tournament steeped in tradition and also embracing artificial intelligence. We should note that a company that makes AI, IBM, is a financial supporter of NPR on our way to letting you know that Wimbledon is using AI to produce highlights of the most exciting moments much more quickly than a human producer. But how does a computer program know what makes for good tennis? At Wimbledon, Brenda Salinas explains. BRENDA SALINAS, BYLINE: Fans from all over the world are gathered around 18 grass tennis courts cheering on their favorite players. (CHEERING) SALINAS: In this match, Russian Daniil Medvedev faces off against the Italian Paolo Lorenzi. (CHEERING) UNIDENTIFIED ANNOUNCER: Twenty, 15. SALINAS: But I'm not watching the tennis drinking a traditional Pimm's cocktail. I'm in a basement where engineers use that very sound you're hearing to power an artificially intelligent program. ALEXANDRA WILLIS: We are in the room affectionately known as The Bunker. SALINAS: Alexandra Willis heads up digital marketing at Wimbledon. She shows me the IBM dashboard that can automatically determine what parts of a tennis match are the most exciting for fans to watch. Every match is automatically clipped and ranked according to three categories. WILLIS: The first is the noise of the crowd. So how they react to that particular point. SALINAS: The computer program knows what point in the match it is. WILLIS: So was it break point? Was it an ace? What kind of point was it? SALINAS: And lastly, the tricky bit, what emotions the human tennis player is feeling. WILLIS: So are they fist pumping? Are they actually looking in complete despair? SALINAS: That's right, the computer can tell whether a tennis player is celebrating or wincing in despair. WILLIS: For a while, player gestures, it was picking up this movement - wiping your face - and thinking, is that some kind of celebration? Actually, it was the player saying, I want my towel. So that's the whole beauty of this, is that we have to test it and learn it constantly. SALINAS: Wimbledon has been using this technology for three years, but this year, it says it's smarter than ever. The highlight reels get distributed all over Wimbledon's digital properties, including on the Jumbotrons and its YouTube channel. IBM engineer Dave Provan shows me how it works. DAVE PROVAN: Good crowd reaction on the volley. Looks like a set point. So the highlight will automatically do the set points, match points and other points like that. SALINAS: The polished highlights reel comes together just two minutes after a match has ended. That's about nine times faster than a human video editor. Plus the program can analyze matches across 18 courts. No human can do that. So did this just come to you like this? PROVAN: Yeah. It comes fully edited like this together. SALINAS: No human input at all? PROVAN: There's human review to make sure that it looks good, but yeah, it's basically an automatic system. SALINAS: Tennis always follows the same structure, but the story of every match is different. That's why Courtney Nguyen is skeptical that robots can capture the most important parts of the game. COURTNEY NGUYEN: Yeah. That's nuts. (Laughter). SALINAS: Nguyen hosts a podcast for the Women's Tennis Association where she analyzes the texture of the game. NGUYEN: I think that when you're actually cutting a highlight package that tells the true story of the match, there could be, you know, in oftentimes, in those situations, something very different happening that maybe even a crowd completely misses, or even a player doesn't even notice is happening, could turn a match. SALINAS: Computer programs are sophisticated enough to capture the emotions of a crowd or on a player's face, but they're not smart enough to capture the tiny moments that can make a match. At least, not yet. (APPLAUSE) SALINAS: At Wimbledon, I'm Brenda Salinas.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-10-739873066": {"title": "Instagram Rolls Out Feature To Minimize Hateful Comments : NPR", "url": "https://www.npr.org/2019/07/10/739873066/instagram-to-flag-hateful-comments-before-you-send-them", "author": "No author found", "published_date": "2019-07-10", "content": "", "section": "Technology", "disclaimer": ""}, "2019-07-11-740871352": {"title": "Big Tech Companies Are Struggling With How To Best Police Their Platforms : NPR", "url": "https://www.npr.org/2019/07/11/740871352/big-tech-companies-are-struggling-with-how-to-best-police-their-platforms", "author": "No author found", "published_date": "2019-07-11", "content": "AUDIE CORNISH, HOST: We're going to take a closer look now at how those companies left out of the White House summit - namely Facebook, Google and Twitter - treat far-right content. Joan Donovan is a leading researcher in this area. She is with the Harvard Kennedy School's Shorenstein Center. Welcome to the program. JOAN DONOVAN: Thank you for having me. CORNISH: So what are some of the criteria these tech platforms use in deciding the content that will stay up, the content that they'll take down or even banning a user? DONOVAN: So one of the ways in which they assess content has a lot to do with explicit calls for violence or explicit condemnation of known marginalized groups. So they will do this either by watching the content and making a summation, or people who viewed the content may flag that content, which then puts it into their content moderation system, and then it's up to a content moderator to decide if the content stays up or gets pulled down. In terms of banning users, that usually takes a whole set of different processes, where a number of pieces of content or video have been flagged at different times. And usually, users are given usually three different strikes before they are banned as a user, and each strike comes with it - they pull back some of their different content features. They'll, you know, stop them from being able to monetize or to use advertising, and they'll stop them from being able to livestream. And then, in the final instance, they will remove the account entirely. CORNISH: Is it too early to give a report card, say, for Facebook, Twitter or Google in terms of how these strategies are working? DONOVAN: I've been researching this for several years now, and it's only been in the last year that there's been acknowledgement from these corporations that there are significant problems with especially white supremacist content on their platforms. It was a watershed moment in Internet history, as well as in American history, that what happened in the Unite the Right rally, where it was very easy to see how much of each platform had been leveraged in order to bring people out for that event. And as a result, platforms knew very concretely the role that their technology had played in organizing that hateful event and what eventually led to the death of two police officers and Heather Heyer. CORNISH: Conservatives have argued that the strategies that these tech companies are using means that they are disproportionately targeted. President Trump has complained directly to the head of Twitter about this. Is there any truth to that? DONOVAN: So when we talk about conservatives, we're not always talking explicitly about white supremacists, but we don't really actually know the extent to which these companies are targeting any specific subset of people. We know they're taking down millions of pieces of content over the course of a year, but we don't know specifically if there's any particular political bias in it. What we do know is we have statistics that show that conservative media does very well on social media. Conservative media outlets are some of the top shares. And so we have a particular subset of people that are complaining about bias that we do not have evidence exists. CORNISH: In the meantime, these social media platforms, these companies, what is their challenge going forward? DONOVAN: These companies do need to come together on a set of rules and potentially enshrined in a kind of regulatory body that ensures that we get consistent content moderation across these platforms. What we know is, when a platform does tend to moderate a group, especially when Twitter started to moderate white supremacists on their platform, these white supremacists moved to other platforms. Ultimately, these corporations have to think about, well, what is the content moderation strategy? But also how do they enforce it across all of the platforms consistently so that you don't get this blowback effect? CORNISH: That's Joan Donovan, director of the Technology and Social Change Research Project at Harvard Kennedy Shorenstein Center. Thank you for explaining it. DONOVAN: Thank you. (SOUNDBITE OF PINBACK'S \"MICROTONIC WAVE\") AUDIE CORNISH, HOST:  We're going to take a closer look now at how those companies left out of the White House summit - namely Facebook, Google and Twitter - treat far-right content. Joan Donovan is a leading researcher in this area. She is with the Harvard Kennedy School's Shorenstein Center. Welcome to the program. JOAN DONOVAN: Thank you for having me. CORNISH: So what are some of the criteria these tech platforms use in deciding the content that will stay up, the content that they'll take down or even banning a user? DONOVAN: So one of the ways in which they assess content has a lot to do with explicit calls for violence or explicit condemnation of known marginalized groups. So they will do this either by watching the content and making a summation, or people who viewed the content may flag that content, which then puts it into their content moderation system, and then it's up to a content moderator to decide if the content stays up or gets pulled down. In terms of banning users, that usually takes a whole set of different processes, where a number of pieces of content or video have been flagged at different times. And usually, users are given usually three different strikes before they are banned as a user, and each strike comes with it - they pull back some of their different content features. They'll, you know, stop them from being able to monetize or to use advertising, and they'll stop them from being able to livestream. And then, in the final instance, they will remove the account entirely. CORNISH: Is it too early to give a report card, say, for Facebook, Twitter or Google in terms of how these strategies are working? DONOVAN: I've been researching this for several years now, and it's only been in the last year that there's been acknowledgement from these corporations that there are significant problems with especially white supremacist content on their platforms. It was a watershed moment in Internet history, as well as in American history, that what happened in the Unite the Right rally, where it was very easy to see how much of each platform had been leveraged in order to bring people out for that event. And as a result, platforms knew very concretely the role that their technology had played in organizing that hateful event and what eventually led to the death of two police officers and Heather Heyer. CORNISH: Conservatives have argued that the strategies that these tech companies are using means that they are disproportionately targeted. President Trump has complained directly to the head of Twitter about this. Is there any truth to that? DONOVAN: So when we talk about conservatives, we're not always talking explicitly about white supremacists, but we don't really actually know the extent to which these companies are targeting any specific subset of people. We know they're taking down millions of pieces of content over the course of a year, but we don't know specifically if there's any particular political bias in it. What we do know is we have statistics that show that conservative media does very well on social media. Conservative media outlets are some of the top shares. And so we have a particular subset of people that are complaining about bias that we do not have evidence exists. CORNISH: In the meantime, these social media platforms, these companies, what is their challenge going forward? DONOVAN: These companies do need to come together on a set of rules and potentially enshrined in a kind of regulatory body that ensures that we get consistent content moderation across these platforms. What we know is, when a platform does tend to moderate a group, especially when Twitter started to moderate white supremacists on their platform, these white supremacists moved to other platforms. Ultimately, these corporations have to think about, well, what is the content moderation strategy? But also how do they enforce it across all of the platforms consistently so that you don't get this blowback effect? CORNISH: That's Joan Donovan, director of the Technology and Social Change Research Project at Harvard Kennedy Shorenstein Center. Thank you for explaining it. DONOVAN: Thank you. (SOUNDBITE OF PINBACK'S \"MICROTONIC WAVE\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-11-740219271": {"title": "Study Projects Automation's Effects On Workers In Different Areas, Occupations : NPR", "url": "https://www.npr.org/2019/07/11/740219271/will-your-job-still-exist-in-2030", "author": "No author found", "published_date": "2019-07-11", "content": "", "section": "Business", "disclaimer": ""}, "2019-07-11-740688073": {"title": "France Approves Tax On Big Tech, And The U.S. Threatens To Retaliate : NPR", "url": "https://www.npr.org/2019/07/11/740688073/france-approves-tax-on-big-tech-and-u-s-threatens-to-retaliate", "author": "No author found", "published_date": "2019-07-11", "content": "", "section": "Europe", "disclaimer": ""}, "2019-07-11-740661470": {"title": "Poker Bot Beats The Professionals At 6-Player Texas Hold 'Em : NPR", "url": "https://www.npr.org/2019/07/11/740661470/bet-on-the-bot-ai-beats-the-professionals-at-6-player-texas-hold-em", "author": "No author found", "published_date": "2019-07-11", "content": "AUDIE CORNISH, HOST: In artificial intelligence, it's a milestone when a computer program can beat top players at a game like chess. But a game like poker, specifically six-player Texas hold 'em has been too tough for a machine to master - until now. NPR's Merrit Kennedy has the story. MERRIT KENNEDY, BYLINE: Darren Elias holds four World Poker Tour titles and has won millions of dollars playing the game. So he was a perfect person to test the skills of a poker bot called Pluribus. Actually, he played Texas hold 'em against a whole table of these bots. DARREN ELIAS: It's just me and then five versions of this AI poker bot, which I would play against every day thousands of hands. KENNEDY: Pluribus learns by playing against itself over and over and remembering which strategies worked best. And Elias would alert the computer scientists who designed the bot when it made a mistake. ELIAS: And it was improving very rapidly, where it went from being a mediocre player to basically a world class-level poker player in a matter of days and weeks, which was pretty scary. KENNEDY: After 5,000 hands, the machine came out ahead of Elias. So scientists tried a different experiment - pitting Pluribus against five professional players at a time. It still won. NOAM BROWN: If you - if this were for live money, the bot would be winning at a rate of about a thousand dollars an hour. KENNEDY: That's Noam Brown from Facebook's AI Research Unit. He designed Pluribus with his adviser at Carnegie Mellon University, and their research was published in the journal Science. There are a couple of reasons why multiplayer poker has been a challenge for AI. The information isn't out in the open, like it is in chess, for example. The cards are hidden. And multiple opponents make it a lot tougher for a bot to figure out a winning strategy. As Pluribus taught itself to play, Brown says some of the tactics it came up with were surprising. BROWN: Because it was developed completely from scratch, without any access to human data, the strategy that it's developed is very different from how humans play poker. KENNEDY: The bot learned to pick its moments and then make huge bets and bluffs - bigger than most humans would make. Here's Elias, the poker pro. ELIAS: The bot was not afraid to make these kind of plays often, which is something that humans could probably do a little more. KENNEDY: And he says the bot was excellent at varying its strategy, even when dealt the exact same hand, so it was very unpredictable. Ultimately, he says Pluribus could spell the end of high-stakes online poker. People might not want to risk a lot of money if they think they might actually be playing against a superhuman bot. ELIAS: It's humbling and a bit sad, I guess, that - to be defeated by a bot like this so quickly in a game that you dedicated, like, your life to. KENNEDY: Brown, the developer, says their AI technology could eventually be useful in many situations where there are multiple people involved and a lot of unknown variables, like getting a self-driving car through traffic. Merrit Kennedy, NPR News. (SOUNDBITE OF LADY GAGA SONG, \"POKER FACE\") AUDIE CORNISH, HOST:  In artificial intelligence, it's a milestone when a computer program can beat top players at a game like chess. But a game like poker, specifically six-player Texas hold 'em has been too tough for a machine to master - until now. NPR's Merrit Kennedy has the story. MERRIT KENNEDY, BYLINE: Darren Elias holds four World Poker Tour titles and has won millions of dollars playing the game. So he was a perfect person to test the skills of a poker bot called Pluribus. Actually, he played Texas hold 'em against a whole table of these bots. DARREN ELIAS: It's just me and then five versions of this AI poker bot, which I would play against every day thousands of hands. KENNEDY: Pluribus learns by playing against itself over and over and remembering which strategies worked best. And Elias would alert the computer scientists who designed the bot when it made a mistake. ELIAS: And it was improving very rapidly, where it went from being a mediocre player to basically a world class-level poker player in a matter of days and weeks, which was pretty scary. KENNEDY: After 5,000 hands, the machine came out ahead of Elias. So scientists tried a different experiment - pitting Pluribus against five professional players at a time. It still won. NOAM BROWN: If you - if this were for live money, the bot would be winning at a rate of about a thousand dollars an hour. KENNEDY: That's Noam Brown from Facebook's AI Research Unit. He designed Pluribus with his adviser at Carnegie Mellon University, and their research was published in the journal Science. There are a couple of reasons why multiplayer poker has been a challenge for AI. The information isn't out in the open, like it is in chess, for example. The cards are hidden. And multiple opponents make it a lot tougher for a bot to figure out a winning strategy. As Pluribus taught itself to play, Brown says some of the tactics it came up with were surprising. BROWN: Because it was developed completely from scratch, without any access to human data, the strategy that it's developed is very different from how humans play poker. KENNEDY: The bot learned to pick its moments and then make huge bets and bluffs - bigger than most humans would make. Here's Elias, the poker pro. ELIAS: The bot was not afraid to make these kind of plays often, which is something that humans could probably do a little more. KENNEDY: And he says the bot was excellent at varying its strategy, even when dealt the exact same hand, so it was very unpredictable. Ultimately, he says Pluribus could spell the end of high-stakes online poker. People might not want to risk a lot of money if they think they might actually be playing against a superhuman bot. ELIAS: It's humbling and a bit sad, I guess, that - to be defeated by a bot like this so quickly in a game that you dedicated, like, your life to. KENNEDY: Brown, the developer, says their AI technology could eventually be useful in many situations where there are multiple people involved and a lot of unknown variables, like getting a self-driving car through traffic. Merrit Kennedy, NPR News. (SOUNDBITE OF LADY GAGA SONG, \"POKER FACE\")", "section": "Science", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-11-740660070": {"title": "Amazon To Spend $700 Million Training 100,000 Employees For Tech Jobs : NPR", "url": "https://www.npr.org/2019/07/11/740660070/from-the-warehouse-to-it-amazon-offering-100-000-workers-tech-training", "author": "No author found", "published_date": "2019-07-11", "content": "", "section": "Business", "disclaimer": ""}, "2019-07-11-740579940": {"title": "White House Social Media Summit Excludes Platforms Twitter And Facebook : NPR", "url": "https://www.npr.org/2019/07/11/740579940/white-house-social-media-summit-leaves-out-key-players", "author": "No author found", "published_date": "2019-07-11", "content": "RACHEL MARTIN, HOST: The White House is hosting a social media summit today. President Trump is known for his passionate use of social media, specifically Twitter, which he often deploys to rally his base. The guest list at the event will feature some members of Congress and several of his social media fans. It's a who's who of conservative personalities, some associated with the \"alt-right. \" Big tech giants like Twitter and Facebook have not been invited. NPR's Jasmine Garsd has the story. JASMINE GARSD, BYLINE: When he got the email invitation to the White House, Twitter personality Carpe Donktum thought it might be a spam. CARPE DONKTUM: When I first got it, I was like, well, this seems kind of official, but I'm not sure that it is. GARSD: Carpe Donktum requested that we withhold his real name. He says he fears retaliation against his family. So how does a stay-at-home dad in Kansas whose real name is not public get invited to the White House? Because on Twitter, he's a celebrity with over 100,000 followers. Carpe Donktum, by the way, roughly comes from Latin. It's a parody meaning seize the donkey, referring to Democrats. He's a prolific creator of pro-Trump images and videos, which he tweets out, like this one, footage of the State of the Union featuring clearly unhappy Democrats in the audience with the band R. E. M. 's \"Everybody Hurts\" playing over it. (SOUNDBITE OF ARCHIVED RECORDING)REM: (Singing) Don't let yourself go. . . PRESIDENT DONALD TRUMP: That America will never be a socialist country. REM: (Singing) . . . 'Cause everybody cries. GARSD: It was retweeted by the president, who has since shared several other of Carpe Donktum's spoofs. Later today, Carpe Donktum will be attending a social media summit organized by the White House. Twitter is President Trump's preferred way of getting his message out. And while the White House has been mum about the guest list, some people have tweeted out their invitations. Many of them are part of the Trump Twitterverse, conservative social media personalities who orbit around Trump's bright Twitter star. But Trump has attacked those same social media platforms. (SOUNDBITE OF ARCHIVED RECORDING)TRUMP: You look at Google, Facebook, Twitter and other social media giants. And I've made it clear that we as a country cannot tolerate political censorship, blacklisting and rigged search results. GARSD: Both Google and Facebook have denied that they censor the right. A lot of people believe that's just not true. Conservative talk show host Bill Mitchell, who's also going to the summit, blames Twitter's algorithm for some of his followers not being able to see his tweets. He hopes at the summit they talk about how social media has become the modern public square. BILL MITCHELL: And if you become the public square, you really need to offer First Amendment protections to people where everybody can have free and open speech. Let's have the open debate, and may the best man win. GARSD: Angelo Carusone, from the liberal nonprofit Media Matters, believes in free speech. But he says some of the guests at the summit alarm him. ANGELO CARUSONE: You know, there's a couple establishment players there. But for the most part, they're far-right figures, they're extremists and a lot of people with ties to white nationalism. GARSD: Mitchell and Carpe Donktum disavowed racism to NPR, but Media Matters also points to other guests, like Charlie Kirk from Turning Point USA, an organization that has been accused of racist viewpoints. Here's Carusone again. CARUSONE: When you start to bring in all these individuals that are connected to or have relationships to these communities, what they do is they go back and they validate - oh, yeah, I met with the president. I was at the White House. You know, we're a part of this. The normalization of this is the part that concerns me. GARSD: And as the president rallies his base ahead of the 2020 elections, Carusone wonders about the effect of elevating these voices on the right. Jasmine Garsd, NPR News, New York. RACHEL MARTIN, HOST:  The White House is hosting a social media summit today. President Trump is known for his passionate use of social media, specifically Twitter, which he often deploys to rally his base. The guest list at the event will feature some members of Congress and several of his social media fans. It's a who's who of conservative personalities, some associated with the \"alt-right. \" Big tech giants like Twitter and Facebook have not been invited. NPR's Jasmine Garsd has the story. JASMINE GARSD, BYLINE: When he got the email invitation to the White House, Twitter personality Carpe Donktum thought it might be a spam. CARPE DONKTUM: When I first got it, I was like, well, this seems kind of official, but I'm not sure that it is. GARSD: Carpe Donktum requested that we withhold his real name. He says he fears retaliation against his family. So how does a stay-at-home dad in Kansas whose real name is not public get invited to the White House? Because on Twitter, he's a celebrity with over 100,000 followers. Carpe Donktum, by the way, roughly comes from Latin. It's a parody meaning seize the donkey, referring to Democrats. He's a prolific creator of pro-Trump images and videos, which he tweets out, like this one, footage of the State of the Union featuring clearly unhappy Democrats in the audience with the band R. E. M. 's \"Everybody Hurts\" playing over it. (SOUNDBITE OF ARCHIVED RECORDING) REM: (Singing) Don't let yourself go. . . PRESIDENT DONALD TRUMP: That America will never be a socialist country. REM: (Singing) . . . 'Cause everybody cries. GARSD: It was retweeted by the president, who has since shared several other of Carpe Donktum's spoofs. Later today, Carpe Donktum will be attending a social media summit organized by the White House. Twitter is President Trump's preferred way of getting his message out. And while the White House has been mum about the guest list, some people have tweeted out their invitations. Many of them are part of the Trump Twitterverse, conservative social media personalities who orbit around Trump's bright Twitter star. But Trump has attacked those same social media platforms. (SOUNDBITE OF ARCHIVED RECORDING) TRUMP: You look at Google, Facebook, Twitter and other social media giants. And I've made it clear that we as a country cannot tolerate political censorship, blacklisting and rigged search results. GARSD: Both Google and Facebook have denied that they censor the right. A lot of people believe that's just not true. Conservative talk show host Bill Mitchell, who's also going to the summit, blames Twitter's algorithm for some of his followers not being able to see his tweets. He hopes at the summit they talk about how social media has become the modern public square. BILL MITCHELL: And if you become the public square, you really need to offer First Amendment protections to people where everybody can have free and open speech. Let's have the open debate, and may the best man win. GARSD: Angelo Carusone, from the liberal nonprofit Media Matters, believes in free speech. But he says some of the guests at the summit alarm him. ANGELO CARUSONE: You know, there's a couple establishment players there. But for the most part, they're far-right figures, they're extremists and a lot of people with ties to white nationalism. GARSD: Mitchell and Carpe Donktum disavowed racism to NPR, but Media Matters also points to other guests, like Charlie Kirk from Turning Point USA, an organization that has been accused of racist viewpoints. Here's Carusone again. CARUSONE: When you start to bring in all these individuals that are connected to or have relationships to these communities, what they do is they go back and they validate - oh, yeah, I met with the president. I was at the White House. You know, we're a part of this. The normalization of this is the part that concerns me. GARSD: And as the president rallies his base ahead of the 2020 elections, Carusone wonders about the effect of elevating these voices on the right. Jasmine Garsd, NPR News, New York.", "section": "Politics", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-12-741330213": {"title": "Federal Trade Commission Will Settle With Facebook For About $5 Billion : NPR", "url": "https://www.npr.org/2019/07/12/741330213/federal-trade-commission-will-settle-with-facebook-for-about-5-billion", "author": "No author found", "published_date": "2019-07-12", "content": "ARI SHAPIRO, HOST: The Federal Trade Commission will settle with Facebook for about $5 billion according to The Wall Street Journal. This follows the FTC investigation that began more than a year ago after reports that Cambridge Analytica had gathered personal data from tens of millions of Facebook users. The data firm worked with President Trump's 2016 election campaign. The Wall Street Journal's Emily Glazer joins us now with more. Hi there. EMILY GLAZER: Hi. How are you? SHAPIRO: What specifically is this $5 billion fine for? GLAZER: So the FTC began an investigation after these reports came out about Cambridge Analytica and these privacy missteps by Facebook. And the FTC investigation centered on whether there was a lapse that violated this 2012 consent decree that the FTC had with Facebook where Facebook agreed to better protect user privacy. So, of course, in this case, the answer in many ways was no. And the FTC has had this long-running probe going on for quite some time, and it also got very politicized and partisan. And finally we learned that they voted this week to approve a $5 billion settlement, a record for the agency. SHAPIRO: And the vote was 3 to 2. Why the split? GLAZER: It got really politicized and partisan, which I guess is resembling our country overall right now. Basically the Republican majority, the commissioners within the FTC were voting for the settlement, but the Democratic commissioners have objected, and they really have for months, which is why this was so drawn out. They wanted the agency to be, you know, even more tough. And it wasn't just about the amount; it's also about the terms. Unfortunately we don't know too much about the terms right now, but a big sticking point for months was whether or not Mark Zuckerberg, the founder and head of Facebook, would be named and any kind of governance that could be action that could change, and we don't know where that stands right now. SHAPIRO: You said $5 billion is a record fine, but Facebook is a record-setting company. Is that fine actually going to cause them much pain? GLAZER: It really won't. You know, $5 billion is a huge amount of money, but it is not going to hurt Facebook as a company. In fact, you know, they just had earnings, and it really won't even - it'll definitely matter, but it doesn't hurt them too much financially. And this does matter a lot for the FTC because it's a record for - record penalty for violating an FTC order. The prior one was a $22. 5 million. . . SHAPIRO: Wow. GLAZER: . . . Fine against Google in 2012. So this is just, like, leaps and bounds more. SHAPIRO: And does this conclude the investigation, or is there more to it? GLAZER: There is more to come. So we don't know exactly how much time it will take for it to totally be finalized. What we know right now is that the matter has been moved to the Justice Department's civil division, and from there, it'll have to be finalized. But again, it's really unclear how long that will take. SHAPIRO: There are other investigations of Facebook happening in Congress, the Justice Department. Does this FTC fine have anything to do with what's happening in those other parts of the government? GLAZER: I think that it's all related at the end of the day, and regulators are always looking to see what their peers are doing. But this doesn't cancel out other investigations or probes necessarily. And right now we're in a place where big tech is certainly in the spotlight. So I think there's a lot that's ramping up right now because big tech is largely unregulated, and there are so many calls, especially around privacy and antitrust and the public and politicians that are really calling for more oversight. SHAPIRO: So a lot more to come on this story. Emily Glazer of The Wall Street Journal, thank you very much. GLAZER: Thank you. (SOUNDBITE OF PYRMDPLAZA'S \"DROWNING\") ARI SHAPIRO, HOST:  The Federal Trade Commission will settle with Facebook for about $5 billion according to The Wall Street Journal. This follows the FTC investigation that began more than a year ago after reports that Cambridge Analytica had gathered personal data from tens of millions of Facebook users. The data firm worked with President Trump's 2016 election campaign. The Wall Street Journal's Emily Glazer joins us now with more. Hi there. EMILY GLAZER: Hi. How are you? SHAPIRO: What specifically is this $5 billion fine for? GLAZER: So the FTC began an investigation after these reports came out about Cambridge Analytica and these privacy missteps by Facebook. And the FTC investigation centered on whether there was a lapse that violated this 2012 consent decree that the FTC had with Facebook where Facebook agreed to better protect user privacy. So, of course, in this case, the answer in many ways was no. And the FTC has had this long-running probe going on for quite some time, and it also got very politicized and partisan. And finally we learned that they voted this week to approve a $5 billion settlement, a record for the agency. SHAPIRO: And the vote was 3 to 2. Why the split? GLAZER: It got really politicized and partisan, which I guess is resembling our country overall right now. Basically the Republican majority, the commissioners within the FTC were voting for the settlement, but the Democratic commissioners have objected, and they really have for months, which is why this was so drawn out. They wanted the agency to be, you know, even more tough. And it wasn't just about the amount; it's also about the terms. Unfortunately we don't know too much about the terms right now, but a big sticking point for months was whether or not Mark Zuckerberg, the founder and head of Facebook, would be named and any kind of governance that could be action that could change, and we don't know where that stands right now. SHAPIRO: You said $5 billion is a record fine, but Facebook is a record-setting company. Is that fine actually going to cause them much pain? GLAZER: It really won't. You know, $5 billion is a huge amount of money, but it is not going to hurt Facebook as a company. In fact, you know, they just had earnings, and it really won't even - it'll definitely matter, but it doesn't hurt them too much financially. And this does matter a lot for the FTC because it's a record for - record penalty for violating an FTC order. The prior one was a $22. 5 million. . . SHAPIRO: Wow. GLAZER: . . . Fine against Google in 2012. So this is just, like, leaps and bounds more. SHAPIRO: And does this conclude the investigation, or is there more to it? GLAZER: There is more to come. So we don't know exactly how much time it will take for it to totally be finalized. What we know right now is that the matter has been moved to the Justice Department's civil division, and from there, it'll have to be finalized. But again, it's really unclear how long that will take. SHAPIRO: There are other investigations of Facebook happening in Congress, the Justice Department. Does this FTC fine have anything to do with what's happening in those other parts of the government? GLAZER: I think that it's all related at the end of the day, and regulators are always looking to see what their peers are doing. But this doesn't cancel out other investigations or probes necessarily. And right now we're in a place where big tech is certainly in the spotlight. So I think there's a lot that's ramping up right now because big tech is largely unregulated, and there are so many calls, especially around privacy and antitrust and the public and politicians that are really calling for more oversight. SHAPIRO: So a lot more to come on this story. Emily Glazer of The Wall Street Journal, thank you very much. GLAZER: Thank you. (SOUNDBITE OF PYRMDPLAZA'S \"DROWNING\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-12-741237638": {"title": "Office Workers Are Taking Slack Home To Organize Their Family Life : NPR", "url": "https://www.npr.org/2019/07/12/741237638/office-workers-are-taking-slack-home-to-organize-their-family-life", "author": "No author found", "published_date": "2019-07-12", "content": "AUDIE CORNISH, HOST: Lots of office workers use apps to manage projects and streamline communication among colleagues. Some of those workers have begun taking those apps home with them and using them to organize their family life. Taylor Lorenz co-wrote the story for The Atlantic headlined \"The Slackification Of The American Home. \" She joins us now to explain the trend. Welcome back, Taylor. TAYLOR LORENZ: Thanks for having me. CORNISH: What are some of the most popular apps people are using at home? LORENZ: Well, so a lot of families will plan a family vacation by starting a Trello board. Some people use Asana, which is another project management software to denote, you know, what type of chores a child needs to get done before they can go out and play. Others use Google Calendar reminders, Slack channels dedicated to grocery shopping. You might have a Slack channel for kids asking for playdates. Any kind of project management tool that's traditionally been used for business purposes, people are bringing home with them. CORNISH: One example that jumps out is a couple where after making decisions, they send each other email recaps saying things like, quote, \"as per hour earlier conversation, we have decided that the children will be enrolled in tennis camp over the summer. Please let me know if you want to follow up on this. \" I am both jealous and appalled. What's the rationale for this kind of communication between people who could look each other in the face and say this? LORENZ: Well, families are busier than ever. You know, our children today are super-over-scheduled. They've got more playdates and activities and extracurriculars than any child in previous generations. And so it's a lot for parents to keep on top of. So you almost need to run things as a business, and you need to set those reminders and send those sort of corporate-sounding emails just to make sure that you're staying on top of things. CORNISH: Your story has gotten a lot of reaction on social media. If I could sum it up in two words, it would be ridicule and horror. One comes from @GordonPlutsky - another must-read from Taylor Lorenz, an absolute nightmare - bringing soul-crushing work technology to your kids. Is there something to that? LORENZ: Well, look. I know a lot of people sort of cringe when you hear about these workplace technologies being brought into the home. It sounds very dystopian. You know, we already have not very strong boundaries between work and home. So I can understand the reaction. But a lot of families actually find this really helpful. You know, sometimes it takes a job to teach us really effective task management systems. And so, you know, bringing that home can actually lead to a more effective family life. And you can spend more time focusing on the fun part. CORNISH: A lot of people are criticizing this basically on the basis that, is this a substitution for what is very necessary communication to make a family work? LORENZ: Yeah. I mean, look. Families thrive on negotiation. That can often be very messy. And so to have it kind of sanitized in this really a corporate way can be off-putting, and I get that. But I think, you know, families that are communicating in these really overly corporate emails, that doesn't necessarily mean that they're talking less. None of the parents that I interviewed or people in relationships that were using this system said that they felt like it had really taken away from the intimacy of their family. The ones who stopped adopting it did so usually just because they found it wasn't useful or it was just another kind of task to manage. CORNISH: This is rising alongside another concern, which is the group of parents out there that is trying to get rid of the screens - right? - to have screen-free time. Can you do that if everyone in the household is communicating this way? LORENZ: You can't. And that's one thing that parents have found as a drawback for adopting these types of platforms and systems. It is another excuse for kids to be on their phone. So a child might start by checking their tasks to do on Trello or Jira and then, you know, quickly pop over to Instagram or start DMing. And it's a very slippery slope. CORNISH: Do you think this is going to catch on? I mean, I assume this right now is among a certain class of white-collar worker. Can you see it spreading? LORENZ: It is right now among white-collar workers and those who have office jobs that use these systems in those environments. I could see it spreading further, though. I mean, we're all trying to organize our lives a little bit. And everyone pretty much has a smartphone these days. So I do see it proliferating a little bit, but we'll see. Maybe somebody will adopt a better platform that's just for the home. CORNISH: That's Taylor Lorenz of the Atlantic. Her story with co-writer Joe Pinsker is \"The Slackification Of The American Home. \" Thanks for speaking with us. LORENZ: Thanks for having me. AUDIE CORNISH, HOST:  Lots of office workers use apps to manage projects and streamline communication among colleagues. Some of those workers have begun taking those apps home with them and using them to organize their family life. Taylor Lorenz co-wrote the story for The Atlantic headlined \"The Slackification Of The American Home. \" She joins us now to explain the trend. Welcome back, Taylor. TAYLOR LORENZ: Thanks for having me. CORNISH: What are some of the most popular apps people are using at home? LORENZ: Well, so a lot of families will plan a family vacation by starting a Trello board. Some people use Asana, which is another project management software to denote, you know, what type of chores a child needs to get done before they can go out and play. Others use Google Calendar reminders, Slack channels dedicated to grocery shopping. You might have a Slack channel for kids asking for playdates. Any kind of project management tool that's traditionally been used for business purposes, people are bringing home with them. CORNISH: One example that jumps out is a couple where after making decisions, they send each other email recaps saying things like, quote, \"as per hour earlier conversation, we have decided that the children will be enrolled in tennis camp over the summer. Please let me know if you want to follow up on this. \" I am both jealous and appalled. What's the rationale for this kind of communication between people who could look each other in the face and say this? LORENZ: Well, families are busier than ever. You know, our children today are super-over-scheduled. They've got more playdates and activities and extracurriculars than any child in previous generations. And so it's a lot for parents to keep on top of. So you almost need to run things as a business, and you need to set those reminders and send those sort of corporate-sounding emails just to make sure that you're staying on top of things. CORNISH: Your story has gotten a lot of reaction on social media. If I could sum it up in two words, it would be ridicule and horror. One comes from @GordonPlutsky - another must-read from Taylor Lorenz, an absolute nightmare - bringing soul-crushing work technology to your kids. Is there something to that? LORENZ: Well, look. I know a lot of people sort of cringe when you hear about these workplace technologies being brought into the home. It sounds very dystopian. You know, we already have not very strong boundaries between work and home. So I can understand the reaction. But a lot of families actually find this really helpful. You know, sometimes it takes a job to teach us really effective task management systems. And so, you know, bringing that home can actually lead to a more effective family life. And you can spend more time focusing on the fun part. CORNISH: A lot of people are criticizing this basically on the basis that, is this a substitution for what is very necessary communication to make a family work? LORENZ: Yeah. I mean, look. Families thrive on negotiation. That can often be very messy. And so to have it kind of sanitized in this really a corporate way can be off-putting, and I get that. But I think, you know, families that are communicating in these really overly corporate emails, that doesn't necessarily mean that they're talking less. None of the parents that I interviewed or people in relationships that were using this system said that they felt like it had really taken away from the intimacy of their family. The ones who stopped adopting it did so usually just because they found it wasn't useful or it was just another kind of task to manage. CORNISH: This is rising alongside another concern, which is the group of parents out there that is trying to get rid of the screens - right? - to have screen-free time. Can you do that if everyone in the household is communicating this way? LORENZ: You can't. And that's one thing that parents have found as a drawback for adopting these types of platforms and systems. It is another excuse for kids to be on their phone. So a child might start by checking their tasks to do on Trello or Jira and then, you know, quickly pop over to Instagram or start DMing. And it's a very slippery slope. CORNISH: Do you think this is going to catch on? I mean, I assume this right now is among a certain class of white-collar worker. Can you see it spreading? LORENZ: It is right now among white-collar workers and those who have office jobs that use these systems in those environments. I could see it spreading further, though. I mean, we're all trying to organize our lives a little bit. And everyone pretty much has a smartphone these days. So I do see it proliferating a little bit, but we'll see. Maybe somebody will adopt a better platform that's just for the home. CORNISH: That's Taylor Lorenz of the Atlantic. Her story with co-writer Joe Pinsker is \"The Slackification Of The American Home. \" Thanks for speaking with us. LORENZ: Thanks for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-12-739873391": {"title": "Location Apps Now Seen As Critical For Safety, Logistics : NPR", "url": "https://www.npr.org/2019/07/12/739873391/once-considered-creepy-location-apps-now-seen-as-critical-for-safety-logistics", "author": "No author found", "published_date": "2019-07-12", "content": "", "section": "Technology", "disclaimer": ""}, "2019-07-12-741038121": {"title": "Rep. Alexandria Ocasio-Cortez Sued Over Blocking Critics On Twitter : NPR", "url": "https://www.npr.org/2019/07/12/741038121/alexandria-ocasio-cortez-is-sued-over-blocking-twitter-followers", "author": "No author found", "published_date": "2019-07-12", "content": "", "section": "Law", "disclaimer": ""}, "2019-07-12-740824015": {"title": "Nir Eyal: How Easy Is It To \"Unhook\" Ourselves From Our Devices? : NPR", "url": "https://www.npr.org/2019/07/12/740824015/nir-eyal-how-easy-is-it-to-unhook-ourselves-from-our-devices", "author": "No author found", "published_date": "2019-07-12", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas around Digital Manipulation. NIR EYAL: I mean, all design is the art of manipulation. And it doesn't matter what type of design we're talking about; we're trying to engineer people's behavior - to do a particular behavior, to think a certain thought, to feel a certain feeling. All design manipulates us. And to be honest, we pay for the privilege. RAZ: This is Nir Eyal. EYAL: And I basically study the intersection of psychology, technology and business. RAZ: Nir actually consults for tech companies, to help them create more engaging products - products that are really good at holding our attention. EYAL: I am what you might call a behavioral designer. RAZ: And to understand just how we get hooked, Nir points to a series of studies conducted in the 1940s by the psychologist B. F. Skinner. EYAL: Skinner did some fascinating experiments where he took these food pellets, and he would give them to a pigeon in a box. (SOUNDBITE OF ARCHIVED RECORDING)B F SKINNER: I will try to pick out some particular pattern or behavior, and. . . EYAL: And every time the pigeon pecked at the disc, they would receive a reward - a little food pellet. (SOUNDBITE OF ARCHIVED RECORDING)SKINNER: The bird is already conditioned to eat from the magazine (ph). Sounds. . . RAZ: OK, simple enough - the pigeon gets a reward. EYAL: And Skinner very quickly realized that, as long as the pigeon was hungry, they would receive this food pellet, and very quickly, he could train them to peck the disc every time. RAZ: But then one day, Skinner ran out of those food pellets. EYAL: He didn't have enough, and so he could only afford to give out the food pellets every once in a while. So one time, he'd give out the food pellet, and the pigeon would peck at the disc, and they would receive one. The next time the pigeon would peck at the disc, they wouldn't receive a reward. (SOUNDBITE OF ARCHIVED RECORDING)SKINNER: Perhaps every 10th time, or perhaps only once every minute, or something like that. EYAL: And when the schedule of reinforcement was variable, when there was some mystery around when the pigeon would receive the reward, the pigeon would peck at the disc more frequently; the rate of response increased. RAZ: The more random the reward, the more the pigeon pecked at the disc. EYAL: This is called a variable reward. And so we see this mechanic in all sorts of things that we find most engaging, most habit-forming, the things that capture our attention. So, of course, online, when you think about the Facebook news feed or the Instagram feed or the LinkedIn feed, I mean, the feed is this masterful manifestation of a variable reward, where, to get more of these rewards, you just have to keep scrolling and scrolling and scrolling, searching for more information. RAZ: And so all those pings and dings and notifications, those are all triggers that tell us to go back for more, until we're so hooked that we don't need those reminders anymore. EYAL: Eventually, they start attaching their product's use to an association inside our own heads - that's when the habit takes hold. RAZ: Nir Eyal picks up this idea from the TED stage. (SOUNDBITE OF TED TALK)EYAL: Where do we go when we're feeling lonely? What app or website do we check? Facebook, of course. What about when we're unsure about something? Before we scan our brains to see if we know the answer we're googling it. And what about when we're feeling bored? Well, that's when people go onto YouTube or Reddit, check stock prices, sports scores, the front-page news. And what we see happening today is that companies building habit-forming technologies are pushing that the easier behavior is to do, the more likely we are to do it. RAZ: So Nir, I mean, hearing this and, you know, knowing that you help companies design some of these products, I mean, it almost feels like these things are deliberately designed to addict us. EYAL: So I think we do have to keep this in perspective. You know, very few people who drink alcohol get addicted to alcohol, even though alcohol is a highly addictive substance. And so what we do when we pathologize this, the problem when we call everything addictive, is we're missing the point here; you know, we're painting this tech boogey man, and we're giving these companies more control, believe it or not, than they deserve. RAZ: But is there an argument that some social media companies are trying to capitalize on our negative emotions, to manipulate us into using their products more? I mean, would you agree that is potentially the case, the reality? EYAL: Guy, every human behavior is prompted from a negative emotion. Everything we do is to escape discomfort. You know, the reason we sit down to watch television or read the newspaper is also because of discomfort. It's worry. It's fear. It's fatigue. That's the way products are designed, is to solve our problems. It's such an easy story to say it's all the company's fault, and they're doing it to us, and they're hijacking our brains. But guess what - we have power over this. The fact is, sitting here and wringing our fist at these companies doesn't change anything, right? If we wait for the politicians to do something and hold our breath, we're going to suffocate. RAZ: So just to be clear - I mean, your view on all this is that we need to take personal responsibility for the decisions we make and that we can't blame technology companies for attracting us, drawing us in, keeping us on their products. Ultimately, it's our own responsibility; we have to make that decision. EYAL: So I think we're going through this adjustment period. Sophocles, the Greek philosopher, said that nothing vast enters the life of mortals without a curse. And so many new technologies have a downside, and we're dealing with that downside today, and we're learning about how to use them appropriately. RAZ: Sure, sure. EYAL: You know, when I was growing up in the 1980s, we had ashtrays all over our house. Now, my parents didn't smoke, and yet we had all these ashtrays because, back then, if you walked into somebody's house, you felt free to light up in their living room, back when smoking rates were at 60% of the adult population. Today, they're at 16%. And if somebody came to my house and lit up a cigarette in my living room, I'd kick them out, and we wouldn't be friends anymore. So the social norms changed, and we are currently learning those norms. RAZ: The analogy with tobacco is an interesting one because other people in the program have made it, which has been - yes, that's right. There was a transition period where we didn't really understand, you know, the effects of tobacco. And similarly, with social media, we don't fully understand it. But we regulated tobacco, and therefore, we could do the same with some of these social media companies and companies that collect vast amounts of data. Do you think that's a fair comparison and argument? EYAL: Well, remember - we're not freebasing Facebook; we're not injecting Instagram here. Nothing is entering into the bloodstream. These are behaviors. These are habits, and habits can change. And so I think that, you know, again, there's so much we can do right now, why would we wait? And I think there is a role for legislation for some things. I think that these companies' monopoly status needs to be looked at. I think that their use of data needs to be looked at. But for this specific problem of tech overuse, this is our problem. This is something we can do something about. RAZ: But, I mean, if, as you say, like, we all have agency - right? - to just stop using these products, which I think a lot of people would disagree with (laughter), I mean, do you even think that it's possible for us to be manipulated? EYAL: Absolutely. So there's two types of manipulation, right? There's persuasion and coercion. Persuasion is helping people do something they want to do; coercion is when we get people to do things they don't want to do, and coercion is always unethical. Now, what's the difference between persuasion and coercion? A simple test is regret. Would the user regret using our product and service? Not only is it ethical, as I mentioned, but, again, if you build a product that people regret using, they stop using your product. It's bad for business. And so it's not us to judge people - you know, many parents today, they're, oh, the video games - that's a terrible waste of time, as they sit down on their couch and watch a football game. Is there anything that's morally superior to watching a football game than playing Candy Crush on your phone? It's difficult for me to say, and who am I to say, to judge people? If that's how you want to spend your pastime, there's nothing wrong with it. RAZ: I'm not a neuroscientist or a psychologist - right? - but what I can say is that I fundamentally believe that people can make choices, but there are certainly choices that are made for people. And I think, look - I'm not denying that you have a legitimate argument - you do - I just don't think it's that clear-cut. EYAL: Yeah. Well, I don't think it's easy, necessarily. I'm not pro-tech all the time; I'm for having a conversation so that we can use them with intent, use them the way we want to, as opposed to maybe the way these companies want us to. If that's playing a video game, if that's spending time on Facebook, if that's listening to a radio program - wonderful. Spend that time with intent, as opposed to letting other people control your behavior. (SOUNDBITE OF MUSIC)RAZ: That's Nir Eyal. He is a behavioral designer. You can see his full talk at ted. com. GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas around Digital Manipulation. NIR EYAL: I mean, all design is the art of manipulation. And it doesn't matter what type of design we're talking about; we're trying to engineer people's behavior - to do a particular behavior, to think a certain thought, to feel a certain feeling. All design manipulates us. And to be honest, we pay for the privilege. RAZ: This is Nir Eyal. EYAL: And I basically study the intersection of psychology, technology and business. RAZ: Nir actually consults for tech companies, to help them create more engaging products - products that are really good at holding our attention. EYAL: I am what you might call a behavioral designer. RAZ: And to understand just how we get hooked, Nir points to a series of studies conducted in the 1940s by the psychologist B. F. Skinner. EYAL: Skinner did some fascinating experiments where he took these food pellets, and he would give them to a pigeon in a box. (SOUNDBITE OF ARCHIVED RECORDING) B F SKINNER: I will try to pick out some particular pattern or behavior, and. . . EYAL: And every time the pigeon pecked at the disc, they would receive a reward - a little food pellet. (SOUNDBITE OF ARCHIVED RECORDING) SKINNER: The bird is already conditioned to eat from the magazine (ph). Sounds. . . RAZ: OK, simple enough - the pigeon gets a reward. EYAL: And Skinner very quickly realized that, as long as the pigeon was hungry, they would receive this food pellet, and very quickly, he could train them to peck the disc every time. RAZ: But then one day, Skinner ran out of those food pellets. EYAL: He didn't have enough, and so he could only afford to give out the food pellets every once in a while. So one time, he'd give out the food pellet, and the pigeon would peck at the disc, and they would receive one. The next time the pigeon would peck at the disc, they wouldn't receive a reward. (SOUNDBITE OF ARCHIVED RECORDING) SKINNER: Perhaps every 10th time, or perhaps only once every minute, or something like that. EYAL: And when the schedule of reinforcement was variable, when there was some mystery around when the pigeon would receive the reward, the pigeon would peck at the disc more frequently; the rate of response increased. RAZ: The more random the reward, the more the pigeon pecked at the disc. EYAL: This is called a variable reward. And so we see this mechanic in all sorts of things that we find most engaging, most habit-forming, the things that capture our attention. So, of course, online, when you think about the Facebook news feed or the Instagram feed or the LinkedIn feed, I mean, the feed is this masterful manifestation of a variable reward, where, to get more of these rewards, you just have to keep scrolling and scrolling and scrolling, searching for more information. RAZ: And so all those pings and dings and notifications, those are all triggers that tell us to go back for more, until we're so hooked that we don't need those reminders anymore. EYAL: Eventually, they start attaching their product's use to an association inside our own heads - that's when the habit takes hold. RAZ: Nir Eyal picks up this idea from the TED stage. (SOUNDBITE OF TED TALK) EYAL: Where do we go when we're feeling lonely? What app or website do we check? Facebook, of course. What about when we're unsure about something? Before we scan our brains to see if we know the answer we're googling it. And what about when we're feeling bored? Well, that's when people go onto YouTube or Reddit, check stock prices, sports scores, the front-page news. And what we see happening today is that companies building habit-forming technologies are pushing that the easier behavior is to do, the more likely we are to do it. RAZ: So Nir, I mean, hearing this and, you know, knowing that you help companies design some of these products, I mean, it almost feels like these things are deliberately designed to addict us. EYAL: So I think we do have to keep this in perspective. You know, very few people who drink alcohol get addicted to alcohol, even though alcohol is a highly addictive substance. And so what we do when we pathologize this, the problem when we call everything addictive, is we're missing the point here; you know, we're painting this tech boogey man, and we're giving these companies more control, believe it or not, than they deserve. RAZ: But is there an argument that some social media companies are trying to capitalize on our negative emotions, to manipulate us into using their products more? I mean, would you agree that is potentially the case, the reality? EYAL: Guy, every human behavior is prompted from a negative emotion. Everything we do is to escape discomfort. You know, the reason we sit down to watch television or read the newspaper is also because of discomfort. It's worry. It's fear. It's fatigue. That's the way products are designed, is to solve our problems. It's such an easy story to say it's all the company's fault, and they're doing it to us, and they're hijacking our brains. But guess what - we have power over this. The fact is, sitting here and wringing our fist at these companies doesn't change anything, right? If we wait for the politicians to do something and hold our breath, we're going to suffocate. RAZ: So just to be clear - I mean, your view on all this is that we need to take personal responsibility for the decisions we make and that we can't blame technology companies for attracting us, drawing us in, keeping us on their products. Ultimately, it's our own responsibility; we have to make that decision. EYAL: So I think we're going through this adjustment period. Sophocles, the Greek philosopher, said that nothing vast enters the life of mortals without a curse. And so many new technologies have a downside, and we're dealing with that downside today, and we're learning about how to use them appropriately. RAZ: Sure, sure. EYAL: You know, when I was growing up in the 1980s, we had ashtrays all over our house. Now, my parents didn't smoke, and yet we had all these ashtrays because, back then, if you walked into somebody's house, you felt free to light up in their living room, back when smoking rates were at 60% of the adult population. Today, they're at 16%. And if somebody came to my house and lit up a cigarette in my living room, I'd kick them out, and we wouldn't be friends anymore. So the social norms changed, and we are currently learning those norms. RAZ: The analogy with tobacco is an interesting one because other people in the program have made it, which has been - yes, that's right. There was a transition period where we didn't really understand, you know, the effects of tobacco. And similarly, with social media, we don't fully understand it. But we regulated tobacco, and therefore, we could do the same with some of these social media companies and companies that collect vast amounts of data. Do you think that's a fair comparison and argument? EYAL: Well, remember - we're not freebasing Facebook; we're not injecting Instagram here. Nothing is entering into the bloodstream. These are behaviors. These are habits, and habits can change. And so I think that, you know, again, there's so much we can do right now, why would we wait? And I think there is a role for legislation for some things. I think that these companies' monopoly status needs to be looked at. I think that their use of data needs to be looked at. But for this specific problem of tech overuse, this is our problem. This is something we can do something about. RAZ: But, I mean, if, as you say, like, we all have agency - right? - to just stop using these products, which I think a lot of people would disagree with (laughter), I mean, do you even think that it's possible for us to be manipulated? EYAL: Absolutely. So there's two types of manipulation, right? There's persuasion and coercion. Persuasion is helping people do something they want to do; coercion is when we get people to do things they don't want to do, and coercion is always unethical. Now, what's the difference between persuasion and coercion? A simple test is regret. Would the user regret using our product and service? Not only is it ethical, as I mentioned, but, again, if you build a product that people regret using, they stop using your product. It's bad for business. And so it's not us to judge people - you know, many parents today, they're, oh, the video games - that's a terrible waste of time, as they sit down on their couch and watch a football game. Is there anything that's morally superior to watching a football game than playing Candy Crush on your phone? It's difficult for me to say, and who am I to say, to judge people? If that's how you want to spend your pastime, there's nothing wrong with it. RAZ: I'm not a neuroscientist or a psychologist - right? - but what I can say is that I fundamentally believe that people can make choices, but there are certainly choices that are made for people. And I think, look - I'm not denying that you have a legitimate argument - you do - I just don't think it's that clear-cut. EYAL: Yeah. Well, I don't think it's easy, necessarily. I'm not pro-tech all the time; I'm for having a conversation so that we can use them with intent, use them the way we want to, as opposed to maybe the way these companies want us to. If that's playing a video game, if that's spending time on Facebook, if that's listening to a radio program - wonderful. Spend that time with intent, as opposed to letting other people control your behavior. (SOUNDBITE OF MUSIC) RAZ: That's Nir Eyal. He is a behavioral designer. You can see his full talk at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-12-740843747": {"title": "Doug Roble: What Happens When Visual Effects Aren't Limited To Just Movies? : NPR", "url": "https://www.npr.org/2019/07/12/740843747/doug-roble-what-happens-when-visual-effects-arent-limited-to-just-movies", "author": "No author found", "published_date": "2019-07-12", "content": "GUY RAZ, HOST: Hello, Doug. DOUG ROBLE: Hello. RAZ: Hello, Doug. It's Guy Raz here. How are you? ROBLE: Very good. How are you? RAZ: Good. I am talking to the real Doug, right? This is real Doug, not digital Doug? ROBLE: You know, before I started this, I did not realize how often I would be asked that question. RAZ: (Laughter). ROBLE: It's quite annoying at work - people keep poking me to make sure that I'm actually there. RAZ: This is Doug Roble, and Doug makes digital humans that look exactly like us. (SOUNDBITE OF TED TALK)ROBLE: I'm not a real person. I'm actually a copy of a real person. Although, I feel like a real person. It's kind of hard to explain. Hold on - I think I saw a real person - there's one. Let's bring him onstage. RAZ: OK, so obviously, Doug, this is your voice on the TED stage. But because we're on the radio and not on TV, can you explain what's going on here? ROBLE: Well, first of all, I'm wearing a motion capture suit. Then I'm also wearing a - basically, a bicycle helmet that has a camera and some lights attached to it. And the reason I'm wearing that is because I'm controlling a digital version of me at the same time that I'm out on stage. RAZ: Yeah, there's a giant screen, and we're just seeing you on the screen. But it's a version of you made up of ones and zeros. ROBLE: It is. It's - I'm basically controlling a computer game character. (SOUNDBITE OF TED TALK)ROBLE: What you see up there is a digital human. We call him DigiDoug. He's actually a 3D character that I'm controlling live, in real time. RAZ: And Doug makes more than just digital versions of himself. For nearly three decades, he's worked on creating digital effects and digital humans for the film industry. ROBLE: The next step would be to do an entire film where one character is just a computer-generated character who you just don't even realize is computer-generated. RAZ: At that point, the possibilities are presumably - are endless, right? You could have that character do whatever you want it to do. ROBLE: Yeah. I mean, it opens up a lot of doors. It could be really cool. (SOUNDBITE OF MUSIC)RAZ: Doug Roble and DigiDoug continue this idea from the TED stage. (SOUNDBITE OF TED TALK)ROBLE: We've been putting humans and creatures into film that you accept as real. If they're happy, you should feel happy. And if they feel pain, you should empathize with them. If you were having a conversation with DigiDoug, one on one, is it real enough so that you could tell whether or not I was lying to you? So that was our goal. But why did we do this? First of all, it is just crazy cool. (LAUGHTER)ROBLE: How cool is it? Well, with the push of a button, I can deliver this talk as a completely different character. (LAUGHTER)ROBLE: This is Elbor. We put him together to test how this would work with a different appearance, right? And the cool thing about this technology is that, while I've changed my character, the performance is still all me. (SOUNDBITE OF MUSIC)RAZ: Right in the middle of your talk - so we're still seeing Digital Doug on the screen - you push a button, and then Digital Doug turns into a character named Elbor. . . ROBLE: (Laughter). RAZ: . . . Who sort of looks like an elf or a troll. But he's mimicking - he's still you. Like, every turn of your head or microexpression - like, Elbor is mimicking that. You become Elbor. ROBLE: Yes. We wanted to see how far we could push this technology. So here, we're taking my motion. And I'm over 6 foot tall. And we're transferring it on to this guy who's, I think, about 3-foot-6. So he's about half my size. It's just like, oh, look. I'm a little critter. And you talk about inhabiting a character. I really inhabited that character. RAZ: Yeah. I mean, every teeny movement of your mouth, of your face, like, Elbor was doing the same thing. And it was at that point in your talk where I freaked out - and not in a good way, Doug. I saw that, and I thought, this is going to be used in a really scary way. Right then at that point when I saw you turn into Elbor, I thought, wow, we can inhabit anybody. We will be able to inhabit anyone and pretend like we are them. ROBLE: We are entirely aware of this. This is one of the big things with this technology - is that it presents so many neat opportunities. But at the exact same time, if we created a digital double of somebody who was famous and I put on the suit and then all of a sudden, I can control that famous person with my body and my face, that gets really, really creepy. And it gets really tricky. The one thing that's nice is to get a character of this quality right now, the barrier's extraordinarily high. When you say you were flabbergasted by it, when I see it, all I see are the flaws. And so there's still a level of reality that we want to push it towards. RAZ: I mean, it's clear that we are heading there, that it will get perfect pretty soon. And at some point, the barrier will actually be pretty low to sort of getting into this. When I was a kid, there was a film that freaked me out. (SOUNDBITE OF FILM, \"THE RUNNING MAN\")UNIDENTIFIED ACTOR: (As character) In the year 2017. . . RAZ: It's called \"The Running Man. \" And it was implausible. It was based around a prison. (SOUNDBITE OF FILM, \"THE RUNNING MAN\")UNIDENTIFIED ACTOR: (As character) It's a game between life and death. RAZ: And Arnold Schwarzenegger was a prisoner in the Running Man. And, you know, the object was to escape being hunted down and then, you know, killed. ROBLE: Yep. RAZ: He was in this prison because he was framed for committing a massacre. And they made a fake video that showed him killing people. And when I was a kid, and you watched it, you thought, that's just implausible. How could you ever do that? And when I was watching your TED Talk, I had a flashback to that movie. . . ROBLE: Yeah. RAZ: . . . Because as absurd as it was in the '80s or in the '90s, it's actually happening. We've already seen politicians' videos being manipulated. We just saw it not too long ago with the speaker of the House. ROBLE: And that was an easy manipulation. And even if you did have a super good way of detecting these things, it's going to take time. And especially with the Internet now and everything, if a video like that goes out and it has a day or two to run around the world unmolested, if you come out later and say, oh, no, that was a fake video, people have moved on already. And they've - the damage is done. It's - hopefully, people have this nice dose of skepticism whenever they look at something and say, where's the source? Is this really what happened? I think that's key 'cause it's going to be really, really hard to stop. RAZ: That's Doug Roble. He's a computer graphics researcher with Digital Domain. You can see Doug's full talk at ted. com. (SOUNDBITE OF SONG, \"DIGITAL WITNESS\")ST VINCENT: (Singing) Digital witnesses. What's the point of even sleeping? If I can't show it, if you can't see me, what's the point of doing anything? RAZ: Hey, thanks for listening to our show about Digital Manipulation this week. If you want to find out more about who was on it, go to ted. npr. org. And to see hundreds more Ted Talks, check out ted. com com or the TED app. Our production staff at NPR includes Jeff Rogers, Sanaz Meshkinpour, Neva Grant, Casey Herman, Rachel Faulkner, Diba Mohtasham, James Delahoussaye and J. C. Howard, with help from Daniel Shubin and Brent Baughman. Our intern is Emmanuel Johnson. Our partners at TED are Chris Anderson, Colin Helms, Anna Phelan and Janet Lee. I'm Guy Raz, and you've been listening to ideas worth spreading right here on the TED Radio Hour from NPR. GUY RAZ, HOST:  Hello, Doug. DOUG ROBLE: Hello. RAZ: Hello, Doug. It's Guy Raz here. How are you? ROBLE: Very good. How are you? RAZ: Good. I am talking to the real Doug, right? This is real Doug, not digital Doug? ROBLE: You know, before I started this, I did not realize how often I would be asked that question. RAZ: (Laughter). ROBLE: It's quite annoying at work - people keep poking me to make sure that I'm actually there. RAZ: This is Doug Roble, and Doug makes digital humans that look exactly like us. (SOUNDBITE OF TED TALK) ROBLE: I'm not a real person. I'm actually a copy of a real person. Although, I feel like a real person. It's kind of hard to explain. Hold on - I think I saw a real person - there's one. Let's bring him onstage. RAZ: OK, so obviously, Doug, this is your voice on the TED stage. But because we're on the radio and not on TV, can you explain what's going on here? ROBLE: Well, first of all, I'm wearing a motion capture suit. Then I'm also wearing a - basically, a bicycle helmet that has a camera and some lights attached to it. And the reason I'm wearing that is because I'm controlling a digital version of me at the same time that I'm out on stage. RAZ: Yeah, there's a giant screen, and we're just seeing you on the screen. But it's a version of you made up of ones and zeros. ROBLE: It is. It's - I'm basically controlling a computer game character. (SOUNDBITE OF TED TALK) ROBLE: What you see up there is a digital human. We call him DigiDoug. He's actually a 3D character that I'm controlling live, in real time. RAZ: And Doug makes more than just digital versions of himself. For nearly three decades, he's worked on creating digital effects and digital humans for the film industry. ROBLE: The next step would be to do an entire film where one character is just a computer-generated character who you just don't even realize is computer-generated. RAZ: At that point, the possibilities are presumably - are endless, right? You could have that character do whatever you want it to do. ROBLE: Yeah. I mean, it opens up a lot of doors. It could be really cool. (SOUNDBITE OF MUSIC) RAZ: Doug Roble and DigiDoug continue this idea from the TED stage. (SOUNDBITE OF TED TALK) ROBLE: We've been putting humans and creatures into film that you accept as real. If they're happy, you should feel happy. And if they feel pain, you should empathize with them. If you were having a conversation with DigiDoug, one on one, is it real enough so that you could tell whether or not I was lying to you? So that was our goal. But why did we do this? First of all, it is just crazy cool. (LAUGHTER) ROBLE: How cool is it? Well, with the push of a button, I can deliver this talk as a completely different character. (LAUGHTER) ROBLE: This is Elbor. We put him together to test how this would work with a different appearance, right? And the cool thing about this technology is that, while I've changed my character, the performance is still all me. (SOUNDBITE OF MUSIC) RAZ: Right in the middle of your talk - so we're still seeing Digital Doug on the screen - you push a button, and then Digital Doug turns into a character named Elbor. . . ROBLE: (Laughter). RAZ: . . . Who sort of looks like an elf or a troll. But he's mimicking - he's still you. Like, every turn of your head or microexpression - like, Elbor is mimicking that. You become Elbor. ROBLE: Yes. We wanted to see how far we could push this technology. So here, we're taking my motion. And I'm over 6 foot tall. And we're transferring it on to this guy who's, I think, about 3-foot-6. So he's about half my size. It's just like, oh, look. I'm a little critter. And you talk about inhabiting a character. I really inhabited that character. RAZ: Yeah. I mean, every teeny movement of your mouth, of your face, like, Elbor was doing the same thing. And it was at that point in your talk where I freaked out - and not in a good way, Doug. I saw that, and I thought, this is going to be used in a really scary way. Right then at that point when I saw you turn into Elbor, I thought, wow, we can inhabit anybody. We will be able to inhabit anyone and pretend like we are them. ROBLE: We are entirely aware of this. This is one of the big things with this technology - is that it presents so many neat opportunities. But at the exact same time, if we created a digital double of somebody who was famous and I put on the suit and then all of a sudden, I can control that famous person with my body and my face, that gets really, really creepy. And it gets really tricky. The one thing that's nice is to get a character of this quality right now, the barrier's extraordinarily high. When you say you were flabbergasted by it, when I see it, all I see are the flaws. And so there's still a level of reality that we want to push it towards. RAZ: I mean, it's clear that we are heading there, that it will get perfect pretty soon. And at some point, the barrier will actually be pretty low to sort of getting into this. When I was a kid, there was a film that freaked me out. (SOUNDBITE OF FILM, \"THE RUNNING MAN\") UNIDENTIFIED ACTOR: (As character) In the year 2017. . . RAZ: It's called \"The Running Man. \" And it was implausible. It was based around a prison. (SOUNDBITE OF FILM, \"THE RUNNING MAN\") UNIDENTIFIED ACTOR: (As character) It's a game between life and death. RAZ: And Arnold Schwarzenegger was a prisoner in the Running Man. And, you know, the object was to escape being hunted down and then, you know, killed. ROBLE: Yep. RAZ: He was in this prison because he was framed for committing a massacre. And they made a fake video that showed him killing people. And when I was a kid, and you watched it, you thought, that's just implausible. How could you ever do that? And when I was watching your TED Talk, I had a flashback to that movie. . . ROBLE: Yeah. RAZ: . . . Because as absurd as it was in the '80s or in the '90s, it's actually happening. We've already seen politicians' videos being manipulated. We just saw it not too long ago with the speaker of the House. ROBLE: And that was an easy manipulation. And even if you did have a super good way of detecting these things, it's going to take time. And especially with the Internet now and everything, if a video like that goes out and it has a day or two to run around the world unmolested, if you come out later and say, oh, no, that was a fake video, people have moved on already. And they've - the damage is done. It's - hopefully, people have this nice dose of skepticism whenever they look at something and say, where's the source? Is this really what happened? I think that's key 'cause it's going to be really, really hard to stop. RAZ: That's Doug Roble. He's a computer graphics researcher with Digital Domain. You can see Doug's full talk at ted. com. (SOUNDBITE OF SONG, \"DIGITAL WITNESS\") ST VINCENT: (Singing) Digital witnesses. What's the point of even sleeping? If I can't show it, if you can't see me, what's the point of doing anything? RAZ: Hey, thanks for listening to our show about Digital Manipulation this week. If you want to find out more about who was on it, go to ted. npr. org. And to see hundreds more Ted Talks, check out ted. com com or the TED app. Our production staff at NPR includes Jeff Rogers, Sanaz Meshkinpour, Neva Grant, Casey Herman, Rachel Faulkner, Diba Mohtasham, James Delahoussaye and J. C. Howard, with help from Daniel Shubin and Brent Baughman. Our intern is Emmanuel Johnson. Our partners at TED are Chris Anderson, Colin Helms, Anna Phelan and Janet Lee. I'm Guy Raz, and you've been listening to ideas worth spreading right here on the TED Radio Hour from NPR.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-12-740797287": {"title": "Finn Myrstad: What Happens When We Sign Away Our Online Privacy? : NPR", "url": "https://www.npr.org/2019/07/12/740797287/finn-myrstad-what-happens-when-we-sign-away-our-online-privacy", "author": "No author found", "published_date": "2019-07-12", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about Digital Manipulation. (SOUNDBITE OF MUSIC)RAZ: All right, let me ask you about Cayla. Tell me, what is Cayla? Tell me about Cayla. FINN MYRSTAD: (Laughter) So Cayla, I would say, was an Internet-connected doll. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED SINGER: (Singing) Cayla knows millions of things. MYRSTAD: The children can talk to the doll. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED CHILD #1: How do you make a cake? COMPUTER-GENERATED VOICE: Mix eggs, flower, milk, butter. . . MYRSTAD: And the doll would use speech-recognition technology and answer the child's questions. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED CHILD #2: My Friend Cayla talks, listens to you, plays games with you and knows millions of things. RAZ: And because Cayla talks to you and listens to you and knows millions of things, Finn Myrstad, who is the director of digital policy at the Norwegian Consumer Council, wanted to see how easily Cayla could be hacked. MYRSTAD: We decided to investigate or look at this doll more closely, and what we discovered was that it had very little security. So anyone within Bluetooth range or within a certain distance could basically connect to the doll through their phones and initiate the two-way conversation. (SOUNDBITE OF TED TALK)UNIDENTIFIED PERSON #2: (As Cayla) Hi, my name is Cayla. What is yours? MYRSTAD: Finn. RAZ: On the TED stage, you do this. You bring out a colleague, who from, like, backstage is talking to you through Cayla and presumably could talk to a child in their bedroom. (SOUNDBITE OF TED TALK)UNIDENTIFIED PERSON #2: (As Cayla) Is your mom close by? MYRSTAD: No, she's in the store. UNIDENTIFIED PERSON #2: (As Cayla) You want to come out and play with me? MYRSTAD: That's a great idea. UNIDENTIFIED PERSON #2: (As Cayla) Oh, great. (SOUNDBITE OF MUSIC)RAZ: It's totally weird. MYRSTAD: Yeah, yeah. And it's completely, of course, unacceptable. And what made this story worse was that there was a label on the packaging saying that this was an Internet-safe doll while it really had no safety or security precautions in it whatsoever. (SOUNDBITE OF MUSIC)RAZ: Now, did Cayla, like, also listen in on the things people were telling her and then using that information? MYRSTAD: Well, we read the terms and conditions that no one reads, and in there, the company reserved the rights to use the voice recordings of the child and use it for targeted advertisement, for example. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED CHILD #2: Cayla knows millions of things. UNIDENTIFIED CHILD #1: How do you make a cake? MYRSTAD: But because we don't have any protections when it comes to data or when it comes to security, the market is flooded with insecure products and products which has a business model of selling our data. And that really decreases trust, and it leaves people completely apathetic to this. (SOUNDBITE OF MUSIC)RAZ: OK, so Cayla might sound like an extreme example of a tech company acting irresponsibly, but is it really? Is it an anomaly? Because when you unwrap Cayla and install the companion app, there is a user agreement. (SOUNDBITE OF TED TALK)MYRSTAD: So we read also the terms and conditions that no one reads. RAZ: The terms and conditions that no one reads. Finn Myrstad continues the story from the TED stage. (SOUNDBITE OF TED TALK)MYRSTAD: Like most of you, I have dozens of apps on my phone. If used properly, they can make our lives easier, more convenient and maybe even healthier. But have we been lulled into a false sense of security? It starts simply by ticking a box. Yes, we say, I've read the terms. But have you really read the terms? Are you sure they didn't look too long, and the last time you tried, they were impossible to understand, and you needed to use the service now? And now the power imbalance is established because we have agreed to our personal information being gathered and used on a scale we could never imagine. This is why my colleagues and I decided to take a deeper look at this. We set out to read the terms of popular apps on the average phone. And to show the world how unrealistic it is to expect consumers to actually read the terms, we printed them - more than 900 pages - and sat down in our office and read them out loud ourselves. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #3: Angry Birds, terms of service - the following terms of service and end-user license agreement. . . (SOUNDBITE OF TED TALK)MYRSTAD: Streaming the experiment live on our website, it took quite a long time. It took us 31 hours, 49 minutes and 11 seconds to read the terms on an average phone - that is longer than a movie marathon of the \"Harry Potter\" movies and \"The Godfather\" movies combined. And reading is one thing; understanding is another story - that would have taken us much, much longer. (SOUNDBITE OF MUSIC)RAZ: But this isn't just about, like, a creepy doll or, like, how incredibly frustrating and unrealistic it is to have to read hundreds of pages of user agreements. I mean, all of this data collected on us - I mean, companies can and do use it in some really, you know, sleazy ways, right? MYRSTAD: Yeah. And what we're seeing increasingly is that this data can also be used to discriminate people. You won't see an ad because you're, for some reason, put in a high-risk category. We know that housing ads have not been shown to people of a certain ethnic minority, for example. So you can discriminate based on ethnicity. We know that job ads have not been shown to people living within a certain zip code or with a certain profile. We know that this data can be used to determine whether you're a risky consumer or not, so access to your - to credit can be a problem. And you won't even know because it will just be a computer giving you a price or denying you access to a service, and you won't know why. (SOUNDBITE OF MUSIC)RAZ: Every single one of us - right? - we download apps, and we go to certain pages. And oftentimes, we give consent without thinking about it for a variety of reasons - 'cause we're out of time, we're in a hurry, whatever it might be. But then these technology companies - their response is, well, listen; we warned you. We gave you all of the information. You made a decision to opt in. But that's actually such an infuriatingly disingenuous response. MYRSTAD: Yeah, I would agree to that because, for example - let's use Facebook as an example. It's the world's most popular social network. And if you want to stay in touch with your friends, for most people, that's how you do it - either through Facebook or Instagram, which is also owned by Facebook. RAZ: Sure. MYRSTAD: So we have to also keep in mind that most people have lots of other things to worry about. They have to go to work. They have to buy food that is safe and good, and they have to take their kids to football practice. So taking these very complicated decisions that could have detrimental effects in the long-term - in the short-term, we are not really equipped psychologically to take that into consideration. RAZ: I can't help but think that a capitalist model makes this an impossible problem to solve because data is so valuable. It is increasingly important to these companies' bottom lines and to their market capitalization. What - why would they possibly give this up? MYRSTAD: No. And probably they won't, unless there is actually external pressure. And I think that's probably where, you know, we the citizens can make a difference, to tell our decision-makers, our politicians that we care about this, that we don't want this to be an individual choice. And I think this should be regulated the same way we regulated the environment, the water, big oil companies, tobacco and all of these things. RAZ: I mean, Google and Facebook - this is their bread and butter. They control a huge percentage of the digital head market. How could they change their business model to, you know - to continue to make money if that's obviously their priority and their responsibility to their shareholders? MYRSTAD: Well, I think there is a discussion now whether you could, for example, serve contextual ads where you actually don't need to collect any data about the user, where, based on which website you're on, you will see ads that would be relevant to users of that website. I would happily use Facebook. And if they asked me every - beginning of every month or beginning of every year, what things are you interested in? What would you like to see of ads? And it was my choice to say, I'm interested in sports. I'm interested in news. And you don't need to track me to - in order to know that you - I could actually willingly be telling them that. RAZ: I mean, the companies argue, look; we're targeting people because it's better for their lives. It improves their lives. It's more efficient. It gives them opportunities to buy the things they need. We need this data to make - you know, to offer a better experience for consumers. They genuinely believe that what they're doing is for the greater good. MYRSTAD: Yeah, I've also heard that, and I also see that when I meet with these companies. It's a lot of really good and smart people working at these big tech companies. But what I feel is that they are not at all - or the corporate culture - the companies' structures are not open for scrutiny. They are not open for transparency. And they say, trust us; we will have your best interests in mind. And I think with all these privacy scandals, people are losing trust, and these companies are becoming increasingly more unpopular. But because they have such a strong grip on the markets - I would call them monopolies - it's really hard for consumers to vote with their feet because they have nowhere else to go. RAZ: That's Finn Myrstad. He's director of digital policy at the Norwegian Consumer Council. You can see his full talk at TED. com. GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about Digital Manipulation. (SOUNDBITE OF MUSIC) RAZ: All right, let me ask you about Cayla. Tell me, what is Cayla? Tell me about Cayla. FINN MYRSTAD: (Laughter) So Cayla, I would say, was an Internet-connected doll. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED SINGER: (Singing) Cayla knows millions of things. MYRSTAD: The children can talk to the doll. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED CHILD #1: How do you make a cake? COMPUTER-GENERATED VOICE: Mix eggs, flower, milk, butter. . . MYRSTAD: And the doll would use speech-recognition technology and answer the child's questions. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED CHILD #2: My Friend Cayla talks, listens to you, plays games with you and knows millions of things. RAZ: And because Cayla talks to you and listens to you and knows millions of things, Finn Myrstad, who is the director of digital policy at the Norwegian Consumer Council, wanted to see how easily Cayla could be hacked. MYRSTAD: We decided to investigate or look at this doll more closely, and what we discovered was that it had very little security. So anyone within Bluetooth range or within a certain distance could basically connect to the doll through their phones and initiate the two-way conversation. (SOUNDBITE OF TED TALK) UNIDENTIFIED PERSON #2: (As Cayla) Hi, my name is Cayla. What is yours? MYRSTAD: Finn. RAZ: On the TED stage, you do this. You bring out a colleague, who from, like, backstage is talking to you through Cayla and presumably could talk to a child in their bedroom. (SOUNDBITE OF TED TALK) UNIDENTIFIED PERSON #2: (As Cayla) Is your mom close by? MYRSTAD: No, she's in the store. UNIDENTIFIED PERSON #2: (As Cayla) You want to come out and play with me? MYRSTAD: That's a great idea. UNIDENTIFIED PERSON #2: (As Cayla) Oh, great. (SOUNDBITE OF MUSIC) RAZ: It's totally weird. MYRSTAD: Yeah, yeah. And it's completely, of course, unacceptable. And what made this story worse was that there was a label on the packaging saying that this was an Internet-safe doll while it really had no safety or security precautions in it whatsoever. (SOUNDBITE OF MUSIC) RAZ: Now, did Cayla, like, also listen in on the things people were telling her and then using that information? MYRSTAD: Well, we read the terms and conditions that no one reads, and in there, the company reserved the rights to use the voice recordings of the child and use it for targeted advertisement, for example. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED CHILD #2: Cayla knows millions of things. UNIDENTIFIED CHILD #1: How do you make a cake? MYRSTAD: But because we don't have any protections when it comes to data or when it comes to security, the market is flooded with insecure products and products which has a business model of selling our data. And that really decreases trust, and it leaves people completely apathetic to this. (SOUNDBITE OF MUSIC) RAZ: OK, so Cayla might sound like an extreme example of a tech company acting irresponsibly, but is it really? Is it an anomaly? Because when you unwrap Cayla and install the companion app, there is a user agreement. (SOUNDBITE OF TED TALK) MYRSTAD: So we read also the terms and conditions that no one reads. RAZ: The terms and conditions that no one reads. Finn Myrstad continues the story from the TED stage. (SOUNDBITE OF TED TALK) MYRSTAD: Like most of you, I have dozens of apps on my phone. If used properly, they can make our lives easier, more convenient and maybe even healthier. But have we been lulled into a false sense of security? It starts simply by ticking a box. Yes, we say, I've read the terms. But have you really read the terms? Are you sure they didn't look too long, and the last time you tried, they were impossible to understand, and you needed to use the service now? And now the power imbalance is established because we have agreed to our personal information being gathered and used on a scale we could never imagine. This is why my colleagues and I decided to take a deeper look at this. We set out to read the terms of popular apps on the average phone. And to show the world how unrealistic it is to expect consumers to actually read the terms, we printed them - more than 900 pages - and sat down in our office and read them out loud ourselves. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #3: Angry Birds, terms of service - the following terms of service and end-user license agreement. . . (SOUNDBITE OF TED TALK) MYRSTAD: Streaming the experiment live on our website, it took quite a long time. It took us 31 hours, 49 minutes and 11 seconds to read the terms on an average phone - that is longer than a movie marathon of the \"Harry Potter\" movies and \"The Godfather\" movies combined. And reading is one thing; understanding is another story - that would have taken us much, much longer. (SOUNDBITE OF MUSIC) RAZ: But this isn't just about, like, a creepy doll or, like, how incredibly frustrating and unrealistic it is to have to read hundreds of pages of user agreements. I mean, all of this data collected on us - I mean, companies can and do use it in some really, you know, sleazy ways, right? MYRSTAD: Yeah. And what we're seeing increasingly is that this data can also be used to discriminate people. You won't see an ad because you're, for some reason, put in a high-risk category. We know that housing ads have not been shown to people of a certain ethnic minority, for example. So you can discriminate based on ethnicity. We know that job ads have not been shown to people living within a certain zip code or with a certain profile. We know that this data can be used to determine whether you're a risky consumer or not, so access to your - to credit can be a problem. And you won't even know because it will just be a computer giving you a price or denying you access to a service, and you won't know why. (SOUNDBITE OF MUSIC) RAZ: Every single one of us - right? - we download apps, and we go to certain pages. And oftentimes, we give consent without thinking about it for a variety of reasons - 'cause we're out of time, we're in a hurry, whatever it might be. But then these technology companies - their response is, well, listen; we warned you. We gave you all of the information. You made a decision to opt in. But that's actually such an infuriatingly disingenuous response. MYRSTAD: Yeah, I would agree to that because, for example - let's use Facebook as an example. It's the world's most popular social network. And if you want to stay in touch with your friends, for most people, that's how you do it - either through Facebook or Instagram, which is also owned by Facebook. RAZ: Sure. MYRSTAD: So we have to also keep in mind that most people have lots of other things to worry about. They have to go to work. They have to buy food that is safe and good, and they have to take their kids to football practice. So taking these very complicated decisions that could have detrimental effects in the long-term - in the short-term, we are not really equipped psychologically to take that into consideration. RAZ: I can't help but think that a capitalist model makes this an impossible problem to solve because data is so valuable. It is increasingly important to these companies' bottom lines and to their market capitalization. What - why would they possibly give this up? MYRSTAD: No. And probably they won't, unless there is actually external pressure. And I think that's probably where, you know, we the citizens can make a difference, to tell our decision-makers, our politicians that we care about this, that we don't want this to be an individual choice. And I think this should be regulated the same way we regulated the environment, the water, big oil companies, tobacco and all of these things. RAZ: I mean, Google and Facebook - this is their bread and butter. They control a huge percentage of the digital head market. How could they change their business model to, you know - to continue to make money if that's obviously their priority and their responsibility to their shareholders? MYRSTAD: Well, I think there is a discussion now whether you could, for example, serve contextual ads where you actually don't need to collect any data about the user, where, based on which website you're on, you will see ads that would be relevant to users of that website. I would happily use Facebook. And if they asked me every - beginning of every month or beginning of every year, what things are you interested in? What would you like to see of ads? And it was my choice to say, I'm interested in sports. I'm interested in news. And you don't need to track me to - in order to know that you - I could actually willingly be telling them that. RAZ: I mean, the companies argue, look; we're targeting people because it's better for their lives. It improves their lives. It's more efficient. It gives them opportunities to buy the things they need. We need this data to make - you know, to offer a better experience for consumers. They genuinely believe that what they're doing is for the greater good. MYRSTAD: Yeah, I've also heard that, and I also see that when I meet with these companies. It's a lot of really good and smart people working at these big tech companies. But what I feel is that they are not at all - or the corporate culture - the companies' structures are not open for scrutiny. They are not open for transparency. And they say, trust us; we will have your best interests in mind. And I think with all these privacy scandals, people are losing trust, and these companies are becoming increasingly more unpopular. But because they have such a strong grip on the markets - I would call them monopolies - it's really hard for consumers to vote with their feet because they have nowhere else to go. RAZ: That's Finn Myrstad. He's director of digital policy at the Norwegian Consumer Council. You can see his full talk at TED. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-12-740792733": {"title": "Scott Galloway: Have We Let The Tech Giants Monopolize More Than The Economy?  : NPR", "url": "https://www.npr.org/2019/07/12/740792733/scott-galloway-have-we-let-the-tech-giants-monopolize-more-than-the-economy", "author": "No author found", "published_date": "2019-07-12", "content": "GUY RAZ, HOST: On the show today, ideas about Digital Manipulation. And as Finn Myrstad was just saying, we as consumers may not have a lot of choice when we're dealing with these massive companies. But just how massive are they? SCOTT GALLOWAY: Well, I'll give you some specific examples. In a 12-month period from June of '17 to, I think, June of '18, Amazon added to its market capitalization the value of the entire CPG industry. RAZ: This is Scott Galloway. GALLOWAY: Professor of marketing, NYU Stern School of Business. RAZ: And that phrase that he just used. . . GALLOWAY: The entire CPG industry. RAZ: . . . Means consumer packaged goods. So we're talking Nestle, Tyson Foods, Pepsi. . . GALLOWAY: So you could take every consumer product sold globally. . . RAZ: . . . Nike, Unilever, Procter and Gamble. . . GALLOWAY: When we're talking about companies - Unilever and PNG - that have been around for decades. . . RAZ: . . . 3M, L'Oreal, Kraft, Heinz. . . GALLOWAY: . . . Not generations. . . RAZ: . . . Adidas, General Mills, Hershey. . . GALLOWAY: . . . Not centuries. . . RAZ: . . . Mattel, Anheuser-Busch, Philip Morris. . . GALLOWAY: . . . That have unbelievably robust consumer brands, global reach, credible supply chain - and Amazon added the entire value of that industry in a 12-month period. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: Amazon is doing fine. UNIDENTIFIED REPORTER #1: May be headed for global dominance. UNIDENTIFIED REPORTER #2: The stock is now up 2%. UNIDENTIFIED REPORTER #3: They're becoming a profit machine. You are seeing their profits. . . (CROSSTALK)RAZ: And it's not just Amazon. In his TED talk, Scott focused on the three other behemoth companies that, with Amazon, make up the big four. They are Apple, Facebook and Google. So what does it mean that these companies are so big and so good at manipulating, well, almost every part of our lives? Here's Scott Galloway's take from the TED stage. (SOUNDBITE OF TED TALK)GALLOWAY: These four companies - at the end of the Great Recession, the market capitalization of these companies was equivalent to the GDP of Niger. Now it is equivalent to the GDP of India, having blown past Russia and Canada in '13 and '14. There are only five nations that have a GDP greater than the combined market capitalization of these four firms. Amazon has become so powerful in the marketplace; they can conduct Jedi mind tricks that can begin damaging other industries just by looking at it. Nike announces they're distributing on Amazon - their stock goes up; every other footwear stock goes down. When Amazon stock goes up, the rest of retail stocks go down because they assume what's good for Amazon is bad for everybody else. They cut the cost on salmon 33% when they acquired Whole Foods. In between the time they announced the acquisition of Whole Foods and when it closed, Kroger, the largest pure-play grocer in America, shed a third of its value because Amazon purchased a grocer one-eleventh the size of Kroger. (SOUNDBITE OF MUSIC)RAZ: When we talk about the four big companies - right? - let's put this into a historical context. Are we kind of living through a similar period in history where you had, like, the railroad barons and the oil barons and the finance barons - the Rockefellers, the Carnegies, the Crockers - is that the world we're living in today? GALLOWAY: Yeah, we've been to this movie before. And history may not repeat itself, but it rhymes, and this definitely rhymes with periods when companies have concentrated or through excellent execution, macroenvironment, luck - whatever you want to call it - become an invasive species; and that is, small companies have a difficult time getting out of the crib. Large companies are kind of euthanized prematurely, and large companies tend to be great employers and good taxpayers. What's different about this era is that we seem to have lost the script around antitrust. And that is, with the railroads, telcos, the aluminum companies, the seven sisters, the oil companies - we decided to move in and break them up and we had, quote, unquote, a \"class traitor\" (ph). And that is, a lot of people think the railroad money elected Teddy Roosevelt, a Republican, but he was willing to go in and say, I love you guys. Thanks for getting me in the office, but I'm now breaking you up because we need to oxygenate the economy. But we have been to these levels of concentration before. (SOUNDBITE OF TED TALK)GALLOWAY: Whose fault is it? It's our fault. We're electing regulators who don't have the backbone to actually go after these companies. Amazon only needs one person for two at Macy's. If they grow their business $20 billion this year, which they will, we will lose $53,000 cashiers and clerks. This is nothing unusual. This has happened all through our economy. We've just never seen companies this good at it. That's one Yankee Stadium of workers. It's even worse in media. Facebook and Google grow their business $22 billion this year, which they will, we're going to lose approximately 150,000 creative directors, planners and copywriters. Or we can fill up 2 1/2 Yankee Stadiums and say, you are out of work, courtesy of Amazon. (SOUNDBITE OF MUSIC)RAZ: When it comes to Google or Amazon or Facebook or even Apple, how much do we control our interaction with those platforms and how much are we controlled by our interaction with those platforms? GALLOWAY: So look - every time you light up a cigarette, you're making a conscious choice. But the question is, at some point, do you become so biologically addicted to nicotine, does the organization require scrutiny around what it's doing to addict you? And power corrupts, and when you're a monopoly and you're Facebook, and the worse that's going to happen to you is a fine that's equivalent of seven days' income or seven weeks of cash flow and maybe some public shaming, you're always going to find a reason not to figure it out. The NRA has never been able to connect the sale of assault weapons with the murder of children. Big Tobacco was never able to connect tobacco with cancer. Big tech is never going to connect the underlying algorithms of their companies and the lack of security and the lack of human discretion and screening of their content with the division or serious division in our culture, the weaponization of these platforms to clear (ph) our elections or teen depression. They will never make that connection on their own because when it's raining money, it blurs your vision. RAZ: See, I think the thing about these big companies that really worries me is how much I feel manipulated by them. And I don't - and it's not that I want to blame them because I am making choices. I am deciding to use these platforms and to check on Facebook or Twitter or buying things on Amazon because it's easy, you know. Like, that's the part of it that really bothers me, more than the sort of the economic cost of it, which is obviously a problem; it's the personal cost. It's how it sort of changes and has changed our behavior. GALLOWAY: Hundred percent. And I would argue that if you're going to look at price, at some point, if we end up with a group of people who can all have Netflix, all have a great phone, all have tons of content and get paper towels delivered really inexpensively, but we're - our wages are flat for five decades, OK, do we need to reevaluate priorities? I would argue that the pricing or the prices we pay for these platforms has skyrocketed. And that is, when we have a country where a lot of us wonder if the elections were manipulated, that's an enormous price we all pay. When we worry about our 16-year-old daughters sitting in their room, knowing they weren't invited to a party, that's one thing. But seeing the party play out in real time, on their phones, as they're in their rooms alone, that's a price that my household pays. I'm worried that my son, my oldest son who's 11 and does a handstand, and I video it, and he asks me to post it on YouTube, and then he gets a like, and he says, can we check back in and see if I got more likes? And then someone makes kind of a snarky comment about his video, and it really bums him out. And Google responds, well, yeah, but your son can learn how to play the piano on YouTube. RAZ: Yeah. GALLOWAY: There's some wonderful things about social media, but the addiction here - we know we're being manipulated. That's an enormous price we all pay. (SOUNDBITE OF MUSIC)RAZ: That's Scott Galloway, professor at NYU's Stern School of Business. You can watch his full talk at ted. com. On the show today, ideas about Digital Manipulation. Stay with us. I'm Guy Raz. And you're listening to the TED Radio Hour from NPR. GUY RAZ, HOST:  On the show today, ideas about Digital Manipulation. And as Finn Myrstad was just saying, we as consumers may not have a lot of choice when we're dealing with these massive companies. But just how massive are they? SCOTT GALLOWAY: Well, I'll give you some specific examples. In a 12-month period from June of '17 to, I think, June of '18, Amazon added to its market capitalization the value of the entire CPG industry. RAZ: This is Scott Galloway. GALLOWAY: Professor of marketing, NYU Stern School of Business. RAZ: And that phrase that he just used. . . GALLOWAY: The entire CPG industry. RAZ: . . . Means consumer packaged goods. So we're talking Nestle, Tyson Foods, Pepsi. . . GALLOWAY: So you could take every consumer product sold globally. . . RAZ: . . . Nike, Unilever, Procter and Gamble. . . GALLOWAY: When we're talking about companies - Unilever and PNG - that have been around for decades. . . RAZ: . . . 3M, L'Oreal, Kraft, Heinz. . . GALLOWAY: . . . Not generations. . . RAZ: . . . Adidas, General Mills, Hershey. . . GALLOWAY: . . . Not centuries. . . RAZ: . . . Mattel, Anheuser-Busch, Philip Morris. . . GALLOWAY: . . . That have unbelievably robust consumer brands, global reach, credible supply chain - and Amazon added the entire value of that industry in a 12-month period. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: Amazon is doing fine. UNIDENTIFIED REPORTER #1: May be headed for global dominance. UNIDENTIFIED REPORTER #2: The stock is now up 2%. UNIDENTIFIED REPORTER #3: They're becoming a profit machine. You are seeing their profits. . . (CROSSTALK) RAZ: And it's not just Amazon. In his TED talk, Scott focused on the three other behemoth companies that, with Amazon, make up the big four. They are Apple, Facebook and Google. So what does it mean that these companies are so big and so good at manipulating, well, almost every part of our lives? Here's Scott Galloway's take from the TED stage. (SOUNDBITE OF TED TALK) GALLOWAY: These four companies - at the end of the Great Recession, the market capitalization of these companies was equivalent to the GDP of Niger. Now it is equivalent to the GDP of India, having blown past Russia and Canada in '13 and '14. There are only five nations that have a GDP greater than the combined market capitalization of these four firms. Amazon has become so powerful in the marketplace; they can conduct Jedi mind tricks that can begin damaging other industries just by looking at it. Nike announces they're distributing on Amazon - their stock goes up; every other footwear stock goes down. When Amazon stock goes up, the rest of retail stocks go down because they assume what's good for Amazon is bad for everybody else. They cut the cost on salmon 33% when they acquired Whole Foods. In between the time they announced the acquisition of Whole Foods and when it closed, Kroger, the largest pure-play grocer in America, shed a third of its value because Amazon purchased a grocer one-eleventh the size of Kroger. (SOUNDBITE OF MUSIC) RAZ: When we talk about the four big companies - right? - let's put this into a historical context. Are we kind of living through a similar period in history where you had, like, the railroad barons and the oil barons and the finance barons - the Rockefellers, the Carnegies, the Crockers - is that the world we're living in today? GALLOWAY: Yeah, we've been to this movie before. And history may not repeat itself, but it rhymes, and this definitely rhymes with periods when companies have concentrated or through excellent execution, macroenvironment, luck - whatever you want to call it - become an invasive species; and that is, small companies have a difficult time getting out of the crib. Large companies are kind of euthanized prematurely, and large companies tend to be great employers and good taxpayers. What's different about this era is that we seem to have lost the script around antitrust. And that is, with the railroads, telcos, the aluminum companies, the seven sisters, the oil companies - we decided to move in and break them up and we had, quote, unquote, a \"class traitor\" (ph). And that is, a lot of people think the railroad money elected Teddy Roosevelt, a Republican, but he was willing to go in and say, I love you guys. Thanks for getting me in the office, but I'm now breaking you up because we need to oxygenate the economy. But we have been to these levels of concentration before. (SOUNDBITE OF TED TALK) GALLOWAY: Whose fault is it? It's our fault. We're electing regulators who don't have the backbone to actually go after these companies. Amazon only needs one person for two at Macy's. If they grow their business $20 billion this year, which they will, we will lose $53,000 cashiers and clerks. This is nothing unusual. This has happened all through our economy. We've just never seen companies this good at it. That's one Yankee Stadium of workers. It's even worse in media. Facebook and Google grow their business $22 billion this year, which they will, we're going to lose approximately 150,000 creative directors, planners and copywriters. Or we can fill up 2 1/2 Yankee Stadiums and say, you are out of work, courtesy of Amazon. (SOUNDBITE OF MUSIC) RAZ: When it comes to Google or Amazon or Facebook or even Apple, how much do we control our interaction with those platforms and how much are we controlled by our interaction with those platforms? GALLOWAY: So look - every time you light up a cigarette, you're making a conscious choice. But the question is, at some point, do you become so biologically addicted to nicotine, does the organization require scrutiny around what it's doing to addict you? And power corrupts, and when you're a monopoly and you're Facebook, and the worse that's going to happen to you is a fine that's equivalent of seven days' income or seven weeks of cash flow and maybe some public shaming, you're always going to find a reason not to figure it out. The NRA has never been able to connect the sale of assault weapons with the murder of children. Big Tobacco was never able to connect tobacco with cancer. Big tech is never going to connect the underlying algorithms of their companies and the lack of security and the lack of human discretion and screening of their content with the division or serious division in our culture, the weaponization of these platforms to clear (ph) our elections or teen depression. They will never make that connection on their own because when it's raining money, it blurs your vision. RAZ: See, I think the thing about these big companies that really worries me is how much I feel manipulated by them. And I don't - and it's not that I want to blame them because I am making choices. I am deciding to use these platforms and to check on Facebook or Twitter or buying things on Amazon because it's easy, you know. Like, that's the part of it that really bothers me, more than the sort of the economic cost of it, which is obviously a problem; it's the personal cost. It's how it sort of changes and has changed our behavior. GALLOWAY: Hundred percent. And I would argue that if you're going to look at price, at some point, if we end up with a group of people who can all have Netflix, all have a great phone, all have tons of content and get paper towels delivered really inexpensively, but we're - our wages are flat for five decades, OK, do we need to reevaluate priorities? I would argue that the pricing or the prices we pay for these platforms has skyrocketed. And that is, when we have a country where a lot of us wonder if the elections were manipulated, that's an enormous price we all pay. When we worry about our 16-year-old daughters sitting in their room, knowing they weren't invited to a party, that's one thing. But seeing the party play out in real time, on their phones, as they're in their rooms alone, that's a price that my household pays. I'm worried that my son, my oldest son who's 11 and does a handstand, and I video it, and he asks me to post it on YouTube, and then he gets a like, and he says, can we check back in and see if I got more likes? And then someone makes kind of a snarky comment about his video, and it really bums him out. And Google responds, well, yeah, but your son can learn how to play the piano on YouTube. RAZ: Yeah. GALLOWAY: There's some wonderful things about social media, but the addiction here - we know we're being manipulated. That's an enormous price we all pay. (SOUNDBITE OF MUSIC) RAZ: That's Scott Galloway, professor at NYU's Stern School of Business. You can watch his full talk at ted. com. On the show today, ideas about Digital Manipulation. Stay with us. I'm Guy Raz. And you're listening to the TED Radio Hour from NPR.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-12-740771021": {"title": "Carole Cadwalladr: How Did Social Media Manipulate Our Votes And Our Elections? : NPR", "url": "https://www.npr.org/2019/07/12/740771021/carole-cadwalladr-how-did-social-media-manipulate-our-votes-and-our-elections", "author": "No author found", "published_date": "2019-07-12", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. I'm Guy Raz. So on the morning of June 24, 2016, British citizens woke up stunned - almost no one, and certainly none of the polls, predicted the majority of them would support Brexit. (SOUNDBITE OF MONTAGE)UNIDENTIFIED PERSON #1: How do you feel about Britain leaving the EU? UNIDENTIFIED PERSON #2: I'm quite shocked, to be honest. I thought we all would've stayed in. UNIDENTIFIED PERSON #3: Yeah, I didn't think anyone really believed that U. K. voters would decide to Brexit. UNIDENTIFIED PERSON #4: And I feel quite shocked by the result. UNIDENTIFIED PERSON #5: To be honest, I was quite shocked. UNIDENTIFIED PERSON #6: I feel quite scared today. I don't know how I feel. RAZ: I remember that morning. I happened to be in London. And the question a lot of people were asking was why? Why did it happen? And why did almost no one predict it? And these were also the questions journalist Carole Cadwalladr asked when she went to a small town in Wales. CAROLE CADWALLADR: It was years since I'd been to Ebbw Vale. It's a very historic, very, you know, real Labour, left-wing heartland. RAZ: That town, Ebbw Vale, voted overwhelmingly in favor of leaving the EU. And Carole wanted to understand why. CADWALLADR: And I went down this lower part of the town, which is where the steel plant used to be. And it was the biggest steel plant in the world, right up until - it was the '80s, I think. And you just had no idea now. It looked like (laughter) - you know, it was a kind of little Manhattan down there - these sort of incredible architecture-designed glass and steel buildings. And all around, these signs saying, paid for by the European Union. (SOUNDBITE OF MUSIC)RAZ: Carole Cadwalladr picks up the story from the TED stage. (SOUNDBITE OF TED TALK)CADWALLADR: I had this sort of weird sense of unreality walking around the town, and it came to a head when I met this young man in front of the sports center. And he told me that he had voted to leave because the European Union had done nothing for him - he was fed up with it. And all around town, people told me the same thing. And they told me that they were most fed up with the immigrants and with the refugees - they'd had enough. Which was odd because, walking around, I didn't meet any immigrants or refugees. And when I checked the figures, I discovered that Ebbw Vale actually has one of the lowest rates of immigration in the country. And so I was just a bit baffled because I couldn't really understand where people were getting their information from. But then after the article came out, this woman got in touch with me. And she was from Ebbw Vale, and she told me about all this stuff that she'd seen on Facebook. And I was like, what stuff? And she said it was all this quite scary stuff about immigration, especially about Turkey. So I tried to find it, but there was nothing there because there's no archive of ads that people see or what had been pushed into their news feeds. No trace of anything - it'd gone completely dark. Because only you see your news feed, and then it vanishes, so it's impossible to research anything. So we have no idea who saw what ads or what impact they had or what data was used to target these people or even who placed the ads or how much money was spent or even what nationality they were - but Facebook does. Facebook has these answers, and it's refused to give them to us. Our Parliament has asked Mark Zuckerberg multiple times to come to Britain and to give us these answers, and every single time, he's refused. And you have to wonder why. Because what I and other journalists have uncovered is that multiple crimes took place during the referendum, and they took place on Facebook. It's because in Britain we limit the amount of money that you can spend in an election. And it's because in the 19th century, people would walk around with, like, literally wheelbarrows of cash and just buy voters. So we passed these strict laws to stop that from happening. But those laws don't work anymore. This referendum took place almost entirely online. And you can spend any amount of money on Facebook or on Google or on YouTube ads, and nobody will know because they're black boxes, and this is what happened before the Brexit vote. We are what happens to a Western democracy when 100 years of electoral laws are disrupted by technology. (SOUNDBITE OF MUSIC)RAZ: And it's that technology that got Carole wondering - how many people might have been misled, even manipulated, into voting a certain way because of Facebook ads, ads that were designed to trigger certain emotions among voters? So Carole started to dig deeper, and over the next two years, she'd come to a very worrying conclusion about technology companies like Facebook and Twitter and even Google. CADWALLADR: You know, this is a massive, global online experiment going on right now, and it's being conducted by some of the - if not the smartest people in the world, who are being employed, paid huge salaries, to come up with new and novel ways to hook us and addict us and draw us in and make us click. I mean, it's old-fashioned corporate greed. These are highly motivated billionaires (laughter) hiring, you know, the smartest people to find new ways of manipulating us. (SOUNDBITE OF MUSIC)RAZ: Every day, every second, trillions of bits of data about you, me, everyone you know, are being collected every time we click a link or even scroll through a social media news feed. And all of that information allows some of the biggest technology companies to build a highly detailed profile of who you are, what triggers you, how to hook you in and, ultimately, how your behavior can be shifted. It's very possible that who we vote for, what we buy, what we believe, even what we see with our own eyes, are more susceptible to manipulation today than ever before in modern history. So on the show today, we're going to explore ideas around the power of digital technology to manipulate our decisions and ways to prevent it from becoming an even bigger problem. CADWALLADR: This is a really profound revolution in the way that we consume information. And I think it's really difficult to understand this massive historical change when you're in the middle of it, when you're - you can't see it. It's - you know, it is like the sun - it's too big to see. (SOUNDBITE OF TED TALK)CADWALLADR: I don't have to tell you that hate and fear are being sown online all across the world. But we only see a tiny amount of what's going on on the surface. And I only found out anything about this dark underbelly because I started looking into a company called Cambridge Analytica. And I spent months tracking down an ex-employee, Christopher Wiley. And he told me how this company that worked for both Trump and Brexit had profiled people politically in order to understand their individual fears, to better target them with Facebook ads. And it did this by illicitly harvesting the profiles of 87 million people from Facebook. The company is owned by Robert Mercer, the billionaire who bankrolled Trump. And he threatened to sue us multiple times to stop us from publishing. But we finally got there, and we were one day ahead of publication - we got another legal threat; not from Cambridge Analytica occur this time, but from Facebook. It told us that if we publish, they would sue us - we did it anyway. (APPLAUSE)CADWALLADR: Facebook, you are on the wrong side of history in that, and you are on the wrong side of history in this - in refusing to give us the answers that we need. And that is why I am here to address you directly, the gods of Silicon Valley. (APPLAUSE)CADWALLADR: Mark Zuckerberg and Sheryl Sandberg and Larry Page and Sergey Brin and Jack Dorsey and your employees and your investors, too - this technology that you have invented has been amazing, but now it's a crime scene, and you have the evidence. And it is not enough to say that you will do better in the future. Because to have any hope of stopping this from happening again, we have to know the truth. (SOUNDBITE OF MUSIC)RAZ: Carole, if you had a chance to sit down with Mark Zuckerberg or some of these other founders and they had to answer your questions, what is it that you would want to know from them? Like, what are the things that they would need to say? CADWALLADR: I want to know how they can live with this, how they can not be taking an ax to, you know, what is currently going on internally. RAZ: Yeah. CADWALLADR: I want to sort of see that they recognize on a human level what is going - because they show no sign of it. The other thing about it is, is that, you know, in my TED Talk, I showed these advertisements which had been on Facebook. RAZ: Yeah. CADWALLADR: We got those after a sort of battle royal the - our Parliament had with Facebook, and then they eventually handed some of them over. But in the States, what is remarkable is you know even less about what happened in your presidential election than we do. And it is - the scale of what happened in the U. S. is so much bigger, so much more money which was spent, so much more sophisticated modelling - we know nothing about it. And, you know, and meanwhile, the U. S. is careering towards the next election. (SOUNDBITE OF TED TALK)CADWALLADR: This is not democracy, spreading lies in darkness; it's subversion. And it is not about left or right or leave or remain or Trump or not; it's about whether it's actually possible to have a free and fair election ever again. Because as it stands, I don't think it is. And so my question to you is, is this what you want? Facebook, is this how you want history to remember you, as the handmaidens to authoritarianism that is on the rise all across the world? Because you set out to connect people, and you are refusing to acknowledge that the same technology is now driving us apart. And my question to everybody else is, is this what we want, to let them get away with it and to sit back and play with our phones, as this darkness falls? Democracy is not guaranteed, and it is not inevitable, and we have to fight, and we have to win, and we cannot let these tech companies have this unchecked power. It's up to us - you, me and all of us. We are the ones who have to take back control. (APPLAUSE)RAZ: That's Carole Cadwalladr. She writes for The Guardian and The Observer. You can find her full talk at ted. com. On the show today, ideas about Digital Manipulation. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. So on the morning of June 24, 2016, British citizens woke up stunned - almost no one, and certainly none of the polls, predicted the majority of them would support Brexit. (SOUNDBITE OF MONTAGE) UNIDENTIFIED PERSON #1: How do you feel about Britain leaving the EU? UNIDENTIFIED PERSON #2: I'm quite shocked, to be honest. I thought we all would've stayed in. UNIDENTIFIED PERSON #3: Yeah, I didn't think anyone really believed that U. K. voters would decide to Brexit. UNIDENTIFIED PERSON #4: And I feel quite shocked by the result. UNIDENTIFIED PERSON #5: To be honest, I was quite shocked. UNIDENTIFIED PERSON #6: I feel quite scared today. I don't know how I feel. RAZ: I remember that morning. I happened to be in London. And the question a lot of people were asking was why? Why did it happen? And why did almost no one predict it? And these were also the questions journalist Carole Cadwalladr asked when she went to a small town in Wales. CAROLE CADWALLADR: It was years since I'd been to Ebbw Vale. It's a very historic, very, you know, real Labour, left-wing heartland. RAZ: That town, Ebbw Vale, voted overwhelmingly in favor of leaving the EU. And Carole wanted to understand why. CADWALLADR: And I went down this lower part of the town, which is where the steel plant used to be. And it was the biggest steel plant in the world, right up until - it was the '80s, I think. And you just had no idea now. It looked like (laughter) - you know, it was a kind of little Manhattan down there - these sort of incredible architecture-designed glass and steel buildings. And all around, these signs saying, paid for by the European Union. (SOUNDBITE OF MUSIC) RAZ: Carole Cadwalladr picks up the story from the TED stage. (SOUNDBITE OF TED TALK) CADWALLADR: I had this sort of weird sense of unreality walking around the town, and it came to a head when I met this young man in front of the sports center. And he told me that he had voted to leave because the European Union had done nothing for him - he was fed up with it. And all around town, people told me the same thing. And they told me that they were most fed up with the immigrants and with the refugees - they'd had enough. Which was odd because, walking around, I didn't meet any immigrants or refugees. And when I checked the figures, I discovered that Ebbw Vale actually has one of the lowest rates of immigration in the country. And so I was just a bit baffled because I couldn't really understand where people were getting their information from. But then after the article came out, this woman got in touch with me. And she was from Ebbw Vale, and she told me about all this stuff that she'd seen on Facebook. And I was like, what stuff? And she said it was all this quite scary stuff about immigration, especially about Turkey. So I tried to find it, but there was nothing there because there's no archive of ads that people see or what had been pushed into their news feeds. No trace of anything - it'd gone completely dark. Because only you see your news feed, and then it vanishes, so it's impossible to research anything. So we have no idea who saw what ads or what impact they had or what data was used to target these people or even who placed the ads or how much money was spent or even what nationality they were - but Facebook does. Facebook has these answers, and it's refused to give them to us. Our Parliament has asked Mark Zuckerberg multiple times to come to Britain and to give us these answers, and every single time, he's refused. And you have to wonder why. Because what I and other journalists have uncovered is that multiple crimes took place during the referendum, and they took place on Facebook. It's because in Britain we limit the amount of money that you can spend in an election. And it's because in the 19th century, people would walk around with, like, literally wheelbarrows of cash and just buy voters. So we passed these strict laws to stop that from happening. But those laws don't work anymore. This referendum took place almost entirely online. And you can spend any amount of money on Facebook or on Google or on YouTube ads, and nobody will know because they're black boxes, and this is what happened before the Brexit vote. We are what happens to a Western democracy when 100 years of electoral laws are disrupted by technology. (SOUNDBITE OF MUSIC) RAZ: And it's that technology that got Carole wondering - how many people might have been misled, even manipulated, into voting a certain way because of Facebook ads, ads that were designed to trigger certain emotions among voters? So Carole started to dig deeper, and over the next two years, she'd come to a very worrying conclusion about technology companies like Facebook and Twitter and even Google. CADWALLADR: You know, this is a massive, global online experiment going on right now, and it's being conducted by some of the - if not the smartest people in the world, who are being employed, paid huge salaries, to come up with new and novel ways to hook us and addict us and draw us in and make us click. I mean, it's old-fashioned corporate greed. These are highly motivated billionaires (laughter) hiring, you know, the smartest people to find new ways of manipulating us. (SOUNDBITE OF MUSIC) RAZ: Every day, every second, trillions of bits of data about you, me, everyone you know, are being collected every time we click a link or even scroll through a social media news feed. And all of that information allows some of the biggest technology companies to build a highly detailed profile of who you are, what triggers you, how to hook you in and, ultimately, how your behavior can be shifted. It's very possible that who we vote for, what we buy, what we believe, even what we see with our own eyes, are more susceptible to manipulation today than ever before in modern history. So on the show today, we're going to explore ideas around the power of digital technology to manipulate our decisions and ways to prevent it from becoming an even bigger problem. CADWALLADR: This is a really profound revolution in the way that we consume information. And I think it's really difficult to understand this massive historical change when you're in the middle of it, when you're - you can't see it. It's - you know, it is like the sun - it's too big to see. (SOUNDBITE OF TED TALK) CADWALLADR: I don't have to tell you that hate and fear are being sown online all across the world. But we only see a tiny amount of what's going on on the surface. And I only found out anything about this dark underbelly because I started looking into a company called Cambridge Analytica. And I spent months tracking down an ex-employee, Christopher Wiley. And he told me how this company that worked for both Trump and Brexit had profiled people politically in order to understand their individual fears, to better target them with Facebook ads. And it did this by illicitly harvesting the profiles of 87 million people from Facebook. The company is owned by Robert Mercer, the billionaire who bankrolled Trump. And he threatened to sue us multiple times to stop us from publishing. But we finally got there, and we were one day ahead of publication - we got another legal threat; not from Cambridge Analytica occur this time, but from Facebook. It told us that if we publish, they would sue us - we did it anyway. (APPLAUSE) CADWALLADR: Facebook, you are on the wrong side of history in that, and you are on the wrong side of history in this - in refusing to give us the answers that we need. And that is why I am here to address you directly, the gods of Silicon Valley. (APPLAUSE) CADWALLADR: Mark Zuckerberg and Sheryl Sandberg and Larry Page and Sergey Brin and Jack Dorsey and your employees and your investors, too - this technology that you have invented has been amazing, but now it's a crime scene, and you have the evidence. And it is not enough to say that you will do better in the future. Because to have any hope of stopping this from happening again, we have to know the truth. (SOUNDBITE OF MUSIC) RAZ: Carole, if you had a chance to sit down with Mark Zuckerberg or some of these other founders and they had to answer your questions, what is it that you would want to know from them? Like, what are the things that they would need to say? CADWALLADR: I want to know how they can live with this, how they can not be taking an ax to, you know, what is currently going on internally. RAZ: Yeah. CADWALLADR: I want to sort of see that they recognize on a human level what is going - because they show no sign of it. The other thing about it is, is that, you know, in my TED Talk, I showed these advertisements which had been on Facebook. RAZ: Yeah. CADWALLADR: We got those after a sort of battle royal the - our Parliament had with Facebook, and then they eventually handed some of them over. But in the States, what is remarkable is you know even less about what happened in your presidential election than we do. And it is - the scale of what happened in the U. S. is so much bigger, so much more money which was spent, so much more sophisticated modelling - we know nothing about it. And, you know, and meanwhile, the U. S. is careering towards the next election. (SOUNDBITE OF TED TALK) CADWALLADR: This is not democracy, spreading lies in darkness; it's subversion. And it is not about left or right or leave or remain or Trump or not; it's about whether it's actually possible to have a free and fair election ever again. Because as it stands, I don't think it is. And so my question to you is, is this what you want? Facebook, is this how you want history to remember you, as the handmaidens to authoritarianism that is on the rise all across the world? Because you set out to connect people, and you are refusing to acknowledge that the same technology is now driving us apart. And my question to everybody else is, is this what we want, to let them get away with it and to sit back and play with our phones, as this darkness falls? Democracy is not guaranteed, and it is not inevitable, and we have to fight, and we have to win, and we cannot let these tech companies have this unchecked power. It's up to us - you, me and all of us. We are the ones who have to take back control. (APPLAUSE) RAZ: That's Carole Cadwalladr. She writes for The Guardian and The Observer. You can find her full talk at ted. com. On the show today, ideas about Digital Manipulation. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-14-741568256": {"title": "Placing Facebook's $5 Billion Fine In Context : NPR", "url": "https://www.npr.org/2019/07/14/741568256/placing-facebooks-5-billion-fine-in-context", "author": "No author found", "published_date": "2019-07-14", "content": "LULU GARCIA-NAVARRO, HOST: Is $5 billion a lot of money? That's how much Facebook will reportedly have to pay after settling with the Federal Trade Commission this past week. And it's a record fine for the agency. But our next guest calls it a, quote, \"embarrassing joke. \" Nilay Patel is editor-in-chief of the tech site The Verge, and he joins us now. Welcome. NILAY PATEL: Thank you for having me. GARCIA-NAVARRO: Can you start by telling us what this fine is actually for? What did Facebook do wrong? PATEL: Well, so the FTC started looking at Facebook again after the Cambridge Analytica scandal broke. You might have noticed, if you've been paying attention, Facebook has had an escalating series of scandals for over a year now. And so, really, it comes down to - Facebook said it was going to keep personal information private and didn't. And the root of it was, obviously, Cambridge Analytica, but it spread out from there. GARCIA-NAVARRO: So you wrote in response to this, Facebook gets away with it again. Explain. PATEL: So Facebook was already under a consent decree with the FTC stemming from 2011. That was a 20-year consent decree and settlement for the exact same thing. If you look at the press release the FTC put out then, the first line is, Facebook said it was going to keep user information private and then repeatedly broke that promise. So Facebook already had broken this rule, had already paid a fine to the FTC, had already promised it was going to do a better job and get user consent before sharing data. And here we are again. It's been a year and a half of scandals. And Facebook is going to pay this fine. Now, $5 billion is a lot of money, but it's really, you know, somewhere between two weeks and a month of Facebook's revenue. It's a quarter of its yearly profit, which is a lot of money, but that was already priced into the stock. And you can see investors are reacting to this fine by saying, hey. That's great. It's done. It's over. The stock went up. And I think it is pretty embarrassing for the United States government to levy a fine - the biggest fine by two orders of magnitude the FTC has ever levied - and for Mark Zuckerberg's net worth to go up. And that is, ultimately, what happened. GARCIA-NAVARRO: Facebook has been heavily criticized for years now. Why do you think the FTC hasn't dealt a more punishing blow to the company? PATEL: That is really unclear. I think it is stunning that the largest fine in FTC history was a 3-2 vote. It was not a unanimous vote. The three Republican commissioners voted for the fine, and the two Democrats voted against it. And the Democrats uniformly wanted something stronger to happen, whether that was holding Mark Zuckerberg and the rest of the executives personally liable for the problems that they've been having, whether it's something stronger in terms of ongoing compliance and regulation of the company and what they're allowed to do or whether it's something quite as far as breaking up the whole company. That's, obviously, the most extreme thing you can do, but there's a huge spectrum between pay us some money, do some legal compliance work and breaking up the company. And somewhere in that spectrum, I think, is where the Democratic commissioners wanted to land. GARCIA-NAVARRO: So looking ahead, I mean, what do you think is going to happen? - because there is a lot of scrutiny. There is a lot of discussion on both sides of the aisle about Facebook - and among the general population. PATEL: I think you can already see Democrats in both the House and the Senate calling for Congress to act. Mark Warner, Richard Blumenthal, David Cicilline and the House - they're all saying, hey. The FTC has just completely blown it here. This is not what we wanted. This is a slap on the wrist for Facebook. They're just going to keep doing whatever they want. We need to take some action. Whether the action looks like a privacy law - which is something, again, that people have been talking about for a long time. Now, you know, we're a year into GDPR in Europe. We have a sense of how it works and how it doesn't work. There's a lot of interest in, can we do something like that over here? Then you have Elizabeth Warren, who, last night, just said, look. Facebook is too big to oversee. This is what I mean. Five billion dollars is a lot of money, but it's not going to stop a company the size of Facebook. It's not going to stop companies the size of Google. We should just break up the company and make these things easier to regulate. I think that's a really interesting conversation, and the volume of that conversation is getting louder every day. GARCIA-NAVARRO: Nilay Patel, editor-in-chief of The Verge, thank you very much. PATEL: Thank you so much for having me. (SOUNDBITE OF MUSIC) LULU GARCIA-NAVARRO, HOST:  Is $5 billion a lot of money? That's how much Facebook will reportedly have to pay after settling with the Federal Trade Commission this past week. And it's a record fine for the agency. But our next guest calls it a, quote, \"embarrassing joke. \" Nilay Patel is editor-in-chief of the tech site The Verge, and he joins us now. Welcome. NILAY PATEL: Thank you for having me. GARCIA-NAVARRO: Can you start by telling us what this fine is actually for? What did Facebook do wrong? PATEL: Well, so the FTC started looking at Facebook again after the Cambridge Analytica scandal broke. You might have noticed, if you've been paying attention, Facebook has had an escalating series of scandals for over a year now. And so, really, it comes down to - Facebook said it was going to keep personal information private and didn't. And the root of it was, obviously, Cambridge Analytica, but it spread out from there. GARCIA-NAVARRO: So you wrote in response to this, Facebook gets away with it again. Explain. PATEL: So Facebook was already under a consent decree with the FTC stemming from 2011. That was a 20-year consent decree and settlement for the exact same thing. If you look at the press release the FTC put out then, the first line is, Facebook said it was going to keep user information private and then repeatedly broke that promise. So Facebook already had broken this rule, had already paid a fine to the FTC, had already promised it was going to do a better job and get user consent before sharing data. And here we are again. It's been a year and a half of scandals. And Facebook is going to pay this fine. Now, $5 billion is a lot of money, but it's really, you know, somewhere between two weeks and a month of Facebook's revenue. It's a quarter of its yearly profit, which is a lot of money, but that was already priced into the stock. And you can see investors are reacting to this fine by saying, hey. That's great. It's done. It's over. The stock went up. And I think it is pretty embarrassing for the United States government to levy a fine - the biggest fine by two orders of magnitude the FTC has ever levied - and for Mark Zuckerberg's net worth to go up. And that is, ultimately, what happened. GARCIA-NAVARRO: Facebook has been heavily criticized for years now. Why do you think the FTC hasn't dealt a more punishing blow to the company? PATEL: That is really unclear. I think it is stunning that the largest fine in FTC history was a 3-2 vote. It was not a unanimous vote. The three Republican commissioners voted for the fine, and the two Democrats voted against it. And the Democrats uniformly wanted something stronger to happen, whether that was holding Mark Zuckerberg and the rest of the executives personally liable for the problems that they've been having, whether it's something stronger in terms of ongoing compliance and regulation of the company and what they're allowed to do or whether it's something quite as far as breaking up the whole company. That's, obviously, the most extreme thing you can do, but there's a huge spectrum between pay us some money, do some legal compliance work and breaking up the company. And somewhere in that spectrum, I think, is where the Democratic commissioners wanted to land. GARCIA-NAVARRO: So looking ahead, I mean, what do you think is going to happen? - because there is a lot of scrutiny. There is a lot of discussion on both sides of the aisle about Facebook - and among the general population. PATEL: I think you can already see Democrats in both the House and the Senate calling for Congress to act. Mark Warner, Richard Blumenthal, David Cicilline and the House - they're all saying, hey. The FTC has just completely blown it here. This is not what we wanted. This is a slap on the wrist for Facebook. They're just going to keep doing whatever they want. We need to take some action. Whether the action looks like a privacy law - which is something, again, that people have been talking about for a long time. Now, you know, we're a year into GDPR in Europe. We have a sense of how it works and how it doesn't work. There's a lot of interest in, can we do something like that over here? Then you have Elizabeth Warren, who, last night, just said, look. Facebook is too big to oversee. This is what I mean. Five billion dollars is a lot of money, but it's not going to stop a company the size of Facebook. It's not going to stop companies the size of Google. We should just break up the company and make these things easier to regulate. I think that's a really interesting conversation, and the volume of that conversation is getting louder every day. GARCIA-NAVARRO: Nilay Patel, editor-in-chief of The Verge, thank you very much. PATEL: Thank you so much for having me. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-15-741827437": {"title": "What Our Voices Say About Us  : NPR", "url": "https://www.npr.org/2019/07/15/741827437/finding-your-voice-how-the-way-we-sound-shapes-our-identities", "author": "No author found", "published_date": "2019-07-15", "content": "SHANKAR VEDANTAM, HOST: This is HIDDEN BRAIN. I'm Shankar Vedantam. At some point in our lives, many of us realize that the way we hear our own voice isn't the way others hear our voice. Shay (ph) had that realization as a child helping out at the family business, a deli in south West Virginia. SHAY: We did a lot of business with the deli, a lot of call-in orders, and I had to answer the phone with the name of the business. VEDANTAM: Often when the phone rang, the same thing happened over and over again. SHAY: I vividly remember always being confused for my mother. They would always say, hey Judy (ph), you know, and either start into their questions or whatever they were looking to speak to my mother about. VEDANTAM: This case of mistaken identity became a running joke. SHAY: You know, it was ha-ha, they thought that he was Judy. VEDANTAM: Shay didn't correct the callers. That's because Shay didn't mind being mistaken for Judy. SHAY: It was just comforting to me because it felt natural. (SOUNDBITE OF MUSIC)VEDANTAM: Shay was raised as a boy. But now, decades later, Shay identifies as a transgender woman. We're not using her legal name at her request because it's a man's name. Shay's experience at the deli became a template for the rest of her life. She listened to her voice, and she listened to the way others heard her voice. There was always a gap between the two. She first tried to sound more masculine to fit in with the way the world saw her. SHAY: So I would consciously make an effort to try to talk a little deeper. It was - you know, I practiced it. (SOUNDBITE OF SONG, \"ROADHOUSE BLUES\")THE DOORS: (Singing) And I woke up in morning, I got myself a beer. VEDANTAM: One way she practiced was to sing along with The Doors, Tool and Nine Inch Nails. SHAY: I tended to sort of go towards heavier music. You know, raspy, deep - yelling, almost - voices. (SOUNDBITE OF SONG, \"ROADHOUSE BLUES\")THE DOORS: (Singing) Let it roll all night long. VEDANTAM: Sounding more masculine became second nature. But it wore on her. SHAY: My entire life, I have been playing the role of a boy. And it is exhausting. It truly is. VEDANTAM: Years and years passed like this. A divorce, a second wife, two kids and a cancer scare later, Shay began to reconsider how she wanted to sound. Instead of trying to sound more masculine, she now started to try to sound more feminine. SHAY: Even prior to accepting that I was trans before I could put a label on what I was, I consciously made an effort to not sound as masculine, and that started in my early 30s. VEDANTAM: Once again, she used music as a way to practice. (SOUNDBITE OF BRITNEY SPEARS SONG, \"OOPS! . . . I DID IT AGAIN\")SHAY: I would always sing in the car alone. And I would attempt Britney Spears. (SOUNDBITE OF SONG, \"OOPS! . . . I DID IT AGAIN\")BRITNEY SPEARS: (Singing) I think I did it again. I made you believe we're more than just friends. Baby. SHAY: It seems silly, but I would, when I'm singing in the car, I would sort of turn my head towards my driver's side window. . . (SOUNDBITE OF SONG, \"OOPS! . . . I DID IT AGAIN\")SPEARS: (Singing) 'Cause to lose all my senses. SHAY: . . . Because it would reflect the sound back to me a little more loudly so that I could hear the pitch and tone of my voice. (SOUNDBITE OF SONG, \"OOPS! . . . I DID IT AGAIN\")SPEARS: (Singing) Oops. I did it again. I played with your heart. SHAY: And I would try to make my voice sound at a higher pitch without it sounding like I was trying. (SOUNDBITE OF SONG, \"OOPS! . . . I DID IT AGAIN\")SPEARS: (Singing) Love, and I'm sent from above. I'm not that innocent. VEDANTAM: What did you hear? Did you hear the voice that you wanted to hear? SHAY: No. No. I've never - I don't know that I have ever actually been able to hear the same voice that I hear come from me. VEDANTAM: Shay has spent a lifetime being dissatisfied with the way she sounds. She viscerally knows something the rest of us often forget - our voices shape who we are. They shape how other people think of us. (SOUNDBITE OF CROSSTALK)VEDANTAM: This week on HIDDEN BRAIN, we look at the relationship between our voices and our identities. RUPAL PATEL: Voice is about who you are. Our voice signals things about our personality. VEDANTAM: Plus how technology might help people with vocal impairments find voices that reflect who they are. . . (SOUNDBITE OF ARCHIVED RECORDING)LONNIE BLANCHARD: Once you close your eyes and let your mind relax, it doesn't take long to escape to the beautiful beach. VEDANTAM: . . . And the ethical quandaries that arise when we can create personalized, customized voices. (SOUNDBITE OF ARCHIVED RECORDING)COMPUTER-GENERATED VOICE #1: (As Donald Trump) This is huge. They can make us say anything now - really, anything. (SOUNDBITE OF MUSIC)VEDANTAM: Jackie Kirk (ph) used to love the sound of her voice. She spent her 20s in San Francisco. And like many young people living in the big city, she enjoyed going out with her friends. She danced to electronic music at clubs and drank at bars. She was outgoing, and she was a flirt. JACKIE KIRK: I have to admit I've always enjoyed flirting. (Laughter). And so I was quite the flirter. (Laughter). It was sort of a fun activity to help pass the day - you know, doing something mundane, you know, at work. VEDANTAM: For Jackie, flirting was also a demonstration of her confidence. KIRK: It reinforces my own identity, how I felt about myself - fun, I'm somebody that people are attracted to, not just physically or sexually, but a person who people like. (SOUNDBITE OF MUSIC)VEDANTAM: Jackie liked to be liked. She liked being someone people wanted to be around. When she thinks about who she was back then she refers to that person as Voice One Jackie. KIRK: Voice One Jackie (laughter) was really fun-loving, always joking, pretty carefree. VEDANTAM: For years, Jackie had been doing backpacking trips with her then-boyfriend. KIRK: All of our gear on our back. Fifty pounds, 60 pounds, et cetera. Even carrying bottles of wine, (laughter), you know, in the backpack. VEDANTAM: She'd been doing these backpacking trips for several years. But during a trip to a national forest in California, Jackie came up short. KIRK: I couldn't go one step further. I - you know, and being young, you think, you know, what could it be? There's nothing wrong with me. I'm 20, whatever, 20-something years old. And I notice I'm getting really short of breath, and it was a real struggle. Yeah. I finally made it, of course, but it was really slow and a real struggle to make it back. VEDANTAM: Hiking became too taxing for her so she cut back and switched to ballet. But one day during a series of releves, a more aerobically challenging dance move, Jackie felt lightheaded and dizzy. It was serious. KIRK: I had a seizure. And, you know, all of the medical follow-up led me to discover that I had a lung disease. (SOUNDBITE OF MUSIC)VEDANTAM: She was diagnosed with idiopathic pulmonary hypertension. It's a rare, progressive disease where the blood vessels in the lungs shrink and oxygen is not distributed properly. KIRK: There's no cure for it except for a lung transplant. VEDANTAM: In 2008, at 32, Jackie received a double lung transplant. The surgery was successful. Within weeks, she was out of the hospital and back in the dance studio. In 2010, she left San Francisco to explore Latin America and Europe, but her body began to reject her new lungs. Before long, Jackie was back in the hospital, this time in Switzerland, waiting for another lung transplant. KIRK: I had the surgery in January of 2013, and I was asleep still for another month and a half. VEDANTAM: When she woke up, she found her surgery had been successful, except for one very important thing. KIRK: I couldn't speak. I couldn't speak. VEDANTAM: During the operation, the medical staff had used a ventilator to help her breathe. KIRK: So I was intubated. That means they basically have a tube that they put inside your mouth, and it goes down your throat. And they send that down into your lungs. And during this intubation, the tube was rigid enough to cause some damage to my vocal cords. VEDANTAM: Jackie began speech therapy, and within a few weeks she slowly regained the ability to speak. But the voice that came out of her mouth, it wasn't her voice. KIRK: My voice changed. Its raspier. It's broken a bit. VEDANTAM: Ever since, speaking has been hard work. KIRK: Yeah. I really have to push my - I feel it. It's actually a physical effort. Like, I'm actually squeezing the vocal cords as hard as I can to make the loudest sound possible to get to be heard. (Laughter). And it's very tiring. VEDANTAM: The harder it became to produce sound, the more self-conscious she became about the way she sounded. KIRK: I feel less confident. I'm aware of how people might perceive me. So I'm a little more shy. I don't approach people like I used to. VEDANTAM: Jackie believes the change in her voice has led to a dramatic change in her personality. For much of her life, her voice was a manifestation of her confidence. KIRK: I used to go to clubs quite a bit, you know? But, you know, when you have a normal voice, you can still talk to people in those environments where it's kind of loud and noisy, or bars to meet friends or to flirt, (laughter), like I like doing. But, you know, those places now, you know, I don't really go to anymore. VEDANTAM: Jackie, a woman who once described herself as carefree and outgoing who took pride in her ability to flirt, became withdrawn, reserved. KIRK: You know, I have tons of scars all over my body, and that plays on my confidence as well. But in public life, people can't see those scars. And I feel like my voice is that, you know, that scar they can hear, you know? They know something's wrong and that they know, oh, maybe she's weak, maybe she's sick, just by hearing my voice. It's this signal. VEDANTAM: Our voices communicate so much more than mere information. They communicate our feelings, our temperament, our identity. When we come back - how scientists are weaving this insight into custom-built voices. MAEVE FLACK: I can't wait for my friends to hear my new voice. (SOUNDBITE OF MUSIC)VEDANTAM: Scientists have been trying for more than two centuries to analyze the human voice, decode its components and recreate it. An early success came from a man named Homer Dudley. He developed an organ-like machine that he called the voder. It worked using special keys and a foot pedal and was capable of creating about 20 different electronic buzzes and sounds. When those sounds were combined, they formed words. The voder fascinated people at the 1939 World's Fair in New York. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: Well, we've heard the voder make a word. And by combining words, of course, we got a sentence. For example, Helen, will you have the voder say, she saw me? VODER-GENERATED VOICE: She saw me. UNIDENTIFIED PERSON #1: That sounded awfully flat. How about a little expression? Say the sentence in answer to these questions. Who saw you? VODER-GENERATED VOICE: She saw me. UNIDENTIFIED PERSON #1: Whom did she see? VODER-GENERATED VOICE: She saw me. UNIDENTIFIED PERSON #1: Well, did she see you or hear you? VODER-GENERATED VOICE: She saw me. VEDANTAM: The voder was an early example of electronic speech, but it was cumbersome to operate and required special training. Over the next 40 years, speech scientists continued studying the components of the human voice. They eventually developed methods to mathematically map the acoustic patterns and phonetic properties of natural speech - vowels, syllable constructions, consonants. (SOUNDBITE OF MONTAGE)COMPUTER-GENERATED VOICE #2: Welcome to the Stockholm speech communication seminar. COMPUTER-GENERATED VOICE #3: Hello, I (unintelligible) machine. Welcome to Mid-Manhattan Library. COMPUTER-GENERATED VOICE #4: (Singing) A, B, C, D, E, F, G. . . COMPUTER-GENERATED VOICE #5: To be or not to be, that is the question. COMPUTER-GENERATED VOICE #6: I can read stories and speak them aloud. I do not understand what the words mean when I read them. COMPUTER-GENERATED VOICE #7: This is such a beautiful (unintelligible). You are listening to the voice of a machine. COMPUTER-GENERATED VOICE #4: (Singing) When we know our ABC's. VEDANTAM: By the '80s, speech synthesis was no longer the stuff of science demonstrations at shows and fairs. COMPUTER-GENERATED VOICE #8: Text-to-speech systems are beginning to be applied in many ways, including aids for the handicapped, medical aids and teaching devices. The first kind of aid to be considered as a talking aid for the vocally handicapped. VEDANTAM: The research of Dennis Klatt at MIT paved the way for the voices we might be familiar with today, many of them used in assistive communication devices. BETTY: I am beautiful Betty, the standard female voice. HARRY: I am huge Harry, a very large person with a deep voice. PAUL: I am the standard male voice, perfect Paul. VEDANTAM: This last one, by the way, became famous after Stephen Hawking adopted it. Speech technology has come a long way in the years since Homer Dudley unveiled the voder, but in many ways, synthetic voices still sounded synthetic. They didn't convey all the information that's packed into the human voice. PATEL: Voice is identity, right? Voice is about who you are. Our voice signals how old we are. Our voice signals our gender. Our voice signals, you know, things about our personality. VEDANTAM: Rupal Patel is a speech scientist at Northeastern University. Perhaps more than many people, she has thought a lot about the human voice. When she misses her mother, for instance, Rupal has a special technique to evoke her presence. PATEL: That's right. My parents now live in LA and I live here in Boston. And oftentimes, I find myself imitating my mom. You know, I'll say, oh, beti (ph) how are you today, you know, or something like that. I'll imitate her the way she might say something. I might say that the same way to my daughter or something like that. But I - what I'm evoking is my mother's voice, primarily, to feel the closeness of her here. VEDANTAM: In 2002, Rupal took these ideas with her to Denmark where she was scheduled to speak at a conference for researchers and patients. PATEL: I was presenting some of my early work showing that individuals with very severe speech disorder still have the ability to make sound, and those sounds have some communicative content in them, some information that could be used. VEDANTAM: After her presentation, she walked out to the exhibit hall, and that's where she noticed something. Lots of people were using devices that produced synthetic voices. What was odd was that many of the voices didn't seem to match the people using them. PATEL: And at that point back in 2002, we had very limited synthetic voice of options available. And so, you - I heard a little girl, or a young girl, using a device to talk with an adult male voice and and having a conversation with another person, a middle-aged man, who also was using the same voice. And so they're using different devices, but their voices were identical. VEDANTAM: She had just presented on the idea that our individual voices carry something unique about us. So why was this not reflected in these synthetic voices? PATEL: Why are we giving them the same black box to speak through? There's got to be something that we can do that we can harness the quality of the voices that they have and imprint those or use that to give them a prosthetic voice that somehow reflects who they are and not just the same voice for everyone. VEDANTAM: Could a synthetic voice capture the richness of natural human speech? Rupal launched a company to answer this question. It's called VocaliD, and it uses machine learning and other artificial intelligence technologies to create personalized voices. PATEL: So what synthetic speech is is taking recordings of anyone and then taking those recordings and building a model of the voice quality of the annunciation abilities, right? You aren't necessarily analyzing it from a top down, saying, well, this person has a high-pitched voice or this person has a low-pitched voice. You're taking the recordings as basically the raw ingredients to feed to a machine-learning algorithm or set of algorithms, really. And those are - they're learning the patterns of the clarity of the person's S, the - you know, how that sound is changed in the different phonetic environments, the voice qualities aspects of - you know, all of these are learned by the machine. It's really re-emulating the human voice by a machine. (SOUNDBITE OF MUSIC)VEDANTAM: In other words, the idea is to build a model of how a person sounds. To do so, you use a vast range of examples of that person's speech. Then you use the model to produce spoken language that incorporates all the idiosyncrasies and texture of that person's voice. One of Rupal's early clients was a young girl, Maeve Flack. PATEL: Maeve was born with cerebral palsy. She's in a beautiful family where she has two other older sisters. VEDANTAM: Rupal's goal was to give Maeve her own unique synthetic voice, one that could express not just her words but her identity. The first step was to record her. MAEVE: (Unintelligible) (Laughter). PATEL: So those sounds were the kinds of sounds that are unique to Maeve. Those are the sounds that she makes where if - you know, when she's in a classroom with several other kids who also have communication disabilities, when she makes that sound, you know it's Maeve speaking. So we harnessed those sounds of Maeve's to create her unique voice for her. VEDANTAM: Then Rupal turned to Maeve's older sisters, Erin and Meghan, who volunteered to record their voices so they could be blended with Maeve's. UNIDENTIFIED PERSON #2: Ice cream is my guilty pleasure. UNIDENTIFIED PERSON #3: That man ran fast. VEDANTAM: Erin and Meghan read hundreds of sentences and phrases and uploaded them to a website for Rupal. Like a painter mixing a palette, Rupal took elements of Maeve's voice and mixed them with those of her sisters and other vocal donors to create what she calls a bespoke voice. MAEVE: I can't wait for my friends to hear my new voice. My parents are really happy I'm not addicted to Fortnite. I want to meet Taylor Swift. PATEL: So we're hearing, you know, Maeve at this age in terms of her sound as well as her siblings' recordings being combined and being produced through this speech synthesis engine. VEDANTAM: It's possible that Maeve may decide as she gets older that her voice needs to age with her. She'll need a new bespoke voice at that point. The same technology can also be used to preserve a person's existing voice. Sometimes this is done when a person faces the prospect of losing his voice. PATEL: These could be individuals who are losing their voice to degenerative conditions - so slowly their voice is changing - such as ALS or Parkinson's disease and then those who the trauma is actually far more pronounced for individuals with something like head and neck cancer where they learn that they're going to have their voice box removed within a couple of weeks. VEDANTAM: Lonnie Blanchard confronted this traumatic news in 2018. Doctors had diagnosed him with cancer and said surgery was the only option. Lonnie had to have his tongue removed. Here he is speaking to the BBC. (SOUNDBITE OF ARCHIVED RECORDING)BLANCHARD: Now that I know I'm going to lose my voice, I got to get some things down on a personal recorder to get what I would normally say to my wife and kids. But every time I go to do that, I draw a blank. VEDANTAM: By the time Lonnie started working with Rupal, he only had a few weeks to back his voice. PATEL: We helped him get set up in terms of the microphone he would need, and things like that. VEDANTAM: Rupal walked with Lonnie to build a database of sound samples before his surgery. He recorded sentences that gave Rupal and her colleagues the different kinds of sounds they would need to build a new voice. (SOUNDBITE OF ARCHIVED RECORDING)BLANCHARD: I wish we could get acquainted. I'm going to be a teacher when I grow up. VEDANTAM: After Lonnie banked his voice, Rupal use the recordings to create a personalized voice for him. The difference between his voice and Maeve's voice is that Rupal didn't need to blend voices from donors. Lonnie was his own donor. PATEL: Those voice samples then are used, are cleaned up and annotated by machine, actually, and then used to feed into the algorithms we have to create the synthetic voice. VEDANTAM: Similar to Maeve, Lonnie uses an assistive device - in his case, an iPad. He can type out what he wants to say and hear his voice speaking to his family. (SOUNDBITE OF ARCHIVED RECORDING)BLANCHARD: Once you close your eyes and let your mind relax, it doesn't take long to escape to the beautiful beach. PATEL: It's really empowering. It's continued to be a way that he can connect to family members and feel that part of him is not fully lost. (SOUNDBITE OF MUSIC)VEDANTAM: While most of us will never have the experience of losing our voices and having to obtain synthetic voices as replacements, increasingly, many of us are coming into contact with these voices. UNIDENTIFIED PERSON #4: Hey, Siri. UNIDENTIFIED PERSON #5: Hey, Google. UNIDENTIFIED PERSON #6: Alexa? UNIDENTIFIED PERSON #4: How many ounces are in a cup? ALEXA: One cup is 8 eight fluid ounces. UNIDENTIFIED PERSON #6: OK, Google. UNIDENTIFIED PERSON #4: Hey, Siri. Set a timer. SIRI: For how long? UNIDENTIFIED PERSON #5: Fifty-six minutes. SIRI: OK. ALEXA: Sure. SIRI: Fifty-six minutes, starting now. UNIDENTIFIED PERSON #6: Alexa? UNIDENTIFIED PERSON #5: Hey, Google? UNIDENTIFIED PERSON #6: Can you play music? UNIDENTIFIED PERSON #5: Play some jazz. SIRI: Here's a station you might like. (SOUNDBITE OF MUSIC)VEDANTAM: Synthetic voices are already changing our lives, and it's likely we're going to become even more reliant on them. In May 2018, Google revealed a new program it was working on. CEO Sundar Pichai presented it to an audience of software developers. The technology is called Google Duplex. It allows you to make a restaurant reservation through a voice assistant. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED RESTAURANT HOST: How may I help you? GOOGLE DUPLEX: Hi. I'd like to reserve a table for Wednesday the 7. UNIDENTIFIED RESTAURANT HOST: For seven people? GOOGLE DUPLEX: It's for four people. UNIDENTIFIED RESTAURANT HOST: Four people? When? Today? Tonight? GOOGLE DUPLEX: Wednesday at 6 p. m. UNIDENTIFIED RESTAURANT HOST: Actually, we reserve for upwards five people. For four people, you can come. GOOGLE DUPLEX: How long is the wait, usually, to be seated? UNIDENTIFIED RESTAURANT HOST: For when? Tomorrow, or a weekday, or. . . GOOGLE DUPLEX: For next Wednesday, the 7. UNIDENTIFIED RESTAURANT HOST: No. It's not too busy. You can come for four people. OK? GOOGLE DUPLEX: Oh - I gotcha. Thanks. UNIDENTIFIED RESTAURANT HOST: Bye-bye. (LAUGHTER, APPLAUSE)VEDANTAM: The audience is laughing and applauding because the man making the call isn't a man but a machine. PATEL: It didn't seem like it was a robotic voice. The robotic voices we're used to are the voices like when you are in a parking garage and you hear the, you know, please place your ticket with the stripe facing to the right - very, very canned sort of speech. This was far more sophisticated and much more like you and I talk, with hesitations and pauses, and ums and ahs. You think it's a human on the other end. VEDANTAM: Now, of course, one of the things about that voice that Google had is that it did seem like a convincing voice, but if you need to convince me that that voice is not just a human voice but a particular human's voice, you need to convince me now this is not just anyone calling for a restaurant reservation, but it's Barack Obama calling for a restaurant reservation. Presumably, now the bar is much, much higher. PATEL: That's right. It is. Barack Obama, though, does have a ton on his audio, (laughter), on the Internet. And there's a lot more audio to make, you know, his voice than there is my voice, for example. And so yeah. But it's absolutely possible to learn. And if you have long enough, you can learn anybody's voice. And if you have enough data. (SOUNDBITE OF MUSIC)VEDANTAM: It's not hard to see how bad actors could misuse this, create havoc in people's lives, trouble at companies, political misinformation. PATEL: That's absolutely - I mean, we're seeing deep fakes in video. We've seen, you know, President Obama's face with - being manipulated and the audio coming out. You know, people creating these fake media in video, and you're also seeing it audio. That's exactly why the security aspects of what we're doing are trying to detect is that fake audio, or is that real? Is part of that fake audio or is part of that real, right? So it is - this isn't completely sci fi. It isn't so far away. It isn't necessarily 20 - you know, 2028. It's probably 2020. So we've got to get our defenses up in terms of questioning where that - the authenticity of audio just as we do video. VEDANTAM: In 2017, a Canadian company called Lyrebird showed how audio deepfakes might work in politics. (SOUNDBITE OF MONTAGE)COMPUTER-GENERATED VOICE #1: (As Donald Trump) This is huge. They can make us say anything now - really anything. COMPUTER-GENERATED VOICE #9: (As Barack Obama) The good news is that they will offer the technology to anyone. COMPUTER-GENERATED VOICE #1: (As Donald Trump) This is huge. How does their technology work? COMPUTER-GENERATED VOICE #10: (As Hillary Clinton) Hey, guys, I think that they use deep learning and artificial neural networks. VEDANTAM: By 2019, deepfake audio technology had gotten even better. Shortly after critics panned the final season of \"Game Of Thrones,\" a YouTube channel called Eating Things With Famous People put out this tongue-in-cheek video showing the supposed remorse of the lead character, Jon Snow. (SOUNDBITE OF YOUTUBE VIDEO, \"BREAKING: JON SNOW FINALLY APOLOGIZED FOR SEASON 8\")COMPUTER-GENERATED VOICE #11: (As Jon Snow) It's time for some apologies. I'm sorry we wasted your time. I'm sorry we didn't learn anything from the ending of \"Lost. \" I have more lines in this video than I had in the last season. I'm sorry we wrote this in, like, six days or something. Now, let us burn the script of Season 8 and just forget it forever. VEDANTAM: Spoofing a TV show is one thing, but imagine such high-quality deepfakes occurring in a more high-stakes setting. Voices are increasingly being used by financial institutions to authenticate the identities of consumers. Recently, Rupal worked with a bank to assess how vulnerable it was to vocal hacking. PATEL: We tested their authentication system by creating synthetic samples or synthetic voices of particular individuals who are enrolled in their authentication system. And we tried to test those voices against the system to see if we could get through with the synthetic voices. And we were not able to do that for every single voice, but we were able to do it for some voices. And so it just starts to show that there is a vulnerability in this technology. VEDANTAM: So how would you guard against it? PATEL: Well, there are many ways to guard against it. One is you can classify the difference between is the audio signal I'm listening to - is it synthetic or is it human? As the synthetic voices become better and better sounding, that will be a more difficult decision to make. And it is something that if we can proactively solve, I think - or at least start to address - we're going to be way ahead of the curve than if we're trying to clean up our mess after the fact. VEDANTAM: Despite the potential risks of these new technologies, Rupal is also optimistic. Voice synthesis tools have the potential to allow people to craft the voice they hear on the outside so that it matches the identity they feel on the inside. PATEL: Ideally, in the future, these decisions are made by the end user themselves. Like, oh, I actually want that to be - sound a little breathier. I'd love that to sound a little bit more confident. And, I mean, how does that translate to the acoustics? We don't quite know yet, but that's actually I think where - when we can finally give the control of what the voice sounds like to the individual, I mean, that's the Holy Grail. (SOUNDBITE OF MONTAGE)VODER-GENERATED VOICE: She saw me. She saw me. She saw me. She saw me. COMPUTER-GENERATED VOICE #2: Hello. I am (unintelligible) machine. COMPUTER-GENERATED VOICE #6: You are listening to the voice of a machine. COMPUTER-GENERATED VOICE #3: (Singing) A, B, C, D, E, F, G, H, I, J, K. . . COMPUTER-GENERATED VOICE #7: The first kind of aid to be considered as a talking aid for the vocally handicapped. BETTY: I am beautiful Betty, the standard female voice. PAUL: I am the standard male voice, perfect Paul. COMPUTER-GENERATED VOICE #12: I was sad because there was no ice cream in the freezer. COMPUTER-GENERATED VOICE #13: The sky is clear and the stars are twinkling. MAEVE: I can't wait for my friends to hear my new voice. COMPUTER-GENERATED VOICE #3: (Singing) When we know our ABC's. ALEXA: Once cup is eight fluid ounces. COMPUTER-GENERATED VOICE #14: You are listening to a machine. COMPUTER-GENERATED VOICE #1: (As Donald Trump) This is huge. How does their technology work? COMPUTER-GENERATED VOICE #10: (As Hillary Clinton) Hey, guys, I think that they use deep learning and artificial neural networks. GOOGLE DUPLEX: Hi. I'd like to reserve a table for Wednesday the 7. (SOUNDBITE OF MUSIC)VEDANTAM: This week's show was produced by Thomas Lu. It was edited by Tara Boyle and Rhaina Cohen. Our team includes Parth Shah, Jenny Schmidt and Laura Kwerel. Special thanks to Brent Baughman, Greg Sauer and Kavon Jones (ph). (SOUNDBITE OF MUSIC)VEDANTAM: Our unsung hero this week is Rebecca Ralph (ph). She's part of NPR's team looking at our changing interactions with smart speakers. She helped us record some of the smart devices you heard in this week's episode. Thanks, Rebecca. If you liked this episode, please share it with a friend. We're always looking for new people to discover HIDDEN BRAIN. I'm Shankar Vedantam, and this is NPR. SHANKAR VEDANTAM, HOST:  This is HIDDEN BRAIN. I'm Shankar Vedantam. At some point in our lives, many of us realize that the way we hear our own voice isn't the way others hear our voice. Shay (ph) had that realization as a child helping out at the family business, a deli in south West Virginia. SHAY: We did a lot of business with the deli, a lot of call-in orders, and I had to answer the phone with the name of the business. VEDANTAM: Often when the phone rang, the same thing happened over and over again. SHAY: I vividly remember always being confused for my mother. They would always say, hey Judy (ph), you know, and either start into their questions or whatever they were looking to speak to my mother about. VEDANTAM: This case of mistaken identity became a running joke. SHAY: You know, it was ha-ha, they thought that he was Judy. VEDANTAM: Shay didn't correct the callers. That's because Shay didn't mind being mistaken for Judy. SHAY: It was just comforting to me because it felt natural. (SOUNDBITE OF MUSIC) VEDANTAM: Shay was raised as a boy. But now, decades later, Shay identifies as a transgender woman. We're not using her legal name at her request because it's a man's name. Shay's experience at the deli became a template for the rest of her life. She listened to her voice, and she listened to the way others heard her voice. There was always a gap between the two. She first tried to sound more masculine to fit in with the way the world saw her. SHAY: So I would consciously make an effort to try to talk a little deeper. It was - you know, I practiced it. (SOUNDBITE OF SONG, \"ROADHOUSE BLUES\") THE DOORS: (Singing) And I woke up in morning, I got myself a beer. VEDANTAM: One way she practiced was to sing along with The Doors, Tool and Nine Inch Nails. SHAY: I tended to sort of go towards heavier music. You know, raspy, deep - yelling, almost - voices. (SOUNDBITE OF SONG, \"ROADHOUSE BLUES\") THE DOORS: (Singing) Let it roll all night long. VEDANTAM: Sounding more masculine became second nature. But it wore on her. SHAY: My entire life, I have been playing the role of a boy. And it is exhausting. It truly is. VEDANTAM: Years and years passed like this. A divorce, a second wife, two kids and a cancer scare later, Shay began to reconsider how she wanted to sound. Instead of trying to sound more masculine, she now started to try to sound more feminine. SHAY: Even prior to accepting that I was trans before I could put a label on what I was, I consciously made an effort to not sound as masculine, and that started in my early 30s. VEDANTAM: Once again, she used music as a way to practice. (SOUNDBITE OF BRITNEY SPEARS SONG, \"OOPS! . . . I DID IT AGAIN\") SHAY: I would always sing in the car alone. And I would attempt Britney Spears. (SOUNDBITE OF SONG, \"OOPS! . . . I DID IT AGAIN\") BRITNEY SPEARS: (Singing) I think I did it again. I made you believe we're more than just friends. Baby. SHAY: It seems silly, but I would, when I'm singing in the car, I would sort of turn my head towards my driver's side window. . . (SOUNDBITE OF SONG, \"OOPS! . . . I DID IT AGAIN\") SPEARS: (Singing) 'Cause to lose all my senses. SHAY: . . . Because it would reflect the sound back to me a little more loudly so that I could hear the pitch and tone of my voice. (SOUNDBITE OF SONG, \"OOPS! . . . I DID IT AGAIN\") SPEARS: (Singing) Oops. I did it again. I played with your heart. SHAY: And I would try to make my voice sound at a higher pitch without it sounding like I was trying. (SOUNDBITE OF SONG, \"OOPS! . . . I DID IT AGAIN\") SPEARS: (Singing) Love, and I'm sent from above. I'm not that innocent. VEDANTAM: What did you hear? Did you hear the voice that you wanted to hear? SHAY: No. No. I've never - I don't know that I have ever actually been able to hear the same voice that I hear come from me. VEDANTAM: Shay has spent a lifetime being dissatisfied with the way she sounds. She viscerally knows something the rest of us often forget - our voices shape who we are. They shape how other people think of us. (SOUNDBITE OF CROSSTALK) VEDANTAM: This week on HIDDEN BRAIN, we look at the relationship between our voices and our identities. RUPAL PATEL: Voice is about who you are. Our voice signals things about our personality. VEDANTAM: Plus how technology might help people with vocal impairments find voices that reflect who they are. . . (SOUNDBITE OF ARCHIVED RECORDING) LONNIE BLANCHARD: Once you close your eyes and let your mind relax, it doesn't take long to escape to the beautiful beach. VEDANTAM: . . . And the ethical quandaries that arise when we can create personalized, customized voices. (SOUNDBITE OF ARCHIVED RECORDING) COMPUTER-GENERATED VOICE #1: (As Donald Trump) This is huge. They can make us say anything now - really, anything. (SOUNDBITE OF MUSIC) VEDANTAM: Jackie Kirk (ph) used to love the sound of her voice. She spent her 20s in San Francisco. And like many young people living in the big city, she enjoyed going out with her friends. She danced to electronic music at clubs and drank at bars. She was outgoing, and she was a flirt. JACKIE KIRK: I have to admit I've always enjoyed flirting. (Laughter). And so I was quite the flirter. (Laughter). It was sort of a fun activity to help pass the day - you know, doing something mundane, you know, at work. VEDANTAM: For Jackie, flirting was also a demonstration of her confidence. KIRK: It reinforces my own identity, how I felt about myself - fun, I'm somebody that people are attracted to, not just physically or sexually, but a person who people like. (SOUNDBITE OF MUSIC) VEDANTAM: Jackie liked to be liked. She liked being someone people wanted to be around. When she thinks about who she was back then she refers to that person as Voice One Jackie. KIRK: Voice One Jackie (laughter) was really fun-loving, always joking, pretty carefree. VEDANTAM: For years, Jackie had been doing backpacking trips with her then-boyfriend. KIRK: All of our gear on our back. Fifty pounds, 60 pounds, et cetera. Even carrying bottles of wine, (laughter), you know, in the backpack. VEDANTAM: She'd been doing these backpacking trips for several years. But during a trip to a national forest in California, Jackie came up short. KIRK: I couldn't go one step further. I - you know, and being young, you think, you know, what could it be? There's nothing wrong with me. I'm 20, whatever, 20-something years old. And I notice I'm getting really short of breath, and it was a real struggle. Yeah. I finally made it, of course, but it was really slow and a real struggle to make it back. VEDANTAM: Hiking became too taxing for her so she cut back and switched to ballet. But one day during a series of releves, a more aerobically challenging dance move, Jackie felt lightheaded and dizzy. It was serious. KIRK: I had a seizure. And, you know, all of the medical follow-up led me to discover that I had a lung disease. (SOUNDBITE OF MUSIC) VEDANTAM: She was diagnosed with idiopathic pulmonary hypertension. It's a rare, progressive disease where the blood vessels in the lungs shrink and oxygen is not distributed properly. KIRK: There's no cure for it except for a lung transplant. VEDANTAM: In 2008, at 32, Jackie received a double lung transplant. The surgery was successful. Within weeks, she was out of the hospital and back in the dance studio. In 2010, she left San Francisco to explore Latin America and Europe, but her body began to reject her new lungs. Before long, Jackie was back in the hospital, this time in Switzerland, waiting for another lung transplant. KIRK: I had the surgery in January of 2013, and I was asleep still for another month and a half. VEDANTAM: When she woke up, she found her surgery had been successful, except for one very important thing. KIRK: I couldn't speak. I couldn't speak. VEDANTAM: During the operation, the medical staff had used a ventilator to help her breathe. KIRK: So I was intubated. That means they basically have a tube that they put inside your mouth, and it goes down your throat. And they send that down into your lungs. And during this intubation, the tube was rigid enough to cause some damage to my vocal cords. VEDANTAM: Jackie began speech therapy, and within a few weeks she slowly regained the ability to speak. But the voice that came out of her mouth, it wasn't her voice. KIRK: My voice changed. Its raspier. It's broken a bit. VEDANTAM: Ever since, speaking has been hard work. KIRK: Yeah. I really have to push my - I feel it. It's actually a physical effort. Like, I'm actually squeezing the vocal cords as hard as I can to make the loudest sound possible to get to be heard. (Laughter). And it's very tiring. VEDANTAM: The harder it became to produce sound, the more self-conscious she became about the way she sounded. KIRK: I feel less confident. I'm aware of how people might perceive me. So I'm a little more shy. I don't approach people like I used to. VEDANTAM: Jackie believes the change in her voice has led to a dramatic change in her personality. For much of her life, her voice was a manifestation of her confidence. KIRK: I used to go to clubs quite a bit, you know? But, you know, when you have a normal voice, you can still talk to people in those environments where it's kind of loud and noisy, or bars to meet friends or to flirt, (laughter), like I like doing. But, you know, those places now, you know, I don't really go to anymore. VEDANTAM: Jackie, a woman who once described herself as carefree and outgoing who took pride in her ability to flirt, became withdrawn, reserved. KIRK: You know, I have tons of scars all over my body, and that plays on my confidence as well. But in public life, people can't see those scars. And I feel like my voice is that, you know, that scar they can hear, you know? They know something's wrong and that they know, oh, maybe she's weak, maybe she's sick, just by hearing my voice. It's this signal. VEDANTAM: Our voices communicate so much more than mere information. They communicate our feelings, our temperament, our identity. When we come back - how scientists are weaving this insight into custom-built voices. MAEVE FLACK: I can't wait for my friends to hear my new voice. (SOUNDBITE OF MUSIC) VEDANTAM: Scientists have been trying for more than two centuries to analyze the human voice, decode its components and recreate it. An early success came from a man named Homer Dudley. He developed an organ-like machine that he called the voder. It worked using special keys and a foot pedal and was capable of creating about 20 different electronic buzzes and sounds. When those sounds were combined, they formed words. The voder fascinated people at the 1939 World's Fair in New York. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: Well, we've heard the voder make a word. And by combining words, of course, we got a sentence. For example, Helen, will you have the voder say, she saw me? VODER-GENERATED VOICE: She saw me. UNIDENTIFIED PERSON #1: That sounded awfully flat. How about a little expression? Say the sentence in answer to these questions. Who saw you? VODER-GENERATED VOICE: She saw me. UNIDENTIFIED PERSON #1: Whom did she see? VODER-GENERATED VOICE: She saw me. UNIDENTIFIED PERSON #1: Well, did she see you or hear you? VODER-GENERATED VOICE: She saw me. VEDANTAM: The voder was an early example of electronic speech, but it was cumbersome to operate and required special training. Over the next 40 years, speech scientists continued studying the components of the human voice. They eventually developed methods to mathematically map the acoustic patterns and phonetic properties of natural speech - vowels, syllable constructions, consonants. (SOUNDBITE OF MONTAGE) COMPUTER-GENERATED VOICE #2: Welcome to the Stockholm speech communication seminar. COMPUTER-GENERATED VOICE #3: Hello, I (unintelligible) machine. Welcome to Mid-Manhattan Library. COMPUTER-GENERATED VOICE #4: (Singing) A, B, C, D, E, F, G. . . COMPUTER-GENERATED VOICE #5: To be or not to be, that is the question. COMPUTER-GENERATED VOICE #6: I can read stories and speak them aloud. I do not understand what the words mean when I read them. COMPUTER-GENERATED VOICE #7: This is such a beautiful (unintelligible). You are listening to the voice of a machine. COMPUTER-GENERATED VOICE #4: (Singing) When we know our ABC's. VEDANTAM: By the '80s, speech synthesis was no longer the stuff of science demonstrations at shows and fairs. COMPUTER-GENERATED VOICE #8: Text-to-speech systems are beginning to be applied in many ways, including aids for the handicapped, medical aids and teaching devices. The first kind of aid to be considered as a talking aid for the vocally handicapped. VEDANTAM: The research of Dennis Klatt at MIT paved the way for the voices we might be familiar with today, many of them used in assistive communication devices. BETTY: I am beautiful Betty, the standard female voice. HARRY: I am huge Harry, a very large person with a deep voice. PAUL: I am the standard male voice, perfect Paul. VEDANTAM: This last one, by the way, became famous after Stephen Hawking adopted it. Speech technology has come a long way in the years since Homer Dudley unveiled the voder, but in many ways, synthetic voices still sounded synthetic. They didn't convey all the information that's packed into the human voice. PATEL: Voice is identity, right? Voice is about who you are. Our voice signals how old we are. Our voice signals our gender. Our voice signals, you know, things about our personality. VEDANTAM: Rupal Patel is a speech scientist at Northeastern University. Perhaps more than many people, she has thought a lot about the human voice. When she misses her mother, for instance, Rupal has a special technique to evoke her presence. PATEL: That's right. My parents now live in LA and I live here in Boston. And oftentimes, I find myself imitating my mom. You know, I'll say, oh, beti (ph) how are you today, you know, or something like that. I'll imitate her the way she might say something. I might say that the same way to my daughter or something like that. But I - what I'm evoking is my mother's voice, primarily, to feel the closeness of her here. VEDANTAM: In 2002, Rupal took these ideas with her to Denmark where she was scheduled to speak at a conference for researchers and patients. PATEL: I was presenting some of my early work showing that individuals with very severe speech disorder still have the ability to make sound, and those sounds have some communicative content in them, some information that could be used. VEDANTAM: After her presentation, she walked out to the exhibit hall, and that's where she noticed something. Lots of people were using devices that produced synthetic voices. What was odd was that many of the voices didn't seem to match the people using them. PATEL: And at that point back in 2002, we had very limited synthetic voice of options available. And so, you - I heard a little girl, or a young girl, using a device to talk with an adult male voice and and having a conversation with another person, a middle-aged man, who also was using the same voice. And so they're using different devices, but their voices were identical. VEDANTAM: She had just presented on the idea that our individual voices carry something unique about us. So why was this not reflected in these synthetic voices? PATEL: Why are we giving them the same black box to speak through? There's got to be something that we can do that we can harness the quality of the voices that they have and imprint those or use that to give them a prosthetic voice that somehow reflects who they are and not just the same voice for everyone. VEDANTAM: Could a synthetic voice capture the richness of natural human speech? Rupal launched a company to answer this question. It's called VocaliD, and it uses machine learning and other artificial intelligence technologies to create personalized voices. PATEL: So what synthetic speech is is taking recordings of anyone and then taking those recordings and building a model of the voice quality of the annunciation abilities, right? You aren't necessarily analyzing it from a top down, saying, well, this person has a high-pitched voice or this person has a low-pitched voice. You're taking the recordings as basically the raw ingredients to feed to a machine-learning algorithm or set of algorithms, really. And those are - they're learning the patterns of the clarity of the person's S, the - you know, how that sound is changed in the different phonetic environments, the voice qualities aspects of - you know, all of these are learned by the machine. It's really re-emulating the human voice by a machine. (SOUNDBITE OF MUSIC) VEDANTAM: In other words, the idea is to build a model of how a person sounds. To do so, you use a vast range of examples of that person's speech. Then you use the model to produce spoken language that incorporates all the idiosyncrasies and texture of that person's voice. One of Rupal's early clients was a young girl, Maeve Flack. PATEL: Maeve was born with cerebral palsy. She's in a beautiful family where she has two other older sisters. VEDANTAM: Rupal's goal was to give Maeve her own unique synthetic voice, one that could express not just her words but her identity. The first step was to record her. MAEVE: (Unintelligible) (Laughter). PATEL: So those sounds were the kinds of sounds that are unique to Maeve. Those are the sounds that she makes where if - you know, when she's in a classroom with several other kids who also have communication disabilities, when she makes that sound, you know it's Maeve speaking. So we harnessed those sounds of Maeve's to create her unique voice for her. VEDANTAM: Then Rupal turned to Maeve's older sisters, Erin and Meghan, who volunteered to record their voices so they could be blended with Maeve's. UNIDENTIFIED PERSON #2: Ice cream is my guilty pleasure. UNIDENTIFIED PERSON #3: That man ran fast. VEDANTAM: Erin and Meghan read hundreds of sentences and phrases and uploaded them to a website for Rupal. Like a painter mixing a palette, Rupal took elements of Maeve's voice and mixed them with those of her sisters and other vocal donors to create what she calls a bespoke voice. MAEVE: I can't wait for my friends to hear my new voice. My parents are really happy I'm not addicted to Fortnite. I want to meet Taylor Swift. PATEL: So we're hearing, you know, Maeve at this age in terms of her sound as well as her siblings' recordings being combined and being produced through this speech synthesis engine. VEDANTAM: It's possible that Maeve may decide as she gets older that her voice needs to age with her. She'll need a new bespoke voice at that point. The same technology can also be used to preserve a person's existing voice. Sometimes this is done when a person faces the prospect of losing his voice. PATEL: These could be individuals who are losing their voice to degenerative conditions - so slowly their voice is changing - such as ALS or Parkinson's disease and then those who the trauma is actually far more pronounced for individuals with something like head and neck cancer where they learn that they're going to have their voice box removed within a couple of weeks. VEDANTAM: Lonnie Blanchard confronted this traumatic news in 2018. Doctors had diagnosed him with cancer and said surgery was the only option. Lonnie had to have his tongue removed. Here he is speaking to the BBC. (SOUNDBITE OF ARCHIVED RECORDING) BLANCHARD: Now that I know I'm going to lose my voice, I got to get some things down on a personal recorder to get what I would normally say to my wife and kids. But every time I go to do that, I draw a blank. VEDANTAM: By the time Lonnie started working with Rupal, he only had a few weeks to back his voice. PATEL: We helped him get set up in terms of the microphone he would need, and things like that. VEDANTAM: Rupal walked with Lonnie to build a database of sound samples before his surgery. He recorded sentences that gave Rupal and her colleagues the different kinds of sounds they would need to build a new voice. (SOUNDBITE OF ARCHIVED RECORDING) BLANCHARD: I wish we could get acquainted. I'm going to be a teacher when I grow up. VEDANTAM: After Lonnie banked his voice, Rupal use the recordings to create a personalized voice for him. The difference between his voice and Maeve's voice is that Rupal didn't need to blend voices from donors. Lonnie was his own donor. PATEL: Those voice samples then are used, are cleaned up and annotated by machine, actually, and then used to feed into the algorithms we have to create the synthetic voice. VEDANTAM: Similar to Maeve, Lonnie uses an assistive device - in his case, an iPad. He can type out what he wants to say and hear his voice speaking to his family. (SOUNDBITE OF ARCHIVED RECORDING) BLANCHARD: Once you close your eyes and let your mind relax, it doesn't take long to escape to the beautiful beach. PATEL: It's really empowering. It's continued to be a way that he can connect to family members and feel that part of him is not fully lost. (SOUNDBITE OF MUSIC) VEDANTAM: While most of us will never have the experience of losing our voices and having to obtain synthetic voices as replacements, increasingly, many of us are coming into contact with these voices. UNIDENTIFIED PERSON #4: Hey, Siri. UNIDENTIFIED PERSON #5: Hey, Google. UNIDENTIFIED PERSON #6: Alexa? UNIDENTIFIED PERSON #4: How many ounces are in a cup? ALEXA: One cup is 8 eight fluid ounces. UNIDENTIFIED PERSON #6: OK, Google. UNIDENTIFIED PERSON #4: Hey, Siri. Set a timer. SIRI: For how long? UNIDENTIFIED PERSON #5: Fifty-six minutes. SIRI: OK. ALEXA: Sure. SIRI: Fifty-six minutes, starting now. UNIDENTIFIED PERSON #6: Alexa? UNIDENTIFIED PERSON #5: Hey, Google? UNIDENTIFIED PERSON #6: Can you play music? UNIDENTIFIED PERSON #5: Play some jazz. SIRI: Here's a station you might like. (SOUNDBITE OF MUSIC) VEDANTAM: Synthetic voices are already changing our lives, and it's likely we're going to become even more reliant on them. In May 2018, Google revealed a new program it was working on. CEO Sundar Pichai presented it to an audience of software developers. The technology is called Google Duplex. It allows you to make a restaurant reservation through a voice assistant. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED RESTAURANT HOST: How may I help you? GOOGLE DUPLEX: Hi. I'd like to reserve a table for Wednesday the 7. UNIDENTIFIED RESTAURANT HOST: For seven people? GOOGLE DUPLEX: It's for four people. UNIDENTIFIED RESTAURANT HOST: Four people? When? Today? Tonight? GOOGLE DUPLEX: Wednesday at 6 p. m. UNIDENTIFIED RESTAURANT HOST: Actually, we reserve for upwards five people. For four people, you can come. GOOGLE DUPLEX: How long is the wait, usually, to be seated? UNIDENTIFIED RESTAURANT HOST: For when? Tomorrow, or a weekday, or. . . GOOGLE DUPLEX: For next Wednesday, the 7. UNIDENTIFIED RESTAURANT HOST: No. It's not too busy. You can come for four people. OK? GOOGLE DUPLEX: Oh - I gotcha. Thanks. UNIDENTIFIED RESTAURANT HOST: Bye-bye. (LAUGHTER, APPLAUSE) VEDANTAM: The audience is laughing and applauding because the man making the call isn't a man but a machine. PATEL: It didn't seem like it was a robotic voice. The robotic voices we're used to are the voices like when you are in a parking garage and you hear the, you know, please place your ticket with the stripe facing to the right - very, very canned sort of speech. This was far more sophisticated and much more like you and I talk, with hesitations and pauses, and ums and ahs. You think it's a human on the other end. VEDANTAM: Now, of course, one of the things about that voice that Google had is that it did seem like a convincing voice, but if you need to convince me that that voice is not just a human voice but a particular human's voice, you need to convince me now this is not just anyone calling for a restaurant reservation, but it's Barack Obama calling for a restaurant reservation. Presumably, now the bar is much, much higher. PATEL: That's right. It is. Barack Obama, though, does have a ton on his audio, (laughter), on the Internet. And there's a lot more audio to make, you know, his voice than there is my voice, for example. And so yeah. But it's absolutely possible to learn. And if you have long enough, you can learn anybody's voice. And if you have enough data. (SOUNDBITE OF MUSIC) VEDANTAM: It's not hard to see how bad actors could misuse this, create havoc in people's lives, trouble at companies, political misinformation. PATEL: That's absolutely - I mean, we're seeing deep fakes in video. We've seen, you know, President Obama's face with - being manipulated and the audio coming out. You know, people creating these fake media in video, and you're also seeing it audio. That's exactly why the security aspects of what we're doing are trying to detect is that fake audio, or is that real? Is part of that fake audio or is part of that real, right? So it is - this isn't completely sci fi. It isn't so far away. It isn't necessarily 20 - you know, 2028. It's probably 2020. So we've got to get our defenses up in terms of questioning where that - the authenticity of audio just as we do video. VEDANTAM: In 2017, a Canadian company called Lyrebird showed how audio deepfakes might work in politics. (SOUNDBITE OF MONTAGE) COMPUTER-GENERATED VOICE #1: (As Donald Trump) This is huge. They can make us say anything now - really anything. COMPUTER-GENERATED VOICE #9: (As Barack Obama) The good news is that they will offer the technology to anyone. COMPUTER-GENERATED VOICE #1: (As Donald Trump) This is huge. How does their technology work? COMPUTER-GENERATED VOICE #10: (As Hillary Clinton) Hey, guys, I think that they use deep learning and artificial neural networks. VEDANTAM: By 2019, deepfake audio technology had gotten even better. Shortly after critics panned the final season of \"Game Of Thrones,\" a YouTube channel called Eating Things With Famous People put out this tongue-in-cheek video showing the supposed remorse of the lead character, Jon Snow. (SOUNDBITE OF YOUTUBE VIDEO, \"BREAKING: JON SNOW FINALLY APOLOGIZED FOR SEASON 8\") COMPUTER-GENERATED VOICE #11: (As Jon Snow) It's time for some apologies. I'm sorry we wasted your time. I'm sorry we didn't learn anything from the ending of \"Lost. \" I have more lines in this video than I had in the last season. I'm sorry we wrote this in, like, six days or something. Now, let us burn the script of Season 8 and just forget it forever. VEDANTAM: Spoofing a TV show is one thing, but imagine such high-quality deepfakes occurring in a more high-stakes setting. Voices are increasingly being used by financial institutions to authenticate the identities of consumers. Recently, Rupal worked with a bank to assess how vulnerable it was to vocal hacking. PATEL: We tested their authentication system by creating synthetic samples or synthetic voices of particular individuals who are enrolled in their authentication system. And we tried to test those voices against the system to see if we could get through with the synthetic voices. And we were not able to do that for every single voice, but we were able to do it for some voices. And so it just starts to show that there is a vulnerability in this technology. VEDANTAM: So how would you guard against it? PATEL: Well, there are many ways to guard against it. One is you can classify the difference between is the audio signal I'm listening to - is it synthetic or is it human? As the synthetic voices become better and better sounding, that will be a more difficult decision to make. And it is something that if we can proactively solve, I think - or at least start to address - we're going to be way ahead of the curve than if we're trying to clean up our mess after the fact. VEDANTAM: Despite the potential risks of these new technologies, Rupal is also optimistic. Voice synthesis tools have the potential to allow people to craft the voice they hear on the outside so that it matches the identity they feel on the inside. PATEL: Ideally, in the future, these decisions are made by the end user themselves. Like, oh, I actually want that to be - sound a little breathier. I'd love that to sound a little bit more confident. And, I mean, how does that translate to the acoustics? We don't quite know yet, but that's actually I think where - when we can finally give the control of what the voice sounds like to the individual, I mean, that's the Holy Grail. (SOUNDBITE OF MONTAGE) VODER-GENERATED VOICE: She saw me. She saw me. She saw me. She saw me. COMPUTER-GENERATED VOICE #2: Hello. I am (unintelligible) machine. COMPUTER-GENERATED VOICE #6: You are listening to the voice of a machine. COMPUTER-GENERATED VOICE #3: (Singing) A, B, C, D, E, F, G, H, I, J, K. . . COMPUTER-GENERATED VOICE #7: The first kind of aid to be considered as a talking aid for the vocally handicapped. BETTY: I am beautiful Betty, the standard female voice. PAUL: I am the standard male voice, perfect Paul. COMPUTER-GENERATED VOICE #12: I was sad because there was no ice cream in the freezer. COMPUTER-GENERATED VOICE #13: The sky is clear and the stars are twinkling. MAEVE: I can't wait for my friends to hear my new voice. COMPUTER-GENERATED VOICE #3: (Singing) When we know our ABC's. ALEXA: Once cup is eight fluid ounces. COMPUTER-GENERATED VOICE #14: You are listening to a machine. COMPUTER-GENERATED VOICE #1: (As Donald Trump) This is huge. How does their technology work? COMPUTER-GENERATED VOICE #10: (As Hillary Clinton) Hey, guys, I think that they use deep learning and artificial neural networks. GOOGLE DUPLEX: Hi. I'd like to reserve a table for Wednesday the 7. (SOUNDBITE OF MUSIC) VEDANTAM: This week's show was produced by Thomas Lu. It was edited by Tara Boyle and Rhaina Cohen. Our team includes Parth Shah, Jenny Schmidt and Laura Kwerel. Special thanks to Brent Baughman, Greg Sauer and Kavon Jones (ph). (SOUNDBITE OF MUSIC) VEDANTAM: Our unsung hero this week is Rebecca Ralph (ph). She's part of NPR's team looking at our changing interactions with smart speakers. She helped us record some of the smart devices you heard in this week's episode. Thanks, Rebecca. If you liked this episode, please share it with a friend. We're always looking for new people to discover HIDDEN BRAIN. I'm Shankar Vedantam, and this is NPR.", "section": "Hidden Brain", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-15-741759995": {"title": "Alan Turing, Computing Genius And WWII Hero, To Be On U.K.'s New 50-Pound Note : NPR", "url": "https://www.npr.org/2019/07/15/741759995/alan-turing-wwii-hero-and-computing-genius-will-be-on-bank-of-englands-50-note", "author": "No author found", "published_date": "2019-07-15", "content": "ARI SHAPIRO, HOST: This morning, the Bank of England unveiled the design of a new 50-pound note. MARY LOUISE KELLY, HOST: It has blueprints for a code-breaking machine on a crimson background. SHAPIRO: A complex mathematical formula in white lettering. KELLY: And on top of it all, a portrait of a well-dressed man with dark hair parted on the side. (SOUNDBITE OF ARCHIVED RECORDING)MARK CARNEY: It's my great pleasure to announce that the scientist that will feature on the 50-pound note is Alan Turing. (APPLAUSE)KELLY: That is Mark Carney, governor of the Bank of England, making the announcement. SHAPIRO: Alan Turing is probably best known for cracking the Nazi code known as Enigma, a feat depicted in the 2014 film \"The Imitation Game. \"(SOUNDBITE OF FILM, \"THE IMITATION GAME\")BENEDICT CUMBERBATCH: (As Alan Turing) I'm designing a machine that will allow us to break every message, every day, instantly. KELLY: Turing is also considered the father of modern computing. ANDREW HODGES: He had to invent the idea of the computer as the thing on which you play out the computer program. KELLY: Andrew Hodges is a mathematical physicist at the University of Oxford. He wrote a biography of Turing. HODGES: And that sounds completely crazy, but it is more or less what he did and gives a flavor of the way in which he was so far ahead of his time. SHAPIRO: Turing's work was cut short in 1952, when he was convicted of gross indecency for having a sexual relationship with another man. He chose to be chemically castrated rather than serve jail time. And his security clearance was taken away. He died two years later, widely thought to have been suicide. KELLY: Turing was cleared of that conviction in 2013, and now, fully recognized for his brilliance, he will appear on the same currency note as the queen who pardoned him. ARI SHAPIRO, HOST:  This morning, the Bank of England unveiled the design of a new 50-pound note. MARY LOUISE KELLY, HOST:  It has blueprints for a code-breaking machine on a crimson background. SHAPIRO: A complex mathematical formula in white lettering. KELLY: And on top of it all, a portrait of a well-dressed man with dark hair parted on the side. (SOUNDBITE OF ARCHIVED RECORDING) MARK CARNEY: It's my great pleasure to announce that the scientist that will feature on the 50-pound note is Alan Turing. (APPLAUSE) KELLY: That is Mark Carney, governor of the Bank of England, making the announcement. SHAPIRO: Alan Turing is probably best known for cracking the Nazi code known as Enigma, a feat depicted in the 2014 film \"The Imitation Game. \" (SOUNDBITE OF FILM, \"THE IMITATION GAME\") BENEDICT CUMBERBATCH: (As Alan Turing) I'm designing a machine that will allow us to break every message, every day, instantly. KELLY: Turing is also considered the father of modern computing. ANDREW HODGES: He had to invent the idea of the computer as the thing on which you play out the computer program. KELLY: Andrew Hodges is a mathematical physicist at the University of Oxford. He wrote a biography of Turing. HODGES: And that sounds completely crazy, but it is more or less what he did and gives a flavor of the way in which he was so far ahead of his time. SHAPIRO: Turing's work was cut short in 1952, when he was convicted of gross indecency for having a sexual relationship with another man. He chose to be chemically castrated rather than serve jail time. And his security clearance was taken away. He died two years later, widely thought to have been suicide. KELLY: Turing was cleared of that conviction in 2013, and now, fully recognized for his brilliance, he will appear on the same currency note as the queen who pardoned him.", "section": "Europe", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-15-740726966": {"title": "2 LGBTQ Former Dell Workers Share Stories Of Alleged Discrimination : NPR", "url": "https://www.npr.org/2019/07/15/740726966/it-s-a-career-ender-2-lgbtq-former-dell-workers-share-their-stories", "author": "No author found", "published_date": "2019-07-15", "content": "NOEL KING, HOST: In this country, there's a growing fight for the rights of transgender people. Over 200 companies, including tech giants like Amazon and Google, have urged the U. S. Supreme Court to rule that federal civil rights law prohibit discrimination against gay and transgender workers. Now, recently, NPR learned of one woman's experience at the computer company Dell. She says she endured discrimination for years and filed a complaint with New York City's Commission on Human Rights. There are two other cases alleging discrimination at Dell. Dell denies liability in one case, and the other's ongoing. And Dell, we should note, is an NPR sponsor. Here's NPR's Jasmine Garsd. (SOUNDBITE OF MUSIC)JASMINE GARSD, BYLINE: Sometimes when she's frustrated, when she has some free time, Helen Harris comes here to blow off steam - a little warehouse Gym in Brooklyn. And these days Harris has too much free time and plenty of frustration, which led her to file a complaint. HELEN HARRIS: I know that my rights have been violated. GARSD: Harris is a 37-year-old systems engineer. Her job - selling technology to major companies and helping them set it up. NPR spoke to four colleagues Harris has worked with. They describe a talented young woman whose career was completely derailed a few years ago. And they, like Harris, suspect it's because of the way she looks. Harris is African American and gender-nonconforming. She was born and identifies as a woman. She uses feminine pronouns. She doesn't wear makeup or jewelry. She favors men's tailored suits and shoes and says, ultimately, that's nobody's business, which is why, in late 2015, when she started taking hormones to become more masculine-looking, she did it quietly. Besides, she always saw the tech industry as this place where, no matter what you look like. . . HARRIS: If you put your head down and you learn the stuff and you do the work, you can change your circumstances. GARSD: This was around the same time when Harris' company, EMC, was merging with Dell. Harris and several other employees were sent to training to step into new roles. She says that's when things started getting weird. She says she got heckled by co-workers when she gave presentations and that one of her instructors kept telling her, in order to work with customers. . . HARRIS: People have to like you for you to be able to do this job - he kept saying stuff like that to me. GARSD: Harris says her colleagues, the ones who were at that training where she had a hard time, they all moved up the ladder. She was told to keep on training. So she did - for three years. In her complaint to New York's Commission on Human Rights, Harris alleges that Dell didn't want to put her in front of customers because of how she looked. During that time, she says that she'd speak with managers, executives, colleagues, HR. HARRIS: What is the problem? Like, if there is a problem with me, like, what I'm doing, can someone please speak up? GARSD: So there's this idea that the tech industry is a place where stuff like this doesn't happen, where you can scooter into work, piercings, tattoos - no problem. As long as you work hard, it doesn't matter who you are. The diversity numbers at big tech companies like Google, Facebook and Apple tell a different story. Take Facebook - less than a quarter of tech roles there are held by women; African Americans make up only about 1%. At Dell, you get a complicated picture. On the one hand, the company has repeatedly been ranked among the 50 best for diversity. But several current and former Dell employees, who requested anonymity for fear of retaliation, said the New York office had the atmosphere of a boys club. A woman named Cicilia Gilbert, also a systems engineer at Dell, says it's not just New York - the tech sales culture in general can be brutal; she says it was for her. When Gilbert decided to transition from male to female, she says a trans co-worker warned her. CICILIA GILBERT: Don't tell these people that you're transgender; it's a career-ender. GARSD: Gilbert was devastated when, in late 2018, right in the middle of her transition, she was let go. GILBERT: They said, we're laying you off because your transgender transition is impeding your ability to travel. GARSD: Gilbert, who is 58, is suing Dell for, among other things, allegedly discriminating against her for her gender transition. The case is ongoing. It's not unusual for a company as large as Dell to have discrimination lawsuits. Jennifer Davis, a spokeswoman for Dell, told NPR that Gilbert's layoff had nothing to do with her gender; it was part of a restructuring where hundreds lost their jobs. And she pointed to the company's support network for trans employees. NPR spoke to two workers who say the extensive medical coverage and support Dell offers made their gender transition possible. But there was another case back in 2017 - the Massachusetts attorney general investigated a complaint from a former intern, also trans, who alleged discrimination. Dell denied wrongdoing but paid $110,000 in settlement. The last time I met with Harris at the gym in early June, she was still on Dell's payroll, but she wasn't even bothering with going into the office on a regular basis anymore. HARRIS: I have problems using the bathroom - that's the truth. After the second time I got harassed about which restroom I was using is when I stopped going. GARSD: Helen Harris says she's exhausted. A few weeks after we met, she quit. I asked Dell about her. Spokeswoman Jennifer Davis wouldn't provide details, saying she wishes to respect Harris' privacy. But she says the matter was resolved amicably. I asked Harris if she'd given up on a career in tech. She said, no way. HARRIS: I want my money. I don't want to be poor. Like, my father, he picked cotton. My grandfather was a sharecropper. I'm a systems engineer, so I'd rather stay. GARSD: She quit Dell, but she's not quitting tech. Jasmine Garsd, NPR News, New York. NOEL KING, HOST:  In this country, there's a growing fight for the rights of transgender people. Over 200 companies, including tech giants like Amazon and Google, have urged the U. S. Supreme Court to rule that federal civil rights law prohibit discrimination against gay and transgender workers. Now, recently, NPR learned of one woman's experience at the computer company Dell. She says she endured discrimination for years and filed a complaint with New York City's Commission on Human Rights. There are two other cases alleging discrimination at Dell. Dell denies liability in one case, and the other's ongoing. And Dell, we should note, is an NPR sponsor. Here's NPR's Jasmine Garsd. (SOUNDBITE OF MUSIC) JASMINE GARSD, BYLINE: Sometimes when she's frustrated, when she has some free time, Helen Harris comes here to blow off steam - a little warehouse Gym in Brooklyn. And these days Harris has too much free time and plenty of frustration, which led her to file a complaint. HELEN HARRIS: I know that my rights have been violated. GARSD: Harris is a 37-year-old systems engineer. Her job - selling technology to major companies and helping them set it up. NPR spoke to four colleagues Harris has worked with. They describe a talented young woman whose career was completely derailed a few years ago. And they, like Harris, suspect it's because of the way she looks. Harris is African American and gender-nonconforming. She was born and identifies as a woman. She uses feminine pronouns. She doesn't wear makeup or jewelry. She favors men's tailored suits and shoes and says, ultimately, that's nobody's business, which is why, in late 2015, when she started taking hormones to become more masculine-looking, she did it quietly. Besides, she always saw the tech industry as this place where, no matter what you look like. . . HARRIS: If you put your head down and you learn the stuff and you do the work, you can change your circumstances. GARSD: This was around the same time when Harris' company, EMC, was merging with Dell. Harris and several other employees were sent to training to step into new roles. She says that's when things started getting weird. She says she got heckled by co-workers when she gave presentations and that one of her instructors kept telling her, in order to work with customers. . . HARRIS: People have to like you for you to be able to do this job - he kept saying stuff like that to me. GARSD: Harris says her colleagues, the ones who were at that training where she had a hard time, they all moved up the ladder. She was told to keep on training. So she did - for three years. In her complaint to New York's Commission on Human Rights, Harris alleges that Dell didn't want to put her in front of customers because of how she looked. During that time, she says that she'd speak with managers, executives, colleagues, HR. HARRIS: What is the problem? Like, if there is a problem with me, like, what I'm doing, can someone please speak up? GARSD: So there's this idea that the tech industry is a place where stuff like this doesn't happen, where you can scooter into work, piercings, tattoos - no problem. As long as you work hard, it doesn't matter who you are. The diversity numbers at big tech companies like Google, Facebook and Apple tell a different story. Take Facebook - less than a quarter of tech roles there are held by women; African Americans make up only about 1%. At Dell, you get a complicated picture. On the one hand, the company has repeatedly been ranked among the 50 best for diversity. But several current and former Dell employees, who requested anonymity for fear of retaliation, said the New York office had the atmosphere of a boys club. A woman named Cicilia Gilbert, also a systems engineer at Dell, says it's not just New York - the tech sales culture in general can be brutal; she says it was for her. When Gilbert decided to transition from male to female, she says a trans co-worker warned her. CICILIA GILBERT: Don't tell these people that you're transgender; it's a career-ender. GARSD: Gilbert was devastated when, in late 2018, right in the middle of her transition, she was let go. GILBERT: They said, we're laying you off because your transgender transition is impeding your ability to travel. GARSD: Gilbert, who is 58, is suing Dell for, among other things, allegedly discriminating against her for her gender transition. The case is ongoing. It's not unusual for a company as large as Dell to have discrimination lawsuits. Jennifer Davis, a spokeswoman for Dell, told NPR that Gilbert's layoff had nothing to do with her gender; it was part of a restructuring where hundreds lost their jobs. And she pointed to the company's support network for trans employees. NPR spoke to two workers who say the extensive medical coverage and support Dell offers made their gender transition possible. But there was another case back in 2017 - the Massachusetts attorney general investigated a complaint from a former intern, also trans, who alleged discrimination. Dell denied wrongdoing but paid $110,000 in settlement. The last time I met with Harris at the gym in early June, she was still on Dell's payroll, but she wasn't even bothering with going into the office on a regular basis anymore. HARRIS: I have problems using the bathroom - that's the truth. After the second time I got harassed about which restroom I was using is when I stopped going. GARSD: Helen Harris says she's exhausted. A few weeks after we met, she quit. I asked Dell about her. Spokeswoman Jennifer Davis wouldn't provide details, saying she wishes to respect Harris' privacy. But she says the matter was resolved amicably. I asked Harris if she'd given up on a career in tech. She said, no way. HARRIS: I want my money. I don't want to be poor. Like, my father, he picked cotton. My grandfather was a sharecropper. I'm a systems engineer, so I'd rather stay. GARSD: She quit Dell, but she's not quitting tech. Jasmine Garsd, NPR News, New York.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-16-742386857": {"title": "Yelp CEO Discusses His Efforts To Convince Congress That Google Is A Monopoly : NPR", "url": "https://www.npr.org/2019/07/16/742386857/yelp-ceo-discusses-his-efforts-to-convince-congress-that-google-is-a-monopoly", "author": "No author found", "published_date": "2019-07-16", "content": "ARI SHAPIRO, HOST:  Today's events in Congress suggest that lawmakers are getting serious about tech giants. There are three hearings looking at how much power Amazon, Apple, Facebook and Google have. One company that has been pushing for this kind of oversight for years is Yelp. The review service has been saying that Google is trampling competitors, and Yelp CEO Jeremy Stoppelman joins us now. Welcome to ALL THINGS CONSIDERED. JEREMY STOPPELMAN: Thanks for having me. SHAPIRO: To start just briefly, why do you believe that Google is a monopoly? STOPPELMAN: Oh, well, I think it's obvious that Google is a monopoly. You really can't search the Web without turning to them. You know, they have the dominant browser, Chrome, which a lot of people search through. Of course, people go to Google directly. And then if you go on your iPhone, you even end up on Google when you type into that search box in Safari. SHAPIRO: And what does that mean for you, Yelp, a company that offers reviews of local businesses, restaurants, et cetera? STOPPELMAN: Well, over the years, Google has built out a similar property to Yelp, which is in Google Maps these days. And they actually bias their Web search such that their property, Google Maps, shows up first. And it makes it really hard to find Yelp these days in search results. SHAPIRO: But Google says that it is just giving users what they want, which is results to the question without having to go to another page - Yelp - and ask the question again. STOPPELMAN: That's a great point. And I think, you know, if a consumer is typing in 2 plus 2, they should absolutely see the answer is 4. But when you're looking for a pediatrician and your child is sick, you want the best results. You want to know, where is there a trusted doctor? And Google might not be able to provide that. That's dependent on the content that they've collected versus the content that other sites, like Yelp, might have. SHAPIRO: You were on Capitol Hill last week talking with lawmakers. Tell me about the kind of responses you got. STOPPELMAN: The mood in Washington is quite different from when we first started on this issue. You know, I began down the path of talking to regulators back in 2011, and there does seem to be a lot more political on both sides to really scrutinize Google, as well as other big tech firms. SHAPIRO: What do you think has changed? STOPPELMAN: I think, you know, in the wake of 2016. . . SHAPIRO: You mean the election interference. STOPPELMAN: Yeah. I mean, I just think people realize that just a couple companies control a massive amount of human attention. And the decisions that they're making - the algorithm decisions - actually affect what people think and what they see. And I think that's troubling to both sides. SHAPIRO: You've been focusing on Google, but presumably, anything Washington does would apply to the tech industry as a whole. Broadly speaking, what do you think a more fair world would look like in this arena? STOPPELMAN: You know, I think it's about creating a level playing field - making sure that if you're a monopoly, you can't block startups, you can't block innovative competitors and protect your monopoly. So in the case of Google, you know, it's actually come out in documents that they were fearful of Yelp in the local space. And so they actually degraded their search results knowingly to push their own product and make sure that that competitor, you know, stayed in its cage. SHAPIRO: Tell me about what regulation that you would be comfortable with would look like. I mean, what would the law actually do? STOPPELMAN: I mean, in our specific vertical, when you're searching for a pediatrician when a child is sick, we just think you should get the best of what's on the Web. And so if Google doesn't have the best answer - which, by the way, it often doesn't in local - it should be willing to surface, you know, other resources with the same prominence it surfaces its own content. SHAPIRO: That seems kind of subjective. I mean, it's hard to imagine Congress writing that into a law. STOPPELMAN: I don't think so. I mean, you know, Google itself has supported things like net neutrality. And so we simply ask the question, what about search neutrality? SHAPIRO: When you started this fight, you were kind of a lone wolf in Silicon Valley complaining about Google. Do you sense that other tech companies have started to join you in criticizing the big guys? STOPPELMAN: Well, we have been the lone wolf when it comes to speaking out on this issue. There is no shortage of companies that have issues with Google and its behavior. It's just that Google is so powerful and so many companies are reliant on Google in various ways that they simply can't speak out. Many of them come to me privately to talk about their issues. SHAPIRO: Europe has been far ahead of the U. S. in this movement. I mean, Europe has already fined Google for monopolistic behavior. Do you think the U. S. is likely to catch up with the Europeans? STOPPELMAN: I certainly hope so. Vestager in Europe, the competition chief there, has done a great job leading the way in identifying some of these issues far before U. S. regulators really took it seriously. And so that's great, but Europe still has more work to do. And you know, we are encouraged that U. S. regulators are finally paying attention. SHAPIRO: Jeremy Stoppelman is the CEO of Yelp. Thank you very much. STOPPELMAN: Thank you. SHAPIRO: And NPR also reached out to Google today. The company said they have no comment. ARI SHAPIRO, HOST:   Today's events in Congress suggest that lawmakers are getting serious about tech giants. There are three hearings looking at how much power Amazon, Apple, Facebook and Google have. One company that has been pushing for this kind of oversight for years is Yelp. The review service has been saying that Google is trampling competitors, and Yelp CEO Jeremy Stoppelman joins us now. Welcome to ALL THINGS CONSIDERED. JEREMY STOPPELMAN: Thanks for having me. SHAPIRO: To start just briefly, why do you believe that Google is a monopoly? STOPPELMAN: Oh, well, I think it's obvious that Google is a monopoly. You really can't search the Web without turning to them. You know, they have the dominant browser, Chrome, which a lot of people search through. Of course, people go to Google directly. And then if you go on your iPhone, you even end up on Google when you type into that search box in Safari. SHAPIRO: And what does that mean for you, Yelp, a company that offers reviews of local businesses, restaurants, et cetera? STOPPELMAN: Well, over the years, Google has built out a similar property to Yelp, which is in Google Maps these days. And they actually bias their Web search such that their property, Google Maps, shows up first. And it makes it really hard to find Yelp these days in search results. SHAPIRO: But Google says that it is just giving users what they want, which is results to the question without having to go to another page - Yelp - and ask the question again. STOPPELMAN: That's a great point. And I think, you know, if a consumer is typing in 2 plus 2, they should absolutely see the answer is 4. But when you're looking for a pediatrician and your child is sick, you want the best results. You want to know, where is there a trusted doctor? And Google might not be able to provide that. That's dependent on the content that they've collected versus the content that other sites, like Yelp, might have. SHAPIRO: You were on Capitol Hill last week talking with lawmakers. Tell me about the kind of responses you got. STOPPELMAN: The mood in Washington is quite different from when we first started on this issue. You know, I began down the path of talking to regulators back in 2011, and there does seem to be a lot more political on both sides to really scrutinize Google, as well as other big tech firms. SHAPIRO: What do you think has changed? STOPPELMAN: I think, you know, in the wake of 2016. . . SHAPIRO: You mean the election interference. STOPPELMAN: Yeah. I mean, I just think people realize that just a couple companies control a massive amount of human attention. And the decisions that they're making - the algorithm decisions - actually affect what people think and what they see. And I think that's troubling to both sides. SHAPIRO: You've been focusing on Google, but presumably, anything Washington does would apply to the tech industry as a whole. Broadly speaking, what do you think a more fair world would look like in this arena? STOPPELMAN: You know, I think it's about creating a level playing field - making sure that if you're a monopoly, you can't block startups, you can't block innovative competitors and protect your monopoly. So in the case of Google, you know, it's actually come out in documents that they were fearful of Yelp in the local space. And so they actually degraded their search results knowingly to push their own product and make sure that that competitor, you know, stayed in its cage. SHAPIRO: Tell me about what regulation that you would be comfortable with would look like. I mean, what would the law actually do? STOPPELMAN: I mean, in our specific vertical, when you're searching for a pediatrician when a child is sick, we just think you should get the best of what's on the Web. And so if Google doesn't have the best answer - which, by the way, it often doesn't in local - it should be willing to surface, you know, other resources with the same prominence it surfaces its own content. SHAPIRO: That seems kind of subjective. I mean, it's hard to imagine Congress writing that into a law. STOPPELMAN: I don't think so. I mean, you know, Google itself has supported things like net neutrality. And so we simply ask the question, what about search neutrality? SHAPIRO: When you started this fight, you were kind of a lone wolf in Silicon Valley complaining about Google. Do you sense that other tech companies have started to join you in criticizing the big guys? STOPPELMAN: Well, we have been the lone wolf when it comes to speaking out on this issue. There is no shortage of companies that have issues with Google and its behavior. It's just that Google is so powerful and so many companies are reliant on Google in various ways that they simply can't speak out. Many of them come to me privately to talk about their issues. SHAPIRO: Europe has been far ahead of the U. S. in this movement. I mean, Europe has already fined Google for monopolistic behavior. Do you think the U. S. is likely to catch up with the Europeans? STOPPELMAN: I certainly hope so. Vestager in Europe, the competition chief there, has done a great job leading the way in identifying some of these issues far before U. S. regulators really took it seriously. And so that's great, but Europe still has more work to do. And you know, we are encouraged that U. S. regulators are finally paying attention. SHAPIRO: Jeremy Stoppelman is the CEO of Yelp. Thank you very much. STOPPELMAN: Thank you. SHAPIRO: And NPR also reached out to Google today. The company said they have no comment.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-16-742308833": {"title": "Google's Search Bias On Trial In Washington : NPR", "url": "https://www.npr.org/2019/07/16/742308833/googles-search-bias-on-trial-in-washington", "author": "No author found", "published_date": "2019-07-16", "content": "", "section": "Technology", "disclaimer": ""}, "2019-07-16-742168185": {"title": "Tech Firms Face Lawmakers Over Antitrust, Digital Currency : NPR", "url": "https://www.npr.org/2019/07/16/742168185/tech-firms-to-face-lawmakers-over-antitrust-digital-currency", "author": "No author found", "published_date": "2019-07-16", "content": "MARY LOUISE KELLY, HOST:  Well, while the House is looking today at whether Google and other tech giants are too big, across Capitol Hill, senators are debating whether it would be OK if Facebook creates its own currency - safe to say Facebook has not convinced either Republicans or Democrats so far. Some lawmakers are downright hostile to the idea that a company that has broken public trust repeatedly, especially in its handling of user data, would venture into high-stakes global finance. NPR's Aarti Shahani reports on today's hearing before the Senate Banking Committee. AARTI SHAHANI, BYLINE: Facebook has proposed with partners to create digital money, a currency called the Libra. The move requires approval by regulators who are overseen by lawmakers who were on fire at today's hearing and using fire metaphors. (SOUNDBITE OF ARCHIVED RECORDING)SHERROD BROWN: Facebook is dangerous. SHAHANI: Senator Sherrod Brown, Ohio Democrat. . . (SOUNDBITE OF ARCHIVED RECORDING)BROWN: Like a toddler who has gotten his hands on a book of matches, Facebook has burned down the house over and over and called every arson a learning experience. SHAHANI: Money is a massive experiment in trust. We trust the dollar. We use it and don't burn it because we trust that it's backed by the full faith and credit of the United States government. Facebook does not have a great track record in trust, as David Marcus, the man at the company who oversees the new currency project, admitted repeatedly as he sat in the hot seat. (SOUNDBITE OF ARCHIVED RECORDING)DAVID MARCUS: Trust is primordial, and we've made mistakes in the past. And we have been working and are continuing to work really hard to get better. SHAHANI: Primordial, meaning from the beginning of time. The endearing origin story of Facebook, the miracle of capitalism - started in a college dorm - was absent. Senators offered an alternative origin story that early on, Facebook killed the local newspapers that keep those with power accountable; that by making hate speech go viral, it helped fuel genocide in the country Myanmar; that Facebook turned a blind eye to Russian interference in the 2016 elections; that the company has lied repeatedly about how it handles users' personal data. Senator Martha McSally, Arizona Republican. . . (SOUNDBITE OF ARCHIVED RECORDING)MARTHA MCSALLY: Mr. Marcus, I don't trust Facebook, and it's because of the repeated violations of your users' privacy, repeated deceit. And I am not alone. SHAHANI: Facebook's Marcus promised that if given permission to mint money, the company would monitor and report money laundering, but Facebook would not exploit users' spending data to target ads though Facebook has said one thing and done another before. The company promised to keep customer data private. Then, charged with lying, it entered a settlement with the Federal Trade Commission in 2011. And now caught up again, Facebook faces a $5 billion fine. Marcus made this case. Digital currency is not a data grab. Rather, it will make Facebook more popular by making it a shopping destination kind of like Amazon. As he tried to explain to Senator McSally, that is a service to an estimated 90 million businesses on Facebook. (SOUNDBITE OF ARCHIVED RECORDING)MARCUS: If they now have the ability to sell to more constituents across the Facebook platform and more people have access to the services and products of those 90 million businesses, then more commerce will happen on the Facebook platform. MCSALLY: Oh, that's wonderful public good that you guys are committed to. I know I'm way over time here, and that was extremely sarcastic. Thank you. SHAHANI: Timing matters. The announcement by Facebook that it plans to create its own currency may be too soon after Wall Street's financial crisis or too soon after Facebook's data scandals, which are still ongoing. Though Senator Pat Toomey, Pennsylvania Republican, did offer some encouragement for the tech giant's foray into digital currency. (SOUNDBITE OF ARCHIVED RECORDING)PAT TOOMEY: To announce in advance that we have to strangle this baby in the crib I think is wildly premature. SHAHANI: Tomorrow Facebook's Marcus appears before the House Financial Services Committee. This is one of a handful of hearings this week examining the power of big tech. Aarti Shahani, NPR News. MARY LOUISE KELLY, HOST:   Well, while the House is looking today at whether Google and other tech giants are too big, across Capitol Hill, senators are debating whether it would be OK if Facebook creates its own currency - safe to say Facebook has not convinced either Republicans or Democrats so far. Some lawmakers are downright hostile to the idea that a company that has broken public trust repeatedly, especially in its handling of user data, would venture into high-stakes global finance. NPR's Aarti Shahani reports on today's hearing before the Senate Banking Committee. AARTI SHAHANI, BYLINE: Facebook has proposed with partners to create digital money, a currency called the Libra. The move requires approval by regulators who are overseen by lawmakers who were on fire at today's hearing and using fire metaphors. (SOUNDBITE OF ARCHIVED RECORDING) SHERROD BROWN: Facebook is dangerous. SHAHANI: Senator Sherrod Brown, Ohio Democrat. . . (SOUNDBITE OF ARCHIVED RECORDING) BROWN: Like a toddler who has gotten his hands on a book of matches, Facebook has burned down the house over and over and called every arson a learning experience. SHAHANI: Money is a massive experiment in trust. We trust the dollar. We use it and don't burn it because we trust that it's backed by the full faith and credit of the United States government. Facebook does not have a great track record in trust, as David Marcus, the man at the company who oversees the new currency project, admitted repeatedly as he sat in the hot seat. (SOUNDBITE OF ARCHIVED RECORDING) DAVID MARCUS: Trust is primordial, and we've made mistakes in the past. And we have been working and are continuing to work really hard to get better. SHAHANI: Primordial, meaning from the beginning of time. The endearing origin story of Facebook, the miracle of capitalism - started in a college dorm - was absent. Senators offered an alternative origin story that early on, Facebook killed the local newspapers that keep those with power accountable; that by making hate speech go viral, it helped fuel genocide in the country Myanmar; that Facebook turned a blind eye to Russian interference in the 2016 elections; that the company has lied repeatedly about how it handles users' personal data. Senator Martha McSally, Arizona Republican. . . (SOUNDBITE OF ARCHIVED RECORDING) MARTHA MCSALLY: Mr. Marcus, I don't trust Facebook, and it's because of the repeated violations of your users' privacy, repeated deceit. And I am not alone. SHAHANI: Facebook's Marcus promised that if given permission to mint money, the company would monitor and report money laundering, but Facebook would not exploit users' spending data to target ads though Facebook has said one thing and done another before. The company promised to keep customer data private. Then, charged with lying, it entered a settlement with the Federal Trade Commission in 2011. And now caught up again, Facebook faces a $5 billion fine. Marcus made this case. Digital currency is not a data grab. Rather, it will make Facebook more popular by making it a shopping destination kind of like Amazon. As he tried to explain to Senator McSally, that is a service to an estimated 90 million businesses on Facebook. (SOUNDBITE OF ARCHIVED RECORDING) MARCUS: If they now have the ability to sell to more constituents across the Facebook platform and more people have access to the services and products of those 90 million businesses, then more commerce will happen on the Facebook platform. MCSALLY: Oh, that's wonderful public good that you guys are committed to. I know I'm way over time here, and that was extremely sarcastic. Thank you. SHAHANI: Timing matters. The announcement by Facebook that it plans to create its own currency may be too soon after Wall Street's financial crisis or too soon after Facebook's data scandals, which are still ongoing. Though Senator Pat Toomey, Pennsylvania Republican, did offer some encouragement for the tech giant's foray into digital currency. (SOUNDBITE OF ARCHIVED RECORDING) PAT TOOMEY: To announce in advance that we have to strangle this baby in the crib I think is wildly premature. SHAHANI: Tomorrow Facebook's Marcus appears before the House Financial Services Committee. This is one of a handful of hearings this week examining the power of big tech. Aarti Shahani, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-16-716693006": {"title": "Global Shipping Is About To Get Greener : NPR", "url": "https://www.npr.org/2019/07/16/716693006/the-dawn-of-low-carbon-shipping", "author": "No author found", "published_date": "2019-07-16", "content": "MARY LOUISE KELLY, HOST:  Time now for All Tech Considered. And today we are considering how technology could make shipping cleaner. The vast majority of the things we buy in the U. S. come here on ships, often from countries oceans away. These ships run on a particularly dirty kind of fuel called heavy fuel oil, or bunker fuel. As NPR's Rebecca Hersher reports, that may be changing. REBECCA HERSHER, BYLINE: The inside of a modern ship smells like oil, and everything is enormous. ALEKSEI BUZLOV: So let's go down. HERSHER: Aleksei Buzlov (ph) is the chief engineer on a coal-carrying ship that I visited when it stopped in Baltimore earlier this year. Its engine room was a cavern four or five stories tall. Wow. There were layers of catwalks around an engine with pistons as long as station wagons. And built into the walls were rows and rows of bulbous storage tanks. BUZLOV: Fuel tanks - this one, 60 tons; another one, 80 tons - small tanks. HERSHER: Small because, in all, the ship carries about 300 tons of heavy fuel. The fuel is darker and thicker than diesel or gasoline. It looks kind of like the engine oil for a car. Aleksei spends a lot of his time down here cleaning gunk out of engine parts. BUZLOV: This is slop. HERSHER: Slop, the dark grease that heavy fuel leaves behind on basically everything it touches. Oh, it's like frosting - or poop. BUZLOV: Don't touch. HERSHER: Keeping the ship's engine clean enough to run efficiently is a never-ending job. This ship burns an average of 50 tons of heavy fuel every day when it's on the ocean. And in addition to the slop, as it burns fuel, the engine emits CO2, carbon dioxide, and soot into the atmosphere. Add that up across all the thousands of ships crisscrossing the Earth every day, and it's a lot of pollution. NERIJUS POSKUS: If shipping was a country, it would be the sixth-largest polluter in the world. About 3% of global emissions are released by ocean freight shipping today. HERSHER: Nerijus Poskus is an analyst at the company Flexport. He says the shipping industry is growing so fast, it's projected to emit more than 15% of greenhouse gases by midcentury if ships keep burning heavy fuel oil, which is untenable if humans hope to avoid the worst effects of climate change. And the shipping industry appears to know that. POSKUS: Things are changing, and they are changing actually quite fast, finally. HERSHER: International regulators have tightened emissions standards for when ships are in port. And one big company, Maersk, has said they intend to make their entire fleet zero emission by 2050 - no greenhouse gases, which will require a new type of fuel. POSKUS: My personal opinion, hydrogen looks like the most promising way to power ships. HERSHER: Poskus thinks it will take at least a decade, partly because ships are designed to last about 30 years. But the first hydrogen-powered prototypes are being built now in just a few places on Earth. Norway is one of them. The other is Oakland, Calif. JOE PRATT: OK. So down here where he's working, you have the two holes. Right? HERSHER: Joe Pratt looks down at the shiny aluminum skeleton of a ship. It's on its way to being a passenger ferry, so you can see the outlines of what will be two catamaran hulls. PRATT: This is the floor. HERSHER: His company, Golden Gate Zero Emission Marine, is building it with money from the state of California. When it's finished in the fall, it will be the first hydrogen-powered vessel to operate in the U. S. Pratt says the technology is not particularly new. They're just proving that it can be used to power a boat. PRATT: Yeah, everything is off the shelf. HERSHER: One issue his team has had to figure out was how to make sure there was enough room for the hydrogen tanks and for the fuel cells. As big as internal combustion engines are, hydrogen fuel cells and their fuel will require even more space. One thing they don't have to worry about anymore, though - smelly exhaust. PRATT: It has an exhaust duct, which will have warm, humid air coming out. HERSHER: The moisture from the exhaust will be collected and used as drinking water onboard. Pratt says his company is already fielding questions from boat tour companies and others who are interested in either building new hydrogen-powered vessels or retrofitting their current ships. There is one more challenge, though, to achieve the goal of zero emissions. The hydrogen fuel that powers a vessel like this isn't necessarily clean. Lennie Klebanoff studies hydrogen fuel cells at Sandia National Lab. LENNIE KLEBANOFF: In the U. S. , about 90% of our hydrogen is made from methane. HERSHER: When you make hydrogen from methane, the process emits some CO2. So even though the hydrogen itself is clean burning, it's not really zero emissions. There are other ways to make hydrogen, though - from biomass, for example, or - cleanest of all - from water. KLEBANOFF: So water is H2O. You get the H's off of the H2O, and you're left with O, which is oxygen. HERSHER: The hope is if even a few companies start buying hydrogen-powered ships in the coming decade, the demand for clean hydrogen will go up, and fossil fuels will stop being the go-to for global shipping. Rebecca Hersher, NPR News. MARY LOUISE KELLY, HOST:   Time now for All Tech Considered. And today we are considering how technology could make shipping cleaner. The vast majority of the things we buy in the U. S. come here on ships, often from countries oceans away. These ships run on a particularly dirty kind of fuel called heavy fuel oil, or bunker fuel. As NPR's Rebecca Hersher reports, that may be changing. REBECCA HERSHER, BYLINE: The inside of a modern ship smells like oil, and everything is enormous. ALEKSEI BUZLOV: So let's go down. HERSHER: Aleksei Buzlov (ph) is the chief engineer on a coal-carrying ship that I visited when it stopped in Baltimore earlier this year. Its engine room was a cavern four or five stories tall. Wow. There were layers of catwalks around an engine with pistons as long as station wagons. And built into the walls were rows and rows of bulbous storage tanks. BUZLOV: Fuel tanks - this one, 60 tons; another one, 80 tons - small tanks. HERSHER: Small because, in all, the ship carries about 300 tons of heavy fuel. The fuel is darker and thicker than diesel or gasoline. It looks kind of like the engine oil for a car. Aleksei spends a lot of his time down here cleaning gunk out of engine parts. BUZLOV: This is slop. HERSHER: Slop, the dark grease that heavy fuel leaves behind on basically everything it touches. Oh, it's like frosting - or poop. BUZLOV: Don't touch. HERSHER: Keeping the ship's engine clean enough to run efficiently is a never-ending job. This ship burns an average of 50 tons of heavy fuel every day when it's on the ocean. And in addition to the slop, as it burns fuel, the engine emits CO2, carbon dioxide, and soot into the atmosphere. Add that up across all the thousands of ships crisscrossing the Earth every day, and it's a lot of pollution. NERIJUS POSKUS: If shipping was a country, it would be the sixth-largest polluter in the world. About 3% of global emissions are released by ocean freight shipping today. HERSHER: Nerijus Poskus is an analyst at the company Flexport. He says the shipping industry is growing so fast, it's projected to emit more than 15% of greenhouse gases by midcentury if ships keep burning heavy fuel oil, which is untenable if humans hope to avoid the worst effects of climate change. And the shipping industry appears to know that. POSKUS: Things are changing, and they are changing actually quite fast, finally. HERSHER: International regulators have tightened emissions standards for when ships are in port. And one big company, Maersk, has said they intend to make their entire fleet zero emission by 2050 - no greenhouse gases, which will require a new type of fuel. POSKUS: My personal opinion, hydrogen looks like the most promising way to power ships. HERSHER: Poskus thinks it will take at least a decade, partly because ships are designed to last about 30 years. But the first hydrogen-powered prototypes are being built now in just a few places on Earth. Norway is one of them. The other is Oakland, Calif. JOE PRATT: OK. So down here where he's working, you have the two holes. Right? HERSHER: Joe Pratt looks down at the shiny aluminum skeleton of a ship. It's on its way to being a passenger ferry, so you can see the outlines of what will be two catamaran hulls. PRATT: This is the floor. HERSHER: His company, Golden Gate Zero Emission Marine, is building it with money from the state of California. When it's finished in the fall, it will be the first hydrogen-powered vessel to operate in the U. S. Pratt says the technology is not particularly new. They're just proving that it can be used to power a boat. PRATT: Yeah, everything is off the shelf. HERSHER: One issue his team has had to figure out was how to make sure there was enough room for the hydrogen tanks and for the fuel cells. As big as internal combustion engines are, hydrogen fuel cells and their fuel will require even more space. One thing they don't have to worry about anymore, though - smelly exhaust. PRATT: It has an exhaust duct, which will have warm, humid air coming out. HERSHER: The moisture from the exhaust will be collected and used as drinking water onboard. Pratt says his company is already fielding questions from boat tour companies and others who are interested in either building new hydrogen-powered vessels or retrofitting their current ships. There is one more challenge, though, to achieve the goal of zero emissions. The hydrogen fuel that powers a vessel like this isn't necessarily clean. Lennie Klebanoff studies hydrogen fuel cells at Sandia National Lab. LENNIE KLEBANOFF: In the U. S. , about 90% of our hydrogen is made from methane. HERSHER: When you make hydrogen from methane, the process emits some CO2. So even though the hydrogen itself is clean burning, it's not really zero emissions. There are other ways to make hydrogen, though - from biomass, for example, or - cleanest of all - from water. KLEBANOFF: So water is H2O. You get the H's off of the H2O, and you're left with O, which is oxygen. HERSHER: The hope is if even a few companies start buying hydrogen-powered ships in the coming decade, the demand for clean hydrogen will go up, and fossil fuels will stop being the go-to for global shipping. Rebecca Hersher, NPR News.", "section": "Getting To Zero Carbon: The Climate Challenge", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-16-742109310": {"title": "Congress' Power Struggle With Big Tech Will Be On Display At Hearings : NPR", "url": "https://www.npr.org/2019/07/16/742109310/congress-power-struggle-with-big-tech-will-be-on-display-at-hearings", "author": "No author found", "published_date": "2019-07-16", "content": "STEVE INSKEEP, HOST:  How much power does the United States want a few tech companies to have? That is the overriding question - multiple congressional hearings today. In one of those hearings, Ted Cruz will pursue conservative allegations that Google searches are biased. Another hearing questions whether the biggest tech companies are monopolies. And then there's a hearing about Facebook, which is acting a little like a country by starting its own currency. NPR's Aarti Shahani will be listening throughout the day. Hi there, Aarti. AARTI SHAHANI, BYLINE: Hi. INSKEEP: What is Facebook's reason - stated reason for starting a currency? SHAHANI: Well, you may have heard Facebook does plan to create its own money called the Libra. And the stated goal is to bank the unbanked. An estimated 1. 7 billion people on Earth do not have a bank account. Many of them are Facebook users. So there's an altruistic spin here. Facebook is going to create its own currency to help them, but thereby taking on a key power of nation states, the power to mint money. INSKEEP: OK. That sounds really great, being innovative and giving people the power of currency. Although, of course, it also gives a certain amount of power to Facebook, which gets to monitor those transactions, monetize those transactions. What happens to some of the different ways that you can make money off of money, if Facebook is doing this? SHAHANI: Right. Well, it definitely raises a lot of concerns, for example, about, you know, what do you do about money laundering? How will the company and its partners protect privacy? What happens to the interest earned on Libra deposits? This stuff is not clear yet. It's in the works. And regulators are speaking up. OK, they are striking very different tones here as well. France's finance minister came out saying this can't and it must not happen. Facebook and its partners shouldn't have a sovereign currency. The U. K. minister says his country is willing to engage with Facebook. And U. S. Treasury Secretary Steve Mnuchin made it clear at a briefing yesterday that he is not comfortable with Facebook launching a currency, at least not yet. (SOUNDBITE OF ARCHIVED RECORDING)STEVEN MNUCHIN: I think they're being very candid with the administration and where they are. I'm not going to publicly speculate how long I think it will take them to get to the point where we're comfortable with it, but they are a long way away. INSKEEP: So you heard him say they've been very candid with the administration. What is Facebook saying to answer concerns? SHAHANI: Well, today the company is going to send David Marcus to testify. He - he's heading the creation of the Libra wallet. He used to head PayPal. And his testimony, which he submitted, it's really diplomatic. He's going to say Facebook won't launch Libra until his team has fully addressed regulatory concerns and received appropriate approvals. That's a real departure from how the most powerful tech companies have been approaching government these last few years. You know, you'll recall Uber, for example, went into cities and opened up shop and hid from regulators who wanted to stop them. Facebook and Google have been criticized for avoiding responsibility in Europe for the hate speech that's gone viral on their platforms. But finance, it's a very regulation-heavy industry. And Facebook is at least talking the talk here. INSKEEP: And maybe Facebook is also aware that they, along with other big tech companies, are being suspected of becoming monopolies, a subject of yet another hearing. SHAHANI: That's right. That's right. Today, over in the House Judiciary Committee, they're going to explore anti-trust issues. Members are going to want to drill into how Facebook, Google, Amazon and Apple each works, how they each deal with their competitors and collaborators. And here's the thing. These companies have differences. For example, Amazon and Google both sell advertising. But Amazon's ad money is basically a fee to vendors for them to show up in Amazon search pages. Google is not that. But it is in the companies' interest to close rank, to not throw each other under the bus and to make the conversation go away. INSKEEP: NPR's Aarti Shahani, thanks so much. SHAHANI: Thank you. STEVE INSKEEP, HOST:   How much power does the United States want a few tech companies to have? That is the overriding question - multiple congressional hearings today. In one of those hearings, Ted Cruz will pursue conservative allegations that Google searches are biased. Another hearing questions whether the biggest tech companies are monopolies. And then there's a hearing about Facebook, which is acting a little like a country by starting its own currency. NPR's Aarti Shahani will be listening throughout the day. Hi there, Aarti. AARTI SHAHANI, BYLINE: Hi. INSKEEP: What is Facebook's reason - stated reason for starting a currency? SHAHANI: Well, you may have heard Facebook does plan to create its own money called the Libra. And the stated goal is to bank the unbanked. An estimated 1. 7 billion people on Earth do not have a bank account. Many of them are Facebook users. So there's an altruistic spin here. Facebook is going to create its own currency to help them, but thereby taking on a key power of nation states, the power to mint money. INSKEEP: OK. That sounds really great, being innovative and giving people the power of currency. Although, of course, it also gives a certain amount of power to Facebook, which gets to monitor those transactions, monetize those transactions. What happens to some of the different ways that you can make money off of money, if Facebook is doing this? SHAHANI: Right. Well, it definitely raises a lot of concerns, for example, about, you know, what do you do about money laundering? How will the company and its partners protect privacy? What happens to the interest earned on Libra deposits? This stuff is not clear yet. It's in the works. And regulators are speaking up. OK, they are striking very different tones here as well. France's finance minister came out saying this can't and it must not happen. Facebook and its partners shouldn't have a sovereign currency. The U. K. minister says his country is willing to engage with Facebook. And U. S. Treasury Secretary Steve Mnuchin made it clear at a briefing yesterday that he is not comfortable with Facebook launching a currency, at least not yet. (SOUNDBITE OF ARCHIVED RECORDING) STEVEN MNUCHIN: I think they're being very candid with the administration and where they are. I'm not going to publicly speculate how long I think it will take them to get to the point where we're comfortable with it, but they are a long way away. INSKEEP: So you heard him say they've been very candid with the administration. What is Facebook saying to answer concerns? SHAHANI: Well, today the company is going to send David Marcus to testify. He - he's heading the creation of the Libra wallet. He used to head PayPal. And his testimony, which he submitted, it's really diplomatic. He's going to say Facebook won't launch Libra until his team has fully addressed regulatory concerns and received appropriate approvals. That's a real departure from how the most powerful tech companies have been approaching government these last few years. You know, you'll recall Uber, for example, went into cities and opened up shop and hid from regulators who wanted to stop them. Facebook and Google have been criticized for avoiding responsibility in Europe for the hate speech that's gone viral on their platforms. But finance, it's a very regulation-heavy industry. And Facebook is at least talking the talk here. INSKEEP: And maybe Facebook is also aware that they, along with other big tech companies, are being suspected of becoming monopolies, a subject of yet another hearing. SHAHANI: That's right. That's right. Today, over in the House Judiciary Committee, they're going to explore anti-trust issues. Members are going to want to drill into how Facebook, Google, Amazon and Apple each works, how they each deal with their competitors and collaborators. And here's the thing. These companies have differences. For example, Amazon and Google both sell advertising. But Amazon's ad money is basically a fee to vendors for them to show up in Amazon search pages. Google is not that. But it is in the companies' interest to close rank, to not throw each other under the bus and to make the conversation go away. INSKEEP: NPR's Aarti Shahani, thanks so much. SHAHANI: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-16-717487081": {"title": "WATCH: In The Future, We Could Control Our Tech With Our Minds : NPR", "url": "https://www.npr.org/2019/07/16/717487081/video-move-objects-with-your-mind-were-getting-there-with-the-help-of-an-armband", "author": "No author found", "published_date": "2019-07-16", "content": "STEVE INSKEEP, HOST:  You know how in \"Star Wars,\" Jedi Knights can pick things up and make things move just by using their minds using the force? There are some scientists and computer programmers out there who are developing digital technology so that you can do that. This is the subject of the latest episode of our video series Future You, and Elise Hu has been trying it out. Hi there, Elise. ELISE HU, BYLINE: Good morning. INSKEEP: How would it be possible in the real world to make things move just by thinking about them moving? HU: By reading the nerve impulses from your arm. So I went to the offices of a company called CTRL-labs, where they've developed an armband. It's the size of a wristwatch. And that can read the nerve impulses in your arm. And when you put it on, just by moving your hand or flicking a finger, you can make things happen on a screen or even in real life with physical objects. INSKEEP: In real life - like what? HU: OK. First, let's take the screen example. Picture a video game on the screen where there's images of objects, a whole world on there. And with this armband on, I could make a motion with my hand. . . JESSLYN TANNADY: You pull like this, kind of like, I want this to, like, come towards me. . . HU: . . . Like I was grabbing something and picking it up or even pushing it away. And, on the screen, the object would respond. The engineer there, Jesslyn Tannady, taught me how to do this. TANNADY: Try pushing something far away using open hand. HU: Hold on. Oh. Oh. . . TANNADY: You got it. You got it. HU: OK. TANNADY: All right. Try - oh. Oh. . . HU: What am I - oh. TANNADY: Towards sky. Towards sky. Towards sky (laughter). INSKEEP: People will be a little familiar with the concept because they've seen video games like Xbox, where the device somehow reads your motion. But here, it's reading your thoughts? HU: That's correct. So this is a lot more sophisticated. And the other thing, Steve, is they've taken this beyond the digital world. They've devised ways that this armband will let you actually move physical objects - for instance, a robot. So this guy - this y'all call the hexapod? TANNADY: This is our little hexapod. HU: They have this little robot that looks like a mechanical spider, about the size of a football. And each of its legs represented one of my fingers. So I could open my hand to make it walk. Or if I held that robot behind my back and lined its legs kind of near my back there, I could scratch my own back. TANNADY: Give yourself back scratches. HU: OK. Hold on. Oh, whoa (laughter). INSKEEP: So assuming this works on a larger and larger scale, what does it mean for human beings? HU: The founder of this company says he wants this nerve impulse reading to fundamentally change the way we actually interface with our devices and machines. His name is Thomas Reardon. THOMAS REARDON: So, yes, an extension of you where interacting with a computer and a machine no longer feels like something you're doing mechanically, but instead is just a fluid extension of your thoughts and subtle movements. HU: So what he's saying there, Steve, is no more typing, no more swiping on your phone or even voice control. REARDON: All these things that we think of as the internet of things that all have their perverse little interfaces, like the Nest thermostat on the wall. . . HU: Right. Right. REARDON: . . . There's no reason for me to go up and touch it and move it. I ought to be able to just look at it and change the temperature. INSKEEP: Sounds powerful and amazing. But in this series, Future You, Elise, you always ask what the downsides of things are. HU: Right. And privacy is a huge concern here because your neural identity is so specific to you. So the signals that we generate neurologically are the most unique identifiers of ourselves. Reardon puts it this way. REARDON: Give us 30 seconds of recordings from a person, and we can identify that person for the rest of their lives. And we have to make sure that people who might have the opportunity to exploit it, to track you and otherwise pervert your superpowers don't have an easy way to go do that. INSKEEP: It's like a fingerprint, only it's a neuro-print? HU: Exactly. And as with a lot of technology, we're making our lives a lot more fluid this way, and it would enhance what we can do. But it's opening up a lot of questions as we go forward too. INSKEEP: What can I do with this technology right now? HU: CTRL-labs is going to release a developer's kit. Other tech designers can make their device work with the armband. So maybe an auto engineer would be able to make a car that responds to your arm movements. And you'll be able to summon that car, or you could change your thermostat, like Reardon says. INSKEEP: Thanks, Elise. HU: You bet. INSKEEP: You can check out Elise trying the force - or more accurately, this CTRL-kit armband - in her latest episode of Future You with Elise Hu. It's at npr. org. (SOUNDBITE OF LONDON SYMPHONY ORCHESTRA'S \"THE HOLOGRAM/BINARY SUNSET\" STEVE INSKEEP, HOST:   You know how in \"Star Wars,\" Jedi Knights can pick things up and make things move just by using their minds using the force? There are some scientists and computer programmers out there who are developing digital technology so that you can do that. This is the subject of the latest episode of our video series Future You, and Elise Hu has been trying it out. Hi there, Elise. ELISE HU, BYLINE: Good morning. INSKEEP: How would it be possible in the real world to make things move just by thinking about them moving? HU: By reading the nerve impulses from your arm. So I went to the offices of a company called CTRL-labs, where they've developed an armband. It's the size of a wristwatch. And that can read the nerve impulses in your arm. And when you put it on, just by moving your hand or flicking a finger, you can make things happen on a screen or even in real life with physical objects. INSKEEP: In real life - like what? HU: OK. First, let's take the screen example. Picture a video game on the screen where there's images of objects, a whole world on there. And with this armband on, I could make a motion with my hand. . . JESSLYN TANNADY: You pull like this, kind of like, I want this to, like, come towards me. . . HU: . . . Like I was grabbing something and picking it up or even pushing it away. And, on the screen, the object would respond. The engineer there, Jesslyn Tannady, taught me how to do this. TANNADY: Try pushing something far away using open hand. HU: Hold on. Oh. Oh. . . TANNADY: You got it. You got it. HU: OK. TANNADY: All right. Try - oh. Oh. . . HU: What am I - oh. TANNADY: Towards sky. Towards sky. Towards sky (laughter). INSKEEP: People will be a little familiar with the concept because they've seen video games like Xbox, where the device somehow reads your motion. But here, it's reading your thoughts? HU: That's correct. So this is a lot more sophisticated. And the other thing, Steve, is they've taken this beyond the digital world. They've devised ways that this armband will let you actually move physical objects - for instance, a robot. So this guy - this y'all call the hexapod? TANNADY: This is our little hexapod. HU: They have this little robot that looks like a mechanical spider, about the size of a football. And each of its legs represented one of my fingers. So I could open my hand to make it walk. Or if I held that robot behind my back and lined its legs kind of near my back there, I could scratch my own back. TANNADY: Give yourself back scratches. HU: OK. Hold on. Oh, whoa (laughter). INSKEEP: So assuming this works on a larger and larger scale, what does it mean for human beings? HU: The founder of this company says he wants this nerve impulse reading to fundamentally change the way we actually interface with our devices and machines. His name is Thomas Reardon. THOMAS REARDON: So, yes, an extension of you where interacting with a computer and a machine no longer feels like something you're doing mechanically, but instead is just a fluid extension of your thoughts and subtle movements. HU: So what he's saying there, Steve, is no more typing, no more swiping on your phone or even voice control. REARDON: All these things that we think of as the internet of things that all have their perverse little interfaces, like the Nest thermostat on the wall. . . HU: Right. Right. REARDON: . . . There's no reason for me to go up and touch it and move it. I ought to be able to just look at it and change the temperature. INSKEEP: Sounds powerful and amazing. But in this series, Future You, Elise, you always ask what the downsides of things are. HU: Right. And privacy is a huge concern here because your neural identity is so specific to you. So the signals that we generate neurologically are the most unique identifiers of ourselves. Reardon puts it this way. REARDON: Give us 30 seconds of recordings from a person, and we can identify that person for the rest of their lives. And we have to make sure that people who might have the opportunity to exploit it, to track you and otherwise pervert your superpowers don't have an easy way to go do that. INSKEEP: It's like a fingerprint, only it's a neuro-print? HU: Exactly. And as with a lot of technology, we're making our lives a lot more fluid this way, and it would enhance what we can do. But it's opening up a lot of questions as we go forward too. INSKEEP: What can I do with this technology right now? HU: CTRL-labs is going to release a developer's kit. Other tech designers can make their device work with the armband. So maybe an auto engineer would be able to make a car that responds to your arm movements. And you'll be able to summon that car, or you could change your thermostat, like Reardon says. INSKEEP: Thanks, Elise. HU: You bet. INSKEEP: You can check out Elise trying the force - or more accurately, this CTRL-kit armband - in her latest episode of Future You with Elise Hu. It's at npr. org. (SOUNDBITE OF LONDON SYMPHONY ORCHESTRA'S \"THE HOLOGRAM/BINARY SUNSET\"", "section": "Video: Future You, With Elise Hu", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-17-742910309": {"title": "Democrat Calls For Investigation Into Viral Russian-Developed FaceApp : NPR", "url": "https://www.npr.org/2019/07/17/742910309/democrats-issue-warnings-against-viral-russia-based-face-morphing-app", "author": "No author found", "published_date": "2019-07-17", "content": "SCOTT SIMON, HOST: If you've seen photos of friends or celebrities posting selfies with silver hair and wrinkles lately, it's not the relentless news cycle. They probably used FaceApp. The app can make your face look older, younger, even style your hair. It can be fun, but lawmakers warn users to take precaution. The Democratic National Committee warned presidential campaigns against using it, and Senate Minority Leader Chuck Schumer has called on the FBI to investigate. He posted this on social media about FaceApp. (SOUNDBITE OF ARCHIVED RECORDING)CHUCK SCHUMER: It allows, quote, \"perpetual, irrevocable and worldwide license to your photos, name or likeness. \" So this is a breathtaking and possibly dangerous level of access. SIMON: Louise Matsakis is a staff writer at Wired magazine and joins us from New York. Thanks so much for being with us. LOUISE MATSAKIS: Thanks for having me. SIMON: In your judgment, is FaceApp really more compromising than Facebook or Instagram, who have billions of users already? MATSAKIS: Absolutely not. I think that one thing to consider here is that this is a much smaller company, and Facebook has argued in the past that they're better at protecting your data because they, you know, inherently have more resources. But I don't think that this FaceApp is necessarily more controversial or troubling because it happens to be made by developers who are Russian. I think that's where kind of the anxiety from lawmakers comes from here. FaceApp, like a lot of these free apps, their business model's around advertising. You know, they're sending some of your data to the Google ad networks. That's what they want here. That doesn't mean it's not troubling for other reasons, but I think lawmakers here are maybe fixating on the wrong issue. SIMON: Why is it troubling for other reasons? MATSAKIS: I think we should be really careful about who we want to share our faces with. This information is really sensitive. It's not something, like Schumer pointed out, that they're going to get back to you. You know, they maintain the rights to these images, and they can do what they want with them. But the reality is we're sharing our facial recognition data with so many companies. And a lot of time, we don't have a lot of control over that. So it's really not just this app, even though it's, you know, called FaceApp, so it seems really literal. But when you upload pictures to dating sites, to Facebook, to, you know, photo-sharing sites like Flickr or Instagram, you're probably in a facial recognition database and you don't know it already. SIMON: I got to tell you. I've been taking pictures of our 16-year-old since she came into our lives. Is it already too late for her? MATSAKIS: Probably. And it's too late for what Georgetown University estimates is probably over half of Americans who are already in a facial recognition database as it stands. You know, that includes when you get a license, right? You know, that license database might be used by the government to create a facial recognition algorithm that includes maybe a dating profile that might be used to identify your gender or something. You know, researchers from Stanford in 2017 actually took photos from a dating network, and they used it to build a very controversial tool that claimed to identify people's sexuality just based on their photo. So, you know, the people who are on that dating site had no idea something like that was going to happen. SIMON: Do companies ever lose control of the data? I mean, that happens with credit card information all the time, doesn't it? MATSAKIS: Sure. And it's not even just losing control. A lot of time, these databases are widely available or they're made public to academic researchers and to corporations in order to foster more scholarship. So a lot of these photos are just out there even if there isn't a data breach to begin with. These big training sets are already available to a, you know, wide array of different actors who want to use them to build tools that they can use to do whatever they want. SIMON: Is there any way to stop this, any way to take steps, even if it's too late to protect your images? MATSAKIS: Definitely. I think making sure that your profiles are not public is a good way of doing that. That's really hard because dating profiles, for instance, are inherently public, right? You want other people to see them. But I think really thinking about, do I need to upload this picture right now, is this worth the risk that it poses by uploading it, do I really need to post a lot of pictures of me maybe publicly on Twitter or something like that, or would I rather just have kind of a private Facebook group where I share pictures of my daughter? SIMON: Even if FaceApp isn't as dangerous as some political figures caution, is it good that this conversation - if you please - has been set off? MATSAKIS: Definitely. I think this is really great that this conversation is centering on your face, which is such a valuable piece of data for these companies that are building these algorithms, often for troubling purposes, like law enforcement or that sort of thing, or identifying people and perhaps identifying you. And I think that's really an important part of this conversation, even though maybe this app is a little silly. SIMON: When you say it's troubling that law enforcement could use it, I mean, if you're looking for someone who committed a violent crime against someone you love, you're glad to have that technology. On the other hand, if you're a dissident in Hong Kong, you're not glad to have that technology. MATSAKIS: Exactly. I think these conversations always involve a trade-off between privacy and security. And we kind of haven't really figured out that balance with facial recognition data yet. And I think another concern is that a lot of these algorithms have been proven to be inaccurate. You know, people have been arrested for crimes. . . SIMON: Yeah. MATSAKIS: . . . They didn't commit because the algorithm, you know, mistakenly identified them. So I think that's another piece that's really important here. SIMON: Louise Matsakis of Wired, thanks so much for being with us. MATSAKIS: Thanks again. SCOTT SIMON, HOST:  If you've seen photos of friends or celebrities posting selfies with silver hair and wrinkles lately, it's not the relentless news cycle. They probably used FaceApp. The app can make your face look older, younger, even style your hair. It can be fun, but lawmakers warn users to take precaution. The Democratic National Committee warned presidential campaigns against using it, and Senate Minority Leader Chuck Schumer has called on the FBI to investigate. He posted this on social media about FaceApp. (SOUNDBITE OF ARCHIVED RECORDING) CHUCK SCHUMER: It allows, quote, \"perpetual, irrevocable and worldwide license to your photos, name or likeness. \" So this is a breathtaking and possibly dangerous level of access. SIMON: Louise Matsakis is a staff writer at Wired magazine and joins us from New York. Thanks so much for being with us. LOUISE MATSAKIS: Thanks for having me. SIMON: In your judgment, is FaceApp really more compromising than Facebook or Instagram, who have billions of users already? MATSAKIS: Absolutely not. I think that one thing to consider here is that this is a much smaller company, and Facebook has argued in the past that they're better at protecting your data because they, you know, inherently have more resources. But I don't think that this FaceApp is necessarily more controversial or troubling because it happens to be made by developers who are Russian. I think that's where kind of the anxiety from lawmakers comes from here. FaceApp, like a lot of these free apps, their business model's around advertising. You know, they're sending some of your data to the Google ad networks. That's what they want here. That doesn't mean it's not troubling for other reasons, but I think lawmakers here are maybe fixating on the wrong issue. SIMON: Why is it troubling for other reasons? MATSAKIS: I think we should be really careful about who we want to share our faces with. This information is really sensitive. It's not something, like Schumer pointed out, that they're going to get back to you. You know, they maintain the rights to these images, and they can do what they want with them. But the reality is we're sharing our facial recognition data with so many companies. And a lot of time, we don't have a lot of control over that. So it's really not just this app, even though it's, you know, called FaceApp, so it seems really literal. But when you upload pictures to dating sites, to Facebook, to, you know, photo-sharing sites like Flickr or Instagram, you're probably in a facial recognition database and you don't know it already. SIMON: I got to tell you. I've been taking pictures of our 16-year-old since she came into our lives. Is it already too late for her? MATSAKIS: Probably. And it's too late for what Georgetown University estimates is probably over half of Americans who are already in a facial recognition database as it stands. You know, that includes when you get a license, right? You know, that license database might be used by the government to create a facial recognition algorithm that includes maybe a dating profile that might be used to identify your gender or something. You know, researchers from Stanford in 2017 actually took photos from a dating network, and they used it to build a very controversial tool that claimed to identify people's sexuality just based on their photo. So, you know, the people who are on that dating site had no idea something like that was going to happen. SIMON: Do companies ever lose control of the data? I mean, that happens with credit card information all the time, doesn't it? MATSAKIS: Sure. And it's not even just losing control. A lot of time, these databases are widely available or they're made public to academic researchers and to corporations in order to foster more scholarship. So a lot of these photos are just out there even if there isn't a data breach to begin with. These big training sets are already available to a, you know, wide array of different actors who want to use them to build tools that they can use to do whatever they want. SIMON: Is there any way to stop this, any way to take steps, even if it's too late to protect your images? MATSAKIS: Definitely. I think making sure that your profiles are not public is a good way of doing that. That's really hard because dating profiles, for instance, are inherently public, right? You want other people to see them. But I think really thinking about, do I need to upload this picture right now, is this worth the risk that it poses by uploading it, do I really need to post a lot of pictures of me maybe publicly on Twitter or something like that, or would I rather just have kind of a private Facebook group where I share pictures of my daughter? SIMON: Even if FaceApp isn't as dangerous as some political figures caution, is it good that this conversation - if you please - has been set off? MATSAKIS: Definitely. I think this is really great that this conversation is centering on your face, which is such a valuable piece of data for these companies that are building these algorithms, often for troubling purposes, like law enforcement or that sort of thing, or identifying people and perhaps identifying you. And I think that's really an important part of this conversation, even though maybe this app is a little silly. SIMON: When you say it's troubling that law enforcement could use it, I mean, if you're looking for someone who committed a violent crime against someone you love, you're glad to have that technology. On the other hand, if you're a dissident in Hong Kong, you're not glad to have that technology. MATSAKIS: Exactly. I think these conversations always involve a trade-off between privacy and security. And we kind of haven't really figured out that balance with facial recognition data yet. And I think another concern is that a lot of these algorithms have been proven to be inaccurate. You know, people have been arrested for crimes. . . SIMON: Yeah. MATSAKIS: . . . They didn't commit because the algorithm, you know, mistakenly identified them. So I think that's another piece that's really important here. SIMON: Louise Matsakis of Wired, thanks so much for being with us. MATSAKIS: Thanks again.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-17-742615426": {"title": "Does Amazon Hurt Competition By Using Sellers' Data? Europe Investigates : NPR", "url": "https://www.npr.org/2019/07/17/742615426/eu-investigates-if-amazon-hurts-competition-by-using-sellers-data", "author": "No author found", "published_date": "2019-07-17", "content": "MARY LOUISE KELLY, HOST:  The European Union opened an antitrust investigation today into Amazon. The EU regulators' central question is this. Does the Internet giant stifle competition by exploiting data it collects from other companies for its own benefit? NPR's Alina Selyukh reports the EU is focusing on Amazon's dual role as a seller and a platform for other sellers. ALINA SELYUKH, BYLINE: Amazon likes to present itself as a cultivator and supporter of small- and medium-sized businesses. It likes to highlight that most of its sales are products from other companies. These other merchants pay Amazon fees to sell on its platform. They also share a lot of their data with Amazon. (SOUNDBITE OF PRESS CONFERENCE)MARGRETHE VESTAGER: And the question here is about the data. SELYUKH: That's EU Competition Commissioner Margrethe Vestager speaking about Amazon at a press conference in September. That's when her team at the European Commission started surveying the merchants who contract with Amazon. This research would later show that Amazon appears to use, quote, \"competitively sensitive information\" about the sellers, their products and transactions. (SOUNDBITE OF PRESS CONFERENCE)VESTAGER: Do you then also use these data to do your own calculations, as, what is the new big thing? What is it that people want? What makes them buy things? SELYUKH: As in, does Amazon use the data it collects to its own advantage or in other anti-competitive ways? Now the European Commission is officially investigating that. This adds to a push by authorities on both sides of the Atlantic to regulate how tech giants use the data of people and organizations on their platforms. The EU has led the charge with a sweeping privacy law and several fines against Google. And just today, German antitrust regulators announced a deal with Amazon to settle their own investigation into the marketplace. As a result, Amazon will make changes to the business terms it offers sellers and not just in Germany but worldwide. Amazon's use of data from sellers also came up this week at an antitrust hearing in Congress. (SOUNDBITE OF ARCHIVED RECORDING)NATE SUTTON: And we don't use individual seller data to directly compete with them. SELYUKH: That's Amazon lawyer Nate Sutton carefully wording his answer. (SOUNDBITE OF ARCHIVED RECORDING)SUTTON: The algorithms are optimized to predict what customers want to buy, regardless of the seller. SELYUKH: A note - Amazon is one of NPR's financial supporters. In a statement, the company says it will cooperate fully with the EU regulators. If the EU finds Amazon in violation of Europe's competition rules, the retailer could face fines of up to 10% of a year's worth of global revenue. For Amazon, that could be tens of billions of dollars. Alina Selyukh, NPR News. MARY LOUISE KELLY, HOST:   The European Union opened an antitrust investigation today into Amazon. The EU regulators' central question is this. Does the Internet giant stifle competition by exploiting data it collects from other companies for its own benefit? NPR's Alina Selyukh reports the EU is focusing on Amazon's dual role as a seller and a platform for other sellers. ALINA SELYUKH, BYLINE: Amazon likes to present itself as a cultivator and supporter of small- and medium-sized businesses. It likes to highlight that most of its sales are products from other companies. These other merchants pay Amazon fees to sell on its platform. They also share a lot of their data with Amazon. (SOUNDBITE OF PRESS CONFERENCE) MARGRETHE VESTAGER: And the question here is about the data. SELYUKH: That's EU Competition Commissioner Margrethe Vestager speaking about Amazon at a press conference in September. That's when her team at the European Commission started surveying the merchants who contract with Amazon. This research would later show that Amazon appears to use, quote, \"competitively sensitive information\" about the sellers, their products and transactions. (SOUNDBITE OF PRESS CONFERENCE) VESTAGER: Do you then also use these data to do your own calculations, as, what is the new big thing? What is it that people want? What makes them buy things? SELYUKH: As in, does Amazon use the data it collects to its own advantage or in other anti-competitive ways? Now the European Commission is officially investigating that. This adds to a push by authorities on both sides of the Atlantic to regulate how tech giants use the data of people and organizations on their platforms. The EU has led the charge with a sweeping privacy law and several fines against Google. And just today, German antitrust regulators announced a deal with Amazon to settle their own investigation into the marketplace. As a result, Amazon will make changes to the business terms it offers sellers and not just in Germany but worldwide. Amazon's use of data from sellers also came up this week at an antitrust hearing in Congress. (SOUNDBITE OF ARCHIVED RECORDING) NATE SUTTON: And we don't use individual seller data to directly compete with them. SELYUKH: That's Amazon lawyer Nate Sutton carefully wording his answer. (SOUNDBITE OF ARCHIVED RECORDING) SUTTON: The algorithms are optimized to predict what customers want to buy, regardless of the seller. SELYUKH: A note - Amazon is one of NPR's financial supporters. In a statement, the company says it will cooperate fully with the EU regulators. If the EU finds Amazon in violation of Europe's competition rules, the retailer could face fines of up to 10% of a year's worth of global revenue. For Amazon, that could be tens of billions of dollars. Alina Selyukh, NPR News.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-18-739934923": {"title": "The Idea That Got Us To The Moon, And The Man Who Pushed It : NPR", "url": "https://www.npr.org/2019/07/18/739934923/meet-john-houbolt-he-figured-out-how-to-go-to-the-moon-but-few-were-listening", "author": "No author found", "published_date": "2019-07-18", "content": "", "section": "The Apollo 11 Moon Landing, 50 Years Later", "disclaimer": ""}, "2019-07-18-742981813": {"title": "Viral Russian-Created FaceApp Sparks Worry Over Privacy : NPR", "url": "https://www.npr.org/2019/07/18/742981813/viral-russian-created-faceapp-sparks-worry-over-privacy", "author": "No author found", "published_date": "2019-07-18", "content": "NOEL KING, HOST: FaceApp is a program that takes your photos and transforms you into a much older version of yourself. By now, you're either probably using it yourself or you've seen a friend or a celebrity show off their transformation on social media. FaceApp was developed in Russia by Russians, and that has raised a big question. How might those photos be used? The Democratic National Committee doesn't want to take chances, and it's telling presidential campaign staffers to, quote, \"delete the app immediately. \"Geoffrey Fowler is a technology columnist at The Washington Post. He ran his own forensic analysis of FaceApp, and he talked to the company's CEO. Geoffrey, good morning. GEOFFREY FOWLER: Good morning. KING: So explain how FaceApp works because the end results are pretty cool. It's neat to see yourself much older. FOWLER: It's a wonder of artificial intelligence that it can map all these points on anybody's face and then figure out how to morph them just enough to kind of figure out what your future self might look like. It's actually been around since 2017 with other capabilities - it can also change your gender or your hair - but people really started paying attention to it recently with this aging effect. KING: You did an analysis of FaceApp. Should users - and this is the big question. Should users be worried about privacy and security? Where are those photos going? FOWLER: All really good questions. I think you should worry about FaceApp, but not necessarily any more than all the other apps on your phone. I think it's great that this app has caused us to have a conversation about, hey, wait a minute; where's my data going? Is it possibly going into the hands of the Russian government? And we have some answers to those questions, but not all the ones that we necessarily want. But I didn't see anything under the hood that made this app seem any more particularly dangerous than a lot of the other ones that are also really popular. KING: The fact that it was developed in Russia by Russians doesn't alone make it a security risk, but Democratic leaders are worried about it. Can you explain why? FOWLER: One of the things that makes FaceApp controversial is that it has this privacy policy and terms of service that privacy advocates have started poring through, and it's really broad. It basically says that this app has the right to take your photo and hold onto it and use it kind of however they want for as long as they want. So that kind of language is actually pretty common in a lot of companies' terms of service. You'll even see that, like, in Instagram and Facebook. But we now look at Russian companies a little bit differently, especially after the 2016 election where we know Russian technology had a role in shaping that election. KING: Yeah. FOWLER: I think we also look differently now at apps that are collecting pictures of our faces because we've been having a big conversation about the power of facial recognition and how that might be used. KING: You talked to FaceApp's CEO. What did he tell you? FOWLER: He offered some specifics about what his app is doing to respect our privacy that the terms of service and privacy policy and his website don't really offer, and I think some of them are important. So he said, for example, FaceApp only uploads one photo at a time, not your phone's entire camera roll. There were some people that presumed that it had access to every single photo and was sending that all up into the cloud, and that is not the case. I also saw that in my forensic analysis. Another important thing he said is that FaceApp deletes most of the photos from its systems after 48 hours. So again, we have to believe the CEO on this, but he says most of them are gone after two days. The other thing he said that I think is important is that they're not using these photos to do something else. They're not running some kind of side business in facial recognition. They're really only using this to provide the service in the app, which is to morph your face into something cool. KING: And if people do want to delete their data, is there someplace they can go and figure out how to do that quickly? FOWLER: Yeah. Well, it doesn't really help just to delete the app itself because, as we've learned, your photos are going into the cloud and being processed in a server elsewhere. But you can go in the app. And if you go into the bug report settings and you submit a bug report and you start it with the word privacy, the company says they will pay attention to that one and delete the data of yours that they've got on their server. KING: Geoffrey Fowler is a technology columnist for The Washington Post. Thanks so much, Geoffrey. FOWLER: You bet. NOEL KING, HOST:  FaceApp is a program that takes your photos and transforms you into a much older version of yourself. By now, you're either probably using it yourself or you've seen a friend or a celebrity show off their transformation on social media. FaceApp was developed in Russia by Russians, and that has raised a big question. How might those photos be used? The Democratic National Committee doesn't want to take chances, and it's telling presidential campaign staffers to, quote, \"delete the app immediately. \" Geoffrey Fowler is a technology columnist at The Washington Post. He ran his own forensic analysis of FaceApp, and he talked to the company's CEO. Geoffrey, good morning. GEOFFREY FOWLER: Good morning. KING: So explain how FaceApp works because the end results are pretty cool. It's neat to see yourself much older. FOWLER: It's a wonder of artificial intelligence that it can map all these points on anybody's face and then figure out how to morph them just enough to kind of figure out what your future self might look like. It's actually been around since 2017 with other capabilities - it can also change your gender or your hair - but people really started paying attention to it recently with this aging effect. KING: You did an analysis of FaceApp. Should users - and this is the big question. Should users be worried about privacy and security? Where are those photos going? FOWLER: All really good questions. I think you should worry about FaceApp, but not necessarily any more than all the other apps on your phone. I think it's great that this app has caused us to have a conversation about, hey, wait a minute; where's my data going? Is it possibly going into the hands of the Russian government? And we have some answers to those questions, but not all the ones that we necessarily want. But I didn't see anything under the hood that made this app seem any more particularly dangerous than a lot of the other ones that are also really popular. KING: The fact that it was developed in Russia by Russians doesn't alone make it a security risk, but Democratic leaders are worried about it. Can you explain why? FOWLER: One of the things that makes FaceApp controversial is that it has this privacy policy and terms of service that privacy advocates have started poring through, and it's really broad. It basically says that this app has the right to take your photo and hold onto it and use it kind of however they want for as long as they want. So that kind of language is actually pretty common in a lot of companies' terms of service. You'll even see that, like, in Instagram and Facebook. But we now look at Russian companies a little bit differently, especially after the 2016 election where we know Russian technology had a role in shaping that election. KING: Yeah. FOWLER: I think we also look differently now at apps that are collecting pictures of our faces because we've been having a big conversation about the power of facial recognition and how that might be used. KING: You talked to FaceApp's CEO. What did he tell you? FOWLER: He offered some specifics about what his app is doing to respect our privacy that the terms of service and privacy policy and his website don't really offer, and I think some of them are important. So he said, for example, FaceApp only uploads one photo at a time, not your phone's entire camera roll. There were some people that presumed that it had access to every single photo and was sending that all up into the cloud, and that is not the case. I also saw that in my forensic analysis. Another important thing he said is that FaceApp deletes most of the photos from its systems after 48 hours. So again, we have to believe the CEO on this, but he says most of them are gone after two days. The other thing he said that I think is important is that they're not using these photos to do something else. They're not running some kind of side business in facial recognition. They're really only using this to provide the service in the app, which is to morph your face into something cool. KING: And if people do want to delete their data, is there someplace they can go and figure out how to do that quickly? FOWLER: Yeah. Well, it doesn't really help just to delete the app itself because, as we've learned, your photos are going into the cloud and being processed in a server elsewhere. But you can go in the app. And if you go into the bug report settings and you submit a bug report and you start it with the word privacy, the company says they will pay attention to that one and delete the data of yours that they've got on their server. KING: Geoffrey Fowler is a technology columnist for The Washington Post. Thanks so much, Geoffrey. FOWLER: You bet.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-19-743567568": {"title": "Dan Coats, Director Of National Intelligence, Picks Election Security Czar : NPR", "url": "https://www.npr.org/2019/07/19/743567568/director-of-national-intelligence-dan-coats-appoints-new-election-security-czar", "author": "No author found", "published_date": "2019-07-19", "content": "SCOTT SIMON, HOST: Dan Coats, the director of National Intelligence, has announced he's created a new position to help coordinate government efforts to protect U. S. elections from outside interference. Intelligence officials have warned that the 2020 elections are still susceptible to attacks from foreign governments. NPR's Pam Fessler covers the issue and joins us in our studios. Pam, thanks so much for being with us. PAM FESSLER, BYLINE: Hi, Scott. SIMON: How does this position change what the government has already been doing? FESSLER: Well, Coats created this new position. It's called election threats executive. And a longtime intelligence official - her name is Shelby Pierson - is going to do the job. She's going to oversee and coordinate all the efforts in the intelligence community, gather information about what kind of threats are out there. But quite frankly, she did something very similar to this during the 2018 elections, but that was part-time; this is going to be full-time. And this really elevates the position. And I think mostly what it's doing is it's designed to send out a message that the government really takes this issue very, very seriously, and also to counter some concerns that the Trump administration hasn't been doing enough. It's worth noting that on multiple occasions, the president has expressed doubts about the conclusion by Dan Coats, the intelligence director, and the entire community, that Russia interfered in the 2016 election to help Trump win. SIMON: Can you tell what kind of security threats they'll be on the lookout for? FESSLER: Well, there's been a lot of focus on the kind of thing that happened in 2016 when Russian intelligence tried to hack into state voter rolls. They also broke into the DNC's email system. And also, we had this massive disinformation campaign on social media. So it's a lot of concern about a repeat of that. But they're also worried about other bad actors in other countries. The Iranians, the North Koreans, the Chinese are all seen as potential threats, or it could even be somebody domestically. Earlier this week, interestingly, Microsoft said that it detected almost 800 cyberattack attempts over the past year against think tanks and U. S. political organizations. And that might be the sign of the kind of things that we're going to be seeing in 2020. And then this new intelligence effort - the whole purpose is to try and gather that information. SIMON: You often remind us in your reporting that elections, actually, are run on the state and local level, not the federal level. How is this new position, if it is, going to affect what they do to protect against interference? FESSLER: Well, it's interesting, actually. Ever since 2016, the Department of Homeland Security has been working really closely with state and local election officials to try and figure out, you know, how they can beef up their security. So that's going to continue. But, you know, they need intelligence to know what they should be doing and where the threat is coming from. So if the process works, this intelligence that's all being gathered will be shared with the state and local officials. Interestingly, they say, though, that they really need more money. Congress approved about $380 million last year for election security, but they say it's not enough. They're still using a lot of old voting equipment that's more vulnerable to attacks. A lot of them don't have IT professionals to help them with security. They say, well, it's great if we have all this intelligence about potential threats, but if we don't have the money to fix the problem, what good is it? SIMON: NPR's Pam Fessler, thanks so much. FESSLER: Thank you. SCOTT SIMON, HOST:  Dan Coats, the director of National Intelligence, has announced he's created a new position to help coordinate government efforts to protect U. S. elections from outside interference. Intelligence officials have warned that the 2020 elections are still susceptible to attacks from foreign governments. NPR's Pam Fessler covers the issue and joins us in our studios. Pam, thanks so much for being with us. PAM FESSLER, BYLINE: Hi, Scott. SIMON: How does this position change what the government has already been doing? FESSLER: Well, Coats created this new position. It's called election threats executive. And a longtime intelligence official - her name is Shelby Pierson - is going to do the job. She's going to oversee and coordinate all the efforts in the intelligence community, gather information about what kind of threats are out there. But quite frankly, she did something very similar to this during the 2018 elections, but that was part-time; this is going to be full-time. And this really elevates the position. And I think mostly what it's doing is it's designed to send out a message that the government really takes this issue very, very seriously, and also to counter some concerns that the Trump administration hasn't been doing enough. It's worth noting that on multiple occasions, the president has expressed doubts about the conclusion by Dan Coats, the intelligence director, and the entire community, that Russia interfered in the 2016 election to help Trump win. SIMON: Can you tell what kind of security threats they'll be on the lookout for? FESSLER: Well, there's been a lot of focus on the kind of thing that happened in 2016 when Russian intelligence tried to hack into state voter rolls. They also broke into the DNC's email system. And also, we had this massive disinformation campaign on social media. So it's a lot of concern about a repeat of that. But they're also worried about other bad actors in other countries. The Iranians, the North Koreans, the Chinese are all seen as potential threats, or it could even be somebody domestically. Earlier this week, interestingly, Microsoft said that it detected almost 800 cyberattack attempts over the past year against think tanks and U. S. political organizations. And that might be the sign of the kind of things that we're going to be seeing in 2020. And then this new intelligence effort - the whole purpose is to try and gather that information. SIMON: You often remind us in your reporting that elections, actually, are run on the state and local level, not the federal level. How is this new position, if it is, going to affect what they do to protect against interference? FESSLER: Well, it's interesting, actually. Ever since 2016, the Department of Homeland Security has been working really closely with state and local election officials to try and figure out, you know, how they can beef up their security. So that's going to continue. But, you know, they need intelligence to know what they should be doing and where the threat is coming from. So if the process works, this intelligence that's all being gathered will be shared with the state and local officials. Interestingly, they say, though, that they really need more money. Congress approved about $380 million last year for election security, but they say it's not enough. They're still using a lot of old voting equipment that's more vulnerable to attacks. A lot of them don't have IT professionals to help them with security. They say, well, it's great if we have all this intelligence about potential threats, but if we don't have the money to fix the problem, what good is it? SIMON: NPR's Pam Fessler, thanks so much. FESSLER: Thank you.", "section": "National Security", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-19-743416668": {"title": "An Imagined Future Speaks In 'Talking To Robots' : NPR", "url": "https://www.npr.org/2019/07/19/743416668/an-imagined-future-speaks-in-talking-to-robots", "author": "No author found", "published_date": "2019-07-19", "content": "", "section": "Book Reviews", "disclaimer": ""}, "2019-07-22-744034861": {"title": "India's Chandrayaan-2 Blasts Toward The Moon : NPR", "url": "https://www.npr.org/2019/07/22/744034861/india-launches-mission-to-the-moon-on-its-second-try", "author": "No author found", "published_date": "2019-07-22", "content": "", "section": "Space", "disclaimer": ""}, "2019-07-22-744050565": {"title": "Equifax Data Breach Settlement Totals Up To $700 Million : NPR", "url": "https://www.npr.org/2019/07/22/744050565/equifax-to-pay-up-to-700-million-in-data-breach-settlement", "author": "No author found", "published_date": "2019-07-22", "content": "RACHEL MARTIN, HOST: The credit bureau Equifax will pay up to $700 million to consumers over a massive data breach two years ago. That hack exposed the personal information of about 147 million people. That is more than half the adult population of the entire United States. NPR's Chris Arnold has been following this and joins us now. Hi, Chris. CHRIS ARNOLD, BYLINE: Hey, Rachel. MARTIN: So how much of this money is actually going to go to the people who were hurt or affected in some way by that data breach? ARNOLD: Well, the Consumer Financial Protection Bureau, which was part of this, says $425 million will be for, quote, \"time and money that people spent to protect themselves from potential threats of identity theft or addressing actual incidents of identity theft as a result of the breach. \"And we're reading this morning. The settlement just came out, so we're, you know, thumbing through it, learning things. And people could get up to $20,000 each, it looks like, for lost time and money. You have to apply and document what happened. It looks like they're going to pay people $25 an hour for up to 20 hours for dealing with a whole range of different things - I mean, whether you had your identity stolen and you dealt with that or just. . . MARTIN: Right. ARNOLD: . . . Signing up for services. And it's unclear, though, like, how people are supposed to know - well, OK, my identity got stolen. Was it because of this breach? I mean, so there's still some unanswered. . . MARTIN: Yeah. ARNOLD: . . . Questions about exactly how that will work. But the goal is to reimburse people if they got hurt. MARTIN: How do you actually do that? How do people try to get reimbursed? ARNOLD: Well, there's a website, like there often is these days. It's equifaxbreachsettlement. com. These things are often difficult to remember on the radio, so I'm going to say it again - equifaxbreachsettlement. com - no spaces or anything. And assuming the court approves the settlement, people can go there and sign up and do everything they have to do. But we should say that people have to do this within six months if they want to get the benefits. MARTIN: So a settlement for more than half a billion dollars sounds like a whole lot of money. Is this a win for consumer groups? Are they happy? ARNOLD: I mean, it depends on who you ask. I mean, some advocates say look; this affected so many people - like you said, more than half the adult population of the United States - and the type of information that the credit bureau's track - it's so potentially damaging they say look; this is not enough. You know, it's ridiculous. Others say $700 million is not an insignificant amount of money. It's a real bite out of the company's profits. And at least a lot of people are going to get some money back. MARTIN: Can you just remind us about the breach itself? I mean, there were hearings in Congress when this happened. There were a lot of lawmakers who were very outraged. Why is this breach such a big deal? ARNOLD: Yeah, I mean, again, first, it's 115 million people, right? So that makes it a big deal. But beyond that, I mean, Equifax affects the financial lives of, you know, almost everybody in this country, right? I mean, it's your credit score. It's your ability to get a mortgage or a car loan. Companies like this collect data on your financial history. They know if you're paying your bills, how many credit cards you have, what the credit card numbers actually are, your social security number. And this hack exposed that, at least with this one company, they just did not have good enough security. And they're supposed to be, you know, safeguarding all this really sensitive information. MARTIN: But it's brought up this other issue about what kind of permission people are or are not giving this company, right? I mean, people don't actively sign up for Equifax. ARNOLD: No. MARTIN: They can order a credit score, and then Equifax dives into their stockpile of all their personal information. ARNOLD: Yeah. And, I mean, it's in some ways kind of crazy. This is the way it's evolved. And there are just all kinds of information. They have a dossier, basically, on all of us. We don't give permission, so that's why there's just been so much concern that they're not keeping us safe. We should say they're going to spend $1 billion, though, on cybersecurity as part of this settlement. MARTIN: NPR's Chris Arnold. Thanks, Chris. ARNOLD: Thanks, Rachel. RACHEL MARTIN, HOST:  The credit bureau Equifax will pay up to $700 million to consumers over a massive data breach two years ago. That hack exposed the personal information of about 147 million people. That is more than half the adult population of the entire United States. NPR's Chris Arnold has been following this and joins us now. Hi, Chris. CHRIS ARNOLD, BYLINE: Hey, Rachel. MARTIN: So how much of this money is actually going to go to the people who were hurt or affected in some way by that data breach? ARNOLD: Well, the Consumer Financial Protection Bureau, which was part of this, says $425 million will be for, quote, \"time and money that people spent to protect themselves from potential threats of identity theft or addressing actual incidents of identity theft as a result of the breach. \" And we're reading this morning. The settlement just came out, so we're, you know, thumbing through it, learning things. And people could get up to $20,000 each, it looks like, for lost time and money. You have to apply and document what happened. It looks like they're going to pay people $25 an hour for up to 20 hours for dealing with a whole range of different things - I mean, whether you had your identity stolen and you dealt with that or just. . . MARTIN: Right. ARNOLD: . . . Signing up for services. And it's unclear, though, like, how people are supposed to know - well, OK, my identity got stolen. Was it because of this breach? I mean, so there's still some unanswered. . . MARTIN: Yeah. ARNOLD: . . . Questions about exactly how that will work. But the goal is to reimburse people if they got hurt. MARTIN: How do you actually do that? How do people try to get reimbursed? ARNOLD: Well, there's a website, like there often is these days. It's equifaxbreachsettlement. com. These things are often difficult to remember on the radio, so I'm going to say it again - equifaxbreachsettlement. com - no spaces or anything. And assuming the court approves the settlement, people can go there and sign up and do everything they have to do. But we should say that people have to do this within six months if they want to get the benefits. MARTIN: So a settlement for more than half a billion dollars sounds like a whole lot of money. Is this a win for consumer groups? Are they happy? ARNOLD: I mean, it depends on who you ask. I mean, some advocates say look; this affected so many people - like you said, more than half the adult population of the United States - and the type of information that the credit bureau's track - it's so potentially damaging they say look; this is not enough. You know, it's ridiculous. Others say $700 million is not an insignificant amount of money. It's a real bite out of the company's profits. And at least a lot of people are going to get some money back. MARTIN: Can you just remind us about the breach itself? I mean, there were hearings in Congress when this happened. There were a lot of lawmakers who were very outraged. Why is this breach such a big deal? ARNOLD: Yeah, I mean, again, first, it's 115 million people, right? So that makes it a big deal. But beyond that, I mean, Equifax affects the financial lives of, you know, almost everybody in this country, right? I mean, it's your credit score. It's your ability to get a mortgage or a car loan. Companies like this collect data on your financial history. They know if you're paying your bills, how many credit cards you have, what the credit card numbers actually are, your social security number. And this hack exposed that, at least with this one company, they just did not have good enough security. And they're supposed to be, you know, safeguarding all this really sensitive information. MARTIN: But it's brought up this other issue about what kind of permission people are or are not giving this company, right? I mean, people don't actively sign up for Equifax. ARNOLD: No. MARTIN: They can order a credit score, and then Equifax dives into their stockpile of all their personal information. ARNOLD: Yeah. And, I mean, it's in some ways kind of crazy. This is the way it's evolved. And there are just all kinds of information. They have a dossier, basically, on all of us. We don't give permission, so that's why there's just been so much concern that they're not keeping us safe. We should say they're going to spend $1 billion, though, on cybersecurity as part of this settlement. MARTIN: NPR's Chris Arnold. Thanks, Chris. ARNOLD: Thanks, Rachel.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-22-743775196": {"title": "The Success Of Streaming Has Been Great For Some, But Is There A Better Way? : NPR", "url": "https://www.npr.org/2019/07/22/743775196/the-success-of-streaming-has-been-great-for-some-but-is-there-a-better-way", "author": "No author found", "published_date": "2019-07-22", "content": "", "section": "Editors' Picks", "disclaimer": ""}, "2019-07-23-744638659": {"title": "U.S. Justice Department Starts Antitrust Review Of Big Tech : NPR", "url": "https://www.npr.org/2019/07/23/744638659/doj-starts-review-of-whether-major-tech-companies-are-too-powerful", "author": "No author found", "published_date": "2019-07-23", "content": "", "section": "Business", "disclaimer": ""}, "2019-07-24-741282397": {"title": "FTC To Hold Facebook CEO Mark Zuckerberg Personally Liable For Future Violations : NPR", "url": "https://www.npr.org/2019/07/24/741282397/facebook-to-pay-5-billion-to-settle-ftc-privacy-case", "author": "No author found", "published_date": "2019-07-24", "content": "MARY LOUISE KELLY, HOST: The Federal Trade Commission announced today that Facebook will pay a $5 billion fine for rampant privacy violations. CEO Mark Zuckerberg will have to answer directly to regulators as part of the settlement. For some perspective, though - the company's quarterly earnings were also announced today. And Facebook made $16. 9 billion. Well, here to talk about the deal is NPR's Aarti Shahani. Hi, Aarti. AARTI SHAHANI, BYLINE: Hi. KELLY: All right. So give me the highlights. What is this deal that has now emerged between the FTC and Facebook? SHAHANI: Yes. It was filed today, and it's kind of a follow-up. Back in 2011, Facebook promised the FTC it would stop sharing user data with third parties, outside apps. And according to the FTC's head of enforcement, the company broke its promise while the ink was still drying on that deal. Facebook had turned users into the product. What a person clicks, likes, who their friends are - these got packaged and monetized. Advertisers and other app developers, like Cambridge Analytica, would pay for access. Facebook promised the government it would stop harvesting and sharing data so liberally, but it didn't. Facebook also tricked users into handing over phone numbers. In theory, it was for security for password resets. In reality, the company used those numbers for advertising too. And regulators say Facebook lied about facial recognition. About 60 million Facebook users can expect to get a note telling them the company was tracking without proper permission. KELLY: Wow. So that is the quite staggering context - 60 million users going to get a note. What in this deal will compel Facebook to change its ways? SHAHANI: So now going forward, Facebook needs to spell out exactly what data it's collecting and what it's sharing. The government hasn't imposed limits from outside. Facebook will decide for itself. But it will have to give quarterly reports to the FTC and its own board. Zuckerberg has to sign them. And if Facebook gets caught violating, Zuckerberg would be subject to civil as well as criminal penalties. I'd also note Facebook, which is an NPR sponsor, disclosed today that the FTC has begun a separate anti-trust investigation. KELLY: To your point that you just made about Mark Zuckerberg and that he will be personally accountable if Facebook does not follow the rules going forward - that must count as a big win from the point of view of the FTC. SHAHANI: Yeah. So the three Republicans at the FTC who approved the deal say, yes. They are getting way more money from Facebook than litigation would ever have gotten and sending a strong message to other CEOs - abuse user privacy, and we'll make you pay. But one FTC member, Democrat Rohit Chopra, who dissented - he thinks Zuckerberg got off the hook. KELLY: Huh. SHAHANI: Yeah. So the CEO was supposed to make sure his company complied with the original settlement order, OK? He didn't. The FTC could have slammed him with a civil suit now. Regulators would not need to prove that Zuckerberg lied, just that he failed in his duty and there's - that's a much lower burden with plenty of evidence. So Chopra wanted to see Zuckerberg deposed in court and forced to disclose what he knew. Now that opportunity is gone. KELLY: Well, what is Facebook saying about all this? SHAHANI: The CEO issued a post today. And I got to say it's classic Zuckerberg - spin the embarrassing development into another example of Facebook's desire to do good and go above and beyond. He wrote, quote, \"we already work hard to live up to this responsibility\" - meaning protecting privacy - \"but now we're going to set a completely new standard for our industry. \" He said hundreds of engineers and more than a thousand people across the company would implement this privacy-focused vision. What he didn't mention is that his legal team fought tooth and nail on every aspect of this deal. That's according to the FTC. As one regulator put it, Facebook settled and didn't go to court because that would have been quite embarrassing. Company documents would have been revealed. KELLY: Yeah. SHAHANI: Now Facebook gets to break the bad news on the same day as the Mueller hearing. KELLY: NPR's Aarti Shahani, thanks so much. SHAHANI: Thank you. MARY LOUISE KELLY, HOST:  The Federal Trade Commission announced today that Facebook will pay a $5 billion fine for rampant privacy violations. CEO Mark Zuckerberg will have to answer directly to regulators as part of the settlement. For some perspective, though - the company's quarterly earnings were also announced today. And Facebook made $16. 9 billion. Well, here to talk about the deal is NPR's Aarti Shahani. Hi, Aarti. AARTI SHAHANI, BYLINE: Hi. KELLY: All right. So give me the highlights. What is this deal that has now emerged between the FTC and Facebook? SHAHANI: Yes. It was filed today, and it's kind of a follow-up. Back in 2011, Facebook promised the FTC it would stop sharing user data with third parties, outside apps. And according to the FTC's head of enforcement, the company broke its promise while the ink was still drying on that deal. Facebook had turned users into the product. What a person clicks, likes, who their friends are - these got packaged and monetized. Advertisers and other app developers, like Cambridge Analytica, would pay for access. Facebook promised the government it would stop harvesting and sharing data so liberally, but it didn't. Facebook also tricked users into handing over phone numbers. In theory, it was for security for password resets. In reality, the company used those numbers for advertising too. And regulators say Facebook lied about facial recognition. About 60 million Facebook users can expect to get a note telling them the company was tracking without proper permission. KELLY: Wow. So that is the quite staggering context - 60 million users going to get a note. What in this deal will compel Facebook to change its ways? SHAHANI: So now going forward, Facebook needs to spell out exactly what data it's collecting and what it's sharing. The government hasn't imposed limits from outside. Facebook will decide for itself. But it will have to give quarterly reports to the FTC and its own board. Zuckerberg has to sign them. And if Facebook gets caught violating, Zuckerberg would be subject to civil as well as criminal penalties. I'd also note Facebook, which is an NPR sponsor, disclosed today that the FTC has begun a separate anti-trust investigation. KELLY: To your point that you just made about Mark Zuckerberg and that he will be personally accountable if Facebook does not follow the rules going forward - that must count as a big win from the point of view of the FTC. SHAHANI: Yeah. So the three Republicans at the FTC who approved the deal say, yes. They are getting way more money from Facebook than litigation would ever have gotten and sending a strong message to other CEOs - abuse user privacy, and we'll make you pay. But one FTC member, Democrat Rohit Chopra, who dissented - he thinks Zuckerberg got off the hook. KELLY: Huh. SHAHANI: Yeah. So the CEO was supposed to make sure his company complied with the original settlement order, OK? He didn't. The FTC could have slammed him with a civil suit now. Regulators would not need to prove that Zuckerberg lied, just that he failed in his duty and there's - that's a much lower burden with plenty of evidence. So Chopra wanted to see Zuckerberg deposed in court and forced to disclose what he knew. Now that opportunity is gone. KELLY: Well, what is Facebook saying about all this? SHAHANI: The CEO issued a post today. And I got to say it's classic Zuckerberg - spin the embarrassing development into another example of Facebook's desire to do good and go above and beyond. He wrote, quote, \"we already work hard to live up to this responsibility\" - meaning protecting privacy - \"but now we're going to set a completely new standard for our industry. \" He said hundreds of engineers and more than a thousand people across the company would implement this privacy-focused vision. What he didn't mention is that his legal team fought tooth and nail on every aspect of this deal. That's according to the FTC. As one regulator put it, Facebook settled and didn't go to court because that would have been quite embarrassing. Company documents would have been revealed. KELLY: Yeah. SHAHANI: Now Facebook gets to break the bad news on the same day as the Mueller hearing. KELLY: NPR's Aarti Shahani, thanks so much. SHAHANI: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-25-745062630": {"title": "The Green New Deal: Where Spotify Stands, And Where Artists Wish It Would : NPR", "url": "https://www.npr.org/2019/07/25/745062630/the-green-new-deal-where-spotify-stands-and-where-artists-wish-it-would", "author": "No author found", "published_date": "2019-07-25", "content": "", "section": "Editors' Picks", "disclaimer": ""}, "2019-07-25-744587413": {"title": "What Do 2020 Democratic Candidates Propose On Strengthening Election Security : NPR", "url": "https://www.npr.org/2019/07/25/744587413/threats-to-u-s-elections-arent-going-away-what-have-the-2020-democrats-proposed", "author": "No author found", "published_date": "2019-07-25", "content": "", "section": "Politics", "disclaimer": ""}, "2019-07-26-745315045": {"title": "Toward New Musics: What The Future Holds For Sound Creativity : NPR", "url": "https://www.npr.org/2019/07/26/745315045/towards-new-musics-what-the-future-holds-for-sound-creativity", "author": "No author found", "published_date": "2019-07-26", "content": "", "section": "Editors' Picks", "disclaimer": ""}, "2019-07-26-745361267": {"title": "What's The (Far) Future Of Music Listening? : NPR", "url": "https://www.npr.org/2019/07/26/745361267/hello-brave-new-world", "author": "No author found", "published_date": "2019-07-26", "content": "", "section": "Editors' Picks", "disclaimer": ""}, "2019-07-27-745835846": {"title": "Grindr Has Left A Cultural Impact On The LGBTQ Community. But Can The App Survive? : NPR", "url": "https://www.npr.org/2019/07/27/745835846/grindr-has-left-a-cultural-impact-on-the-lgbtq-community-but-can-the-app-survive", "author": "No author found", "published_date": "2019-07-27", "content": "SCOTT SIMON, HOST: I'm Scott Simon. Grindr, the mobile dating app, is now on the market. It's been on the hunt for a buyer after its current Chinese owner was accused of sharing users' HIV status with third parties. Grindr was launched a decade ago as a way for gay men to meet, and not just for dinner and a movie. But in addition to its legal issues, Grindr has disappointed some LGBTQ people. Mathew Rodriguez is a former staff writer at Grindr's own news site, Into. He joins us from New York. Thanks so much for being with us, Mr. Rodriguez. MATHEW RODRIGUEZ: Thank you for having me. SIMON: What did Grindr want to be for queer people? RODRIGUEZ: Grindr was at the intersection of two very interesting phenomena and historical timelines. Grindr was the newest iteration in a long history of the way that queer people found each other, right? So we're talking about going back to classified ads, chat rooms, you know. So I think Into and Grindr more broadly were trying to evolve and really find out, hey, what does it mean for queer people to connect? And it doesn't only have to be for dating or for beyond dinner and a movie, like you said. I think that it could also be, like, what does it mean to use stories about queer people to foster connection? SIMON: How did things change when Kunlun, the Chinese owner, took over? RODRIGUEZ: So I would say that because of all of the ambitions that Grindr had when I got there in July of 2017, it was a very potent time. It was a time where there was a lot of excitement for all the things that Into could do. And I would just say that there was a cultural shift when Kunlun took over and that there was a - somewhat of a cultural mismatch. You know, when you buy an app, I don't think that - coming from a completely different world and having a lot of heterosexual leadership, I don't think they understood that this wasn't an app that was just on your phone that maybe you didn't have as much to do with your identity. This was something that people felt really strong emotions behind. SIMON: What did it mean for a lot of people who worked at Grindr, established to be a voice and a part of the lives of queer people in the United States, to have the president of Grindr say marriage should always be between a man and a woman? RODRIGUEZ: Well, I can tell you there were a lot of people who were saying, you know, it's really important that the people who are in charge of an app that is so much a part of queer life really have queer people's best interests at heart and felt that the story that we told was an important one. So that was very validating. SIMON: Why would somebody who has those views on marriage, that it's only between a man and a woman, want anything to do, much less own and run an app for queer people? RODRIGUEZ: Grindr made money. And I do think that, at the end of the day, you know, we were under a corporation that had to make money. But I think that that is really the question that's at the heart of it, is can someone who doesn't understand Grindr's place in the community - should they be the person making money off of the app, right? That's also something that we were really concerned about. Who is making that money, and who are the people leading and making decisions? And are they aware of the very complex role that Grindr plays not only in American queer people's lives, but queer people's lives abroad? So I think that's really at the heart of it as well. SIMON: Mathew Rodriguez, an editor with TheBody. com, which provides HIV information and support. Thanks so much for being with us. RODRIGUEZ: Thank you. SCOTT SIMON, HOST:  I'm Scott Simon. Grindr, the mobile dating app, is now on the market. It's been on the hunt for a buyer after its current Chinese owner was accused of sharing users' HIV status with third parties. Grindr was launched a decade ago as a way for gay men to meet, and not just for dinner and a movie. But in addition to its legal issues, Grindr has disappointed some LGBTQ people. Mathew Rodriguez is a former staff writer at Grindr's own news site, Into. He joins us from New York. Thanks so much for being with us, Mr. Rodriguez. MATHEW RODRIGUEZ: Thank you for having me. SIMON: What did Grindr want to be for queer people? RODRIGUEZ: Grindr was at the intersection of two very interesting phenomena and historical timelines. Grindr was the newest iteration in a long history of the way that queer people found each other, right? So we're talking about going back to classified ads, chat rooms, you know. So I think Into and Grindr more broadly were trying to evolve and really find out, hey, what does it mean for queer people to connect? And it doesn't only have to be for dating or for beyond dinner and a movie, like you said. I think that it could also be, like, what does it mean to use stories about queer people to foster connection? SIMON: How did things change when Kunlun, the Chinese owner, took over? RODRIGUEZ: So I would say that because of all of the ambitions that Grindr had when I got there in July of 2017, it was a very potent time. It was a time where there was a lot of excitement for all the things that Into could do. And I would just say that there was a cultural shift when Kunlun took over and that there was a - somewhat of a cultural mismatch. You know, when you buy an app, I don't think that - coming from a completely different world and having a lot of heterosexual leadership, I don't think they understood that this wasn't an app that was just on your phone that maybe you didn't have as much to do with your identity. This was something that people felt really strong emotions behind. SIMON: What did it mean for a lot of people who worked at Grindr, established to be a voice and a part of the lives of queer people in the United States, to have the president of Grindr say marriage should always be between a man and a woman? RODRIGUEZ: Well, I can tell you there were a lot of people who were saying, you know, it's really important that the people who are in charge of an app that is so much a part of queer life really have queer people's best interests at heart and felt that the story that we told was an important one. So that was very validating. SIMON: Why would somebody who has those views on marriage, that it's only between a man and a woman, want anything to do, much less own and run an app for queer people? RODRIGUEZ: Grindr made money. And I do think that, at the end of the day, you know, we were under a corporation that had to make money. But I think that that is really the question that's at the heart of it, is can someone who doesn't understand Grindr's place in the community - should they be the person making money off of the app, right? That's also something that we were really concerned about. Who is making that money, and who are the people leading and making decisions? And are they aware of the very complex role that Grindr plays not only in American queer people's lives, but queer people's lives abroad? So I think that's really at the heart of it as well. SIMON: Mathew Rodriguez, an editor with TheBody. com, which provides HIV information and support. Thanks so much for being with us. RODRIGUEZ: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-28-745949428": {"title": "Did Facebook CEO Mark Zuckerberg Intend To Deceive? : NPR", "url": "https://www.npr.org/2019/07/28/745949428/did-facebook-ceo-mark-zuckerberg-intend-to-deceive", "author": "No author found", "published_date": "2019-07-28", "content": "LULU GARCIA-NAVARRO, HOST:  This past week, the Federal Trade Commission decided to enter into a settlement with Mark Zuckerberg, head of Facebook, which I'll say here is an NPR sponsor. The FTC entered into that settlement without interviewing Zuckerberg first. And as NPR's Aarti Shahani reports, the government may have missed its chance to find out more about Facebook's practices. AARTI SHAHANI, BYLINE: The year was 2014. Mark Zuckerberg was onstage at F8, Facebook's annual conference, with a promise to users. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: The thing is we don't ever want anyone to be surprised about how they're sharing on Facebook. I mean, that's not good for anyone. SHAHANI: His company had gotten caught taking the personal data of Facebook users and, without consent, handing it off to outsiders, third-party app developers. So this was his promise. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: Now we're going to change this, and we're going to make it so that now everyone has to choose to share their own data with an app themselves. So we think that this is a really important step for giving people power and control. SHAHANI: Sounds great - only, it wasn't true. According to the Federal Trade Commission, Facebook kept handing over user data secretly without consent to dozens of outside developers. Mark Zuckerberg said one thing while his company did another. It was not an isolated incident. In 2018, he did it again - this time, not on his own stage but in front of the entire country. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: It's clear now that we didn't do enough to prevent these tools from being used for harm as well. SHAHANI: Zuckerberg, summoned to the U. S. Congress, apologized for enabling Russian interference in the American elections, for helping to spread hate speech and also for violating the privacy of users. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: And that was a big mistake, and it was my mistake, and I'm sorry. I started Facebook. I run it. And I'm responsible for what happens here. SHAHANI: Sounds great - only, in the same month Zuckerberg gave that speech, regulators say the company began to use facial recognition tracking on some 60 million users - again, without consent. Rohit Chopra is an FTC commissioner who voted against entering the settlement with Facebook. ROHIT CHOPRA: It's still really a mystery to me as to what role he played. SHAHANI: The majority of his colleagues - three Republicans - voted in favor of settling. Facebook got fined for $5 billion. The regulators say that's more than they would have gotten in court if they'd litigated. Chopra, a Democrat, says his agency underplayed its hand and sacrificed the truth in the process. CHOPRA: We cut off this investigation too early. Facebook was willing to pay more money in order to hide Mark Zuckerberg's testimony from this investigation. SHAHANI: The FTC spoke to Zuckerberg's lawyers, never to him. He was not required to answer questions or turn over his emails. And the settlement lets the CEO off the hook for the many privacy mishaps the FTC scrutinized. It bothers Chopra that his agency didn't pursue the truth. Zuckerberg's actions may stand at odds with the philanthropic, altruistic image he's worked so hard to cultivate. And he isn't just a CEO. He structured the stock so that he controls the majority of votes in Facebook. Chopra says in other cases, when a chief calls the shots in a firm, the FTC takes a hard look at him or her. CHOPRA: We didn't even want to look at something that seemed fundamentally important and instead traded it away for a higher fine. And none of that money will actually go to Facebook's users. SHAHANI: The $5 billion goes to the U. S. Treasury. Zuckerberg lauded the settlement, saying in a post that his company has a privacy-focused vision. Facebook did not respond to Chopra's criticisms when NPR asked. Meanwhile, Zuckerberg's team is on Capitol Hill trying to get permission to mint money, a new digital currency. This cannot succeed without the public trust. Facebook is making the case that lawmakers and regulators should trust it. Chopra says he doesn't trust Mark Zuckerberg or his company. Aarti Shahani, NPR News. LULU GARCIA-NAVARRO, HOST:   This past week, the Federal Trade Commission decided to enter into a settlement with Mark Zuckerberg, head of Facebook, which I'll say here is an NPR sponsor. The FTC entered into that settlement without interviewing Zuckerberg first. And as NPR's Aarti Shahani reports, the government may have missed its chance to find out more about Facebook's practices. AARTI SHAHANI, BYLINE: The year was 2014. Mark Zuckerberg was onstage at F8, Facebook's annual conference, with a promise to users. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: The thing is we don't ever want anyone to be surprised about how they're sharing on Facebook. I mean, that's not good for anyone. SHAHANI: His company had gotten caught taking the personal data of Facebook users and, without consent, handing it off to outsiders, third-party app developers. So this was his promise. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: Now we're going to change this, and we're going to make it so that now everyone has to choose to share their own data with an app themselves. So we think that this is a really important step for giving people power and control. SHAHANI: Sounds great - only, it wasn't true. According to the Federal Trade Commission, Facebook kept handing over user data secretly without consent to dozens of outside developers. Mark Zuckerberg said one thing while his company did another. It was not an isolated incident. In 2018, he did it again - this time, not on his own stage but in front of the entire country. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: It's clear now that we didn't do enough to prevent these tools from being used for harm as well. SHAHANI: Zuckerberg, summoned to the U. S. Congress, apologized for enabling Russian interference in the American elections, for helping to spread hate speech and also for violating the privacy of users. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: And that was a big mistake, and it was my mistake, and I'm sorry. I started Facebook. I run it. And I'm responsible for what happens here. SHAHANI: Sounds great - only, in the same month Zuckerberg gave that speech, regulators say the company began to use facial recognition tracking on some 60 million users - again, without consent. Rohit Chopra is an FTC commissioner who voted against entering the settlement with Facebook. ROHIT CHOPRA: It's still really a mystery to me as to what role he played. SHAHANI: The majority of his colleagues - three Republicans - voted in favor of settling. Facebook got fined for $5 billion. The regulators say that's more than they would have gotten in court if they'd litigated. Chopra, a Democrat, says his agency underplayed its hand and sacrificed the truth in the process. CHOPRA: We cut off this investigation too early. Facebook was willing to pay more money in order to hide Mark Zuckerberg's testimony from this investigation. SHAHANI: The FTC spoke to Zuckerberg's lawyers, never to him. He was not required to answer questions or turn over his emails. And the settlement lets the CEO off the hook for the many privacy mishaps the FTC scrutinized. It bothers Chopra that his agency didn't pursue the truth. Zuckerberg's actions may stand at odds with the philanthropic, altruistic image he's worked so hard to cultivate. And he isn't just a CEO. He structured the stock so that he controls the majority of votes in Facebook. Chopra says in other cases, when a chief calls the shots in a firm, the FTC takes a hard look at him or her. CHOPRA: We didn't even want to look at something that seemed fundamentally important and instead traded it away for a higher fine. And none of that money will actually go to Facebook's users. SHAHANI: The $5 billion goes to the U. S. Treasury. Zuckerberg lauded the settlement, saying in a post that his company has a privacy-focused vision. Facebook did not respond to Chopra's criticisms when NPR asked. Meanwhile, Zuckerberg's team is on Capitol Hill trying to get permission to mint money, a new digital currency. This cannot succeed without the public trust. Facebook is making the case that lawmakers and regulators should trust it. Chopra says he doesn't trust Mark Zuckerberg or his company. Aarti Shahani, NPR News.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-29-746275123": {"title": "Electric Planes And Carbon Offsets Are Ways To Reduce Air Travel Emissions : NPR", "url": "https://www.npr.org/2019/07/29/746275123/with-an-eye-toward-lower-emissions-clean-air-travel-gets-off-the-ground", "author": "No author found", "published_date": "2019-07-29", "content": "ARI SHAPIRO, HOST: Electric cars are all over the roads these days. On this week's All Tech Considered, we're talking about the potential for electric planes. (SOUNDBITE OF MUSIC)SHAPIRO: Air travel accounts for about 2% of global carbon emissions. And people see clean air travel as a key part of slowing global warming. To explain why and what is being done about it, we are joined by Umair Irfan. He writes about climate change energy and the environment for Vox. Welcome to the studio. UMAIR IRFAN: Thanks for having me. SHAPIRO: Why focus on air travel when it accounts for just 2%, I mean, a much smaller share of carbon emissions than cars? IRFAN: Well, air travel is going to grow in the coming century. We're expecting to see massive growth. The International Civil Aviation Organization projects upward of 700% growth by the middle of the century. So while it is small, it is going to be a larger and larger share. And the other side of the problem is that air travel is really hard to decarbonize. SHAPIRO: You've written that we need electric airplanes. Why? IRFAN: Well, with that growth, we need to be able to counteract that somehow. Electrifying air travel is one key option, particularly for shorter flights. And we're already seeing at least a couple airlines experimenting with it. There's one in the Pacific Northwest that aims to electrify its whole fleet, but it only flies routes smaller than 100 miles. And so right now that's where the technology is. It can basically support small aircraft going short distances. But you absolutely need something that's going to decarbonize air travel to some extent because it is going to be a large and growing share of the problem. SHAPIRO: Are any of the small airlines in the air carrying passengers already? IRFAN: Not right now. One of the big issues is that the Federal Aviation Administration hasn't approved electric aircraft for passenger air travel. The regulations are actually a big hangup right now because much of the rules are structured around conventional aircraft. And now they need a new set of rules to deal with battery electric aircraft. That's also going to be challenged in the coming years. SHAPIRO: When you look at where the technology stands today, how far are we from having an electric airplane that could cross the Atlantic Ocean? IRFAN: Well, the key problem right now is that jet fuel is extremely energy dense and batteries are not. And in an airplane, space, as you may have noticed, is at a premium, and so is weight. And so you want to pack as much power into as small a space as possible. Batteries are improving but not fast enough right now. They need to be able to store much more energy in a much smaller space. SHAPIRO: I think some airlines are experimenting with biofuels. Is that helpful? IRFAN: Yeah. Biofuels are a big part of it as well. The idea is that if you use a crop that takes CO2 from the air and you burn that instead, effectively it's carbon neutral. That's a little bit tricky to pull off, though, because you want to make sure you get more energy out of the fuel than you expend to grow it in the first place. Biofuels are also a little bit more expensive than conventional fuels. And so you have to make sort of an economic argument. Right now, airlines are experimenting with it, but it's more expensive than conventional fuels. And so we need a really substantial price drop in order to make inroads. SHAPIRO: For people who want to do something about their flight-related carbon emissions now, is buying carbon offsets really the only option? IRFAN: Well, the biggest option - the biggest impact option would be just to fly less. And there's a global movement that's kind of sprung up to try to encourage people to do that. I think the estimate is that each leg of a journey on a roundtrip flight across the Atlantic emits about a ton of CO2. So that's 2 tons of CO2. And that's roughly enough to melt about 30 square feet of Arctic ice. So that's a very direct relationship between your actions and your impact on the environment, and for some, people that's been a pretty startling revelation. SHAPIRO: Do you think the airlines feel the urgency? Is there really a race to get an electric plane up in the air? IRFAN: There is some pressure on the airlines right now because of this global flying shame movement that's kind of taken off. There are - there is some pressure. . . SHAPIRO: Global flying shame, meaning make people feel guilty for the fact that their flights are causing all of these carbon emissions? IRFAN: That's right. The Swedes have even coined a word for it. They call it flygskam. For some reason, Sweden is the epicenter for this movement. SHAPIRO: That's flying shame. IRFAN: Flying shame, right. And yeah, their customers are now increasing pressure on the airlines. They want to sort of assuage their guilt for flying. Many people see it as necessary, but they want to see more heavy lifting coming from the airlines themselves in terms of getting more fuel efficient or coming up with better offsetting schemes or mitigating greenhouse gas emissions in other ways. SHAPIRO: Umair Irfan is a staff writer at Vox. Thanks for coming in. IRFAN: Thanks for having me. ARI SHAPIRO, HOST:  Electric cars are all over the roads these days. On this week's All Tech Considered, we're talking about the potential for electric planes. (SOUNDBITE OF MUSIC) SHAPIRO: Air travel accounts for about 2% of global carbon emissions. And people see clean air travel as a key part of slowing global warming. To explain why and what is being done about it, we are joined by Umair Irfan. He writes about climate change energy and the environment for Vox. Welcome to the studio. UMAIR IRFAN: Thanks for having me. SHAPIRO: Why focus on air travel when it accounts for just 2%, I mean, a much smaller share of carbon emissions than cars? IRFAN: Well, air travel is going to grow in the coming century. We're expecting to see massive growth. The International Civil Aviation Organization projects upward of 700% growth by the middle of the century. So while it is small, it is going to be a larger and larger share. And the other side of the problem is that air travel is really hard to decarbonize. SHAPIRO: You've written that we need electric airplanes. Why? IRFAN: Well, with that growth, we need to be able to counteract that somehow. Electrifying air travel is one key option, particularly for shorter flights. And we're already seeing at least a couple airlines experimenting with it. There's one in the Pacific Northwest that aims to electrify its whole fleet, but it only flies routes smaller than 100 miles. And so right now that's where the technology is. It can basically support small aircraft going short distances. But you absolutely need something that's going to decarbonize air travel to some extent because it is going to be a large and growing share of the problem. SHAPIRO: Are any of the small airlines in the air carrying passengers already? IRFAN: Not right now. One of the big issues is that the Federal Aviation Administration hasn't approved electric aircraft for passenger air travel. The regulations are actually a big hangup right now because much of the rules are structured around conventional aircraft. And now they need a new set of rules to deal with battery electric aircraft. That's also going to be challenged in the coming years. SHAPIRO: When you look at where the technology stands today, how far are we from having an electric airplane that could cross the Atlantic Ocean? IRFAN: Well, the key problem right now is that jet fuel is extremely energy dense and batteries are not. And in an airplane, space, as you may have noticed, is at a premium, and so is weight. And so you want to pack as much power into as small a space as possible. Batteries are improving but not fast enough right now. They need to be able to store much more energy in a much smaller space. SHAPIRO: I think some airlines are experimenting with biofuels. Is that helpful? IRFAN: Yeah. Biofuels are a big part of it as well. The idea is that if you use a crop that takes CO2 from the air and you burn that instead, effectively it's carbon neutral. That's a little bit tricky to pull off, though, because you want to make sure you get more energy out of the fuel than you expend to grow it in the first place. Biofuels are also a little bit more expensive than conventional fuels. And so you have to make sort of an economic argument. Right now, airlines are experimenting with it, but it's more expensive than conventional fuels. And so we need a really substantial price drop in order to make inroads. SHAPIRO: For people who want to do something about their flight-related carbon emissions now, is buying carbon offsets really the only option? IRFAN: Well, the biggest option - the biggest impact option would be just to fly less. And there's a global movement that's kind of sprung up to try to encourage people to do that. I think the estimate is that each leg of a journey on a roundtrip flight across the Atlantic emits about a ton of CO2. So that's 2 tons of CO2. And that's roughly enough to melt about 30 square feet of Arctic ice. So that's a very direct relationship between your actions and your impact on the environment, and for some, people that's been a pretty startling revelation. SHAPIRO: Do you think the airlines feel the urgency? Is there really a race to get an electric plane up in the air? IRFAN: There is some pressure on the airlines right now because of this global flying shame movement that's kind of taken off. There are - there is some pressure. . . SHAPIRO: Global flying shame, meaning make people feel guilty for the fact that their flights are causing all of these carbon emissions? IRFAN: That's right. The Swedes have even coined a word for it. They call it flygskam. For some reason, Sweden is the epicenter for this movement. SHAPIRO: That's flying shame. IRFAN: Flying shame, right. And yeah, their customers are now increasing pressure on the airlines. They want to sort of assuage their guilt for flying. Many people see it as necessary, but they want to see more heavy lifting coming from the airlines themselves in terms of getting more fuel efficient or coming up with better offsetting schemes or mitigating greenhouse gas emissions in other ways. SHAPIRO: Umair Irfan is a staff writer at Vox. Thanks for coming in. IRFAN: Thanks for having me.", "section": "Getting To Zero Carbon: The Climate Challenge", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-30-746572830": {"title": "Huawei Says Revenue Jumped 23% Despite Tensions With U.S. : NPR", "url": "https://www.npr.org/2019/07/30/746572830/huawei-says-revenue-jumped-23-despite-tensions-with-u-s", "author": "No author found", "published_date": "2019-07-30", "content": "", "section": "Business", "disclaimer": ""}, "2019-07-30-745484563": {"title": "'It Came From Something Awful' Links 4Chan And Today's Political Discourse : NPR", "url": "https://www.npr.org/2019/07/30/745484563/it-came-from-something-awful-links-4chan-and-todays-political-discourse", "author": "No author found", "published_date": "2019-07-30", "content": "", "section": "Book Reviews", "disclaimer": ""}, "2019-07-30-746475401": {"title": "Suspect Arrested In Capitol One Data Breach Affecting 100 Million Customers : NPR", "url": "https://www.npr.org/2019/07/30/746475401/woman-charged-as-hacker-of-capital-one-data-that-exposes-over-100-million-custom", "author": "No author found", "published_date": "2019-07-30", "content": "", "section": "Business", "disclaimer": ""}, "2019-07-31-746878763": {"title": "How Tech Companies Track Your Every Move And Put Your Data Up For Sale  : NPR", "url": "https://www.npr.org/2019/07/31/746878763/how-tech-companies-track-your-every-move-and-put-your-data-up-for-sale", "author": "No author found", "published_date": "2019-07-31", "content": "TERRY GROSS, HOST: This is FRESH AIR. I'm Terry Gross. If you ever get the creepy feeling you're being monitored when you use your computer, smartphone or smart speaker, our guest Geoffrey Fowler is here to tell you you are. Fowler writes a consumer-oriented technology column for The Washington Post. He's been investigating the ways our browsers and phone apps harvest personal information about us even while we're sleeping. And he discovered that Amazon had kept four years' worth of recorded audio from his home, captured by his Alexa smart speaker, including family conversations about medications and a friend doing a business transaction. Geoffrey Fowler joined the Post in 2017 after 16 years with the Wall Street Journal, writing about consumer technology, Silicon Valley, national affairs and China. He writes his technology column from San Francisco. He spoke with FRESH AIR's Dave Davies. DAVE DAVIES, BYLINE: Well, Geoffrey Fowler, welcome to FRESH AIR. You have a recent column. The headline is \"I Found Your Data. It's For Sale. \" What kind of personal data did you find available for sale on the Internet? GEOFFREY FOWLER: I found all kinds of things that normal people would consider secrets and that corporations spend a lot of money - millions and millions of dollars - to try to keep out of the hands of their competitors and criminals. I found people's flight records. I found people's records from their doctors prescribing them medications. I found people's tax documents that they were - thought they were only sharing with their tax preparer. And they were available with one click. I could have opened them up and downloaded them. DAVIES: Right. And where did this data come from? FOWLER: It came from their Web browsers. And what we discovered along the way is that there is a giant hole in people's Web browsers that we're installing ourselves, and they're called extensions. These are these little apps, these little programs that you add into Chrome or into Firefox that are supposed to help you do things on the Web more easily, like keep track of your passwords or, you know, maybe get discounts on certain websites. A lot of them do that, but it turns out a surprisingly large number of them have a side hustle in your data. And they were in the business of watching everything you did on the Web, sending it out somewhere else and then that site was sending it on to someone else, who then made it available for sale. DAVIES: So when we click on the I agree box after not reading all - the long thing, what does that allow the add-on to harvest from us? FOWLER: It allows the add-on to look at every webpage you go to. They can read the contents of the page. They can also look at the exact address at the top of the page. That's all the letters that appear after - through the HTTP that you see. And that actually contains a lot more secrets than you might realize. For example, it might contain your username or your password or, in the case of some pages that we saw in this system for sale from doctor's offices, contain the name of the doctor and even medication that was being renewed. I have to say - I, as a technology journalist, knew that extensions were a risk, but I had no idea how much of a risk they were until I heard from an independent researcher - a guy named Sam Jadali, who runs his own Web hosting service and found some of his customers' data for sale and kind of became a half-a-year-long investigation for him to figure out how this happened. What he showed me, actually, was data coming from the Washington Post's own newsroom for sale on the Internet. DAVIES: Wow. Before we get to how this ends up for sale, let's get to the mechanism of how it gets into the wrong hands. So we install an extension on our browser. It's by some company, whatever. They then harvest the data. Then what do they do with it? FOWLER: They either use it for themselves - there's actually a lot of companies that are in the business of - like, sometimes they call it marketing analytics - of trying to figure out what people are doing on the Internet. Amazon has a very large business doing this with a company called Alexa. In Amazon's case, though, they only keep it for themselves. They don't sell that and share it to other people. And they anonymize that data so that people who might tap into it can't see the exact page that you, Dave, were - was looking at. But not everybody in this sort of shadowy business is so ethical. And so we found this data through the research of that independent researcher I mentioned, Sam Jadali. We found it for sale on a site called Nacho Analytics. It was buying it from perhaps some other party who perhaps was getting it from some other party. It's really hard to connect these dots because the companies won't talk about who they are getting their data from - and then, you know, putting it all for sale, down to the individual pages that people were clicking on, on a site called Nacho Analytics. DAVIES: Right. So you went on to Nacho Analytics. And what did you find? FOWLER: On Nacho Analytics, you could pay, starting at about 40 bucks per month, to look at the webpages that people were surfing on individual domains - so places like NPR. org or even onedrive. com, which is Microsoft's cloud storage service. And when you pulled up those pages, you could see the exact URLs of pages. So - and then search through them and search through the metadata. So that's things like the titles and the address of the computer that they were using to surf there. You could see all of this information and search through it. DAVIES: And in some cases, things that appeared to be tax returns or medical records, right? FOWLER: Yeah. For example - so we went to onedrive. com and typed in tax, and that pulled up a bunch of documents based on the title of the page that appeared to be tax returns. Now, I did not want to further dig into anybody's privacy, so we did not actually click on those links, but we could have to see them. DAVIES: So if you wanted to look for a specific person on Nacho Analytics, you could find them. FOWLER: Well, I'll take the case of what we found from the Post newsroom. So I asked this researcher, Sam Jadali, to see if he could find any data from the Post's internal network. That's washpost. com. So he pulled up the pages he could see being surfed there. And there we saw someone logging into our internal networks - so not even something that's public to the world. And it turned out to be one of my colleagues, Nick. And we saw his username. So then I quickly called up Nick, and I said, hey, Nick, did you know that your data is for sale on the Internet? And he was like, what? And I said, well, I think the problem is an extension that you've got running on your computer. And we looked through them together. And sure enough, there was an extension there that looked really innocent but was sending out every page that he was visiting. DAVIES: Now, Nacho Analytics that provided all this information for a fee - you talked to them, I assume. What did they say? FOWLER: They said that they didn't do anything wrong. And they may have a point - that their business is not necessarily illegal. And I think it's really telling about sort of the state of the economy - of the Internet economy that what they're doing is actually considered pretty common. So they said that before they put data up for sale on Nacho Analytics, they would scrub it for personally identifiable information. But as we saw when we looked through the data together, clearly, they were not doing a very good job at doing that 'cause we found lots and lots and lots of secrets that were still available in the data. When I also - as well as this researcher, Sam Jadali - contacted Google and Firefox, they immediately shut down some of the extensions that were doing this leaking. So that, I believe, cut off some of the data supply for Nacho Analytics because a few days later, Nacho said it was essentially pausing its business. It would no longer take new customers, and it had suffered a data outage. And so it couldn't even provide data for people who'd already paid for it. DAVIES: Right. But again, I'm just a little puzzled by what kinds of extensions - like, for example, when I checked - after I read your story - on my Google Chrome browser, I had several extensions that allowed me to access Google Documents. Those are OK? FOWLER: Those are made by Google itself. So you know, I think with each of these cases, you have to ask, do I trust this company? We should have a conversation about whether you should trust Google. That's a whole other topic. But with a lot of other people, there are many other kinds of extensions in there. I'll take the example of the one that my colleague at the Post was using. It was called Hover Zoom. He had read about it on the website Reddit. And it's for people who use Reddit a lot and want to quickly be able to enlarge the photos on the Reddit website. He installed it, didn't think twice about it, and it was just sitting there running. And when you press certain keys, it would automatically enlarge the photos on any page. Turns out, they had a side business in taking every webpage he was going to and selling it on the Internet. DAVIES: Are ad-blocking extensions a good idea? Do they track you? FOWLER: They can be good or they can be bad. And this is one of the problems with these extension and add-on stores. It's really hard to know what these guys are up to. I have seen evidence of some that are really, really good and some that are just collecting your data. And, you know, you can't really tell just from the reviews or from sort of the presence in those stores what they're up to. You know what other kinds of software is frequently in the business of tracking you is actually VPN and other kinds of security software; sometimes antivirus software even. You'll think these are the companies that I trust to protect my privacy and security, but they may be paying for it by taking data about what you're doing on your computer and selling it. DAVIES: Including the big, well-known names in security? FOWLER: Yes. Some very big names in the antivirus and VPN world may be providing the services that they offer but may also be in the business of collecting data about what you're doing on your computer. DAVIES: Geoffrey Fowler writes a consumer-oriented technology column for The Washington Post. We'll continue our conversation after a short break. This is FRESH AIR. (SOUNDBITE OF ALEXANDRE DESPLAT'S \"SPY MEETING\")DAVIES: This is FRESH AIR, and we're speaking with Washington Post columnist Geoffrey Fowler. He writes consumer-oriented columns about navigating the confusing world of personal technology, including computers, smartphones, smart speakers and so on. So you wrote in June that you looked under the hood of the Chrome web browser, which is commonly used, and found it had brought along a few thousand friends. Who were the friends? FOWLER: They were lots and lots of companies that are in the business of tracking everything that you do on the Internet. Some of them are advertising companies. Some of them are analytics companies that help, you know, figure out how to make websites run better. Some of them were tech giants - Google and Facebook. Others were data brokers that are just in the business of trying to connect the dots in your digital life to build out a profile of you so that they can sell it. DAVIES: Right. Now, these are not the browser extensions - right? - that are specifically - you download. These are cookies, right? What exactly are cookies? FOWLER: Yeah. Cookies are baked in - pardon the pun - baked into the way that the Web works these days. So they're tiny little files that basically tag your browser and say, yep, Geoff was here. And then when you pull up another site, they'll check, oh, I see this cookie from before. So now we can - they can connect the dots. Think of them as little breadcrumbs that follow you around on the Internet. Turns out, the biggest maker of cookies on the Internet is Google itself. That's one of the ways that they help track you down and build out a profile and help advertisers target you with advertising. So Google also makes the most popular web browser, Chrome. So Chrome does not do very much to stop this cookie behavior from happening. In fact, they quite actively encourage it. DAVIES: Now, you've written about how these browser extensions that we get on, we at least agree to download them. Do we agree to accept cookies in some way? How does that happen? Are we informed? Or is our consent sought however passively? FOWLER: This is a really good question, and this goes right to the heart of a big conversation we're having about technology right now. What is consent? What is being informed? Yes, lots of websites now, because of a European data law, put up a little notice on the bottom that says, oh, by the way, we use cookies. Are you cool with that? And you either ignore it or you click OK, and you don't really think about it. But does that mean that we really understand that, you know, in the course of a week that over 11,000 times - at least for me - that companies are going to be able to be pushing out these requests to track and follow you around? I don't really think it does. I think, in fact, that we rely on the company that makes the web browser software, Chrome or Firefox, to have our interests at heart, right? And our interests would be to not be tracked. And yet Chrome is not doing that for us. That's in pretty big contrast to its much smaller rival, Firefox, which is made by a nonprofit called Mozilla. Now, as of a couple of weeks ago, it changed its default settings so that when you install it, it blocks those cookies by default, the ones that are involved in tracking. So in the case of my week of web surfing, the 11,000 cookies that Chrome would've let through, Mozilla let through none. DAVIES: Wow. And is there an option to opt out of cookies on Google Chrome? FOWLER: There are options to say block all cookies in Google Chrome, but that would then break some things about the way websites work. Not all cookies are bad. Some help remember, for example, your login to the Washington Post website. So if you turn those on, it really kind of tends to sort of break your web experience. One of the things that Mozilla figured out how to do was block just the ones that are sort of naughty, that are in the business of tracking you for advertising or for marketing, and allow through the ones that you want. So it's about sort of seeking that balance. Now, for me, this experience made me realize that Chrome has essentially become surveillance software. It's surveillance software for the advertising industry and for Google itself. So I made a switch to Mozilla, and I'm very happy I did. DAVIES: You recently did a report on what your iPhone was doing while you were sleeping. What did you find? FOWLER: I found that my iPhone is very busy while I'm sleeping, talking to lots of companies that I've never heard of and sharing with it lots of personal details, things like my exact address and my email address and my name. And that really, really surprised me. DAVIES: So what's happening here? FOWLER: We fill up our phones with apps. And when we do that, we presume that because they came from Apple's app store that they've been vetted and they're not - you know, they're respecting all of the privacy practices that we have come to expect from Apple because of its marketing. But it turns out, apps use something that are called trackers. They're a little bit like cookies that you get on the Web, but they're just embedded inside the apps themselves. And these trackers do lots of different things. Some of them help app-makers just figure out how people are using the apps so that they can make them work better. Others belong to data companies and advertising companies. Google makes trackers. Facebook makes trackers. Some are in there to gather data about people that are using apps so that they can sell it. And that's one way app-makers can make money. And these trackers inside the apps, as long as they're on your phone, they have the ability to essentially run whenever they want to, including while you're sleeping at night and not even using your phone. DAVIES: Wow. Can you shut them down at night? Should you - if you power your phone off, does it inhibit this? FOWLER: If you power off your phone, nothing will be coming out of your phone, either from trackers or from calls that might come in or go out of your phone. But that doesn't really stop the problem. Because as soon as you power your phone back up, the apps will wake back up, and they'll get back in touch with their trackers. The problem is really at the core of how apps are made and the kinds of requirements that Apple and Google and other phone, you know, store, app store-makers place on them. DAVIES: What kinds of requirements they place or fail to place - what do you mean? FOWLER: Indeed. Right now, you know, to get into the Apple app store or the Google Play store, you have to have your app reviewed by one of those companies. They do look to make sure that, you know, they are - that they have a privacy policy, that they are generally abiding by, you know, the rules that they set out in their app store guidelines. But they don't really look under the hood to understand, who are these other companies that they might be talking to, these tracker companies? And what sorts of data are they sending to them? They don't do that vetting for us. And unfortunately, we as consumers can't really see that, either. To figure out what my phone was doing while I slept at night and also during the day, I had to hack my phone. I went to a guy who used to work for the NSA. His name is Patrick Jackson. He now works for a technology company called Disconnect. And he showed me how to do something called a man-in-the-middle attack on my iPhone that basically, you know, kept a copy of all of the data going in and out of my phone while I slept at night so that we could look through it together. That's the level I had to go through to figure out what kind of data was flowing out of my phone and what trackers were running. I couldn't learn any of that from - either from Apple's software or from reading the privacy policies of these companies. DAVIES: So I understand this - so when an app is permitted to be sold in the iPhone store, does Apple require them not to use trackers and some people just aren't honest about it, or do Apple's rules permit them to include trackers in the apps that you download? FOWLER: Until very recently, Apple's rules permitted them to use whatever trackers they wanted. If you had given an app permission to collect your location - and it does pop up a thing saying, can we collect your location? - if you'd given it that then it could share that with whatever trackers it wanted. About two weeks after my story came out in The Washington Post about what my iPhone did while I was sleeping, Apple announced that it was going to now ban trackers in children's apps. So ones that were, you know, targeting, you know, people under the age of 13, they said they would no longer allow them to use third-party trackers. That is an admirable move in many ways. But then my question is, why is it OK in adult apps but not in kids' apps? DAVIES: And what kinds of information is the tracker transmitting about us? FOWLER: It could really be a wide range of things. You know, when I looked, you know, underneath the hood, just while I was sleeping, apps that I saw were using trackers included things like weather. com or - The Washington Post website had trackers. There was another one that's a popular app for kind of, like, checking with the police scanners, called Citizen. It was sending its trackers a lot of information, including my exact GPS coordinates and my email. And in that case, that violated its own privacy policy, and it later changed that after I called them. But still, it was happening. GROSS: We're listening to the interview FRESH AIR's Dave Davies recorded with Geoffrey Fowler, who writes a consumer-oriented technology column for The Washington Post. They'll pick up where they left off after a break, and critic Soraya Nadia McDonald will review the final season of the Netflix series \"Orange Is The New Black. \" I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF MUSIC)GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to the interview FRESH AIR's Dave Davies recorded with Washington Post technology columnist Geoffrey Fowler. Fowler has been investigating ways our computers, smartphones and smart speakers collect our personal information and what they do with it. Fowler has covered digital technology and Silicon Valley for years. When we left off, Fowler was describing how some phone apps have trackers that collect our data, which is often then sold to marketing and advertising companies. One example Fowler found was food delivery apps like Grubhub, Caviar and DoorDash. DAVIES: You found one food delivery app called DoorDash sends data to nine different trackers. FOWLER: Yeah, this one was pretty shocking to me. Again, think about this from the perspective of all of us. You know, you put an app on your phone like DoorDash. And you think, OK, DoorDash is here. When I open DoorDash, I have a relationship with DoorDash, with this company that's going to have someone bring food to my house. You don't think that you're going to have a relationship with nine other companies, including Facebook and Google, who get to know now and keep a log of every time you're hungry (laughter) and open this app and, you know, want to order some pizza. None of that is disclosed to us. This is all lost in this murky world of data. And that's sort of been the impetus behind a whole series of stories I've been doing for the Post. I kind of call it the secret life of your data, just kind of looking under the hood at all the things, all the data about us that's being passed around and traded and sold that we don't normally have a way to understand or see. Because if we don't know where our data is going, how can we even begin to hope to protect our privacy? DAVIES: And just so I understand the relationships, a food delivery app would would give data to nine trackers because each of them pays them a couple of fractions of pennies for it, or - why? FOWLER: There might be a variety of reasons. So for example, one tracker might be in the business of just giving data back to DoorDash to say, like, this is how your app functions. This is the areas of the app that people spend the most time on. These are the areas where people swipe around the most. It might also help them combat fraud. I guess there's a problem with people setting up fake phones to put in fake orders. So they say they want to be able to tell whether your phone is literally physically moving using the gyroscope in it to see if it's a real human on the other end of it. That's another kind of tracker. There's also trackers in there for advertising companies. So Google and Facebook are in there. And DoorDash says, oh, well, they put them in there just so that they could see whether their advertising was working because DoorDash does a lot of advertising with Google and Facebook. But the cost of all of this is that all of these different kinds of companies get to know every time you're opening DoorDash. And any time a company has data about you, who's really making sure that they're using it appropriately? Who's making sure that they're going to delete it soon enough? Who's making sure that they're - they have good security, and it's not going to get stolen and sold off somewhere? DAVIES: And do these trackers slow the apps down or cause further battery drain, or do they impose data charges on us? FOWLER: That's the other piece of app trackers, is that they do a whole bunch of bad things for our phone. Over the course of a week, I found 5,400 different trackers activated on my iPhone. Yours might be different. I may have more apps than you. But that's still quite a lot. If you multiplied that out by an entire month, it would have taken up 1. 5 gigabytes of data just going to trackers from my phone. To put that in some context, the basic data plan from AT&T is only 3 gigabytes. So imagine (laughter), half of your data plan is just eaten up by tracker companies who you don't want running anyway. So it's not only eating up your data. But then, every time it pings the network, that's another hit on your battery life too. So it's - this stuff really isn't in our interest, either from our privacy or from just keeping our phones running. DAVIES: You know, and sometimes when we're browsing, and we want to go to a site or fire up an application, we get a screen that says, do you want to sign in using Facebook? What happens? FOWLER: No, you don't. DAVIES: OK. (LAUGHTER)DAVIES: Why? FOWLER: Because you're giving Facebook the ability to then track every time you're using that website or every time you're using that app. And I'm sure that that website and app then have lots of other tracker pixels or cookies or software baked into them that send other information to Facebook about what you're up to. Google does the same thing. You'll sometimes get a button - do you want a sign in with Google? Same problem. These companies are in the business of data. They might tell everybody that they don't sell our data, but it is certainly very valuable to them. Actually, that's why they don't sell it because they want to keep it for themselves because they then use all this information about what apps you're using, what you're doing in them, what websites you go to, where you are physically with your phone - like, where you are in the world according to your GPS coordinates. They put all of that into their dossiers on each of us to - so that companies can target us with advertising. DAVIES: So if I'm going to sign into a music service like Spotify, and I sign in with Facebook, does that mean Spotify then has access to all of my Facebook data and my friends' data? FOWLER: Not necessarily. It used to be that Facebook was a lot more open about sharing your Facebook data and your friends' data with apps. They shut down a lot of that a couple of years ago. And that's the problem they got into with Cambridge Analytica as well, that they were allowing apps to collect data about your friends and then pass that along. So Facebook has gotten a lot more controlling with its data. Again, they'll say that's for privacy, but it's really because they realize that data is too valuable. They want to keep it for themselves so they can charge companies to market to you through them. DAVIES: You know, you have some columns that are on very specific topics that are really interesting. And one of them - you write about how to handle robocallers on your cellphones, and you mention getting the Do Not Call list. But there are some fight-back apps that people pay for that really get aggressive on this. You want to explain this? FOWLER: Yeah. There are some apps out there that - beyond just blocking bad calls, that they actually will try to torture robocallers. So they intercept the calls, and they listen briefly to see if they detect it as being a robocaller. And if they do, instead of passing the phone call on to you, they'll stay on the line. And they use artificial intelligence to try to keep the - either the person or the robot on the other end on the line by sort of teasing them. Sometimes they'll have, like, a Donald Trump impersonator voice that talks to them or somebody snoring or somebody just talking a lot. And then, for kicks, they will send you - they'll send you a recording of what happened afterwards. DAVIES: All right. So if you're really into it (laughter). FOWLER: If you're really into it. DAVIES: You write about smart speakers like Alexa and their potential to be conducting surveillance. And I guess it's worth noting that Alexa's owned by Amazon, whose chair, Jeff Bezos, is the owner of the paper that you work for, the Washington Post. How much is - are Alexa smart speakers recording of our lives? FOWLER: First, I'll note, yes, the Post is owned by Jeff Bezos, but I'm happy to report that I am at liberty to criticize Amazon as much as I'd like, including by digging into what Alexa is really getting up to. So I think one thing that a lot of folks do not realize about smart speakers with Alexa or with Google's Assistant or Siri is that, by default, they're keeping the recordings of everything that you say. So that means that you sort of think like, OK, well, but it only records when you call out the name, when you call out Alexa or call out Siri. Well, actually that's not the case. It records whenever it thinks it hears one of those calls to action. So I did an experiment where I went back to four years of recordings that Alexa has made of me and my family at home because, again, they're all kept there by default. And I spent a couple of days listening to all of them. And when I did that, I found, you know, all of these strange fragments of my life. So things you would expect, like setting the spaghetti timer or, you know, asking to hear a song, that was in there. But there were also lots of times, dozens of occasions, where Alexa was recording snippets of conversation or television just kind of randomly on its own. For example, it seemed to kind of go off a lot on its own whenever I was watching \"Downton Abbey. \"DAVIES: (Laughter). FOWLER: I had a lot of people with that particular posh British accent in the collection. It also went off when my family members were talking about medication. It went off when a friend was doing a business transaction. And listening to this archive, which any of the listeners can do themselves - if you go to amazon. com/alexaprivacy, you'll be able to dig into your collection as well - listening to this really made me think differently about what Alexa has become in our lives. It's - it is, yes, an assistant, but it is also an eavesdropper. And it is collecting this information and not giving us the power to tell it to stop. Now, Amazon does give you an option that you can go in and delete past recordings. But you can't tell it just don't keep the recording in the first place. DAVIES: And they keep everything forever? FOWLER: They keep everything forever. What's interesting is we were talking before about how these tech companies sometimes give us false choices. And this is, again, one of these false choices. Amazon says it needs all of this data to make Alexa smarter, to improve its artificial intelligence. And yet, archrival Google actually now by default does not do this. It does not keep the recordings from Google Assistant or its smart speakers or its phone-based assistant. It does not keep those by default. It deletes. You have - you'd have to go in and tell it that you want it to keep those. So here, you know, who's right? Do you really have to keep it, Amazon? I think not. And the reason they're doing it is because they can get away with it because most of us haven't noticed and haven't sort of spent the time to kind of dig into the details. DAVIES: We're speaking with Geoffrey Fowler. He writes a consumer-oriented technology column for the Washington Post. We will take a short break here, then we will talk some more. This is FRESH AIR. (SOUNDBITE OF MUSIC)DAVIES: This is FRESH AIR. And we're speaking with Washington Post columnist Geoffrey Fowler. He writes consumer-oriented columns about navigating the confusing world of personal technology, including computers, smartphones and smart speakers. Alexa seems to listen to an awful lot. Does Siri do the same thing on our iPhones? FOWLER: Siri does do the same thing on our iPhones and on the HomePod, which is their home speaker. So I have a corner of my living room where all of the virtual assistants live together. I have all of the connected speakers. I'm sure someday they'll start talking to each other. But I watch to see which ones go off at random times. I have to say, the Apple one goes off quite frequently, maybe even more than the others. Apple's policy is it keeps the recordings of everything that it hears, and it does not give you the option to tell it not to keep the recordings. But they anonymize it, so they don't associate it with your individual account in your name. Amazon still associates it with who you are. DAVIES: So you - when you say that the Apple device goes off more frequently, it starts recording more frequently, or seems to. FOWLER: That's right. Last night, I was watching a TV show. And in the middle of it, Siri, on the HomePod, perked up and said, sorry, I can't answer that question. And no one had asked Siri a question. DAVIES: (Laughter) Does this make you want to disconnect Alexa? FOWLER: I have to say, after doing this reporting, I now keep my Alexa speaker on permanent mute. There is a button on the top, that physical button you can press, that says, don't record and don't activate when you hear the name Alexa. But that, of course, sort of defeats the purpose of the device. So again, I just think this is really bad product design on the part of Amazon. They shouldn't put us in a situation where, again, the choice is don't use the technology or give into this kind of surveillance. Thing I would add is that this project of listening to my Alexa recordings made me wonder - I wonder what all the other ways are that Amazon is eavesdropping on my home because anybody with one of these smart speakers probably knows that, like, oh, you can hook them up to connected devices in your home, right? You can connect it up to light bulbs and thermostats and doorbells and all sorts of things. And I had certainly done that. I'm a technology journalist, and I review all this stuff. And my house is filled with gadgets. And so I went down the path of trying to figure out, OK, well, what other data from my home other than just me and my family's voices is it keeping? And I found, I mean, enough that would make the - you know, the East German police blush to see this kind of data. For example, my Nest thermostat was collecting in 15-minute increments over the last six or seven years not only the temperature in my home, but also whether there had been a person that passed in front of it. So there was this perfect record of every time there had been someone in my hallway for years and years and years that was being sent both to Google and to Amazon - because Amazon's requirement is if your gadget connects in with Alexa that they get to keep a copy of all this data, too. And then it started multiplying. So yeah, there was the thermostat, but there's also my garage door. It was doing the same thing. Then there was my connected lights. Literally, Amazon was getting a record of every time a light switched on and off in my house. DAVIES: And how did you discover that Amazon had these records of your thermostat changes and when people were walking down the hallway? FOWLER: I started asking. So I went to the companies that make these devices, and I said, hey, can you tell me what data you're collecting and who you're sharing it with? Some of them would not answer that question. And that - my frustration with that really animated what's become a yearlong project by me to sort of see if I can look under the hood and figure out what data is being collected and who it's being shared with. I've been looking at that and all sorts of things - connected devices in our home, our Web browser. And I've got even more - more and more projects coming down the pike. But some companies did answer. Some just pointed me to their privacy policies, which didn't really specify what they were up to. So it's really quite difficult for us as consumers to understand the secret life of these devices and the data they're collecting and who they're sharing it with. And I think that's a big problem. DAVIES: There's a lot of talk of congressional regulation of digital media. Are you seeing things that encourage you? FOWLER: There's a lot of talk in Congress, but not a lot of action when it comes to data and privacy. You know, we see a lot of individual members of Congress, you know, writing letters or holding hearings. But there's been little effort to really turn that into legislation that could even get to the point of being voted on. Right now, we see Congress also interested in these big questions about whether these tech companies are too big - right? - and that they need to be broken up for antitrust reasons, which, at the end of the day, is also about data because the reason why they're so big and so powerful and have made billions of dollars is because they have control over so much data about our lives. The thing that actually gives me the most hope is what I see happening in the states. So California passed the California Consumer Privacy Act, which is going to take effect in January. And it is, I think, going to become the closest thing that we have to an American privacy law. Of course, it only applies to the residents of California, but there are so many residents of California that a lot of companies are going to have to sort of essentially comply with it for everyone. And it really - it's about disclosure, which is, I think, where this all needs to start - you know, when this law takes effect, that these companies - even if they're not tech companies - any company that collects data about you is going to have to be able to say - you know, answer the question that I - some of the questions I posed before. What's being collected? Who is it being shared with? Tell me if it's being sold. Give me the chance to say, no, you're not allowed to sell it. I'm actually super excited when this law kicks in. I already have plans for January 1, 2020, to send out lots of request letters to companies as a California resident to tell me what data they're collecting and what they're doing with it. DAVIES: Well, Geoffrey Fowler, thanks so much for speaking with us. FOWLER: My pleasure. GROSS: Geoffrey Fowler writes a consumer-oriented technology column for The Washington Post. He spoke to FRESH AIR's Dave Davies. Coming up, Soraya Nadia McDonald reviews the final season of \"Orange Is The New Black. \" This is FRESH AIR. (SOUNDBITE OF GILAD HEKSELMAN'S \"DO RE MI FA SOL\") TERRY GROSS, HOST:  This is FRESH AIR. I'm Terry Gross. If you ever get the creepy feeling you're being monitored when you use your computer, smartphone or smart speaker, our guest Geoffrey Fowler is here to tell you you are. Fowler writes a consumer-oriented technology column for The Washington Post. He's been investigating the ways our browsers and phone apps harvest personal information about us even while we're sleeping. And he discovered that Amazon had kept four years' worth of recorded audio from his home, captured by his Alexa smart speaker, including family conversations about medications and a friend doing a business transaction. Geoffrey Fowler joined the Post in 2017 after 16 years with the Wall Street Journal, writing about consumer technology, Silicon Valley, national affairs and China. He writes his technology column from San Francisco. He spoke with FRESH AIR's Dave Davies. DAVE DAVIES, BYLINE: Well, Geoffrey Fowler, welcome to FRESH AIR. You have a recent column. The headline is \"I Found Your Data. It's For Sale. \" What kind of personal data did you find available for sale on the Internet? GEOFFREY FOWLER: I found all kinds of things that normal people would consider secrets and that corporations spend a lot of money - millions and millions of dollars - to try to keep out of the hands of their competitors and criminals. I found people's flight records. I found people's records from their doctors prescribing them medications. I found people's tax documents that they were - thought they were only sharing with their tax preparer. And they were available with one click. I could have opened them up and downloaded them. DAVIES: Right. And where did this data come from? FOWLER: It came from their Web browsers. And what we discovered along the way is that there is a giant hole in people's Web browsers that we're installing ourselves, and they're called extensions. These are these little apps, these little programs that you add into Chrome or into Firefox that are supposed to help you do things on the Web more easily, like keep track of your passwords or, you know, maybe get discounts on certain websites. A lot of them do that, but it turns out a surprisingly large number of them have a side hustle in your data. And they were in the business of watching everything you did on the Web, sending it out somewhere else and then that site was sending it on to someone else, who then made it available for sale. DAVIES: So when we click on the I agree box after not reading all - the long thing, what does that allow the add-on to harvest from us? FOWLER: It allows the add-on to look at every webpage you go to. They can read the contents of the page. They can also look at the exact address at the top of the page. That's all the letters that appear after - through the HTTP that you see. And that actually contains a lot more secrets than you might realize. For example, it might contain your username or your password or, in the case of some pages that we saw in this system for sale from doctor's offices, contain the name of the doctor and even medication that was being renewed. I have to say - I, as a technology journalist, knew that extensions were a risk, but I had no idea how much of a risk they were until I heard from an independent researcher - a guy named Sam Jadali, who runs his own Web hosting service and found some of his customers' data for sale and kind of became a half-a-year-long investigation for him to figure out how this happened. What he showed me, actually, was data coming from the Washington Post's own newsroom for sale on the Internet. DAVIES: Wow. Before we get to how this ends up for sale, let's get to the mechanism of how it gets into the wrong hands. So we install an extension on our browser. It's by some company, whatever. They then harvest the data. Then what do they do with it? FOWLER: They either use it for themselves - there's actually a lot of companies that are in the business of - like, sometimes they call it marketing analytics - of trying to figure out what people are doing on the Internet. Amazon has a very large business doing this with a company called Alexa. In Amazon's case, though, they only keep it for themselves. They don't sell that and share it to other people. And they anonymize that data so that people who might tap into it can't see the exact page that you, Dave, were - was looking at. But not everybody in this sort of shadowy business is so ethical. And so we found this data through the research of that independent researcher I mentioned, Sam Jadali. We found it for sale on a site called Nacho Analytics. It was buying it from perhaps some other party who perhaps was getting it from some other party. It's really hard to connect these dots because the companies won't talk about who they are getting their data from - and then, you know, putting it all for sale, down to the individual pages that people were clicking on, on a site called Nacho Analytics. DAVIES: Right. So you went on to Nacho Analytics. And what did you find? FOWLER: On Nacho Analytics, you could pay, starting at about 40 bucks per month, to look at the webpages that people were surfing on individual domains - so places like NPR. org or even onedrive. com, which is Microsoft's cloud storage service. And when you pulled up those pages, you could see the exact URLs of pages. So - and then search through them and search through the metadata. So that's things like the titles and the address of the computer that they were using to surf there. You could see all of this information and search through it. DAVIES: And in some cases, things that appeared to be tax returns or medical records, right? FOWLER: Yeah. For example - so we went to onedrive. com and typed in tax, and that pulled up a bunch of documents based on the title of the page that appeared to be tax returns. Now, I did not want to further dig into anybody's privacy, so we did not actually click on those links, but we could have to see them. DAVIES: So if you wanted to look for a specific person on Nacho Analytics, you could find them. FOWLER: Well, I'll take the case of what we found from the Post newsroom. So I asked this researcher, Sam Jadali, to see if he could find any data from the Post's internal network. That's washpost. com. So he pulled up the pages he could see being surfed there. And there we saw someone logging into our internal networks - so not even something that's public to the world. And it turned out to be one of my colleagues, Nick. And we saw his username. So then I quickly called up Nick, and I said, hey, Nick, did you know that your data is for sale on the Internet? And he was like, what? And I said, well, I think the problem is an extension that you've got running on your computer. And we looked through them together. And sure enough, there was an extension there that looked really innocent but was sending out every page that he was visiting. DAVIES: Now, Nacho Analytics that provided all this information for a fee - you talked to them, I assume. What did they say? FOWLER: They said that they didn't do anything wrong. And they may have a point - that their business is not necessarily illegal. And I think it's really telling about sort of the state of the economy - of the Internet economy that what they're doing is actually considered pretty common. So they said that before they put data up for sale on Nacho Analytics, they would scrub it for personally identifiable information. But as we saw when we looked through the data together, clearly, they were not doing a very good job at doing that 'cause we found lots and lots and lots of secrets that were still available in the data. When I also - as well as this researcher, Sam Jadali - contacted Google and Firefox, they immediately shut down some of the extensions that were doing this leaking. So that, I believe, cut off some of the data supply for Nacho Analytics because a few days later, Nacho said it was essentially pausing its business. It would no longer take new customers, and it had suffered a data outage. And so it couldn't even provide data for people who'd already paid for it. DAVIES: Right. But again, I'm just a little puzzled by what kinds of extensions - like, for example, when I checked - after I read your story - on my Google Chrome browser, I had several extensions that allowed me to access Google Documents. Those are OK? FOWLER: Those are made by Google itself. So you know, I think with each of these cases, you have to ask, do I trust this company? We should have a conversation about whether you should trust Google. That's a whole other topic. But with a lot of other people, there are many other kinds of extensions in there. I'll take the example of the one that my colleague at the Post was using. It was called Hover Zoom. He had read about it on the website Reddit. And it's for people who use Reddit a lot and want to quickly be able to enlarge the photos on the Reddit website. He installed it, didn't think twice about it, and it was just sitting there running. And when you press certain keys, it would automatically enlarge the photos on any page. Turns out, they had a side business in taking every webpage he was going to and selling it on the Internet. DAVIES: Are ad-blocking extensions a good idea? Do they track you? FOWLER: They can be good or they can be bad. And this is one of the problems with these extension and add-on stores. It's really hard to know what these guys are up to. I have seen evidence of some that are really, really good and some that are just collecting your data. And, you know, you can't really tell just from the reviews or from sort of the presence in those stores what they're up to. You know what other kinds of software is frequently in the business of tracking you is actually VPN and other kinds of security software; sometimes antivirus software even. You'll think these are the companies that I trust to protect my privacy and security, but they may be paying for it by taking data about what you're doing on your computer and selling it. DAVIES: Including the big, well-known names in security? FOWLER: Yes. Some very big names in the antivirus and VPN world may be providing the services that they offer but may also be in the business of collecting data about what you're doing on your computer. DAVIES: Geoffrey Fowler writes a consumer-oriented technology column for The Washington Post. We'll continue our conversation after a short break. This is FRESH AIR. (SOUNDBITE OF ALEXANDRE DESPLAT'S \"SPY MEETING\") DAVIES: This is FRESH AIR, and we're speaking with Washington Post columnist Geoffrey Fowler. He writes consumer-oriented columns about navigating the confusing world of personal technology, including computers, smartphones, smart speakers and so on. So you wrote in June that you looked under the hood of the Chrome web browser, which is commonly used, and found it had brought along a few thousand friends. Who were the friends? FOWLER: They were lots and lots of companies that are in the business of tracking everything that you do on the Internet. Some of them are advertising companies. Some of them are analytics companies that help, you know, figure out how to make websites run better. Some of them were tech giants - Google and Facebook. Others were data brokers that are just in the business of trying to connect the dots in your digital life to build out a profile of you so that they can sell it. DAVIES: Right. Now, these are not the browser extensions - right? - that are specifically - you download. These are cookies, right? What exactly are cookies? FOWLER: Yeah. Cookies are baked in - pardon the pun - baked into the way that the Web works these days. So they're tiny little files that basically tag your browser and say, yep, Geoff was here. And then when you pull up another site, they'll check, oh, I see this cookie from before. So now we can - they can connect the dots. Think of them as little breadcrumbs that follow you around on the Internet. Turns out, the biggest maker of cookies on the Internet is Google itself. That's one of the ways that they help track you down and build out a profile and help advertisers target you with advertising. So Google also makes the most popular web browser, Chrome. So Chrome does not do very much to stop this cookie behavior from happening. In fact, they quite actively encourage it. DAVIES: Now, you've written about how these browser extensions that we get on, we at least agree to download them. Do we agree to accept cookies in some way? How does that happen? Are we informed? Or is our consent sought however passively? FOWLER: This is a really good question, and this goes right to the heart of a big conversation we're having about technology right now. What is consent? What is being informed? Yes, lots of websites now, because of a European data law, put up a little notice on the bottom that says, oh, by the way, we use cookies. Are you cool with that? And you either ignore it or you click OK, and you don't really think about it. But does that mean that we really understand that, you know, in the course of a week that over 11,000 times - at least for me - that companies are going to be able to be pushing out these requests to track and follow you around? I don't really think it does. I think, in fact, that we rely on the company that makes the web browser software, Chrome or Firefox, to have our interests at heart, right? And our interests would be to not be tracked. And yet Chrome is not doing that for us. That's in pretty big contrast to its much smaller rival, Firefox, which is made by a nonprofit called Mozilla. Now, as of a couple of weeks ago, it changed its default settings so that when you install it, it blocks those cookies by default, the ones that are involved in tracking. So in the case of my week of web surfing, the 11,000 cookies that Chrome would've let through, Mozilla let through none. DAVIES: Wow. And is there an option to opt out of cookies on Google Chrome? FOWLER: There are options to say block all cookies in Google Chrome, but that would then break some things about the way websites work. Not all cookies are bad. Some help remember, for example, your login to the Washington Post website. So if you turn those on, it really kind of tends to sort of break your web experience. One of the things that Mozilla figured out how to do was block just the ones that are sort of naughty, that are in the business of tracking you for advertising or for marketing, and allow through the ones that you want. So it's about sort of seeking that balance. Now, for me, this experience made me realize that Chrome has essentially become surveillance software. It's surveillance software for the advertising industry and for Google itself. So I made a switch to Mozilla, and I'm very happy I did. DAVIES: You recently did a report on what your iPhone was doing while you were sleeping. What did you find? FOWLER: I found that my iPhone is very busy while I'm sleeping, talking to lots of companies that I've never heard of and sharing with it lots of personal details, things like my exact address and my email address and my name. And that really, really surprised me. DAVIES: So what's happening here? FOWLER: We fill up our phones with apps. And when we do that, we presume that because they came from Apple's app store that they've been vetted and they're not - you know, they're respecting all of the privacy practices that we have come to expect from Apple because of its marketing. But it turns out, apps use something that are called trackers. They're a little bit like cookies that you get on the Web, but they're just embedded inside the apps themselves. And these trackers do lots of different things. Some of them help app-makers just figure out how people are using the apps so that they can make them work better. Others belong to data companies and advertising companies. Google makes trackers. Facebook makes trackers. Some are in there to gather data about people that are using apps so that they can sell it. And that's one way app-makers can make money. And these trackers inside the apps, as long as they're on your phone, they have the ability to essentially run whenever they want to, including while you're sleeping at night and not even using your phone. DAVIES: Wow. Can you shut them down at night? Should you - if you power your phone off, does it inhibit this? FOWLER: If you power off your phone, nothing will be coming out of your phone, either from trackers or from calls that might come in or go out of your phone. But that doesn't really stop the problem. Because as soon as you power your phone back up, the apps will wake back up, and they'll get back in touch with their trackers. The problem is really at the core of how apps are made and the kinds of requirements that Apple and Google and other phone, you know, store, app store-makers place on them. DAVIES: What kinds of requirements they place or fail to place - what do you mean? FOWLER: Indeed. Right now, you know, to get into the Apple app store or the Google Play store, you have to have your app reviewed by one of those companies. They do look to make sure that, you know, they are - that they have a privacy policy, that they are generally abiding by, you know, the rules that they set out in their app store guidelines. But they don't really look under the hood to understand, who are these other companies that they might be talking to, these tracker companies? And what sorts of data are they sending to them? They don't do that vetting for us. And unfortunately, we as consumers can't really see that, either. To figure out what my phone was doing while I slept at night and also during the day, I had to hack my phone. I went to a guy who used to work for the NSA. His name is Patrick Jackson. He now works for a technology company called Disconnect. And he showed me how to do something called a man-in-the-middle attack on my iPhone that basically, you know, kept a copy of all of the data going in and out of my phone while I slept at night so that we could look through it together. That's the level I had to go through to figure out what kind of data was flowing out of my phone and what trackers were running. I couldn't learn any of that from - either from Apple's software or from reading the privacy policies of these companies. DAVIES: So I understand this - so when an app is permitted to be sold in the iPhone store, does Apple require them not to use trackers and some people just aren't honest about it, or do Apple's rules permit them to include trackers in the apps that you download? FOWLER: Until very recently, Apple's rules permitted them to use whatever trackers they wanted. If you had given an app permission to collect your location - and it does pop up a thing saying, can we collect your location? - if you'd given it that then it could share that with whatever trackers it wanted. About two weeks after my story came out in The Washington Post about what my iPhone did while I was sleeping, Apple announced that it was going to now ban trackers in children's apps. So ones that were, you know, targeting, you know, people under the age of 13, they said they would no longer allow them to use third-party trackers. That is an admirable move in many ways. But then my question is, why is it OK in adult apps but not in kids' apps? DAVIES: And what kinds of information is the tracker transmitting about us? FOWLER: It could really be a wide range of things. You know, when I looked, you know, underneath the hood, just while I was sleeping, apps that I saw were using trackers included things like weather. com or - The Washington Post website had trackers. There was another one that's a popular app for kind of, like, checking with the police scanners, called Citizen. It was sending its trackers a lot of information, including my exact GPS coordinates and my email. And in that case, that violated its own privacy policy, and it later changed that after I called them. But still, it was happening. GROSS: We're listening to the interview FRESH AIR's Dave Davies recorded with Geoffrey Fowler, who writes a consumer-oriented technology column for The Washington Post. They'll pick up where they left off after a break, and critic Soraya Nadia McDonald will review the final season of the Netflix series \"Orange Is The New Black. \" I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF MUSIC) GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to the interview FRESH AIR's Dave Davies recorded with Washington Post technology columnist Geoffrey Fowler. Fowler has been investigating ways our computers, smartphones and smart speakers collect our personal information and what they do with it. Fowler has covered digital technology and Silicon Valley for years. When we left off, Fowler was describing how some phone apps have trackers that collect our data, which is often then sold to marketing and advertising companies. One example Fowler found was food delivery apps like Grubhub, Caviar and DoorDash. DAVIES: You found one food delivery app called DoorDash sends data to nine different trackers. FOWLER: Yeah, this one was pretty shocking to me. Again, think about this from the perspective of all of us. You know, you put an app on your phone like DoorDash. And you think, OK, DoorDash is here. When I open DoorDash, I have a relationship with DoorDash, with this company that's going to have someone bring food to my house. You don't think that you're going to have a relationship with nine other companies, including Facebook and Google, who get to know now and keep a log of every time you're hungry (laughter) and open this app and, you know, want to order some pizza. None of that is disclosed to us. This is all lost in this murky world of data. And that's sort of been the impetus behind a whole series of stories I've been doing for the Post. I kind of call it the secret life of your data, just kind of looking under the hood at all the things, all the data about us that's being passed around and traded and sold that we don't normally have a way to understand or see. Because if we don't know where our data is going, how can we even begin to hope to protect our privacy? DAVIES: And just so I understand the relationships, a food delivery app would would give data to nine trackers because each of them pays them a couple of fractions of pennies for it, or - why? FOWLER: There might be a variety of reasons. So for example, one tracker might be in the business of just giving data back to DoorDash to say, like, this is how your app functions. This is the areas of the app that people spend the most time on. These are the areas where people swipe around the most. It might also help them combat fraud. I guess there's a problem with people setting up fake phones to put in fake orders. So they say they want to be able to tell whether your phone is literally physically moving using the gyroscope in it to see if it's a real human on the other end of it. That's another kind of tracker. There's also trackers in there for advertising companies. So Google and Facebook are in there. And DoorDash says, oh, well, they put them in there just so that they could see whether their advertising was working because DoorDash does a lot of advertising with Google and Facebook. But the cost of all of this is that all of these different kinds of companies get to know every time you're opening DoorDash. And any time a company has data about you, who's really making sure that they're using it appropriately? Who's making sure that they're going to delete it soon enough? Who's making sure that they're - they have good security, and it's not going to get stolen and sold off somewhere? DAVIES: And do these trackers slow the apps down or cause further battery drain, or do they impose data charges on us? FOWLER: That's the other piece of app trackers, is that they do a whole bunch of bad things for our phone. Over the course of a week, I found 5,400 different trackers activated on my iPhone. Yours might be different. I may have more apps than you. But that's still quite a lot. If you multiplied that out by an entire month, it would have taken up 1. 5 gigabytes of data just going to trackers from my phone. To put that in some context, the basic data plan from AT&T is only 3 gigabytes. So imagine (laughter), half of your data plan is just eaten up by tracker companies who you don't want running anyway. So it's not only eating up your data. But then, every time it pings the network, that's another hit on your battery life too. So it's - this stuff really isn't in our interest, either from our privacy or from just keeping our phones running. DAVIES: You know, and sometimes when we're browsing, and we want to go to a site or fire up an application, we get a screen that says, do you want to sign in using Facebook? What happens? FOWLER: No, you don't. DAVIES: OK. (LAUGHTER) DAVIES: Why? FOWLER: Because you're giving Facebook the ability to then track every time you're using that website or every time you're using that app. And I'm sure that that website and app then have lots of other tracker pixels or cookies or software baked into them that send other information to Facebook about what you're up to. Google does the same thing. You'll sometimes get a button - do you want a sign in with Google? Same problem. These companies are in the business of data. They might tell everybody that they don't sell our data, but it is certainly very valuable to them. Actually, that's why they don't sell it because they want to keep it for themselves because they then use all this information about what apps you're using, what you're doing in them, what websites you go to, where you are physically with your phone - like, where you are in the world according to your GPS coordinates. They put all of that into their dossiers on each of us to - so that companies can target us with advertising. DAVIES: So if I'm going to sign into a music service like Spotify, and I sign in with Facebook, does that mean Spotify then has access to all of my Facebook data and my friends' data? FOWLER: Not necessarily. It used to be that Facebook was a lot more open about sharing your Facebook data and your friends' data with apps. They shut down a lot of that a couple of years ago. And that's the problem they got into with Cambridge Analytica as well, that they were allowing apps to collect data about your friends and then pass that along. So Facebook has gotten a lot more controlling with its data. Again, they'll say that's for privacy, but it's really because they realize that data is too valuable. They want to keep it for themselves so they can charge companies to market to you through them. DAVIES: You know, you have some columns that are on very specific topics that are really interesting. And one of them - you write about how to handle robocallers on your cellphones, and you mention getting the Do Not Call list. But there are some fight-back apps that people pay for that really get aggressive on this. You want to explain this? FOWLER: Yeah. There are some apps out there that - beyond just blocking bad calls, that they actually will try to torture robocallers. So they intercept the calls, and they listen briefly to see if they detect it as being a robocaller. And if they do, instead of passing the phone call on to you, they'll stay on the line. And they use artificial intelligence to try to keep the - either the person or the robot on the other end on the line by sort of teasing them. Sometimes they'll have, like, a Donald Trump impersonator voice that talks to them or somebody snoring or somebody just talking a lot. And then, for kicks, they will send you - they'll send you a recording of what happened afterwards. DAVIES: All right. So if you're really into it (laughter). FOWLER: If you're really into it. DAVIES: You write about smart speakers like Alexa and their potential to be conducting surveillance. And I guess it's worth noting that Alexa's owned by Amazon, whose chair, Jeff Bezos, is the owner of the paper that you work for, the Washington Post. How much is - are Alexa smart speakers recording of our lives? FOWLER: First, I'll note, yes, the Post is owned by Jeff Bezos, but I'm happy to report that I am at liberty to criticize Amazon as much as I'd like, including by digging into what Alexa is really getting up to. So I think one thing that a lot of folks do not realize about smart speakers with Alexa or with Google's Assistant or Siri is that, by default, they're keeping the recordings of everything that you say. So that means that you sort of think like, OK, well, but it only records when you call out the name, when you call out Alexa or call out Siri. Well, actually that's not the case. It records whenever it thinks it hears one of those calls to action. So I did an experiment where I went back to four years of recordings that Alexa has made of me and my family at home because, again, they're all kept there by default. And I spent a couple of days listening to all of them. And when I did that, I found, you know, all of these strange fragments of my life. So things you would expect, like setting the spaghetti timer or, you know, asking to hear a song, that was in there. But there were also lots of times, dozens of occasions, where Alexa was recording snippets of conversation or television just kind of randomly on its own. For example, it seemed to kind of go off a lot on its own whenever I was watching \"Downton Abbey. \" DAVIES: (Laughter). FOWLER: I had a lot of people with that particular posh British accent in the collection. It also went off when my family members were talking about medication. It went off when a friend was doing a business transaction. And listening to this archive, which any of the listeners can do themselves - if you go to amazon. com/alexaprivacy, you'll be able to dig into your collection as well - listening to this really made me think differently about what Alexa has become in our lives. It's - it is, yes, an assistant, but it is also an eavesdropper. And it is collecting this information and not giving us the power to tell it to stop. Now, Amazon does give you an option that you can go in and delete past recordings. But you can't tell it just don't keep the recording in the first place. DAVIES: And they keep everything forever? FOWLER: They keep everything forever. What's interesting is we were talking before about how these tech companies sometimes give us false choices. And this is, again, one of these false choices. Amazon says it needs all of this data to make Alexa smarter, to improve its artificial intelligence. And yet, archrival Google actually now by default does not do this. It does not keep the recordings from Google Assistant or its smart speakers or its phone-based assistant. It does not keep those by default. It deletes. You have - you'd have to go in and tell it that you want it to keep those. So here, you know, who's right? Do you really have to keep it, Amazon? I think not. And the reason they're doing it is because they can get away with it because most of us haven't noticed and haven't sort of spent the time to kind of dig into the details. DAVIES: We're speaking with Geoffrey Fowler. He writes a consumer-oriented technology column for the Washington Post. We will take a short break here, then we will talk some more. This is FRESH AIR. (SOUNDBITE OF MUSIC) DAVIES: This is FRESH AIR. And we're speaking with Washington Post columnist Geoffrey Fowler. He writes consumer-oriented columns about navigating the confusing world of personal technology, including computers, smartphones and smart speakers. Alexa seems to listen to an awful lot. Does Siri do the same thing on our iPhones? FOWLER: Siri does do the same thing on our iPhones and on the HomePod, which is their home speaker. So I have a corner of my living room where all of the virtual assistants live together. I have all of the connected speakers. I'm sure someday they'll start talking to each other. But I watch to see which ones go off at random times. I have to say, the Apple one goes off quite frequently, maybe even more than the others. Apple's policy is it keeps the recordings of everything that it hears, and it does not give you the option to tell it not to keep the recordings. But they anonymize it, so they don't associate it with your individual account in your name. Amazon still associates it with who you are. DAVIES: So you - when you say that the Apple device goes off more frequently, it starts recording more frequently, or seems to. FOWLER: That's right. Last night, I was watching a TV show. And in the middle of it, Siri, on the HomePod, perked up and said, sorry, I can't answer that question. And no one had asked Siri a question. DAVIES: (Laughter) Does this make you want to disconnect Alexa? FOWLER: I have to say, after doing this reporting, I now keep my Alexa speaker on permanent mute. There is a button on the top, that physical button you can press, that says, don't record and don't activate when you hear the name Alexa. But that, of course, sort of defeats the purpose of the device. So again, I just think this is really bad product design on the part of Amazon. They shouldn't put us in a situation where, again, the choice is don't use the technology or give into this kind of surveillance. Thing I would add is that this project of listening to my Alexa recordings made me wonder - I wonder what all the other ways are that Amazon is eavesdropping on my home because anybody with one of these smart speakers probably knows that, like, oh, you can hook them up to connected devices in your home, right? You can connect it up to light bulbs and thermostats and doorbells and all sorts of things. And I had certainly done that. I'm a technology journalist, and I review all this stuff. And my house is filled with gadgets. And so I went down the path of trying to figure out, OK, well, what other data from my home other than just me and my family's voices is it keeping? And I found, I mean, enough that would make the - you know, the East German police blush to see this kind of data. For example, my Nest thermostat was collecting in 15-minute increments over the last six or seven years not only the temperature in my home, but also whether there had been a person that passed in front of it. So there was this perfect record of every time there had been someone in my hallway for years and years and years that was being sent both to Google and to Amazon - because Amazon's requirement is if your gadget connects in with Alexa that they get to keep a copy of all this data, too. And then it started multiplying. So yeah, there was the thermostat, but there's also my garage door. It was doing the same thing. Then there was my connected lights. Literally, Amazon was getting a record of every time a light switched on and off in my house. DAVIES: And how did you discover that Amazon had these records of your thermostat changes and when people were walking down the hallway? FOWLER: I started asking. So I went to the companies that make these devices, and I said, hey, can you tell me what data you're collecting and who you're sharing it with? Some of them would not answer that question. And that - my frustration with that really animated what's become a yearlong project by me to sort of see if I can look under the hood and figure out what data is being collected and who it's being shared with. I've been looking at that and all sorts of things - connected devices in our home, our Web browser. And I've got even more - more and more projects coming down the pike. But some companies did answer. Some just pointed me to their privacy policies, which didn't really specify what they were up to. So it's really quite difficult for us as consumers to understand the secret life of these devices and the data they're collecting and who they're sharing it with. And I think that's a big problem. DAVIES: There's a lot of talk of congressional regulation of digital media. Are you seeing things that encourage you? FOWLER: There's a lot of talk in Congress, but not a lot of action when it comes to data and privacy. You know, we see a lot of individual members of Congress, you know, writing letters or holding hearings. But there's been little effort to really turn that into legislation that could even get to the point of being voted on. Right now, we see Congress also interested in these big questions about whether these tech companies are too big - right? - and that they need to be broken up for antitrust reasons, which, at the end of the day, is also about data because the reason why they're so big and so powerful and have made billions of dollars is because they have control over so much data about our lives. The thing that actually gives me the most hope is what I see happening in the states. So California passed the California Consumer Privacy Act, which is going to take effect in January. And it is, I think, going to become the closest thing that we have to an American privacy law. Of course, it only applies to the residents of California, but there are so many residents of California that a lot of companies are going to have to sort of essentially comply with it for everyone. And it really - it's about disclosure, which is, I think, where this all needs to start - you know, when this law takes effect, that these companies - even if they're not tech companies - any company that collects data about you is going to have to be able to say - you know, answer the question that I - some of the questions I posed before. What's being collected? Who is it being shared with? Tell me if it's being sold. Give me the chance to say, no, you're not allowed to sell it. I'm actually super excited when this law kicks in. I already have plans for January 1, 2020, to send out lots of request letters to companies as a California resident to tell me what data they're collecting and what they're doing with it. DAVIES: Well, Geoffrey Fowler, thanks so much for speaking with us. FOWLER: My pleasure. GROSS: Geoffrey Fowler writes a consumer-oriented technology column for The Washington Post. He spoke to FRESH AIR's Dave Davies. Coming up, Soraya Nadia McDonald reviews the final season of \"Orange Is The New Black. \" This is FRESH AIR. (SOUNDBITE OF GILAD HEKSELMAN'S \"DO RE MI FA SOL\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-07-31-742223881": {"title": "Facebook's Libra Money Plan Raises Stakes For China's Cryptocurrency Ambitions : NPR", "url": "https://www.npr.org/2019/07/31/742223881/facebooks-digital-money-plan-raises-stakes-for-china-s-cryptocurrency-ambitions", "author": "No author found", "published_date": "2019-07-31", "content": "", "section": "World", "disclaimer": ""}, "2019-08-03-747086462": {"title": "Lawmaker Aims To Curb Social Media Addiction With New Bill : NPR", "url": "https://www.npr.org/2019/08/03/747086462/lawmaker-aims-to-curb-social-media-addiction-with-new-bill", "author": "No author found", "published_date": "2019-08-03", "content": "", "section": "National", "disclaimer": ""}, "2019-08-05-748179892": {"title": "El Paso Shooting: 8chan Website Dropped By Security Firm After Attack : NPR", "url": "https://www.npr.org/2019/08/05/748179892/uniquely-lawless-security-firm-drops-8chan-website-following-el-paso-shooting", "author": "No author found", "published_date": "2019-08-05", "content": "", "section": "National", "disclaimer": ""}, "2019-08-05-748219809": {"title": "Franky Zapata Crosses English Channel On Hoverboard : NPR", "url": "https://www.npr.org/2019/08/05/748219809/this-time-franky-zapata-makes-it-across-the-english-channel-on-a-hoverboard", "author": "No author found", "published_date": "2019-08-05", "content": "", "section": "Sports", "disclaimer": ""}, "2019-08-05-748163839": {"title": "Flying Frenchman Leapfrogs English Channel On His Homemade Hoverboard : NPR", "url": "https://www.npr.org/2019/08/05/748163839/flying-frenchman-leapfrogs-english-channel-on-his-homemade-hoverboard", "author": "No author found", "published_date": "2019-08-05", "content": "RACHEL MARTIN, HOST: Good morning. Franky Zapata wasn't giving up. Last month, the French inventor tried to cross the English Channel on a jet-powered hoverboard. But he had a mishap halfway through and fell into the sea. He spent the next several weeks rebuilding his hovercraft. And yesterday, he tried it again. This time, he made it - 20 minutes flying like something from the future from France to the U. K. After he landed across the channel, Zapata said next he's going to work on a flying car. But first he's going to take a vacation. RACHEL MARTIN, HOST:  Good morning. Franky Zapata wasn't giving up. Last month, the French inventor tried to cross the English Channel on a jet-powered hoverboard. But he had a mishap halfway through and fell into the sea. He spent the next several weeks rebuilding his hovercraft. And yesterday, he tried it again. This time, he made it - 20 minutes flying like something from the future from France to the U. K. After he landed across the channel, Zapata said next he's going to work on a flying car. But first he's going to take a vacation.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-05-748166877": {"title": "The Website Where Violent White Supremacists State Their Case : NPR", "url": "https://www.npr.org/2019/08/05/748166877/the-website-where-violent-white-supremacists-state-their-case", "author": "No author found", "published_date": "2019-08-05", "content": "DAVID GREENE, HOST: So as we just heard, many experts say 8chan has become this big, powerful, dangerous site. And we want to dig a little deeper and hear a little more about how it has become a real go-to for white terrorist extremists. Here's NPR's Aarti Shahani. AARTI SHAHANI, BYLINE: In the last five months, the El Paso shooter allegedly posted a manifesto on the website 8chan just before he targeted innocent Latinos in a killing spree. The Christchurch, New Zealand, shooter also posted an 8chan manifesto before targeting Muslims. The Poway Synagogue shooter in Poway, Calif. , also posted an 8chan manifesto before targeting Jews. Robert Evans, who studies extremism on the Internet, says the 8chan manifesto is a key part of how white extremism is growing. ROBERT EVANS: That if you shotgun these manifestos out in front of tens of thousands of readers, several times a year, somebody will take the bait. And that's the game they're playing. That's the strategy and it works. SHAHANI: 8chan is a message board that's become a soapbox for extremists. This past weekend, a founder of 8chan, who's since left the company, called for the site to be shut down. He said it was a receptive audience for domestic terrorists, according to The Washington Post. Evans, who publishes his research on Bellingcat, a site for online investigations, analyzed the Twitter and LinkedIn posts of the El Paso shooter. Up to 2017, his social media presence suggested he was a normal-seeming conservative, Evans says. But then the young man turned to 8chan. EVANS: The El Paso shooter directly cited in his manifesto the Christchurch shooter's manifesto as an inspiration. And, you know, I don't think this attack happens without the other one. SHAHANI: 8chan is not on the fringes of the Internet. It's popular, particularly in America. Amazon, which tracks website traffic, says 8Chan ranks 1,813 in the U. S. Advertising experts say that's high. McDonald's, by way of comparison, ranks 1,992. 8chan gets about 15 million unique visitors a month, according to SimilarWeb. Given this popularity, Evans believes 8chan deserves more attention from law enforcement. EVANS: If 8chan was an Islamic extremist gathering place on the Internet, it would not be around anymore. SHAHANI: Top politicians have pointed to different causes behind the recent mass shootings. On Sunday, President Trump said the mass shooters have a mental illness problem. Leading Republican and House Minority Leader Kevin McCarthy put the blame on videogames that have shooters. Experts who study extremism on the Internet say political leaders need to stop looking for excuses. Brittan Heller, a fellow at the Harvard Kennedy School, says they need to acknowledge the crisis of white extremism that's being aided by online platforms. She rejects the videogame thesis. BRITTAN HELLER: That is a cop-out. And I think, as Americans, we should demand better. SHAHANI: 8chan did not respond to NPR's request for comment. After the Christchurch, New Zealand, mass shooting, 8chan's current chief, Jim Watkins, said his technology was not at fault for providing the shooter with a platform. His site is just a tool, and his company cooperates fully with law enforcement. Aarti Shahani, NPR News. DAVID GREENE, HOST:  So as we just heard, many experts say 8chan has become this big, powerful, dangerous site. And we want to dig a little deeper and hear a little more about how it has become a real go-to for white terrorist extremists. Here's NPR's Aarti Shahani. AARTI SHAHANI, BYLINE: In the last five months, the El Paso shooter allegedly posted a manifesto on the website 8chan just before he targeted innocent Latinos in a killing spree. The Christchurch, New Zealand, shooter also posted an 8chan manifesto before targeting Muslims. The Poway Synagogue shooter in Poway, Calif. , also posted an 8chan manifesto before targeting Jews. Robert Evans, who studies extremism on the Internet, says the 8chan manifesto is a key part of how white extremism is growing. ROBERT EVANS: That if you shotgun these manifestos out in front of tens of thousands of readers, several times a year, somebody will take the bait. And that's the game they're playing. That's the strategy and it works. SHAHANI: 8chan is a message board that's become a soapbox for extremists. This past weekend, a founder of 8chan, who's since left the company, called for the site to be shut down. He said it was a receptive audience for domestic terrorists, according to The Washington Post. Evans, who publishes his research on Bellingcat, a site for online investigations, analyzed the Twitter and LinkedIn posts of the El Paso shooter. Up to 2017, his social media presence suggested he was a normal-seeming conservative, Evans says. But then the young man turned to 8chan. EVANS: The El Paso shooter directly cited in his manifesto the Christchurch shooter's manifesto as an inspiration. And, you know, I don't think this attack happens without the other one. SHAHANI: 8chan is not on the fringes of the Internet. It's popular, particularly in America. Amazon, which tracks website traffic, says 8Chan ranks 1,813 in the U. S. Advertising experts say that's high. McDonald's, by way of comparison, ranks 1,992. 8chan gets about 15 million unique visitors a month, according to SimilarWeb. Given this popularity, Evans believes 8chan deserves more attention from law enforcement. EVANS: If 8chan was an Islamic extremist gathering place on the Internet, it would not be around anymore. SHAHANI: Top politicians have pointed to different causes behind the recent mass shootings. On Sunday, President Trump said the mass shooters have a mental illness problem. Leading Republican and House Minority Leader Kevin McCarthy put the blame on videogames that have shooters. Experts who study extremism on the Internet say political leaders need to stop looking for excuses. Brittan Heller, a fellow at the Harvard Kennedy School, says they need to acknowledge the crisis of white extremism that's being aided by online platforms. She rejects the videogame thesis. BRITTAN HELLER: That is a cop-out. And I think, as Americans, we should demand better. SHAHANI: 8chan did not respond to NPR's request for comment. After the Christchurch, New Zealand, mass shooting, 8chan's current chief, Jim Watkins, said his technology was not at fault for providing the shooter with a platform. His site is just a tool, and his company cooperates fully with law enforcement. Aarti Shahani, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-05-748163919": {"title": "TikTok, The Internet's Hottest Meme Breeding Ground, Turns 1 : NPR", "url": "https://www.npr.org/2019/08/05/748163919/tiktok-the-internets-hottest-meme-breeding-ground-turns-1", "author": "No author found", "published_date": "2019-08-05", "content": "RACHEL MARTIN, HOST: Have you heard of TikTok? This is the social media app that lets users create super-short videos and then loop them. And if it seems like tick tock showed up on our phones 15 seconds ago, you might be shocked to learn that the app is now an entire year old. And now, as if on cue, its popularity is in decline. Taylor Lorenz covers technology for The Atlantic. And she's with me now to talk about it. Hey, Taylor. TAYLOR LORENZ: Hey. MARTIN: So before we talk about what is hot being less so, explain how TikTok works. LORENZ: TikTok is a short-form video app. It allows you to record videos as long as 15 seconds to a minute but to - down. That could be music or a voiceover or anything. It had a lot of fun video-editing techniques. MARTIN: So we can make short videos on other platforms. What makes this one special, and what's been the impact of it? LORENZ: Sure. It's been really catchy. I mean, yeah, you can make short videos on other platforms. But you can't really share them as easily. And you can't make them as entertaining to watch, I guess. I think the ability to add kind of any sound inspires creativity and sort of - it's just fun. I don't know. It's fun to make. They're fun to watch. A lot of people make them together. MARTIN: And it's attracted corporate America, right? I mean, major brands have started using TikTok. LORENZ: Yeah. MARTIN: Has that changed it? LORENZ: Major - yeah. Chipotle actually did a massive campaign with YouTuber David Dobrik. They've seen great success. Lots of other brands are on there, too. But no, this hasn't really changed it. It's still small. It's still emerging in the advertising space. But it really packs a punch. I mean, people spending their dollars there are getting more impact than spending them even on Instagram and Facebook. MARTIN: I mean, one year in regular life is like seven years in Internet life or something like that. (LAUGHTER)MARTIN: What does that mean that in the course of a year, TikTok got so popular, and now popularity is already now fading? LORENZ: How is the popularity fading? I don't think - I would kind of push back on that. I haven't seen it really decline. If anything, it seems to continue to be growing. It started off a year ago. It had already been launched internationally and was kind of huge in Southeast Asia, India, all over the world. And then ByteDance, which owns TikTok, launched in the U. S. And it kind of - I would say it, like, was under the radar. People thought it was cringey at first. But in the past six months, it's really exploded. And, you know, I feel like it kind of just continues to grow. I mean, you'll see this even evident in terms of, like, the videos that go viral. They get more and more and more views. MARTIN: How much of that has to do with the company that started this, the tech startup ByteDance, which is the most valuable tech startup in the world? I mean, is that going to give TikTok more staying power, do you think, in the long term? LORENZ: Yeah. I mean, again TikTok's popularity is only rising. ByteDance just - as you mentioned, one of the most valuable startups in the world - is pouring tons of money into this. So, you know, spent a billion - they spent a billion dollars in marketing alone last year, you know, getting people to download the app, promoting it to American users. And they could continue to spend a billion dollars every year and be fine. This company is massive. MARTIN: Taylor Lorenz, a staff writer for The Atlantic. You can probably see some of her TikTok videos, I imagine, somewhere on the Internet. Taylor, thanks. We appreciate it. LORENZ: Thanks, guys. RACHEL MARTIN, HOST:  Have you heard of TikTok? This is the social media app that lets users create super-short videos and then loop them. And if it seems like tick tock showed up on our phones 15 seconds ago, you might be shocked to learn that the app is now an entire year old. And now, as if on cue, its popularity is in decline. Taylor Lorenz covers technology for The Atlantic. And she's with me now to talk about it. Hey, Taylor. TAYLOR LORENZ: Hey. MARTIN: So before we talk about what is hot being less so, explain how TikTok works. LORENZ: TikTok is a short-form video app. It allows you to record videos as long as 15 seconds to a minute but to - down. That could be music or a voiceover or anything. It had a lot of fun video-editing techniques. MARTIN: So we can make short videos on other platforms. What makes this one special, and what's been the impact of it? LORENZ: Sure. It's been really catchy. I mean, yeah, you can make short videos on other platforms. But you can't really share them as easily. And you can't make them as entertaining to watch, I guess. I think the ability to add kind of any sound inspires creativity and sort of - it's just fun. I don't know. It's fun to make. They're fun to watch. A lot of people make them together. MARTIN: And it's attracted corporate America, right? I mean, major brands have started using TikTok. LORENZ: Yeah. MARTIN: Has that changed it? LORENZ: Major - yeah. Chipotle actually did a massive campaign with YouTuber David Dobrik. They've seen great success. Lots of other brands are on there, too. But no, this hasn't really changed it. It's still small. It's still emerging in the advertising space. But it really packs a punch. I mean, people spending their dollars there are getting more impact than spending them even on Instagram and Facebook. MARTIN: I mean, one year in regular life is like seven years in Internet life or something like that. (LAUGHTER) MARTIN: What does that mean that in the course of a year, TikTok got so popular, and now popularity is already now fading? LORENZ: How is the popularity fading? I don't think - I would kind of push back on that. I haven't seen it really decline. If anything, it seems to continue to be growing. It started off a year ago. It had already been launched internationally and was kind of huge in Southeast Asia, India, all over the world. And then ByteDance, which owns TikTok, launched in the U. S. And it kind of - I would say it, like, was under the radar. People thought it was cringey at first. But in the past six months, it's really exploded. And, you know, I feel like it kind of just continues to grow. I mean, you'll see this even evident in terms of, like, the videos that go viral. They get more and more and more views. MARTIN: How much of that has to do with the company that started this, the tech startup ByteDance, which is the most valuable tech startup in the world? I mean, is that going to give TikTok more staying power, do you think, in the long term? LORENZ: Yeah. I mean, again TikTok's popularity is only rising. ByteDance just - as you mentioned, one of the most valuable startups in the world - is pouring tons of money into this. So, you know, spent a billion - they spent a billion dollars in marketing alone last year, you know, getting people to download the app, promoting it to American users. And they could continue to spend a billion dollars every year and be fine. This company is massive. MARTIN: Taylor Lorenz, a staff writer for The Atlantic. You can probably see some of her TikTok videos, I imagine, somewhere on the Internet. Taylor, thanks. We appreciate it. LORENZ: Thanks, guys.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-06-748810990": {"title": "In The Age Of Smartphones, Parents Are Encouraged To Be Media Mentors, Not Gatekeepers : NPR", "url": "https://www.npr.org/2019/08/06/748810990/in-the-age-of-smartphones-parents-are-encouraged-to-be-media-mentors-not-gatekee", "author": "No author found", "published_date": "2019-08-06", "content": "AUDIE CORNISH, HOST: Parenting in the age of smartphones can be really stressful. Health experts from the World Health Organization on down say we should limit kids' screen time to a, quote, \"healthy level. \" But infants aside, that doesn't mean zero. There's a growing push to encourage parents to be media mentors rather than gatekeepers. NPR's Anya Kamenetz has looked into this for our Life Kit parenting podcast. She's here to talk us through it. Welcome back to the program, Anya. ANYA KAMENETZ, BYLINE: Thanks, Audie. CORNISH: Define media mentoring for us. KAMENETZ: So the philosophy behind this is pretty simple, right? It's trying to use digital devices together with your children as much as you can and assisting them in understanding what it is that they're doing on those devices. So one proponent of this is Mimi Ito. She's a researcher at the University of California, Irvine. And she says we need to face the facts that media, especially things like video games, are a major source of fun for kids. MIMI ITO: Unless parents can find a way to somehow understand and engage with that in a positive way, video games can often become a source of tension between parents and kids. And so we see time and time again that parents aren't engaged in the kind of mentoring and guidance around video games that they do for other parts of kids' play and growing up and friendship relationships. KAMENETZ: So she says you need to get in there and play video games with your kids. And she also says that this is fun. ITO: It's a lot more fun than clocking screen time and, you know, doing the finger-wagging thing. CORNISH: I thought it was pediatricians who told us to do the finger-wagging thing. (Laughter) I'm a little bit offended by this. KAMENETZ: I know, right? CORNISH: So how does this work out in real life? KAMENETZ: So I visited a family in Washington, D. C. - Chris Wallace, Latoya Peterson and their son Gavin, who's 5. This is Gavin. (SOUNDBITE OF KOJI KONDO'S \"OVERWORLD THEME\")GAVIN: (Vocalizing). CORNISH: Good to know that game is still popular. I recognize the tune (laughter). KAMENETZ: Oh, my gosh. Nintendo's having this incredible comeback. And that's his favorite stuffed animal ever - that somehow matches up the \"Mario Bros. \" game plus Captain Marvel. Anyway, almost every night after dinner, this family jumps on the couch and plays big, complex PlayStation video games. GAVIN: Oh, my gosh. You want to see what happens? Hit X, and it'll make a sound. See. They're trying to fake. He's a darkness guy. CORNISH: What are they playing there? KAMENETZ: OK, so it's a big game called \"Kingdom Hearts\" that has all these different Disney characters kind of on one universe. And they play other games together, too, even some that are not necessarily meant for young kids, like one called \"Persona. \" Latoya Peterson says it's just certain parts of that game that are age-appropriate. She stresses this. LATOYA PETERSON: Normally, he's playing with me. Normally, we play together. KAMENETZ: And I should say, you know, all of this comes really naturally to Peterson. You know, she grew up playing video games, even though her dad didn't necessarily want her using his system. PETERSON: I would just wait until dad wasn't home, sneak into the room (laughter) and play. KAMENETZ: And today, she's been really successful in new media. And she's the co-founder of an all-women-of-color-led video game company called Glow Up Games. CORNISH: Can I ask something, Anya, here? Essentially, are they arguing that you can play video games along with your kids the same way you would read along with your kids and get some kind of benefit from it? KAMENETZ: That's exactly right. When you are sharing media time with your kids, you're giving them the chance to understand better the messages that are coming across. You can learn social and emotional skills from this, just as you would from a story. CORNISH: How does this square with health recommendations that kids should actually limit screen time, especially before bed? KAMENETZ: So being a media mentor doesn't mean that you say yes all the time, and you're always handing out candy. The American Academy of Pediatrics says parents should keep your schedule, prioritize kids' sleep, outdoor play and family meals. And Latoya Peterson and Chris Wallace actually do all of this. CORNISH: Are there certain things parents should be doing when they're using screens with their kids? KAMENETZ: So consistently having conversations about what they're playing or watching is what experts call active mediation. And Latoya Peterson sees video games as an opportunity. She sees them as a way that Gavin can get comfortable with technology, to pick up new skills, not just tech skills, either. PETERSON: One of the big things we're working on right now is the concept of resiliency and not quitting when something is hard. And games are great with that because the whole idea - like, I think we were in some castle. And he's like, Mom, this castle - 'cause I died, like, twice in this castle, like, immediately. And Gavin's like, Mom, this castle's too hard. We should stop. And I was like, Gavin, this is the point. Like, sometimes, things are hard, and you have to go back and try again, or you try something different. And I've noticed he does that in his real life. GAVIN: Sometimes, you lose and lose and lose. And in \"Persona,\" sometimes, when a monster kills us and gets our blue heart, we die. We lost. And that means our battle game is over. CORNISH: Gavin sounds amazingly sweet. There are parents, though, who, let's say, use screens to occupy their kids so that they can get some stuff done. KAMENETZ: I don't know what you're talking about. I've never done that. CORNISH: I don't know parents like this. I know they're out there. So what if you can't make time to have this kind of hands-on interaction the way Chris Wallace and Latoya Peterson are doing? KAMENETZ: So this is a key point. I'm glad you brought it up. Dr. Jenny Radesky - she is the pediatrician who lead-authored that American Academy of Pediatrics guidelines on kids and media. So she's the rule maker. And she says that, yes, sometimes, kids are going to use screens by themselves. And what happens after that is you try to have a dialogue with them and ask them questions about what they're watching, what they're playing. JENNY RADESKY: What do you like about this? And what seems annoying or creepy about it, too? KAMENETZ: And Dr. Radesky says through these conversations, we can help our kids develop a bit of self-regulation around screen time, also. RADESKY: Do you think it's OK to sit and watch slime videos for an hour? Like, what's good about that? What's not good about that? CORNISH: This all makes sense when they're very young. As kids get older, they can be less interested in hanging out with their parents. Does this media mentoring idea work at older ages? KAMENETZ: Absolutely, it can, according to Mimi Ito. She's the researcher at UC Irvine. Her children now are 18 and 21. But when her son was a teen she saw her role shifting. ITO: To me asking a lot of questions and observing my son's gameplay and being more of a interested observer, supporter, cheerleader rather than somebody that was actually playing the same games. KAMENETZ: So being that cheerleader and supporting her kids' interests - she credits that with kind of leading to both of her children now studying computer science, for example. CORNISH: Finally, Anya, we've been talking about this in the context of video games. But for many parents, it's more likely to involve our smartphones and our tablets. How should we be mentoring our behavior with those? KAMENETZ: That's a great point. So the point is here - our kids are watching and learning from us 24 hours a day, even when we're not being exemplars. So if you are constantly kind of pulled into your smartphone, they're going to absorb that that's an OK way to treat your family members. On the other hand, on the positive side, you know, most of us use technology in the course of our work, our personal passions to learn about the world, to discover new music, to keep in touch with friends and family. And those are all positive things that we can share with our kids by modeling that, as well. CORNISH: That's NPR's Anya Kamenetz. Anya, thanks so much. KAMENETZ: Thanks, Audie. CORNISH: And Anya hosts NPR's Life Kit parenting podcast. The Life Kit series has practical tips on all sorts of things. You can find it at npr. org/lifekit. AUDIE CORNISH, HOST:  Parenting in the age of smartphones can be really stressful. Health experts from the World Health Organization on down say we should limit kids' screen time to a, quote, \"healthy level. \" But infants aside, that doesn't mean zero. There's a growing push to encourage parents to be media mentors rather than gatekeepers. NPR's Anya Kamenetz has looked into this for our Life Kit parenting podcast. She's here to talk us through it. Welcome back to the program, Anya. ANYA KAMENETZ, BYLINE: Thanks, Audie. CORNISH: Define media mentoring for us. KAMENETZ: So the philosophy behind this is pretty simple, right? It's trying to use digital devices together with your children as much as you can and assisting them in understanding what it is that they're doing on those devices. So one proponent of this is Mimi Ito. She's a researcher at the University of California, Irvine. And she says we need to face the facts that media, especially things like video games, are a major source of fun for kids. MIMI ITO: Unless parents can find a way to somehow understand and engage with that in a positive way, video games can often become a source of tension between parents and kids. And so we see time and time again that parents aren't engaged in the kind of mentoring and guidance around video games that they do for other parts of kids' play and growing up and friendship relationships. KAMENETZ: So she says you need to get in there and play video games with your kids. And she also says that this is fun. ITO: It's a lot more fun than clocking screen time and, you know, doing the finger-wagging thing. CORNISH: I thought it was pediatricians who told us to do the finger-wagging thing. (Laughter) I'm a little bit offended by this. KAMENETZ: I know, right? CORNISH: So how does this work out in real life? KAMENETZ: So I visited a family in Washington, D. C. - Chris Wallace, Latoya Peterson and their son Gavin, who's 5. This is Gavin. (SOUNDBITE OF KOJI KONDO'S \"OVERWORLD THEME\") GAVIN: (Vocalizing). CORNISH: Good to know that game is still popular. I recognize the tune (laughter). KAMENETZ: Oh, my gosh. Nintendo's having this incredible comeback. And that's his favorite stuffed animal ever - that somehow matches up the \"Mario Bros. \" game plus Captain Marvel. Anyway, almost every night after dinner, this family jumps on the couch and plays big, complex PlayStation video games. GAVIN: Oh, my gosh. You want to see what happens? Hit X, and it'll make a sound. See. They're trying to fake. He's a darkness guy. CORNISH: What are they playing there? KAMENETZ: OK, so it's a big game called \"Kingdom Hearts\" that has all these different Disney characters kind of on one universe. And they play other games together, too, even some that are not necessarily meant for young kids, like one called \"Persona. \" Latoya Peterson says it's just certain parts of that game that are age-appropriate. She stresses this. LATOYA PETERSON: Normally, he's playing with me. Normally, we play together. KAMENETZ: And I should say, you know, all of this comes really naturally to Peterson. You know, she grew up playing video games, even though her dad didn't necessarily want her using his system. PETERSON: I would just wait until dad wasn't home, sneak into the room (laughter) and play. KAMENETZ: And today, she's been really successful in new media. And she's the co-founder of an all-women-of-color-led video game company called Glow Up Games. CORNISH: Can I ask something, Anya, here? Essentially, are they arguing that you can play video games along with your kids the same way you would read along with your kids and get some kind of benefit from it? KAMENETZ: That's exactly right. When you are sharing media time with your kids, you're giving them the chance to understand better the messages that are coming across. You can learn social and emotional skills from this, just as you would from a story. CORNISH: How does this square with health recommendations that kids should actually limit screen time, especially before bed? KAMENETZ: So being a media mentor doesn't mean that you say yes all the time, and you're always handing out candy. The American Academy of Pediatrics says parents should keep your schedule, prioritize kids' sleep, outdoor play and family meals. And Latoya Peterson and Chris Wallace actually do all of this. CORNISH: Are there certain things parents should be doing when they're using screens with their kids? KAMENETZ: So consistently having conversations about what they're playing or watching is what experts call active mediation. And Latoya Peterson sees video games as an opportunity. She sees them as a way that Gavin can get comfortable with technology, to pick up new skills, not just tech skills, either. PETERSON: One of the big things we're working on right now is the concept of resiliency and not quitting when something is hard. And games are great with that because the whole idea - like, I think we were in some castle. And he's like, Mom, this castle - 'cause I died, like, twice in this castle, like, immediately. And Gavin's like, Mom, this castle's too hard. We should stop. And I was like, Gavin, this is the point. Like, sometimes, things are hard, and you have to go back and try again, or you try something different. And I've noticed he does that in his real life. GAVIN: Sometimes, you lose and lose and lose. And in \"Persona,\" sometimes, when a monster kills us and gets our blue heart, we die. We lost. And that means our battle game is over. CORNISH: Gavin sounds amazingly sweet. There are parents, though, who, let's say, use screens to occupy their kids so that they can get some stuff done. KAMENETZ: I don't know what you're talking about. I've never done that. CORNISH: I don't know parents like this. I know they're out there. So what if you can't make time to have this kind of hands-on interaction the way Chris Wallace and Latoya Peterson are doing? KAMENETZ: So this is a key point. I'm glad you brought it up. Dr. Jenny Radesky - she is the pediatrician who lead-authored that American Academy of Pediatrics guidelines on kids and media. So she's the rule maker. And she says that, yes, sometimes, kids are going to use screens by themselves. And what happens after that is you try to have a dialogue with them and ask them questions about what they're watching, what they're playing. JENNY RADESKY: What do you like about this? And what seems annoying or creepy about it, too? KAMENETZ: And Dr. Radesky says through these conversations, we can help our kids develop a bit of self-regulation around screen time, also. RADESKY: Do you think it's OK to sit and watch slime videos for an hour? Like, what's good about that? What's not good about that? CORNISH: This all makes sense when they're very young. As kids get older, they can be less interested in hanging out with their parents. Does this media mentoring idea work at older ages? KAMENETZ: Absolutely, it can, according to Mimi Ito. She's the researcher at UC Irvine. Her children now are 18 and 21. But when her son was a teen she saw her role shifting. ITO: To me asking a lot of questions and observing my son's gameplay and being more of a interested observer, supporter, cheerleader rather than somebody that was actually playing the same games. KAMENETZ: So being that cheerleader and supporting her kids' interests - she credits that with kind of leading to both of her children now studying computer science, for example. CORNISH: Finally, Anya, we've been talking about this in the context of video games. But for many parents, it's more likely to involve our smartphones and our tablets. How should we be mentoring our behavior with those? KAMENETZ: That's a great point. So the point is here - our kids are watching and learning from us 24 hours a day, even when we're not being exemplars. So if you are constantly kind of pulled into your smartphone, they're going to absorb that that's an OK way to treat your family members. On the other hand, on the positive side, you know, most of us use technology in the course of our work, our personal passions to learn about the world, to discover new music, to keep in touch with friends and family. And those are all positive things that we can share with our kids by modeling that, as well. CORNISH: That's NPR's Anya Kamenetz. Anya, thanks so much. KAMENETZ: Thanks, Audie. CORNISH: And Anya hosts NPR's Life Kit parenting podcast. The Life Kit series has practical tips on all sorts of things. You can find it at npr. org/lifekit.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-06-748721893": {"title": "Man Accused Of Bribing AT&T Employees In Conspiracy To Unlock Millions Of Phones : NPR", "url": "https://www.npr.org/2019/08/06/748721893/man-accused-of-bribing-at-t-employees-in-conspiracy-to-unlock-millions-of-phones", "author": "No author found", "published_date": "2019-08-06", "content": "", "section": "Law", "disclaimer": ""}, "2019-08-06-747609884": {"title": "'Trick Mirror' Finds Hope That Little Truths Will Emerge Amid Absurdities  : NPR", "url": "https://www.npr.org/2019/08/06/747609884/trick-mirror-finds-hope-that-little-truths-will-emerge-amid-absurdities", "author": "No author found", "published_date": "2019-08-06", "content": "", "section": "Book Reviews", "disclaimer": ""}, "2019-08-08-749474600": {"title": "Court Allows Illinois Facebook Users To Sue Over Facial Recognition Software : NPR", "url": "https://www.npr.org/2019/08/08/749474600/users-can-sue-facebook-over-facial-recognition-software-court-rules", "author": "No author found", "published_date": "2019-08-08", "content": "", "section": "Technology", "disclaimer": ""}, "2019-08-08-749318323": {"title": "Government Deadline Approaches To Ban Chinese-Made Surveillance Cameras : NPR", "url": "https://www.npr.org/2019/08/08/749318323/government-deadline-approaches-to-ban-chinese-made-surveillance-cameras", "author": "No author found", "published_date": "2019-08-08", "content": "DAVID GREENE, HOST: Countless federal agencies, military bases and other government buildings are facing a looming deadline. Starting next week, they will be barred from buying Chinese-made surveillance equipment. This ban was tucked into a provision of the 2019 National Defense Authorization Act. But there's confusion over what the law means, how it affects equipment that's already in use and how it can be implemented on time. Here's more from NPR's Jackie Northam. JACKIE NORTHAM, BYLINE: Any time you pass through security at a government building, you'll see them - small surveillance cameras watching you. They've become an accepted part of our lives. There's a good chance those cameras are made by a Chinese company or have Chinese components in them. They're meant to be providing security, but Congress sees them as a way for China to spy on the U. S. Peter Kusnic is a tech writer with Freedonia Group, a business research organization. PETER KUSNIC: The very close relationship between the Chinese government and its tech sector raises these concerns that these security cameras can be used to hack into American life. NORTHAM: Now Congress is putting the brakes on federal agencies using the Chinese surveillance cameras made by companies such as Hikvision, which is partially owned by the Chinese government. Despite an upcoming deadline, there are thousands of these cameras still installed across the country, says Katherine Gronberg with ForeScout Technologies, an IT security firm whose clients include the Defense Department and other federal agencies. Gronberg says her company recently did what she calls a small sampling of some government offices. KATHERINE GRONBERG: We found more than 1,700 of the two varieties of banned cameras. We don't know what agencies those were at, but 1,700 is a lot. They'll have to be removed by mid-August. WILL CARTER: There is zero chance that this law could be enacted fully and implemented fully by August 13. NORTHAM: That's Will Carter, a Chinese tech specialist at the Center for Strategic and International Studies. He says many federal agencies have waited until the eleventh hour to take action. CARTER: I get the sense that part of it is that people thought that this was so impractical that someone would do something about it. The other thing is - you have to keep in mind that with all of the different provisions of that bill, many people didn't realize that this was in there or that it impacted them. NORTHAM: For its part, Hikvision says it's committed to complying with U. S. laws. The ban is in line with the broader U. S. clampdown on Chinese tech companies. That includes blacklisting telecommunications giant Huawei in May. Rick Williams is the general manager of Selcom, a security contractor company in Selma, Ala. , that often uses Hikvision equipment. He says the prohibition on the cameras is confusing because it's not clear whether it applies to just new purchases or whether all of the existing Hikvision cameras need to be ripped out and replaced. RICK WILLIAMS: They're saying - well, that's an assumption that that equipment has to come out. You just replace that equipment later - you know, two years from now, when it's time to replace a piece of equipment. It makes no sense. NORTHAM: Will Carter, with the Center for Strategic and International Studies, says if the Chinese-made cameras are considered a national security threat, then they should come out. CARTER: If you look at the reasoning behind it - the concerns that Chinese surveillance products could be used by the Chinese government for espionage - the logic would say you should rip out what's already in place. You know, just because they're already in our federal networks or in government buildings doesn't mean we should be any more comfortable with them than new ones that we buy. NORTHAM: But security contractor Williams says if he has to replace cameras, it likely won't make much difference if he's using Hikvision or another brand. He says Hikvision sells components or original equipment for over 80 separate companies. WILLIAMS: I can remove a Hikvision camera, reach down in a box, pick up one that says Panasonic on it - or Samsung - stick it in the corner, and that's OK - even though it came from the exact same manufacturer. NORTHAM: Last month, Williams attended a public hearing hosted by the General Services Administration. He was looking for clarity about the prohibition on cameras but didn't get any. Repeated requests to the GSA for comment went unanswered. With the August 13 deadline approaching, it's still unclear when or who will be making a decision about what to do with thousands of Chinese-made surveillance cameras. Jackie Northam, NPR News, Washington. DAVID GREENE, HOST:  Countless federal agencies, military bases and other government buildings are facing a looming deadline. Starting next week, they will be barred from buying Chinese-made surveillance equipment. This ban was tucked into a provision of the 2019 National Defense Authorization Act. But there's confusion over what the law means, how it affects equipment that's already in use and how it can be implemented on time. Here's more from NPR's Jackie Northam. JACKIE NORTHAM, BYLINE: Any time you pass through security at a government building, you'll see them - small surveillance cameras watching you. They've become an accepted part of our lives. There's a good chance those cameras are made by a Chinese company or have Chinese components in them. They're meant to be providing security, but Congress sees them as a way for China to spy on the U. S. Peter Kusnic is a tech writer with Freedonia Group, a business research organization. PETER KUSNIC: The very close relationship between the Chinese government and its tech sector raises these concerns that these security cameras can be used to hack into American life. NORTHAM: Now Congress is putting the brakes on federal agencies using the Chinese surveillance cameras made by companies such as Hikvision, which is partially owned by the Chinese government. Despite an upcoming deadline, there are thousands of these cameras still installed across the country, says Katherine Gronberg with ForeScout Technologies, an IT security firm whose clients include the Defense Department and other federal agencies. Gronberg says her company recently did what she calls a small sampling of some government offices. KATHERINE GRONBERG: We found more than 1,700 of the two varieties of banned cameras. We don't know what agencies those were at, but 1,700 is a lot. They'll have to be removed by mid-August. WILL CARTER: There is zero chance that this law could be enacted fully and implemented fully by August 13. NORTHAM: That's Will Carter, a Chinese tech specialist at the Center for Strategic and International Studies. He says many federal agencies have waited until the eleventh hour to take action. CARTER: I get the sense that part of it is that people thought that this was so impractical that someone would do something about it. The other thing is - you have to keep in mind that with all of the different provisions of that bill, many people didn't realize that this was in there or that it impacted them. NORTHAM: For its part, Hikvision says it's committed to complying with U. S. laws. The ban is in line with the broader U. S. clampdown on Chinese tech companies. That includes blacklisting telecommunications giant Huawei in May. Rick Williams is the general manager of Selcom, a security contractor company in Selma, Ala. , that often uses Hikvision equipment. He says the prohibition on the cameras is confusing because it's not clear whether it applies to just new purchases or whether all of the existing Hikvision cameras need to be ripped out and replaced. RICK WILLIAMS: They're saying - well, that's an assumption that that equipment has to come out. You just replace that equipment later - you know, two years from now, when it's time to replace a piece of equipment. It makes no sense. NORTHAM: Will Carter, with the Center for Strategic and International Studies, says if the Chinese-made cameras are considered a national security threat, then they should come out. CARTER: If you look at the reasoning behind it - the concerns that Chinese surveillance products could be used by the Chinese government for espionage - the logic would say you should rip out what's already in place. You know, just because they're already in our federal networks or in government buildings doesn't mean we should be any more comfortable with them than new ones that we buy. NORTHAM: But security contractor Williams says if he has to replace cameras, it likely won't make much difference if he's using Hikvision or another brand. He says Hikvision sells components or original equipment for over 80 separate companies. WILLIAMS: I can remove a Hikvision camera, reach down in a box, pick up one that says Panasonic on it - or Samsung - stick it in the corner, and that's OK - even though it came from the exact same manufacturer. NORTHAM: Last month, Williams attended a public hearing hosted by the General Services Administration. He was looking for clarity about the prohibition on cameras but didn't get any. Repeated requests to the GSA for comment went unanswered. With the August 13 deadline approaching, it's still unclear when or who will be making a decision about what to do with thousands of Chinese-made surveillance cameras. Jackie Northam, NPR News, Washington.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-09-749932975": {"title": "A Look At The E-Verify Program And Its Blind Spots : NPR", "url": "https://www.npr.org/2019/08/09/749932975/a-look-at-the-e-verify-program-and-its-blind-spots", "author": "No author found", "published_date": "2019-08-09", "content": "ARI SHAPIRO, HOST: Mississippi is one of several states that require employers to use a federal program designed to stop them from hiring people in the U. S. illegally. That program, known as E-Verify, lets employers check whether new hires are eligible to work in the U. S. Two of the companies raided this week by Immigration and Customs Enforcement, including Koch Foods, say they do use E-Verify, and yet the government believes they hired hundreds of workers without legal authorization. To talk about E-Verify and its blind spots, I spoke with Madeline Zavodny. She is an economist at the University of North Florida, and she first explained how E-Verify works. MADELINE ZAVODNY: Once the employer's made an offer to a worker, the worker's required to supply identity documents, like a passport, driver's license, Social Security number. The employer fills out what's called an I-9 form. If the employer has signed up with E-Verify, the employer enters that information into a website run by the Department of Homeland Security. The Department of Homeland Security then checks that information - the name, the Social Security number and stuff - and makes sure that that name and number are valid for working in the United States. It then returns a notice to the employer if it's not valid, and the employer then is supposed to tell the worker and either dismiss the worker if the worker can't, you know, come up with valid documents or the worker, you know, cleans up whatever's going on. SHAPIRO: So explain how these Mississippi companies could say they use E-Verify, and yet ICE arrests hundreds of people working there who apparently are undocumented. ZAVODNY: So the companies are definitely registered with E-Verify, and there's no reason to believe that they didn't use it. The problem is, that if the worker presents valid documents that are not the worker's but are instead just identity fraud, the. . . SHAPIRO: So somebody presents my Social Security number, for example. ZAVODNY: Absolutely. In your name - then they're going to get through the system. SHAPIRO: Is that the primary way that people slip through the cracks or are there companies that are kind of complicit in helping people avoid getting identified by E-Verify? ZAVODNY: It's hard to know. I think you have some cases where the employer really wants the worker and maybe is going to turn a blind eye. But in other cases, you know, the worker has purchased documents that appear to be completely valid and that the system is going to just send through. SHAPIRO: Does agriculture generally support this system or are the shortcomings of this system something that they are OK with? ZAVODNY: So about half of agricultural workers in the country are undocumented immigrants. And we've seen a slow but steady shift toward using legal, temporary foreign workers through what's called the H-2A program over time, as unauthorized immigration into the United States has, you know, eased and there's been fewer entries, particularly from Mexico. But nonetheless, employers really are depending on having an undocumented workforce in a lot of agricultural areas. SHAPIRO: How well do you think the system generally works? ZAVODNY: I think it's a mixed bag. About half of new hires nationally are being run through the system. But this, you know, illustrates that the system has flaws. There are no biometrics associated with the system, so as long as there's a valid name and number, there's no reason for the worker not to be approved. In most of the states that have required all employers to use it, there was an initial drop in the number of unauthorized immigrants living in those states; Arizona is the most notable case of that. Alabama was - saw a big drop when it adopted its law as well. But we don't know a whole lot at the employer-level as to what's happened. SHAPIRO: If you were to wave a magic wand and somehow change the system to make it more effective, what do you think would have the biggest impact? ZAVODNY: Biometrics. But it would be costly to employers, to us as taxpayers, to some workers as well. SHAPIRO: Madeline Zavodny is an economist at the University of North Florida. Thanks for talking with us today. ZAVODNY: Thank you. (SOUNDBITE OF MUSIC) ARI SHAPIRO, HOST:  Mississippi is one of several states that require employers to use a federal program designed to stop them from hiring people in the U. S. illegally. That program, known as E-Verify, lets employers check whether new hires are eligible to work in the U. S. Two of the companies raided this week by Immigration and Customs Enforcement, including Koch Foods, say they do use E-Verify, and yet the government believes they hired hundreds of workers without legal authorization. To talk about E-Verify and its blind spots, I spoke with Madeline Zavodny. She is an economist at the University of North Florida, and she first explained how E-Verify works. MADELINE ZAVODNY: Once the employer's made an offer to a worker, the worker's required to supply identity documents, like a passport, driver's license, Social Security number. The employer fills out what's called an I-9 form. If the employer has signed up with E-Verify, the employer enters that information into a website run by the Department of Homeland Security. The Department of Homeland Security then checks that information - the name, the Social Security number and stuff - and makes sure that that name and number are valid for working in the United States. It then returns a notice to the employer if it's not valid, and the employer then is supposed to tell the worker and either dismiss the worker if the worker can't, you know, come up with valid documents or the worker, you know, cleans up whatever's going on. SHAPIRO: So explain how these Mississippi companies could say they use E-Verify, and yet ICE arrests hundreds of people working there who apparently are undocumented. ZAVODNY: So the companies are definitely registered with E-Verify, and there's no reason to believe that they didn't use it. The problem is, that if the worker presents valid documents that are not the worker's but are instead just identity fraud, the. . . SHAPIRO: So somebody presents my Social Security number, for example. ZAVODNY: Absolutely. In your name - then they're going to get through the system. SHAPIRO: Is that the primary way that people slip through the cracks or are there companies that are kind of complicit in helping people avoid getting identified by E-Verify? ZAVODNY: It's hard to know. I think you have some cases where the employer really wants the worker and maybe is going to turn a blind eye. But in other cases, you know, the worker has purchased documents that appear to be completely valid and that the system is going to just send through. SHAPIRO: Does agriculture generally support this system or are the shortcomings of this system something that they are OK with? ZAVODNY: So about half of agricultural workers in the country are undocumented immigrants. And we've seen a slow but steady shift toward using legal, temporary foreign workers through what's called the H-2A program over time, as unauthorized immigration into the United States has, you know, eased and there's been fewer entries, particularly from Mexico. But nonetheless, employers really are depending on having an undocumented workforce in a lot of agricultural areas. SHAPIRO: How well do you think the system generally works? ZAVODNY: I think it's a mixed bag. About half of new hires nationally are being run through the system. But this, you know, illustrates that the system has flaws. There are no biometrics associated with the system, so as long as there's a valid name and number, there's no reason for the worker not to be approved. In most of the states that have required all employers to use it, there was an initial drop in the number of unauthorized immigrants living in those states; Arizona is the most notable case of that. Alabama was - saw a big drop when it adopted its law as well. But we don't know a whole lot at the employer-level as to what's happened. SHAPIRO: If you were to wave a magic wand and somehow change the system to make it more effective, what do you think would have the biggest impact? ZAVODNY: Biometrics. But it would be costly to employers, to us as taxpayers, to some workers as well. SHAPIRO: Madeline Zavodny is an economist at the University of North Florida. Thanks for talking with us today. ZAVODNY: Thank you. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-10-750086287": {"title": "Inside Saudi Arabia's Disinformation Campaign : NPR", "url": "https://www.npr.org/2019/08/10/750086287/inside-saudi-arabias-disinformation-campaign", "author": "No author found", "published_date": "2019-08-10", "content": "SACHA PFEIFFER, HOST: Last week, Facebook took down 350 accounts and pages from its site. All of them, it said, were linked to Saudi Arabia. The social media platform said the Arab country's government was engaging in, quote, \"coordinated inauthentic behavior. \" An open source investigative website called Bellingcat uncovered one of the accounts that led to Facebook's decision. That research was done by our next guest, who asked that we not use his name to protect his identity. Thanks for coming on the program. ANONYMOUS RESEARCHER: Thanks for having me. PFEIFFER: We want to make clear to our listeners why we're having you be anonymous because there's a certain seeming double standard that we're talking about something involving anonymity and misinformation, yet we're allowing you to be anonymous. ANONYMOUS RESEARCHER: Sure. PFEIFFER: So explain to us why you gave that request, which we agree is a legitimate request. ANONYMOUS RESEARCHER: Security. So I wrote an in-depth and possibly embarrassing report on a top aide to Saudi Arabia's Crown Prince Mohammed bin Salman. The report was on Saud al-Qahtani, who's best known in the West for masterminding the murder of Jamal Khashoggi, the Washington Post journalist. But he also heads up the kingdom's efforts to intimidate and silence critics, primarily through online hacking. So if there is a computer hacker sitting in Riyadh who's been tasked with finding out who wrote this report on his boss, I want to try to make his life a little bit harder. PFEIFFER: So in terms of what you found in your research, what does Facebook mean when it says that Saudi Arabia engaged in coordinated inauthentic behavior? ANONYMOUS RESEARCHER: So that's Facebook's catch-all term for groups of accounts that work together to mislead about either who they are or what they're doing. So like you mentioned, in the case of Saudi Arabia, they created more than 300 accounts and pages masquerading as local news organizations in countries throughout the Middle East and North Africa. PFEIFFER: And what kind of misinformation were those intended to spread? ANONYMOUS RESEARCHER: Facebook didn't give very many examples of the sort of content that they pulled down, but they did point to basically two categories of promoting pro-Saudi content, especially content revolving around the Crown Prince Mohammed bin Salman. There were posts praising his social reform plan, Vision 2030. And then they also targeted enemies of Saudi Arabia, including Amnesty International, Al Jazeera, regional foes like Iran, so, yeah. PFEIFFER: What part, if any, did Jamal Khashoggi's death play in this Saudi misinformation campaign? ANONYMOUS RESEARCHER: So Facebook didn't point to any Khashoggi-related content with the pages that they took down and the accounts. But in the aftermath of his death, Saudi's network of bot accounts on Twitter jumped into action with the primary aim of dominating the online conversation, particularly in Arabic. So they worked to drown out and harass those pointing the finger at Saudi Arabia. And they also promoted and amplified tweets with the kingdom's version of events. PFEIFFER: I wonder a lot how aware people are, whether on Facebook or Twitter or other social media platforms, how many of the accounts they're looking at could be fake fronts for deliberate misinformation, possibly by a foreign government. What does your research tell you about how aware the public is about this? ANONYMOUS RESEARCHER: So it's hard to blame people for consuming disinformation, which by its very nature is intended to mislead and appear genuine. Some countries are very good at it. So Russia - like, they created Facebook accounts which started events that were attended by real Americans in real life across the political spectrum. They had no idea that these events were organized by people sitting in St. Petersburg. And the most recent campaign taken down by Facebook regarding Saudi Arabia, they said, had more than 1 million followers. So, yeah, it's a challenge for people to know what they're looking at is actually genuine if it's not an established source of news, for instance. PFEIFFER: That was a researcher for Bellingcat, an open source investigative website. NPR granted him anonymity to protect his safety. (SOUNDBITE OF MUSIC) SACHA PFEIFFER, HOST:  Last week, Facebook took down 350 accounts and pages from its site. All of them, it said, were linked to Saudi Arabia. The social media platform said the Arab country's government was engaging in, quote, \"coordinated inauthentic behavior. \" An open source investigative website called Bellingcat uncovered one of the accounts that led to Facebook's decision. That research was done by our next guest, who asked that we not use his name to protect his identity. Thanks for coming on the program. ANONYMOUS RESEARCHER: Thanks for having me. PFEIFFER: We want to make clear to our listeners why we're having you be anonymous because there's a certain seeming double standard that we're talking about something involving anonymity and misinformation, yet we're allowing you to be anonymous. ANONYMOUS RESEARCHER: Sure. PFEIFFER: So explain to us why you gave that request, which we agree is a legitimate request. ANONYMOUS RESEARCHER: Security. So I wrote an in-depth and possibly embarrassing report on a top aide to Saudi Arabia's Crown Prince Mohammed bin Salman. The report was on Saud al-Qahtani, who's best known in the West for masterminding the murder of Jamal Khashoggi, the Washington Post journalist. But he also heads up the kingdom's efforts to intimidate and silence critics, primarily through online hacking. So if there is a computer hacker sitting in Riyadh who's been tasked with finding out who wrote this report on his boss, I want to try to make his life a little bit harder. PFEIFFER: So in terms of what you found in your research, what does Facebook mean when it says that Saudi Arabia engaged in coordinated inauthentic behavior? ANONYMOUS RESEARCHER: So that's Facebook's catch-all term for groups of accounts that work together to mislead about either who they are or what they're doing. So like you mentioned, in the case of Saudi Arabia, they created more than 300 accounts and pages masquerading as local news organizations in countries throughout the Middle East and North Africa. PFEIFFER: And what kind of misinformation were those intended to spread? ANONYMOUS RESEARCHER: Facebook didn't give very many examples of the sort of content that they pulled down, but they did point to basically two categories of promoting pro-Saudi content, especially content revolving around the Crown Prince Mohammed bin Salman. There were posts praising his social reform plan, Vision 2030. And then they also targeted enemies of Saudi Arabia, including Amnesty International, Al Jazeera, regional foes like Iran, so, yeah. PFEIFFER: What part, if any, did Jamal Khashoggi's death play in this Saudi misinformation campaign? ANONYMOUS RESEARCHER: So Facebook didn't point to any Khashoggi-related content with the pages that they took down and the accounts. But in the aftermath of his death, Saudi's network of bot accounts on Twitter jumped into action with the primary aim of dominating the online conversation, particularly in Arabic. So they worked to drown out and harass those pointing the finger at Saudi Arabia. And they also promoted and amplified tweets with the kingdom's version of events. PFEIFFER: I wonder a lot how aware people are, whether on Facebook or Twitter or other social media platforms, how many of the accounts they're looking at could be fake fronts for deliberate misinformation, possibly by a foreign government. What does your research tell you about how aware the public is about this? ANONYMOUS RESEARCHER: So it's hard to blame people for consuming disinformation, which by its very nature is intended to mislead and appear genuine. Some countries are very good at it. So Russia - like, they created Facebook accounts which started events that were attended by real Americans in real life across the political spectrum. They had no idea that these events were organized by people sitting in St. Petersburg. And the most recent campaign taken down by Facebook regarding Saudi Arabia, they said, had more than 1 million followers. So, yeah, it's a challenge for people to know what they're looking at is actually genuine if it's not an established source of news, for instance. PFEIFFER: That was a researcher for Bellingcat, an open source investigative website. NPR granted him anonymity to protect his safety. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-11-750314487": {"title": "Troll Watch: Trending Hashtags : NPR", "url": "https://www.npr.org/2019/08/11/750314487/troll-watch-trending-hashtags", "author": "No author found", "published_date": "2019-08-11", "content": "MICHEL MARTIN, HOST: It's a big weekend in politics with a lot of Democratic contenders at the Iowa State Fair and different candidates making the benchmarks for the next scheduled debates in September. So that's one reason we thought it would be worthwhile to look back at the last round of debates. You might have noticed that hashtags like #DemDebateSoWhite and #KamalaHarrisDestroyed started trending, and you might have wondered why. Well, in the time since, we've learned more about what made those hashtags go viral and who was behind them, so we've decided to take this to our regular segment Troll Watch. (SOUNDBITE OF MUSIC)MARTIN: We're joined now by Emily Stewart. She's a reporter at Vox, and she recently wrote about the hashtag #KamalaHarrisDestroyed. She says it reveals how much we still don't understand about social media manipulation, and she's with us now. Emily Stewart, welcome. Thanks for joining us. EMILY STEWART: Thank you for having me. MARTIN: So let's start with the conclusion and work backward. The trending hashtag #KamalaHarrisDestroyed - look, it's a reference to the confrontation between Hawaii Congresswoman Tulsi Gabbard when she criticized California Senator Kamala Harris' record as a prosecutor. So it was a moment, but there were people who thought it odd that it got so much traction and wondered whether bots or conservative activists were kind of pushing it out. So do we know? STEWART: Well, I think that's sort of the big question here. And I think the answer is that it seems as though it is a combination. It looks like that specific hashtag was started by conservative activists, but then Fox found it and spread it and helped it to get farther than it might have otherwise. So that seems to be what happened. But, you know, I think it reveals broader questions that we just really still do not know how social media manipulation works. And we can't figure out what to trust and what not to trust. MARTIN: You say that Twitter and other platforms have improved their practices post-2016. They're still not perfect in your reporting. Do the social media companies take this seriously? STEWART: They take it seriously to the extent that it would be a bad look if they didn't look like they were trying at all. But obviously, social media companies make money when we are there and we are engaged. And we know that what keeps people engaged is controversy. You know, you hear a lot of talk about social media bias. Well, social media's bias is toward extremism, towards keeping people excited. And so, yes, does Facebook and Twitter and Google want to do a little bit better? Maybe. But it's not convenient for them to be boring. And so this sort of stuff is what drums people up and keeps people coming back. MARTIN: One of the points that you make in the piece is this - I just want to read something from your piece. You said, it's not just the misinformation itself that sows division. It's also the debate about it. People are confused about what social media manipulation is, how it works and whether it's happening. And they've also got their own political motivations to believe whether or not it exists. You say that Harris's camp had an incentive to claim that negative hashtag about her is Russian propaganda, but her opponents have an incentive to brush it off. And so the more people debate what is and isn't fake news, the harder it becomes for voters to determine the truth. Maybe this is sort of a tautology here, but I wonder if - could it possibly work the other way? That people perhaps can be better educated about whether they should hold back before they decide whether something's true because there's so much fog around it? STEWART: Yeah. I mean, I think that there is - it's hard to say hold on and wait for all the information, especially now when the Internet has made communication so frictionless. I think it's important to keep motivations in mind, and it's good to hang back and wait for more information before you make conclusions. But it's also - I mean, it's human nature. It's hard to do sometimes. MARTIN: That's Emily Stewart, reporter at Vox. We're talking about her piece \"#KamalaHarrisDestroyed Debate Signals How Much We Still Don't Understand About Social Media Manipulation. \"Emily, thanks so much for talking to us. STEWART: Thank you. MARTIN: This is NPR News. MICHEL MARTIN, HOST:  It's a big weekend in politics with a lot of Democratic contenders at the Iowa State Fair and different candidates making the benchmarks for the next scheduled debates in September. So that's one reason we thought it would be worthwhile to look back at the last round of debates. You might have noticed that hashtags like #DemDebateSoWhite and #KamalaHarrisDestroyed started trending, and you might have wondered why. Well, in the time since, we've learned more about what made those hashtags go viral and who was behind them, so we've decided to take this to our regular segment Troll Watch. (SOUNDBITE OF MUSIC) MARTIN: We're joined now by Emily Stewart. She's a reporter at Vox, and she recently wrote about the hashtag #KamalaHarrisDestroyed. She says it reveals how much we still don't understand about social media manipulation, and she's with us now. Emily Stewart, welcome. Thanks for joining us. EMILY STEWART: Thank you for having me. MARTIN: So let's start with the conclusion and work backward. The trending hashtag #KamalaHarrisDestroyed - look, it's a reference to the confrontation between Hawaii Congresswoman Tulsi Gabbard when she criticized California Senator Kamala Harris' record as a prosecutor. So it was a moment, but there were people who thought it odd that it got so much traction and wondered whether bots or conservative activists were kind of pushing it out. So do we know? STEWART: Well, I think that's sort of the big question here. And I think the answer is that it seems as though it is a combination. It looks like that specific hashtag was started by conservative activists, but then Fox found it and spread it and helped it to get farther than it might have otherwise. So that seems to be what happened. But, you know, I think it reveals broader questions that we just really still do not know how social media manipulation works. And we can't figure out what to trust and what not to trust. MARTIN: You say that Twitter and other platforms have improved their practices post-2016. They're still not perfect in your reporting. Do the social media companies take this seriously? STEWART: They take it seriously to the extent that it would be a bad look if they didn't look like they were trying at all. But obviously, social media companies make money when we are there and we are engaged. And we know that what keeps people engaged is controversy. You know, you hear a lot of talk about social media bias. Well, social media's bias is toward extremism, towards keeping people excited. And so, yes, does Facebook and Twitter and Google want to do a little bit better? Maybe. But it's not convenient for them to be boring. And so this sort of stuff is what drums people up and keeps people coming back. MARTIN: One of the points that you make in the piece is this - I just want to read something from your piece. You said, it's not just the misinformation itself that sows division. It's also the debate about it. People are confused about what social media manipulation is, how it works and whether it's happening. And they've also got their own political motivations to believe whether or not it exists. You say that Harris's camp had an incentive to claim that negative hashtag about her is Russian propaganda, but her opponents have an incentive to brush it off. And so the more people debate what is and isn't fake news, the harder it becomes for voters to determine the truth. Maybe this is sort of a tautology here, but I wonder if - could it possibly work the other way? That people perhaps can be better educated about whether they should hold back before they decide whether something's true because there's so much fog around it? STEWART: Yeah. I mean, I think that there is - it's hard to say hold on and wait for all the information, especially now when the Internet has made communication so frictionless. I think it's important to keep motivations in mind, and it's good to hang back and wait for more information before you make conclusions. But it's also - I mean, it's human nature. It's hard to do sometimes. MARTIN: That's Emily Stewart, reporter at Vox. We're talking about her piece \"#KamalaHarrisDestroyed Debate Signals How Much We Still Don't Understand About Social Media Manipulation. \" Emily, thanks so much for talking to us. STEWART: Thank you. MARTIN: This is NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-11-743612373": {"title": "Congress On Verge Of Creating Space Force After Years Of Delays : NPR", "url": "https://www.npr.org/2019/08/11/743612373/with-congressional-blessing-space-force-is-closer-to-launch", "author": "No author found", "published_date": "2019-08-11", "content": "", "section": "Politics", "disclaimer": ""}, "2019-08-12-750577697": {"title": "What Parents May Not Realize When They Post About Their Kids Online : NPR", "url": "https://www.npr.org/2019/08/12/750577697/what-parents-may-not-realize-when-they-post-about-their-kids-online", "author": "No author found", "published_date": "2019-08-12", "content": "MARY LOUISE KELLY, HOST: Scroll through Facebook or Instagram, you will see tons of photos of children - photos posted by their parents. One British study found parents post about 1,500 images of their kids on social media before those kids turn 5 years old. In this week's All Tech Considered, we and NPR's Life Kit are looking at sharenting, what moms and dads may not realize when they post online about their kids and what their kids have to say about it. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")KELLY: Let's bring in Anya Kamenetz. She is a host of the Life Kit parenting podcast, and she's been taking a look at sharenting and children's privacy. She's going to walk us through the facts and, I hope, give us some advice, which I could certainly use. Anya, welcome. ANYA KAMENETZ, BYLINE: Hi, Mary Louise. KELLY: Hey there. So my kids, who are now teenagers, lectured me about this early on, and their lecture boiled down basically to, don't even think about posting pictures. (LAUGHTER)KELLY: But I do see my friends doing it all the time and posting about kids, you know, from very young, still in diapers, all the way up through their teenagers. KAMENETZ: Sure. And, I mean, I've done the same myself. But, you know, we don't often get to hear about the kids' perspective. So that's why we were so interested. When we did the NPR Student Podcast Challenge earlier this year, we heard from Chelsea Whitwer's fifth grade class at Westchester Elementary in Kirkwood, Mo. And here's what they said. (SOUNDBITE OF ARCHIVED NPR BROADCAST)CHELSEA WHITWER: OK. So what do you guys think about your parents posting on the Internet? UNIDENTIFIED STUDENT #1: I don't really like it because, like, I didn't ask and it's sometimes an embarrassing photo. WHITWER: What about you? How do you feel about your parents posting on the Internet? UNIDENTIFIED STUDENT #2: If it's, like, an OK photo, then sure. It's fine. But if, like - if it's - yeah, if it's an embarrassing photo, then I don't want it. And I want them to ask me to post something. KELLY: They want to be asked. Anya, you followed up and talked to some of these kids' parents. What did they tell you? KAMENETZ: Yeah, we talked to Jenna Mihms. She's one of the mothers. And she said the podcast really surprised her. (SOUNDBITE OF ARCHIVED NPR BROADCAST)JENNA MIHMS: I thought it was interesting how sensitive the kids were of the thought of being embarrassed by things that parents post. KAMENETZ: And, Mary Louise, Jenna Mihms had some really interesting insights into why parents post. She says, personally, she sometimes feels a social pressure from other parents to post online, even though, personally, she's inclined to be more private. And she asked her kids about this. (SOUNDBITE OF ARCHIVED NPR BROADCAST)MIHMS: And I've actually asked them before if it makes them feel bad that I don't post about them because I know that's an opportunity for parents to sort of brag on their kids and highlight accomplishments. KAMENETZ: And so when she goes online, she says. . . (SOUNDBITE OF ARCHIVED NPR BROADCAST)MIHMS: I see all these great accomplishments and proud parent moments, that I didn't want my kids to feel that their parents didn't love them just as much as every other parent. KELLY: And, Anya, I totally get that. You want to brag about your kids. I mean, let me push back on behalf of all the parents listening who might be thinking, I'm the adult here. I'm your parent. This is my social media account, and I should know what's best for you. KAMENETZ: Right. I totally understand that. And one expert we talked to, Stacey Steinberg, kind of brought out what this conflict is. She's a professor at the University of Florida Levin College of Law. She's also a photographer herself and a mother of three, and so she started to wonder. . . (SOUNDBITE OF ARCHIVED NPR BROADCAST)STACEY STEINBERG: How could we balance our kids' right to privacy with our interest in sharing our stories? KAMENETZ: So notice that Steinberg says balance here, right? So parents, yes, we have legitimate interest in talking about our kids, sharing our struggles, getting support. But Stacey Steinberg also suggests that there are risks we should be aware of, as well as trade-offs. KELLY: And what are the risks she's pointing to? KAMENETZ: Well, one terrifying thing that she spelled out was that you should never post online any photos or videos that show your children in states of undress, even briefly. And the reason is that we've seen from recent reporting that the nature of the Internet and its recommendation algorithms is that those innocent pictures can essentially be served up to predators online. KELLY: Yeah. I mean, this is where it gets not just tricky but really scary because I would certainly be aware of that with a teenager but less so with cute baby in the bubble bath. KAMENETZ: Yeah, exactly. And this gets into a bigger point, Mary Louise. I mean, we've been talking about the personal, emotional, family dynamics of sharenting, but the bigger question, I think, is why is the Internet set up this way? You know, why is it so hard to keep our kids safe just when we're going about our daily lives and sharing with family and friends? KELLY: And why is it so hard to keep our kids safe? Are there changes that could be made here? KAMENETZ: So to get a little bit more of the policy perspective, I actually called up Sonia Livingstone. She's a leading expert in kids online at the London School of Economics. And Livingstone says that she'll be in the U. S. , and she'll see parents snapping photos, even of other kids, not even just their own kids. And that seems very strange to her. SONIA LIVINGSTONE: Can this be right that parents will take pictures of children at a public park or at a swimming pool? And it just really made me think, in Britain, we just don't do that anymore. We never take a picture of anyone else's child because we'll get - we know that they have - you know, that child has its own rights, as it were. KAMENETZ: And so when Livingstone says that about children's rights, she actually means something very specific, Mary Louise. She pointed to something called the United Nations Convention on the Rights of the Child. And this is a big document that was adopted back in 1990 by all countries, essentially, except the U. S. And it talks about what's good for the child, and that means access to resources to help them develop, the ability to express themselves, as well as protections from risk or harm, and that even includes participating in mass media and being able to access mass media but in a way that's safe. And so Livingstone says that in the U. K. and really all across Europe, this perspective of children's rights is starting to inform policy. KELLY: Inform policy in what way? What's being changed? KAMENETZ: Well, the U. K. government is currently developing what they call an age-appropriate design code. And this is the first of its kind in the world. And it's essentially like zoning laws for the Internet. It means any online service that's likely to be accessed by children will need to be very, very careful and very transparent on how they store and how they share data. And so this really turns the notion of sharenting on its head a bit. You know, instead of us parents having to be so careful of how we guard our children's images and their stories, it puts a little bit more responsibility out on the platforms. KELLY: And so that's what's happening in Europe. What about here in the U. S. , Anya? KAMENETZ: In the U. S. , there are early stages of efforts to update the Children's Online Privacy Protection Act. And that's kind of the biggest piece of legislation on this topic. But it's really kind of early days. KELLY: Early days - OK. So while they wrangle that wherever it's going to go on Capitol Hill, what should we do? Give us some news we can use. KAMENETZ: Well, one idea I'll share from Stacey Steinberg is this. (SOUNDBITE OF ARCHIVED NPR BROADCAST)STEINBERG: We need to give kids veto power over what it is that we share about them online. KAMENETZ: For example, after her 8-year-old's gymnastics meet, she puts a laptop up on the kitchen counter, and she lets him pick what photos to post and respond to the comments, like from his uncle. So this not only gives kids a way to stay in touch with friends and family, but it's also a great way of role modeling good judgment on social media, which I know, you know, your teenagers and my kids always show. KELLY: Always. (LAUGHTER)KAMENETZ: And so like the kids from Missouri said, also, you know, they probably want you to do that. They want you to check in. KELLY: NPR's Anya Kamenetz, thanks so much for coming on and talking about sharenting. KAMENETZ: Thanks, Mary Louise. KELLY: Anya hosts NPR's Life Kit parenting podcast. The Life Kit series has practical tips on all sorts of things, and you can find it on your smartphone or your laptop at npr. org/lifekit. MARY LOUISE KELLY, HOST:  Scroll through Facebook or Instagram, you will see tons of photos of children - photos posted by their parents. One British study found parents post about 1,500 images of their kids on social media before those kids turn 5 years old. In this week's All Tech Considered, we and NPR's Life Kit are looking at sharenting, what moms and dads may not realize when they post online about their kids and what their kids have to say about it. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") KELLY: Let's bring in Anya Kamenetz. She is a host of the Life Kit parenting podcast, and she's been taking a look at sharenting and children's privacy. She's going to walk us through the facts and, I hope, give us some advice, which I could certainly use. Anya, welcome. ANYA KAMENETZ, BYLINE: Hi, Mary Louise. KELLY: Hey there. So my kids, who are now teenagers, lectured me about this early on, and their lecture boiled down basically to, don't even think about posting pictures. (LAUGHTER) KELLY: But I do see my friends doing it all the time and posting about kids, you know, from very young, still in diapers, all the way up through their teenagers. KAMENETZ: Sure. And, I mean, I've done the same myself. But, you know, we don't often get to hear about the kids' perspective. So that's why we were so interested. When we did the NPR Student Podcast Challenge earlier this year, we heard from Chelsea Whitwer's fifth grade class at Westchester Elementary in Kirkwood, Mo. And here's what they said. (SOUNDBITE OF ARCHIVED NPR BROADCAST) CHELSEA WHITWER: OK. So what do you guys think about your parents posting on the Internet? UNIDENTIFIED STUDENT #1: I don't really like it because, like, I didn't ask and it's sometimes an embarrassing photo. WHITWER: What about you? How do you feel about your parents posting on the Internet? UNIDENTIFIED STUDENT #2: If it's, like, an OK photo, then sure. It's fine. But if, like - if it's - yeah, if it's an embarrassing photo, then I don't want it. And I want them to ask me to post something. KELLY: They want to be asked. Anya, you followed up and talked to some of these kids' parents. What did they tell you? KAMENETZ: Yeah, we talked to Jenna Mihms. She's one of the mothers. And she said the podcast really surprised her. (SOUNDBITE OF ARCHIVED NPR BROADCAST) JENNA MIHMS: I thought it was interesting how sensitive the kids were of the thought of being embarrassed by things that parents post. KAMENETZ: And, Mary Louise, Jenna Mihms had some really interesting insights into why parents post. She says, personally, she sometimes feels a social pressure from other parents to post online, even though, personally, she's inclined to be more private. And she asked her kids about this. (SOUNDBITE OF ARCHIVED NPR BROADCAST) MIHMS: And I've actually asked them before if it makes them feel bad that I don't post about them because I know that's an opportunity for parents to sort of brag on their kids and highlight accomplishments. KAMENETZ: And so when she goes online, she says. . . (SOUNDBITE OF ARCHIVED NPR BROADCAST) MIHMS: I see all these great accomplishments and proud parent moments, that I didn't want my kids to feel that their parents didn't love them just as much as every other parent. KELLY: And, Anya, I totally get that. You want to brag about your kids. I mean, let me push back on behalf of all the parents listening who might be thinking, I'm the adult here. I'm your parent. This is my social media account, and I should know what's best for you. KAMENETZ: Right. I totally understand that. And one expert we talked to, Stacey Steinberg, kind of brought out what this conflict is. She's a professor at the University of Florida Levin College of Law. She's also a photographer herself and a mother of three, and so she started to wonder. . . (SOUNDBITE OF ARCHIVED NPR BROADCAST) STACEY STEINBERG: How could we balance our kids' right to privacy with our interest in sharing our stories? KAMENETZ: So notice that Steinberg says balance here, right? So parents, yes, we have legitimate interest in talking about our kids, sharing our struggles, getting support. But Stacey Steinberg also suggests that there are risks we should be aware of, as well as trade-offs. KELLY: And what are the risks she's pointing to? KAMENETZ: Well, one terrifying thing that she spelled out was that you should never post online any photos or videos that show your children in states of undress, even briefly. And the reason is that we've seen from recent reporting that the nature of the Internet and its recommendation algorithms is that those innocent pictures can essentially be served up to predators online. KELLY: Yeah. I mean, this is where it gets not just tricky but really scary because I would certainly be aware of that with a teenager but less so with cute baby in the bubble bath. KAMENETZ: Yeah, exactly. And this gets into a bigger point, Mary Louise. I mean, we've been talking about the personal, emotional, family dynamics of sharenting, but the bigger question, I think, is why is the Internet set up this way? You know, why is it so hard to keep our kids safe just when we're going about our daily lives and sharing with family and friends? KELLY: And why is it so hard to keep our kids safe? Are there changes that could be made here? KAMENETZ: So to get a little bit more of the policy perspective, I actually called up Sonia Livingstone. She's a leading expert in kids online at the London School of Economics. And Livingstone says that she'll be in the U. S. , and she'll see parents snapping photos, even of other kids, not even just their own kids. And that seems very strange to her. SONIA LIVINGSTONE: Can this be right that parents will take pictures of children at a public park or at a swimming pool? And it just really made me think, in Britain, we just don't do that anymore. We never take a picture of anyone else's child because we'll get - we know that they have - you know, that child has its own rights, as it were. KAMENETZ: And so when Livingstone says that about children's rights, she actually means something very specific, Mary Louise. She pointed to something called the United Nations Convention on the Rights of the Child. And this is a big document that was adopted back in 1990 by all countries, essentially, except the U. S. And it talks about what's good for the child, and that means access to resources to help them develop, the ability to express themselves, as well as protections from risk or harm, and that even includes participating in mass media and being able to access mass media but in a way that's safe. And so Livingstone says that in the U. K. and really all across Europe, this perspective of children's rights is starting to inform policy. KELLY: Inform policy in what way? What's being changed? KAMENETZ: Well, the U. K. government is currently developing what they call an age-appropriate design code. And this is the first of its kind in the world. And it's essentially like zoning laws for the Internet. It means any online service that's likely to be accessed by children will need to be very, very careful and very transparent on how they store and how they share data. And so this really turns the notion of sharenting on its head a bit. You know, instead of us parents having to be so careful of how we guard our children's images and their stories, it puts a little bit more responsibility out on the platforms. KELLY: And so that's what's happening in Europe. What about here in the U. S. , Anya? KAMENETZ: In the U. S. , there are early stages of efforts to update the Children's Online Privacy Protection Act. And that's kind of the biggest piece of legislation on this topic. But it's really kind of early days. KELLY: Early days - OK. So while they wrangle that wherever it's going to go on Capitol Hill, what should we do? Give us some news we can use. KAMENETZ: Well, one idea I'll share from Stacey Steinberg is this. (SOUNDBITE OF ARCHIVED NPR BROADCAST) STEINBERG: We need to give kids veto power over what it is that we share about them online. KAMENETZ: For example, after her 8-year-old's gymnastics meet, she puts a laptop up on the kitchen counter, and she lets him pick what photos to post and respond to the comments, like from his uncle. So this not only gives kids a way to stay in touch with friends and family, but it's also a great way of role modeling good judgment on social media, which I know, you know, your teenagers and my kids always show. KELLY: Always. (LAUGHTER) KAMENETZ: And so like the kids from Missouri said, also, you know, they probably want you to do that. They want you to check in. KELLY: NPR's Anya Kamenetz, thanks so much for coming on and talking about sharenting. KAMENETZ: Thanks, Mary Louise. KELLY: Anya hosts NPR's Life Kit parenting podcast. The Life Kit series has practical tips on all sorts of things, and you can find it on your smartphone or your laptop at npr. org/lifekit.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-12-736643714": {"title": "Are There Too Many Unicorn Startup Companies? : NPR", "url": "https://www.npr.org/2019/08/12/736643714/with-so-many-startups-growing-into-unicorns-can-they-still-be-magical", "author": "No author found", "published_date": "2019-08-12", "content": "", "section": "Technology", "disclaimer": ""}, "2019-08-13-750985881": {"title": "FAA Bans MacBook Pro Laptops With Recalled Batteries : NPR", "url": "https://www.npr.org/2019/08/13/750985881/u-s-air-regulators-ban-macbook-pros-with-recalled-batteries-from-flights", "author": "No author found", "published_date": "2019-08-13", "content": "", "section": "Technology", "disclaimer": ""}, "2019-08-13-748299817": {"title": "Hooked On The Internet, South Korean Teens Go Into Digital Detox : NPR", "url": "https://www.npr.org/2019/08/13/748299817/hooked-on-the-internet-south-korean-teens-go-into-digital-detox", "author": "No author found", "published_date": "2019-08-13", "content": "", "section": "World", "disclaimer": ""}, "2019-08-14-751235998": {"title": "Computer Science Professors Says We Can Probably Make Email Better For Everyone : NPR", "url": "https://www.npr.org/2019/08/14/751235998/computer-science-professors-says-we-can-probably-make-email-better-for-everyone", "author": "No author found", "published_date": "2019-08-14", "content": "MARY LOUISE KELLY, HOST: A new essay in The New Yorker argues that our grandkids are going to look back at the way we work today in bewilderment. Specifically, they will be mystified by our addiction to email. Cal Newport wrote the piece which describes this moment in workplace history as one where we all frantically check our inboxes every few minutes, exhausted by the deluge of complex and ambiguous messages while applauding ourselves for eliminating the need to speak face-to-face. Yep, guilty as charged. Well, happily, Cal Newport is also a computer science professor at Georgetown, and he has some thoughts about how we got here and how we might do it better. Cal Newport, welcome. CAL NEWPORT: Thank you for having me. KELLY: So we're going to get to the evils of email, but start with the story that opens your essay. We are deep inside CIA headquarters. It's the 1960s. And nestled inside the walls are something like 30 miles of steel tubing. Why? What was it for? NEWPORT: Communication, and, in particular, communication that is asynchronous, a way for me to send a message to over 150 different stations in the headquarters where it can arrive and be there waiting for the recipient to read it. So they essentially built email, but using pneumatic tubes and fiberglass containers and electromagnetic switches. KELLY: We're talking about this because, as you mentioned, this was a prime, early example of asynchronous messaging, which I gather basically was about convenience. I can write you when I feel like it, and you can reply when you want. NEWPORT: Yeah, this was seen as a silver bullet for a really big problem that emerged in the 20th century, which was workspaces that used to just be four or five people; if I needed something, I would just talk to you. But in the 20th century, we saw the arrival of very large offices and very large organizations. And so the problem was, how do we coordinate and collaborate when there's 800, 1,000, 2,000 of us in the same building? And asynchrony was seen as the magic solution. KELLY: So various systems were tried. We're still trying to figure out the perfect system. But in the meantime, in the late 20th century, email arrives, and it's like the killer app of asynchronous communication. NEWPORT: Yeah. We assumed this would solve the problem. I mean, the pneumatic tubes or what have you was interesting, but very few organizations could actually afford to build these. But email any organization could have. Everyone could send messages to everyone else when they wanted, instantaneously, have them be read when the recipient was ready. This was seen as the thing that was going to solve the problem of collaboration in big organizations, which is why it spread incredibly rapidly into essentially every corner of knowledge work. KELLY: Sounds so promising and delightful, except (laughter), as anyone who's ever had an email account knows, email is great for many things, but collaboration ain't one of them. NEWPORT: Yeah, unintended consequences. So it turned out that during this same period where people in the world of business thought asynchrony was going to solve all these problems, there was mathematicians in my field that were studying asynchrony in computer networks and finding, uh-oh, when you get rid of real-time back-and-forth conversation, suddenly it becomes much harder to collaborate. It's much more subtle. It requires much more messages. KELLY: And it just takes longer - that experience we've all had of sending two dozen messages back-and-forth when you could've just picked up your phone or leaned out of your cubicle and hollered at your workmate. NEWPORT: Yeah, that's right. We thought that we could take the five-minute conversation and replace it with one quick email message, but the reality is that five-minute conversation required 15 back-and-forth email messages throughout the day. So we soon found ourselves overwhelmed by the massive increase in messages. KELLY: What does that mean for those of us who are sitting here twitching to check our email? In practical terms, how should we be communicating? NEWPORT: Well, what we know is that humans are much better at back-and-forth in real time - so on the phone, sitting together in the same room, on video chat - where you can actually go back-and-forth, where I say something and I know that you hear it right away and you can respond right away. We can look at our body language. We can look at our cues. We can look at how our voices change in volume and modulation. This is an incredibly efficient way for human beings to coordinate and collaborate. KELLY: So what is the solution - we stop checking email so much and pick up our phone more? NEWPORT: Well, what I found is that going back to synchrony successfully in the world of business requires structure. So if you just say, get on the phone more, use less email, that's probably not going to work. But if you have systems in place - this is how we collaborate. This is - we have these meetings at these times. Here's how we've set up these meetings so that they don't become long and full of bloviation. This type of structured synchrony is starting to have a comeback in the world of business, and people are finding that they're getting by with much less messaging. KELLY: You also write about the old-fashioned notion of office hours. I'm available at this time. I can talk to you face-to-face. If you can't come during my office hours, too bad. (Laughter) You have to solve your problem or answer your query another way. NEWPORT: Yeah, I'm a professor, so I'm used to office hours, but we're seeing this more in commercial industry as well. The company Basecamp does this. You can sign up for various experts' office hours online, and then you show up and talk to them face-to-face with their problem. It's a little inconvenient if you have to wait a while until their next office hours. But as their CEO says, inconvenience is not really our issue. What we're trying to do is get things done. And so we can put up with a little bit of inconvenience if it means we're freeing people from this constant distracting, fragmented communication. KELLY: What happened to the CIA tubes, by the way? NEWPORT: They expanded the headquarters. And so as they were expanding the headquarters, doubling their size, they felt like this would be a little bit old-fashioned to be building new tubes into the new side of the headquarters. And at the same time, email was just arriving on the scene. And so email was, in some sense, the death of the tubes in the CIA, and probably also the death of some of their productivity. KELLY: Cal Newport - he's professor of computer science at Georgetown University and author of The New Yorker piece \"Was E-mail A Mistake? \"Cal Newport, thanks. NEWPORT: Thank you. MARY LOUISE KELLY, HOST:  A new essay in The New Yorker argues that our grandkids are going to look back at the way we work today in bewilderment. Specifically, they will be mystified by our addiction to email. Cal Newport wrote the piece which describes this moment in workplace history as one where we all frantically check our inboxes every few minutes, exhausted by the deluge of complex and ambiguous messages while applauding ourselves for eliminating the need to speak face-to-face. Yep, guilty as charged. Well, happily, Cal Newport is also a computer science professor at Georgetown, and he has some thoughts about how we got here and how we might do it better. Cal Newport, welcome. CAL NEWPORT: Thank you for having me. KELLY: So we're going to get to the evils of email, but start with the story that opens your essay. We are deep inside CIA headquarters. It's the 1960s. And nestled inside the walls are something like 30 miles of steel tubing. Why? What was it for? NEWPORT: Communication, and, in particular, communication that is asynchronous, a way for me to send a message to over 150 different stations in the headquarters where it can arrive and be there waiting for the recipient to read it. So they essentially built email, but using pneumatic tubes and fiberglass containers and electromagnetic switches. KELLY: We're talking about this because, as you mentioned, this was a prime, early example of asynchronous messaging, which I gather basically was about convenience. I can write you when I feel like it, and you can reply when you want. NEWPORT: Yeah, this was seen as a silver bullet for a really big problem that emerged in the 20th century, which was workspaces that used to just be four or five people; if I needed something, I would just talk to you. But in the 20th century, we saw the arrival of very large offices and very large organizations. And so the problem was, how do we coordinate and collaborate when there's 800, 1,000, 2,000 of us in the same building? And asynchrony was seen as the magic solution. KELLY: So various systems were tried. We're still trying to figure out the perfect system. But in the meantime, in the late 20th century, email arrives, and it's like the killer app of asynchronous communication. NEWPORT: Yeah. We assumed this would solve the problem. I mean, the pneumatic tubes or what have you was interesting, but very few organizations could actually afford to build these. But email any organization could have. Everyone could send messages to everyone else when they wanted, instantaneously, have them be read when the recipient was ready. This was seen as the thing that was going to solve the problem of collaboration in big organizations, which is why it spread incredibly rapidly into essentially every corner of knowledge work. KELLY: Sounds so promising and delightful, except (laughter), as anyone who's ever had an email account knows, email is great for many things, but collaboration ain't one of them. NEWPORT: Yeah, unintended consequences. So it turned out that during this same period where people in the world of business thought asynchrony was going to solve all these problems, there was mathematicians in my field that were studying asynchrony in computer networks and finding, uh-oh, when you get rid of real-time back-and-forth conversation, suddenly it becomes much harder to collaborate. It's much more subtle. It requires much more messages. KELLY: And it just takes longer - that experience we've all had of sending two dozen messages back-and-forth when you could've just picked up your phone or leaned out of your cubicle and hollered at your workmate. NEWPORT: Yeah, that's right. We thought that we could take the five-minute conversation and replace it with one quick email message, but the reality is that five-minute conversation required 15 back-and-forth email messages throughout the day. So we soon found ourselves overwhelmed by the massive increase in messages. KELLY: What does that mean for those of us who are sitting here twitching to check our email? In practical terms, how should we be communicating? NEWPORT: Well, what we know is that humans are much better at back-and-forth in real time - so on the phone, sitting together in the same room, on video chat - where you can actually go back-and-forth, where I say something and I know that you hear it right away and you can respond right away. We can look at our body language. We can look at our cues. We can look at how our voices change in volume and modulation. This is an incredibly efficient way for human beings to coordinate and collaborate. KELLY: So what is the solution - we stop checking email so much and pick up our phone more? NEWPORT: Well, what I found is that going back to synchrony successfully in the world of business requires structure. So if you just say, get on the phone more, use less email, that's probably not going to work. But if you have systems in place - this is how we collaborate. This is - we have these meetings at these times. Here's how we've set up these meetings so that they don't become long and full of bloviation. This type of structured synchrony is starting to have a comeback in the world of business, and people are finding that they're getting by with much less messaging. KELLY: You also write about the old-fashioned notion of office hours. I'm available at this time. I can talk to you face-to-face. If you can't come during my office hours, too bad. (Laughter) You have to solve your problem or answer your query another way. NEWPORT: Yeah, I'm a professor, so I'm used to office hours, but we're seeing this more in commercial industry as well. The company Basecamp does this. You can sign up for various experts' office hours online, and then you show up and talk to them face-to-face with their problem. It's a little inconvenient if you have to wait a while until their next office hours. But as their CEO says, inconvenience is not really our issue. What we're trying to do is get things done. And so we can put up with a little bit of inconvenience if it means we're freeing people from this constant distracting, fragmented communication. KELLY: What happened to the CIA tubes, by the way? NEWPORT: They expanded the headquarters. And so as they were expanding the headquarters, doubling their size, they felt like this would be a little bit old-fashioned to be building new tubes into the new side of the headquarters. And at the same time, email was just arriving on the scene. And so email was, in some sense, the death of the tubes in the CIA, and probably also the death of some of their productivity. KELLY: Cal Newport - he's professor of computer science at Georgetown University and author of The New Yorker piece \"Was E-mail A Mistake? \" Cal Newport, thanks. NEWPORT: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-14-750585438": {"title": "Republican Senator Pushes Bill To Curb Social Media Addiction : NPR", "url": "https://www.npr.org/2019/08/14/750585438/senator-pushes-bill-to-curb-exploitative-and-addictive-social-media-practices", "author": "No author found", "published_date": "2019-08-14", "content": "AILSA CHANG, HOST: Americans are spending more and more time glued to social media apps like Instagram, Facebook and YouTube. So much so, concerns about technology addiction and rising political anger against social media companies have led to some legislative proposals that would scale back their power. NPR's Tim Mak has more on one Republican lawmaker's effort to ban what he sees as the addictive elements of social media. TIM MAK, BYLINE: Senator Josh Hawley is a freshman senator from Missouri, but he is quickly making a name for himself as the representative for an increasing number of Americans who are angry with social media companies. JOSH HAWLEY: Yeah, well, their business model is increasingly exploitative in nature. And I think that, look; these are companies that have tried to evade accountability. MAK: Hawley proposed a bill that would require social media companies to tell users every 30 minutes how long they've been on the platform each day. It would also ban features that he views as addictive. The legislation would make illegal infinite scroll, which endlessly populates apps with additional content. It would also prohibit the autoplay of video and audio. HAWLEY: The big tech platforms have adopted a business model that takes our private information without telling us, sells it without our consent and then tries to use exploitative and addictive practices in order to get us to spend more time on their platforms so they can take more stuff from us. MAK: Hawley's proposal strikes at the heart of that business model. LINDSAY GORMAN: Seventy percent of the time spent on the YouTube platform, for example, is spent watching videos that the algorithm recommends. MAK: That's Lindsay Gorman, a fellow for emerging technologies at The German Marshall Fund, explaining just how crucial these sorts of features are to these businesses. GORMAN: Their business model is based on user engagement and time spent on the platform. Certainly, they're using sophisticated psychological measures, like the autoplay feature, and others to keep people on the platform. MAK: Hawley's legislation isn't likely to pass, but the openness with which this legislation has been greeted illustrates something deeper about the mood in Washington. The lack of regulations on social media companies as compared to their power is nudging conservatives to go against their general principle, a hands-off approach to business. Hawley's bill would have government micromanage what features these tech companies can use. But Republican Senator Ted Cruz, a champion of free markets, seems at least open to it. TED CRUZ: Nobody wants to see a federal speech police. But at the same time, allowing a handful of Silicon Valley billionaires to be the censors of all political speech in America is a terrible outcome. And so I think Senator Hawley's bill is a positive step in the right direction. MAK: Democrats have also increasingly turned against big tech but for different reasons. Senator Mark Warner, the top Democrat on the Senate Intelligence Committee, said he sees elements of Hawley's proposal that he could support. MARK WARNER: Like in any business, there are already prohibitions about deceptive practices. There's basic consumer protections. We don't have any of that in the social media world. The rub comes in how you define those practices. MAK: All of this is to say that Hawley's proposal is more than just a longshot bill. His proposal represents the changing nature of the conversation around technology in Washington, D. C. , and a converging frustration about big tech that is bringing lawmakers out of their comfort zones to propose unorthodox solutions. Tim Mak, NPR News, Washington. AILSA CHANG, HOST:  Americans are spending more and more time glued to social media apps like Instagram, Facebook and YouTube. So much so, concerns about technology addiction and rising political anger against social media companies have led to some legislative proposals that would scale back their power. NPR's Tim Mak has more on one Republican lawmaker's effort to ban what he sees as the addictive elements of social media. TIM MAK, BYLINE: Senator Josh Hawley is a freshman senator from Missouri, but he is quickly making a name for himself as the representative for an increasing number of Americans who are angry with social media companies. JOSH HAWLEY: Yeah, well, their business model is increasingly exploitative in nature. And I think that, look; these are companies that have tried to evade accountability. MAK: Hawley proposed a bill that would require social media companies to tell users every 30 minutes how long they've been on the platform each day. It would also ban features that he views as addictive. The legislation would make illegal infinite scroll, which endlessly populates apps with additional content. It would also prohibit the autoplay of video and audio. HAWLEY: The big tech platforms have adopted a business model that takes our private information without telling us, sells it without our consent and then tries to use exploitative and addictive practices in order to get us to spend more time on their platforms so they can take more stuff from us. MAK: Hawley's proposal strikes at the heart of that business model. LINDSAY GORMAN: Seventy percent of the time spent on the YouTube platform, for example, is spent watching videos that the algorithm recommends. MAK: That's Lindsay Gorman, a fellow for emerging technologies at The German Marshall Fund, explaining just how crucial these sorts of features are to these businesses. GORMAN: Their business model is based on user engagement and time spent on the platform. Certainly, they're using sophisticated psychological measures, like the autoplay feature, and others to keep people on the platform. MAK: Hawley's legislation isn't likely to pass, but the openness with which this legislation has been greeted illustrates something deeper about the mood in Washington. The lack of regulations on social media companies as compared to their power is nudging conservatives to go against their general principle, a hands-off approach to business. Hawley's bill would have government micromanage what features these tech companies can use. But Republican Senator Ted Cruz, a champion of free markets, seems at least open to it. TED CRUZ: Nobody wants to see a federal speech police. But at the same time, allowing a handful of Silicon Valley billionaires to be the censors of all political speech in America is a terrible outcome. And so I think Senator Hawley's bill is a positive step in the right direction. MAK: Democrats have also increasingly turned against big tech but for different reasons. Senator Mark Warner, the top Democrat on the Senate Intelligence Committee, said he sees elements of Hawley's proposal that he could support. MARK WARNER: Like in any business, there are already prohibitions about deceptive practices. There's basic consumer protections. We don't have any of that in the social media world. The rub comes in how you define those practices. MAK: All of this is to say that Hawley's proposal is more than just a longshot bill. His proposal represents the changing nature of the conversation around technology in Washington, D. C. , and a converging frustration about big tech that is bringing lawmakers out of their comfort zones to propose unorthodox solutions. Tim Mak, NPR News, Washington.", "section": "Politics", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-17-751986787": {"title": "How One Mom Talks To Her Sons About Hate On The Internet  : NPR", "url": "https://www.npr.org/2019/08/17/751986787/writer-joanna-schroeder-on-preventing-teenage-boys-from-turning-to-hate", "author": "No author found", "published_date": "2019-08-17", "content": "SCOTT SIMON, HOST: Do you have white teenage sons? Listen up. So begins a thread on Twitter this week by Joanna Schroeder that continues, quote, \"social media and vloggers are actively laying groundwork in white teens to turn them into alt-right white supremacists. \"Joanna Schroeder is a writer and mother of three in Southern California and joins us now. Thank you for being with us. JOANNA SCHROEDER: Thank you for having me, Scott. SIMON: You have, I gather, two sons and a daughter. What did you notice online? SCHROEDER: You know, I noticed that the more time my kids spent on YouTube and on social media, especially Instagram, the more they were raising questions that showed me that they were being exposed to some extreme right-wing propaganda. SIMON: Well, like what specifically? SCHROEDER: So for instance, my son came to me, and he asked, why is it that white people can't borrow black people's culture but black people can steal from white people culture? And he also asked questions about transgender folks that seemed like they came from places other than us. We've talked to our kids about LGBTQ community. We know trans people personally. And so one of my kids said, if you can be trans and just decide what you are, then how come you can't just decide to be a penguin? Which sounds kind of like a normal kid question, but I also wanted to know where is this coming from? SIMON: Yeah. SCHROEDER: And when one of my sons answered, well, I saw a meme on Instagram, I knew it was time to start looking at their social media use and figuring out what they were being exposed to. SIMON: So what did you notice? SCHROEDER: So, you know, the way I use Instagram is I look at my news feed and I see pictures of my friends' babies and my mom's vacation. The way they use Instagram is so different. They switch over to their Explore screen, and they see photos and memes that are related to the things they've been searching and the things they've been liking and watching of their friends. So what I noticed was there was a lot of joke memes that were about Nazis or Hitler or about, you know, feminazis. And then when I started looking at their YouTube use, that's when I started to get more concerned. What I found was that my kids were looking at, you know, how to build a certain thing in Minecraft or what was happening in Fortnite, but what was being suggested over to the right were videos by political vloggers. And they frame their sort of right-wing propaganda as jokes and funny and, you know, ha, ha, ha, this about feminists. The more the kids watch that suggested video, the more extremist the videos that are suggested to them become. SIMON: I have to ask. I mean, did you say to your kids, show this to us, tell us what your - show us what you're seeing? SCHROEDER: Yeah, we did. We - the first thing we wanted to make sure was that we didn't approach it with any shaming because they're kids. And we can't expect them to automatically be able to detect propaganda when it's being presented to them. But beyond that, I think when you start to mock or roll your eyes or be like, that's garbage, that's trash, your kids shut you out. So instead, we inquired more. Where did you hear this? Where did you see this? Can you show me that? And when they showed us, I - the first thing we tried to do was say, I get why this seems funny on the surface, and I totally get why it's confusing. And they were also younger at the time, so they were probably a little more open than a kid would be at 16 or 17. SIMON: Well, you anticipate one of my questions as you talk about as youngsters age. I mean, don't teens manage to find a way to evade the best parental advice in any case? SCHROEDER: Oh, yeah, for sure. I mean, what I hoped that we were able to build with our kids from when they were young is a foundation where they believe that when we say something is not great, maybe they disobey, but deep inside there's a little voice that's going to say to them, you know, I should question this. I should question why this seems so, you know, funny and, yet, I feel like I have to keep it a secret. But, you know, we have had parental filters on our computers and on our media. As the kids got older, we realized pretty much every kid can evade any filter. SIMON: What about the girls in your sons' classes? SCHROEDER: When a friend of mine pointed out that her daughter was being targeted by boys at school using a lot of these kinds of memes and words that they were learning from these extremists, I was devastated. And she told me her daughter was getting images of guns in her direct messages and, you're a feminazi and feminazis are - should be eliminated from this Earth. And to a 13-year-old girl, it's really scary. SIMON: As a writer, have you figured out why some people find this stuff appealing? SCHROEDER: Yeah. I think there's a vulnerable group of boys, and even men, in society that - and I don't know who the forces are online. I don't know if it's malevolent media or just vloggers that want more views, but they've learned that they can target these men and boys. With men, I learned it's a lot of men who are male survivors of sexual assault or men who've been disenfranchised from their economic opportunities, divorced men. These communities target those men and their willingness to believe that society is out to get white men, their willingness to believe that women are all money-grabbing social climbers. And with boys, I do think neurodivergent boys are being targeted, kids who may have learning difficulties, kids who have - are on the autism spectrum. But it's not just those boys. I think at this age, they're trying to figure out where they fit. They're insecure. They feel like girls have it all. Girls are happy and pretty, and white men are the enemies. And so these right-wing groups are tapping into that shame and feeding it to try and propagandize to them. SIMON: How does this make you feel about boys these days? SCHROEDER: You know, I worry for all of our kids, the way that, you know, propaganda is being spread online. But what I've noticed about boys of this generation is that they're also the most open-hearted, potentially kindest, critical thinking, loving, group of boys that I've ever seen. They hug other people's moms. They high-five and hug each other. And I think the potential for real greatness is there with our boys. SIMON: Joanna Schroeder, writer and a mother in Southern California. Thank you so much. SCHROEDER: Thank you so much for having me. (SOUNDBITE OF MUSIC) SCOTT SIMON, HOST:  Do you have white teenage sons? Listen up. So begins a thread on Twitter this week by Joanna Schroeder that continues, quote, \"social media and vloggers are actively laying groundwork in white teens to turn them into alt-right white supremacists. \" Joanna Schroeder is a writer and mother of three in Southern California and joins us now. Thank you for being with us. JOANNA SCHROEDER: Thank you for having me, Scott. SIMON: You have, I gather, two sons and a daughter. What did you notice online? SCHROEDER: You know, I noticed that the more time my kids spent on YouTube and on social media, especially Instagram, the more they were raising questions that showed me that they were being exposed to some extreme right-wing propaganda. SIMON: Well, like what specifically? SCHROEDER: So for instance, my son came to me, and he asked, why is it that white people can't borrow black people's culture but black people can steal from white people culture? And he also asked questions about transgender folks that seemed like they came from places other than us. We've talked to our kids about LGBTQ community. We know trans people personally. And so one of my kids said, if you can be trans and just decide what you are, then how come you can't just decide to be a penguin? Which sounds kind of like a normal kid question, but I also wanted to know where is this coming from? SIMON: Yeah. SCHROEDER: And when one of my sons answered, well, I saw a meme on Instagram, I knew it was time to start looking at their social media use and figuring out what they were being exposed to. SIMON: So what did you notice? SCHROEDER: So, you know, the way I use Instagram is I look at my news feed and I see pictures of my friends' babies and my mom's vacation. The way they use Instagram is so different. They switch over to their Explore screen, and they see photos and memes that are related to the things they've been searching and the things they've been liking and watching of their friends. So what I noticed was there was a lot of joke memes that were about Nazis or Hitler or about, you know, feminazis. And then when I started looking at their YouTube use, that's when I started to get more concerned. What I found was that my kids were looking at, you know, how to build a certain thing in Minecraft or what was happening in Fortnite, but what was being suggested over to the right were videos by political vloggers. And they frame their sort of right-wing propaganda as jokes and funny and, you know, ha, ha, ha, this about feminists. The more the kids watch that suggested video, the more extremist the videos that are suggested to them become. SIMON: I have to ask. I mean, did you say to your kids, show this to us, tell us what your - show us what you're seeing? SCHROEDER: Yeah, we did. We - the first thing we wanted to make sure was that we didn't approach it with any shaming because they're kids. And we can't expect them to automatically be able to detect propaganda when it's being presented to them. But beyond that, I think when you start to mock or roll your eyes or be like, that's garbage, that's trash, your kids shut you out. So instead, we inquired more. Where did you hear this? Where did you see this? Can you show me that? And when they showed us, I - the first thing we tried to do was say, I get why this seems funny on the surface, and I totally get why it's confusing. And they were also younger at the time, so they were probably a little more open than a kid would be at 16 or 17. SIMON: Well, you anticipate one of my questions as you talk about as youngsters age. I mean, don't teens manage to find a way to evade the best parental advice in any case? SCHROEDER: Oh, yeah, for sure. I mean, what I hoped that we were able to build with our kids from when they were young is a foundation where they believe that when we say something is not great, maybe they disobey, but deep inside there's a little voice that's going to say to them, you know, I should question this. I should question why this seems so, you know, funny and, yet, I feel like I have to keep it a secret. But, you know, we have had parental filters on our computers and on our media. As the kids got older, we realized pretty much every kid can evade any filter. SIMON: What about the girls in your sons' classes? SCHROEDER: When a friend of mine pointed out that her daughter was being targeted by boys at school using a lot of these kinds of memes and words that they were learning from these extremists, I was devastated. And she told me her daughter was getting images of guns in her direct messages and, you're a feminazi and feminazis are - should be eliminated from this Earth. And to a 13-year-old girl, it's really scary. SIMON: As a writer, have you figured out why some people find this stuff appealing? SCHROEDER: Yeah. I think there's a vulnerable group of boys, and even men, in society that - and I don't know who the forces are online. I don't know if it's malevolent media or just vloggers that want more views, but they've learned that they can target these men and boys. With men, I learned it's a lot of men who are male survivors of sexual assault or men who've been disenfranchised from their economic opportunities, divorced men. These communities target those men and their willingness to believe that society is out to get white men, their willingness to believe that women are all money-grabbing social climbers. And with boys, I do think neurodivergent boys are being targeted, kids who may have learning difficulties, kids who have - are on the autism spectrum. But it's not just those boys. I think at this age, they're trying to figure out where they fit. They're insecure. They feel like girls have it all. Girls are happy and pretty, and white men are the enemies. And so these right-wing groups are tapping into that shame and feeding it to try and propagandize to them. SIMON: How does this make you feel about boys these days? SCHROEDER: You know, I worry for all of our kids, the way that, you know, propaganda is being spread online. But what I've noticed about boys of this generation is that they're also the most open-hearted, potentially kindest, critical thinking, loving, group of boys that I've ever seen. They hug other people's moms. They high-five and hug each other. And I think the potential for real greatness is there with our boys. SIMON: Joanna Schroeder, writer and a mother in Southern California. Thank you so much. SCHROEDER: Thank you so much for having me. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-19-752328453": {"title": "San Francisco's FogCam, Billed As The Oldest Running Webcam, Is Set To Go Dark : NPR", "url": "https://www.npr.org/2019/08/19/752328453/the-end-is-nigh-for-fogcam-billed-as-the-internets-oldest-running-webcam", "author": "No author found", "published_date": "2019-08-19", "content": "MARY LOUISE KELLY, HOST: Lots of things have changed in San Francisco since nearby Silicon Valley sparked a tech revolution. But for the past quarter-century, you could at least count on a couple of things - the fog that often shrouds the city and the live webcam that watches it. FogCam is said to be the world's oldest running webcam. It's one of the oldest websites, period. But as NPR's Colin Dwyer reports, the end is near for this Internet landmark. COLIN DWYER, BYLINE: The two guys who created FogCam didn't say too much when they announced its coming demise - just a single tweet Sunday, saying that their webcam at San Francisco State University is going dark at the end of the month. That kind of simplicity is fitting for an operation that has been pretty understated from the very beginning. Here's Jeff Schwartz, who started the cam with Dan Wong on a shoestring budget back when they were students in 1994. JEFF SCHWARTZ: Dan and I scrounged together some equipment - I mean, old equipment that no one really wanted to use, bought a little cheap camera at the college bookstore and threw together some AppleScript and some freeware and create a webcam. DWYER: Schwartz and Wong - who go by the nicknames Webdog and Danno online - say that theirs was not the first webcam. That distinction goes to a camera that watched a communal coffee pot at the University of Cambridge. But while that camera shut down nearly two decades ago, FogCam is still going strong for now, not that there that there was ever any big, serious mission behind it. SCHWARTZ: We've just kept it going for 25 years because it was just this kind of little cool thing that we liked. DWYER: But these days, keeping the cam up and running has just become too much trouble. Schwartz says that the school just kind of tolerated its presence, and they've had to move it around a bunch over the years. At one point, it was looking down on a coffee shop, which Schwartz found funny - an inside joke that was a nod to the Cambridge coffee pot cam. Currently, FogCam is taking images of a street on campus, refreshed every 20 seconds. But to Schwartz, it's about more than a stretch of Holloway Avenue. FogCam is a vestige of a different era. SCHWARTZ: Anyone, just some individual, could just create something cool, set it up, publish it on the Internet, and people could come and look at it. It was a fun time, an interesting time, and I think in some ways the Internet lacks that today. DWYER: The forecast for FogCam's final days is partly cloudy, hopefully with just a touch of fog. Colin Dwyer, NPR News. (SOUNDBITE OF LCD SOUNDSYSTEM'S \"BLACK SCREEN\") MARY LOUISE KELLY, HOST:  Lots of things have changed in San Francisco since nearby Silicon Valley sparked a tech revolution. But for the past quarter-century, you could at least count on a couple of things - the fog that often shrouds the city and the live webcam that watches it. FogCam is said to be the world's oldest running webcam. It's one of the oldest websites, period. But as NPR's Colin Dwyer reports, the end is near for this Internet landmark. COLIN DWYER, BYLINE: The two guys who created FogCam didn't say too much when they announced its coming demise - just a single tweet Sunday, saying that their webcam at San Francisco State University is going dark at the end of the month. That kind of simplicity is fitting for an operation that has been pretty understated from the very beginning. Here's Jeff Schwartz, who started the cam with Dan Wong on a shoestring budget back when they were students in 1994. JEFF SCHWARTZ: Dan and I scrounged together some equipment - I mean, old equipment that no one really wanted to use, bought a little cheap camera at the college bookstore and threw together some AppleScript and some freeware and create a webcam. DWYER: Schwartz and Wong - who go by the nicknames Webdog and Danno online - say that theirs was not the first webcam. That distinction goes to a camera that watched a communal coffee pot at the University of Cambridge. But while that camera shut down nearly two decades ago, FogCam is still going strong for now, not that there that there was ever any big, serious mission behind it. SCHWARTZ: We've just kept it going for 25 years because it was just this kind of little cool thing that we liked. DWYER: But these days, keeping the cam up and running has just become too much trouble. Schwartz says that the school just kind of tolerated its presence, and they've had to move it around a bunch over the years. At one point, it was looking down on a coffee shop, which Schwartz found funny - an inside joke that was a nod to the Cambridge coffee pot cam. Currently, FogCam is taking images of a street on campus, refreshed every 20 seconds. But to Schwartz, it's about more than a stretch of Holloway Avenue. FogCam is a vestige of a different era. SCHWARTZ: Anyone, just some individual, could just create something cool, set it up, publish it on the Internet, and people could come and look at it. It was a fun time, an interesting time, and I think in some ways the Internet lacks that today. DWYER: The forecast for FogCam's final days is partly cloudy, hopefully with just a touch of fog. Colin Dwyer, NPR News. (SOUNDBITE OF LCD SOUNDSYSTEM'S \"BLACK SCREEN\")", "section": "National", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-19-752327370": {"title": "Trump Administration Announces Another 90-Day Reprieve For Huawei : NPR", "url": "https://www.npr.org/2019/08/19/752327370/u-s-gives-huawei-another-90-day-reprieve-amid-concerns-of-rural-service-disrupti", "author": "No author found", "published_date": "2019-08-19", "content": "", "section": "Business", "disclaimer": ""}, "2019-08-19-751794126": {"title": "In the U.K., Female Lawmakers Pushed Government To Regulate Big Tech : NPR", "url": "https://www.npr.org/2019/08/19/751794126/trolled-online-women-in-politics-fight-to-hold-big-tech-accountable-in-the-u-k", "author": "No author found", "published_date": "2019-08-19", "content": "NOEL KING, HOST:  In Europe, unlike in the U. S. , governments have been holding the tech companies Google, Facebook and Twitter liable for social media attacks that happen on their platforms. Those governments got tough on tech in part because of the experiences of women in politics. NPR's Aarti Shahani dug into the backstory in the U. K. And we should note that Google and Facebook are financial contributors to NPR. AARTI SHAHANI, BYLINE: Lisa Cameron is a member of the British Parliament. She's also a victim and survivor of online trolls. LISA CAMERON: You feel very alone when this happens to you. SHAHANI: Cameron was new to politics in 2015 when she was elected. She'd been a psychologist, a wife and mom, a trade union rep, the kind of person democracies want to run. But once she entered public life, the sludge of the internet attacked her, not just for her policy stances. It was personal. She says her inbox, Facebook and Twitter accounts filled with rape fantasies, anti-Semitic slurs - she is Jewish - pictures of decapitated bodies, threats to her family. CAMERON: It makes you question whether you are doing something wrong in your job, whether politics is right for you. SHAHANI: She wondered if she'd made a mistake, if running was unfair to her family. Then a horrific attack - not against her, but against a female colleague sworn in in the same class - changed the conversation for Cameron and for the U. K. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER: The murder of British MP Jo Cox shocked Britain and the world. SHAHANI: Jo Cox, member of Parliament, was gunned down and stabbed on the streets by a white supremacist. Prosecutors said he was radicalized on the internet. His motive appeared to be policy - he was for Brexit, Cox against - but Cox's colleagues believe her gender made her a target, too. The Prime Minister's office reached out to men and women in Parliament to ask if they'd been intimidated online. The final report didn't set out to spotlight women in office, but it ended up doing that because they had stories. One reported, quote, \"people wish to see me raped. They wish to come to find my sons hanging from a tree because I don't care about men. \" MP Lisa Cameron said it was cathartic to speak up. CAMERON: I had kind of felt at the start that, you know, I must have been doing something wrong until I realized that this is so widespread. SHAHANI: In another study, a survey of Parliament, male MPs reported being insulted online for job performance more than female MPs did. But women reported far more threats of rape, death and violence to their children and loved ones. Online attacks were affecting real-world behavior. Nearly half of women in Parliament reported making fewer social outings. For men, it was 5%. CHLOE COLLIVER: There are probably a number of politicians, especially male politicians, who were unaware that that was happening to the extent it was. SHAHANI: Chloe Colliver is a researcher with the Institute for Strategic Dialogue, which helps governments study extremism. Colliver says the investigations didn't lie dormant in a dusty filing cabinet. They caused a sea change. Before, government leaders focused on Islamic extremism. But after, they began to see misogyny and the vitriolic hate of women as a rising organized threat. Female MPs organized in Cox's wake to explore how to keep women interested in politics. COLLIVER: Her fellow colleagues, who obviously felt very affected by this incident, have themselves joined across parties to try and see action taken on this front. SHAHANI: The clearest legal impact of this awakening came in a much anticipated blueprint for how to regulate big tech. The government wrote the abuse faced by women in office is unacceptable. The U. K. proposed hefty fines against companies that don't address this harm promptly. Chloe Colliver. COLLIVER: A female MP who has death threats directed at her through an online platform like Facebook or Twitter or YouTube could take that complaint to a regulator if it wasn't removed in a timely manner. SHAHANI: The U. K. is not troll-free, but women are now empowered in a way they were not just a few years ago. MP Lisa Cameron recently cast a vote against abortion. Pro-choice trolls sent her violent images and threatened to abort her. Now she says at least one internet giant is taking responsibility. CAMERON: If someone is abusive online, I can have that dialogue with Facebook. I know who to go to. SHAHANI: The company is helping her sift through the pile of attacks to ID real threats and refer them to law enforcement. That's a step up from shame, she says, and a step forward for democracy. Aarti Shahani, NPR News. (SOUNDBITE OF EVOCATIV'S \"CASTAWAY\") NOEL KING, HOST:   In Europe, unlike in the U. S. , governments have been holding the tech companies Google, Facebook and Twitter liable for social media attacks that happen on their platforms. Those governments got tough on tech in part because of the experiences of women in politics. NPR's Aarti Shahani dug into the backstory in the U. K. And we should note that Google and Facebook are financial contributors to NPR. AARTI SHAHANI, BYLINE: Lisa Cameron is a member of the British Parliament. She's also a victim and survivor of online trolls. LISA CAMERON: You feel very alone when this happens to you. SHAHANI: Cameron was new to politics in 2015 when she was elected. She'd been a psychologist, a wife and mom, a trade union rep, the kind of person democracies want to run. But once she entered public life, the sludge of the internet attacked her, not just for her policy stances. It was personal. She says her inbox, Facebook and Twitter accounts filled with rape fantasies, anti-Semitic slurs - she is Jewish - pictures of decapitated bodies, threats to her family. CAMERON: It makes you question whether you are doing something wrong in your job, whether politics is right for you. SHAHANI: She wondered if she'd made a mistake, if running was unfair to her family. Then a horrific attack - not against her, but against a female colleague sworn in in the same class - changed the conversation for Cameron and for the U. K. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER: The murder of British MP Jo Cox shocked Britain and the world. SHAHANI: Jo Cox, member of Parliament, was gunned down and stabbed on the streets by a white supremacist. Prosecutors said he was radicalized on the internet. His motive appeared to be policy - he was for Brexit, Cox against - but Cox's colleagues believe her gender made her a target, too. The Prime Minister's office reached out to men and women in Parliament to ask if they'd been intimidated online. The final report didn't set out to spotlight women in office, but it ended up doing that because they had stories. One reported, quote, \"people wish to see me raped. They wish to come to find my sons hanging from a tree because I don't care about men. \" MP Lisa Cameron said it was cathartic to speak up. CAMERON: I had kind of felt at the start that, you know, I must have been doing something wrong until I realized that this is so widespread. SHAHANI: In another study, a survey of Parliament, male MPs reported being insulted online for job performance more than female MPs did. But women reported far more threats of rape, death and violence to their children and loved ones. Online attacks were affecting real-world behavior. Nearly half of women in Parliament reported making fewer social outings. For men, it was 5%. CHLOE COLLIVER: There are probably a number of politicians, especially male politicians, who were unaware that that was happening to the extent it was. SHAHANI: Chloe Colliver is a researcher with the Institute for Strategic Dialogue, which helps governments study extremism. Colliver says the investigations didn't lie dormant in a dusty filing cabinet. They caused a sea change. Before, government leaders focused on Islamic extremism. But after, they began to see misogyny and the vitriolic hate of women as a rising organized threat. Female MPs organized in Cox's wake to explore how to keep women interested in politics. COLLIVER: Her fellow colleagues, who obviously felt very affected by this incident, have themselves joined across parties to try and see action taken on this front. SHAHANI: The clearest legal impact of this awakening came in a much anticipated blueprint for how to regulate big tech. The government wrote the abuse faced by women in office is unacceptable. The U. K. proposed hefty fines against companies that don't address this harm promptly. Chloe Colliver. COLLIVER: A female MP who has death threats directed at her through an online platform like Facebook or Twitter or YouTube could take that complaint to a regulator if it wasn't removed in a timely manner. SHAHANI: The U. K. is not troll-free, but women are now empowered in a way they were not just a few years ago. MP Lisa Cameron recently cast a vote against abortion. Pro-choice trolls sent her violent images and threatened to abort her. Now she says at least one internet giant is taking responsibility. CAMERON: If someone is abusive online, I can have that dialogue with Facebook. I know who to go to. SHAHANI: The company is helping her sift through the pile of attacks to ID real threats and refer them to law enforcement. That's a step up from shame, she says, and a step forward for democracy. Aarti Shahani, NPR News. (SOUNDBITE OF EVOCATIV'S \"CASTAWAY\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-20-749946265": {"title": "'Because Internet' By Gretchen McCulloch Tracks The Evolution Of Language Online : NPR", "url": "https://www.npr.org/2019/08/20/749946265/opinion-ironic-informal-and-expressive-new-rules-of-language-evolve-online", "author": "No author found", "published_date": "2019-08-20", "content": "DAVE DAVIES, HOST: This is FRESH AIR. With all our texting, tweeting and social media posting, billions of people are using typed words for the kind of everyday communication that used to happen more often in conversation. A new book argues that we've created a unique new language to reproduce the shades of meaning we used to convey orally. Our linguist Geoff Nunberg reflects on the new rules of language that he calls chatspeak. GEOFF NUNBERG, BYLINE: You recall back in 2004 when George W. Bush referred to rumors on the internets. That instantly became a classic Bush-ism. But to my mind, he got it right, not just because what we call the internet originated as a collection of networks 40 years ago, but because what people call internet culture is an ocean of yammer strewn with innumerable islands and continents, each with its own rules, customs and conversations. But in a lively and wide-ranging new book called \"Because Internet,\" the linguist Gretchen McCullough argues that a huge part of our internet lives is conducted in a common vernacular, what she calls in her subtitle, the new rules of language. I think of it as chatspeak. McCullough traces its origins back to the early days of email, but it came into full flower in the modern era of social media. It's the style of informal writing people use when they go to the Internet to reproduce the sense of connectedness they get from hanging out in physical gathering places like high school cafeterias, bars and coffee houses. To that end, they've had to devise ways of compensating for the loss of the gestures, expressions and vocal effects that convey meaning in face-to-face conversation. That is, they've had to create what McCulloch calls a typographical tone of voice. They've done that through what social scientists called bricolage; cobbling together the straps of material you happen to have on hand. They convey emphasis by putting spaces between letters or using all caps or repeating exclamation points. They lengthen words like yes or no to suggest a friendly intimacy or they achieve other effects by repurposing the rules of formal writing. There's no need to mark the end of a text message with a period, so when somebody does use one, it's taken as a sign of passive aggressiveness - I'm fine, period. Then there are all the graphic indicators, such as emoticons like smileys, which morphed into a whole army of emojis, particularly the emblematic ones like thumbs up and praying hands. McCulloch likens those to digital gestures. Or if you really want to display an emotion, you can conscript somebody else to do it for you in the form of an animated GIF - a frustrated Whoopi Goldberg, a shimmying Steph Curry, a little girl cringing in disgust. It's striking how many ways there are to signal irony on the Internet. You can bracket a word with tildes, the wavy little mark over the N in ma\u00f1ana, as in, well, isn't that ~special~? Or you can just write #irony after a sentence. It's a curious innovation. People have been proposing punctuation marks to indicate irony since the 17th century, but authors have preferred to let readers figure it out for themselves. It's hard to imagine Jane Austen sprinkling her works with smileys and winkies. But the social Internet is awash with overt irony, often tipping into sarcasm. That has its dangers. As McCulloch notes, irony can be used to make abhorrent beliefs look appealing. On the message boards of the extreme right, people routinely espouse racist views while saying it's all in fun. That's not a new phenomenon. During World War II, the philosopher Jean-Paul Sartre made the same point about how Nazi anti-Semites resorted to jokiness to put their ideas across. The need for those overt tip-offs of irony reflects a big difference between chat speak and face-to-face conversation. You don't always know who's out there or how your words are going to land. American adolescents average around 300 Facebook friends and 150 Instagram followers. That's a whole lot more than the posse they hang out with after school. When the audience is that diffuse and open-ended, their posts take the shape of a self-consciously crafted performance. They post to proclaim their presence, to connect with others of their kind and to evoke responses from anyone within earshot, whether it's likes or forwards or replies. And they time their posts for maximum impact. Usually, in the morning, people start tweeting around the same time birds do and for the same reasons. McCulloch argues that all our texting and tweeting has enabled ordinary people to write with exquisite levels of social nuance, as she puts it, which used to be the province of professionals. It's certainly not doing any damage to the English language, despite the fulminations of the people I think of as the great unwired. Those stories about texting slang and abbreviations showing up on students' writing are pure urban legends. Bear in mind that the whole point of slang is knowing who you're not supposed to use it with. But if chat speak helps fulfill people's yearnings for connectedness, it isn't very well-adapted to developing any idea that's bigger than an aphorism. That belongs to another continent of Internet discourse on the blogs and websites where you can find a vast accumulation of intelligent commentary in critical writing by millions of authors who could never make their ideas public before. It's not exactly formal writing. In fact, it's inherited a lot of informality from the give and take of Internet chatter. But it's still a language where writers are conscientious about breaking up their thoughts with periods and where they don't telegraph their irony. I don't know whether a fluency with McCulloch's new rules of writing gives you a leg up when it comes to mastering the old ones. But if it makes you reflective about the way you use written words, it's a good place to start. DAVIES: Geoff Nunberg is a linguist at the University of California Berkeley School of Information. On tomorrow's show, our guest will be character actor Stephen Root. He's probably best known for his role as the sad-sack office worker in the cult favorite film \"Office Space. \" He was a voice actor for the animated series \"King Of The Hill,\" and he played the station manager on NBC's \"NewsRadio. \" After appearing in nearly 800 TV episodes and a hundred films, he's received his first Emmy nomination for his role as the handler of a hitman in the HBO series \"Barry. \" Hope you can join us. (SOUNDBITE OF MUSIC)DAVIES: FRESH AIR's executive producer is Danny Miller. Our interviews and reviews are produced and edited by Amy Salit, Phyllis Myers, Sam Briger, Lauren Krenzel, Therese Madden, Ann Marie Baldonado, Mooj Zadie, Thea Chaloner, Seth Kelley and Joel Wolfram. For Terry Gross, I'm Dave Davies. (SOUNDBITE OF MUSIC) DAVE DAVIES, HOST:  This is FRESH AIR. With all our texting, tweeting and social media posting, billions of people are using typed words for the kind of everyday communication that used to happen more often in conversation. A new book argues that we've created a unique new language to reproduce the shades of meaning we used to convey orally. Our linguist Geoff Nunberg reflects on the new rules of language that he calls chatspeak. GEOFF NUNBERG, BYLINE: You recall back in 2004 when George W. Bush referred to rumors on the internets. That instantly became a classic Bush-ism. But to my mind, he got it right, not just because what we call the internet originated as a collection of networks 40 years ago, but because what people call internet culture is an ocean of yammer strewn with innumerable islands and continents, each with its own rules, customs and conversations. But in a lively and wide-ranging new book called \"Because Internet,\" the linguist Gretchen McCullough argues that a huge part of our internet lives is conducted in a common vernacular, what she calls in her subtitle, the new rules of language. I think of it as chatspeak. McCullough traces its origins back to the early days of email, but it came into full flower in the modern era of social media. It's the style of informal writing people use when they go to the Internet to reproduce the sense of connectedness they get from hanging out in physical gathering places like high school cafeterias, bars and coffee houses. To that end, they've had to devise ways of compensating for the loss of the gestures, expressions and vocal effects that convey meaning in face-to-face conversation. That is, they've had to create what McCulloch calls a typographical tone of voice. They've done that through what social scientists called bricolage; cobbling together the straps of material you happen to have on hand. They convey emphasis by putting spaces between letters or using all caps or repeating exclamation points. They lengthen words like yes or no to suggest a friendly intimacy or they achieve other effects by repurposing the rules of formal writing. There's no need to mark the end of a text message with a period, so when somebody does use one, it's taken as a sign of passive aggressiveness - I'm fine, period. Then there are all the graphic indicators, such as emoticons like smileys, which morphed into a whole army of emojis, particularly the emblematic ones like thumbs up and praying hands. McCulloch likens those to digital gestures. Or if you really want to display an emotion, you can conscript somebody else to do it for you in the form of an animated GIF - a frustrated Whoopi Goldberg, a shimmying Steph Curry, a little girl cringing in disgust. It's striking how many ways there are to signal irony on the Internet. You can bracket a word with tildes, the wavy little mark over the N in ma\u00f1ana, as in, well, isn't that ~special~? Or you can just write #irony after a sentence. It's a curious innovation. People have been proposing punctuation marks to indicate irony since the 17th century, but authors have preferred to let readers figure it out for themselves. It's hard to imagine Jane Austen sprinkling her works with smileys and winkies. But the social Internet is awash with overt irony, often tipping into sarcasm. That has its dangers. As McCulloch notes, irony can be used to make abhorrent beliefs look appealing. On the message boards of the extreme right, people routinely espouse racist views while saying it's all in fun. That's not a new phenomenon. During World War II, the philosopher Jean-Paul Sartre made the same point about how Nazi anti-Semites resorted to jokiness to put their ideas across. The need for those overt tip-offs of irony reflects a big difference between chat speak and face-to-face conversation. You don't always know who's out there or how your words are going to land. American adolescents average around 300 Facebook friends and 150 Instagram followers. That's a whole lot more than the posse they hang out with after school. When the audience is that diffuse and open-ended, their posts take the shape of a self-consciously crafted performance. They post to proclaim their presence, to connect with others of their kind and to evoke responses from anyone within earshot, whether it's likes or forwards or replies. And they time their posts for maximum impact. Usually, in the morning, people start tweeting around the same time birds do and for the same reasons. McCulloch argues that all our texting and tweeting has enabled ordinary people to write with exquisite levels of social nuance, as she puts it, which used to be the province of professionals. It's certainly not doing any damage to the English language, despite the fulminations of the people I think of as the great unwired. Those stories about texting slang and abbreviations showing up on students' writing are pure urban legends. Bear in mind that the whole point of slang is knowing who you're not supposed to use it with. But if chat speak helps fulfill people's yearnings for connectedness, it isn't very well-adapted to developing any idea that's bigger than an aphorism. That belongs to another continent of Internet discourse on the blogs and websites where you can find a vast accumulation of intelligent commentary in critical writing by millions of authors who could never make their ideas public before. It's not exactly formal writing. In fact, it's inherited a lot of informality from the give and take of Internet chatter. But it's still a language where writers are conscientious about breaking up their thoughts with periods and where they don't telegraph their irony. I don't know whether a fluency with McCulloch's new rules of writing gives you a leg up when it comes to mastering the old ones. But if it makes you reflective about the way you use written words, it's a good place to start. DAVIES: Geoff Nunberg is a linguist at the University of California Berkeley School of Information. On tomorrow's show, our guest will be character actor Stephen Root. He's probably best known for his role as the sad-sack office worker in the cult favorite film \"Office Space. \" He was a voice actor for the animated series \"King Of The Hill,\" and he played the station manager on NBC's \"NewsRadio. \" After appearing in nearly 800 TV episodes and a hundred films, he's received his first Emmy nomination for his role as the handler of a hitman in the HBO series \"Barry. \" Hope you can join us. (SOUNDBITE OF MUSIC) DAVIES: FRESH AIR's executive producer is Danny Miller. Our interviews and reviews are produced and edited by Amy Salit, Phyllis Myers, Sam Briger, Lauren Krenzel, Therese Madden, Ann Marie Baldonado, Mooj Zadie, Thea Chaloner, Seth Kelley and Joel Wolfram. For Terry Gross, I'm Dave Davies. (SOUNDBITE OF MUSIC)", "section": "Book Reviews", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-20-752695554": {"title": "Ransomware Attack Affects Computers In 22 Towns In Texas : NPR", "url": "https://www.npr.org/2019/08/20/752695554/23-texas-towns-hit-with-ransomware-attack-in-new-front-of-cyberassault", "author": "No author found", "published_date": "2019-08-20", "content": "NOEL KING, HOST: In Texas, officials in nearly two dozen cities are scrambling to restore their computer networks. Hackers crippled the city systems and then demanded a ransom. Now, this is not just Texas. It's a growing problem, where cyberattackers will hijack a local government in search of a profit. But as NPR's Bobby Allyn reports, security experts say the sheer number of Texas agencies targeted all at once is a new front in ransomware attacks. BOBBY ALLYN, BYLINE: Keene, Texas, is a speck of a town about 40 miles south of Fort Worth. Mayor Gary Heinrich says it's usually a pretty quiet place, until recently, when he learned that anonymous cyberattackers had infiltrated the city's computer network. He's been hearing a lot about it from the city's 6,000 residents. GARY HEINRICH: Well, just everything we do at City Hall that's computerized is impacted. ALLYN: The disruption has been felt across Texas. State officials say 22 municipalities in mostly rural swaths of the state were victimized. Who's behind it? Authorities say one single threat actor. Heinrich says the ransomware was aimed at an information technology firm that provides services to local governments around the state. HEINRICH: They just got into our software provider, the guys who kind of run our IT systems. That happens to a lot of folks in Texas that use providers to do that because we don't have a staff big enough to have IT in house. ALLYN: State investigators and the FBI are rushing to bring computer systems back on while the cyberattackers continue to hold the government systems hostage. Heinrich says the hackers are asking for a ransom of $2. 5 million. HEINRICH: Stupid people. You know, there's just no sense in this at all. ALLYN: Local governments are now the new favorite target of cybercriminals. In recent months, attacks have hit Georgia's court system, public schools in Oklahoma and the city of Baltimore. Allan Liska with the research firm Recorded Future has been tracking how often hackers break into local government computer systems. He says there have already been more than 60 such attacks this year. But the Texas breach stands out to him. ALLAN LISKA: This is the first time that we've seen a coordinated attack strike so many municipalities at the same time. ALLYN: Liska says this is how they usually happen. Someone gets what looks like a seemingly harmless email from a colleague and opens an attachment, allowing hackers to launch a program that locks up an entire computer system. For residents, it's a huge headache. TAD MCGALLIARD: If you are in one of those cities and you're trying to buy a house today, you probably can't do that because title services are offline. If you are trying to pay your water bill, you can't do that. In Atlanta and Baltimore, we saw that court cases had to be delayed because dockets were encrypted. ALLYN: Tad McGalliard studies local government cybersecurity at the Washington-based city manager group ICMA. He says the Texas case should be a wakeup call to cities in all corners of the country. MCGALLIARD: We might have thought this was a big-city problem or, at least, an affluent city or county problem. But I think what's clear now is that just about any local government is vulnerable. ALLYN: Back in Keene, Texas, Heinrich says even if the city had a spare $2. 5 million, there's no way he'd cough it up to the hackers. HEINRICH: 'Cause once you pay it, They say, this is good, let's do it again. ALLYN: The cybercriminals who hit Baltimore asked for a ransom in bitcoin equivalent to around $76,000. City officials refused to pay it. The city's estimated cost of recovering from the ransomware attack, $18 million. Bobby Allyn, NPR News. NOEL KING, HOST:  In Texas, officials in nearly two dozen cities are scrambling to restore their computer networks. Hackers crippled the city systems and then demanded a ransom. Now, this is not just Texas. It's a growing problem, where cyberattackers will hijack a local government in search of a profit. But as NPR's Bobby Allyn reports, security experts say the sheer number of Texas agencies targeted all at once is a new front in ransomware attacks. BOBBY ALLYN, BYLINE: Keene, Texas, is a speck of a town about 40 miles south of Fort Worth. Mayor Gary Heinrich says it's usually a pretty quiet place, until recently, when he learned that anonymous cyberattackers had infiltrated the city's computer network. He's been hearing a lot about it from the city's 6,000 residents. GARY HEINRICH: Well, just everything we do at City Hall that's computerized is impacted. ALLYN: The disruption has been felt across Texas. State officials say 22 municipalities in mostly rural swaths of the state were victimized. Who's behind it? Authorities say one single threat actor. Heinrich says the ransomware was aimed at an information technology firm that provides services to local governments around the state. HEINRICH: They just got into our software provider, the guys who kind of run our IT systems. That happens to a lot of folks in Texas that use providers to do that because we don't have a staff big enough to have IT in house. ALLYN: State investigators and the FBI are rushing to bring computer systems back on while the cyberattackers continue to hold the government systems hostage. Heinrich says the hackers are asking for a ransom of $2. 5 million. HEINRICH: Stupid people. You know, there's just no sense in this at all. ALLYN: Local governments are now the new favorite target of cybercriminals. In recent months, attacks have hit Georgia's court system, public schools in Oklahoma and the city of Baltimore. Allan Liska with the research firm Recorded Future has been tracking how often hackers break into local government computer systems. He says there have already been more than 60 such attacks this year. But the Texas breach stands out to him. ALLAN LISKA: This is the first time that we've seen a coordinated attack strike so many municipalities at the same time. ALLYN: Liska says this is how they usually happen. Someone gets what looks like a seemingly harmless email from a colleague and opens an attachment, allowing hackers to launch a program that locks up an entire computer system. For residents, it's a huge headache. TAD MCGALLIARD: If you are in one of those cities and you're trying to buy a house today, you probably can't do that because title services are offline. If you are trying to pay your water bill, you can't do that. In Atlanta and Baltimore, we saw that court cases had to be delayed because dockets were encrypted. ALLYN: Tad McGalliard studies local government cybersecurity at the Washington-based city manager group ICMA. He says the Texas case should be a wakeup call to cities in all corners of the country. MCGALLIARD: We might have thought this was a big-city problem or, at least, an affluent city or county problem. But I think what's clear now is that just about any local government is vulnerable. ALLYN: Back in Keene, Texas, Heinrich says even if the city had a spare $2. 5 million, there's no way he'd cough it up to the hackers. HEINRICH: 'Cause once you pay it, They say, this is good, let's do it again. ALLYN: The cybercriminals who hit Baltimore asked for a ransom in bitcoin equivalent to around $76,000. City officials refused to pay it. The city's estimated cost of recovering from the ransomware attack, $18 million. Bobby Allyn, NPR News.", "section": "National", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-20-752668835": {"title": "Hong Kong Protests: China Uses Twitter And Facebook To Share Disinformation : NPR", "url": "https://www.npr.org/2019/08/20/752668835/how-china-uses-twitter-and-facebook-to-share-disinformation-about-hong-kong", "author": "No author found", "published_date": "2019-08-20", "content": "", "section": "Asia", "disclaimer": ""}, "2019-08-20-752670444": {"title": "Employees Demand Google Publicly Commit To Not Work With ICE : NPR", "url": "https://www.npr.org/2019/08/20/752670444/employees-demand-google-publicly-commit-to-not-work-with-ice", "author": "No author found", "published_date": "2019-08-20", "content": "DAVID GREENE, HOST: More than a thousand Google employees have demanded that the company publicly commit not to work with U. S. immigration enforcement agencies, and that includes the Border Patrol and ICE. This petition is forcing Google's hand at a time when it's risky for big tech to criticize the Trump administration. Here's NPR's Aarti Shahani. AARTI SHAHANI, BYLINE: U. S. Customs and Border Protection announced it's looking for a cloud provider, a tech company at the cutting edge of big data storage, to give IT support to its mission of securing the nation's border. Mark Egerman, a product manager at Google and an author of the petition, says while his company has not yet applied for the job, it's not too early to protest. MARK EGERMAN: There's the old parable that the best time to plant a tree was 20 years ago; the second-best time is today. SHAHANI: Googlers (ph) asked their bosses to commit to not applying, but Egerman says the bosses wouldn't. And given the enormous scale of the job - Border Patrol collects extensive data, including biometrics on millions of people coming in and out - he says it's very likely Google would apply. EGERMAN: There are about three companies in the United States capable of providing the cloud computing resources that CBP is asking for. Google is one of them. SHAHANI: Amazon and Microsoft are the others he's referring to. Those two companies are competing for a lucrative Pentagon contract. Google, which makes money from ads, wants to take a lead in the cloud business. It's a huge financial opportunity, as governments and companies move data online. But many rank-and-file Googlers are critical of Trump's immigration track record, the imprisonments of asylum-seekers, separating migrant children from their parents, the alleged death of children in custody and sexual abuse. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PROTESTERS: (Chanting) Let them out. Let them out. SHAHANI: Google founder Sergey Brin turned his company into a prominent voice against the administration when he showed up to San Francisco International Airport in 2017 to protest the president's ban against travelers from certain Muslim countries. Google did not respond to NPR's request for comment to this new employee petition. Egerman says he knows it's putting a spotlight on the company that leaders don't want, especially given the right-wing charge that Silicon Valley giants have an anti-conservative bias. EGERMAN: I am not envious of them for having to figure out how to best navigate this situation. SHAHANI: In an interview with Fox News, acting CBP Commissioner Mark Morgan called the Google petition absolutely irresponsible. Aarti Shahani, NPR News. (SOUNDBITE OF SOULAR ORDER'S \"KEYFRAMES\") DAVID GREENE, HOST:  More than a thousand Google employees have demanded that the company publicly commit not to work with U. S. immigration enforcement agencies, and that includes the Border Patrol and ICE. This petition is forcing Google's hand at a time when it's risky for big tech to criticize the Trump administration. Here's NPR's Aarti Shahani. AARTI SHAHANI, BYLINE: U. S. Customs and Border Protection announced it's looking for a cloud provider, a tech company at the cutting edge of big data storage, to give IT support to its mission of securing the nation's border. Mark Egerman, a product manager at Google and an author of the petition, says while his company has not yet applied for the job, it's not too early to protest. MARK EGERMAN: There's the old parable that the best time to plant a tree was 20 years ago; the second-best time is today. SHAHANI: Googlers (ph) asked their bosses to commit to not applying, but Egerman says the bosses wouldn't. And given the enormous scale of the job - Border Patrol collects extensive data, including biometrics on millions of people coming in and out - he says it's very likely Google would apply. EGERMAN: There are about three companies in the United States capable of providing the cloud computing resources that CBP is asking for. Google is one of them. SHAHANI: Amazon and Microsoft are the others he's referring to. Those two companies are competing for a lucrative Pentagon contract. Google, which makes money from ads, wants to take a lead in the cloud business. It's a huge financial opportunity, as governments and companies move data online. But many rank-and-file Googlers are critical of Trump's immigration track record, the imprisonments of asylum-seekers, separating migrant children from their parents, the alleged death of children in custody and sexual abuse. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PROTESTERS: (Chanting) Let them out. Let them out. SHAHANI: Google founder Sergey Brin turned his company into a prominent voice against the administration when he showed up to San Francisco International Airport in 2017 to protest the president's ban against travelers from certain Muslim countries. Google did not respond to NPR's request for comment to this new employee petition. Egerman says he knows it's putting a spotlight on the company that leaders don't want, especially given the right-wing charge that Silicon Valley giants have an anti-conservative bias. EGERMAN: I am not envious of them for having to figure out how to best navigate this situation. SHAHANI: In an interview with Fox News, acting CBP Commissioner Mark Morgan called the Google petition absolutely irresponsible. Aarti Shahani, NPR News. (SOUNDBITE OF SOULAR ORDER'S \"KEYFRAMES\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-21-753213161": {"title": "What Can Be Done To Fight Back Against Ransomware Attacks  : NPR", "url": "https://www.npr.org/2019/08/21/753213161/what-can-be-done-to-fight-back-against-ransomware-attacks", "author": "No author found", "published_date": "2019-08-21", "content": "AUDIE CORNISH, HOST: In Texas, authorities are dealing with the aftermath of a ransomware attack last week. Twenty-two mostly rural towns had their computer systems locked down by hackers who demanded millions of dollars in ransom. Experts say attacks like these are on the rise across the U. S. They're targeting both local governments and private companies. So what can be done? Well, Josephine Wolff has some thoughts about this. She studies cybersecurity policy at Tufts University. She recently wrote an op-ed in The New York Times entitled \"They Stole Your Files, You Don't Have To Pay The Ransom. \"Welcome to ALL THINGS CONSIDERED. JOSEPHINE WOLFF: Hi. Thanks so much for having me. CORNISH: So first, just set the stage for us. How common are these kinds of attacks on computer systems for cities and towns? And what happens once that ball gets rolling? WOLFF: So it's very hard to put an exact number on how common they are because a lot of the time, they go unreported. But just in the past couple years, we've seen Atlanta. We've seen Baltimore. We've now seen all of these towns in Texas. We've seen a bunch of towns in Florida. So even just the ones that do get publicly revealed that we know about, it's sort of astonishing how many of them we've seen in just the past two years alone. CORNISH: We don't know for sure, as you said, because they don't report. But is there a sense that people are paying up? WOLFF: There is certainly a sense that people are paying up, including public agency victims. We know at least two of the towns in Florida exceeded to fairly steep ransom demands in part because we think their insurers were covering a large chunk of that cost. So definitely, it is a situation where even the government actors who we would hope would be sort of role models in this are giving in to these ransom demands in some cases. CORNISH: Now, you talk about the No More Ransom initiative. Can you tell us more about what it is and how it would help ransomware victims - individuals - to start to get their data back? WOLFF: So the No Ransom Project is a collaboration between a bunch of law enforcement agencies - primarily in Europe but also all over the world - and private companies that develop tools to help people reverse the effects of ransomware without having to pay ransoms. And the law enforcement agencies provide an interface for people who've been affected to go online, upload one of their encrypted files or the ransom note that's been left behind, so they can figure out which strain of ransomware they're dealing with. We do know that people are using them. We know that they're working - not all the time. They can't be used for every strand of malware. But a lot of the time, we've seen people able to recover their files this way. CORNISH: Are these tools that can be applied on a larger scale, right? I mean, we've been talking about cities and towns dealing with these kinds of attacks. WOLFF: Absolutely. And I think one of the things that has been very frustrating for some of the companies that developed these tools is that none of the U. S. law enforcement agencies have partnered with No More Ransom or been willing to advertise or publicize any of these tools on their website. So some of the towns and cities in the U. S. may not even know about them, even though we know they've been infected by strands of ransomware that sometimes can be susceptible to these tools. CORNISH: You know, when someone is attacked in this way, a panic sets in, right? You're effectively told, look, we're holding all of your data hostage, and if you are a city or a hospital, you just want to get it back. You don't have a lot of time to Google no more ransomware. I mean, how should people be thinking about this in those moments? WOLFF: So I think that's a really good point, especially because a lot of these ransom demands have ticking clocks on them. They say, you know, in 24 hours, if we don't receive your payment, we're going to delete all of your files or the ransom is going to increase. And so people are often very frenzied in that moment. And that's one of the reasons, I think, it's so important to have a proactive awareness-raising campaign around this. CORNISH: Josephine Wolff is assistant professor of cybersecurity policy at the Tufts Fletcher School of Law and Diplomacy. Thank you for explaining this to us. WOLFF: Thanks so much for having me. AUDIE CORNISH, HOST:  In Texas, authorities are dealing with the aftermath of a ransomware attack last week. Twenty-two mostly rural towns had their computer systems locked down by hackers who demanded millions of dollars in ransom. Experts say attacks like these are on the rise across the U. S. They're targeting both local governments and private companies. So what can be done? Well, Josephine Wolff has some thoughts about this. She studies cybersecurity policy at Tufts University. She recently wrote an op-ed in The New York Times entitled \"They Stole Your Files, You Don't Have To Pay The Ransom. \" Welcome to ALL THINGS CONSIDERED. JOSEPHINE WOLFF: Hi. Thanks so much for having me. CORNISH: So first, just set the stage for us. How common are these kinds of attacks on computer systems for cities and towns? And what happens once that ball gets rolling? WOLFF: So it's very hard to put an exact number on how common they are because a lot of the time, they go unreported. But just in the past couple years, we've seen Atlanta. We've seen Baltimore. We've now seen all of these towns in Texas. We've seen a bunch of towns in Florida. So even just the ones that do get publicly revealed that we know about, it's sort of astonishing how many of them we've seen in just the past two years alone. CORNISH: We don't know for sure, as you said, because they don't report. But is there a sense that people are paying up? WOLFF: There is certainly a sense that people are paying up, including public agency victims. We know at least two of the towns in Florida exceeded to fairly steep ransom demands in part because we think their insurers were covering a large chunk of that cost. So definitely, it is a situation where even the government actors who we would hope would be sort of role models in this are giving in to these ransom demands in some cases. CORNISH: Now, you talk about the No More Ransom initiative. Can you tell us more about what it is and how it would help ransomware victims - individuals - to start to get their data back? WOLFF: So the No Ransom Project is a collaboration between a bunch of law enforcement agencies - primarily in Europe but also all over the world - and private companies that develop tools to help people reverse the effects of ransomware without having to pay ransoms. And the law enforcement agencies provide an interface for people who've been affected to go online, upload one of their encrypted files or the ransom note that's been left behind, so they can figure out which strain of ransomware they're dealing with. We do know that people are using them. We know that they're working - not all the time. They can't be used for every strand of malware. But a lot of the time, we've seen people able to recover their files this way. CORNISH: Are these tools that can be applied on a larger scale, right? I mean, we've been talking about cities and towns dealing with these kinds of attacks. WOLFF: Absolutely. And I think one of the things that has been very frustrating for some of the companies that developed these tools is that none of the U. S. law enforcement agencies have partnered with No More Ransom or been willing to advertise or publicize any of these tools on their website. So some of the towns and cities in the U. S. may not even know about them, even though we know they've been infected by strands of ransomware that sometimes can be susceptible to these tools. CORNISH: You know, when someone is attacked in this way, a panic sets in, right? You're effectively told, look, we're holding all of your data hostage, and if you are a city or a hospital, you just want to get it back. You don't have a lot of time to Google no more ransomware. I mean, how should people be thinking about this in those moments? WOLFF: So I think that's a really good point, especially because a lot of these ransom demands have ticking clocks on them. They say, you know, in 24 hours, if we don't receive your payment, we're going to delete all of your files or the ransom is going to increase. And so people are often very frenzied in that moment. And that's one of the reasons, I think, it's so important to have a proactive awareness-raising campaign around this. CORNISH: Josephine Wolff is assistant professor of cybersecurity policy at the Tufts Fletcher School of Law and Diplomacy. Thank you for explaining this to us. WOLFF: Thanks so much for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-21-751524877": {"title": "More U.S. Towns Are Feeling The Pinch As Recycling Becomes Costlier  : NPR", "url": "https://www.npr.org/2019/08/21/751524877/more-u-s-towns-are-feeling-the-pinch-as-recycling-becomes-costlier", "author": "No author found", "published_date": "2019-08-21", "content": "", "section": "Environment", "disclaimer": ""}, "2019-08-21-752484720": {"title": "Facial Recognition Technology And A Tip Helped The FBI Catch A Killer : NPR", "url": "https://www.npr.org/2019/08/21/752484720/how-a-tip-and-facial-recognition-technology-helped-the-fbi-catch-a-killer", "author": "No author found", "published_date": "2019-08-21", "content": "AILSA CHANG, HOST: There's a lot of debate and controversy over how law enforcement uses facial recognition technology. The FBI says it's an important investigative tool. Take the case of Walter Yovany-Gomez. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #1: One of America's most wanted criminals, a fugitive and an alleged member of the vicious MS-13 gang, now captured. CHANG: Captured in part because of facial recognition. NPR justice reporter Ryan Lucas explains how it all went down. RYAN LUCAS, BYLINE: Back in 2011, Walter Yovany-Gomez wanted to become a full member of the local branch of the MS-13 street gang in Plainfield, N. J. In May of that year, he got his chance. Gang leaders ordered him to kill a recruit named Julio Matute who had allegedly been associating with a rival gang. And so after a night of partying with Matute, Gomez and an associate beat him with a baseball bat, stabbed him with a screwdriver and slit his throat. Police closed in on Gomez in 2011, but he jumped out of a second-story window and escaped. For six years, his trail went cold. Then in April of 2017, the FBI made an announcement that put Gomez in the national crime spotlight. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #2: The violent gang member is the newest addition to the FBI's 10 most wanted fugitives list. An up to $100,000 reward is available for information leading directly to the arrest of Walter Yovany-Gomez. LUCAS: The move quickly paid off. A tipster came forward that July claiming to have information on Gomez. It fell to Special Agent Richard Stallings in the FBI's Washington field office to follow up. So Stallings and a gang task force colleague arranged a face-to-face meeting with the tipster. They wanted to vet the source and make sure, as Stallings puts it, they weren't running up a dead tree. They quickly determined that they were not. RICHARD STALLINGS: He truly was like, I know that guy. And he says, I don't know him by the name that's on the screen, Gomez. But he knew him on a different name. LUCAS: The tipster pointed the FBI to Facebook pages with photos of the man he believed was Gomez. Agents pulled the photos and sent them to the bureau's FACE Services Unit. There, they were run through an FBI database using facial recognition software in search of a match. About a week later, a match came back but for Jesus Lopez Centoreo. Centoreo had been picked up for marijuana possession in 2014 after jumping a Metro turnstile in Arlington, Va. His fingerprints and mug shot were taken, and then he was released. Centoreo had then failed to appear for his court date, so there was a warrant out for his arrest. STALLINGS: We were kind of baffled in how we got this name associated with our picture. But then the picture they had that they associated with in the arrest - like, that's our guy. LUCAS: Investigators still felt the tipster's information was solid, in part, Stallings says, because there were other physical identifiers like tattoos that made them think they were on the right track. They also had leads to work. Remember those Facebook pages the tipster provided? Agents also found several photos of the suspect with a female associate. Agents used facial recognition software to run the Facebook photos of her through a database of criminal mug shots. They came back with a match and an address. So the FBI and officers from the Fairfax County gang task force set up surveillance on her. One afternoon in August 2017, officers set up a stakeout outside her residence. STALLINGS: A couple hours later, I get this call back from their sergeant. We got our guy. And I was like, what? And he said, no, it's our guy. And, you know, the first thing in my mind, are you sure? Because we didn't know for sure he was here at any point. LUCAS: The agents had tailed the woman to a gym parking lot in Woodbridge, Va. , about 20 miles south of Washington, D. C. And who walks up to her car but a man who looked a lot like Gomez. Their surveillance team swooped in and arrested him, but there was a twist. The sergeant on the phone said the man gave his name as Jesus Lopez Centoreo, the same name that the FBI turned up in its facial recognition search. But when confronted with the evidence investigators had collected, the man said that his real name was in fact Walter Yovany-Gomez. STALLINGS: So in showing him actually his top 10 most wanted poster and being asked, is this you, he says yes. LUCAS: Since that night, Gomez has pleaded guilty to a racketeering conspiracy and admitted to Matute's murder. Last month, a federal judge in New Jersey sentenced him to 25 years in prison. Ryan Lucas, NPR News, Washington. AILSA CHANG, HOST:  There's a lot of debate and controversy over how law enforcement uses facial recognition technology. The FBI says it's an important investigative tool. Take the case of Walter Yovany-Gomez. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #1: One of America's most wanted criminals, a fugitive and an alleged member of the vicious MS-13 gang, now captured. CHANG: Captured in part because of facial recognition. NPR justice reporter Ryan Lucas explains how it all went down. RYAN LUCAS, BYLINE: Back in 2011, Walter Yovany-Gomez wanted to become a full member of the local branch of the MS-13 street gang in Plainfield, N. J. In May of that year, he got his chance. Gang leaders ordered him to kill a recruit named Julio Matute who had allegedly been associating with a rival gang. And so after a night of partying with Matute, Gomez and an associate beat him with a baseball bat, stabbed him with a screwdriver and slit his throat. Police closed in on Gomez in 2011, but he jumped out of a second-story window and escaped. For six years, his trail went cold. Then in April of 2017, the FBI made an announcement that put Gomez in the national crime spotlight. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #2: The violent gang member is the newest addition to the FBI's 10 most wanted fugitives list. An up to $100,000 reward is available for information leading directly to the arrest of Walter Yovany-Gomez. LUCAS: The move quickly paid off. A tipster came forward that July claiming to have information on Gomez. It fell to Special Agent Richard Stallings in the FBI's Washington field office to follow up. So Stallings and a gang task force colleague arranged a face-to-face meeting with the tipster. They wanted to vet the source and make sure, as Stallings puts it, they weren't running up a dead tree. They quickly determined that they were not. RICHARD STALLINGS: He truly was like, I know that guy. And he says, I don't know him by the name that's on the screen, Gomez. But he knew him on a different name. LUCAS: The tipster pointed the FBI to Facebook pages with photos of the man he believed was Gomez. Agents pulled the photos and sent them to the bureau's FACE Services Unit. There, they were run through an FBI database using facial recognition software in search of a match. About a week later, a match came back but for Jesus Lopez Centoreo. Centoreo had been picked up for marijuana possession in 2014 after jumping a Metro turnstile in Arlington, Va. His fingerprints and mug shot were taken, and then he was released. Centoreo had then failed to appear for his court date, so there was a warrant out for his arrest. STALLINGS: We were kind of baffled in how we got this name associated with our picture. But then the picture they had that they associated with in the arrest - like, that's our guy. LUCAS: Investigators still felt the tipster's information was solid, in part, Stallings says, because there were other physical identifiers like tattoos that made them think they were on the right track. They also had leads to work. Remember those Facebook pages the tipster provided? Agents also found several photos of the suspect with a female associate. Agents used facial recognition software to run the Facebook photos of her through a database of criminal mug shots. They came back with a match and an address. So the FBI and officers from the Fairfax County gang task force set up surveillance on her. One afternoon in August 2017, officers set up a stakeout outside her residence. STALLINGS: A couple hours later, I get this call back from their sergeant. We got our guy. And I was like, what? And he said, no, it's our guy. And, you know, the first thing in my mind, are you sure? Because we didn't know for sure he was here at any point. LUCAS: The agents had tailed the woman to a gym parking lot in Woodbridge, Va. , about 20 miles south of Washington, D. C. And who walks up to her car but a man who looked a lot like Gomez. Their surveillance team swooped in and arrested him, but there was a twist. The sergeant on the phone said the man gave his name as Jesus Lopez Centoreo, the same name that the FBI turned up in its facial recognition search. But when confronted with the evidence investigators had collected, the man said that his real name was in fact Walter Yovany-Gomez. STALLINGS: So in showing him actually his top 10 most wanted poster and being asked, is this you, he says yes. LUCAS: Since that night, Gomez has pleaded guilty to a racketeering conspiracy and admitted to Matute's murder. Last month, a federal judge in New Jersey sentenced him to 25 years in prison. Ryan Lucas, NPR News, Washington.", "section": "National Security", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-22-753524482": {"title": "Fighting Robocalls: Telecom Companies, States Reach Deal To Target Spoofing : NPR", "url": "https://www.npr.org/2019/08/22/753524482/phone-companies-ink-deal-with-all-50-states-and-d-c-to-combat-robocalls", "author": "No author found", "published_date": "2019-08-22", "content": "", "section": "Technology", "disclaimer": ""}, "2019-08-22-752765606": {"title": "Face Recognition Lets Palestinians Cross Israeli Checkposts Fast, But Raises Concerns : NPR", "url": "https://www.npr.org/2019/08/22/752765606/face-recognition-lets-palestinians-cross-israeli-checkposts-fast-but-raises-conc", "author": "No author found", "published_date": "2019-08-22", "content": "", "section": "World", "disclaimer": ""}, "2019-08-23-753662354": {"title": "Putin Orders 'A Symmetrical Response' To U.S. Defense Department's Missile Test : NPR", "url": "https://www.npr.org/2019/08/23/753662354/putin-to-russian-military-prepare-a-symmetrical-response-to-u-s-missile-test", "author": "No author found", "published_date": "2019-08-23", "content": "", "section": "National Security", "disclaimer": ""}, "2019-08-23-753626357": {"title": "210 YouTube Accounts Shut Down Over Disinformation in Hong Kong Protests : NPR", "url": "https://www.npr.org/2019/08/23/753626357/youtube-channels-suspended-for-coordinated-influence-campaign-against-hong-kong", "author": "No author found", "published_date": "2019-08-23", "content": "", "section": "Asia", "disclaimer": ""}, "2019-08-25-754122937": {"title": "Robots Compete In Disaster Test : NPR", "url": "https://www.npr.org/2019/08/25/754122937/robots-compete-in-disaster-test", "author": "No author found", "published_date": "2019-08-25", "content": "LEILA FADEL, HOST: Robotic engineers from all over the world gathered in Pennsylvania this week for a competition at an abandoned coal mine. It's called the Subterranean Challenge. And as Kathleen Davis reports from WESA in Pittsburgh, it tests robots on how well they can find objects in a disaster scenario. KATHLEEN DAVIS, BYLINE: Team Pluto is waiting anxiously outside the cavernous mouth of a research mine. Inside is one of their four-legged robots, which is searching for objects. No team members are allowed inside the mine, so all they can do is wait for the bot to send information back to them via Bluetooth. Finally. . . (CHEERING)DAVIS: . . . The robot, which looks a little like a dog, found a cellphone, earning a point for the team. Team Pluto is made up of students from the University of Pennsylvania, along with engineers from private robotics firms. It's one of 11 teams from all over the world participating in the Subterranean Challenge, held by the Department of Defense's research agency, DARPA. This circuit simulates a mine disaster scenario. The teams get points when the robots find objects like a backpack or a drill or a mannequin in mining gear named Rescue Randy. Tim Chung manages the challenge. He says the hope is that it will spur development of robots that can assist in war zones and disaster scenarios. Ideally, if a situation is dangerous, robots will be able to scope out what's happening and map out where things are. He says this would benefit EMTs, firefighters and the military. TIM CHUNG: They're really interested in knowing where things are, whether that's hazards or fires or survivors. They really need to know relatively precisely so they're not sent to erroneous locations. DAVIS: Future competitions in the challenge will be held in urban and cave environments so the robots will have to work in different situations. Some teams have their own ideal applications for the robots. Ali Agha is with Team CoSTAR, which has members from NASA's Jet Propulsion Laboratory. They're using a fleet of wheeled robots for the competition, which can navigate uneven terrain. ALI AGHA: One of the biggest questions for NASA is, is there life beyond our planet, right? And one of the best places to find lives are planetary caves. DAVIS: Another reason NASA is interested - if colonization of the moon and other planets is going to happen, people would likely live in these same caves. After their run, Team Pluto's lead, C. J. Taylor, said the team was feeling pretty good about how well their robots did. C J TAYLOR: We will continue to think and strategize and figure out what we want to do. There are places that we're doing experiments where the terrain is so treacherous. DAVIS: This is the first of four competitions in the Subterranean Challenge, which runs through 2021. For NPR News, I'm Kathleen Davis in Pittsburgh. LEILA FADEL, HOST:  Robotic engineers from all over the world gathered in Pennsylvania this week for a competition at an abandoned coal mine. It's called the Subterranean Challenge. And as Kathleen Davis reports from WESA in Pittsburgh, it tests robots on how well they can find objects in a disaster scenario. KATHLEEN DAVIS, BYLINE: Team Pluto is waiting anxiously outside the cavernous mouth of a research mine. Inside is one of their four-legged robots, which is searching for objects. No team members are allowed inside the mine, so all they can do is wait for the bot to send information back to them via Bluetooth. Finally. . . (CHEERING) DAVIS: . . . The robot, which looks a little like a dog, found a cellphone, earning a point for the team. Team Pluto is made up of students from the University of Pennsylvania, along with engineers from private robotics firms. It's one of 11 teams from all over the world participating in the Subterranean Challenge, held by the Department of Defense's research agency, DARPA. This circuit simulates a mine disaster scenario. The teams get points when the robots find objects like a backpack or a drill or a mannequin in mining gear named Rescue Randy. Tim Chung manages the challenge. He says the hope is that it will spur development of robots that can assist in war zones and disaster scenarios. Ideally, if a situation is dangerous, robots will be able to scope out what's happening and map out where things are. He says this would benefit EMTs, firefighters and the military. TIM CHUNG: They're really interested in knowing where things are, whether that's hazards or fires or survivors. They really need to know relatively precisely so they're not sent to erroneous locations. DAVIS: Future competitions in the challenge will be held in urban and cave environments so the robots will have to work in different situations. Some teams have their own ideal applications for the robots. Ali Agha is with Team CoSTAR, which has members from NASA's Jet Propulsion Laboratory. They're using a fleet of wheeled robots for the competition, which can navigate uneven terrain. ALI AGHA: One of the biggest questions for NASA is, is there life beyond our planet, right? And one of the best places to find lives are planetary caves. DAVIS: Another reason NASA is interested - if colonization of the moon and other planets is going to happen, people would likely live in these same caves. After their run, Team Pluto's lead, C. J. Taylor, said the team was feeling pretty good about how well their robots did. C J TAYLOR: We will continue to think and strategize and figure out what we want to do. There are places that we're doing experiments where the terrain is so treacherous. DAVIS: This is the first of four competitions in the Subterranean Challenge, which runs through 2021. For NPR News, I'm Kathleen Davis in Pittsburgh.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-26-754336716": {"title": "How to Avoid Distractions and Do Meaningful Work : NPR", "url": "https://www.npr.org/2019/08/26/754336716/you-2-0-deep-work", "author": "No author found", "published_date": "2019-08-26", "content": "SHANKAR VEDANTAM, HOST: From NPR, this is HIDDEN BRAIN. I'm Shankar Vedantam. For many people, this is what work sounds like nowadays. (SOUNDBITE OF NOTIFICATION BELLS AND VIBRATIONS)VEDANTAM: A constant thrum of notifications, tweets and messages. Every time we respond to an email or a text or Google a question that's just popped into our head, we pay a small price. In the moment, this price is imperceptible, but over time, it adds up. And we haven't quite come to terms with the cost of constant distraction. (SOUNDBITE OF ARCHIVED NPR BROADCAST)CAL NEWPORT: We treat it, I think, in this more general sense of, eh, I probably should be less distracted. And I think it's more urgent than people realize. VEDANTAM: This week on HIDDEN BRAIN, we continue our annual summer series, You 2. 0. . . (SOUNDBITE OF ARCHIVED NPR BROADCAST)FRANCESCA GINO: Authenticity is contagious. MING KUO: (Laughter) I have been dragged into this all the way kicking and screaming. VEDANTAM: . . . Ideas and advice about how you can respond to life\u2019s chaos. . . NEWPORT: Let me just do a just-check to my inbox. . . Just-check. . . Just-check. . . Just-check to my phone real quick. VEDANTAM: . . . With wisdom. (SOUNDBITE OF MUSIC)VEDANTAM: This episode, we explore ways to immerse ourselves in meaningful work. (SOUNDBITE OF MUSIC)VEDANTAM: Today, we look at the challenge of cultivating deep attention and what we gain by immersing ourselves in meaningful work. I spoke to someone who might seem like an unlikely advocate for technological restrain - a computer scientist. Cal Newport is a computer science professor at Georgetown University. He's deliberately tried to break away from the distractions of modern technology, and he's trying to get the rest of us to follow his lead. (SOUNDBITE OF ARCHIVED NPR BROADCAST)VEDANTAM: Cal is the author of \"Deep Work: Rules For Focused Success In A Distracted World. \" Cal, welcome to HIDDEN BRAIN. NEWPORT: Well, thanks for having me on. VEDANTAM: You talk in your book about several highly influential thinkers, people like the psychiatrist Carl Jung, the writers Mark Twain, J. K. Rowling. And you say they all have a set of habits that are quite striking in terms of how they're able to get great work done. NEWPORT: This was something I noticed was very common to influential thinkers, is that they all seem to have this drive to, on a regular basis, cut themselves off from their lives of busyness and communication and distraction and isolate themselves to think deeply. VEDANTAM: What do they do specifically? NEWPORT: Well, what you'll notice is that they often will have a location, a separate location they go to when they want to think deeply that's often cut off from the rest of their life. So Carl Jung would go out to the Bollingen Tower, a stone house without electricity or running water he built by the lakeside outside of a small village in the countryside beyond Zurich. And J. K. Rowling, when she was struggling to finish \"The Deathly Hallows,\" rented out this big suite at the Balmoral Hotel next to the big castle in downtown Edinburgh where she'd go and just think \"Harry Potter\"-style thoughts. Mark Twain had a cabin for a long period of his life he would go to on the property of their house that was so far from the house that his family had to blow a horn to try to catch his attention. . . VEDANTAM: (Laughter). NEWPORT: . . . And let him know that dinner was ready. They'd go somewhere physically isolated and different where they can, without distraction, think deeply. VEDANTAM: What does work look like for the rest of us? When you look at the average American worker, for example, are most of us doing this kind of deep, sustained work? NEWPORT: The type of deep work I talk about is almost nonexistent, as far as I can tell, in most knowledge work positions. Even when people think that they're single-tasking, they say, I've learned a lesson that I'm not supposed to multitask. I'm not supposed to be on the phone and do email while I write. I'm just working on one thing at a time. What they're still doing is every five or 10 minutes, a just-check. Let me just do a just-check to my inbox. Let me just do a just-check to my phone real quick and then back to my work. And it feels like single-tasking. And it feels like you're predominantly working on one thing. But even those very brief checks that switch your context even briefly can have this massive negative impact on your cognitive performance. It's the switch itself that hurts, not how long you actually switch. So I actually think even very conscientious knowledge workers, who think they're pretty good at focusing on one thing at a time, are actually still working far from the sort of high-performance, deep work ideal. VEDANTAM: What is the evidence that the switching causes harm to the quality of your thinking? NEWPORT: Well, we've seen this show up in different types of scientific studies and from different types of perspectives. I think one angle that makes it pretty clear is the work that professor Sophie Leroy has done on an effect called attention residue. This is actually something that's pretty easy to isolate in the laboratory. You essentially give a subject something cognitively demanding to do that you can measure, like trying to solve hard puzzles. And then at some point, you distract them briefly as the experimenter, have them look at something else, change their context very briefly. When they then turn back to the original cognitively demanding task, you see their performance drops, and it drops for a while. It takes a while for this attention residue to clear out. And this is essentially what we're doing to ourselves when we do that quick glance at the inbox or to the phone. VEDANTAM: Cal, I'm wondering, do you think most of us are aware of the cost of distraction in our lives? NEWPORT: People I think intuit that they're too distracted, and it's making them feel fragmented and exhausted and anxious. But we treat it, I think, in this more general sense of, eh, I probably should be less distracted. And I think it's more urgent than people realize, that if your brain is how you make a living, then you really have to worry about this cognitive fitness. I mean, how are you getting performance out of your brain? Are you taking care to get good performance out of your brain or not? And people would probably be surprised, the more they think about it, you know, how much they're leaving on the table by the way they're currently working right now. VEDANTAM: I understand that folks at Atlantic Media once tried to quantify the financial cost of email, the amount of time people spend reading and responding to email. NEWPORT: Yeah, it was the CTO of Atlantic Media at the time did this study, where they actually went through and calculated how many emails people were receiving, how long the average email was, what their average reading speed was, so he could contemplate or calculate how many hours were being spent collectively. Then he cross-tabulated that with their salary, which he could then calculate what's their effective hourly rate and figured out that basically they were spending about the price of a Learjet every year paying people to send and receive emails. VEDANTAM: (Laughter) Well, most of us, I think, when we think about email, we actually imagine that, you know, addressing, handling, responding to 25 emails, 30 emails, maybe a hundred emails every day, that's just part of our job. We actually think of it as actually being part of our productivity. You're making the argument that, potentially, this is a cost that we're paying that actually impedes our productivity. NEWPORT: No one's ever made a fortune by being really good at sending and receiving emails. I think right now, we're - the early stages of digital knowledge work, we've adopted this workflow that's very convenient and very simple, which is let's just give an email address to every person and let work unfold in this sort of ad hoc, ongoing conversation that happens with this - messages going back and forth and back and forth. And it's very easy, and it's very convenient, but it's also drastically reducing the human brains that are the main resource of these organizations. So my way of thinking about this is that we've built up a culture of convenience and simplicity in knowledge work at the cost of effectiveness and true productivity. And this is something that we need to change. VEDANTAM: Does it matter that most of us are not trying to win a Nobel Prize or a, you know, a Pulitzer Prize, that we're not necessarily geniuses? Does it actually matter for most of us that we don't regularly put ourselves in a state of deep work? NEWPORT: Well, this is a big shift that I think has happened in our economy because it's an increasing portion of our economy that are essentially making a living by using their brain to process information and produce new information. And even if you're not trying to write great literature or solve a great theorem, if you're using your brain primarily to produce value, be it writing marketing copy or putting together a new plan for your business startup, these type of things matter. The human brain has become one of the main capital resources in our economy. It's what, in the knowledge economy, we spend most of our money on, is supporting human brains to process things and produce value. So we should care. I think the ability to do deep work would be relevant to the professional success of almost everyone in the knowledge work field, which is a huge part of our economy. VEDANTAM: I'd like to run a little thought experiment. Imagine we're following a doctor as she's making the rounds of a hospital. And she's looking at many patients. And presumably, the patients all present with different problems and complications and so forth. And I think what we would expect is for this doctor to very quickly flit from one subject, one topic, one patient to the next, that if the doctor were to say, you know, I can only do my best work if I can focus on one patient, deeply understand that case, spend a lot of time with it, yes, that might be true. But it's going to come at a cost, which is all the other patients that the doctor is not going to see. What are the costs of deep work? NEWPORT: Well, where I'd want to get with the doctor is just the ability, even if you're relatively briefly staying with each patient, to actually be able to stay just with that patient. So a case study I uncovered actually after the book came out was of two different groups at the same elite-level residency. One group had a culture of email. So hey, I need something. Here's a question. What about this patient? And they're expected to constantly be available by email. The other group consolidated that type of administrative or logistical conversations to set meetings. And what the doctor from that hospital told me is that they had a real hard time keeping people in that first group, where in the second group, people were much happier. So deep work doesn't necessarily mean I can sit, you know, half a day and just think about this one patient. But just the ability to walk into a room and just think about that patient and not have to see 16 emails as you walk into the next room and have that eating away at your attention, that can really make a big difference. VEDANTAM: You said that the people who were engaged in deep work ended up being happier. So it's not just a question of being more productive, but you're making the case that deep work produces a kind of intrinsic reward that doesn't come from being distracted. NEWPORT: It seems to. And in fact, this caught me off guard when I was researching my book. I ended up adding a chapter to the book, that was not in the original proposal, that was all about these findings I kept coming across and these stories I kept coming across about deep living also just being good living. People who spend a larger proportion of their professional time concentrating intensely on a single high-skill or high-craft target tend to enjoy their work a lot more. And there's a lot of different factors about why that might be true. But I, you know, ended by saying a deep life is a good life, and that's something I really believe in. It can take a knowledge work career and make it much more satisfying than being in a persistent state of putting out fires and busy distraction. VEDANTAM: Can you cite any professions where deep work is probably not called for and might even be a problem? NEWPORT: Sure. There's plenty of examples I think where deep work is probably not that relevant. A couple of the common examples I give is actually I think being a CEO of a large company. You're probably going to better serve your company or your stockholders by being a decision engine for other people who are doing deep work, someone who people can come to. OK, what about this? What should we do here? You can be a consistent source of the vision and push these decisions in a consistent way. Another example is let's say you're in what they would call here in D. C. government relations, where really most of what you do is contacts and connections and connecting the right people to the right other people and keeping up with what's going on in people's lives. That's another example of a place where long, solitary concentration is not going to make a difference. I think there's plenty of jobs, in other words, in which deep work doesn't make a difference. But I've also found, in my experience, that the number of jobs for which this is true is smaller than people expect. VEDANTAM: It seems to me there are connections here with ideas related to mindfulness or ideas related to flow, that you should be in the moment, focused on what you are doing. It seems to me that those ideas are intimately connected with deep work. NEWPORT: They are connected. So deep work can induce flow states, which is one of the reasons why people find a career pushed more towards deep work is more satisfying. It's not entirely synonymous with flow. We know there's other types of states that also count as deep work that would not fall under most definitions of flow. So for example, being in a state of deliberate practice where you're systematically pushing your skills past where you're comfortable so that you can improve, that's different than a flow state. It doesn't feel pleasurable. You don't lose yourself in the time. When you're practicing like that, you feel every single second because it's very difficult. But that also falls under the umbrella of deep work. VEDANTAM: And what about mindfulness, the idea that we should just be immersed in what we're doing, paying attention to what's going on in the moment? NEWPORT: There are deep connections to mindfulness. And one of the more important connections is that we know from the study and practice of mindfulness, such as mindfulness meditation, that getting better at that type of presence is something that requires practice and training. And we see this exactly happening with deep work in a professional setting. It's something that you train and get better at, just like you can get better at certain types of meditation, that it's something you have to work at systematically. It's a skill to be practiced, not a habit that you already know how to do and just try to make more time for. VEDANTAM: When we come back, we'll talk about how you can retrain your mind to focus, to sit with a single idea for a long period of time. And we'll talk about whether creating a deep work culture for some people means that others will inevitably have to pick up the slack. (SOUNDBITE OF MUSIC)VEDANTAM: Stay with us. (SOUNDBITE OF MUSIC)VEDANTAM: I'm speaking to Cal Newport, a Georgetown University computer science professor who's the author of the book \"Deep Work. \" It's about how we can cultivate the ability to focus on work free of all distraction. Cal leads an enormously disciplined life with a lot of rules and rituals. I asked him to explain how he structures his day to allow plenty of time for focused work. (SOUNDBITE OF ARCHIVED NPR BROADCAST)NEWPORT: There's a few things I do. One is I've never had a social media account, and that's on purpose. It's not that I think I'm better than social media. But to quote George Packer's essay on this, it's because I'm afraid I'd let my kids go hungry if I exposed myself to that. So that's one thing I do. Two - I'm very organized with my time. I work during very set hours during the day, and I plan out the day like a chess player moving the pieces around. This is what I'm going to work on when. I don't let my mood dictate how my day unfolds. And then three, I've made myself very comfortable with annoying people. I'm bad at email. I have just set the expectations that I'm just not available a lot. I'm not someone that you can expect a quick answer from. And that also causes some trouble, of course. But all this adds up to allowing me to regularly have long portions of many of my days focused on deeper thinking. VEDANTAM: I understand you actually keep a tally of how much deep work you've done, how many hours you've spent being uninterrupted. And you actually have targets that you must meet at the end of the day or the end of the week. NEWPORT: Yeah, that's right. This is something until recently I was doing. I was tallying, you know, how many deep work hours, so I had to confront that. I had to confront the reality. So if I was really avoiding deep work, I would see it. I've since added a new habit to my arsenal here, where I now block out my deep work on my calendar up to four weeks in advance so I have that time protected so far out in the future that I can be sure it'll stay protected. So now I have a record on my calendar of exactly what deep work I'm doing. VEDANTAM: Do people call you obsessive or basically say, you know, you're wedded too much to your calendar? NEWPORT: People do. People also seem, which surprises me, worried that that would somehow diminish creativity, that somehow having structure in your schedule means you're not going to be able to do unstructured thinking. Well, what I've found is actually quite the opposite. Being able to protect my time and to have long periods of undistracted time allows me to be a lot more creative. So I often push back a bit on that particular critique, that if I was just sort of ad hoc checking emails and social media and in a state of semi-distraction all day, I would probably be much less creative than my more structured approach. VEDANTAM: I understand you have a fairly structured approach to shutting down at the end of the day, of making sure that the tasks that remain unfinished don't bleed into your evening and your family life. NEWPORT: I have an actual ritual I do at the end of each workday where, pretty systematically, I'll look at my weekly plan. I'll look at my task list. I'll look at my calendar, make sure that nothing is left hanging, and then I'll do a little shutdown mantra. You'll say an actual phrase that means I'm now done work for the day. VEDANTAM: What's the phrase that you tell yourself at the end of the day? NEWPORT: I used to be embarrassed to admit the phrase was schedule shutdown complete, but I now have this small but strong fan group that use that exact same phrase proudly. So now I'm willing to admit it's schedule shutdown complete. VEDANTAM: Do you say this with others around? NEWPORT: No (laughter). It doesn't really matter what the phrase is. You know, I invented that phrase when I was a graduate student working on my dissertation and was really having a hard time with coming home from the office and having all these concerns. Hey, what if this proof never fixes? Or what if this proof breaks? What if I'm - my dissertation falls apart? And I needed something to allow me to definitively shut down. And so I was younger then. I came up with this phrase. But now it became habit, so I stuck with it. VEDANTAM: Cal, what would you say to people who would say, you're asking us to turn into computers, you're asking us to behave like robots? NEWPORT: Well, see, I would argue that that's what people are doing right now. We've turned ourselves into sort of human network routers. We just sit here and process messages and sort through task lists and have this sense of busyness that treats our mind like a digital computer processor, something that you just feed instructions to and it executes one after another. I think what I'm doing is actually way more human - this idea that our brain is not like a computer. It's not like any other machine we know. It's something that you have a personal connection to. And it's something that you really have to take care of, something that you have to coax high performance out of. So to have a structured day, for example, to protect your mind from distraction, I actually think makes you more human and less robotic than what most people do, which is to sit there like a human network router and just sort of process messages and tasks all day like a blind computer processor. VEDANTAM: There seems to be a paradox here because I think what I'm hearing you say is that scheduling yourself, or even overscheduling yourself, is the way to actually gain control over your life. Whereas people would sort of say, if you're actually scheduling every second and sort of deciding four weeks ahead of time when you're going to stop work on a certain Wednesday, you've actually turned yourself into a robot. NEWPORT: It's a paradox that shows up a lot. It confuses people. But I think you're right to point it out - is that if you study, especially really creative people, professional creatives, they are surprisingly structured in how they approach their day. I took a quote, at one point, from David Brooks, the columnist, and I might be paraphrasing here. But basically, he pointed out this observation that great creative thinkers approach their time like accountants, that this is this great disconnect, is that they're very structured and systematic about their time and produce the most unstructured, brilliant, creative insights. So it's a key paradox to point out because I really want to emphasize it. Adding structure and control to your time really can be the key to getting the biggest insights and most interesting work produced. VEDANTAM: I'm wondering if part of the tension comes about because we actually think of inspiration as being the thing that strikes us unexpectedly. And I think the case that you're making is that inspiration actually can be scheduled to arrive on command. NEWPORT: Well, as, you know, Chuck Close said - the artist - inspiration is for amateurs. I think we overfocus on the inspiration piece. If you're systematically pushing yourself and your knowledge and your craft, you will have inspiration. It'll happen in the shower. It'll happen while you walk to work. What's important is, you know, setting yourself up to have that inspiration and then giving yourself the time and structure you need to act on it, to actually produce something of value out of it. So I downplay the importance of inspiration, and I emphasize the importance of creating a life where inspiration is possible, and you're well-suited to act on it. VEDANTAM: I want to ask you a couple of questions that push back against this idea from a practical standpoint. What if people are in workplaces where they have managers and bosses who aren't enlightened enough to say, yes, you should spend several hours engaged in deep work? People can't always choose for themselves what kind of work they pursue. NEWPORT: Something that has seemed to be effective is, in that type of situation, having a conversation with whoever your boss is, whoever supervises you, and say, I want to talk about deep work. Here's what deep work is. And I want to talk about, you know, nondeep work or shallow work, and here's what that is. And both are important to my job. And I want to have a conversation and decide, what should my ratio be? That is, in a typical work week, what ratio of my hours should be deep work versus shallow work and actually nailing down a number, an aspirational target, that everyone agrees, yeah, this is right for your position in our company. It's not saying hey, boss, stop emailing me so much; you annoy me. VEDANTAM: (Laughter). NEWPORT: It's instead saying, hey, let's try to optimize myself. So what should I be going for here? Let me get your feedback on this. And people are reporting back to me, you know, tales of drastic changes to work cultures that they thought, there's no way. There's no way I'm going to get away with this, that I'm supposed to be on Slack all the time, or I'm supposed to be answering my emails all the time. They have this conversation. In the next week, they're spending 50% of their hours undistracted. So I've been pushing that particular managerial hack as a good, positive way forward to trying to fix some of these issues. VEDANTAM: I'm wondering if some people might say your advice is really advice for people who, in some ways, are at the top of their food chains. So if you have an author who basically is able to say, I'm going to disconnect from the world for 18 months - I'm just going to focus on writing this book - you know, someone else is probably picking up after this person in all kinds of different ways. If Cal Newport says, you know, I'm going to close the door in my office, I'm not going to answer my phone, I'm not going to check my email, but someone needs to get in touch with you in an emergency, that person is probably going to reach an assistant of yours. And that assistant doesn't have the same luxury of deep work as you do because he or she needs to be available to hear what the emergency is or to hear what the request is. Does having a group of people who are engaged in deep work necessarily mean there must be essentially a second tier of workers who are engaged in shallow work to allow the deep thinkers to do their deep thinking? NEWPORT: It doesn't require that, but it usually requires some type of reconfiguration of communication channels and expectations. So when I work, for example, with people maybe in a small consultancy that is client-facing, where they're used to this idea that clients need to reach us, issues pop up, what's important there is just to actually change the communication expectations. That maybe instead of having a client just have individual people's email addresses, the company sets up an email address for that client. And the company has set up some agreement on their end that there'll always be someone monitoring that, and here's the expectation of when you can get a response. Or maybe setting up a - it's sometimes called the bat phone or emergency phone idea, where you say, OK, here's a number you can call me at if there's an emergency when I'm in one of these deep work sessions. People set these up and say they get called maybe once a year. So I don't think you need actual extra people involved to make space for deep work. But I do think it almost always requires some effort, some sort of reconfiguration of people's expectations on how and when they can reach you. VEDANTAM: So I'm going to ask you a question now that's part serious and part teasing. You and I were scheduled to talk last week, and you didn't get the appointment down in your calendar, and I was sitting here waiting for you. And, of course, this kind of thing happens all the time. But in your case, I couldn't help but wonder, did he miss this because he actually hadn't spent the time doing the shallow work to get this in his calendar? And is it possible that when we engage in deep work, we are essentially, you know, getting the benefit of all of that deep work - we're getting the deep thinking, we're getting the accomplishments - but some of the cost is borne by other people, and they might actually be the people who are getting mad at you when they can't reach you? NEWPORT: Well, it's a good point. And I think that's actually - was what happened. Because I spend a lot of time working away from my computer, these type of problems happen to me more often. In this case - and, you know, I'm embarrassed it happened, but my vague memory was I saw this communication on my phone because I had to be on there to send something to someone, but I was far away from a computer. And so I wasn't able to easily add it to a calendar. And I was like, OK, I'll remember to do this when I get back to my office next, and I forgot. And it did cause problems. And I - so I'm embarrassed about it. And that type of thing does happen. And I think this hits on a big point, which is deep work, or a professional life focused on deep work, is less convenient for most people involved. But on the other hand, I want to put out there this notion that that might not be so bad, that it's possible that in this age of digital communication, we are focusing too much on convenience over effectiveness. VEDANTAM: I think, in some ways, what you're saying is also the tension between the short term and the long term. If I don't respond to a colleague's request or a manager's, you know, instructions to do something right away, it's irritating for the person at the other end of the line. And so I think most of us actually conform to the social norm of saying, yes, I'm just going to be responsive. I'm going to be available. I'm going to answer the question as soon as it's asked. The point that you're making, though, is that there might be long-term goals, deeper institutional goals, that are essentially - we're not thinking about. And, of course, when those goals are not met because they're not articulated, no one notices their absence. So people will notice it if you don't show up at an interview. People are not going to notice it if you don't write that bestseller or the next great idea. And so there's really a cultural bias in favor of the trivial over protecting what actually is most important. NEWPORT: I really agree with that point. And I would add to it that I think a big part of it is lack of metrics. So if we look at two parallel case studies, two different industries, let's look at the Industrial Revolution and the rise of mass industrial production. This was a world where the metrics for productivity were very clear. How many cars per hour is our factory producing? And what we saw in that world where bottom-line value was very easy to measure is that, very quickly, the structure of work moved away from what was convenient for the workers and towards what produced more value. It moved away from the old system in factories, where you had people work in teams at one spot in the floor to assemble the car, towards things like the assembly line, which are incredibly inconvenient. It's very hard to manage an assembly line. It's very hard to get it right. It causes lots of issues. It's annoying, but it produces a lot more value. You move to digital knowledge work. We don't have those metrics. It's much harder to measure, OK, what's the cost to our bottom line if you're more distracted or less distracted? And so my conjecture is that without those metrics, we are going to fall back on these interpersonal or cultural biases. We're wired to be social. We don't want to upset someone. These type of biases take over because it's much harder to measure, in this new world, the impact of different behaviors. VEDANTAM: I'm wondering if there's also a psychological explanation for the phenomenon you're describing. You know, I took a vacation a couple of weeks ago. And for the first time in a long time, I actually decided to unplug. So I didn't have Internet access. I wasn't checking my email. I literally was cut off from things going on at work. And when I got back, there were a number of things that had happened in my absence, some of which I wish I'd had the chance to weigh in on. But when I looked at the aggregate, the overall conclusion I got was really that the world did just fine in my absence. Things went fine. I actually wasn't as indispensable as I thought I was. I'm wondering if that might be a psychological driver in people being unwilling to actually cut themselves off - because not only might they discover that they are more productive, but they might also discover the world does just fine, thank you very much, without you. NEWPORT: You know, I think that's one of three big psychological drivers that have led us to this world we're in now with the sort of constant connectivity business. So that's certainly one, I think - this notion of - we get a sense a meaning and usefulness out of constantly being involved in interaction. I think the other two psychological drivers - one is just we're wired to be tribal. And it's very difficult for us, psychologically, to know there's an email waiting that we're not answering. And even if we know for a fact that the person who sent that message does not need a fast response, it still feels like we're at the tribal fire. And there's a tribe member standing there tapping you on the shoulder, and you're ignoring them. We just have a very hard time with that. And I think the third driver is knowledge work is much less structured. And so how do you prove to your organization or to your boss that you're valuable? And busyness, as a proxy for productivity, is something that a lot of people have defaulted to. Well, at the very least, if you see I'm sending lots of messages, you know I'm working. And so I think those three different factors are all intertwining to get us to this place where we find ourselves just constantly sending messages as opposed to thinking hard thoughts or producing new things. VEDANTAM: Cal Newport is a computer science professor at Georgetown University. He's the author of \"Deep Work: Rules For Focused Success In A Distracted World. \" Cal, thank you for joining me today on HIDDEN BRAIN. NEWPORT: Well, thank you. (SOUNDBITE OF MUSIC)VEDANTAM: This episode of HIDDEN BRAIN was produced by Rhaina Cohen and edited by Tara Boyle. Our team includes Jenny Schmidt, Laura Kwerel, Parth Shah and Thomas Lu. Our unsung hero this week is Stacey Foxwell. She's NPR's vice president of operations, which means she handles everything from where people sit to hiring the next generation of NPR journalists. Stacey keeps NPR running smoothly, and she does so calmly, cheerfully and with a remarkable sense of humor. For more HIDDEN BRAIN, you can follow us on Facebook and Twitter. If you liked this episode, please be sure to share it with one friend. We're always looking for new people to discover HIDDEN BRAIN. Next week, we conclude our You 2. 0 series with an episode about decision-making. (SOUNDBITE OF ARCHIVED NPR BROADCAST)DAN GILBERT: I realized that had you asked me a year earlier how I would be fairing, the answer would've been, oh, my gosh, I'll be devastated. But I wasn't devastated. It wasn't a good year, but it was OK. VEDANTAM: I'm Shankar Vedantam, and this is NPR. (SOUNDBITE OF MACHINE SHUTTING DOWN)VEDANTAM: System shutdown complete. SHANKAR VEDANTAM, HOST:  From NPR, this is HIDDEN BRAIN. I'm Shankar Vedantam. For many people, this is what work sounds like nowadays. (SOUNDBITE OF NOTIFICATION BELLS AND VIBRATIONS) VEDANTAM: A constant thrum of notifications, tweets and messages. Every time we respond to an email or a text or Google a question that's just popped into our head, we pay a small price. In the moment, this price is imperceptible, but over time, it adds up. And we haven't quite come to terms with the cost of constant distraction. (SOUNDBITE OF ARCHIVED NPR BROADCAST) CAL NEWPORT: We treat it, I think, in this more general sense of, eh, I probably should be less distracted. And I think it's more urgent than people realize. VEDANTAM: This week on HIDDEN BRAIN, we continue our annual summer series, You 2. 0. . . (SOUNDBITE OF ARCHIVED NPR BROADCAST) FRANCESCA GINO: Authenticity is contagious. MING KUO: (Laughter) I have been dragged into this all the way kicking and screaming. VEDANTAM: . . . Ideas and advice about how you can respond to life\u2019s chaos. . . NEWPORT: Let me just do a just-check to my inbox. . . Just-check. . . Just-check. . . Just-check to my phone real quick. VEDANTAM: . . . With wisdom. (SOUNDBITE OF MUSIC) VEDANTAM: This episode, we explore ways to immerse ourselves in meaningful work. (SOUNDBITE OF MUSIC) VEDANTAM: Today, we look at the challenge of cultivating deep attention and what we gain by immersing ourselves in meaningful work. I spoke to someone who might seem like an unlikely advocate for technological restrain - a computer scientist. Cal Newport is a computer science professor at Georgetown University. He's deliberately tried to break away from the distractions of modern technology, and he's trying to get the rest of us to follow his lead. (SOUNDBITE OF ARCHIVED NPR BROADCAST) VEDANTAM: Cal is the author of \"Deep Work: Rules For Focused Success In A Distracted World. \" Cal, welcome to HIDDEN BRAIN. NEWPORT: Well, thanks for having me on. VEDANTAM: You talk in your book about several highly influential thinkers, people like the psychiatrist Carl Jung, the writers Mark Twain, J. K. Rowling. And you say they all have a set of habits that are quite striking in terms of how they're able to get great work done. NEWPORT: This was something I noticed was very common to influential thinkers, is that they all seem to have this drive to, on a regular basis, cut themselves off from their lives of busyness and communication and distraction and isolate themselves to think deeply. VEDANTAM: What do they do specifically? NEWPORT: Well, what you'll notice is that they often will have a location, a separate location they go to when they want to think deeply that's often cut off from the rest of their life. So Carl Jung would go out to the Bollingen Tower, a stone house without electricity or running water he built by the lakeside outside of a small village in the countryside beyond Zurich. And J. K. Rowling, when she was struggling to finish \"The Deathly Hallows,\" rented out this big suite at the Balmoral Hotel next to the big castle in downtown Edinburgh where she'd go and just think \"Harry Potter\"-style thoughts. Mark Twain had a cabin for a long period of his life he would go to on the property of their house that was so far from the house that his family had to blow a horn to try to catch his attention. . . VEDANTAM: (Laughter). NEWPORT: . . . And let him know that dinner was ready. They'd go somewhere physically isolated and different where they can, without distraction, think deeply. VEDANTAM: What does work look like for the rest of us? When you look at the average American worker, for example, are most of us doing this kind of deep, sustained work? NEWPORT: The type of deep work I talk about is almost nonexistent, as far as I can tell, in most knowledge work positions. Even when people think that they're single-tasking, they say, I've learned a lesson that I'm not supposed to multitask. I'm not supposed to be on the phone and do email while I write. I'm just working on one thing at a time. What they're still doing is every five or 10 minutes, a just-check. Let me just do a just-check to my inbox. Let me just do a just-check to my phone real quick and then back to my work. And it feels like single-tasking. And it feels like you're predominantly working on one thing. But even those very brief checks that switch your context even briefly can have this massive negative impact on your cognitive performance. It's the switch itself that hurts, not how long you actually switch. So I actually think even very conscientious knowledge workers, who think they're pretty good at focusing on one thing at a time, are actually still working far from the sort of high-performance, deep work ideal. VEDANTAM: What is the evidence that the switching causes harm to the quality of your thinking? NEWPORT: Well, we've seen this show up in different types of scientific studies and from different types of perspectives. I think one angle that makes it pretty clear is the work that professor Sophie Leroy has done on an effect called attention residue. This is actually something that's pretty easy to isolate in the laboratory. You essentially give a subject something cognitively demanding to do that you can measure, like trying to solve hard puzzles. And then at some point, you distract them briefly as the experimenter, have them look at something else, change their context very briefly. When they then turn back to the original cognitively demanding task, you see their performance drops, and it drops for a while. It takes a while for this attention residue to clear out. And this is essentially what we're doing to ourselves when we do that quick glance at the inbox or to the phone. VEDANTAM: Cal, I'm wondering, do you think most of us are aware of the cost of distraction in our lives? NEWPORT: People I think intuit that they're too distracted, and it's making them feel fragmented and exhausted and anxious. But we treat it, I think, in this more general sense of, eh, I probably should be less distracted. And I think it's more urgent than people realize, that if your brain is how you make a living, then you really have to worry about this cognitive fitness. I mean, how are you getting performance out of your brain? Are you taking care to get good performance out of your brain or not? And people would probably be surprised, the more they think about it, you know, how much they're leaving on the table by the way they're currently working right now. VEDANTAM: I understand that folks at Atlantic Media once tried to quantify the financial cost of email, the amount of time people spend reading and responding to email. NEWPORT: Yeah, it was the CTO of Atlantic Media at the time did this study, where they actually went through and calculated how many emails people were receiving, how long the average email was, what their average reading speed was, so he could contemplate or calculate how many hours were being spent collectively. Then he cross-tabulated that with their salary, which he could then calculate what's their effective hourly rate and figured out that basically they were spending about the price of a Learjet every year paying people to send and receive emails. VEDANTAM: (Laughter) Well, most of us, I think, when we think about email, we actually imagine that, you know, addressing, handling, responding to 25 emails, 30 emails, maybe a hundred emails every day, that's just part of our job. We actually think of it as actually being part of our productivity. You're making the argument that, potentially, this is a cost that we're paying that actually impedes our productivity. NEWPORT: No one's ever made a fortune by being really good at sending and receiving emails. I think right now, we're - the early stages of digital knowledge work, we've adopted this workflow that's very convenient and very simple, which is let's just give an email address to every person and let work unfold in this sort of ad hoc, ongoing conversation that happens with this - messages going back and forth and back and forth. And it's very easy, and it's very convenient, but it's also drastically reducing the human brains that are the main resource of these organizations. So my way of thinking about this is that we've built up a culture of convenience and simplicity in knowledge work at the cost of effectiveness and true productivity. And this is something that we need to change. VEDANTAM: Does it matter that most of us are not trying to win a Nobel Prize or a, you know, a Pulitzer Prize, that we're not necessarily geniuses? Does it actually matter for most of us that we don't regularly put ourselves in a state of deep work? NEWPORT: Well, this is a big shift that I think has happened in our economy because it's an increasing portion of our economy that are essentially making a living by using their brain to process information and produce new information. And even if you're not trying to write great literature or solve a great theorem, if you're using your brain primarily to produce value, be it writing marketing copy or putting together a new plan for your business startup, these type of things matter. The human brain has become one of the main capital resources in our economy. It's what, in the knowledge economy, we spend most of our money on, is supporting human brains to process things and produce value. So we should care. I think the ability to do deep work would be relevant to the professional success of almost everyone in the knowledge work field, which is a huge part of our economy. VEDANTAM: I'd like to run a little thought experiment. Imagine we're following a doctor as she's making the rounds of a hospital. And she's looking at many patients. And presumably, the patients all present with different problems and complications and so forth. And I think what we would expect is for this doctor to very quickly flit from one subject, one topic, one patient to the next, that if the doctor were to say, you know, I can only do my best work if I can focus on one patient, deeply understand that case, spend a lot of time with it, yes, that might be true. But it's going to come at a cost, which is all the other patients that the doctor is not going to see. What are the costs of deep work? NEWPORT: Well, where I'd want to get with the doctor is just the ability, even if you're relatively briefly staying with each patient, to actually be able to stay just with that patient. So a case study I uncovered actually after the book came out was of two different groups at the same elite-level residency. One group had a culture of email. So hey, I need something. Here's a question. What about this patient? And they're expected to constantly be available by email. The other group consolidated that type of administrative or logistical conversations to set meetings. And what the doctor from that hospital told me is that they had a real hard time keeping people in that first group, where in the second group, people were much happier. So deep work doesn't necessarily mean I can sit, you know, half a day and just think about this one patient. But just the ability to walk into a room and just think about that patient and not have to see 16 emails as you walk into the next room and have that eating away at your attention, that can really make a big difference. VEDANTAM: You said that the people who were engaged in deep work ended up being happier. So it's not just a question of being more productive, but you're making the case that deep work produces a kind of intrinsic reward that doesn't come from being distracted. NEWPORT: It seems to. And in fact, this caught me off guard when I was researching my book. I ended up adding a chapter to the book, that was not in the original proposal, that was all about these findings I kept coming across and these stories I kept coming across about deep living also just being good living. People who spend a larger proportion of their professional time concentrating intensely on a single high-skill or high-craft target tend to enjoy their work a lot more. And there's a lot of different factors about why that might be true. But I, you know, ended by saying a deep life is a good life, and that's something I really believe in. It can take a knowledge work career and make it much more satisfying than being in a persistent state of putting out fires and busy distraction. VEDANTAM: Can you cite any professions where deep work is probably not called for and might even be a problem? NEWPORT: Sure. There's plenty of examples I think where deep work is probably not that relevant. A couple of the common examples I give is actually I think being a CEO of a large company. You're probably going to better serve your company or your stockholders by being a decision engine for other people who are doing deep work, someone who people can come to. OK, what about this? What should we do here? You can be a consistent source of the vision and push these decisions in a consistent way. Another example is let's say you're in what they would call here in D. C. government relations, where really most of what you do is contacts and connections and connecting the right people to the right other people and keeping up with what's going on in people's lives. That's another example of a place where long, solitary concentration is not going to make a difference. I think there's plenty of jobs, in other words, in which deep work doesn't make a difference. But I've also found, in my experience, that the number of jobs for which this is true is smaller than people expect. VEDANTAM: It seems to me there are connections here with ideas related to mindfulness or ideas related to flow, that you should be in the moment, focused on what you are doing. It seems to me that those ideas are intimately connected with deep work. NEWPORT: They are connected. So deep work can induce flow states, which is one of the reasons why people find a career pushed more towards deep work is more satisfying. It's not entirely synonymous with flow. We know there's other types of states that also count as deep work that would not fall under most definitions of flow. So for example, being in a state of deliberate practice where you're systematically pushing your skills past where you're comfortable so that you can improve, that's different than a flow state. It doesn't feel pleasurable. You don't lose yourself in the time. When you're practicing like that, you feel every single second because it's very difficult. But that also falls under the umbrella of deep work. VEDANTAM: And what about mindfulness, the idea that we should just be immersed in what we're doing, paying attention to what's going on in the moment? NEWPORT: There are deep connections to mindfulness. And one of the more important connections is that we know from the study and practice of mindfulness, such as mindfulness meditation, that getting better at that type of presence is something that requires practice and training. And we see this exactly happening with deep work in a professional setting. It's something that you train and get better at, just like you can get better at certain types of meditation, that it's something you have to work at systematically. It's a skill to be practiced, not a habit that you already know how to do and just try to make more time for. VEDANTAM: When we come back, we'll talk about how you can retrain your mind to focus, to sit with a single idea for a long period of time. And we'll talk about whether creating a deep work culture for some people means that others will inevitably have to pick up the slack. (SOUNDBITE OF MUSIC) VEDANTAM: Stay with us. (SOUNDBITE OF MUSIC) VEDANTAM: I'm speaking to Cal Newport, a Georgetown University computer science professor who's the author of the book \"Deep Work. \" It's about how we can cultivate the ability to focus on work free of all distraction. Cal leads an enormously disciplined life with a lot of rules and rituals. I asked him to explain how he structures his day to allow plenty of time for focused work. (SOUNDBITE OF ARCHIVED NPR BROADCAST) NEWPORT: There's a few things I do. One is I've never had a social media account, and that's on purpose. It's not that I think I'm better than social media. But to quote George Packer's essay on this, it's because I'm afraid I'd let my kids go hungry if I exposed myself to that. So that's one thing I do. Two - I'm very organized with my time. I work during very set hours during the day, and I plan out the day like a chess player moving the pieces around. This is what I'm going to work on when. I don't let my mood dictate how my day unfolds. And then three, I've made myself very comfortable with annoying people. I'm bad at email. I have just set the expectations that I'm just not available a lot. I'm not someone that you can expect a quick answer from. And that also causes some trouble, of course. But all this adds up to allowing me to regularly have long portions of many of my days focused on deeper thinking. VEDANTAM: I understand you actually keep a tally of how much deep work you've done, how many hours you've spent being uninterrupted. And you actually have targets that you must meet at the end of the day or the end of the week. NEWPORT: Yeah, that's right. This is something until recently I was doing. I was tallying, you know, how many deep work hours, so I had to confront that. I had to confront the reality. So if I was really avoiding deep work, I would see it. I've since added a new habit to my arsenal here, where I now block out my deep work on my calendar up to four weeks in advance so I have that time protected so far out in the future that I can be sure it'll stay protected. So now I have a record on my calendar of exactly what deep work I'm doing. VEDANTAM: Do people call you obsessive or basically say, you know, you're wedded too much to your calendar? NEWPORT: People do. People also seem, which surprises me, worried that that would somehow diminish creativity, that somehow having structure in your schedule means you're not going to be able to do unstructured thinking. Well, what I've found is actually quite the opposite. Being able to protect my time and to have long periods of undistracted time allows me to be a lot more creative. So I often push back a bit on that particular critique, that if I was just sort of ad hoc checking emails and social media and in a state of semi-distraction all day, I would probably be much less creative than my more structured approach. VEDANTAM: I understand you have a fairly structured approach to shutting down at the end of the day, of making sure that the tasks that remain unfinished don't bleed into your evening and your family life. NEWPORT: I have an actual ritual I do at the end of each workday where, pretty systematically, I'll look at my weekly plan. I'll look at my task list. I'll look at my calendar, make sure that nothing is left hanging, and then I'll do a little shutdown mantra. You'll say an actual phrase that means I'm now done work for the day. VEDANTAM: What's the phrase that you tell yourself at the end of the day? NEWPORT: I used to be embarrassed to admit the phrase was schedule shutdown complete, but I now have this small but strong fan group that use that exact same phrase proudly. So now I'm willing to admit it's schedule shutdown complete. VEDANTAM: Do you say this with others around? NEWPORT: No (laughter). It doesn't really matter what the phrase is. You know, I invented that phrase when I was a graduate student working on my dissertation and was really having a hard time with coming home from the office and having all these concerns. Hey, what if this proof never fixes? Or what if this proof breaks? What if I'm - my dissertation falls apart? And I needed something to allow me to definitively shut down. And so I was younger then. I came up with this phrase. But now it became habit, so I stuck with it. VEDANTAM: Cal, what would you say to people who would say, you're asking us to turn into computers, you're asking us to behave like robots? NEWPORT: Well, see, I would argue that that's what people are doing right now. We've turned ourselves into sort of human network routers. We just sit here and process messages and sort through task lists and have this sense of busyness that treats our mind like a digital computer processor, something that you just feed instructions to and it executes one after another. I think what I'm doing is actually way more human - this idea that our brain is not like a computer. It's not like any other machine we know. It's something that you have a personal connection to. And it's something that you really have to take care of, something that you have to coax high performance out of. So to have a structured day, for example, to protect your mind from distraction, I actually think makes you more human and less robotic than what most people do, which is to sit there like a human network router and just sort of process messages and tasks all day like a blind computer processor. VEDANTAM: There seems to be a paradox here because I think what I'm hearing you say is that scheduling yourself, or even overscheduling yourself, is the way to actually gain control over your life. Whereas people would sort of say, if you're actually scheduling every second and sort of deciding four weeks ahead of time when you're going to stop work on a certain Wednesday, you've actually turned yourself into a robot. NEWPORT: It's a paradox that shows up a lot. It confuses people. But I think you're right to point it out - is that if you study, especially really creative people, professional creatives, they are surprisingly structured in how they approach their day. I took a quote, at one point, from David Brooks, the columnist, and I might be paraphrasing here. But basically, he pointed out this observation that great creative thinkers approach their time like accountants, that this is this great disconnect, is that they're very structured and systematic about their time and produce the most unstructured, brilliant, creative insights. So it's a key paradox to point out because I really want to emphasize it. Adding structure and control to your time really can be the key to getting the biggest insights and most interesting work produced. VEDANTAM: I'm wondering if part of the tension comes about because we actually think of inspiration as being the thing that strikes us unexpectedly. And I think the case that you're making is that inspiration actually can be scheduled to arrive on command. NEWPORT: Well, as, you know, Chuck Close said - the artist - inspiration is for amateurs. I think we overfocus on the inspiration piece. If you're systematically pushing yourself and your knowledge and your craft, you will have inspiration. It'll happen in the shower. It'll happen while you walk to work. What's important is, you know, setting yourself up to have that inspiration and then giving yourself the time and structure you need to act on it, to actually produce something of value out of it. So I downplay the importance of inspiration, and I emphasize the importance of creating a life where inspiration is possible, and you're well-suited to act on it. VEDANTAM: I want to ask you a couple of questions that push back against this idea from a practical standpoint. What if people are in workplaces where they have managers and bosses who aren't enlightened enough to say, yes, you should spend several hours engaged in deep work? People can't always choose for themselves what kind of work they pursue. NEWPORT: Something that has seemed to be effective is, in that type of situation, having a conversation with whoever your boss is, whoever supervises you, and say, I want to talk about deep work. Here's what deep work is. And I want to talk about, you know, nondeep work or shallow work, and here's what that is. And both are important to my job. And I want to have a conversation and decide, what should my ratio be? That is, in a typical work week, what ratio of my hours should be deep work versus shallow work and actually nailing down a number, an aspirational target, that everyone agrees, yeah, this is right for your position in our company. It's not saying hey, boss, stop emailing me so much; you annoy me. VEDANTAM: (Laughter). NEWPORT: It's instead saying, hey, let's try to optimize myself. So what should I be going for here? Let me get your feedback on this. And people are reporting back to me, you know, tales of drastic changes to work cultures that they thought, there's no way. There's no way I'm going to get away with this, that I'm supposed to be on Slack all the time, or I'm supposed to be answering my emails all the time. They have this conversation. In the next week, they're spending 50% of their hours undistracted. So I've been pushing that particular managerial hack as a good, positive way forward to trying to fix some of these issues. VEDANTAM: I'm wondering if some people might say your advice is really advice for people who, in some ways, are at the top of their food chains. So if you have an author who basically is able to say, I'm going to disconnect from the world for 18 months - I'm just going to focus on writing this book - you know, someone else is probably picking up after this person in all kinds of different ways. If Cal Newport says, you know, I'm going to close the door in my office, I'm not going to answer my phone, I'm not going to check my email, but someone needs to get in touch with you in an emergency, that person is probably going to reach an assistant of yours. And that assistant doesn't have the same luxury of deep work as you do because he or she needs to be available to hear what the emergency is or to hear what the request is. Does having a group of people who are engaged in deep work necessarily mean there must be essentially a second tier of workers who are engaged in shallow work to allow the deep thinkers to do their deep thinking? NEWPORT: It doesn't require that, but it usually requires some type of reconfiguration of communication channels and expectations. So when I work, for example, with people maybe in a small consultancy that is client-facing, where they're used to this idea that clients need to reach us, issues pop up, what's important there is just to actually change the communication expectations. That maybe instead of having a client just have individual people's email addresses, the company sets up an email address for that client. And the company has set up some agreement on their end that there'll always be someone monitoring that, and here's the expectation of when you can get a response. Or maybe setting up a - it's sometimes called the bat phone or emergency phone idea, where you say, OK, here's a number you can call me at if there's an emergency when I'm in one of these deep work sessions. People set these up and say they get called maybe once a year. So I don't think you need actual extra people involved to make space for deep work. But I do think it almost always requires some effort, some sort of reconfiguration of people's expectations on how and when they can reach you. VEDANTAM: So I'm going to ask you a question now that's part serious and part teasing. You and I were scheduled to talk last week, and you didn't get the appointment down in your calendar, and I was sitting here waiting for you. And, of course, this kind of thing happens all the time. But in your case, I couldn't help but wonder, did he miss this because he actually hadn't spent the time doing the shallow work to get this in his calendar? And is it possible that when we engage in deep work, we are essentially, you know, getting the benefit of all of that deep work - we're getting the deep thinking, we're getting the accomplishments - but some of the cost is borne by other people, and they might actually be the people who are getting mad at you when they can't reach you? NEWPORT: Well, it's a good point. And I think that's actually - was what happened. Because I spend a lot of time working away from my computer, these type of problems happen to me more often. In this case - and, you know, I'm embarrassed it happened, but my vague memory was I saw this communication on my phone because I had to be on there to send something to someone, but I was far away from a computer. And so I wasn't able to easily add it to a calendar. And I was like, OK, I'll remember to do this when I get back to my office next, and I forgot. And it did cause problems. And I - so I'm embarrassed about it. And that type of thing does happen. And I think this hits on a big point, which is deep work, or a professional life focused on deep work, is less convenient for most people involved. But on the other hand, I want to put out there this notion that that might not be so bad, that it's possible that in this age of digital communication, we are focusing too much on convenience over effectiveness. VEDANTAM: I think, in some ways, what you're saying is also the tension between the short term and the long term. If I don't respond to a colleague's request or a manager's, you know, instructions to do something right away, it's irritating for the person at the other end of the line. And so I think most of us actually conform to the social norm of saying, yes, I'm just going to be responsive. I'm going to be available. I'm going to answer the question as soon as it's asked. The point that you're making, though, is that there might be long-term goals, deeper institutional goals, that are essentially - we're not thinking about. And, of course, when those goals are not met because they're not articulated, no one notices their absence. So people will notice it if you don't show up at an interview. People are not going to notice it if you don't write that bestseller or the next great idea. And so there's really a cultural bias in favor of the trivial over protecting what actually is most important. NEWPORT: I really agree with that point. And I would add to it that I think a big part of it is lack of metrics. So if we look at two parallel case studies, two different industries, let's look at the Industrial Revolution and the rise of mass industrial production. This was a world where the metrics for productivity were very clear. How many cars per hour is our factory producing? And what we saw in that world where bottom-line value was very easy to measure is that, very quickly, the structure of work moved away from what was convenient for the workers and towards what produced more value. It moved away from the old system in factories, where you had people work in teams at one spot in the floor to assemble the car, towards things like the assembly line, which are incredibly inconvenient. It's very hard to manage an assembly line. It's very hard to get it right. It causes lots of issues. It's annoying, but it produces a lot more value. You move to digital knowledge work. We don't have those metrics. It's much harder to measure, OK, what's the cost to our bottom line if you're more distracted or less distracted? And so my conjecture is that without those metrics, we are going to fall back on these interpersonal or cultural biases. We're wired to be social. We don't want to upset someone. These type of biases take over because it's much harder to measure, in this new world, the impact of different behaviors. VEDANTAM: I'm wondering if there's also a psychological explanation for the phenomenon you're describing. You know, I took a vacation a couple of weeks ago. And for the first time in a long time, I actually decided to unplug. So I didn't have Internet access. I wasn't checking my email. I literally was cut off from things going on at work. And when I got back, there were a number of things that had happened in my absence, some of which I wish I'd had the chance to weigh in on. But when I looked at the aggregate, the overall conclusion I got was really that the world did just fine in my absence. Things went fine. I actually wasn't as indispensable as I thought I was. I'm wondering if that might be a psychological driver in people being unwilling to actually cut themselves off - because not only might they discover that they are more productive, but they might also discover the world does just fine, thank you very much, without you. NEWPORT: You know, I think that's one of three big psychological drivers that have led us to this world we're in now with the sort of constant connectivity business. So that's certainly one, I think - this notion of - we get a sense a meaning and usefulness out of constantly being involved in interaction. I think the other two psychological drivers - one is just we're wired to be tribal. And it's very difficult for us, psychologically, to know there's an email waiting that we're not answering. And even if we know for a fact that the person who sent that message does not need a fast response, it still feels like we're at the tribal fire. And there's a tribe member standing there tapping you on the shoulder, and you're ignoring them. We just have a very hard time with that. And I think the third driver is knowledge work is much less structured. And so how do you prove to your organization or to your boss that you're valuable? And busyness, as a proxy for productivity, is something that a lot of people have defaulted to. Well, at the very least, if you see I'm sending lots of messages, you know I'm working. And so I think those three different factors are all intertwining to get us to this place where we find ourselves just constantly sending messages as opposed to thinking hard thoughts or producing new things. VEDANTAM: Cal Newport is a computer science professor at Georgetown University. He's the author of \"Deep Work: Rules For Focused Success In A Distracted World. \" Cal, thank you for joining me today on HIDDEN BRAIN. NEWPORT: Well, thank you. (SOUNDBITE OF MUSIC) VEDANTAM: This episode of HIDDEN BRAIN was produced by Rhaina Cohen and edited by Tara Boyle. Our team includes Jenny Schmidt, Laura Kwerel, Parth Shah and Thomas Lu. Our unsung hero this week is Stacey Foxwell. She's NPR's vice president of operations, which means she handles everything from where people sit to hiring the next generation of NPR journalists. Stacey keeps NPR running smoothly, and she does so calmly, cheerfully and with a remarkable sense of humor. For more HIDDEN BRAIN, you can follow us on Facebook and Twitter. If you liked this episode, please be sure to share it with one friend. We're always looking for new people to discover HIDDEN BRAIN. Next week, we conclude our You 2. 0 series with an episode about decision-making. (SOUNDBITE OF ARCHIVED NPR BROADCAST) DAN GILBERT: I realized that had you asked me a year earlier how I would be fairing, the answer would've been, oh, my gosh, I'll be devastated. But I wasn't devastated. It wasn't a good year, but it was OK. VEDANTAM: I'm Shankar Vedantam, and this is NPR. (SOUNDBITE OF MACHINE SHUTTING DOWN) VEDANTAM: System shutdown complete.", "section": "Hidden Brain", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-26-753721881": {"title": "Congress Considers Funding Ban On Chinese Rail Cars  : NPR", "url": "https://www.npr.org/2019/08/26/753721881/congress-mulls-ban-on-chinese-trains-and-buses-oh-come-on-builder-says", "author": "No author found", "published_date": "2019-08-26", "content": "", "section": "Politics", "disclaimer": ""}, "2019-08-27-754362629": {"title": "Teens, Screens And Mental Health: Scientists Debate The Link : NPR", "url": "https://www.npr.org/2019/08/27/754362629/the-scientific-debate-over-teens-screens-and-mental-health", "author": "No author found", "published_date": "2019-08-27", "content": "", "section": "Life Kit", "disclaimer": ""}, "2019-08-29-751116338": {"title": "Chinese App WeChat Messages Sent Outside China Are Censored Too, Say Researchers : NPR", "url": "https://www.npr.org/2019/08/29/751116338/china-intercepts-wechat-texts-from-u-s-and-abroad-researcher-says", "author": "No author found", "published_date": "2019-08-29", "content": "ARI SHAPIRO, HOST: Who is spying on us, and how? We're trying to answer those questions in this month's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")SHAPIRO: It's well known that China screens its citizens' social media accounts and personal messages. Now people outside China say the same thing is happening to them. NPR's Emily Feng reports. EMILY FENG, BYLINE: Activist Zhou Fengsuo was a student leader in the 1989 pro-democracy protests in Beijing's Tiananmen Square. When the protests were crushed, Zhou went to prison, then political reeducation, before moving to the U. S. in 1995. Today, he lives in a quiet Puerto Rican New Jersey neighborhood, and China's most popular social media app, WeChat, is his main link back to China. He's in constant communication with hundreds of people in China, advocating for political prisoners. ZHOU FENGSUO: I have so many groups. It's probably less than a hundred but more than 50. FENG: But in January, he noticed something strange. People weren't responding to his WeChat messages. ZHOU: I probably realized because I was expecting some feedback and there was no feedback. FENG: It was then he realized his messages were being censored so no one ever saw them. China routinely uses WeChat to censor Chinese Internet users, but now people outside China are also getting caught up. Dutch cyber researcher Victor Gevers thinks he knows why. He scans the Internet for vulnerabilities, and sometimes he finds odd things, shocking things, like a Chinese database containing 3. 7 billion WeChat conversations collected on a single day. Some 19 million of the messages were sent by people outside of China. Some messages then were censored. Other bits of the database made Gevers wonder exactly what was going on. VICTOR GEVERS: Why are there persons identified there with their ID number? Why is this database being built like that? Why are these messages being flagged? FENG: Here's how it worked. Gevers told me anyone using WeChat to send sensitive phrases would have their entire conversation scraped into this public security database no matter where they were in the world - phrases like Tiananmen, Xi Jinping. If the user is in China, the database automatically alerts the nearest Chinese police station. Tencent, WeChat's owner, declined to comment. And the database was completely unprotected, meaning anyone online could change its contents. Gevers is still trying to figure out why WeChat archived these specific messages. GEVERS: Who builds a mass surveillance system that is open to the Internet and you can enter without any username or password and change the data? That's horrible. FENG: For years, cybersecurity outfit Citizen Lab at the University of Toronto has tracked how WeChat uses keyword algorithms to automatically identify sensitive phrases the app then censors. Here's Jeffrey Knockel, a postdoctoral fellow at the lab. JEFFREY KNOCKEL: Chat filtering on WeChat applies to anyone who has created a WeChat account using a mainland Chinese phone number. This means that even if you move to another country and switch your WeChat account's phone number to that of that country, the censorship will still apply to you. FENG: But Chinese tech companies like Tencent, WeChat's owners, now have millions of international users, expanding who China can surveil. Sarah Cook is an analyst at nonprofit Freedom House. She points out WeChat is used abroad not just by Chinese tourists but also politicians in democracies communicating with Chinese constituents and dissidents. SARAH COOK: Maybe they're communicating with somebody else who's outside of China who has reached out, but they're still, for the most part, operating under the rules that are inside China. FENG: And those rules are leaving American WeChat users confused and scared about why they were blocked. DAVID: Although I was able to read the other people's messages, when I posted my message and nobody can see it - like I'm not there. FENG: That's David, a doctor who has lived in the U. S. for almost three decades. He doesn't want to use his full name because his family still lives in China. Like Zhou Fengsuo, his WeChat group messaging was blocked as well, then reinstated when, he says, he stopped sharing political articles. He self-censors now. DAVID: This censorship has just affected me psychologically and my behavior - both. FENG: Back in New Jersey, the activist Zhou Fengsuo is not giving up on WeChat. He says his activist work depends too much on the app. ZHOU: I have to use it. I just have to know what's going on. FENG: Even if, he adds, doing so can be very dangerous. Emily Feng, NPR News, Beijing. (SOUNDBITE OF MUSIC) ARI SHAPIRO, HOST:  Who is spying on us, and how? We're trying to answer those questions in this month's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") SHAPIRO: It's well known that China screens its citizens' social media accounts and personal messages. Now people outside China say the same thing is happening to them. NPR's Emily Feng reports. EMILY FENG, BYLINE: Activist Zhou Fengsuo was a student leader in the 1989 pro-democracy protests in Beijing's Tiananmen Square. When the protests were crushed, Zhou went to prison, then political reeducation, before moving to the U. S. in 1995. Today, he lives in a quiet Puerto Rican New Jersey neighborhood, and China's most popular social media app, WeChat, is his main link back to China. He's in constant communication with hundreds of people in China, advocating for political prisoners. ZHOU FENGSUO: I have so many groups. It's probably less than a hundred but more than 50. FENG: But in January, he noticed something strange. People weren't responding to his WeChat messages. ZHOU: I probably realized because I was expecting some feedback and there was no feedback. FENG: It was then he realized his messages were being censored so no one ever saw them. China routinely uses WeChat to censor Chinese Internet users, but now people outside China are also getting caught up. Dutch cyber researcher Victor Gevers thinks he knows why. He scans the Internet for vulnerabilities, and sometimes he finds odd things, shocking things, like a Chinese database containing 3. 7 billion WeChat conversations collected on a single day. Some 19 million of the messages were sent by people outside of China. Some messages then were censored. Other bits of the database made Gevers wonder exactly what was going on. VICTOR GEVERS: Why are there persons identified there with their ID number? Why is this database being built like that? Why are these messages being flagged? FENG: Here's how it worked. Gevers told me anyone using WeChat to send sensitive phrases would have their entire conversation scraped into this public security database no matter where they were in the world - phrases like Tiananmen, Xi Jinping. If the user is in China, the database automatically alerts the nearest Chinese police station. Tencent, WeChat's owner, declined to comment. And the database was completely unprotected, meaning anyone online could change its contents. Gevers is still trying to figure out why WeChat archived these specific messages. GEVERS: Who builds a mass surveillance system that is open to the Internet and you can enter without any username or password and change the data? That's horrible. FENG: For years, cybersecurity outfit Citizen Lab at the University of Toronto has tracked how WeChat uses keyword algorithms to automatically identify sensitive phrases the app then censors. Here's Jeffrey Knockel, a postdoctoral fellow at the lab. JEFFREY KNOCKEL: Chat filtering on WeChat applies to anyone who has created a WeChat account using a mainland Chinese phone number. This means that even if you move to another country and switch your WeChat account's phone number to that of that country, the censorship will still apply to you. FENG: But Chinese tech companies like Tencent, WeChat's owners, now have millions of international users, expanding who China can surveil. Sarah Cook is an analyst at nonprofit Freedom House. She points out WeChat is used abroad not just by Chinese tourists but also politicians in democracies communicating with Chinese constituents and dissidents. SARAH COOK: Maybe they're communicating with somebody else who's outside of China who has reached out, but they're still, for the most part, operating under the rules that are inside China. FENG: And those rules are leaving American WeChat users confused and scared about why they were blocked. DAVID: Although I was able to read the other people's messages, when I posted my message and nobody can see it - like I'm not there. FENG: That's David, a doctor who has lived in the U. S. for almost three decades. He doesn't want to use his full name because his family still lives in China. Like Zhou Fengsuo, his WeChat group messaging was blocked as well, then reinstated when, he says, he stopped sharing political articles. He self-censors now. DAVID: This censorship has just affected me psychologically and my behavior - both. FENG: Back in New Jersey, the activist Zhou Fengsuo is not giving up on WeChat. He says his activist work depends too much on the app. ZHOU: I have to use it. I just have to know what's going on. FENG: Even if, he adds, doing so can be very dangerous. Emily Feng, NPR News, Beijing. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-08-30-756034720": {"title": "How Gamergate Became A Template For Malicious Action Online : NPR", "url": "https://www.npr.org/2019/08/30/756034720/how-gamergate-became-a-template-for-malicious-action-online", "author": "No author found", "published_date": "2019-08-30", "content": "AUDIE CORNISH, HOST: Today, in 2019, the average person knows a lot about what's wrong with social media. There are headlines about Russian Twitter bots, mass shootings announced in advance on message boards, YouTube video wormholes that inspire white supremacists. Now, before all that, there was Gamergate. And this month marks five years since the beginning of a leaderless, mostly anonymous harassment campaign - a campaign targeting women in the industry, developers and journalists; anyone who called for change in the way women and people of color were represented in leadership or in games themselves. It was a warning and a demonstration of how bad actors could abuse the power of social networks to achieve malicious ends. Brianna Wu is one of the women targeted by the movement. In 2014, she was co-founder of the independent game studio called Giant Spacekat. And today she is a Democratic candidate for Congress in Massachusetts. Welcome to the program. BRIANNA WU: Thank you for having me. CORNISH: So to begin, how do you describe Gamergate in a nutshell? WU: Well, I think you nailed it. It was an organized harassment campaign against women in the video game industry. And what they found out was, when they made the cost of speaking out high enough, many women in games would quit rather than continue speaking up. So what they did is they sent us rape threats. They sent us death threats, and they harassed us until many women simply left the game industry. CORNISH: Now, you spoke out about some of the issues that Gamergate at least started to be about on your podcast. And after that, you became targeted. When you say death threats and rape threats, I mean, what form is this coming in? - how frequently? WU: So it's - it was all the time. It was constant. You know, one of the weird things about being a woman in the tech industry is you gain a kind of dark ability to judge the seriousness of a death threat. So I got one yesterday of a man telling me he was going to stab me to death. That just - you don't take that seriously. The ones I got were very credible. They had my address. They had information about my family. They were very specific about the violence they were going to do to me. It was so serious that, actually, the FBI got involved. Even local Congresswoman Katherine Clark got involved. CORNISH: At a certain point, there were women who - it seems like they kind of created a witness protection-style life for themselves, right? I mean, they had to move from their homes or stay with friends, be totally off the radar. Is that something you tried? And is it even possible in this day and age? WU: Well, I tried as best as I could. You know, we removed our name from every single database we could. And I went so far as to moving to a new house and having a system set up to, like, re-mail me packages and letters. And, you know, I would even use an anonymous name when ordering things from Amazon. So it was very convoluted. And the thing that made my heart break one day is - I got home from a movie with my husband, and someone had sent me pictures of standing right behind me in the movie theater, just to say, hey, I know where you live. CORNISH: That's how Gamergate felt to Brianna Wu from the inside. What happened to her has become a model for Internet harassment campaigns to this day. WHITNEY PHILLIPS: Gamergate was really a touchstone moment for people studying disinformation and trying to make sense of our present hellscape (ph). CORNISH: That's Whitney Phillips at Syracuse University, one of the people trying to make sense of that so-called hellscape. She says online harassment didn't start with Gamergate, but it brought the issue into the national spotlight. And the tactics used in Gamergate are all still around. PHILLIPS: The first is what's known as brigading. That's coordinated harassment against an individual. So a bunch of people kind of get together, decide who to target, and then they go after that particular person. So that was something that you saw very frequently in Gamergate. Another tactic that was used throughout Gamergate is doxing - so releasing personal identifying information, like home addresses. . . CORNISH: That are mostly public documents, right? So it's a matter of finding them and distributing it more widely. PHILLIPS: Some of the information is public. But in other cases, the information, like someone's Social Security number, is not publicly available and is accessed through a variety of hacking methods. And so it is absolutely devastating and terrifying for the people on the receiving end of that attention. CORNISH: You said that some of these techniques clearly existed before Gamergate. How did that situation make it worse? And where are the places we've seen it since? PHILLIPS: So one of the main differences between pre-Gamergate and post-Gamergate behavior is that before Gamergate, these behaviors tended to be anonymous. After Gamergate, it was clear for chaos entrepreneurs and other folks who realized that there was - there could be big business around. . . CORNISH: I got to stop you there. Did you say chaos entrepreneur? PHILLIPS: Chaos entrepreneur, yes - people who are stirring the pot, maybe because they are themselves reactionaries, maybe because they adhere to white supremacist ideology and maybe because they're trying to make money or some combination of both. And Gamergate was when that life choice, business strategy, became an active road that someone could travel. CORNISH: Is anonymous harassment a consequence of living in a society with open access to a lot of information? Or do you think there are safeguards that we could be using? PHILLIPS: One of the things about Gamergate that is so distressing is that it showed us the dangers of lack of moderation on social media platforms. And in response to Gamergate, social platforms did nothing. And the fact that they were rewarded - journalists wrote about them, social media participants talked about them - there was every reason for these individuals to continue doing what they were doing and, in fact, to do it worse. CORNISH: That was Syracuse University communications professor Whitney Phillips. Brianna Wu, the game developer targeted by Gamergate - well, she agrees with Phillips' assessment of what's wrong. WU: If you're going to have a community of people, I think you have a responsibility to moderate that, to take out this kind of, you know, frankly, illegal threats on people. So the problem is the law isn't quite there right now. And I agree with other people that we need to examine this, and I think we need to open up certain situations to civil liability. CORNISH: You're trying to straddle both worlds, right? You're running for Congress; you want a legislative fix. But you're also of the community, you know? You are someone who does this work and works in the industry. Is the industry ready to regulate? WU: No, not self-regulate. It's never going to do that. It will say what it has to say, and it will keep doing the status quo. To me, the most lasting legacy of action from the game industry - it's not any kind of action standing up for women employees. They've just simply funded a lot of catered Women in Tech luncheons. That's not what we need. My big lesson from Gamergate is asking the men in charge to do the right thing does not work. So we need women, we need people of color in positions of power not just in the game industry but at social media and tech companies and in Congress. CORNISH: Finally, an interview like this one - is that going to reignite harassment for you? WU: It does every single time. I believe I got four death threats yesterday. I had a piece in The New York Times a week ago. I've had to have very difficult conversations with my husband, like are we prepared to put our lives on the line to speak out about this? And we are. It certainly takes a toll. CORNISH: Brianna Wu, co-founder of the game studio Giant Spacekat, also Democratic candidate for Congress in Massachusetts, thank you for speaking with us. WU: Thank you for having me. (SOUNDBITE OF MUSIC) AUDIE CORNISH, HOST:  Today, in 2019, the average person knows a lot about what's wrong with social media. There are headlines about Russian Twitter bots, mass shootings announced in advance on message boards, YouTube video wormholes that inspire white supremacists. Now, before all that, there was Gamergate. And this month marks five years since the beginning of a leaderless, mostly anonymous harassment campaign - a campaign targeting women in the industry, developers and journalists; anyone who called for change in the way women and people of color were represented in leadership or in games themselves. It was a warning and a demonstration of how bad actors could abuse the power of social networks to achieve malicious ends. Brianna Wu is one of the women targeted by the movement. In 2014, she was co-founder of the independent game studio called Giant Spacekat. And today she is a Democratic candidate for Congress in Massachusetts. Welcome to the program. BRIANNA WU: Thank you for having me. CORNISH: So to begin, how do you describe Gamergate in a nutshell? WU: Well, I think you nailed it. It was an organized harassment campaign against women in the video game industry. And what they found out was, when they made the cost of speaking out high enough, many women in games would quit rather than continue speaking up. So what they did is they sent us rape threats. They sent us death threats, and they harassed us until many women simply left the game industry. CORNISH: Now, you spoke out about some of the issues that Gamergate at least started to be about on your podcast. And after that, you became targeted. When you say death threats and rape threats, I mean, what form is this coming in? - how frequently? WU: So it's - it was all the time. It was constant. You know, one of the weird things about being a woman in the tech industry is you gain a kind of dark ability to judge the seriousness of a death threat. So I got one yesterday of a man telling me he was going to stab me to death. That just - you don't take that seriously. The ones I got were very credible. They had my address. They had information about my family. They were very specific about the violence they were going to do to me. It was so serious that, actually, the FBI got involved. Even local Congresswoman Katherine Clark got involved. CORNISH: At a certain point, there were women who - it seems like they kind of created a witness protection-style life for themselves, right? I mean, they had to move from their homes or stay with friends, be totally off the radar. Is that something you tried? And is it even possible in this day and age? WU: Well, I tried as best as I could. You know, we removed our name from every single database we could. And I went so far as to moving to a new house and having a system set up to, like, re-mail me packages and letters. And, you know, I would even use an anonymous name when ordering things from Amazon. So it was very convoluted. And the thing that made my heart break one day is - I got home from a movie with my husband, and someone had sent me pictures of standing right behind me in the movie theater, just to say, hey, I know where you live. CORNISH: That's how Gamergate felt to Brianna Wu from the inside. What happened to her has become a model for Internet harassment campaigns to this day. WHITNEY PHILLIPS: Gamergate was really a touchstone moment for people studying disinformation and trying to make sense of our present hellscape (ph). CORNISH: That's Whitney Phillips at Syracuse University, one of the people trying to make sense of that so-called hellscape. She says online harassment didn't start with Gamergate, but it brought the issue into the national spotlight. And the tactics used in Gamergate are all still around. PHILLIPS: The first is what's known as brigading. That's coordinated harassment against an individual. So a bunch of people kind of get together, decide who to target, and then they go after that particular person. So that was something that you saw very frequently in Gamergate. Another tactic that was used throughout Gamergate is doxing - so releasing personal identifying information, like home addresses. . . CORNISH: That are mostly public documents, right? So it's a matter of finding them and distributing it more widely. PHILLIPS: Some of the information is public. But in other cases, the information, like someone's Social Security number, is not publicly available and is accessed through a variety of hacking methods. And so it is absolutely devastating and terrifying for the people on the receiving end of that attention. CORNISH: You said that some of these techniques clearly existed before Gamergate. How did that situation make it worse? And where are the places we've seen it since? PHILLIPS: So one of the main differences between pre-Gamergate and post-Gamergate behavior is that before Gamergate, these behaviors tended to be anonymous. After Gamergate, it was clear for chaos entrepreneurs and other folks who realized that there was - there could be big business around. . . CORNISH: I got to stop you there. Did you say chaos entrepreneur? PHILLIPS: Chaos entrepreneur, yes - people who are stirring the pot, maybe because they are themselves reactionaries, maybe because they adhere to white supremacist ideology and maybe because they're trying to make money or some combination of both. And Gamergate was when that life choice, business strategy, became an active road that someone could travel. CORNISH: Is anonymous harassment a consequence of living in a society with open access to a lot of information? Or do you think there are safeguards that we could be using? PHILLIPS: One of the things about Gamergate that is so distressing is that it showed us the dangers of lack of moderation on social media platforms. And in response to Gamergate, social platforms did nothing. And the fact that they were rewarded - journalists wrote about them, social media participants talked about them - there was every reason for these individuals to continue doing what they were doing and, in fact, to do it worse. CORNISH: That was Syracuse University communications professor Whitney Phillips. Brianna Wu, the game developer targeted by Gamergate - well, she agrees with Phillips' assessment of what's wrong. WU: If you're going to have a community of people, I think you have a responsibility to moderate that, to take out this kind of, you know, frankly, illegal threats on people. So the problem is the law isn't quite there right now. And I agree with other people that we need to examine this, and I think we need to open up certain situations to civil liability. CORNISH: You're trying to straddle both worlds, right? You're running for Congress; you want a legislative fix. But you're also of the community, you know? You are someone who does this work and works in the industry. Is the industry ready to regulate? WU: No, not self-regulate. It's never going to do that. It will say what it has to say, and it will keep doing the status quo. To me, the most lasting legacy of action from the game industry - it's not any kind of action standing up for women employees. They've just simply funded a lot of catered Women in Tech luncheons. That's not what we need. My big lesson from Gamergate is asking the men in charge to do the right thing does not work. So we need women, we need people of color in positions of power not just in the game industry but at social media and tech companies and in Congress. CORNISH: Finally, an interview like this one - is that going to reignite harassment for you? WU: It does every single time. I believe I got four death threats yesterday. I had a piece in The New York Times a week ago. I've had to have very difficult conversations with my husband, like are we prepared to put our lives on the line to speak out about this? And we are. It certainly takes a toll. CORNISH: Brianna Wu, co-founder of the game studio Giant Spacekat, also Democratic candidate for Congress in Massachusetts, thank you for speaking with us. WU: Thank you for having me. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-03-748604202": {"title": "Historic Recordings Revitalize Language For Passamaquoddy Tribal Members : NPR", "url": "https://www.npr.org/2019/09/03/748604202/historic-recordings-revitalize-language-for-passamaquoddy-tribal-members", "author": "No author found", "published_date": "2019-09-03", "content": "MARY LOUISE KELLY, HOST: Some of the oldest audio recordings in the world have been digitally restored and returned to the descendants of the people who made them, the Passamaquoddy tribe of eastern Maine. Now younger members of the tribe are learning songs and stories once thought to be forever lost. Reporter Erin Slomski-Pritz has the story. (SOUNDBITE OF ARCHIVED RECORDING)DWAYNE TOMAH: (Singing in Passamaquoddy). ERIN SLOMSKI-PRITZ, BYLINE: Growing up, Dwayne Tomah learned many Passamaquoddy songs from his grandmother, but the one you're hearing now was not among them. Tomah learned this song as an adult off of a 129-year-old wax cylinder recording. This recording. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: (Singing in Passamaquoddy). SLOMSKI-PRITZ: This song is part of a collection of 26 surviving songs, stories and histories recorded on wax cylinders. They are the oldest ethnographic recordings in the world, made with one of Thomas Edison's brand-new inventions. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: (Speaking in Passamaquoddy). SLOMSKI-PRITZ: In 1890, anthropologist Jesse Walter Fewkes traveled to remote Calais, Maine, to meet with the Passamaquoddy and test out Edison's phonograph. In this archival recording, Fewkes shows the Passamaquoddy how the device works. (SOUNDBITE OF ARCHIVED RECORDING)JESSE WALTER FEWKES: You can talk into it as fast as you like, or you can speak as deliberately as you choose. In either case, it reproduces exactly what you say. SLOMSKI-PRITZ: A handful of respected members of the Passamaquoddy tribe took turns recording their voices onto 35 wax cylinders. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: (Singing in Passamaquoddy). SLOMSKI-PRITZ: A century would pass before the Passamaquoddy would hear these recordings again. While the cylinders gathered dust in a museum archive, many of the documented songs and stories disappeared from cultural memory, largely due to state-sanctioned policies in Maine that discourage the Passamaquoddy from speaking their language. In that time, the rate of fluency went from 100% to around 10%. Passamaquoddy historian Donald Soctomah, who is 64 years old, experienced this shift firsthand. DAVID SOCTOMAH: I grew up in a household where everybody spoke the language. But when they spoke to me, they spoke in English. SLOMSKI-PRITZ: The recordings first resurfaced around 1980. The tribe received a collection of cassettes from the American Folklife Center at the Library of Congress. One of the Passamaquoddy elders listened to the tapes. Here's what he heard. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #3: (Singing in Passamaquoddy). SLOMSKI-PRITZ: The quality was bad. But underneath the static, according to Soctomah, the elder was able to identify the speaker. SOCTOMAH: He did recognize the voice, and he said that's Passamaquoddy, and that's my godfather. SLOMSKI-PRITZ: As meaningful as it was to have these recordings, they were just too hard to understand. Again, they sat in storage. Then in 2016, the Library of Congress digitized the recordings, and, thanks to another advance in technology, removed a good portion of the static. For the first time, fluent Passamaquoddy speakers, like Dwayne Tomah, could not only recognize the voice. They could understand the words. TOMAH: It was really actually a very emotional moment for me. That I am a descendant of the people that are actually documenting this stuff and to be able to understand them - it was really, really a powerful moment. TOMAH: Now Tomah has joined a group of Passamaquoddy elders on a project to reintroduce these songs and stories into their community. TOMAH: If we didn't have access to these cylinders, these songs would never be sung ever again. They have a second life. I think it's really - it's amazing, really. SLOMSKI-PRITZ: This second life involves passing on these songs to the next generation of Passamaquoddy. (SOUNDBITE OF ARCHIVED RECORDING)TOMAH: So it was an honor for me to be here today to honor the students who are graduating. SLOMSKI-PRITZ: Just a few miles away from where these first recordings were made in Calais, Maine, Dwayne Tomah stood dressed in deer skins in a crowded high school gymnasium. He told the crowd of students and families, many of whom were Passamaquoddy, that he had chosen a special song for this day. (SOUNDBITE OF ARCHIVED RECORDING)TOMAH: And this song was a gathering song to bring people together as one people. SLOMSKI-PRITZ: It's a song that many of their ancestors would have sung, a song that was almost lost, a song that Tomah has brought home. (SOUNDBITE OF ARCHIVED RECORDING)TOMAH: (Singing in Passamaquoddy). SLOMSKI-PRITZ: For NPR News, I'm Erin Slomski-Pritz. (SOUNDBITE OF BLUE LAB BEATS' \"PINEAPPLE\") MARY LOUISE KELLY, HOST:  Some of the oldest audio recordings in the world have been digitally restored and returned to the descendants of the people who made them, the Passamaquoddy tribe of eastern Maine. Now younger members of the tribe are learning songs and stories once thought to be forever lost. Reporter Erin Slomski-Pritz has the story. (SOUNDBITE OF ARCHIVED RECORDING) DWAYNE TOMAH: (Singing in Passamaquoddy). ERIN SLOMSKI-PRITZ, BYLINE: Growing up, Dwayne Tomah learned many Passamaquoddy songs from his grandmother, but the one you're hearing now was not among them. Tomah learned this song as an adult off of a 129-year-old wax cylinder recording. This recording. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: (Singing in Passamaquoddy). SLOMSKI-PRITZ: This song is part of a collection of 26 surviving songs, stories and histories recorded on wax cylinders. They are the oldest ethnographic recordings in the world, made with one of Thomas Edison's brand-new inventions. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: (Speaking in Passamaquoddy). SLOMSKI-PRITZ: In 1890, anthropologist Jesse Walter Fewkes traveled to remote Calais, Maine, to meet with the Passamaquoddy and test out Edison's phonograph. In this archival recording, Fewkes shows the Passamaquoddy how the device works. (SOUNDBITE OF ARCHIVED RECORDING) JESSE WALTER FEWKES: You can talk into it as fast as you like, or you can speak as deliberately as you choose. In either case, it reproduces exactly what you say. SLOMSKI-PRITZ: A handful of respected members of the Passamaquoddy tribe took turns recording their voices onto 35 wax cylinders. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #2: (Singing in Passamaquoddy). SLOMSKI-PRITZ: A century would pass before the Passamaquoddy would hear these recordings again. While the cylinders gathered dust in a museum archive, many of the documented songs and stories disappeared from cultural memory, largely due to state-sanctioned policies in Maine that discourage the Passamaquoddy from speaking their language. In that time, the rate of fluency went from 100% to around 10%. Passamaquoddy historian Donald Soctomah, who is 64 years old, experienced this shift firsthand. DAVID SOCTOMAH: I grew up in a household where everybody spoke the language. But when they spoke to me, they spoke in English. SLOMSKI-PRITZ: The recordings first resurfaced around 1980. The tribe received a collection of cassettes from the American Folklife Center at the Library of Congress. One of the Passamaquoddy elders listened to the tapes. Here's what he heard. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #3: (Singing in Passamaquoddy). SLOMSKI-PRITZ: The quality was bad. But underneath the static, according to Soctomah, the elder was able to identify the speaker. SOCTOMAH: He did recognize the voice, and he said that's Passamaquoddy, and that's my godfather. SLOMSKI-PRITZ: As meaningful as it was to have these recordings, they were just too hard to understand. Again, they sat in storage. Then in 2016, the Library of Congress digitized the recordings, and, thanks to another advance in technology, removed a good portion of the static. For the first time, fluent Passamaquoddy speakers, like Dwayne Tomah, could not only recognize the voice. They could understand the words. TOMAH: It was really actually a very emotional moment for me. That I am a descendant of the people that are actually documenting this stuff and to be able to understand them - it was really, really a powerful moment. TOMAH: Now Tomah has joined a group of Passamaquoddy elders on a project to reintroduce these songs and stories into their community. TOMAH: If we didn't have access to these cylinders, these songs would never be sung ever again. They have a second life. I think it's really - it's amazing, really. SLOMSKI-PRITZ: This second life involves passing on these songs to the next generation of Passamaquoddy. (SOUNDBITE OF ARCHIVED RECORDING) TOMAH: So it was an honor for me to be here today to honor the students who are graduating. SLOMSKI-PRITZ: Just a few miles away from where these first recordings were made in Calais, Maine, Dwayne Tomah stood dressed in deer skins in a crowded high school gymnasium. He told the crowd of students and families, many of whom were Passamaquoddy, that he had chosen a special song for this day. (SOUNDBITE OF ARCHIVED RECORDING) TOMAH: And this song was a gathering song to bring people together as one people. SLOMSKI-PRITZ: It's a song that many of their ancestors would have sung, a song that was almost lost, a song that Tomah has brought home. (SOUNDBITE OF ARCHIVED RECORDING) TOMAH: (Singing in Passamaquoddy). SLOMSKI-PRITZ: For NPR News, I'm Erin Slomski-Pritz. (SOUNDBITE OF BLUE LAB BEATS' \"PINEAPPLE\")", "section": "National", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-03-756007555": {"title": "States Upgrade Election Equipment, But See 'A Race Without A Finish Line' : NPR", "url": "https://www.npr.org/2019/09/03/756007555/states-upgrade-election-equipment-wary-of-a-race-without-a-finish-line", "author": "No author found", "published_date": "2019-09-03", "content": "STEVE INSKEEP, HOST: OK, America, just to let you know, you have five months - five months - until presidential primary voting begins. That means election officials are focused now on keeping elections safe from cyberattacks. In Pennsylvania, every county is getting new paper ballot machines. NPR's Pam Fessler paid a visit. PAM FESSLER, BYLINE: On an oppressively hot weekday night, 200 residents of Bucks County, north of Philadelphia, showed up at a local college to weigh in on how best to protect democracy. DOLORES MUCCI: Nancy, you go in and vote. Go in and vote. FESSLER: They were there to test the different voting machines the county's considering buying to replace its old equipment. It was a little like speed dating as Dolores Mucci and her friends circulated among the vendors, writing down their likes and dislikes after casting some make-believe votes. MUCCI: Yeah. Mary Bailey from \"It's A Wonderful Life\" and Thomas Edison. UNIDENTIFIED PERSON #1: You like Thomas Edison. MUCCI: And I'm putting Carl Sagan. Oh, vote for three. UNIDENTIFIED PERSON #2: What if they vote for too many? What happens? FESSLER: The sales representative from Dominion Voting says the touch-screen machine won't allow over voting. It's one of its features. It can also change the language of the ballot with a touch of a button. UNIDENTIFIED SALES REPRESENTATIVE #1: Easy peasy's the name of the game. FESSLER: He shows how the machine prints the ballot once the voter is done. It's then fed into a scanner to be counted. Voters can also do it the old-fashioned way, using a pen to fill in ovals on a preprinted ballot. Nearby, another vendor, Election Systems & Software, demonstrates a third option - a big touch screen, which prints the voter's choices on a blank piece of paper, then displays it in a window. UNIDENTIFIED SALES REPRESENTATIVE #2: If it's how you want to vote, you press another button, it tabulates it and drops it into a ballot box. So that one is one-stop shopping instead of a two-fold system. UNIDENTIFIED PERSON #3: That eliminates a step then. UNIDENTIFIED SALES REPRESENTATIVE #2: Exactly. FESSLER: Eliminating steps is pretty popular here, although many security experts think anything short of a hand-marked ballot presents a risk. Still, Marian Schneider, a former Pennsylvania election official, thinks whatever counties decide, the state's in much better shape than it was in 2016 when more than 80% of its voters cast ballots on paperless machines. MARIAN SCHNEIDER: Whatever the computer said, the computer said. You were done. FESSLER: Now there will be a physical record that can be reviewed for accuracy. Schneider runs Verified Voting, a group that's long promoted paper ballots, which almost every U. S. voter will use next year. But she says that's not enough. SCHNEIDER: You have to check the paper afterwards. You have to randomly sample those ballots and make sure that the results that the software reported matches what's on the paper ballots. FESSLER: Something called risk-limiting audits. Pennsylvania is among a dozen states now testing the idea. It's all part of a multipronged national effort to secure next year's elections. Kathy Boockvar, Pennsylvania's acting secretary, says her state, like others, is working with the U. S. Department of Homeland Security and has participated in tabletop exercises to practice responding to cyberattacks. They're also upgrading their voter registration database, another potential target. But Boockvar has a warning. KATHY BOOCKVAR: You know, it's a race without a finish line, right? So the key is that we have to be building and reinforcing our walls faster than those that are trying to tear them down. FESSLER: And like other election officials, she says that requires more resources, not only to build a wall against attacks. . . BOOCKVAR: But the wall of confidence for each voter to know that their vote is secure and their vote is being counted accurately. FESSLER: Last year, Congress approved $380 million to help do that. Pennsylvania only got $14 million, a drop in the bucket. Liz Howard of the Brennan Center for Justice says her group estimates the states could easily use another $2 billion over five years, especially to secure local election offices, which tend to be more vulnerable. LIZ HOWARD: With over 8,000 election jurisdictions across the country, there's some not insubstantial portion of them that do not have IT support at the local level. FESSLER: Still, at the Bucks County demo, security was clearly a secondary concern. Resident Milo Morris says he wants machines that are easy to use. MILO MORRIS: I don't want to see voters get bogged down once they get into the voting booth, you know, because all that does is discourage the practice altogether. And we need everybody to come out and vote. We want people to come out and vote. FESSLER: Something local officials also have to consider when picking the new machines. Pam Fessler, NPR News. (SOUNDBITE OF STAN FOREBEE'S \"THE MONSOON\") STEVE INSKEEP, HOST:  OK, America, just to let you know, you have five months - five months - until presidential primary voting begins. That means election officials are focused now on keeping elections safe from cyberattacks. In Pennsylvania, every county is getting new paper ballot machines. NPR's Pam Fessler paid a visit. PAM FESSLER, BYLINE: On an oppressively hot weekday night, 200 residents of Bucks County, north of Philadelphia, showed up at a local college to weigh in on how best to protect democracy. DOLORES MUCCI: Nancy, you go in and vote. Go in and vote. FESSLER: They were there to test the different voting machines the county's considering buying to replace its old equipment. It was a little like speed dating as Dolores Mucci and her friends circulated among the vendors, writing down their likes and dislikes after casting some make-believe votes. MUCCI: Yeah. Mary Bailey from \"It's A Wonderful Life\" and Thomas Edison. UNIDENTIFIED PERSON #1: You like Thomas Edison. MUCCI: And I'm putting Carl Sagan. Oh, vote for three. UNIDENTIFIED PERSON #2: What if they vote for too many? What happens? FESSLER: The sales representative from Dominion Voting says the touch-screen machine won't allow over voting. It's one of its features. It can also change the language of the ballot with a touch of a button. UNIDENTIFIED SALES REPRESENTATIVE #1: Easy peasy's the name of the game. FESSLER: He shows how the machine prints the ballot once the voter is done. It's then fed into a scanner to be counted. Voters can also do it the old-fashioned way, using a pen to fill in ovals on a preprinted ballot. Nearby, another vendor, Election Systems & Software, demonstrates a third option - a big touch screen, which prints the voter's choices on a blank piece of paper, then displays it in a window. UNIDENTIFIED SALES REPRESENTATIVE #2: If it's how you want to vote, you press another button, it tabulates it and drops it into a ballot box. So that one is one-stop shopping instead of a two-fold system. UNIDENTIFIED PERSON #3: That eliminates a step then. UNIDENTIFIED SALES REPRESENTATIVE #2: Exactly. FESSLER: Eliminating steps is pretty popular here, although many security experts think anything short of a hand-marked ballot presents a risk. Still, Marian Schneider, a former Pennsylvania election official, thinks whatever counties decide, the state's in much better shape than it was in 2016 when more than 80% of its voters cast ballots on paperless machines. MARIAN SCHNEIDER: Whatever the computer said, the computer said. You were done. FESSLER: Now there will be a physical record that can be reviewed for accuracy. Schneider runs Verified Voting, a group that's long promoted paper ballots, which almost every U. S. voter will use next year. But she says that's not enough. SCHNEIDER: You have to check the paper afterwards. You have to randomly sample those ballots and make sure that the results that the software reported matches what's on the paper ballots. FESSLER: Something called risk-limiting audits. Pennsylvania is among a dozen states now testing the idea. It's all part of a multipronged national effort to secure next year's elections. Kathy Boockvar, Pennsylvania's acting secretary, says her state, like others, is working with the U. S. Department of Homeland Security and has participated in tabletop exercises to practice responding to cyberattacks. They're also upgrading their voter registration database, another potential target. But Boockvar has a warning. KATHY BOOCKVAR: You know, it's a race without a finish line, right? So the key is that we have to be building and reinforcing our walls faster than those that are trying to tear them down. FESSLER: And like other election officials, she says that requires more resources, not only to build a wall against attacks. . . BOOCKVAR: But the wall of confidence for each voter to know that their vote is secure and their vote is being counted accurately. FESSLER: Last year, Congress approved $380 million to help do that. Pennsylvania only got $14 million, a drop in the bucket. Liz Howard of the Brennan Center for Justice says her group estimates the states could easily use another $2 billion over five years, especially to secure local election offices, which tend to be more vulnerable. LIZ HOWARD: With over 8,000 election jurisdictions across the country, there's some not insubstantial portion of them that do not have IT support at the local level. FESSLER: Still, at the Bucks County demo, security was clearly a secondary concern. Resident Milo Morris says he wants machines that are easy to use. MILO MORRIS: I don't want to see voters get bogged down once they get into the voting booth, you know, because all that does is discourage the practice altogether. And we need everybody to come out and vote. We want people to come out and vote. FESSLER: Something local officials also have to consider when picking the new machines. Pam Fessler, NPR News. (SOUNDBITE OF STAN FOREBEE'S \"THE MONSOON\")", "section": "2020 Election: Secure Your Vote", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-04-757626090": {"title": "Google To Pay $170 Million Over Allegations YouTube Collected Kids' Personal Info  : NPR", "url": "https://www.npr.org/2019/09/04/757626090/google-to-pay-170-million-over-allegations-that-youtube-collected-kids-personal-", "author": "No author found", "published_date": "2019-09-04", "content": "MARY LOUISE KELLY, HOST: Google will pay $170 million to settle allegations by the Federal Trade Commission and New York's attorney general. The complaint alleges Google's video platform YouTube earned hundreds of millions of dollars by tracking, profiling and targeting ads to children without parental consent. That is a violation of federal law. NPR's Anya Kamenetz reports the FTC means to make YouTube safer, but some members of the commission itself say the settlement is too lenient. ANYA KAMENETZ, BYLINE: Busy parents know that YouTube is packed with videos that appeal to children, toddlers, even infants, like ChuChu TV, a channel with 26 million subscribers. (SOUNDBITE OF SONG, \"THE WHEELS ON THE BUS\")UNIDENTIFIED SINGER: (Singing) The people on the bus go up and down, up and down, up and down. KAMENETZ: But YouTube has maintained the fiction that its terms of service restrict the platform to use by people 13 years and older. Josh Golin of the Campaign for a Commercial Free Childhood, an advocacy group, says that jig is now up. JOSH GOLIN: They had to acknowledge that, in fact, much of their site is child-directed and, therefore, they have to comply with the law. KAMENETZ: The Children's Online Privacy Protection Act does not allow companies to profile young users or target them with advertising. Golin explains. . . GOLIN: Every time any of us is watching a video on YouTube, there's information being collected about us - our geolocation, our watch history, marketing profiles created about us, about our likes and dislikes and interests. KAMENETZ: All the better to advertise. But when it comes to children. . . GOLIN: The law says that you can't do that with children unless you get parental permission first. And Google did not do that. KAMENETZ: The FTC alleges Google is highly aware of YouTube's popularity with children. In presentations to two big advertisers, the toy companies Mattel and Hasbro, Google proclaimed that YouTube is the No. 1 kids website and the new Saturday morning cartoons. The proposed FTC settlement is the largest involving children's privacy, but it doesn't charge any executives with wrongdoing. And two members of a five-member bipartisan commission dissented from today's announcement, saying that the penalty isn't severe enough. FTC Commissioner Rohit Chopra said in a statement the company, quote, \"baited children using nursery rhymes and cartoons to feed its massively profitable advertising business. \"PHYLLIS MARCUS: They do say the law of settling is when no one's happy. KAMENETZ: That's Phyllis Marcus, who worked at the FTC for 17 years and helped lead the children's privacy program there. She says, nevertheless, this decision may lead to more high-quality kids content. MARCUS: Now it seems there are going to be more robust choices. KAMENETZ: A Google spokeswoman referred NPR to an official blog post by YouTube CEO Susan Wojcicki which says that YouTube will limit data collection and stop serving personalized ads on kids videos, as well as removing comments and notifications on those videos. These small steps may help give parents a little more peace of mind. Anya Kamenetz, NPR News. (SOUNDBITE OF BALMORHEA SONG, \"SETTLER\") MARY LOUISE KELLY, HOST:  Google will pay $170 million to settle allegations by the Federal Trade Commission and New York's attorney general. The complaint alleges Google's video platform YouTube earned hundreds of millions of dollars by tracking, profiling and targeting ads to children without parental consent. That is a violation of federal law. NPR's Anya Kamenetz reports the FTC means to make YouTube safer, but some members of the commission itself say the settlement is too lenient. ANYA KAMENETZ, BYLINE: Busy parents know that YouTube is packed with videos that appeal to children, toddlers, even infants, like ChuChu TV, a channel with 26 million subscribers. (SOUNDBITE OF SONG, \"THE WHEELS ON THE BUS\") UNIDENTIFIED SINGER: (Singing) The people on the bus go up and down, up and down, up and down. KAMENETZ: But YouTube has maintained the fiction that its terms of service restrict the platform to use by people 13 years and older. Josh Golin of the Campaign for a Commercial Free Childhood, an advocacy group, says that jig is now up. JOSH GOLIN: They had to acknowledge that, in fact, much of their site is child-directed and, therefore, they have to comply with the law. KAMENETZ: The Children's Online Privacy Protection Act does not allow companies to profile young users or target them with advertising. Golin explains. . . GOLIN: Every time any of us is watching a video on YouTube, there's information being collected about us - our geolocation, our watch history, marketing profiles created about us, about our likes and dislikes and interests. KAMENETZ: All the better to advertise. But when it comes to children. . . GOLIN: The law says that you can't do that with children unless you get parental permission first. And Google did not do that. KAMENETZ: The FTC alleges Google is highly aware of YouTube's popularity with children. In presentations to two big advertisers, the toy companies Mattel and Hasbro, Google proclaimed that YouTube is the No. 1 kids website and the new Saturday morning cartoons. The proposed FTC settlement is the largest involving children's privacy, but it doesn't charge any executives with wrongdoing. And two members of a five-member bipartisan commission dissented from today's announcement, saying that the penalty isn't severe enough. FTC Commissioner Rohit Chopra said in a statement the company, quote, \"baited children using nursery rhymes and cartoons to feed its massively profitable advertising business. \" PHYLLIS MARCUS: They do say the law of settling is when no one's happy. KAMENETZ: That's Phyllis Marcus, who worked at the FTC for 17 years and helped lead the children's privacy program there. She says, nevertheless, this decision may lead to more high-quality kids content. MARCUS: Now it seems there are going to be more robust choices. KAMENETZ: A Google spokeswoman referred NPR to an official blog post by YouTube CEO Susan Wojcicki which says that YouTube will limit data collection and stop serving personalized ads on kids videos, as well as removing comments and notifications on those videos. These small steps may help give parents a little more peace of mind. Anya Kamenetz, NPR News. (SOUNDBITE OF BALMORHEA SONG, \"SETTLER\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-04-757441886": {"title": "Google, YouTube To Pay $170 Million Penalty Over Collecting Kids' Personal Info : NPR", "url": "https://www.npr.org/2019/09/04/757441886/google-youtube-to-pay-170-million-penalty-over-collecting-kids-personal-info", "author": "No author found", "published_date": "2019-09-04", "content": "", "section": "Technology", "disclaimer": ""}, "2019-09-04-755066523": {"title": "In 2020, Millions Will Cast Ballots Using Insecure Machines, Experts Say : NPR", "url": "https://www.npr.org/2019/09/04/755066523/cyber-experts-warn-of-vulnerabilities-facing-2020-election-machines", "author": "No author found", "published_date": "2019-09-04", "content": "ARI SHAPIRO, HOST: Remember the hanging chads that caused so much confusion in the 2000 Bush vs. Gore election? Well, since then, the federal government has spent billions of dollars to modernize voting systems. But how secure are today's systems? NPR's Miles Parks has been keeping track of this as we get ready for next year's presidential election, and he is here in the studio. Hey, Miles. MILES PARKS, BYLINE: Hi, there. SHAPIRO: Let's start with defining what makes voting equipment safe. How do you draw the distinction between the good and the bad? PARKS: So the key, according to cyber experts, is something called software independence. But this is basically just a paper trail. Basically, you want something where voters can look at their ballot before they cast it, something that if there's a malfunction or if there's a hack - a lot of these machines are more than 10 years old, and some of them do malfunction - but you want something where you're not reliant on the technology to be able to spot a problem. And this is sort of the future of election security - not only protecting the vote, but protecting our ability to then go back and double-check the results so if there is a problem, we can fix it. SHAPIRO: To kind of do an audit. But some voting systems today don't have that ability. They don't have a paper trail. Why not? PARKS: Right. So this goes back to that 2000 election, where America just decided we're going to overhaul this whole thing and spend a bunch of money, buy a bunch of new electronic voting machines. But people at this time weren't thinking about security at the - kind of the front of their minds. I talked to Matt Blaze, who's a cybersecurity and voting expert at Georgetown University about this. MATT BLAZE: Even if you asked us back then what exactly should we do to build secure voting machines, we wouldn't have really been able to tell you precisely what you needed to do. Today, we can. PARKS: That answer comes down to paper. So there's been this push really over the last decade to get these outdated paperless machines completely out of the American voting system. SHAPIRO: And how's that push going? How many of those machines will be around in 2020? PARKS: So it kind of depends on who you ask, really. The Brennan Center for Justice released a report this summer that said the amount of voters who are going to be voting on these sorts of machines in 2020 has basically been cut in half since the 2016 election. And a lot of that comes from Georgia, who is overhauling their entire statewide voting system before that election. But the number is still pretty high. It was 20% in 2016. In 2020, the Brennan Center seems to think it's going to be about 12%. That's about 16 million voters. And security advocates say this is the most low-hanging fruit in the voting world, something we've been talking about for much of this decade. A foreign adversary attacked the 2016 election, and we couldn't get this thing fixed. There's frustration there. SHAPIRO: Right, so we know that Russia tried to hack into U. S. election systems - totally separate from the misinformation campaign. Are experts expecting more of the same or even more than we saw in 2016 in 2020? PARKS: Well, when we look at 2016, it is important to note that there is no evidence that any vote tallies were actually changed in that attack. Attackers were able to break into registration systems and were able to steal some voter data. But the Senate Intelligence Committee did release in their report this summer and they said basically there is a possibility that what Russia was doing them in breaking into those systems wasn't attacking us. Potentially, they were intelligence gathering for a future attack. I talked to Bruce Schneier, who's a fellow at Harvard's Berkman Center for Internet and Society, about whether he thinks 2016 was kind of the worst of the worst when it comes to cyberattacks on our elections. BRUCE SCHNEIER: So the odds that we've seen the worst in cyberattacks in any space seems small to me. I mean, this is as bad as it could possibly get for the rest of the future of humanity? That just seems implausible, right? I mean, as soon as I say it, that seems dumb. PARKS: So it's kind of off when you hear politicians pointing at the 2018 midterms, which went really smoothly, and saying OK, the problem is behind us; everything's fixed. According to people I've talked to, cyber experts, there's still a lot of problems still there and the 2020 election and beyond. SHAPIRO: Miles, that sounds really ominous. PARKS: I don't know what to tell you. SHAPIRO: (Laughter). PARKS: I mean, you know, we're working on it. SHAPIRO: That's NPR's Miles Parks on election security ahead of 2020. Thank you, Miles. PARKS: Thank you. (SOUNDBITE OF THE BOOKS AND JOSE GONZALEZ'S \"CELLO SONG\") ARI SHAPIRO, HOST:  Remember the hanging chads that caused so much confusion in the 2000 Bush vs. Gore election? Well, since then, the federal government has spent billions of dollars to modernize voting systems. But how secure are today's systems? NPR's Miles Parks has been keeping track of this as we get ready for next year's presidential election, and he is here in the studio. Hey, Miles. MILES PARKS, BYLINE: Hi, there. SHAPIRO: Let's start with defining what makes voting equipment safe. How do you draw the distinction between the good and the bad? PARKS: So the key, according to cyber experts, is something called software independence. But this is basically just a paper trail. Basically, you want something where voters can look at their ballot before they cast it, something that if there's a malfunction or if there's a hack - a lot of these machines are more than 10 years old, and some of them do malfunction - but you want something where you're not reliant on the technology to be able to spot a problem. And this is sort of the future of election security - not only protecting the vote, but protecting our ability to then go back and double-check the results so if there is a problem, we can fix it. SHAPIRO: To kind of do an audit. But some voting systems today don't have that ability. They don't have a paper trail. Why not? PARKS: Right. So this goes back to that 2000 election, where America just decided we're going to overhaul this whole thing and spend a bunch of money, buy a bunch of new electronic voting machines. But people at this time weren't thinking about security at the - kind of the front of their minds. I talked to Matt Blaze, who's a cybersecurity and voting expert at Georgetown University about this. MATT BLAZE: Even if you asked us back then what exactly should we do to build secure voting machines, we wouldn't have really been able to tell you precisely what you needed to do. Today, we can. PARKS: That answer comes down to paper. So there's been this push really over the last decade to get these outdated paperless machines completely out of the American voting system. SHAPIRO: And how's that push going? How many of those machines will be around in 2020? PARKS: So it kind of depends on who you ask, really. The Brennan Center for Justice released a report this summer that said the amount of voters who are going to be voting on these sorts of machines in 2020 has basically been cut in half since the 2016 election. And a lot of that comes from Georgia, who is overhauling their entire statewide voting system before that election. But the number is still pretty high. It was 20% in 2016. In 2020, the Brennan Center seems to think it's going to be about 12%. That's about 16 million voters. And security advocates say this is the most low-hanging fruit in the voting world, something we've been talking about for much of this decade. A foreign adversary attacked the 2016 election, and we couldn't get this thing fixed. There's frustration there. SHAPIRO: Right, so we know that Russia tried to hack into U. S. election systems - totally separate from the misinformation campaign. Are experts expecting more of the same or even more than we saw in 2016 in 2020? PARKS: Well, when we look at 2016, it is important to note that there is no evidence that any vote tallies were actually changed in that attack. Attackers were able to break into registration systems and were able to steal some voter data. But the Senate Intelligence Committee did release in their report this summer and they said basically there is a possibility that what Russia was doing them in breaking into those systems wasn't attacking us. Potentially, they were intelligence gathering for a future attack. I talked to Bruce Schneier, who's a fellow at Harvard's Berkman Center for Internet and Society, about whether he thinks 2016 was kind of the worst of the worst when it comes to cyberattacks on our elections. BRUCE SCHNEIER: So the odds that we've seen the worst in cyberattacks in any space seems small to me. I mean, this is as bad as it could possibly get for the rest of the future of humanity? That just seems implausible, right? I mean, as soon as I say it, that seems dumb. PARKS: So it's kind of off when you hear politicians pointing at the 2018 midterms, which went really smoothly, and saying OK, the problem is behind us; everything's fixed. According to people I've talked to, cyber experts, there's still a lot of problems still there and the 2020 election and beyond. SHAPIRO: Miles, that sounds really ominous. PARKS: I don't know what to tell you. SHAPIRO: (Laughter). PARKS: I mean, you know, we're working on it. SHAPIRO: That's NPR's Miles Parks on election security ahead of 2020. Thank you, Miles. PARKS: Thank you. (SOUNDBITE OF THE BOOKS AND JOSE GONZALEZ'S \"CELLO SONG\")", "section": "2020 Election: Secure Your Vote", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-05-757885278": {"title": "Facebook, Google Meet With National Security Officials About 2020 Election : NPR", "url": "https://www.npr.org/2019/09/05/757885278/facebook-and-big-tech-meet-with-feds-to-confer-about-2020-election-security", "author": "No author found", "published_date": "2019-09-05", "content": "", "section": "2020 Election: Secure Your Vote", "disclaimer": ""}, "2019-09-06-758399814": {"title": "Town Avoids Paying Massive $5 Million Ransom In Cyberattack : NPR", "url": "https://www.npr.org/2019/09/06/758399814/town-avoids-paying-massive-5-million-ransom-in-cyberattack", "author": "No author found", "published_date": "2019-09-06", "content": "", "section": "National", "disclaimer": ""}, "2019-09-06-758232704": {"title": "States Take On Facebook In Antitrust Investigation; DOJ Eyeing Google : NPR", "url": "https://www.npr.org/2019/09/06/758232704/is-facebook-too-big-state-attorneys-general-want-to-know", "author": "No author found", "published_date": "2019-09-06", "content": "RACHEL MARTIN, HOST: Facebook is under fire again. Attorneys general from several states have launched a formal investigation into the social media giant over anti-competitive practices. The New York attorney general's office confirmed this morning they are leading an investigation that will look at whether Facebook is hurting consumers. This is the latest move by federal and state government to get tough on big social media companies. We should note here Facebook is an NPR sponsor. We've got NPR's Aarti Shahani on the line to tell us more about this. Hi, Aarti. AARTI SHAHANI, BYLINE: Hi. MARTIN: What do you know about the investigation thus far? SHAHANI: So the New York state attorney general, Letitia James, announced she's leading a bipartisan investigation - you heard me right, bipartisan. MARTIN: (Laughter). SHAHANI: That's right. MARTIN: Hurray for that. SHAHANI: It is multipronged. First, has Facebook been negligent with consumer data? Two billion-plus users signed up for the service. That's about a quarter of the population on Earth entrusting the company. MARTIN: Wow. SHAHANI: And Facebook has then turned around and offered exclusive access to these stockpiles of data to partners who could maybe help Facebook grow even bigger and more powerful like Cambridge Analytica and also Airbnb. Another question that James and her colleagues will investigate is whether Facebook's growth is harming consumer choice. I'll give you an example here. Just this week, the company brought its matchmaking service to the U. S. , OK? Plenty of other dating apps already exist here. Facebook takes a lot of features from those apps and could easily drive the competitors out of business not because Facebook is offering a better service but because Facebook's so dominant, startups can't compete for attention. Some advertisers have expressed concern about how much control Facebook has over the market. The AGs will take a look into that, too. MARTIN: Is this just about Facebook? Or are there other companies targeted here? SHAHANI: So another set of AGs is looking at Google. And that's being led by the Texas attorney general, a Republican, Ken Paxton. He's leading a multi-state probe into Google, according to The Wall Street Journal. Back in June, Paxton raised the concern that information about how people spend their lives - GPS location data from Google Maps or Waze, what we search for, what we view on YouTube. This data, he said, has become extremely valuable, especially when aggregated into large sets and analyzed and packaged for targeted marketing. And he was concerned that the biggest platforms like Google's YouTube don't have an incentive to protect consumers. You know, just this week, Google settled a case for covertly and illegally tracking little kids as they're watching \"Peppa Pig. \" Paxton and 42 other AGs asked the Federal Trade Commission to work closely with them to look at predatory conduct and anti-competitive practices. So presumably, that's the effort he's leading. MARTIN: So, you know, consumers have been hearing these stories about Facebook or Google for a long time now, right? And I just wonder if you're seeing that take any - have any kind of effect because it seems to me every bad news story that comes out about these companies, consumers are, like, well, I get it. But it's just so integral in my life. It's convenient. It helps me keep in touch with family and friends. SHAHANI: Right. But I actually think that that picture is changing, particularly as we start to understand the real stakes of it, right? There was a school of thought that maybe is still out there a bit of, hey, it's free, so it can't be bad for me. I don't pay for Facebook or YouTube. But then we have to note, Rachel, that we're entering a new chapter in history, right? That chapter is the data economy. The companies that own information, which they will then sell to other businesses - they have a whole lot of control. They can set prices. They can wipe out entire sectors. They can control what we believe and think to be true. I mean, we've reported on this repeatedly. MARTIN: Right. SHAHANI: So while the individual consumer right now might not feel harm in the moment, pain in the moment, in the long run, there could be a massive transfer of wealth and power that hurts us all. So I think that the picture is changing there. I would add that Google says it's working with regulators, including AGs. MARTIN: All right. NPR's Aarti Shahani. Aarti, thanks. We appreciate it. SHAHANI: Thank you. RACHEL MARTIN, HOST:  Facebook is under fire again. Attorneys general from several states have launched a formal investigation into the social media giant over anti-competitive practices. The New York attorney general's office confirmed this morning they are leading an investigation that will look at whether Facebook is hurting consumers. This is the latest move by federal and state government to get tough on big social media companies. We should note here Facebook is an NPR sponsor. We've got NPR's Aarti Shahani on the line to tell us more about this. Hi, Aarti. AARTI SHAHANI, BYLINE: Hi. MARTIN: What do you know about the investigation thus far? SHAHANI: So the New York state attorney general, Letitia James, announced she's leading a bipartisan investigation - you heard me right, bipartisan. MARTIN: (Laughter). SHAHANI: That's right. MARTIN: Hurray for that. SHAHANI: It is multipronged. First, has Facebook been negligent with consumer data? Two billion-plus users signed up for the service. That's about a quarter of the population on Earth entrusting the company. MARTIN: Wow. SHAHANI: And Facebook has then turned around and offered exclusive access to these stockpiles of data to partners who could maybe help Facebook grow even bigger and more powerful like Cambridge Analytica and also Airbnb. Another question that James and her colleagues will investigate is whether Facebook's growth is harming consumer choice. I'll give you an example here. Just this week, the company brought its matchmaking service to the U. S. , OK? Plenty of other dating apps already exist here. Facebook takes a lot of features from those apps and could easily drive the competitors out of business not because Facebook is offering a better service but because Facebook's so dominant, startups can't compete for attention. Some advertisers have expressed concern about how much control Facebook has over the market. The AGs will take a look into that, too. MARTIN: Is this just about Facebook? Or are there other companies targeted here? SHAHANI: So another set of AGs is looking at Google. And that's being led by the Texas attorney general, a Republican, Ken Paxton. He's leading a multi-state probe into Google, according to The Wall Street Journal. Back in June, Paxton raised the concern that information about how people spend their lives - GPS location data from Google Maps or Waze, what we search for, what we view on YouTube. This data, he said, has become extremely valuable, especially when aggregated into large sets and analyzed and packaged for targeted marketing. And he was concerned that the biggest platforms like Google's YouTube don't have an incentive to protect consumers. You know, just this week, Google settled a case for covertly and illegally tracking little kids as they're watching \"Peppa Pig. \" Paxton and 42 other AGs asked the Federal Trade Commission to work closely with them to look at predatory conduct and anti-competitive practices. So presumably, that's the effort he's leading. MARTIN: So, you know, consumers have been hearing these stories about Facebook or Google for a long time now, right? And I just wonder if you're seeing that take any - have any kind of effect because it seems to me every bad news story that comes out about these companies, consumers are, like, well, I get it. But it's just so integral in my life. It's convenient. It helps me keep in touch with family and friends. SHAHANI: Right. But I actually think that that picture is changing, particularly as we start to understand the real stakes of it, right? There was a school of thought that maybe is still out there a bit of, hey, it's free, so it can't be bad for me. I don't pay for Facebook or YouTube. But then we have to note, Rachel, that we're entering a new chapter in history, right? That chapter is the data economy. The companies that own information, which they will then sell to other businesses - they have a whole lot of control. They can set prices. They can wipe out entire sectors. They can control what we believe and think to be true. I mean, we've reported on this repeatedly. MARTIN: Right. SHAHANI: So while the individual consumer right now might not feel harm in the moment, pain in the moment, in the long run, there could be a massive transfer of wealth and power that hurts us all. So I think that the picture is changing there. I would add that Google says it's working with regulators, including AGs. MARTIN: All right. NPR's Aarti Shahani. Aarti, thanks. We appreciate it. SHAHANI: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-07-758337388": {"title": "Americans Prepare To Safeguard 2020 Vote. Is It Too Much \u2014 Or Will It Be Enough? : NPR", "url": "https://www.npr.org/2019/09/07/758337388/will-a-massive-effort-to-secure-the-2020-vote-end-up-superfluous-or-not-enough", "author": "No author found", "published_date": "2019-09-07", "content": "", "section": "2020 Election: Secure Your Vote", "disclaimer": ""}, "2019-09-08-758858687": {"title": "Troll Watch: Deepfakes And 2020 : NPR", "url": "https://www.npr.org/2019/09/08/758858687/troll-watch-deepfakes-and-2020", "author": "No author found", "published_date": "2019-09-08", "content": "MICHEL MARTIN, HOST: We know that our last presidential election in 2016 was the target of trolls - both domestic and foreign players who set out to sway and divide voters with lies and distortions spread through social media. This week, a new report outlines the threats we should be watching out for in 2020 and where they're coming from. PAUL BARRETT: To begin with, I think the Russians will be back. But joining them this time around, I fear, may be the Iranians, who have already been here and started to spread disinformation on social media - and perhaps the Chinese as well. MARTIN: That's the report's author, Paul Barrett of the Center for Business and Human Rights at NYU's Stern School for Business (ph). He warns that the threat from domestic disinformation is also growing. BARRETT: The volume of material coming from within the United States is undoubtedly greater than that that comes from abroad. In other words, we're doing this to ourselves. MARTIN: But what exactly should we be watching for as the 2020 election approaches? That's what we want to talk about in our regular Troll Watch segment. (SOUNDBITE OF MUSIC)MARTIN: Back in 2016, it was Facebook and Twitter bots that were the main sources of election-related disinformation. Now, though, Paul Barrett says, there are other platforms to worry about. BARRETT: Well, we identified Instagram as being in particular probably the one to watch the most closely. Disinformation is increasingly accomplished by means of images as opposed to text, and Instagram, of course, specializes in images. Facebook, which is Instagram's parent, has not done as much with Instagram in terms of protecting users as it has with the main Facebook platform, and that's another reason to worry about Instagram. In addition, WhatsApp, which is a different kind of platform - a messaging platform as opposed to a public posting platform - was instrumental in disinformation being spread in the presidential elections in Brazil and in India. And for that reason, I think it bears watching here as well. MARTIN: So I want to talk a little bit more about how this is actually done. And you mentioned deepfake technology as a major threat. And these are, you know, fake videos or audio that can be manipulated to make somebody do or say something they actually did not do or say. And I just want to play a clip - this is something that comedian, director and producer Jordan Peele made last year with Buzzfeed to warn about the dangers of deepfakes and disinformation. And this - his voice impersonation is superimposed onto the video image of former President Barack Obama. (SOUNDBITE OF ARCHIVED RECORDING)JORDAN PEELE: We're entering an era in which our enemies can make it look like anyone is saying anything at any point in time, even if they would never say those things. For instance, they could have me say things like Killmonger was right. MARTIN: Killmonger, of course, being a reference to the antagonist in \"Black Panther\" - the hit, \"Black Panther. \" Talk about why this is such a great threat now. BARRETT: Yeah. It's a great threat now because disinformation is increasingly an image game as opposed to just a text game. That's the first step. The second step is technological. The artificial intelligence that's used to make deepfake videos has been advancing steadily and is now readily available in open-source form so that anyone with any - you know, basic coding talent and a laptop and the desire to mess around with elections can begin to cobble together these very convincing but fake videos. And the companies are aware of this and are scrambling, perhaps belatedly, to try to respond to it. MARTIN: So I'm going to ask you just in the time we have left to just spread the responsibility around. Is there - are there things that social media companies should be doing right now to protect the public from misleading information? Are there things that regulators should be doing? And are there things that citizens should be doing? BARRETT: Well, let's start with citizens first. People have to be very skeptical about what they look at, how they react to it and whether they want to share it. I think there are things for Congress to do, but they're generally fairly narrow things. There's legislation called the Honest Ads Act that's pending that could vastly increase the amount of disclosure that goes with political advertising online. I think that would be a very good law to be passed. There's also legislation pending that would more severely punish voter suppression disinformation, which is one of the main and most pernicious forms of disinformation. As for the companies, I think one thing they could do is act more aggressively against provably false information, which when it's identified at present, is generally just demoted or D-ranked, which means that it's distributed to fewer people and possibly labeled or annotated. I would argue that they ought to just take that kind of material off their sites altogether. MARTIN: That's Paul Barrett with the NYU Stern School for Business (ph). His report outlines the disinformation threats to the 2020 presidential election. Paul Barrett, thanks so much for joining us. BARRETT: My pleasure. Thank you. MICHEL MARTIN, HOST:  We know that our last presidential election in 2016 was the target of trolls - both domestic and foreign players who set out to sway and divide voters with lies and distortions spread through social media. This week, a new report outlines the threats we should be watching out for in 2020 and where they're coming from. PAUL BARRETT: To begin with, I think the Russians will be back. But joining them this time around, I fear, may be the Iranians, who have already been here and started to spread disinformation on social media - and perhaps the Chinese as well. MARTIN: That's the report's author, Paul Barrett of the Center for Business and Human Rights at NYU's Stern School for Business (ph). He warns that the threat from domestic disinformation is also growing. BARRETT: The volume of material coming from within the United States is undoubtedly greater than that that comes from abroad. In other words, we're doing this to ourselves. MARTIN: But what exactly should we be watching for as the 2020 election approaches? That's what we want to talk about in our regular Troll Watch segment. (SOUNDBITE OF MUSIC) MARTIN: Back in 2016, it was Facebook and Twitter bots that were the main sources of election-related disinformation. Now, though, Paul Barrett says, there are other platforms to worry about. BARRETT: Well, we identified Instagram as being in particular probably the one to watch the most closely. Disinformation is increasingly accomplished by means of images as opposed to text, and Instagram, of course, specializes in images. Facebook, which is Instagram's parent, has not done as much with Instagram in terms of protecting users as it has with the main Facebook platform, and that's another reason to worry about Instagram. In addition, WhatsApp, which is a different kind of platform - a messaging platform as opposed to a public posting platform - was instrumental in disinformation being spread in the presidential elections in Brazil and in India. And for that reason, I think it bears watching here as well. MARTIN: So I want to talk a little bit more about how this is actually done. And you mentioned deepfake technology as a major threat. And these are, you know, fake videos or audio that can be manipulated to make somebody do or say something they actually did not do or say. And I just want to play a clip - this is something that comedian, director and producer Jordan Peele made last year with Buzzfeed to warn about the dangers of deepfakes and disinformation. And this - his voice impersonation is superimposed onto the video image of former President Barack Obama. (SOUNDBITE OF ARCHIVED RECORDING) JORDAN PEELE: We're entering an era in which our enemies can make it look like anyone is saying anything at any point in time, even if they would never say those things. For instance, they could have me say things like Killmonger was right. MARTIN: Killmonger, of course, being a reference to the antagonist in \"Black Panther\" - the hit, \"Black Panther. \" Talk about why this is such a great threat now. BARRETT: Yeah. It's a great threat now because disinformation is increasingly an image game as opposed to just a text game. That's the first step. The second step is technological. The artificial intelligence that's used to make deepfake videos has been advancing steadily and is now readily available in open-source form so that anyone with any - you know, basic coding talent and a laptop and the desire to mess around with elections can begin to cobble together these very convincing but fake videos. And the companies are aware of this and are scrambling, perhaps belatedly, to try to respond to it. MARTIN: So I'm going to ask you just in the time we have left to just spread the responsibility around. Is there - are there things that social media companies should be doing right now to protect the public from misleading information? Are there things that regulators should be doing? And are there things that citizens should be doing? BARRETT: Well, let's start with citizens first. People have to be very skeptical about what they look at, how they react to it and whether they want to share it. I think there are things for Congress to do, but they're generally fairly narrow things. There's legislation called the Honest Ads Act that's pending that could vastly increase the amount of disclosure that goes with political advertising online. I think that would be a very good law to be passed. There's also legislation pending that would more severely punish voter suppression disinformation, which is one of the main and most pernicious forms of disinformation. As for the companies, I think one thing they could do is act more aggressively against provably false information, which when it's identified at present, is generally just demoted or D-ranked, which means that it's distributed to fewer people and possibly labeled or annotated. I would argue that they ought to just take that kind of material off their sites altogether. MARTIN: That's Paul Barrett with the NYU Stern School for Business (ph). His report outlines the disinformation threats to the 2020 presidential election. Paul Barrett, thanks so much for joining us. BARRETT: My pleasure. Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-09-759100352": {"title": "With Next Goal To Secure 2020 Elections, Feds Seek To Absorb Lessons From 2016 : NPR", "url": "https://www.npr.org/2019/09/09/759100352/with-next-goal-to-secure-2020-elections-feds-seek-to-absorb-lessons-from-2016", "author": "No author found", "published_date": "2019-09-09", "content": "", "section": "2020 Election: Secure Your Vote", "disclaimer": ""}, "2019-09-09-759089460": {"title": "48 States Investigating Whether Google's Dominance Hurts Competition  : NPR", "url": "https://www.npr.org/2019/09/09/759089460/48-states-investigating-whether-googles-dominance-hurts-competition", "author": "No author found", "published_date": "2019-09-09", "content": "AILSA CHANG, HOST: Google is on notice. Fifty attorneys general are teaming up to investigate the tech giant for anti-competitive practices and harm to consumers. State Attorney General Ken Paxton of Texas launched the investigation from the steps of the Supreme Court this afternoon. (SOUNDBITE OF ARCHIVED RECORDING)KEN PAXTON: What we've all learned is that while many consumers believe that the Internet is free, certainly, we know from Google's profits that the Internet is not free. CHANG: A closer look on today's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CHANG: All right, here to debrief with us is NPR's Aarti Shahani. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. CHANG: So it seems like every week now the government is opening some new investigation into big tech. It's really been a steady drumbeat. Can you just tell us how this investigation feels different to you? SHAHANI: Yeah, no, it was definitely a show of force. And as you said, 50 attorneys general are launching an investigation. That's 48 states, Ailsa, as well as the District of Columbia and Puerto Rico. The two states sitting it out for now are California and Alabama, and they could still join in, OK? The group has already subpoenaed Google, though to be clear, this is not a top-to-bottom probe of everything that Google or its parent company, Alphabet, does. It's a look at Google's search and advertising and how those two services may be intertwined in ways that are overall harmful to consumers. To explain what that means, have a listen to one of the AGs who spoke this afternoon, Arkansas Attorney General Leslie Rutledge. (SOUNDBITE OF ARCHIVED RECORDING)LESLIE RUTLEDGE: As a new mom, when my daughter is sick and I search online for advice or doctors, I want the best advice from the best doctors, not the doctor and not the clinic who can spend the most on advertising. SHAHANI: So she and other AGs are worried that Google's so-called free service comes at a cost to consumers, as well as to companies who fear they're frozen out of business unless they pay Google. The European Union has made this point, too, and move to fine Google billions of dollars. The AGs referred to Europe repeatedly today. CHANG: OK, and we should note that Google is an NPR sponsor. So what has Google been saying in response to this new investigation? SHAHANI: Yeah, so Google declined to comment on the latest announcement. But on Friday, a top Googler on public policy, Kent Walker, put out a blog saying things that were science fiction a few years ago are now free for everyone, meaning Google's making your life better, helping you answer any question you have, putting out an incredibly powerful tool to translate just about any language you can think of. So I would point out that this investigation, and also the one into Facebook that nine AGs announced last week - they're looking at the same issue. It's digital advertising and the decisions people make based on what they're seeing on the Internet. Digital ads are Google and Facebook's cash cow. Those two companies together control nearly 60% of digital ad sales. And according to eMarketer, their control this year is actually dipping a bit because of Amazon's rise. But still, it's a lot of control. CHANG: All right, so as this investigation unfolds, what are you going to be looking for? SHAHANI: Whether it expands to questions, for example, on consumer data and privacy. You know, I would also point out there's a change in the mindset right now. The Texas attorney general's office in 2011 investigated Google for its search engine and decided not to pursue a case. But now, people understand the Internet is where commerce happens. They are the new railroads, and railroads didn't regulate themselves. CHANG: That's NPR's Aarti Shahani. Thanks, Aarti. SHAHANI: Thank you. AILSA CHANG, HOST:  Google is on notice. Fifty attorneys general are teaming up to investigate the tech giant for anti-competitive practices and harm to consumers. State Attorney General Ken Paxton of Texas launched the investigation from the steps of the Supreme Court this afternoon. (SOUNDBITE OF ARCHIVED RECORDING) KEN PAXTON: What we've all learned is that while many consumers believe that the Internet is free, certainly, we know from Google's profits that the Internet is not free. CHANG: A closer look on today's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CHANG: All right, here to debrief with us is NPR's Aarti Shahani. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. CHANG: So it seems like every week now the government is opening some new investigation into big tech. It's really been a steady drumbeat. Can you just tell us how this investigation feels different to you? SHAHANI: Yeah, no, it was definitely a show of force. And as you said, 50 attorneys general are launching an investigation. That's 48 states, Ailsa, as well as the District of Columbia and Puerto Rico. The two states sitting it out for now are California and Alabama, and they could still join in, OK? The group has already subpoenaed Google, though to be clear, this is not a top-to-bottom probe of everything that Google or its parent company, Alphabet, does. It's a look at Google's search and advertising and how those two services may be intertwined in ways that are overall harmful to consumers. To explain what that means, have a listen to one of the AGs who spoke this afternoon, Arkansas Attorney General Leslie Rutledge. (SOUNDBITE OF ARCHIVED RECORDING) LESLIE RUTLEDGE: As a new mom, when my daughter is sick and I search online for advice or doctors, I want the best advice from the best doctors, not the doctor and not the clinic who can spend the most on advertising. SHAHANI: So she and other AGs are worried that Google's so-called free service comes at a cost to consumers, as well as to companies who fear they're frozen out of business unless they pay Google. The European Union has made this point, too, and move to fine Google billions of dollars. The AGs referred to Europe repeatedly today. CHANG: OK, and we should note that Google is an NPR sponsor. So what has Google been saying in response to this new investigation? SHAHANI: Yeah, so Google declined to comment on the latest announcement. But on Friday, a top Googler on public policy, Kent Walker, put out a blog saying things that were science fiction a few years ago are now free for everyone, meaning Google's making your life better, helping you answer any question you have, putting out an incredibly powerful tool to translate just about any language you can think of. So I would point out that this investigation, and also the one into Facebook that nine AGs announced last week - they're looking at the same issue. It's digital advertising and the decisions people make based on what they're seeing on the Internet. Digital ads are Google and Facebook's cash cow. Those two companies together control nearly 60% of digital ad sales. And according to eMarketer, their control this year is actually dipping a bit because of Amazon's rise. But still, it's a lot of control. CHANG: All right, so as this investigation unfolds, what are you going to be looking for? SHAHANI: Whether it expands to questions, for example, on consumer data and privacy. You know, I would also point out there's a change in the mindset right now. The Texas attorney general's office in 2011 investigated Google for its search engine and decided not to pursue a case. But now, people understand the Internet is where commerce happens. They are the new railroads, and railroads didn't regulate themselves. CHANG: That's NPR's Aarti Shahani. Thanks, Aarti. SHAHANI: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-10-759500972": {"title": "Apple TV+ Will Cost $4.99 Per Month, Undercutting Netflix And Others : NPR", "url": "https://www.npr.org/2019/09/10/759500972/apple-launches-video-streaming-service-for-4-99-a-month", "author": "No author found", "published_date": "2019-09-10", "content": "", "section": "Technology", "disclaimer": ""}, "2019-09-10-759296492": {"title": "Chelsea Manning Comments Publicly On The Late Hacker Adrian Lamo : NPR", "url": "https://www.npr.org/2019/09/10/759296492/chelsea-manning-comments-publicly-on-the-late-hacker-adrian-lamo", "author": "No author found", "published_date": "2019-09-10", "content": "DAVID GREENE, HOST: When Chelsea Manning leaked classified information to WikiLeaks, she also reached out to someone else - Adrian Lamo, who was once revered in the hacker community. Lamo turned Manning into authorities when she revealed herself as the leaker in 2010, and then last year, Lamo died under strange circumstances. As part of an NPR special series, Dina Temple-Raston looks into that mysterious death. DINA TEMPLE-RASTON, BYLINE: If you were to trace the U. S. government's case against Julian Assange, you probably would have to start with this. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: . . . Have individuals with weapons. UNIDENTIFIED PERSON #2: . . . Four radio. UNIDENTIFIED PERSON #1: Yep, he's got a weapon, too. TEMPLE-RASTON: That's from a classified military video former Army intelligence analyst Chelsea Manning gave to WikiLeaks nine years ago. You may remember it. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #3: I got a black vehicle under target. It's arriving right to the north of the mosque. UNIDENTIFIED PERSON #4: Yeah, I would like that. Over. TEMPLE-RASTON: These are the actual conversations between two Apache helicopter pilots. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: Hotel 26, this is Crazy Horse 18. UNIDENTIFIED PERSON #3: All right, We got a guy with an RPG. TEMPLE-RASTON: We know now that the video captured a tragic mistake. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: I'm going to fire. UNIDENTIFIED PERSON #3: You're clear. UNIDENTIFIED PERSON #1: All right, firing. UNIDENTIFIED PERSON #3: Light them all up. UNIDENTIFIED PERSON #1: Come on, fire. (SOUNDBITE OF GUNFIRE)UNIDENTIFIED PERSON #3: Keep shooting. (SOUNDBITE OF GUNFIRE)TEMPLE-RASTON: The American pilots killed a group of 12 civilians on a Baghdad street. When the video was leaked, it raised new questions about civilian casualties and rules of engagement in Iraq. It also marked the beginning of something else - the weaponization of the Internet. A few months after the release, Chelsea Manning started an online chat with a hacker named Adrian Lamo, and she revealed that she not only leaked the video but hundreds of thousands of other classified documents as well. She said she had been talking to a, quote, \"crazy, white-haired Aussie who can't stay in one country very long,\" unquote. She meant Julian Assange. Lamo couldn't keep this information to himself. GLENN MORROW: Once it became clear that it was such a serious thing that had happened, I don't think he could stand by and live with the implications of just sitting on it. TEMPLE-RASTON: That's Adrian Lamo's cousin, Glenn Morrow. MORROW: I don't think he anticipated, when he started, the gravity of what Manning was actually saying. TEMPLE-RASTON: According to official case documents obtained by NPR, investigators testified that Lamo placed two phone calls shortly after chatting with Manning. One call went to a friend of his, a former Army intelligence officer named Tim Webster. And according to the documents, Webster called the FBI shortly after hanging up with Lamo. Despite repeated attempts to contact him, Webster declined to comment for this story. Lamo's friends told NPR that it's unlikely Webster called the authorities without asking Lamo first. So that's the first phone call. Lamo, according to documents, then placed a second call, this time to a business partner. A short time later, the business partner left a message on the Army's Criminal Investigative Division tip line. That was on May 23, 2010. By the end of that week, Manning was in custody in Iraq, accused of one of the largest leaks of classified records in American history. Many supported Manning. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PROTESTER: Free. UNIDENTIFIED PROTESTERS: Manning. UNIDENTIFIED PROTESTER: Free. UNIDENTIFIED PROTESTERS: Manning. UNIDENTIFIED PROTESTER: Free. UNIDENTIFIED PROTESTERS: Manning. TEMPLE-RASTON: But many also blamed Lamo for turning her in. ANDREW BLAKE: People hated him. He couldn't log on to any sort of Internet platform under his actual name without instantly getting some sort of hate directed toward him. TEMPLE-RASTON: Andrew Blake was a friend of Adrian Lamo's. And before he met Manning, Lamo was a famous hacker. After Manning was arrested, Lamo was labeled a snitch and found himself on the receiving end of death threats. BLAKE: Even when Adrian would do something with the absolute best of intentions, as soon as anyone realized that it was Adrian Lamo who did it, they didn't want anything to do with it. TEMPLE-RASTON: Then Lamo turned up dead. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #5: 911 Wichita. TEMPLE-RASTON: They found his body March 14, 2018. He'd been dead for days. In the state of Kansas, medical examiners have five categories for determining a cause of death - natural, accident, suicide, homicide or, in Lamo's case, the most unsatisfying, undetermined. SCOTT KIPPER: It's certainly possible to have a known cause of death but still have an undetermined manner of death. TEMPLE-RASTON: That's the local deputy medical examiner, Scott Kipper. He did Lamo's autopsy. KIPPER: So example - if we find a body at the base of a tall building and it looks like he fell off the building, I can bring the body in. I can document the injuries. What I can't tell you is how he ended up on the sidewalk. TEMPLE-RASTON: It wasn't just Lamo's celebrity that made this a mystery; he was living in Kansas, a place to which he had no previous connection. He was found dead in a senior living facility, but he was only 37 years old. There were rumors of his working for the government, only fueled by a mysterious sticker the medical examiner found on his thigh. It read, Project Vigilant and had a Washington, D. C. , address. As we retraced Lamo's steps during the last two years of his life, we found simple explanations for everything. He came to Kansas to live with a friend's parents. He was in a senior living facility because he qualified for low-income housing, and there were apartments available there. And Project Vigilant, it was a business he had started with that partner who called the authorities about Manning so many years ago. But the business never really went anywhere. In the end, our reporting found there were no assassins lying in wait, no government officials eager for a briefing. Adrian Lamo was profoundly alone. Andrew Blake again. BLAKE: I think just people tended to associate Adrian with the Adrian who snitched on Manning, not the Adrian who did a whole bunch of cool other stuff. TEMPLE-RASTON: Blake said Adrian wasn't so much forgotten as unforgiven, which is ironic because a couple of months ago, we reached out to Chelsea Manning. We wondered, after all these years, how she felt about Lamo. She'd never commented about him on the record - until now. Quote, \"I've never had any ill will toward Adrian at any time,\" unquote, she wrote NPR in a note from jail. And then she added, quote, \"I'm more mad at the government for using him. \" Julian Assange's extradition hearing is set for February 25, 2020. Chelsea Manning remains in a Virginia jail for refusing to testify against him. Dina Temple-Raston, NPR News, Washington. (SOUNDBITE OF THE CINEMATIC ORCHESTRA'S \"REEL LIFE (EVOLUTION II)\") DAVID GREENE, HOST:  When Chelsea Manning leaked classified information to WikiLeaks, she also reached out to someone else - Adrian Lamo, who was once revered in the hacker community. Lamo turned Manning into authorities when she revealed herself as the leaker in 2010, and then last year, Lamo died under strange circumstances. As part of an NPR special series, Dina Temple-Raston looks into that mysterious death. DINA TEMPLE-RASTON, BYLINE: If you were to trace the U. S. government's case against Julian Assange, you probably would have to start with this. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: . . . Have individuals with weapons. UNIDENTIFIED PERSON #2: . . . Four radio. UNIDENTIFIED PERSON #1: Yep, he's got a weapon, too. TEMPLE-RASTON: That's from a classified military video former Army intelligence analyst Chelsea Manning gave to WikiLeaks nine years ago. You may remember it. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #3: I got a black vehicle under target. It's arriving right to the north of the mosque. UNIDENTIFIED PERSON #4: Yeah, I would like that. Over. TEMPLE-RASTON: These are the actual conversations between two Apache helicopter pilots. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: Hotel 26, this is Crazy Horse 18. UNIDENTIFIED PERSON #3: All right, We got a guy with an RPG. TEMPLE-RASTON: We know now that the video captured a tragic mistake. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: I'm going to fire. UNIDENTIFIED PERSON #3: You're clear. UNIDENTIFIED PERSON #1: All right, firing. UNIDENTIFIED PERSON #3: Light them all up. UNIDENTIFIED PERSON #1: Come on, fire. (SOUNDBITE OF GUNFIRE) UNIDENTIFIED PERSON #3: Keep shooting. (SOUNDBITE OF GUNFIRE) TEMPLE-RASTON: The American pilots killed a group of 12 civilians on a Baghdad street. When the video was leaked, it raised new questions about civilian casualties and rules of engagement in Iraq. It also marked the beginning of something else - the weaponization of the Internet. A few months after the release, Chelsea Manning started an online chat with a hacker named Adrian Lamo, and she revealed that she not only leaked the video but hundreds of thousands of other classified documents as well. She said she had been talking to a, quote, \"crazy, white-haired Aussie who can't stay in one country very long,\" unquote. She meant Julian Assange. Lamo couldn't keep this information to himself. GLENN MORROW: Once it became clear that it was such a serious thing that had happened, I don't think he could stand by and live with the implications of just sitting on it. TEMPLE-RASTON: That's Adrian Lamo's cousin, Glenn Morrow. MORROW: I don't think he anticipated, when he started, the gravity of what Manning was actually saying. TEMPLE-RASTON: According to official case documents obtained by NPR, investigators testified that Lamo placed two phone calls shortly after chatting with Manning. One call went to a friend of his, a former Army intelligence officer named Tim Webster. And according to the documents, Webster called the FBI shortly after hanging up with Lamo. Despite repeated attempts to contact him, Webster declined to comment for this story. Lamo's friends told NPR that it's unlikely Webster called the authorities without asking Lamo first. So that's the first phone call. Lamo, according to documents, then placed a second call, this time to a business partner. A short time later, the business partner left a message on the Army's Criminal Investigative Division tip line. That was on May 23, 2010. By the end of that week, Manning was in custody in Iraq, accused of one of the largest leaks of classified records in American history. Many supported Manning. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PROTESTER: Free. UNIDENTIFIED PROTESTERS: Manning. UNIDENTIFIED PROTESTER: Free. UNIDENTIFIED PROTESTERS: Manning. UNIDENTIFIED PROTESTER: Free. UNIDENTIFIED PROTESTERS: Manning. TEMPLE-RASTON: But many also blamed Lamo for turning her in. ANDREW BLAKE: People hated him. He couldn't log on to any sort of Internet platform under his actual name without instantly getting some sort of hate directed toward him. TEMPLE-RASTON: Andrew Blake was a friend of Adrian Lamo's. And before he met Manning, Lamo was a famous hacker. After Manning was arrested, Lamo was labeled a snitch and found himself on the receiving end of death threats. BLAKE: Even when Adrian would do something with the absolute best of intentions, as soon as anyone realized that it was Adrian Lamo who did it, they didn't want anything to do with it. TEMPLE-RASTON: Then Lamo turned up dead. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #5: 911 Wichita. TEMPLE-RASTON: They found his body March 14, 2018. He'd been dead for days. In the state of Kansas, medical examiners have five categories for determining a cause of death - natural, accident, suicide, homicide or, in Lamo's case, the most unsatisfying, undetermined. SCOTT KIPPER: It's certainly possible to have a known cause of death but still have an undetermined manner of death. TEMPLE-RASTON: That's the local deputy medical examiner, Scott Kipper. He did Lamo's autopsy. KIPPER: So example - if we find a body at the base of a tall building and it looks like he fell off the building, I can bring the body in. I can document the injuries. What I can't tell you is how he ended up on the sidewalk. TEMPLE-RASTON: It wasn't just Lamo's celebrity that made this a mystery; he was living in Kansas, a place to which he had no previous connection. He was found dead in a senior living facility, but he was only 37 years old. There were rumors of his working for the government, only fueled by a mysterious sticker the medical examiner found on his thigh. It read, Project Vigilant and had a Washington, D. C. , address. As we retraced Lamo's steps during the last two years of his life, we found simple explanations for everything. He came to Kansas to live with a friend's parents. He was in a senior living facility because he qualified for low-income housing, and there were apartments available there. And Project Vigilant, it was a business he had started with that partner who called the authorities about Manning so many years ago. But the business never really went anywhere. In the end, our reporting found there were no assassins lying in wait, no government officials eager for a briefing. Adrian Lamo was profoundly alone. Andrew Blake again. BLAKE: I think just people tended to associate Adrian with the Adrian who snitched on Manning, not the Adrian who did a whole bunch of cool other stuff. TEMPLE-RASTON: Blake said Adrian wasn't so much forgotten as unforgiven, which is ironic because a couple of months ago, we reached out to Chelsea Manning. We wondered, after all these years, how she felt about Lamo. She'd never commented about him on the record - until now. Quote, \"I've never had any ill will toward Adrian at any time,\" unquote, she wrote NPR in a note from jail. And then she added, quote, \"I'm more mad at the government for using him. \" Julian Assange's extradition hearing is set for February 25, 2020. Chelsea Manning remains in a Virginia jail for refusing to testify against him. Dina Temple-Raston, NPR News, Washington. (SOUNDBITE OF THE CINEMATIC ORCHESTRA'S \"REEL LIFE (EVOLUTION II)\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-11-759691730": {"title": "California Lawmakers Advance Bill To Redefine And Protect Gig Economy Workers : NPR", "url": "https://www.npr.org/2019/09/11/759691730/california-lawmakers-advance-bill-to-redefine-and-protect-gig-workers", "author": "No author found", "published_date": "2019-09-11", "content": "", "section": "National", "disclaimer": ""}, "2019-09-12-760121373": {"title": "Edward Snowden Tells NPR: 'I Have Been Criticizing The Russian Government' : NPR", "url": "https://www.npr.org/2019/09/12/760121373/edward-snowden-tells-npr-i-have-been-criticizing-the-russian-government", "author": "No author found", "published_date": "2019-09-12", "content": "SCOTT SIMON, HOST: Edward Snowden has written a book. It is a memoir, a coming-of-age-with-the-Internet story, a spy tale and, his critics would say, an attempt to try to justify betraying his country, by a man who was charged in 2013 with two counts of violating the Espionage Act and with theft of government property - confidential, national security information. Mr. Snowden's book is \"Permanent Record. \" Edward Snowden joins us now from Moscow. Thanks so much for being with us. EDWARD SNOWDEN: Thank you for having me on. SIMON: I think a lot of people don't want to hear anything you have to say until I've asked you this question. Are you being used by Vladimir Putin? SNOWDEN: (Laughter) No, I don't think so. When people look at this, you know, particularly with Russia in the news as much as it is, there's always this cloud of suspicion that's leveled against anybody who can be, in the most stretched way, associated with Russia. It wasn't my choice to be in Russia. SIMON: Most stretched way - you're living there in Moscow. You have been for six years. SNOWDEN: Right, but it was not my choice to be here. And this is what people forget. I applied for asylum in 27 different countries around the world, and it was the government, the United States government, then-Secretary John Kerry, that canceled my passport as I was leaving from Hong Kong en route to Ecuador. And this locked me in place. I believe they panicked. And I think the reason that I'm in Russia today is because what we know - this was actually publicly reported in 2013. Every time one of these other countries, one that the United States public would be much more comfortable with - a France, a Norway, a Germany - one of two people would call the Foreign Ministry of that country. And it would be either Secretary of State John Kerry or then-Vice President Joe Biden. The idea here is they would go, look; we understand that he has been charged with political crimes. This means you don't qualify for extradition, and you almost always do qualify for asylum protections. And the government - we know you can do this, but if you do, we want you to understand there will be a response. We're not going to say what it will be, but it will be severe because we don't want to see the public seeing this guy as a whistleblower, which the public then was coming around to do. SIMON: You say the U. S. government panicked. Did the U. S. government panic, or just they felt it was important to the national interest of the United States to make certain you - your movement was limited? SNOWDEN: What is the thing they are arguing is in the interest of the United States here? Sort of like in your introduction, you say some people say I betrayed the United States. Well, how did I betray the United States? All of my information was given to the American public through some of the most trusted institutions in journalism, institutions like The Washington Post. Now, as a condition of access to this archive material, these journalists were required not to publish any story that they thought was harmful, no story simply because it was interesting, no story simply because it was newsworthy - only stories that they were willing to make an institutional argument and stand for. It was in the public interest to know. And here, as an extraordinary safeguard on top of this, I required each of the journalists working with this material in advance of publication to go to the government before they ran stories. And this is why in 2013 we heard exactly the extraordinary rhetoric that you raised before. But now in 2019, we don't hear this anymore. We have seen the laws changed. We have seen the programs changed. And we have even seen officials in the United States intelligence community - former Deputy Director Richard Ledgett, for example - say that he thought the NSA had made a mistake in concealing this program, the particularly unconstitutional phone records program, because he believed that if the NSA hadn't played these secrecy games and denied the American people the right to know, much less the right to vote on it, they could've won that sort of persuasive argument. But they didn't do it. They had made a mistake, and it had harmed the rights of everyone in the United States and everyone around the world as a consequence. And they call me the traitor? SIMON: You recount in this book how Russian intelligence representatives met you at the airport in Moscow and said to you - I'm going to quote from your own book - \"life for a person in your situation can be very difficult without friends who can help. Is there some information you could share with us? \" You're there six years later. You can see why people might be suspicious, can't you? SNOWDEN: I was trapped in that airport for 40 days. If I had played ball, I would've left Day 1 in a limo. You know, I would've been living in a palace. You would see them giving me parades in Red Square. The reality is this. I had destroyed my access to all of the classified material that I provided to journalists before leaving Hong Kong precisely because I didn't know what was going to happen next. SIMON: Are you - at the same time, though, you're in Moscow. Are you, a very smart man, naive to think that Vladimir Putin is going to give you asylum without expecting something in return? SNOWDEN: All throughout the Cold War in the United States, we protected dissidents from the Soviet government. These are, you know, writers. These are speakers. These are physicists. These are not people who can benefit the United States government even if they had wanted to. And we protected them nonetheless because of the message it sent. Now, the Russian government doesn't get many chances in this context internationally, on the global stage, to do the right thing. I have been criticizing the Russian government while I am here. What more can I do to satisfy you or any of these critics who hold these positions? The reality is there is nothing that will satisfy them because it is their suspicion, it is their skepticism, it is their distrust of the Russian government as an institution which is motivating this. SIMON: I mean, do I have to detail for you the ways in which the Putin government has earned (laughter) some suspicion? SNOWDEN: No, no. Absolutely not. Again, I agree with you. This (laughter) - look, look; this is why I have been criticizing the Russian government. There's no distance between us on that. I'm not saying Vladimir Putin is an angel. I'm not even saying Vladimir Putin is a decent guy. What I'm saying is you have to understand there doesn't need to be a quid pro quo here for it to make sense. SIMON: Edward Snowden. Elsewhere in the program, he talks about his work at the NSA before he leaked classified information and tells us what's keeping him from returning to the U. S. to face trial. (SOUNDBITE OF MUSIC) SCOTT SIMON, HOST:  Edward Snowden has written a book. It is a memoir, a coming-of-age-with-the-Internet story, a spy tale and, his critics would say, an attempt to try to justify betraying his country, by a man who was charged in 2013 with two counts of violating the Espionage Act and with theft of government property - confidential, national security information. Mr. Snowden's book is \"Permanent Record. \" Edward Snowden joins us now from Moscow. Thanks so much for being with us. EDWARD SNOWDEN: Thank you for having me on. SIMON: I think a lot of people don't want to hear anything you have to say until I've asked you this question. Are you being used by Vladimir Putin? SNOWDEN: (Laughter) No, I don't think so. When people look at this, you know, particularly with Russia in the news as much as it is, there's always this cloud of suspicion that's leveled against anybody who can be, in the most stretched way, associated with Russia. It wasn't my choice to be in Russia. SIMON: Most stretched way - you're living there in Moscow. You have been for six years. SNOWDEN: Right, but it was not my choice to be here. And this is what people forget. I applied for asylum in 27 different countries around the world, and it was the government, the United States government, then-Secretary John Kerry, that canceled my passport as I was leaving from Hong Kong en route to Ecuador. And this locked me in place. I believe they panicked. And I think the reason that I'm in Russia today is because what we know - this was actually publicly reported in 2013. Every time one of these other countries, one that the United States public would be much more comfortable with - a France, a Norway, a Germany - one of two people would call the Foreign Ministry of that country. And it would be either Secretary of State John Kerry or then-Vice President Joe Biden. The idea here is they would go, look; we understand that he has been charged with political crimes. This means you don't qualify for extradition, and you almost always do qualify for asylum protections. And the government - we know you can do this, but if you do, we want you to understand there will be a response. We're not going to say what it will be, but it will be severe because we don't want to see the public seeing this guy as a whistleblower, which the public then was coming around to do. SIMON: You say the U. S. government panicked. Did the U. S. government panic, or just they felt it was important to the national interest of the United States to make certain you - your movement was limited? SNOWDEN: What is the thing they are arguing is in the interest of the United States here? Sort of like in your introduction, you say some people say I betrayed the United States. Well, how did I betray the United States? All of my information was given to the American public through some of the most trusted institutions in journalism, institutions like The Washington Post. Now, as a condition of access to this archive material, these journalists were required not to publish any story that they thought was harmful, no story simply because it was interesting, no story simply because it was newsworthy - only stories that they were willing to make an institutional argument and stand for. It was in the public interest to know. And here, as an extraordinary safeguard on top of this, I required each of the journalists working with this material in advance of publication to go to the government before they ran stories. And this is why in 2013 we heard exactly the extraordinary rhetoric that you raised before. But now in 2019, we don't hear this anymore. We have seen the laws changed. We have seen the programs changed. And we have even seen officials in the United States intelligence community - former Deputy Director Richard Ledgett, for example - say that he thought the NSA had made a mistake in concealing this program, the particularly unconstitutional phone records program, because he believed that if the NSA hadn't played these secrecy games and denied the American people the right to know, much less the right to vote on it, they could've won that sort of persuasive argument. But they didn't do it. They had made a mistake, and it had harmed the rights of everyone in the United States and everyone around the world as a consequence. And they call me the traitor? SIMON: You recount in this book how Russian intelligence representatives met you at the airport in Moscow and said to you - I'm going to quote from your own book - \"life for a person in your situation can be very difficult without friends who can help. Is there some information you could share with us? \" You're there six years later. You can see why people might be suspicious, can't you? SNOWDEN: I was trapped in that airport for 40 days. If I had played ball, I would've left Day 1 in a limo. You know, I would've been living in a palace. You would see them giving me parades in Red Square. The reality is this. I had destroyed my access to all of the classified material that I provided to journalists before leaving Hong Kong precisely because I didn't know what was going to happen next. SIMON: Are you - at the same time, though, you're in Moscow. Are you, a very smart man, naive to think that Vladimir Putin is going to give you asylum without expecting something in return? SNOWDEN: All throughout the Cold War in the United States, we protected dissidents from the Soviet government. These are, you know, writers. These are speakers. These are physicists. These are not people who can benefit the United States government even if they had wanted to. And we protected them nonetheless because of the message it sent. Now, the Russian government doesn't get many chances in this context internationally, on the global stage, to do the right thing. I have been criticizing the Russian government while I am here. What more can I do to satisfy you or any of these critics who hold these positions? The reality is there is nothing that will satisfy them because it is their suspicion, it is their skepticism, it is their distrust of the Russian government as an institution which is motivating this. SIMON: I mean, do I have to detail for you the ways in which the Putin government has earned (laughter) some suspicion? SNOWDEN: No, no. Absolutely not. Again, I agree with you. This (laughter) - look, look; this is why I have been criticizing the Russian government. There's no distance between us on that. I'm not saying Vladimir Putin is an angel. I'm not even saying Vladimir Putin is a decent guy. What I'm saying is you have to understand there doesn't need to be a quid pro quo here for it to make sense. SIMON: Edward Snowden. Elsewhere in the program, he talks about his work at the NSA before he leaked classified information and tells us what's keeping him from returning to the U. S. to face trial. (SOUNDBITE OF MUSIC)", "section": "Author Interviews", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-12-759876637": {"title": "Lawsuits Say Lyft Doesn't Do Enough To Protect Women From Predatory Drivers : NPR", "url": "https://www.npr.org/2019/09/12/759876637/lawsuits-say-lyft-doesnt-do-enough-to-protect-women-from-predator-drivers", "author": "No author found", "published_date": "2019-09-12", "content": "", "section": "Criminal Justice Collaborative", "disclaimer": ""}, "2019-09-12-752341188": {"title": "School Safety Tech Raises Questions About Student Privacy : NPR", "url": "https://www.npr.org/2019/09/12/752341188/when-school-safety-becomes-school-surveillance", "author": "No author found", "published_date": "2019-09-12", "content": "", "section": "Education", "disclaimer": ""}, "2019-09-13-760564667": {"title": "Uber Jump Bikes To Be Pulled From Atlanta, San Diego : NPR", "url": "https://www.npr.org/2019/09/13/760564667/uber-to-pull-jump-bikes-from-two-cities", "author": "No author found", "published_date": "2019-09-13", "content": "", "section": "Business", "disclaimer": ""}, "2019-09-13-760478177": {"title": "Regulate Big Tech, Microsoft's Brad Smith Says; Democracy Is At Stake : NPR", "url": "https://www.npr.org/2019/09/13/760478177/microsoft-president-democracy-is-at-stake-regulate-big-tech", "author": "No author found", "published_date": "2019-09-13", "content": "MARY LOUISE KELLY, HOST: Regulate us, please. That is the unexpected message from one of the country's leading tech executives. In his new book, the president of Microsoft argues that governments need to put some guardrails around engineers and the tech titans they serve. NPR's Aarti Shahani spoke with him at Microsoft headquarters. And we should note Microsoft is an NPR sponsor. AARTI SHAHANI, BYLINE: Microsoft is not in the crosshairs right now. But when it was, back in the 1990s, Brad Smith was the man repeating that well-worn refrain about how regulation kills innovation. BRAD SMITH: And I go back. And I look at the things that we said. I look at the things I said 20 years ago. There were many things that we got wrong. SHAHANI: Smith has co-authored \"Tools And Weapons. \" Just like a knife, digital technology can be a tool or a weapon. According to Smith, the threat that Microsoft posed decades back was economic. Today the tech giants whose tools have been used to interfere in fair and free elections are posing a much bigger threat. SMITH: We need to work together. We need to work with governments to protect, frankly, something that is far more important than technology - democracy. It was here before us. It needs to be here and healthy after us. SHAHANI: Smith has proposals that are not popular in Silicon Valley. For one, he argues it's time to reform the U. S. law that says Internet platforms are not liable for just about any of the content running through their pipes - could be hate speech or death threats, ads for counterfeit goods or illegal guns. That law has enabled Microsoft's competitors, Facebook, Google's YouTube and Amazon, to grow at breakneck speed. SMITH: Almost no technology has gone so entirely unregulated for so long as digital technology. SHAHANI: Skeptics say that Smith's rhetoric masks an agenda to keep Microsoft on top. A tough law making Internet platforms accountable for content poses a greater threat to the competition than to Microsoft. Also, calling for regulation doesn't mean the strongest regulation. Earlier this year in its home state of Washington, Microsoft pushed back on a facial recognition bill that protected civil rights in favor of a less restrictive bill. And other tech chiefs, like Facebook's Mark Zuckerberg, have said there's a need for some regulation. But Smith goes further, deeper than his peers, by arguing that governments need to probe the fundamentals of the data economy. He poses prescient questions. Where does our data go? Who gets to call the shots on how our data gets used - the few companies, Microsoft included, that have collected it? SMITH: I worry that if all of the data on which the world relies is in the hands of a small number of tech companies, you're going to see a massive transfer of economic wealth. SHAHANI: This week, nearly every state attorney general in the U. S. joined an antitrust probe into Google. Last week, nine AGs joined one against Facebook. It'll be years before a court ruling, if any, lands. Smith encourages his fellow tech leaders to look for places where they can compromise. He's the one who pushed Gates and Microsoft to enter a settlement, which was hard emotionally. SMITH: There were days when people would say, why are you such a wimp? And the answer, in my view, was because it was the wrong fight to fight; that it often takes more courage to compromise than it does to keep fighting. SHAHANI: Geopolitical, business and marriage advice from Brad Smith. (LAUGHTER)SMITH: Well, at least two out of the three. SHAHANI: Maybe he's got another book in him. Aarti Shahani, NPR News, Redmond. MARY LOUISE KELLY, HOST:  Regulate us, please. That is the unexpected message from one of the country's leading tech executives. In his new book, the president of Microsoft argues that governments need to put some guardrails around engineers and the tech titans they serve. NPR's Aarti Shahani spoke with him at Microsoft headquarters. And we should note Microsoft is an NPR sponsor. AARTI SHAHANI, BYLINE: Microsoft is not in the crosshairs right now. But when it was, back in the 1990s, Brad Smith was the man repeating that well-worn refrain about how regulation kills innovation. BRAD SMITH: And I go back. And I look at the things that we said. I look at the things I said 20 years ago. There were many things that we got wrong. SHAHANI: Smith has co-authored \"Tools And Weapons. \" Just like a knife, digital technology can be a tool or a weapon. According to Smith, the threat that Microsoft posed decades back was economic. Today the tech giants whose tools have been used to interfere in fair and free elections are posing a much bigger threat. SMITH: We need to work together. We need to work with governments to protect, frankly, something that is far more important than technology - democracy. It was here before us. It needs to be here and healthy after us. SHAHANI: Smith has proposals that are not popular in Silicon Valley. For one, he argues it's time to reform the U. S. law that says Internet platforms are not liable for just about any of the content running through their pipes - could be hate speech or death threats, ads for counterfeit goods or illegal guns. That law has enabled Microsoft's competitors, Facebook, Google's YouTube and Amazon, to grow at breakneck speed. SMITH: Almost no technology has gone so entirely unregulated for so long as digital technology. SHAHANI: Skeptics say that Smith's rhetoric masks an agenda to keep Microsoft on top. A tough law making Internet platforms accountable for content poses a greater threat to the competition than to Microsoft. Also, calling for regulation doesn't mean the strongest regulation. Earlier this year in its home state of Washington, Microsoft pushed back on a facial recognition bill that protected civil rights in favor of a less restrictive bill. And other tech chiefs, like Facebook's Mark Zuckerberg, have said there's a need for some regulation. But Smith goes further, deeper than his peers, by arguing that governments need to probe the fundamentals of the data economy. He poses prescient questions. Where does our data go? Who gets to call the shots on how our data gets used - the few companies, Microsoft included, that have collected it? SMITH: I worry that if all of the data on which the world relies is in the hands of a small number of tech companies, you're going to see a massive transfer of economic wealth. SHAHANI: This week, nearly every state attorney general in the U. S. joined an antitrust probe into Google. Last week, nine AGs joined one against Facebook. It'll be years before a court ruling, if any, lands. Smith encourages his fellow tech leaders to look for places where they can compromise. He's the one who pushed Gates and Microsoft to enter a settlement, which was hard emotionally. SMITH: There were days when people would say, why are you such a wimp? And the answer, in my view, was because it was the wrong fight to fight; that it often takes more courage to compromise than it does to keep fighting. SHAHANI: Geopolitical, business and marriage advice from Brad Smith. (LAUGHTER) SMITH: Well, at least two out of the three. SHAHANI: Maybe he's got another book in him. Aarti Shahani, NPR News, Redmond.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-13-759833071": {"title": "Book Review: In 'Permanent Record,' Edward Snowden Makes His Case Against Mass Surveillance : NPR", "url": "https://www.npr.org/2019/09/13/759833071/in-permanent-record-edward-snowden-makes-his-case-against-mass-surveillance", "author": "No author found", "published_date": "2019-09-13", "content": "", "section": "Book Reviews", "disclaimer": ""}, "2019-09-15-749547034": {"title": "Drones And Planes Are Replacing Human Fire Lookouts : NPR", "url": "https://www.npr.org/2019/09/15/749547034/a-fire-lookout-on-whats-lost-in-a-transition-to-technology", "author": "No author found", "published_date": "2019-09-15", "content": "", "section": "Environment And Energy Collaborative", "disclaimer": ""}, "2019-09-16-759908026": {"title": "Most Isolated Tribe In Continental U.S. Gets Broadband : NPR", "url": "https://www.npr.org/2019/09/16/759908026/most-isolated-tribe-in-continental-u-s-gets-broadband", "author": "No author found", "published_date": "2019-09-16", "content": "AUDIE CORNISH, HOST: It's time now for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CORNISH: This month, we're looking at the tech issues that most affect rural America. And today, we go to a remote place where one of the most isolated Native American tribes is finally getting more broadband Internet service, and it hasn't been easy. But if the Havasupai can install reliable high-speed Internet at the bottom of the Grand Canyon, they could bridge the digital divide for other tribes. From member station KJZZ, Laurel Morales reports. LAUREL MORALES, BYLINE: The Havasupai Reservation is only accessible by foot, by mule or by helicopter. (SOUNDBITE OF HELICOPTER BUZZING)MORALES: It's a five-minute flight from the rim of the canyon to Supai Village, where 450 tribal members live in small homes made of panel siding and materials that can be easily hauled to the canyon floor. It's no wonder Internet access has been a challenge. But recently, the Havasupai have had some help from the Oakland-based nonprofit MuralNet. MARIEL TRIGGS: Oh, yeah. Look at that. We got bars. Dang. MORALES: MuralNet's Mariel Triggs trains the Havasupai how to install a network box outside a home. Triggs, with the help of Flagstaff-based Niles Radio, built what's called a microwave hop from towers at the rim that beam a signal down to Supai Village. TRIGGS: And we were able to put up the network in just a few hours for less than the cost of a Toyota Corolla, frankly. MORALES: Triggs says the geography wasn't the issue. It was policy holding up the process. The Federal Communications Commission finally granted the tribe a permanent license last spring. Now the Havasupai want to increase the signal, but they've run into another hurdle. Another Internet provider says it may be interested in bringing broadband here. So now the FCC is dealing with concerns over competition. TRIGGS: We have the funds. We have the money. We could do it. We could put the materials that are needed in the towers and connect everything, but we have to wait for all this policy stuff again to sort out. MORALES: Triggs says she's worried it could take years. TRIGGS: It just kills me because I feel like policy is actually causing the digital divide right now rather than helping to fix it. MORALES: Currently, only seven Havasupai families can connect, including the Balderramas. Sally Balderrama's son Evan is 9 but tests at the kindergarten level. A therapist flies down into the canyon twice a month to work with Evan and other kids with special needs. SALLY BALDERRAMA: It's not enough time. You know, it's not enough. I wish they can stay at least for, you know, three to four days. MORALES: Thanks to the new Internet connection, the therapist can work with Evan three times a week via Skype. Have you noticed a difference in Evan? BALDERRAMA: Oh, yeah. Yeah. And the Internet is good - it is - but we have times, you know, when it pauses, and it spins. And, you know, that's kind of frustrating. MORALES: One of the people trying to get the tribe more reliable high-speed Internet for kids with special needs is Councilwoman Ophelia Watahomigie-Corliss. She points to a whole host of other reasons - so teens can take high school courses online instead of being sent away to boarding schools, so they can have better emergency communication during one of their many flash floods and for better health care. OPHELIA WATAHOMIGIE-CORLISS: We have different generations of telemedicine equipment that have just been sitting around collecting dust because it's been unable to be established. Broadband speeds, high-speed Internet - those things are specific. MORALES: Meanwhile, the FCC estimates at least a third of people living on tribal lands don't have access to high-speed Internet, so the agency is giving tribes first dibs on applying for available broadband spectrum ahead of commercial companies at the beginning of 2020. But the problem is this. The FCC would require the tribes to build their infrastructure - the towers and antennas - in half the time required of major telecom companies. For NPR News, I'm Laurel Morales in Flagstaff. (SOUNDBITE OF SHAKEY GRAVES SONG, \"IF NOT FOR YOU\") AUDIE CORNISH, HOST:  It's time now for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CORNISH: This month, we're looking at the tech issues that most affect rural America. And today, we go to a remote place where one of the most isolated Native American tribes is finally getting more broadband Internet service, and it hasn't been easy. But if the Havasupai can install reliable high-speed Internet at the bottom of the Grand Canyon, they could bridge the digital divide for other tribes. From member station KJZZ, Laurel Morales reports. LAUREL MORALES, BYLINE: The Havasupai Reservation is only accessible by foot, by mule or by helicopter. (SOUNDBITE OF HELICOPTER BUZZING) MORALES: It's a five-minute flight from the rim of the canyon to Supai Village, where 450 tribal members live in small homes made of panel siding and materials that can be easily hauled to the canyon floor. It's no wonder Internet access has been a challenge. But recently, the Havasupai have had some help from the Oakland-based nonprofit MuralNet. MARIEL TRIGGS: Oh, yeah. Look at that. We got bars. Dang. MORALES: MuralNet's Mariel Triggs trains the Havasupai how to install a network box outside a home. Triggs, with the help of Flagstaff-based Niles Radio, built what's called a microwave hop from towers at the rim that beam a signal down to Supai Village. TRIGGS: And we were able to put up the network in just a few hours for less than the cost of a Toyota Corolla, frankly. MORALES: Triggs says the geography wasn't the issue. It was policy holding up the process. The Federal Communications Commission finally granted the tribe a permanent license last spring. Now the Havasupai want to increase the signal, but they've run into another hurdle. Another Internet provider says it may be interested in bringing broadband here. So now the FCC is dealing with concerns over competition. TRIGGS: We have the funds. We have the money. We could do it. We could put the materials that are needed in the towers and connect everything, but we have to wait for all this policy stuff again to sort out. MORALES: Triggs says she's worried it could take years. TRIGGS: It just kills me because I feel like policy is actually causing the digital divide right now rather than helping to fix it. MORALES: Currently, only seven Havasupai families can connect, including the Balderramas. Sally Balderrama's son Evan is 9 but tests at the kindergarten level. A therapist flies down into the canyon twice a month to work with Evan and other kids with special needs. SALLY BALDERRAMA: It's not enough time. You know, it's not enough. I wish they can stay at least for, you know, three to four days. MORALES: Thanks to the new Internet connection, the therapist can work with Evan three times a week via Skype. Have you noticed a difference in Evan? BALDERRAMA: Oh, yeah. Yeah. And the Internet is good - it is - but we have times, you know, when it pauses, and it spins. And, you know, that's kind of frustrating. MORALES: One of the people trying to get the tribe more reliable high-speed Internet for kids with special needs is Councilwoman Ophelia Watahomigie-Corliss. She points to a whole host of other reasons - so teens can take high school courses online instead of being sent away to boarding schools, so they can have better emergency communication during one of their many flash floods and for better health care. OPHELIA WATAHOMIGIE-CORLISS: We have different generations of telemedicine equipment that have just been sitting around collecting dust because it's been unable to be established. Broadband speeds, high-speed Internet - those things are specific. MORALES: Meanwhile, the FCC estimates at least a third of people living on tribal lands don't have access to high-speed Internet, so the agency is giving tribes first dibs on applying for available broadband spectrum ahead of commercial companies at the beginning of 2020. But the problem is this. The FCC would require the tribes to build their infrastructure - the towers and antennas - in half the time required of major telecom companies. For NPR News, I'm Laurel Morales in Flagstaff. (SOUNDBITE OF SHAKEY GRAVES SONG, \"IF NOT FOR YOU\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-17-761718975": {"title": "Free Software Pioneer Quits MIT Over His Comments On Epstein Sex Trafficking Case : NPR", "url": "https://www.npr.org/2019/09/17/761718975/free-software-pioneer-quits-mit-over-his-comments-on-epstein-sex-trafficking-cas", "author": "No author found", "published_date": "2019-09-17", "content": "", "section": "Education", "disclaimer": ""}, "2019-09-17-758146019": {"title": "China Used Twitter To Disrupt Hong Kong Protests, But Efforts Began Years Earlier : NPR", "url": "https://www.npr.org/2019/09/17/758146019/china-used-twitter-to-disrupt-hong-kong-protests-but-efforts-began-years-earlier", "author": "No author found", "published_date": "2019-09-17", "content": "", "section": "World", "disclaimer": ""}, "2019-09-17-761682912": {"title": "Using AI In Malawi To Save Elephants : NPR", "url": "https://www.npr.org/2019/09/17/761682912/using-ai-in-malawi-to-save-elephants", "author": "No author found", "published_date": "2019-09-17", "content": "ARI SHAPIRO, HOST: A failing National Park in Malawi has become ground zero for an experiment in conservation. Officials are using artificial intelligence and machine learning to stay one step ahead of poachers and keep elephants from killing villagers. NPR's Dina Temple-Raston reports from Malawi as part of the NPR series \"I'll Be Seeing You\" about the technologies that watch us. DINA TEMPLE-RASTON, BYLINE: The best way to get close to an elephant is to use an old-fashioned tracking method. We saw the fresh dung piles along the road and followed those and found him in this clearing. An enormous elephant is munching on grass less than 20 feet in front of us. It's magical. Liwonde National Park is 200 square miles of scenes just like that, and seeing them makes it hard to believe that just four years ago, the park was on the verge of collapse. CRAIG REID: So I always describe Liwonde as we found it as being in a state of terminal decline. TEMPLE-RASTON: That's park manager Craig Reid. He arrived here four years ago with a nonprofit organization called African Parks, and his job was to bring Liwonde back from the brink. REID: So effectively, what would have happened had we not intervened would be a total elimination of all wildlife over the 10 year period following. TEMPLE-RASTON: The things that go wrong in a failed park go wrong very fast. In Liwonde's case, it went beyond crumbling infrastructure or washed-out roads. Poaching was endemic, and elephants were killing villagers outside the park. Various Donzani has a small house just outside the park's fence line. He's a retired schoolteacher. VARIOUS DONZANI: My house is 50 meters from the fence. TEMPLE-RASTON: Fifty meters from the fence? DONZANI: Fifty meters. TEMPLE-RASTON: Donzani is a subsistence farmer, and he and his family rely on the garden for food. He grows mangoes and corn and rice, which happen to be three of an elephant's favorite foods. Elephants have a sweet tooth. So proximity, food and opportunity - that's all you need to spark a human-elephant conflict. (SOUNDBITE OF ELEPHANT TRUMPETING)TEMPLE-RASTON: This is the sound from a video of an elephant charging in Liwonde. After hippos, elephants kill more people in Africa than any other animal. Try shooing an elephant off a field, and this is what you face. (SOUNDBITE OF ELEPHANT TRUMPETING)TEMPLE-RASTON: An adult female weighs over three tons. A bull elephant is typically over six. Park manager Craig Reid says being killed by an elephant is a horrifying thing. REID: These big bulls will charge a person, and when people are caught by an elephant, they're normally using their trunk to knock them on the ground and then kneel on them, crushing them either with their knees or with the base of their trunk, which, at that point, is all curled up and almost like a fist. TEMPLE-RASTON: Donzani has seen it happen. He said several years ago, a herd of elephants came out of the park and into the fields. DONZANI: I tell you, it was a disaster. Seven people were killed that day. TEMPLE-RASTON: Seven people killed in one day. These kinds of deadly episodes have a ripple effect. Smugglers and international cartels will look for local farmers - not Donzani - who have had these kinds of run-ins with elephants, and they'll promise them enough money to buy food for their families. All they have to do is help the poachers locate an elephant. On the black market, a single elephant tusk can sell for hundreds of thousands of dollars. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER: A new study, the Great Elephant Census, suggests a failure to protect the world's largest land mammals, elephants. TEMPLE-RASTON: It so happens that around the time Liwonde Park manager Craig Reid arrived in Malawi, one of the founders of Microsoft, Paul Allen, was putting the finishing touches on something he called the Great Elephant Census. It was a massive, multi-million dollar project aimed at counting all the savanna elephants in Africa, and it found that in the seven years between 2007 and 2014, nearly a third of Africa's elephants were killed, mainly by poachers. Allen wanted a high-tech solution, so he funded a company called Vulcan, and they created a program called EarthRanger. And what it does is it takes all those things park managers know from years of experience and all that paperwork park rangers fill out and puts it through a machine learning program to find patterns and to learn the rhythms of the park. LAWRENCE MUNRO: Message public (unintelligible). TEMPLE-RASTON: Liwonde's operations manager is a man named Lawrence Munro, and part of his job is to plan and schedule ranger patrols. He's been using EarthRanger to help him deploy those rangers more efficiently. MUNRO: You can kill the (unintelligible) for this circle. TEMPLE-RASTON: Munro was standing in front of an image of the park up on a flat screen. It shows the river and clearings and forests. There are little elephant icons tracking GPS location signals from collared elephants, and little rhino icons move across the screen as well. Munro starts clicking a mouse through various EarthRanger menus. He moves through an animation of suspicious activity in the park. You can see a concentration of snares, little icons that look like little Western lassos, and there are a bunch of them right off the main road. MUNRO: So you can see those snares are up there just to the post. TEMPLE-RASTON: Munro and the park's head of security Paul Chidyera decide to set up new checkpoints right where EarthRanger recorded a large number of snares. Before EarthRanger, Munro used to do this kind of analysis with a map and magnets. MUNRO: You have to rely a lot more on memory, a lot more on radio traffic. Here, you can do a lot more pre-emptive stuff because you can see the picture. TEMPLE-RASTON: Munro said it used to take a month to start to see patterns emerging. Now he can do that daily. MUNRO: We study it intently every Wednesday because we want to deploy our guys accordingly, but you can do it daily. It's basically the speed at which you can strategize. TEMPLE-RASTON: Strategize and respond. Earlier this year, the head of law enforcement at Liwonde, Paul Chidyera, got an alert on his cell phone after an EarthRanger sensor was tripped. He set up a poacher cam in the area to try to get a beat on what it was. The poacher cam has a special algorithm inside that helps the camera determine whether something going past has a human shape or an animal one. If it looks human, it snaps a picture and sends it with GPS coordinates to EarthRanger and staff cell phones. The poacher cam got a photo of someone coming in. PAUL CHIDYERA: When he was arrested, he was confronted, and he revealed all of what he has been doing. TEMPLE-RASTON: In fact, as they scrolled back through EarthRanger's database, they found other pictures of him coming into the park with weapons. Those pictures were submitted as evidence in his trial. In the end, the poacher was sentenced to 27 years as a repeat offender. In the past two years, poaching in Liwonde has plummeted. The park hasn't lost a single high-value animal in 30 months, and those deadly unscheduled elephant trips outside the park - they're way down, too. While all of this has been going on, EarthRanger's machine learning algorithm has been training. The program will use game theory and behavioral analysis to tell people like Munro where poachers will go before they get there, allowing rangers to intercept bad guys pre-bang. Poachers will go to where they believe the elephants will be, and they'll find rangers waiting there for them instead. Pawan Nrisimha of Vulcan, who helped create EarthRanger, says AI can take a long time to learn. PAWAN NRISIMHA: The biggest difficulty here is really gathering that data to train up that artificial intelligence model to know what to predict. TEMPLE-RASTON: Now that EarthRanger has been deployed in Liwonde for two years, it's getting close to having enough data to start making predictions. Nrisimha sees EarthRanger getting poachers before they can even fire a shot in the next year or two. Dina Temple-Raston, NPR News. ARI SHAPIRO, HOST:  A failing National Park in Malawi has become ground zero for an experiment in conservation. Officials are using artificial intelligence and machine learning to stay one step ahead of poachers and keep elephants from killing villagers. NPR's Dina Temple-Raston reports from Malawi as part of the NPR series \"I'll Be Seeing You\" about the technologies that watch us. DINA TEMPLE-RASTON, BYLINE: The best way to get close to an elephant is to use an old-fashioned tracking method. We saw the fresh dung piles along the road and followed those and found him in this clearing. An enormous elephant is munching on grass less than 20 feet in front of us. It's magical. Liwonde National Park is 200 square miles of scenes just like that, and seeing them makes it hard to believe that just four years ago, the park was on the verge of collapse. CRAIG REID: So I always describe Liwonde as we found it as being in a state of terminal decline. TEMPLE-RASTON: That's park manager Craig Reid. He arrived here four years ago with a nonprofit organization called African Parks, and his job was to bring Liwonde back from the brink. REID: So effectively, what would have happened had we not intervened would be a total elimination of all wildlife over the 10 year period following. TEMPLE-RASTON: The things that go wrong in a failed park go wrong very fast. In Liwonde's case, it went beyond crumbling infrastructure or washed-out roads. Poaching was endemic, and elephants were killing villagers outside the park. Various Donzani has a small house just outside the park's fence line. He's a retired schoolteacher. VARIOUS DONZANI: My house is 50 meters from the fence. TEMPLE-RASTON: Fifty meters from the fence? DONZANI: Fifty meters. TEMPLE-RASTON: Donzani is a subsistence farmer, and he and his family rely on the garden for food. He grows mangoes and corn and rice, which happen to be three of an elephant's favorite foods. Elephants have a sweet tooth. So proximity, food and opportunity - that's all you need to spark a human-elephant conflict. (SOUNDBITE OF ELEPHANT TRUMPETING) TEMPLE-RASTON: This is the sound from a video of an elephant charging in Liwonde. After hippos, elephants kill more people in Africa than any other animal. Try shooing an elephant off a field, and this is what you face. (SOUNDBITE OF ELEPHANT TRUMPETING) TEMPLE-RASTON: An adult female weighs over three tons. A bull elephant is typically over six. Park manager Craig Reid says being killed by an elephant is a horrifying thing. REID: These big bulls will charge a person, and when people are caught by an elephant, they're normally using their trunk to knock them on the ground and then kneel on them, crushing them either with their knees or with the base of their trunk, which, at that point, is all curled up and almost like a fist. TEMPLE-RASTON: Donzani has seen it happen. He said several years ago, a herd of elephants came out of the park and into the fields. DONZANI: I tell you, it was a disaster. Seven people were killed that day. TEMPLE-RASTON: Seven people killed in one day. These kinds of deadly episodes have a ripple effect. Smugglers and international cartels will look for local farmers - not Donzani - who have had these kinds of run-ins with elephants, and they'll promise them enough money to buy food for their families. All they have to do is help the poachers locate an elephant. On the black market, a single elephant tusk can sell for hundreds of thousands of dollars. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER: A new study, the Great Elephant Census, suggests a failure to protect the world's largest land mammals, elephants. TEMPLE-RASTON: It so happens that around the time Liwonde Park manager Craig Reid arrived in Malawi, one of the founders of Microsoft, Paul Allen, was putting the finishing touches on something he called the Great Elephant Census. It was a massive, multi-million dollar project aimed at counting all the savanna elephants in Africa, and it found that in the seven years between 2007 and 2014, nearly a third of Africa's elephants were killed, mainly by poachers. Allen wanted a high-tech solution, so he funded a company called Vulcan, and they created a program called EarthRanger. And what it does is it takes all those things park managers know from years of experience and all that paperwork park rangers fill out and puts it through a machine learning program to find patterns and to learn the rhythms of the park. LAWRENCE MUNRO: Message public (unintelligible). TEMPLE-RASTON: Liwonde's operations manager is a man named Lawrence Munro, and part of his job is to plan and schedule ranger patrols. He's been using EarthRanger to help him deploy those rangers more efficiently. MUNRO: You can kill the (unintelligible) for this circle. TEMPLE-RASTON: Munro was standing in front of an image of the park up on a flat screen. It shows the river and clearings and forests. There are little elephant icons tracking GPS location signals from collared elephants, and little rhino icons move across the screen as well. Munro starts clicking a mouse through various EarthRanger menus. He moves through an animation of suspicious activity in the park. You can see a concentration of snares, little icons that look like little Western lassos, and there are a bunch of them right off the main road. MUNRO: So you can see those snares are up there just to the post. TEMPLE-RASTON: Munro and the park's head of security Paul Chidyera decide to set up new checkpoints right where EarthRanger recorded a large number of snares. Before EarthRanger, Munro used to do this kind of analysis with a map and magnets. MUNRO: You have to rely a lot more on memory, a lot more on radio traffic. Here, you can do a lot more pre-emptive stuff because you can see the picture. TEMPLE-RASTON: Munro said it used to take a month to start to see patterns emerging. Now he can do that daily. MUNRO: We study it intently every Wednesday because we want to deploy our guys accordingly, but you can do it daily. It's basically the speed at which you can strategize. TEMPLE-RASTON: Strategize and respond. Earlier this year, the head of law enforcement at Liwonde, Paul Chidyera, got an alert on his cell phone after an EarthRanger sensor was tripped. He set up a poacher cam in the area to try to get a beat on what it was. The poacher cam has a special algorithm inside that helps the camera determine whether something going past has a human shape or an animal one. If it looks human, it snaps a picture and sends it with GPS coordinates to EarthRanger and staff cell phones. The poacher cam got a photo of someone coming in. PAUL CHIDYERA: When he was arrested, he was confronted, and he revealed all of what he has been doing. TEMPLE-RASTON: In fact, as they scrolled back through EarthRanger's database, they found other pictures of him coming into the park with weapons. Those pictures were submitted as evidence in his trial. In the end, the poacher was sentenced to 27 years as a repeat offender. In the past two years, poaching in Liwonde has plummeted. The park hasn't lost a single high-value animal in 30 months, and those deadly unscheduled elephant trips outside the park - they're way down, too. While all of this has been going on, EarthRanger's machine learning algorithm has been training. The program will use game theory and behavioral analysis to tell people like Munro where poachers will go before they get there, allowing rangers to intercept bad guys pre-bang. Poachers will go to where they believe the elephants will be, and they'll find rangers waiting there for them instead. Pawan Nrisimha of Vulcan, who helped create EarthRanger, says AI can take a long time to learn. PAWAN NRISIMHA: The biggest difficulty here is really gathering that data to train up that artificial intelligence model to know what to predict. TEMPLE-RASTON: Now that EarthRanger has been deployed in Liwonde for two years, it's getting close to having enough data to start making predictions. Nrisimha sees EarthRanger getting poachers before they can even fire a shot in the next year or two. Dina Temple-Raston, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-17-760649353": {"title": "NASA's Dragonfly Drone Willl Explore Saturn's Largest Moon : NPR", "url": "https://www.npr.org/2019/09/17/760649353/meet-the-nuclear-powered-self-driving-drone-nasa-is-sending-to-a-moon-of-saturn", "author": "No author found", "published_date": "2019-09-17", "content": "DAVID GREENE, HOST: Earlier this summer, NASA announced that it would fund an ambitious new mission to send a quadcopter-style drone to a moon of Saturn. This drone will leave earth in 2026, but work has already begun on it. NPR's Geoff Brumfiel has been visiting with the team of scientists behind this mission. GEOFF BRUMFIEL, BYLINE: This drone will head to a moon called Titan. And the first thing to know about Titan is that it's cool - like literally, it's really cold. ELIZABETH TURTLE: It's 94 Kelvin and negative 290 Fahrenheit. BRUMFIEL: Zibi Turtle is head of the mission, which is being run out of the Johns Hopkins Applied Physics Laboratory. Turtle - it should come as no surprise - also thinks Titan's figuratively cool. TURTLE: Titan is a really fascinating world. It's the largest moon of Saturn. It's the only satellite in the solar system that has a dense atmosphere. In fact, its atmosphere is denser than Earth's atmosphereBRUMFIEL: And there's more Earthlike things about it. Titan has dunes, mountains, gullies, even rivers and lakes. Though, on Titan, it's so cold the lakes are filled with liquid methane, not water. Think of it as a little, frigid Earthlette (ph) floating around the outer solar system. And that's what has Turtle and her teams so interested. Like Earth, Titan is home to a lot of different kinds of organic molecules. The climate's probably too harsh for those molecules to turn into life. But Turtle thinks Titan could provide clues to how life started here on Earth. TURTLE: All of these materials have been basically doing chemistry experiments for us. And so what we want to be able to do is go pick up the results of those experiments to understand, you know, the same kinds of steps that were taken here on Earth toward life. BRUMFIEL: But look, I haven't told you the coolest thing about Titan yet. TURTLE: If you had a good way to keep warm and some oxygen with you to breath and put wings on, you'd be able to fly. BRUMFIEL: What - you mean, like, flapping? TURTLE: Exactly. A human being would be able to fly on Titan. It's that much easier to fly on Titan than it is on Earth. BRUMFIEL: Titan's dense atmosphere and low gravity make getting off the ground a cinch. And that's why Turtle's plan is to explore with a drone rather than a rover. Down the hall from her office in a conference room, there's a giant quadcopter. Wow. TURTLE: Sweet. I didn't know we had that - the larger one. . . BRUMFIEL: This is fantastic. Look at this thing. Hi. TURTLE: Before I forget. . . DOUG ADAMS: Hey. I'm Doug Adams. BRUMFIEL: Doug Adams is one of the lead engineers on the project. The drone he's showing me takes up the whole table. And it's only a fraction of the size of what they have in mind. Oh, that's quarter scale? ADAMS: That's quarter scale. TURTLE: That's quarter scale. BRUMFIEL: Oh. . . TURTLE: Yeah. BRUMFIEL: . . . This thing's big. TURTLE: It is. BRUMFIEL: The real drone, known as Dragonfly, will be roughly the size of a compact car. Titan's distance from Earth means that nobody can fly Dragonfly by remote control. It'll have to be completely autonomous. And there's no way to recharge it, which - if you've ever owned a drone - you know needs to happen a lot. And at this point, you may be thinking what I was thinking - really? ADAMS: Almost everyone that gets exposed to Dragonfly has a similar thought process. The first time you see it, you think, you've got to be kidding. That's crazy. BRUMFIEL: But Adams says the mission really is possible using technology we use all the time on Earth. Quadcopter-style drones, for example, are all over the place. This one's just a little bigger. Self-driving technology is increasingly common - and bonus, it should be easy on Titan because there aren't any obstacles. ADAMS: We make the joke, if we hit a tree then we win - right? - because, you know, we found a tree on Titan. BRUMFIEL: Recharging is a problem, but they've got a solution for that, too - a nuclear battery. NASA actually already uses one on its Mars rover. Turtle says, as ambitious as Dragonfly sounds, it's just a bunch of old tech bolted together. TURTLE: One of the strategies that lowers risk for a mission is to use proven technology (laughter). BRUMFIEL: Yeah. But, I mean, you're building a nuclear-powered, self-driving drone for a moon of Saturn. . . TURTLE: (Laughter). BRUMFIEL: . . . So it is something new, isn't it? I mean, let's not. . . ADAMS: It is. . . BRUMFIEL: . . . Not understate that. ADAMS: So we won't understate that. However, as Zibi pointed out, the secret is to limit the miracles, right? We're assembling as many technologies that are already existing as possible and limiting what we have to do. BRUMFIEL: Even Dragonfly's scientific instruments that it will use to take samples and send data back to Earth have been tested on other missions. In fact, what Adams is most worried about is something we all have at our fingertips here on Earth that he can't take to Titan. ADAMS: We don't actually have a map. There's no GPS. There's no magnetic field even to orient yourself. BRUMFIEL: The biggest challenge facing Dragonfly is how to find its way around. Then again, any good explorer should get a little lost, right? Geoff Brumfiel, NPR News. (SOUNDBITE OF LANDING'S \"FLUENCY OF COLORS\") DAVID GREENE, HOST:  Earlier this summer, NASA announced that it would fund an ambitious new mission to send a quadcopter-style drone to a moon of Saturn. This drone will leave earth in 2026, but work has already begun on it. NPR's Geoff Brumfiel has been visiting with the team of scientists behind this mission. GEOFF BRUMFIEL, BYLINE: This drone will head to a moon called Titan. And the first thing to know about Titan is that it's cool - like literally, it's really cold. ELIZABETH TURTLE: It's 94 Kelvin and negative 290 Fahrenheit. BRUMFIEL: Zibi Turtle is head of the mission, which is being run out of the Johns Hopkins Applied Physics Laboratory. Turtle - it should come as no surprise - also thinks Titan's figuratively cool. TURTLE: Titan is a really fascinating world. It's the largest moon of Saturn. It's the only satellite in the solar system that has a dense atmosphere. In fact, its atmosphere is denser than Earth's atmosphere BRUMFIEL: And there's more Earthlike things about it. Titan has dunes, mountains, gullies, even rivers and lakes. Though, on Titan, it's so cold the lakes are filled with liquid methane, not water. Think of it as a little, frigid Earthlette (ph) floating around the outer solar system. And that's what has Turtle and her teams so interested. Like Earth, Titan is home to a lot of different kinds of organic molecules. The climate's probably too harsh for those molecules to turn into life. But Turtle thinks Titan could provide clues to how life started here on Earth. TURTLE: All of these materials have been basically doing chemistry experiments for us. And so what we want to be able to do is go pick up the results of those experiments to understand, you know, the same kinds of steps that were taken here on Earth toward life. BRUMFIEL: But look, I haven't told you the coolest thing about Titan yet. TURTLE: If you had a good way to keep warm and some oxygen with you to breath and put wings on, you'd be able to fly. BRUMFIEL: What - you mean, like, flapping? TURTLE: Exactly. A human being would be able to fly on Titan. It's that much easier to fly on Titan than it is on Earth. BRUMFIEL: Titan's dense atmosphere and low gravity make getting off the ground a cinch. And that's why Turtle's plan is to explore with a drone rather than a rover. Down the hall from her office in a conference room, there's a giant quadcopter. Wow. TURTLE: Sweet. I didn't know we had that - the larger one. . . BRUMFIEL: This is fantastic. Look at this thing. Hi. TURTLE: Before I forget. . . DOUG ADAMS: Hey. I'm Doug Adams. BRUMFIEL: Doug Adams is one of the lead engineers on the project. The drone he's showing me takes up the whole table. And it's only a fraction of the size of what they have in mind. Oh, that's quarter scale? ADAMS: That's quarter scale. TURTLE: That's quarter scale. BRUMFIEL: Oh. . . TURTLE: Yeah. BRUMFIEL: . . . This thing's big. TURTLE: It is. BRUMFIEL: The real drone, known as Dragonfly, will be roughly the size of a compact car. Titan's distance from Earth means that nobody can fly Dragonfly by remote control. It'll have to be completely autonomous. And there's no way to recharge it, which - if you've ever owned a drone - you know needs to happen a lot. And at this point, you may be thinking what I was thinking - really? ADAMS: Almost everyone that gets exposed to Dragonfly has a similar thought process. The first time you see it, you think, you've got to be kidding. That's crazy. BRUMFIEL: But Adams says the mission really is possible using technology we use all the time on Earth. Quadcopter-style drones, for example, are all over the place. This one's just a little bigger. Self-driving technology is increasingly common - and bonus, it should be easy on Titan because there aren't any obstacles. ADAMS: We make the joke, if we hit a tree then we win - right? - because, you know, we found a tree on Titan. BRUMFIEL: Recharging is a problem, but they've got a solution for that, too - a nuclear battery. NASA actually already uses one on its Mars rover. Turtle says, as ambitious as Dragonfly sounds, it's just a bunch of old tech bolted together. TURTLE: One of the strategies that lowers risk for a mission is to use proven technology (laughter). BRUMFIEL: Yeah. But, I mean, you're building a nuclear-powered, self-driving drone for a moon of Saturn. . . TURTLE: (Laughter). BRUMFIEL: . . . So it is something new, isn't it? I mean, let's not. . . ADAMS: It is. . . BRUMFIEL: . . . Not understate that. ADAMS: So we won't understate that. However, as Zibi pointed out, the secret is to limit the miracles, right? We're assembling as many technologies that are already existing as possible and limiting what we have to do. BRUMFIEL: Even Dragonfly's scientific instruments that it will use to take samples and send data back to Earth have been tested on other missions. In fact, what Adams is most worried about is something we all have at our fingertips here on Earth that he can't take to Titan. ADAMS: We don't actually have a map. There's no GPS. There's no magnetic field even to orient yourself. BRUMFIEL: The biggest challenge facing Dragonfly is how to find its way around. Then again, any good explorer should get a little lost, right? Geoff Brumfiel, NPR News. (SOUNDBITE OF LANDING'S \"FLUENCY OF COLORS\")", "section": "Space", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-18-762046356": {"title": "Computer Scientists Work To Fix Easily Fooled AI : NPR", "url": "https://www.npr.org/2019/09/18/762046356/u-s-military-researchers-work-to-fix-easily-fooled-ai", "author": "No author found", "published_date": "2019-09-18", "content": "AUDIE CORNISH, HOST: Researchers are discovering something unnerving about artificial intelligence. It's easy to fool - so easy, in fact, there's a whole field of study known as adversarial AI that actually aims to make artificial intelligence a little smarter. As part of an NPR special series on the technologies that watch us, Dina Temple-Raston has more. DINA TEMPLE-RASTON, BYLINE: Artificial intelligence is all about showing a machine millions of examples so it can learn to recognize things in the real world. And there's a pretty famous experiment about how easily this can go wrong. It was conducted by a team of researchers led by UC Berkeley professor Dawn Song. DAWN SONG: Let me start playing the video. TEMPLE-RASTON: She and her colleagues made a video that showed how they fooled AI and, in this case, fooled a system while it was driving a car. The video is less than a minute long. And it doesn't have any sound, but it rocked the AI community. SONG: So in the video, you'll see two frames side by side. TEMPLE-RASTON: Think split screen. All you need to know now is that each split screen is subtitled so you could see how the AI, and specifically a subset of AI called image classification, is making decisions inside the autonomous car. SONG: You see the prediction given by the image classification system to try to predict what the traffic sign is. TEMPLE-RASTON: So sort of like the car starting to think, a sign is coming; I'm going to have to make a decision. SONG: Right. TEMPLE-RASTON: So Song and her team had the AI system read two stop signs. One was a perfectly normal stop sign. The other was manipulated. Song had put one sticker below the S and another above the O in stop. And as the car gets closer to it, the subtitles are describing the AI's decision-making process. It reads the regular stop sign just fine and is telling the car to prepare to stop. But the one with the stickers, it thinks the sign reads, speed limit 45 miles an hour, which would allow the car, if this wasn't an experiment, to blow right through the intersection. Two carefully placed stickers was all it took to make a self-driving car run a stop sign. So you were expecting it to misread the sign. And then it did, and you were happy about that. SONG: It is surprising still, given how well it works. TEMPLE-RASTON: It works so well that people who are developing driverless cars tapped the brakes. Now, to be fair, Song's team didn't just randomly throw some stickers onto a sign. They knew exactly how the AI's image classification system worked. They knew which pixels of the sign to manipulate to fool it, which got the attention of people over at DARPA, the Defense Advanced Research Projects Agency. And to understand why the military's top research arm was so concerned, I went to DARPA headquarters to meet with Hava Siegelmann. I'm Dina Temple-Raston. HAVA SIEGELMANN: Hey, Dina. I'm Hava. TEMPLE-RASTON: Thank you for making the time. She's the director of something called the GARD project. GARD stands for Guaranteeing AI Robustness against Deception. And just like it sounds, it's looking for ways to make artificial intelligence more hack-proof. The way AI makes decisions is a bit of a black box. But Siegelmann says if you understand what the system has chosen to focus on, you can fool it. And if you're DARPA, you're less worried about a stop sign than, say, putting a sticker on a tank. SIEGELMANN: And because that sticker that has particular color, we think that this tank is actually an ambulance. And immediately, we open the gates to let the ambulance go in. TEMPLE-RASTON: The reason to study all of this isn't to scare us about AI, although it does that, too. Researchers want to understand the limits of AI so they can fix it, kind of like old-fashioned hackers who used to call up software companies and let them know about flaws in their coding so they could send out patches. Dawn Song says the bottom line is machine learning and AI aren't as powerful as people think they are. SONG: We do really need new and more breakthroughs before we can really get there. TEMPLE-RASTON: So would you ride in a driverless car? SONG: Not today (laughter). I mean, I'll enjoy having a test drive, but (laughter). . . TEMPLE-RASTON: And by the way, Dawn Song's special stop sign with the stickers isn't fooling driverless cars anymore. It's now hanging in the Science Museum in London. It's part of an exhibit about our driverless future. Dina Temple-Raston, NPR News, Washington. AUDIE CORNISH, HOST:  Researchers are discovering something unnerving about artificial intelligence. It's easy to fool - so easy, in fact, there's a whole field of study known as adversarial AI that actually aims to make artificial intelligence a little smarter. As part of an NPR special series on the technologies that watch us, Dina Temple-Raston has more. DINA TEMPLE-RASTON, BYLINE: Artificial intelligence is all about showing a machine millions of examples so it can learn to recognize things in the real world. And there's a pretty famous experiment about how easily this can go wrong. It was conducted by a team of researchers led by UC Berkeley professor Dawn Song. DAWN SONG: Let me start playing the video. TEMPLE-RASTON: She and her colleagues made a video that showed how they fooled AI and, in this case, fooled a system while it was driving a car. The video is less than a minute long. And it doesn't have any sound, but it rocked the AI community. SONG: So in the video, you'll see two frames side by side. TEMPLE-RASTON: Think split screen. All you need to know now is that each split screen is subtitled so you could see how the AI, and specifically a subset of AI called image classification, is making decisions inside the autonomous car. SONG: You see the prediction given by the image classification system to try to predict what the traffic sign is. TEMPLE-RASTON: So sort of like the car starting to think, a sign is coming; I'm going to have to make a decision. SONG: Right. TEMPLE-RASTON: So Song and her team had the AI system read two stop signs. One was a perfectly normal stop sign. The other was manipulated. Song had put one sticker below the S and another above the O in stop. And as the car gets closer to it, the subtitles are describing the AI's decision-making process. It reads the regular stop sign just fine and is telling the car to prepare to stop. But the one with the stickers, it thinks the sign reads, speed limit 45 miles an hour, which would allow the car, if this wasn't an experiment, to blow right through the intersection. Two carefully placed stickers was all it took to make a self-driving car run a stop sign. So you were expecting it to misread the sign. And then it did, and you were happy about that. SONG: It is surprising still, given how well it works. TEMPLE-RASTON: It works so well that people who are developing driverless cars tapped the brakes. Now, to be fair, Song's team didn't just randomly throw some stickers onto a sign. They knew exactly how the AI's image classification system worked. They knew which pixels of the sign to manipulate to fool it, which got the attention of people over at DARPA, the Defense Advanced Research Projects Agency. And to understand why the military's top research arm was so concerned, I went to DARPA headquarters to meet with Hava Siegelmann. I'm Dina Temple-Raston. HAVA SIEGELMANN: Hey, Dina. I'm Hava. TEMPLE-RASTON: Thank you for making the time. She's the director of something called the GARD project. GARD stands for Guaranteeing AI Robustness against Deception. And just like it sounds, it's looking for ways to make artificial intelligence more hack-proof. The way AI makes decisions is a bit of a black box. But Siegelmann says if you understand what the system has chosen to focus on, you can fool it. And if you're DARPA, you're less worried about a stop sign than, say, putting a sticker on a tank. SIEGELMANN: And because that sticker that has particular color, we think that this tank is actually an ambulance. And immediately, we open the gates to let the ambulance go in. TEMPLE-RASTON: The reason to study all of this isn't to scare us about AI, although it does that, too. Researchers want to understand the limits of AI so they can fix it, kind of like old-fashioned hackers who used to call up software companies and let them know about flaws in their coding so they could send out patches. Dawn Song says the bottom line is machine learning and AI aren't as powerful as people think they are. SONG: We do really need new and more breakthroughs before we can really get there. TEMPLE-RASTON: So would you ride in a driverless car? SONG: Not today (laughter). I mean, I'll enjoy having a test drive, but (laughter). . . TEMPLE-RASTON: And by the way, Dawn Song's special stop sign with the stickers isn't fooling driverless cars anymore. It's now hanging in the Science Museum in London. It's part of an exhibit about our driverless future. Dina Temple-Raston, NPR News, Washington.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-19-760317486": {"title": "What Killed Adrian Lamo, The Hacker Who Turned In Whistleblower Chelsea Manning? : NPR", "url": "https://www.npr.org/2019/09/19/760317486/the-mysterious-death-of-the-hacker-who-turned-in-chelsea-manning", "author": "No author found", "published_date": "2019-09-19", "content": "DINA TEMPLE-RASTON (HOST): This is I'LL BE SEEING YOU, a four-part series about the technologies that watch us from NPR. I'm Dina Temple-Raston, and today we're looking at the life and mysterious death of one of the world's most famous hackers, Adrian Lamo. And our story starts with the discovery of a body. (SOUNDBITE OF ARCHIVED RECORDING)\rUNIDENTIFIED PERSON #1 (911 OPERATOR): 911 Wichita. UNIDENTIFIED PERSON #2: Requesting a dispatch for commercial medical alarm. TEMPLE-RASTON: This is the actual 911 call that came in on March 14 of last year. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: OK, and tell me exactly what happened. UNIDENTIFIED PERSON #2: It just got this on 54 Room 222, alert by medical. UNIDENTIFIED PERSON #1: How old is the patient? UNIDENTIFIED PERSON #2: Don't know - don't have any information on them. UNIDENTIFIED PERSON #1: Patient male or female? UNIDENTIFIED PERSON #2: Don't know. UNIDENTIFIED PERSON #1: Are they breathing? UNIDENTIFIED PERSON #2: Don't know. TEMPLE-RASTON: All the caller seemed to be sure about was that something bad had happened and someone needed to get there quickly. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: Call back if anything changes. UNIDENTIFIED PERSON #2: Thank you. UNIDENTIFIED PERSON #1: Thank you. TEMPLE-RASTON: This was probably one of the few moments in Adrian Lamo's life when he was truly anonymous because before all this happened, he was a hacking rock star. (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LEO LAPORTE (TELEVISION HOST): You know him as the guy who hacked Yahoo, AOL, Time Warner, MCI WorldCom, Microsoft and very famously, the New York Times. Adrian Lamo, one of the most celebrated hackers in the world today, welcome to \"The Screen Savers. \"TEMPLE-RASTON: \"The Screen Savers\" was an old cable television show all about technology. It aired in the late '90s and early 2000s, and the host was a guy named Leo Laporte. (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LAPORTE: Do you define yourself as a hacker? Do you consider yourself a hacker? ADRIAN LAMO (HACKER): It's not a term that I try to sell myself as. LAPORTE: Yeah. LAMO: People use hacker to mean a lot of different. . . LORRAINE MURPHY (JOURNALIST, FRIEND OF ADRIAN LAMO): He definitely had an original approach to things. TEMPLE-RASTON: Lorraine Murphy writes about hacking, and she knew Adrian for years. MURPHY: The hacker mindset is all about looking at something and going, how can I use this for something it wasn't intended to? Or, how far can I push this before it does something unpredictable? He definitely had that. TEMPLE-RASTON: She says that because Adrian came to the hacking community so early, he was a bit of an inspiration, even a hero of sorts. People asked his advice, they wanted his approval, and he had almost a cult following. MURPHY: He's like the Tony Robbins of the hacking world. It's one thing to be gifted at hacking. It's another thing to be able to tell a world of civilians that the thing exists and you are good at it, and Adrian had both of those. TEMPLE-RASTON: Adrian Lamo was born in Boston, but he grew up outside Bogota, Colombia. His early hacker's resume tracks like that of just about every hacker I've talked to. He got a hand-me-down Commodore 64 as kid. He hacked into computer games. He played with viruses on floppy disks. Floppy disks - remember those? And eventually, he began tapping into strangers' phone lines and finding ways to spoof the phone company to get free long-distance calls. When his family moved to California, that only made it easier to pursue his interest in computers. After that, what we have is a persona Adrian Lamo carefully created. He thought of himself as an ethical vigilante, a gray hat hacker, a self-styled Robin Hood of the early Internet. And like Robin Hood, he took aim at the king - one of the largest internet providers in 2001, a company called MCI WorldCom. MCI would say later that Adrian slipped in through a security hole. He'd found a piece of bad code that allowed him to fool the network into thinking that he belonged there. But according to Adrian, the hack itself was even simpler than that. He got into MCI by guessing passwords. As it turns out, the company had set up temporary passwords for employees based on Social Security numbers. Adrian simply Googled some employees, quickly found their Social Security numbers online, and a short time later, he was inside the network. Getting into these networks, he told anyone that would listen, was incredibly easy. (SOUNDBITE OF ARCHIVED RECORDING)LAMO: Some of the companies that you do business with every day had passwords that were simple dictionary words, names of animals. TEMPLE-RASTON: Adrian explained how he broke into MCI in an unreleased documentary that he started. It was called \"Hackers Wanted. \"(SOUNDBITE OF ARCHIVED RECORDING)LAMO: Anyone at all with this information could have connected to each and every router that handled the data for Bank of America, Ford, Chrysler, NASA. There were very detailed. . . TEMPLE-RASTON: We hear about things like this all the time now, but back when Adrian was saying all this, people were just starting to realize how unsafe the Internet was. Adrian was foreshadowing the future, warning that once bad actors figured out how easy it was to get into network systems, no one would be safe. (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LAPORTE: You may remember Adrian Lamo. He's been on the show before. KEVIN ROSE (TELEVISION HOST): Been on the show a couple of - real nice guy. LAPORTE: Wonderful guy - he's a hacker but a kind of a gray hat hacker. TEMPLE-RASTON: \"The Screen Savers\" show again. . . (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")ROSE: Right. He doesn't steal any information. He doesn't take anything and use it for bad. He's a good hacker. LAPORTE: Well, I'll give you an example. TEMPLE-RASTON: Back when Adrian and his friends were cracking into computer networks, the goal was just to see if they could. What they were looking for was bragging rights, tinkering in order to learn. MURPHY: Adrian was never really monetized. He was not motivated primarily by money. Media, fame - that sort of thing motivated him. TEMPLE-RASTON: Here's Lorraine Murphy again. MURPHY: He wanted to be a household name. TEMPLE-RASTON: Some of the hacking tools Adrian helped develop back then are still being used by the hacker community today. What made Adrian a little different from the others around him, though, is that he saw darker forces forming. He could imagine a day when technologies would goose-step out of the pages of science fiction into our daily lives, and these technologies would allow governments, bad actors, even companies to watch us without our knowing. But Adrian went too far when he hacked into the New York Times. The way it all unfolded would sound familiar to anyone who was in the hacking underground at the time. Adrian was very good at figuring out passwords, either getting someone to unwittingly give him one or guessing at default passwords that hadn't been changed. That's how he got into the internal server at the Times. He gave himself administrator credentials and a login and a password for their LexisNexis account. Then he played a little joke. He added himself to the paper's internal database of experts and listed himself as an expert in hacking. Pretty funny in a hacker kind of way, but The New York Times didn't exactly see the humor in it. The newspaper pressed charges, and in August 2003, the FBI issued a warrant for Adrian's arrest. A reasonable person could ask, why does it seem these hacks keep on happening? Lorraine Murphy says part of it has to do with the way Web sites are designed. MURPHY: Well, when you have a Web site, you need to be able to let people into it, like, under the hood into the engine to tinker with it. That's everybody from the reporters who have to be able to put up information to the IT guys who have to be able to work on the actual machinery of the Web site. TEMPLE-RASTON: So that's the first part. A Web site is always being updated, so the system has to be open, and Adrian took advantage of that. The second part is more fundamental. Most of the code that holds Web sites together is problematic. It's buggy, which means it's full of mistakes, full of holes, which makes it really easy for hackers to get in. Here's an amazing fact. Back in Adrian's day, commercial software like Windows XP contained 20 to 30 bugs for every thousand lines of code. That means there could have been close to a million bugs in that operating system alone. And the problem is better now. The latest version of Windows operating system, Windows 10, is thought to have about five bugs per thousand lines of code. The more surprising thing is that you don't need to be a coding genius to find those bugs. You can actually buy software that finds them for you. MURPHY: You can buy those programs on the dark web, and you can buy them in plain sight. There are Facebook groups that sell this kind of code. Facebook keeps trying to shut them down, but I've been in one for years and years. TEMPLE-RASTON: So let's say you're a hacker just starting out. All you have to do is download the software, get the code for some Web site you want to hack and then run the program and see what it does. You don't need to code anything. Those overseas scammers trying to steal your personal information, they aren't computer geniuses. They just know where to buy the cheap sniffer programs - kind of brilliant, and at the same time, kind of scary. Adrian was known to buy the odd sniffer program to save time, but what he was really good at was social engineering. . . MURPHY: Which is more or less the con man skill set. He was very good at impersonating people, impersonating entire groups. He was great at creating personas and getting you to believe the persona. TEMPLE-RASTON: In real life, Adrian could be socially awkward and anxious, but on the Internet, he was bold, taking on the darker forces of corporate America. One of the most popular cartoons in New Yorker Magazine history ran in 1993. It was a drawing of a dog sitting on a chair behind a keyboard and a computer screen, and he's chatting to another dog sitting beside him, who's watching him type. On the Internet, he says, nobody knows you're a dog - which, if you think about it, is one of the things we love about the Internet. We can be whoever we want to be. Certainly, that was one of the things that Adrian loved. (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LAPORTE: There is a big story just breaking now. ROSE: Right now. . . LAPORTE: We wanted to bring it to you. We've got an exclusive scoop on this. Things have turned bad, I guess, for Adrian Lamo because. . . TEMPLE-RASTON: What had turned bad was the New York Times hack we just talked about. Adrian was charged under the Computer Fraud and Abuse Act, a cybersecurity law which, at the time, had been rarely enforced. The act codified the computer equivalent of trespass. You didn't need to steal anything or be actively malicious to run afoul of it. Just gaining unauthorized access to a system and having a company decide to press charges was enough to trigger it. Adrian pleaded guilty to a felony and was sentenced to six months of home detention and two years probation. His conviction was a reminder to the wider world that hacking was now seen as a clear and present danger. To the hackers themselves, it signaled that they could no longer crack into internal servers with impunity. The world was changing. Adrian, for his part, acted contrite. . . (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LAMO: I do think that there are some lines that I stepped over in my access. I want to take responsibility for this. LAPORTE: Sure. LAMO: I want to put it behind me. TEMPLE-RASTON: . . . But not so humbled that he couldn't resist poking fun at the whole episode. (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LAMO: On a tangentially related note, the U. S. marshals actually let me retake my mugshot until I thought I looked pretty, so. . . LAPORTE: You're kidding. Really? TEMPLE-RASTON: And Adrian was pretty - olive skin, light eyes, dimples, kind of impish - and his mugshot was pretty, too. He was half-smiling, looking a little smug. And it says something about Adrian that he expected that story to turn out completely differently. MURPHY: He was really appalled that he didn't get a job offer out of that, actually. TEMPLE-RASTON: What job was he expecting, IT analyst? MURPHY: Well, security consultant. TEMPLE-RASTON: OK. MURPHY: Literally the pipe dream of every, you know, best kid in the drama club at high school is to go to Broadway. The pipe dream of every skid in every hackerspace in the world is to get a paid job from a major corporation as a security consultant, and all you do is sit there all day and find their weaknesses. TEMPLE-RASTON: OK, that might not be what everyone or every skid in the notoriously anti-establishment hacking community wants, but the fact that Adrian wanted that kind of job so badly shows how much of a white hat he aspired to be right up until the time he died. Our show today is about the mysterious death of a hacking pioneer, Adrian Lamo. I'm Dina Temple-Raston, and you're listening to I'LL BE SEEING YOU from NPR. After the break, how does one man go from computer hero to hacking pariah? ANDREW BLAKE (FRIEND OF ADRIAN LAMO): People hated him. He couldn't log onto any sort of Internet platform without instantly getting some sort of hate directed toward him. TEMPLE-RASTON: Stay with us. TEMPLE-RASTON: This is I'LL BE SEEING YOU from NPR. I'm Dina Temple-Raston. And on the show today, we're investigating the mysterious death of one of the world's most famous hackers, Adrian Lamo. And as you'll hear, the deeper we dug into this, the weirder it got. It began as a story about an unexpected death, and then it became something else - a story not just about hacking in the Internet but about how hacking has evolved. It's gone from kids doing something slightly subversive to global syndicates trying to alter the course of history. And most people didn't notice that change until this. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #3: I got a black vehicle under target. It's arriving right to the north of the mosque. UNIDENTIFIED PERSON #4: Yeah, I would like that. Over. TEMPLE-RASTON: That's from a classified military video that was leaked back in 2010. It was filmed from the gun sights of an American helicopter in Iraq. It was all shot in black and white and runs for about 39 minutes. You're hearing actual conversations between the pilots of two Apache helicopters. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #5: Hotel 26, this is Crazy Horse 18. TEMPLE-RASTON: The helicopter is flying over a residential neighborhood. And from the camera's viewfinder, you can see low cinderblock buildings and some palm trees and a mosque. Then the camera angle shifts, and it zooms in on a handful of men walking down the street. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #5: I have individuals with weapons. UNIDENTIFIED PERSON #6: . . . Four radio. UNIDENTIFIED PERSON #5: Yep, he's got a weapon, too. UNIDENTIFIED PERSON #3: All right, we got a guy with an RPG. UNIDENTIFIED PERSON #5: I'm going to fire. UNIDENTIFIED PERSON #3: You're clear. UNIDENTIFIED PERSON #5: All right, firing. TEMPLE-RASTON: And then everything changes. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #3: Light them all up. UNIDENTIFIED PERSON #5: Come on, fire. (SOUNDBITE OF GUNFIRE)UNIDENTIFIED PERSON #3: Keep shooting. (SOUNDBITE OF GUNFIRE)\rTEMPLE-RASTON: We know now that this was live footage of a terrible mistake, and the weapons the pilots thought they saw weren't weapons at all. It was actually a camera with a telephoto lens. The men they thought were the enemy were actually reporters - Namir Noor-Eldeen, a Reuters photographer, and his assistant and driver, Saeed Chmagh. Twelve people were killed in the assault. The video, when it was released, raised questions about civilian casualties and American rules of engagement in Iraq. And it focused the world's attention on three things - the little-known organization called WikiLeaks, the young Army intelligence analyst who leaked it and, rather improbably, Adrian Lamo. Now, if you don't remember the story, this is how it unfolded with a few new details. About two months after the helicopter video went viral, Adrian got an instant message from Chelsea Manning. She began by telling him that she had a copy of a documentary Adrian is starred in on her desktop. So Adrian thought it was going to be just another conversation from a fan. And then, all of a sudden, it wasn't. Manning started talking about her family, how hard it was to be in Iraq, about her gender identity issues and how she had to hide them from people she was working with. I know all about creating a persona, Adrian wrote. And then Manning went a step further - I think I'm in more potential heat than you ever were. How so, Adrian asked. And then she told him, told him about how she'd passed the now-famous helicopter video to WikiLeaks, along with hundreds of thousands of classified State Department cables. She'd been in touch with a crazy white-haired Aussie who can't seem to stay in one country very long. She was talking about Julian Assange, the head of WikiLeaks. He's fighting extradition from the U. K. to the U. S. now. I can't believe what I'm telling you, Manning wrote. Adrian couldn't believe it either. GLENN MORROW (COUSIN OF ADRIAN LAMO): I don't think he anticipated, when he started, the gravity of what Manning was actually saying. TEMPLE-RASTON: That's Glenn Morrow, Adrian's cousin. And they were close. He and Adrian had talked about all this while it was happening, and he remembers Adrian saying that, at first, he thought Manning was just another hacker looking for affirmation. MORROW: When you reach a sort of critical mass of fame, you just have so many people come out of the woodwork, so many, you know, people wanting to share something they've discovered or something they've done. TEMPLE-RASTON: But the Manning conversation was about breaking the law, not in the ambiguous way that Adrian was used to but in what seemed to him a very black-and-white way. Manning had leaked classified documents. Adrian quizzed her about what she downloaded, and Manning said she didn't really look at what they were. She just passed them along. The more Manning revealed, the more unsettled Adrian became. So he made a choice, and his choice altered the course of his life and Manning's, too. He called the authorities. MORROW: Once it became clear that it was such a serious thing that had happened, I don't think he could stand by and really live with the implications of just sitting on it. TEMPLE-RASTON: Chelsea Manning was arrested within days. Adrian thought he'd be celebrated as a patriot. Chelsea Manning was in a war zone, vacuuming up classified documents and distributing them. To Adrian, it didn't seem like a close call. Here's Lorraine Murphy, again. MURPHY: And it backfired, spectacularly, on him. TEMPLE-RASTON: What Adrian hadn't fully understood was how people in the hacker community would react. He was sure he could make them see how he had no choice but to turn Manning in. Weeks later, he crashed headlong into the reality of what he had done at a New York hackers' conference called HOPE. MORROW: Hope stands for Hackers on Planet Earth. TEMPLE-RASTON: Glenn Morrow went with him, and they rather naively thought it would be a great opportunity to meet people. It sounds crazy now, but it hadn't occurred to either one of them that Adrian's decision to turn in Manning would hijack the conference. MORROW: The first day at the conference, there was a lot of people, you know, yelling out snitch, at least one occasion that I recall of somebody spitting in his direction. TEMPLE-RASTON: And then the organizers hastily put together a panel they called Informants - Villains or Heroes? (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #7: Should we avoid the controversy, or should we dive right into it? I say, of course, we dive right into it. We confront this thing head on, right? (APPLAUSE)UNIDENTIFIED PERSON #7: With that, I'd like to introduce Adrian Lamo. And we'll let him say his piece, ask some questions and hopefully learn something. Adrian. (BOOING)UNIDENTIFIED PERSON #7: All right, you can boo. Go ahead. TEMPLE-RASTON: The audience was clearly against Adrian. One after another, people came to the microphone to berate him for violating the unwritten rules of hackerdom. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #8: I think we need to be clear about what's acceptable to do and what's not acceptable to do. And I think you [expletive] up huge. (APPLAUSE)UNIDENTIFIED PERSON #9: I see what you have done as treason. (APPLAUSE)UNIDENTIFIED PERSON #10: I think you belong in Guantanamo. TEMPLE-RASTON: As the group saw it, Adrian had broken the hacker code. (SOUNDBITE OF ARCHIVED RECORDING)MARK ABENE (HACKER): As soon as you make up your mind to choose a side, politically speaking, you cease to be a hacker. TEMPLE-RASTON: That's a guy named Mark Abene, better known as Phiber Optik. He was a high-profile hacker in the 1980s and 1990s and an icon of sorts. He made clear he thought Adrian had crossed a line. Hackers were supposed to be faintly subversive, not law enforcement informants. (SOUNDBITE OF ARCHIVED RECORDING)ABENE: You had a choice, and you made the wrong choice. You could've simply walked away, and none of this would've happened. LAMO: I could have, but I wouldn't have been able to live with myself. (APPLAUSE)ABENE: I disagree. MORROW: It was a little tough for me to hear. Everybody knew who he was. And up until that point, Adrian had been, you know, a hero. In the culture, you know, the worst thing you could be was a snitch. TEMPLE-RASTON: It didn't help matters that all this happened at a time when hackers were just beginning to consider the moral implications of what they were doing. MURPHY: And in the early days, the hackers did not think that there were rules when it came to websites. It was the Wild West. It wasn't against the law to hack a particular website for years and years and years. TEMPLE-RASTON: Adrian had forced the community to address fundamental questions, like what did ethical hacking really mean? If you cracked into someone's system but you didn't do any damage, was that OK? And if someone tells you they're leaking classified information, are you obliged to say something? They all wanted hacking to remain a force for good, but they weren't quite sure how to make that happen. After the conference, there was no ambiguity about how the community felt. Adrian was shunned. He received death threats. Fake bombs were mailed to his parents in California. Rumors went around that Adrian was actually a spy, ratting out fellow hackers to the government. BLAKE: People hated him. He couldn't log on to any sort of Internet platform under his actual name without instantly getting some sort of hate directed toward him. TEMPLE-RASTON: Andrew Blake is another longtime friend of Adrian's. BLAKE: Even when Adrian would do something with the absolute best of intentions, as soon as anyone realized that it was Adrian Lamo who did it, they didn't want anything to do with it. LAUREN FISHER (EX-WIFE OF ADRIAN LAMO): He used to say that I like to believe in a world where things can happen, even if I have to do them myself. He just liked to make the extraordinary happen. TEMPLE-RASTON: That's Adrian's ex-wife, Lauren Fisher. This is the first time she's talked publicly about her marriage to him. And the reason why we're talking to her is because all these rumors about Adrian working for the government may have started with something she and Adrian did years before he'd ever heard of Chelsea Manning. They started a business together in 2008, and they called it Reality Planning. FISHER: Reality Planning was a go at getting him to have a sort of a la carte system where you could ask him to test your website or test your company. It was all very vague, but it was really just to get him back into the PR spotlight. And it kind of worked. TEMPLE-RASTON: Kind of worked because someone contacted them about speaking at a computer expo in Europe. Adrian was going to get them to pick up business-class airfare. He wanted a luxury hotel. But almost before they got out of the gate, there were unexpected complications from the State Department. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #11: We have a hold in our system on your passport application. I need to know whether or not you are still on probation. . . TEMPLE-RASTON: This is an actual voicemail they got at the time. Fisher had saved it, and it was about the felony conviction we mentioned earlier for hacking The New York Times. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #11: We have information indicating that you had some kind of legal matter or something going on. So give me a call if you have any questions. Have a good day. TEMPLE-RASTON: The Europe trip, speaking engagements, being in the spotlight once again - that never happened. Fisher says there were lots of things that didn't go their way back then. Anxiety often got the better of Adrian. Sometimes, he wouldn't leave the house for days. Fisher says it was around that time that she first heard Adrian mention something called Project Vigilant. She overheard him talking to people on Skype about a project that would use his hacking skills for good, something that would put him back on top. FISHER: It was just kind of like Reality Planning, though. It was just all vague and - but it seemed a bit - for me, it seemed a bit more - it seemed bigger, obviously. And it seemed more secret. TEMPLE-RASTON: All she knew was that it had something to do with a part of the Internet called the dark web. Beyond that, Adrian didn't seem to want to talk much about it. The secrecy was in keeping with the two Adrians she was always trying to keep up with. FISHER: There were times where we would be together, and he would be in the Adrian Lamo persona. We would go to a 2600 Hacker - the monthly meet up, you know, in San Francisco. TEMPLE-RASTON: We'll talk about 2600 in a minute. FISHER: And he liked to shine in his Adrian Lamo kind of persona. But there was also the times where he was just - the walls were down completely and he wasn't the Adrian Lamo that he himself made himself believe that he was, you know? TEMPLE-RASTON: And when that happened, he medicated, looking for some little door within himself that would control his anxiety. FISHER: It was body hacking, trying to contain all the different feelings and keep them in check. TEMPLE-RASTON: Valerian root, vitamins - the list was as long as your arm, and at some point - no one is quite sure when - that list included an herbal supplement called kratom. Traditionally, it's used in lighter doses for stress and anxiety. Kratom, which is legal in most states, made it easier to socialize, which just kept getting harder for Adrian. BLAKE: Hello? TEMPLE-RASTON: Hey. Andrew? BLAKE: Hello? TEMPLE-RASTON: Are you there? Can you hear me? BLAKE: Yeah. I'm the only one home all day today. TEMPLE-RASTON: That's Andrew Blake, a friend of Adrian's, and he helped Adrian out with a couch and a meal and a place to stay over the years. And he was well aware of Adrian's kratom use. BLAKE: He used to get it mailed in envelopes as just a fine powder. It's kind of like a flower, like a dust. And he didn't explain that kratom was supposed to work on the same brain receptors that opioids did. TEMPLE-RASTON: Blake told me that Adrian had given him kratom for Christmas, but he never got around to using it. BLAKE: Adrian would get it in, like, a big bag, and I'm looking to see if I could find one in, like, our pantry. TEMPLE-RASTON: So it's important to understand that hackers like Adrian look at drugs a little differently than most of us do. Early hacker conferences, like something called HoHoCon, were drug-addled and alcohol-soaked affairs. Attendees would party so hard they would get banned from hotels. (SOUNDBITE OF ARCHIVED RECORDING)DOUGLAS BARNES (MEMBER, ELECTRONIC FRONTIER FOUNDATION): As a result of things that happened last night, I want everyone to repeat after me. I want to talk to my lawyer. TEMPLE-RASTON: Drugs weren't just a way to have fun. They were seen as a way to expand abilities. Adrian saw them as a way to expand his powers, too. On drugs, he felt invincible. He was a super coder. MURPHY: He took those drugs, and he did a lot of remarkable things. We would never have heard of him if he hadn't done these remarkable things. TEMPLE-RASTON: Lorraine Murphy first met Adrian on Facebook. MURPHY: I'm in over 600 Facebook groups. One of them I joined - and I was delighted to join - was called 2600. TEMPLE-RASTON: Facebook's 2600 group grew out of a magazine of the same name, 2600: The Hacker Quarterly. It was founded back in 1984 and had become like a bible for people who were testing the security of computer systems. It was full of technical information and invitations to meet up with other hackers around the world. MURPHY: I'm not a programmer, but it was really interesting to me what they were doing and how they explained it. And it sort of became my job to explain hacking in plain English, so I learned enough to follow along. TEMPLE-RASTON: And eventually, she followed along so well they made her moderator for the group, and the person she reported to was Adrian. They knew each other for years, and Murphy liked Adrian but still was suspicious of him. MURPHY: The claim was that Adrian was using the group to spy on people, and I'm like, they're public Facebook posts, dude. Anyone can read them. I don't think he's using it to spy on people. What he was doing was using Facebook to find people to spy on. TEMPLE-RASTON: People to spy on later. She always believed that Adrian was working for the government. MURPHY: He told me at one point that his job was to provide intel on non-Americans operating outside the United States. TEMPLE-RASTON: He never actually revealed precisely who he worked for. MURPHY: He never said, but either the U. S. government or a contractor who is reporting to the U. S. government. TEMPLE-RASTON: Was Adrian a government agent? Was the Manning episode the beginning of a long, secretive relationship between Adrian and the intelligence community? Certainly, conspiracy theorists thought so, so when the coroner listed his cause of death as undetermined, they went wild. (SOUNDBITE OF MONTAGE)UNIDENTIFIED PERSON #12: This guy's got a lot of history. UNIDENTIFIED PERSON #13: This is very curious, folks. UNIDENTIFIED PERSON #14: It seems to suggest that this person was literally a government agent. UNIDENTIFIED PERSON #13: This guy was offed. UNIDENTIFIED PERSON #14: Something out of Jason Bourne. . . UNIDENTIFIED PERSON #12: And you didn't even know that he died. TEMPLE-RASTON: There were lots of other questions. What was Adrian doing in Wichita, and why did police find him in the Shady Brook Senior Apartments if he was only 37 years old? Those close to Adrian had questions, too. His father was suspicious, and he wondered, among other things, where had all his son's computers gone? Hackers who were still talking to Adrian wondered why he disappeared from the Web a week before he died. That was way out of character. Were his killers cleaning a crime scene or moving the body? Then, during the autopsy, something else a little odd, something the local medical examiner had never seen before - on Adrian's left thigh under his clothes, there was a sticker with a name and an address. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #15: Do you typically find stickers on dead bodies? UNIDENTIFIED PERSON #16: That was a first for me. UNIDENTIFIED PERSON #15: Did you look to see if there was anything under it? UNIDENTIFIED PERSON #16: Yes. We took the sticker off. There was nothing under it. UNIDENTIFIED PERSON #15: No needle marks or anything like that? UNIDENTIFIED PERSON #16: No. TEMPLE-RASTON: The sticker read, Adrian Lamo, Assistant Director, Project Vigilant. Was Adrian trying to tell the world where to begin investigating? MURPHY: I mean, the last conversation we had was basically - homeless in Wichita is what he said to me. I said, how are you doing? He said, homeless in Wichita but better than a lot of people. TEMPLE-RASTON: Our show today is about the mysterious death of a hacking pioneer, Adrian Lamo. I'm Dina Temple-Raston, and you're listening to I'LL BE SEEING YOU from NPR. After the break, we dig into the conspiracy theories that have swirled around the death of Adrian Lamo. (SOUNDBITE OF ARCHIVED RECORDING)LAMO: I can't come to the phone right now due to connectivity issues, distraction or my death. TEMPLE-RASTON: That's an old voicemail greeting of his. (SOUNDBITE OF ARCHIVED RECORDING)LAMO: If I'm dead, I'm telling you that I love you from beyond the grave. You should consider this moment rather unique. Thank you, and have a wonderful day. TEMPLE-RASTON: Stay with us. TEMPLE-RASTON: From NPR, this is I'LL BE SEEING YOU, a four-part series about the technologies that watch us. I'm Dina Temple-Raston, and on today's show, we're investigating the death of a famous hacker, Adrian Lamo. He died of undetermined causes last year, and anyone following the story would have been pulled into a world of conspiracy theories. To search for what really killed Adrian Lamo, we had to go to where it happened - Wichita, Kan. When we started our investigation, we could see how the conspiracy theories developed. I mean, why was he in a senior living facility? Was it a safe house? Had the government put him there? And what about the sticker they found on Adrian's body and the mysterious words printed on it? Project Vigilant - did that had anything to do with his death? The answers had to be in Wichita, and right when we got there, we found out that Adrian's characterization of his time in Wichita began with a lie. He had told Lorraine Murphy and some of his other friends that he was homeless, but he wasn't. For most of the time that he was in Wichita, Adrian Lamo was living with Debbie and Bill Scroggin, parents of a friend of his who had taken him in. They lived in a single-story ranch house on five acres at the end of a dirt road. DEBBIE SCROGGIN (FRIEND OF ADRIAN LAMO): Hi. I'm Debbie. Come on in. TEMPLE-RASTON: Right away, Debbie told us living with Adrian could be weird. He was up a lot at night, wandering around in the dark. SCROGGIN: I could hear him, and I'd see this little flashlight going down the hall. And he always slept either on the couch, and if he slept on the bed, it was always on top of it. TEMPLE-RASTON: Sometimes, he'd just pile up his clothes and sleep on top of them like he was preparing for a quick getaway. And then there were the mysterious packages that arrived on the doorstep. SCROGGIN: He used our mailing address. BILL SCROGGIN (FRIEND OF ADRIAN LAMO): Did not use his real name. Most of the stuff that came would be to Adrian Alfonso. SCROGGIN: Alfonso. . . SCROGGIN: His middle name. TEMPLE-RASTON: And while he didn't seem to have a paying job, he was hard at work in the basement. SCROGGIN: Doing some research that had to do with the dark web, hacking into ISIS stuff. . . TEMPLE-RASTON: And he seemed to suggest that he was in Kansas on a secret assignment for Project Vigilant. SCROGGIN: It might have had something to do with the Department of Homeland Security, but I can't say that for sure. TEMPLE-RASTON: There were DHS stickers on nearly all of his notebooks downstairs, which seemed kind of weird. Why would you have DHS stickers on everything if it was supposed to be a secret? SCROGGIN: But yeah, I think in his own mind, he worked for this country, and you know what? SCROGGIN: I do believe that he kind of thought that he was an agent in some way. TEMPLE-RASTON: Was he working undercover for Project Vigilant? What was Project Vigilant? A man named Chet Uber incorporated the company in Florida in 2011. Chet Uber was on the other end of the line of those Skype calls Adrian's ex-wife Lauren had overheard. We dug up the company records, and it had nine corporate officers and directors. Adrian was one of them, and we started calling the others. DUANE JOHNSON (FORMER CHIEF RESEARCH OFFICER, AMES LABORATORY): Hello. Duane Johnson. TEMPLE-RASTON: Duane Johnson was listed as their chief technical officer, but here's the thing. Before we called him, he says he'd never heard of Project Vigilant. We sent him the incorporation papers, and Johnson says he thinks he knows how they came up with his title. JOHNSON: It was using a title that was closely related to my title at the time. I was chief research officer of a laboratory. TEMPLE-RASTON: Not just any laboratory - he was the chief research officer at the Ames Laboratory, one of the Department of Energy's national labs. JOHNSON: I'm not sure how they chose me, but certainly, it was misappropriated with some kind of intent. TEMPLE-RASTON: So from the outset, right at the time of incorporation, there was something a little off about Project Vigilant. Other officers or directors we called had heard of Project Vigilant, but they declined to speak on the record because they had signed nondisclosure agreements. Some of them were former government officials from the Justice Department and DHS, and just as we were about to give up, a former NSA official named Ira Winkler called us back. IRA WINKLER (PRESIDENT, SECURE MENTEM): Hi. I'm Ira Winkler, president of Secure Mentem and author of. . . TEMPLE-RASTON: Winkler is a delightfully geeky guy who helps companies beef up their cybersecurity by probing their systems for vulnerabilities. It sounded a little like what Adrian used to do, but Winkler does it legally. Winkler said he met Chet Uber at a hacker's conference and he asked him to be part of this company. Winkler said he'd be happy to help, and it was that informal. He said he was made director of intelligence, and Adrian was supposed to pass anything he discovered along to him. The idea was to use hackers like Adrian to find bad people on the dark web and then use Project Vigilant as a vehicle to tell the authorities. WINKLER: It was supposed to look for illegal, immoral actions on the Internet that pertained to foreign intelligence, terrorism, child exploitation. TEMPLE-RASTON: Which more or less tracked with what Debbie and Bill Scroggin had told us. Getting to the dark web isn't as hard as it sounds. All you have to do to start is download something called Tor, which stands for the onion router. Tor is basically a Web browser, and the only thing you need to know about it is that it allows you to move around anonymously on the Web. It drags branches behind your digital footsteps so people can't tell where you're going or where you've been. The dark web itself isn't illegal, but not surprisingly, some people want to take advantage of its anonymity to do things they aren't supposed to do, like trafficking in porn or recruiting for ISIS. I logged on and found something called the Darknet Heroes League. It says it's a marketplace for drugs and it sells pot and opioids and benzos and steroids, among other things. And to give you an idea of what I'm seeing, it looks like the Web did 20 years ago. Remember when the fonts were all irregular, and there were shabby pictures with misspelled links beneath? It's like that. And the sellers here are actually rated, like eBay or Amazon, and they offer bargains. But if Adrian ever discovered anything criminal during his trips to the dark web, he never passed it along. Winkler said he never received anything from him. In the end, he said. . . WINKLER: What Project Vigilant did was absolutely nothing as far as I can tell. TEMPLE-RASTON: If it had a mysterious connection to the government, aside from listing former government officials as officers and directors, we couldn't find it. Adrian did get money from the government, but it was from the Defense Department just reimbursing him for travel expenses related to his testimony at Manning's court martial. The official documents we saw said that Adrian's relationship with the U. S. government ended in July 2011. As for the mystery of how Adrian wound up in a senior living facility, that was a lot easier to solve. The Scroggins sent him there. About a year before Adrian died, Bill Scroggin had come across an old camera and set it up in his office. He put it on motion activated. SCROGGIN: Kind of like fishing for catfish on trotlines. You put the bait on there. And you come back, and you check it four or five hours later and see if you've got anything. WINKLER: The fish he caught was Adrian, slipping into the office with his flashlight. Scroggin said he was looking for some medications he could steal. It had happened before, and now he was caught on camera doing exactly that. SCROGGIN: The temper I - got a hold of me, and I literally blew up. (SOUNDBITE OF ARCHIVED RECORDING)SCROGGIN: And then in here is your computer, your books. . . TEMPLE-RASTON: Debbie helped Adrian pack up, and he secretly recorded it. (SOUNDBITE OF ARCHIVED RECORDING)SCROGGIN: Socks, underwear, your meds. LAMO: (Unintelligible). SCROGGIN: I'm sorry, Adrian. TEMPLE-RASTON: He went to a nearby homeless shelter, and then Debbie found him an apartment. It happened to be in that senior living facility. It turns out anyone with low income could qualify to live there. I saw some of Adrian's tax returns. He was declaring less than a thousand dollars a year in income. He was on public assistance, and the Scroggins helped him out with the rest. SCROGGIN: We gave him a coffeemaker. We gave him some furniture. But I was like, Adrian, don't you want to buy a new mattress, a bed? No, the couch is fine. So he didn't even have a bed in his apartment. TEMPLE-RASTON: The manager of Shadybrook Senior Apartments is the one who found Adrian's dead body lying on a pile of clothes in the bedroom. She pulled the medical alert cord in the apartment to call 911. It was typical of the calls that came in from a place like that. (SOUNDBITE OF ARCHIVED RECORDING)\rUNIDENTIFIED PERSON #17 (911 OPERATOR): 911 Wichita. TEMPLE-RASTON: The alert, with so few details, was a metaphor for what Adrian's life had become. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #18: Don't know. Don't know. Don't know. Don't have any information on them. TEMPLE-RASTON: Investigators found an apartment in complete disarray - huge piles of trash, dirty dishes, pills and powders everywhere. The medical examiner took photographs and then loaded Adrian's body into a van. Debbie Scroggin called Adrian's father and then went out to the apartment to tidy it up a bit before he arrived. SCROGGIN: One of the things I did that I probably shouldn't have done is I threw away all the empty - his prescription bottles and all of those things. TEMPLE-RASTON: She told us that Adrian called his father only when he had good news, when he'd learned to make lasagna or to tell him about Christmas presents. She didn't want Adrian's father to see how Adrian was living or how many pills he was taking. SCROGGIN: He said where's his medications? I was like, oh, I don't know. Somebody may. . . TEMPLE-RASTON: She was trying to protect him, and in a way, protect Adrian, too. (SOUNDBITE OF SONG, \"CHANGES\")PHIL OCHS (SINGER): (Singing) Sit by my side. Come as close as the air. TEMPLE-RASTON: There were only a handful of people at Adrian's memorial service. This is music from a video Adrian's father made for the occasion. (SOUNDBITE OF SONG, \"CHANGES\")OCHS: (Singing) And wander in my words. Dream about the. . . BLAKE: I was the only one of Adrian's, like, friends that was there. TEMPLE-RASTON: That's Andrew Blake again. BLAKE: No one his, you know, his age, no one who knew him, you know, besides his father for more than a few years. And just knowing that had I not gone that no one besides the people in Kansas and his father would have been there, like, that baffled me. TEMPLE-RASTON: Blake said Adrian wasn't so much forgotten as unforgiven. BLAKE: I think just people tended to associate Adrian with the Adrian who snitched on Manning, not the Adrian who did a whole bunch of cool other stuff. (SOUNDBITE OF SONG, \"CHANGES\")OCHS: (Singing) . . . The pictures that I play of changes. TEMPLE-RASTON: So by now, we understood what Adrian was doing or not doing for Project Vigilant, how he ended up in Wichita and why he was living in a senior apartment. But we still hadn't ruled out murder. Even the local medical examiner's office stopped short of doing that. SCOTT KIPPER (DEPUTY MEDICAL EXAMINER, SEDGWICK COUNTY, KS): There are some things that can be done to a body that leave minimal or no findings at autopsy. TEMPLE-RASTON: That's deputy medical examiner Scott Kipper. For example? KIPPER: Things that I would rather not discuss on the radio. TEMPLE-RASTON: You don't want to discuss it on the radio show because you don't want to give anybody any ideas? KIPPER: That's correct. TEMPLE-RASTON: Dr. Timothy Rohrig is the county's chief medical examiner, and he began reading through the list of chemicals he'd found in Adrian's bloodstream. TIMOTHY ROHRIG (CHIEF TOXICOLOGIST, SEDGWICK COUNTY, KS): Phenazepam, etizolam, flubromazepam, Benadryl, chlorpheniramine, citalopram, gabapentin. TEMPLE-RASTON: The coroner's list didn't surprise Debbie Scroggin. SCROGGIN: He would overmedicate because his anxiety was so high. There were times where he would supplement just to come up to have dinner. And he'd fall asleep in his food, literally it - face down in his food. TEMPLE-RASTON: About a month before Adrian died, the FDA came out with an alert - a warning, really - against mixing benzodiazepines with kratom. It had been linked to dozens of deaths. Dr. Rohrig said Adrian had a handful of what he called designer benzos in his system, some of which weren't available by prescription here in the U. S. ROHRIG: The most common way of getting these particular ones was basically off the Internet. You can order them and have them shipped to whatever address you want. TEMPLE-RASTON: Debbie Scroggin figured there were lots of pills and supplements coming into the house in those packages addressed to Adrian Alfonso. Adrian had left a voice note to himself just hours before he died. He was clearly in pain. (SOUNDBITE OF ARCHIVED RECORDING)LAMO: I really hurt a muscle, so it's hard for me to move around. Agonizing pain from a twisted leg, period. TEMPLE-RASTON: Because kratom isn't regulated by the FDA, it's impossible to tell if Adrian was ingesting potent doses of it on one day and weak doses the next. It can change that much from batch to batch. BERTHA MADRAS (PROFESSOR, HARVARD MEDICAL SCHOOL): It's a strange drug. It has some of the characteristics of pure opioid, which means it can cause sleeplessness. It can. . . TEMPLE-RASTON: Dr. Bertha Madras is a professor of psychobiology at Harvard Medical School and a former member of the president's commission on combating drug addiction and the opioid crisis. While she wouldn't say exactly what killed Adrian Lamo, she did say that people who were mixing things like kratom with benzos and other natural supplements were essentially conducting their own human experiments. MADRAS: They have no clue what they're putting into their body and what the consequences could be. TEMPLE-RASTON: So this is where all the evidence pointed us. Hacking may have killed Adrian Lamo, but it wasn't the Internet kind. His body hacking, the constant intake of pills and powders and liquids, is likely what did him in. (SOUNDBITE OF ARCHIVED RECORDING)LAMO: Hey. This is Adrian. I'm not ignoring you on purpose. TEMPLE-RASTON: This was the last voicemail Debbie Scroggin received from Adrian a few days before he died. (SOUNDBITE OF ARCHIVED RECORDING)LAMO: I had trouble with my phone. Give me a ring or a note when you can. My phone service is active again. Love you. Bye. TEMPLE-RASTON: It offered a clue. Hackers had noticed that Adrian hadn't been on the Internet the week before he died, and that seemed suspicious, but there was a simple answer. He hadn't paid his cell phone bill, and he used his cell phone to get online. In retrospect, as we retraced Adrian's steps during the last two years of his life, it's clear that there were no assassins lying in wait, no government officials eager for a briefing. Adrian was profoundly alone. After he turned in Manning, the hacker community went one way, and Adrian went another. And if you look at all we uncovered, it's easy to imagine that Adrian's last night went something like this. After spending some time on the computer and having dinner, he took something to help him relax and maybe ease some of that muscle pain. He went into the bedroom, laid down on the clothes, curled up and just stopped breathing. It wasn't a murder or a suicide. It was an accident. And that left us with just one unsolved mystery - that address label they found on Adrian's thigh. It read Adrian Lamo, Assistant Director, Project Vigilant, 70 Bates Street, Washington, D. C. So we looked up the property records of the place - who owned it, any renters. Project Vigilant wasn't registered there, but there was one name I did recognize. BLAKE: That's an address that I lived at for a brief time, and Adrian stayed with me occasionally off and on. TEMPLE-RASTON: That's Adrian's friend Andrew Blake, the one who went to the funeral. He didn't even know the sticker existed until he read about it in the autopsy report. BLAKE: That's when I laughed, and that's actually the first time in the weeks after his death where I actually kind of felt a little bit of closure. It almost felt like a joke from Adrian to me, maybe just like a signal. MURPHY: That Project Vigilant sticker - I think maybe it was where he put his hopes, and it didn't go anywhere, so maybe he just wanted to be reminded of that. TEMPLE-RASTON: And we have an epilogue here. A couple of months ago, we reached out to Chelsea Manning. She's being held in a detention center in Alexandria, Va. , for refusing to testify against that crazy white-haired guy she told Adrian about, Julian Assange. Through her lawyer, I asked her if she forgave Adrian, and she said there was nothing to forgive. In a handwritten note that she passed to us, she wrote, I've never had any ill will toward Adrian at any time. And then she added, I'm more mad at the government for using him. Adrian, had he lived, probably would have been a witness for the prosecution in the Assange case to talk about what Manning had told him about that crazy white-haired guy, and Adrian at last would've been back where he wanted to be - back in that public spotlight. (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LAPORTE: You know him as the guy who hacked Yahoo, AOL, Time Warner, MCI WorldCom, Microsoft and. . . TEMPLE-RASTON: I'LL BE SEEING YOU is written or reported by me, Dina Temple-Raston. Our producer is Adelina Lancianese, and she scored our show, too. Special thanks to NPR's investigations team, the NPR story lab and to Josephine Wolff of Tufts University. In our next show, AI and elephants - the beginning of a beautiful friendship. Until then, I'm Dina Temple-Raston, and I'll be seeing you. DINA TEMPLE-RASTON (HOST): This is I'LL BE SEEING YOU, a four-part series about the technologies that watch us from NPR. I'm Dina Temple-Raston, and today we're looking at the life and mysterious death of one of the world's most famous hackers, Adrian Lamo. And our story starts with the discovery of a body. (SOUNDBITE OF ARCHIVED RECORDING)\rUNIDENTIFIED PERSON #1 (911 OPERATOR): 911 Wichita. UNIDENTIFIED PERSON #2: Requesting a dispatch for commercial medical alarm. TEMPLE-RASTON: This is the actual 911 call that came in on March 14 of last year. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: OK, and tell me exactly what happened. UNIDENTIFIED PERSON #2: It just got this on 54 Room 222, alert by medical. UNIDENTIFIED PERSON #1: How old is the patient? UNIDENTIFIED PERSON #2: Don't know - don't have any information on them. UNIDENTIFIED PERSON #1: Patient male or female? UNIDENTIFIED PERSON #2: Don't know. UNIDENTIFIED PERSON #1: Are they breathing? UNIDENTIFIED PERSON #2: Don't know. TEMPLE-RASTON: All the caller seemed to be sure about was that something bad had happened and someone needed to get there quickly. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: Call back if anything changes. UNIDENTIFIED PERSON #2: Thank you. UNIDENTIFIED PERSON #1: Thank you. TEMPLE-RASTON: This was probably one of the few moments in Adrian Lamo's life when he was truly anonymous because before all this happened, he was a hacking rock star. (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LEO LAPORTE (TELEVISION HOST): You know him as the guy who hacked Yahoo, AOL, Time Warner, MCI WorldCom, Microsoft and very famously, the New York Times. Adrian Lamo, one of the most celebrated hackers in the world today, welcome to \"The Screen Savers. \"TEMPLE-RASTON: \"The Screen Savers\" was an old cable television show all about technology. It aired in the late '90s and early 2000s, and the host was a guy named Leo Laporte. (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LAPORTE: Do you define yourself as a hacker? Do you consider yourself a hacker? ADRIAN LAMO (HACKER): It's not a term that I try to sell myself as. LAPORTE: Yeah. LAMO: People use hacker to mean a lot of different. . . LORRAINE MURPHY (JOURNALIST, FRIEND OF ADRIAN LAMO): He definitely had an original approach to things. TEMPLE-RASTON: Lorraine Murphy writes about hacking, and she knew Adrian for years. MURPHY: The hacker mindset is all about looking at something and going, how can I use this for something it wasn't intended to? Or, how far can I push this before it does something unpredictable? He definitely had that. TEMPLE-RASTON: She says that because Adrian came to the hacking community so early, he was a bit of an inspiration, even a hero of sorts. People asked his advice, they wanted his approval, and he had almost a cult following. MURPHY: He's like the Tony Robbins of the hacking world. It's one thing to be gifted at hacking. It's another thing to be able to tell a world of civilians that the thing exists and you are good at it, and Adrian had both of those. TEMPLE-RASTON: Adrian Lamo was born in Boston, but he grew up outside Bogota, Colombia. His early hacker's resume tracks like that of just about every hacker I've talked to. He got a hand-me-down Commodore 64 as kid. He hacked into computer games. He played with viruses on floppy disks. Floppy disks - remember those? And eventually, he began tapping into strangers' phone lines and finding ways to spoof the phone company to get free long-distance calls. When his family moved to California, that only made it easier to pursue his interest in computers. After that, what we have is a persona Adrian Lamo carefully created. He thought of himself as an ethical vigilante, a gray hat hacker, a self-styled Robin Hood of the early Internet. And like Robin Hood, he took aim at the king - one of the largest internet providers in 2001, a company called MCI WorldCom. MCI would say later that Adrian slipped in through a security hole. He'd found a piece of bad code that allowed him to fool the network into thinking that he belonged there. But according to Adrian, the hack itself was even simpler than that. He got into MCI by guessing passwords. As it turns out, the company had set up temporary passwords for employees based on Social Security numbers. Adrian simply Googled some employees, quickly found their Social Security numbers online, and a short time later, he was inside the network. Getting into these networks, he told anyone that would listen, was incredibly easy. (SOUNDBITE OF ARCHIVED RECORDING)LAMO: Some of the companies that you do business with every day had passwords that were simple dictionary words, names of animals. TEMPLE-RASTON: Adrian explained how he broke into MCI in an unreleased documentary that he started. It was called \"Hackers Wanted. \"(SOUNDBITE OF ARCHIVED RECORDING)LAMO: Anyone at all with this information could have connected to each and every router that handled the data for Bank of America, Ford, Chrysler, NASA. There were very detailed. . . TEMPLE-RASTON: We hear about things like this all the time now, but back when Adrian was saying all this, people were just starting to realize how unsafe the Internet was. Adrian was foreshadowing the future, warning that once bad actors figured out how easy it was to get into network systems, no one would be safe. (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LAPORTE: You may remember Adrian Lamo. He's been on the show before. KEVIN ROSE (TELEVISION HOST): Been on the show a couple of - real nice guy. LAPORTE: Wonderful guy - he's a hacker but a kind of a gray hat hacker. TEMPLE-RASTON: \"The Screen Savers\" show again. . . (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")ROSE: Right. He doesn't steal any information. He doesn't take anything and use it for bad. He's a good hacker. LAPORTE: Well, I'll give you an example. TEMPLE-RASTON: Back when Adrian and his friends were cracking into computer networks, the goal was just to see if they could. What they were looking for was bragging rights, tinkering in order to learn. MURPHY: Adrian was never really monetized. He was not motivated primarily by money. Media, fame - that sort of thing motivated him. TEMPLE-RASTON: Here's Lorraine Murphy again. MURPHY: He wanted to be a household name. TEMPLE-RASTON: Some of the hacking tools Adrian helped develop back then are still being used by the hacker community today. What made Adrian a little different from the others around him, though, is that he saw darker forces forming. He could imagine a day when technologies would goose-step out of the pages of science fiction into our daily lives, and these technologies would allow governments, bad actors, even companies to watch us without our knowing. But Adrian went too far when he hacked into the New York Times. The way it all unfolded would sound familiar to anyone who was in the hacking underground at the time. Adrian was very good at figuring out passwords, either getting someone to unwittingly give him one or guessing at default passwords that hadn't been changed. That's how he got into the internal server at the Times. He gave himself administrator credentials and a login and a password for their LexisNexis account. Then he played a little joke. He added himself to the paper's internal database of experts and listed himself as an expert in hacking. Pretty funny in a hacker kind of way, but The New York Times didn't exactly see the humor in it. The newspaper pressed charges, and in August 2003, the FBI issued a warrant for Adrian's arrest. A reasonable person could ask, why does it seem these hacks keep on happening? Lorraine Murphy says part of it has to do with the way Web sites are designed. MURPHY: Well, when you have a Web site, you need to be able to let people into it, like, under the hood into the engine to tinker with it. That's everybody from the reporters who have to be able to put up information to the IT guys who have to be able to work on the actual machinery of the Web site. TEMPLE-RASTON: So that's the first part. A Web site is always being updated, so the system has to be open, and Adrian took advantage of that. The second part is more fundamental. Most of the code that holds Web sites together is problematic. It's buggy, which means it's full of mistakes, full of holes, which makes it really easy for hackers to get in. Here's an amazing fact. Back in Adrian's day, commercial software like Windows XP contained 20 to 30 bugs for every thousand lines of code. That means there could have been close to a million bugs in that operating system alone. And the problem is better now. The latest version of Windows operating system, Windows 10, is thought to have about five bugs per thousand lines of code. The more surprising thing is that you don't need to be a coding genius to find those bugs. You can actually buy software that finds them for you. MURPHY: You can buy those programs on the dark web, and you can buy them in plain sight. There are Facebook groups that sell this kind of code. Facebook keeps trying to shut them down, but I've been in one for years and years. TEMPLE-RASTON: So let's say you're a hacker just starting out. All you have to do is download the software, get the code for some Web site you want to hack and then run the program and see what it does. You don't need to code anything. Those overseas scammers trying to steal your personal information, they aren't computer geniuses. They just know where to buy the cheap sniffer programs - kind of brilliant, and at the same time, kind of scary. Adrian was known to buy the odd sniffer program to save time, but what he was really good at was social engineering. . . MURPHY: Which is more or less the con man skill set. He was very good at impersonating people, impersonating entire groups. He was great at creating personas and getting you to believe the persona. TEMPLE-RASTON: In real life, Adrian could be socially awkward and anxious, but on the Internet, he was bold, taking on the darker forces of corporate America. One of the most popular cartoons in New Yorker Magazine history ran in 1993. It was a drawing of a dog sitting on a chair behind a keyboard and a computer screen, and he's chatting to another dog sitting beside him, who's watching him type. On the Internet, he says, nobody knows you're a dog - which, if you think about it, is one of the things we love about the Internet. We can be whoever we want to be. Certainly, that was one of the things that Adrian loved. (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LAPORTE: There is a big story just breaking now. ROSE: Right now. . . LAPORTE: We wanted to bring it to you. We've got an exclusive scoop on this. Things have turned bad, I guess, for Adrian Lamo because. . . TEMPLE-RASTON: What had turned bad was the New York Times hack we just talked about. Adrian was charged under the Computer Fraud and Abuse Act, a cybersecurity law which, at the time, had been rarely enforced. The act codified the computer equivalent of trespass. You didn't need to steal anything or be actively malicious to run afoul of it. Just gaining unauthorized access to a system and having a company decide to press charges was enough to trigger it. Adrian pleaded guilty to a felony and was sentenced to six months of home detention and two years probation. His conviction was a reminder to the wider world that hacking was now seen as a clear and present danger. To the hackers themselves, it signaled that they could no longer crack into internal servers with impunity. The world was changing. Adrian, for his part, acted contrite. . . (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LAMO: I do think that there are some lines that I stepped over in my access. I want to take responsibility for this. LAPORTE: Sure. LAMO: I want to put it behind me. TEMPLE-RASTON: . . . But not so humbled that he couldn't resist poking fun at the whole episode. (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LAMO: On a tangentially related note, the U. S. marshals actually let me retake my mugshot until I thought I looked pretty, so. . . LAPORTE: You're kidding. Really? TEMPLE-RASTON: And Adrian was pretty - olive skin, light eyes, dimples, kind of impish - and his mugshot was pretty, too. He was half-smiling, looking a little smug. And it says something about Adrian that he expected that story to turn out completely differently. MURPHY: He was really appalled that he didn't get a job offer out of that, actually. TEMPLE-RASTON: What job was he expecting, IT analyst? MURPHY: Well, security consultant. TEMPLE-RASTON: OK. MURPHY: Literally the pipe dream of every, you know, best kid in the drama club at high school is to go to Broadway. The pipe dream of every skid in every hackerspace in the world is to get a paid job from a major corporation as a security consultant, and all you do is sit there all day and find their weaknesses. TEMPLE-RASTON: OK, that might not be what everyone or every skid in the notoriously anti-establishment hacking community wants, but the fact that Adrian wanted that kind of job so badly shows how much of a white hat he aspired to be right up until the time he died. Our show today is about the mysterious death of a hacking pioneer, Adrian Lamo. I'm Dina Temple-Raston, and you're listening to I'LL BE SEEING YOU from NPR. After the break, how does one man go from computer hero to hacking pariah? ANDREW BLAKE (FRIEND OF ADRIAN LAMO): People hated him. He couldn't log onto any sort of Internet platform without instantly getting some sort of hate directed toward him. TEMPLE-RASTON: Stay with us. TEMPLE-RASTON: This is I'LL BE SEEING YOU from NPR. I'm Dina Temple-Raston. And on the show today, we're investigating the mysterious death of one of the world's most famous hackers, Adrian Lamo. And as you'll hear, the deeper we dug into this, the weirder it got. It began as a story about an unexpected death, and then it became something else - a story not just about hacking in the Internet but about how hacking has evolved. It's gone from kids doing something slightly subversive to global syndicates trying to alter the course of history. And most people didn't notice that change until this. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #3: I got a black vehicle under target. It's arriving right to the north of the mosque. UNIDENTIFIED PERSON #4: Yeah, I would like that. Over. TEMPLE-RASTON: That's from a classified military video that was leaked back in 2010. It was filmed from the gun sights of an American helicopter in Iraq. It was all shot in black and white and runs for about 39 minutes. You're hearing actual conversations between the pilots of two Apache helicopters. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #5: Hotel 26, this is Crazy Horse 18. TEMPLE-RASTON: The helicopter is flying over a residential neighborhood. And from the camera's viewfinder, you can see low cinderblock buildings and some palm trees and a mosque. Then the camera angle shifts, and it zooms in on a handful of men walking down the street. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #5: I have individuals with weapons. UNIDENTIFIED PERSON #6: . . . Four radio. UNIDENTIFIED PERSON #5: Yep, he's got a weapon, too. UNIDENTIFIED PERSON #3: All right, we got a guy with an RPG. UNIDENTIFIED PERSON #5: I'm going to fire. UNIDENTIFIED PERSON #3: You're clear. UNIDENTIFIED PERSON #5: All right, firing. TEMPLE-RASTON: And then everything changes. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #3: Light them all up. UNIDENTIFIED PERSON #5: Come on, fire. (SOUNDBITE OF GUNFIRE)UNIDENTIFIED PERSON #3: Keep shooting. (SOUNDBITE OF GUNFIRE)\rTEMPLE-RASTON: We know now that this was live footage of a terrible mistake, and the weapons the pilots thought they saw weren't weapons at all. It was actually a camera with a telephoto lens. The men they thought were the enemy were actually reporters - Namir Noor-Eldeen, a Reuters photographer, and his assistant and driver, Saeed Chmagh. Twelve people were killed in the assault. The video, when it was released, raised questions about civilian casualties and American rules of engagement in Iraq. And it focused the world's attention on three things - the little-known organization called WikiLeaks, the young Army intelligence analyst who leaked it and, rather improbably, Adrian Lamo. Now, if you don't remember the story, this is how it unfolded with a few new details. About two months after the helicopter video went viral, Adrian got an instant message from Chelsea Manning. She began by telling him that she had a copy of a documentary Adrian is starred in on her desktop. So Adrian thought it was going to be just another conversation from a fan. And then, all of a sudden, it wasn't. Manning started talking about her family, how hard it was to be in Iraq, about her gender identity issues and how she had to hide them from people she was working with. I know all about creating a persona, Adrian wrote. And then Manning went a step further - I think I'm in more potential heat than you ever were. How so, Adrian asked. And then she told him, told him about how she'd passed the now-famous helicopter video to WikiLeaks, along with hundreds of thousands of classified State Department cables. She'd been in touch with a crazy white-haired Aussie who can't seem to stay in one country very long. She was talking about Julian Assange, the head of WikiLeaks. He's fighting extradition from the U. K. to the U. S. now. I can't believe what I'm telling you, Manning wrote. Adrian couldn't believe it either. GLENN MORROW (COUSIN OF ADRIAN LAMO): I don't think he anticipated, when he started, the gravity of what Manning was actually saying. TEMPLE-RASTON: That's Glenn Morrow, Adrian's cousin. And they were close. He and Adrian had talked about all this while it was happening, and he remembers Adrian saying that, at first, he thought Manning was just another hacker looking for affirmation. MORROW: When you reach a sort of critical mass of fame, you just have so many people come out of the woodwork, so many, you know, people wanting to share something they've discovered or something they've done. TEMPLE-RASTON: But the Manning conversation was about breaking the law, not in the ambiguous way that Adrian was used to but in what seemed to him a very black-and-white way. Manning had leaked classified documents. Adrian quizzed her about what she downloaded, and Manning said she didn't really look at what they were. She just passed them along. The more Manning revealed, the more unsettled Adrian became. So he made a choice, and his choice altered the course of his life and Manning's, too. He called the authorities. MORROW: Once it became clear that it was such a serious thing that had happened, I don't think he could stand by and really live with the implications of just sitting on it. TEMPLE-RASTON: Chelsea Manning was arrested within days. Adrian thought he'd be celebrated as a patriot. Chelsea Manning was in a war zone, vacuuming up classified documents and distributing them. To Adrian, it didn't seem like a close call. Here's Lorraine Murphy, again. MURPHY: And it backfired, spectacularly, on him. TEMPLE-RASTON: What Adrian hadn't fully understood was how people in the hacker community would react. He was sure he could make them see how he had no choice but to turn Manning in. Weeks later, he crashed headlong into the reality of what he had done at a New York hackers' conference called HOPE. MORROW: Hope stands for Hackers on Planet Earth. TEMPLE-RASTON: Glenn Morrow went with him, and they rather naively thought it would be a great opportunity to meet people. It sounds crazy now, but it hadn't occurred to either one of them that Adrian's decision to turn in Manning would hijack the conference. MORROW: The first day at the conference, there was a lot of people, you know, yelling out snitch, at least one occasion that I recall of somebody spitting in his direction. TEMPLE-RASTON: And then the organizers hastily put together a panel they called Informants - Villains or Heroes? (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #7: Should we avoid the controversy, or should we dive right into it? I say, of course, we dive right into it. We confront this thing head on, right? (APPLAUSE)UNIDENTIFIED PERSON #7: With that, I'd like to introduce Adrian Lamo. And we'll let him say his piece, ask some questions and hopefully learn something. Adrian. (BOOING)UNIDENTIFIED PERSON #7: All right, you can boo. Go ahead. TEMPLE-RASTON: The audience was clearly against Adrian. One after another, people came to the microphone to berate him for violating the unwritten rules of hackerdom. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #8: I think we need to be clear about what's acceptable to do and what's not acceptable to do. And I think you [expletive] up huge. (APPLAUSE)UNIDENTIFIED PERSON #9: I see what you have done as treason. (APPLAUSE)UNIDENTIFIED PERSON #10: I think you belong in Guantanamo. TEMPLE-RASTON: As the group saw it, Adrian had broken the hacker code. (SOUNDBITE OF ARCHIVED RECORDING)MARK ABENE (HACKER): As soon as you make up your mind to choose a side, politically speaking, you cease to be a hacker. TEMPLE-RASTON: That's a guy named Mark Abene, better known as Phiber Optik. He was a high-profile hacker in the 1980s and 1990s and an icon of sorts. He made clear he thought Adrian had crossed a line. Hackers were supposed to be faintly subversive, not law enforcement informants. (SOUNDBITE OF ARCHIVED RECORDING)ABENE: You had a choice, and you made the wrong choice. You could've simply walked away, and none of this would've happened. LAMO: I could have, but I wouldn't have been able to live with myself. (APPLAUSE)ABENE: I disagree. MORROW: It was a little tough for me to hear. Everybody knew who he was. And up until that point, Adrian had been, you know, a hero. In the culture, you know, the worst thing you could be was a snitch. TEMPLE-RASTON: It didn't help matters that all this happened at a time when hackers were just beginning to consider the moral implications of what they were doing. MURPHY: And in the early days, the hackers did not think that there were rules when it came to websites. It was the Wild West. It wasn't against the law to hack a particular website for years and years and years. TEMPLE-RASTON: Adrian had forced the community to address fundamental questions, like what did ethical hacking really mean? If you cracked into someone's system but you didn't do any damage, was that OK? And if someone tells you they're leaking classified information, are you obliged to say something? They all wanted hacking to remain a force for good, but they weren't quite sure how to make that happen. After the conference, there was no ambiguity about how the community felt. Adrian was shunned. He received death threats. Fake bombs were mailed to his parents in California. Rumors went around that Adrian was actually a spy, ratting out fellow hackers to the government. BLAKE: People hated him. He couldn't log on to any sort of Internet platform under his actual name without instantly getting some sort of hate directed toward him. TEMPLE-RASTON: Andrew Blake is another longtime friend of Adrian's. BLAKE: Even when Adrian would do something with the absolute best of intentions, as soon as anyone realized that it was Adrian Lamo who did it, they didn't want anything to do with it. LAUREN FISHER (EX-WIFE OF ADRIAN LAMO): He used to say that I like to believe in a world where things can happen, even if I have to do them myself. He just liked to make the extraordinary happen. TEMPLE-RASTON: That's Adrian's ex-wife, Lauren Fisher. This is the first time she's talked publicly about her marriage to him. And the reason why we're talking to her is because all these rumors about Adrian working for the government may have started with something she and Adrian did years before he'd ever heard of Chelsea Manning. They started a business together in 2008, and they called it Reality Planning. FISHER: Reality Planning was a go at getting him to have a sort of a la carte system where you could ask him to test your website or test your company. It was all very vague, but it was really just to get him back into the PR spotlight. And it kind of worked. TEMPLE-RASTON: Kind of worked because someone contacted them about speaking at a computer expo in Europe. Adrian was going to get them to pick up business-class airfare. He wanted a luxury hotel. But almost before they got out of the gate, there were unexpected complications from the State Department. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #11: We have a hold in our system on your passport application. I need to know whether or not you are still on probation. . . TEMPLE-RASTON: This is an actual voicemail they got at the time. Fisher had saved it, and it was about the felony conviction we mentioned earlier for hacking The New York Times. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #11: We have information indicating that you had some kind of legal matter or something going on. So give me a call if you have any questions. Have a good day. TEMPLE-RASTON: The Europe trip, speaking engagements, being in the spotlight once again - that never happened. Fisher says there were lots of things that didn't go their way back then. Anxiety often got the better of Adrian. Sometimes, he wouldn't leave the house for days. Fisher says it was around that time that she first heard Adrian mention something called Project Vigilant. She overheard him talking to people on Skype about a project that would use his hacking skills for good, something that would put him back on top. FISHER: It was just kind of like Reality Planning, though. It was just all vague and - but it seemed a bit - for me, it seemed a bit more - it seemed bigger, obviously. And it seemed more secret. TEMPLE-RASTON: All she knew was that it had something to do with a part of the Internet called the dark web. Beyond that, Adrian didn't seem to want to talk much about it. The secrecy was in keeping with the two Adrians she was always trying to keep up with. FISHER: There were times where we would be together, and he would be in the Adrian Lamo persona. We would go to a 2600 Hacker - the monthly meet up, you know, in San Francisco. TEMPLE-RASTON: We'll talk about 2600 in a minute. FISHER: And he liked to shine in his Adrian Lamo kind of persona. But there was also the times where he was just - the walls were down completely and he wasn't the Adrian Lamo that he himself made himself believe that he was, you know? TEMPLE-RASTON: And when that happened, he medicated, looking for some little door within himself that would control his anxiety. FISHER: It was body hacking, trying to contain all the different feelings and keep them in check. TEMPLE-RASTON: Valerian root, vitamins - the list was as long as your arm, and at some point - no one is quite sure when - that list included an herbal supplement called kratom. Traditionally, it's used in lighter doses for stress and anxiety. Kratom, which is legal in most states, made it easier to socialize, which just kept getting harder for Adrian. BLAKE: Hello? TEMPLE-RASTON: Hey. Andrew? BLAKE: Hello? TEMPLE-RASTON: Are you there? Can you hear me? BLAKE: Yeah. I'm the only one home all day today. TEMPLE-RASTON: That's Andrew Blake, a friend of Adrian's, and he helped Adrian out with a couch and a meal and a place to stay over the years. And he was well aware of Adrian's kratom use. BLAKE: He used to get it mailed in envelopes as just a fine powder. It's kind of like a flower, like a dust. And he didn't explain that kratom was supposed to work on the same brain receptors that opioids did. TEMPLE-RASTON: Blake told me that Adrian had given him kratom for Christmas, but he never got around to using it. BLAKE: Adrian would get it in, like, a big bag, and I'm looking to see if I could find one in, like, our pantry. TEMPLE-RASTON: So it's important to understand that hackers like Adrian look at drugs a little differently than most of us do. Early hacker conferences, like something called HoHoCon, were drug-addled and alcohol-soaked affairs. Attendees would party so hard they would get banned from hotels. (SOUNDBITE OF ARCHIVED RECORDING)DOUGLAS BARNES (MEMBER, ELECTRONIC FRONTIER FOUNDATION): As a result of things that happened last night, I want everyone to repeat after me. I want to talk to my lawyer. TEMPLE-RASTON: Drugs weren't just a way to have fun. They were seen as a way to expand abilities. Adrian saw them as a way to expand his powers, too. On drugs, he felt invincible. He was a super coder. MURPHY: He took those drugs, and he did a lot of remarkable things. We would never have heard of him if he hadn't done these remarkable things. TEMPLE-RASTON: Lorraine Murphy first met Adrian on Facebook. MURPHY: I'm in over 600 Facebook groups. One of them I joined - and I was delighted to join - was called 2600. TEMPLE-RASTON: Facebook's 2600 group grew out of a magazine of the same name, 2600: The Hacker Quarterly. It was founded back in 1984 and had become like a bible for people who were testing the security of computer systems. It was full of technical information and invitations to meet up with other hackers around the world. MURPHY: I'm not a programmer, but it was really interesting to me what they were doing and how they explained it. And it sort of became my job to explain hacking in plain English, so I learned enough to follow along. TEMPLE-RASTON: And eventually, she followed along so well they made her moderator for the group, and the person she reported to was Adrian. They knew each other for years, and Murphy liked Adrian but still was suspicious of him. MURPHY: The claim was that Adrian was using the group to spy on people, and I'm like, they're public Facebook posts, dude. Anyone can read them. I don't think he's using it to spy on people. What he was doing was using Facebook to find people to spy on. TEMPLE-RASTON: People to spy on later. She always believed that Adrian was working for the government. MURPHY: He told me at one point that his job was to provide intel on non-Americans operating outside the United States. TEMPLE-RASTON: He never actually revealed precisely who he worked for. MURPHY: He never said, but either the U. S. government or a contractor who is reporting to the U. S. government. TEMPLE-RASTON: Was Adrian a government agent? Was the Manning episode the beginning of a long, secretive relationship between Adrian and the intelligence community? Certainly, conspiracy theorists thought so, so when the coroner listed his cause of death as undetermined, they went wild. (SOUNDBITE OF MONTAGE)UNIDENTIFIED PERSON #12: This guy's got a lot of history. UNIDENTIFIED PERSON #13: This is very curious, folks. UNIDENTIFIED PERSON #14: It seems to suggest that this person was literally a government agent. UNIDENTIFIED PERSON #13: This guy was offed. UNIDENTIFIED PERSON #14: Something out of Jason Bourne. . . UNIDENTIFIED PERSON #12: And you didn't even know that he died. TEMPLE-RASTON: There were lots of other questions. What was Adrian doing in Wichita, and why did police find him in the Shady Brook Senior Apartments if he was only 37 years old? Those close to Adrian had questions, too. His father was suspicious, and he wondered, among other things, where had all his son's computers gone? Hackers who were still talking to Adrian wondered why he disappeared from the Web a week before he died. That was way out of character. Were his killers cleaning a crime scene or moving the body? Then, during the autopsy, something else a little odd, something the local medical examiner had never seen before - on Adrian's left thigh under his clothes, there was a sticker with a name and an address. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #15: Do you typically find stickers on dead bodies? UNIDENTIFIED PERSON #16: That was a first for me. UNIDENTIFIED PERSON #15: Did you look to see if there was anything under it? UNIDENTIFIED PERSON #16: Yes. We took the sticker off. There was nothing under it. UNIDENTIFIED PERSON #15: No needle marks or anything like that? UNIDENTIFIED PERSON #16: No. TEMPLE-RASTON: The sticker read, Adrian Lamo, Assistant Director, Project Vigilant. Was Adrian trying to tell the world where to begin investigating? MURPHY: I mean, the last conversation we had was basically - homeless in Wichita is what he said to me. I said, how are you doing? He said, homeless in Wichita but better than a lot of people. TEMPLE-RASTON: Our show today is about the mysterious death of a hacking pioneer, Adrian Lamo. I'm Dina Temple-Raston, and you're listening to I'LL BE SEEING YOU from NPR. After the break, we dig into the conspiracy theories that have swirled around the death of Adrian Lamo. (SOUNDBITE OF ARCHIVED RECORDING)LAMO: I can't come to the phone right now due to connectivity issues, distraction or my death. TEMPLE-RASTON: That's an old voicemail greeting of his. (SOUNDBITE OF ARCHIVED RECORDING)LAMO: If I'm dead, I'm telling you that I love you from beyond the grave. You should consider this moment rather unique. Thank you, and have a wonderful day. TEMPLE-RASTON: Stay with us. TEMPLE-RASTON: From NPR, this is I'LL BE SEEING YOU, a four-part series about the technologies that watch us. I'm Dina Temple-Raston, and on today's show, we're investigating the death of a famous hacker, Adrian Lamo. He died of undetermined causes last year, and anyone following the story would have been pulled into a world of conspiracy theories. To search for what really killed Adrian Lamo, we had to go to where it happened - Wichita, Kan. When we started our investigation, we could see how the conspiracy theories developed. I mean, why was he in a senior living facility? Was it a safe house? Had the government put him there? And what about the sticker they found on Adrian's body and the mysterious words printed on it? Project Vigilant - did that had anything to do with his death? The answers had to be in Wichita, and right when we got there, we found out that Adrian's characterization of his time in Wichita began with a lie. He had told Lorraine Murphy and some of his other friends that he was homeless, but he wasn't. For most of the time that he was in Wichita, Adrian Lamo was living with Debbie and Bill Scroggin, parents of a friend of his who had taken him in. They lived in a single-story ranch house on five acres at the end of a dirt road. DEBBIE SCROGGIN (FRIEND OF ADRIAN LAMO): Hi. I'm Debbie. Come on in. TEMPLE-RASTON: Right away, Debbie told us living with Adrian could be weird. He was up a lot at night, wandering around in the dark. SCROGGIN: I could hear him, and I'd see this little flashlight going down the hall. And he always slept either on the couch, and if he slept on the bed, it was always on top of it. TEMPLE-RASTON: Sometimes, he'd just pile up his clothes and sleep on top of them like he was preparing for a quick getaway. And then there were the mysterious packages that arrived on the doorstep. SCROGGIN: He used our mailing address. BILL SCROGGIN (FRIEND OF ADRIAN LAMO): Did not use his real name. Most of the stuff that came would be to Adrian Alfonso. SCROGGIN: Alfonso. . . SCROGGIN: His middle name. TEMPLE-RASTON: And while he didn't seem to have a paying job, he was hard at work in the basement. SCROGGIN: Doing some research that had to do with the dark web, hacking into ISIS stuff. . . TEMPLE-RASTON: And he seemed to suggest that he was in Kansas on a secret assignment for Project Vigilant. SCROGGIN: It might have had something to do with the Department of Homeland Security, but I can't say that for sure. TEMPLE-RASTON: There were DHS stickers on nearly all of his notebooks downstairs, which seemed kind of weird. Why would you have DHS stickers on everything if it was supposed to be a secret? SCROGGIN: But yeah, I think in his own mind, he worked for this country, and you know what? SCROGGIN: I do believe that he kind of thought that he was an agent in some way. TEMPLE-RASTON: Was he working undercover for Project Vigilant? What was Project Vigilant? A man named Chet Uber incorporated the company in Florida in 2011. Chet Uber was on the other end of the line of those Skype calls Adrian's ex-wife Lauren had overheard. We dug up the company records, and it had nine corporate officers and directors. Adrian was one of them, and we started calling the others. DUANE JOHNSON (FORMER CHIEF RESEARCH OFFICER, AMES LABORATORY): Hello. Duane Johnson. TEMPLE-RASTON: Duane Johnson was listed as their chief technical officer, but here's the thing. Before we called him, he says he'd never heard of Project Vigilant. We sent him the incorporation papers, and Johnson says he thinks he knows how they came up with his title. JOHNSON: It was using a title that was closely related to my title at the time. I was chief research officer of a laboratory. TEMPLE-RASTON: Not just any laboratory - he was the chief research officer at the Ames Laboratory, one of the Department of Energy's national labs. JOHNSON: I'm not sure how they chose me, but certainly, it was misappropriated with some kind of intent. TEMPLE-RASTON: So from the outset, right at the time of incorporation, there was something a little off about Project Vigilant. Other officers or directors we called had heard of Project Vigilant, but they declined to speak on the record because they had signed nondisclosure agreements. Some of them were former government officials from the Justice Department and DHS, and just as we were about to give up, a former NSA official named Ira Winkler called us back. IRA WINKLER (PRESIDENT, SECURE MENTEM): Hi. I'm Ira Winkler, president of Secure Mentem and author of. . . TEMPLE-RASTON: Winkler is a delightfully geeky guy who helps companies beef up their cybersecurity by probing their systems for vulnerabilities. It sounded a little like what Adrian used to do, but Winkler does it legally. Winkler said he met Chet Uber at a hacker's conference and he asked him to be part of this company. Winkler said he'd be happy to help, and it was that informal. He said he was made director of intelligence, and Adrian was supposed to pass anything he discovered along to him. The idea was to use hackers like Adrian to find bad people on the dark web and then use Project Vigilant as a vehicle to tell the authorities. WINKLER: It was supposed to look for illegal, immoral actions on the Internet that pertained to foreign intelligence, terrorism, child exploitation. TEMPLE-RASTON: Which more or less tracked with what Debbie and Bill Scroggin had told us. Getting to the dark web isn't as hard as it sounds. All you have to do to start is download something called Tor, which stands for the onion router. Tor is basically a Web browser, and the only thing you need to know about it is that it allows you to move around anonymously on the Web. It drags branches behind your digital footsteps so people can't tell where you're going or where you've been. The dark web itself isn't illegal, but not surprisingly, some people want to take advantage of its anonymity to do things they aren't supposed to do, like trafficking in porn or recruiting for ISIS. I logged on and found something called the Darknet Heroes League. It says it's a marketplace for drugs and it sells pot and opioids and benzos and steroids, among other things. And to give you an idea of what I'm seeing, it looks like the Web did 20 years ago. Remember when the fonts were all irregular, and there were shabby pictures with misspelled links beneath? It's like that. And the sellers here are actually rated, like eBay or Amazon, and they offer bargains. But if Adrian ever discovered anything criminal during his trips to the dark web, he never passed it along. Winkler said he never received anything from him. In the end, he said. . . WINKLER: What Project Vigilant did was absolutely nothing as far as I can tell. TEMPLE-RASTON: If it had a mysterious connection to the government, aside from listing former government officials as officers and directors, we couldn't find it. Adrian did get money from the government, but it was from the Defense Department just reimbursing him for travel expenses related to his testimony at Manning's court martial. The official documents we saw said that Adrian's relationship with the U. S. government ended in July 2011. As for the mystery of how Adrian wound up in a senior living facility, that was a lot easier to solve. The Scroggins sent him there. About a year before Adrian died, Bill Scroggin had come across an old camera and set it up in his office. He put it on motion activated. SCROGGIN: Kind of like fishing for catfish on trotlines. You put the bait on there. And you come back, and you check it four or five hours later and see if you've got anything. WINKLER: The fish he caught was Adrian, slipping into the office with his flashlight. Scroggin said he was looking for some medications he could steal. It had happened before, and now he was caught on camera doing exactly that. SCROGGIN: The temper I - got a hold of me, and I literally blew up. (SOUNDBITE OF ARCHIVED RECORDING)SCROGGIN: And then in here is your computer, your books. . . TEMPLE-RASTON: Debbie helped Adrian pack up, and he secretly recorded it. (SOUNDBITE OF ARCHIVED RECORDING)SCROGGIN: Socks, underwear, your meds. LAMO: (Unintelligible). SCROGGIN: I'm sorry, Adrian. TEMPLE-RASTON: He went to a nearby homeless shelter, and then Debbie found him an apartment. It happened to be in that senior living facility. It turns out anyone with low income could qualify to live there. I saw some of Adrian's tax returns. He was declaring less than a thousand dollars a year in income. He was on public assistance, and the Scroggins helped him out with the rest. SCROGGIN: We gave him a coffeemaker. We gave him some furniture. But I was like, Adrian, don't you want to buy a new mattress, a bed? No, the couch is fine. So he didn't even have a bed in his apartment. TEMPLE-RASTON: The manager of Shadybrook Senior Apartments is the one who found Adrian's dead body lying on a pile of clothes in the bedroom. She pulled the medical alert cord in the apartment to call 911. It was typical of the calls that came in from a place like that. (SOUNDBITE OF ARCHIVED RECORDING)\rUNIDENTIFIED PERSON #17 (911 OPERATOR): 911 Wichita. TEMPLE-RASTON: The alert, with so few details, was a metaphor for what Adrian's life had become. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #18: Don't know. Don't know. Don't know. Don't have any information on them. TEMPLE-RASTON: Investigators found an apartment in complete disarray - huge piles of trash, dirty dishes, pills and powders everywhere. The medical examiner took photographs and then loaded Adrian's body into a van. Debbie Scroggin called Adrian's father and then went out to the apartment to tidy it up a bit before he arrived. SCROGGIN: One of the things I did that I probably shouldn't have done is I threw away all the empty - his prescription bottles and all of those things. TEMPLE-RASTON: She told us that Adrian called his father only when he had good news, when he'd learned to make lasagna or to tell him about Christmas presents. She didn't want Adrian's father to see how Adrian was living or how many pills he was taking. SCROGGIN: He said where's his medications? I was like, oh, I don't know. Somebody may. . . TEMPLE-RASTON: She was trying to protect him, and in a way, protect Adrian, too. (SOUNDBITE OF SONG, \"CHANGES\")PHIL OCHS (SINGER): (Singing) Sit by my side. Come as close as the air. TEMPLE-RASTON: There were only a handful of people at Adrian's memorial service. This is music from a video Adrian's father made for the occasion. (SOUNDBITE OF SONG, \"CHANGES\")OCHS: (Singing) And wander in my words. Dream about the. . . BLAKE: I was the only one of Adrian's, like, friends that was there. TEMPLE-RASTON: That's Andrew Blake again. BLAKE: No one his, you know, his age, no one who knew him, you know, besides his father for more than a few years. And just knowing that had I not gone that no one besides the people in Kansas and his father would have been there, like, that baffled me. TEMPLE-RASTON: Blake said Adrian wasn't so much forgotten as unforgiven. BLAKE: I think just people tended to associate Adrian with the Adrian who snitched on Manning, not the Adrian who did a whole bunch of cool other stuff. (SOUNDBITE OF SONG, \"CHANGES\")OCHS: (Singing) . . . The pictures that I play of changes. TEMPLE-RASTON: So by now, we understood what Adrian was doing or not doing for Project Vigilant, how he ended up in Wichita and why he was living in a senior apartment. But we still hadn't ruled out murder. Even the local medical examiner's office stopped short of doing that. SCOTT KIPPER (DEPUTY MEDICAL EXAMINER, SEDGWICK COUNTY, KS): There are some things that can be done to a body that leave minimal or no findings at autopsy. TEMPLE-RASTON: That's deputy medical examiner Scott Kipper. For example? KIPPER: Things that I would rather not discuss on the radio. TEMPLE-RASTON: You don't want to discuss it on the radio show because you don't want to give anybody any ideas? KIPPER: That's correct. TEMPLE-RASTON: Dr. Timothy Rohrig is the county's chief medical examiner, and he began reading through the list of chemicals he'd found in Adrian's bloodstream. TIMOTHY ROHRIG (CHIEF TOXICOLOGIST, SEDGWICK COUNTY, KS): Phenazepam, etizolam, flubromazepam, Benadryl, chlorpheniramine, citalopram, gabapentin. TEMPLE-RASTON: The coroner's list didn't surprise Debbie Scroggin. SCROGGIN: He would overmedicate because his anxiety was so high. There were times where he would supplement just to come up to have dinner. And he'd fall asleep in his food, literally it - face down in his food. TEMPLE-RASTON: About a month before Adrian died, the FDA came out with an alert - a warning, really - against mixing benzodiazepines with kratom. It had been linked to dozens of deaths. Dr. Rohrig said Adrian had a handful of what he called designer benzos in his system, some of which weren't available by prescription here in the U. S. ROHRIG: The most common way of getting these particular ones was basically off the Internet. You can order them and have them shipped to whatever address you want. TEMPLE-RASTON: Debbie Scroggin figured there were lots of pills and supplements coming into the house in those packages addressed to Adrian Alfonso. Adrian had left a voice note to himself just hours before he died. He was clearly in pain. (SOUNDBITE OF ARCHIVED RECORDING)LAMO: I really hurt a muscle, so it's hard for me to move around. Agonizing pain from a twisted leg, period. TEMPLE-RASTON: Because kratom isn't regulated by the FDA, it's impossible to tell if Adrian was ingesting potent doses of it on one day and weak doses the next. It can change that much from batch to batch. BERTHA MADRAS (PROFESSOR, HARVARD MEDICAL SCHOOL): It's a strange drug. It has some of the characteristics of pure opioid, which means it can cause sleeplessness. It can. . . TEMPLE-RASTON: Dr. Bertha Madras is a professor of psychobiology at Harvard Medical School and a former member of the president's commission on combating drug addiction and the opioid crisis. While she wouldn't say exactly what killed Adrian Lamo, she did say that people who were mixing things like kratom with benzos and other natural supplements were essentially conducting their own human experiments. MADRAS: They have no clue what they're putting into their body and what the consequences could be. TEMPLE-RASTON: So this is where all the evidence pointed us. Hacking may have killed Adrian Lamo, but it wasn't the Internet kind. His body hacking, the constant intake of pills and powders and liquids, is likely what did him in. (SOUNDBITE OF ARCHIVED RECORDING)LAMO: Hey. This is Adrian. I'm not ignoring you on purpose. TEMPLE-RASTON: This was the last voicemail Debbie Scroggin received from Adrian a few days before he died. (SOUNDBITE OF ARCHIVED RECORDING)LAMO: I had trouble with my phone. Give me a ring or a note when you can. My phone service is active again. Love you. Bye. TEMPLE-RASTON: It offered a clue. Hackers had noticed that Adrian hadn't been on the Internet the week before he died, and that seemed suspicious, but there was a simple answer. He hadn't paid his cell phone bill, and he used his cell phone to get online. In retrospect, as we retraced Adrian's steps during the last two years of his life, it's clear that there were no assassins lying in wait, no government officials eager for a briefing. Adrian was profoundly alone. After he turned in Manning, the hacker community went one way, and Adrian went another. And if you look at all we uncovered, it's easy to imagine that Adrian's last night went something like this. After spending some time on the computer and having dinner, he took something to help him relax and maybe ease some of that muscle pain. He went into the bedroom, laid down on the clothes, curled up and just stopped breathing. It wasn't a murder or a suicide. It was an accident. And that left us with just one unsolved mystery - that address label they found on Adrian's thigh. It read Adrian Lamo, Assistant Director, Project Vigilant, 70 Bates Street, Washington, D. C. So we looked up the property records of the place - who owned it, any renters. Project Vigilant wasn't registered there, but there was one name I did recognize. BLAKE: That's an address that I lived at for a brief time, and Adrian stayed with me occasionally off and on. TEMPLE-RASTON: That's Adrian's friend Andrew Blake, the one who went to the funeral. He didn't even know the sticker existed until he read about it in the autopsy report. BLAKE: That's when I laughed, and that's actually the first time in the weeks after his death where I actually kind of felt a little bit of closure. It almost felt like a joke from Adrian to me, maybe just like a signal. MURPHY: That Project Vigilant sticker - I think maybe it was where he put his hopes, and it didn't go anywhere, so maybe he just wanted to be reminded of that. TEMPLE-RASTON: And we have an epilogue here. A couple of months ago, we reached out to Chelsea Manning. She's being held in a detention center in Alexandria, Va. , for refusing to testify against that crazy white-haired guy she told Adrian about, Julian Assange. Through her lawyer, I asked her if she forgave Adrian, and she said there was nothing to forgive. In a handwritten note that she passed to us, she wrote, I've never had any ill will toward Adrian at any time. And then she added, I'm more mad at the government for using him. Adrian, had he lived, probably would have been a witness for the prosecution in the Assange case to talk about what Manning had told him about that crazy white-haired guy, and Adrian at last would've been back where he wanted to be - back in that public spotlight. (SOUNDBITE OF TV SHOW, \"THE SCREEN SAVERS\")LAPORTE: You know him as the guy who hacked Yahoo, AOL, Time Warner, MCI WorldCom, Microsoft and. . . TEMPLE-RASTON: I'LL BE SEEING YOU is written or reported by me, Dina Temple-Raston. Our producer is Adelina Lancianese, and she scored our show, too. Special thanks to NPR's investigations team, the NPR story lab and to Josephine Wolff of Tufts University. In our next show, AI and elephants - the beginning of a beautiful friendship. Until then, I'm Dina Temple-Raston, and I'll be seeing you.", "section": "I'll Be Seeing You", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-19-762336929": {"title": "Amazon Makes 'Climate Pledge' As Workers Plan Walkout : NPR", "url": "https://www.npr.org/2019/09/19/762336929/amazon-makes-climate-pledge-as-workers-plan-walkout", "author": "No author found", "published_date": "2019-09-19", "content": "RACHEL MARTIN, HOST: While most of us were sleeping, hundreds of thousands of people around the world were on the march warning of the dangers of climate change. People here in the U. S. are joining in today, including workers at e-commerce giant Amazon. But before those workers could strike, their boss unexpectedly stole the narrative. I'm joined now by NPR's Alina Selyukh. And I'm just going to note here before we start - Amazon is an NPR sponsor. So, Alina, before we get to what Jeff Bezos did, what can you tell us about what is happening today? How many Amazon employees are taking part in these climate protests? ALINA SELYUKH, BYLINE: They're expecting more than 1,500 Amazon workers to walk off the job today. They are expecting to be joined by other tech workers from Google and Facebook. And this is important context because there is this group inside Amazon called Amazon Employees for Climate Justice. And they previously organized a shareholder resolution that was focused on climate change; for example, asking Amazon to report how it plans to reduce its dependence on fossil fuels. MARTIN: Mmm hmm. SELYUKH: And this resolution got signed by thousands of Amazon workers - publicly with their names and titles - which was quite unprecedented for Amazon. There's a video of Amazon worker Emily Cunningham presenting this resolution at the shareholder meeting in May. I spoke to her yesterday. EMILY CUNNINGHAM: I just knew that I had to speak up. I felt like it was my moral responsibility. But at the same time, I was also speaking up against my boss, who also is the richest man in the world. And so while I was very determined to do it, it was also very scary. And in the video, you can see that I'm shaking. SELYUKH: The resolution failed, but it got the attention of more Amazon employees. And over 8,000 workers ended up signing an open letter to Bezos, asking to aggressively reduce emissions, to stop contracts with oil and gas companies, to stop campaign contributions to politicians who deny the existence of climate change. And their next act today is the climate walkout. So it's not a coincidence that Bezos chose yesterday to unveil Amazon's new climate pledge. MARTIN: He's like, surprise. I've heard you. I've come out with this new climate pledge. Does it meet those people's demands? What's in it? SELYUKH: Not entirely. It seems to be a separate thing. It was presented as a very separate thing but not a coincidence. And it's Amazon's plan, as they put it, to meet the Paris climate accord 10 years ahead of schedule. Here's Bezos speaking yesterday. (SOUNDBITE OF ARCHIVED RECORDING)JEFF BEZOS: We've been in the middle of the herd on this issue. And we want to move to the forefront. We want to say, look, if a company of Amazon's complexity, physical infrastructure, delivering 10 billion items can do this, so can you. SELYUKH: Amazon says by 2030, it will use 100% renewable energy to power it's global infrastructure, which, if you think about it, includes trucks and warehouses and massive data centers. So for example, Amazon has put in a huge order of 100,000 electric delivery vans. The company's goal is to be carbon neutral by 2040, meaning it would emit far less carbon dioxide or offset those emissions by investing in things like wind farms and planting trees. MARTIN: So we heard Bezos say Amazon has been in the middle of the herd or the middle of the pack. What does that mean exactly? SELYUKH: Right. So when I talked to environmental advocates and groups that followed this, they argued Bezos may have been a bit generous with that description. But so - Amazon does have a group focused on sustainability. They're working on reducing packaging waste. Bezos himself has talked about how important climate concerns are before yesterday. MARTIN: Mmm hmm. SELYUKH: But for example, Amazon had not disclosed its carbon footprint, like many companies have done, until yesterday. And it's a big one. It gets a bit tricky comparing different companies here. But if you look at Amazon's plans on just renewable energy, other tech giants like Google and Facebook have already or almost reached the goal that Amazon just set for 2030. But on the retail side, Amazon's goals are more ambitious than its biggest competitor, Walmart. So inside Amazon, the climate advocates definitely celebrated the news of the climate pledge. . . MARTIN: Yeah. SELYUKH: . . . But the workers still plan to walk out today for what they call the, quote, \"fight for a livable future. \"MARTIN: NPR's Alina Selyukh. RACHEL MARTIN, HOST:  While most of us were sleeping, hundreds of thousands of people around the world were on the march warning of the dangers of climate change. People here in the U. S. are joining in today, including workers at e-commerce giant Amazon. But before those workers could strike, their boss unexpectedly stole the narrative. I'm joined now by NPR's Alina Selyukh. And I'm just going to note here before we start - Amazon is an NPR sponsor. So, Alina, before we get to what Jeff Bezos did, what can you tell us about what is happening today? How many Amazon employees are taking part in these climate protests? ALINA SELYUKH, BYLINE: They're expecting more than 1,500 Amazon workers to walk off the job today. They are expecting to be joined by other tech workers from Google and Facebook. And this is important context because there is this group inside Amazon called Amazon Employees for Climate Justice. And they previously organized a shareholder resolution that was focused on climate change; for example, asking Amazon to report how it plans to reduce its dependence on fossil fuels. MARTIN: Mmm hmm. SELYUKH: And this resolution got signed by thousands of Amazon workers - publicly with their names and titles - which was quite unprecedented for Amazon. There's a video of Amazon worker Emily Cunningham presenting this resolution at the shareholder meeting in May. I spoke to her yesterday. EMILY CUNNINGHAM: I just knew that I had to speak up. I felt like it was my moral responsibility. But at the same time, I was also speaking up against my boss, who also is the richest man in the world. And so while I was very determined to do it, it was also very scary. And in the video, you can see that I'm shaking. SELYUKH: The resolution failed, but it got the attention of more Amazon employees. And over 8,000 workers ended up signing an open letter to Bezos, asking to aggressively reduce emissions, to stop contracts with oil and gas companies, to stop campaign contributions to politicians who deny the existence of climate change. And their next act today is the climate walkout. So it's not a coincidence that Bezos chose yesterday to unveil Amazon's new climate pledge. MARTIN: He's like, surprise. I've heard you. I've come out with this new climate pledge. Does it meet those people's demands? What's in it? SELYUKH: Not entirely. It seems to be a separate thing. It was presented as a very separate thing but not a coincidence. And it's Amazon's plan, as they put it, to meet the Paris climate accord 10 years ahead of schedule. Here's Bezos speaking yesterday. (SOUNDBITE OF ARCHIVED RECORDING) JEFF BEZOS: We've been in the middle of the herd on this issue. And we want to move to the forefront. We want to say, look, if a company of Amazon's complexity, physical infrastructure, delivering 10 billion items can do this, so can you. SELYUKH: Amazon says by 2030, it will use 100% renewable energy to power it's global infrastructure, which, if you think about it, includes trucks and warehouses and massive data centers. So for example, Amazon has put in a huge order of 100,000 electric delivery vans. The company's goal is to be carbon neutral by 2040, meaning it would emit far less carbon dioxide or offset those emissions by investing in things like wind farms and planting trees. MARTIN: So we heard Bezos say Amazon has been in the middle of the herd or the middle of the pack. What does that mean exactly? SELYUKH: Right. So when I talked to environmental advocates and groups that followed this, they argued Bezos may have been a bit generous with that description. But so - Amazon does have a group focused on sustainability. They're working on reducing packaging waste. Bezos himself has talked about how important climate concerns are before yesterday. MARTIN: Mmm hmm. SELYUKH: But for example, Amazon had not disclosed its carbon footprint, like many companies have done, until yesterday. And it's a big one. It gets a bit tricky comparing different companies here. But if you look at Amazon's plans on just renewable energy, other tech giants like Google and Facebook have already or almost reached the goal that Amazon just set for 2030. But on the retail side, Amazon's goals are more ambitious than its biggest competitor, Walmart. So inside Amazon, the climate advocates definitely celebrated the news of the climate pledge. . . MARTIN: Yeah. SELYUKH: . . . But the workers still plan to walk out today for what they call the, quote, \"fight for a livable future. \" MARTIN: NPR's Alina Selyukh.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-19-761962531": {"title": "People Are Looking At Your LinkedIn Profile. They Might Be Chinese Spies : NPR", "url": "https://www.npr.org/2019/09/19/761962531/people-are-looking-at-your-linkedin-profile-they-might-be-chinese-spies", "author": "No author found", "published_date": "2019-09-19", "content": "", "section": "National Security", "disclaimer": ""}, "2019-09-20-762799187": {"title": "Twitter Removes Thousands of Accounts For Platform Manipulation : NPR", "url": "https://www.npr.org/2019/09/20/762799187/twitter-removes-thousands-of-accounts-for-manipulating-their-platform", "author": "No author found", "published_date": "2019-09-20", "content": "", "section": "Technology", "disclaimer": ""}, "2019-09-20-762499808": {"title": "Senate Allocates $250M For Election Security. Activists Say Hill Must Do More : NPR", "url": "https://www.npr.org/2019/09/20/762499808/senate-breaks-logjam-on-election-security-cash-but-activists-say-more-is-needed", "author": "No author found", "published_date": "2019-09-20", "content": "", "section": "2020 Election: Secure Your Vote", "disclaimer": ""}, "2019-09-22-760572640": {"title": "Google Street View Has Gaps. Tawanda Kanhema Is Trying To Fill Them In : NPR", "url": "https://www.npr.org/2019/09/22/760572640/hes-trying-to-fill-in-the-gaps-on-google-street-view-starting-with-zimbabwe", "author": "No author found", "published_date": "2019-09-22", "content": "MICHEL MARTIN, HOST: If you've ever used Google Street View to plan a vacation or house-hunt, then you know that you can see almost every corner in the U. S. But Africa and other places are almost completely left off the map. NPR's Chloee Weiner has the story of someone who's trying to fill in the map, starting with Zimbabwe. CHLOEE WEINER, BYLINE: Tawanda Kanhema is driving up Samora Machel Avenue, a crowded street in his hometown, Harare, the capital city of Zimbabwe. TAWANDA KANHEMA: And to my left is the Reserve Bank of Zimbabwe, which happens to be the tallest building in the city. And you can feel the energy of this commercial center and cultural center of about 2. 6 million people that live here. WEINER: If he sounds like a tour guide, he is, a tour guide for the whole world. He has a camera strapped on the roof of his car. KANHEMA: The camera kind of looks like an alien. It's got six eyes and what looks like a mouth. WEINER: It's an odd sight, Kanhema says. So he gets a lot of questions from people as he passes. KANHEMA: Is that a camera? What are you recording? What are you filming? WEINER: It's a 360-degree camera that Kanhema is using to put images of his hometown on the map - Google Street View map, that is. Kanhema spend a few thousand dollars of his own money to do it because Google didn't pay. STAFFORD MARQUARDT: We do pay them back in a lot of other ways. WEINER: That Stafford Marquardt, a product manager for Street View. He says Google's ultimate goal is to make a Street View map of the whole world. To do that, Google relies on volunteers to capture places the company hasn't prioritized. MARQUARDT: Google pays Street View car drivers through our official Google Street View fleet. When contributors decide to map the areas that are important to them, we offer a free platform to host gigabytes and terabytes of imagery and publish it to the entire world absolutely for free. KANHEMA: There's not always going to be a business case to tell the story of how people live across the world. WEINER: That's Kanhema again. He sees these gaps on Street View as a digital divide. KANHEMA: I found it quite jarring that a lot of the countries in the region were not on the map. WEINER: When Kanhema's not working as a product manager in Silicon Valley, he's a freelance photographer. He traverses Zimbabwe by foot, by car, by boat, even on a helicopter ride over Victoria Falls. KANHEMA: Imagine being able to lend a ticket to get on a helicopter tour of one of the seven natural wonders of the world and being able to bring at least a million other people with you. WEINER: He sees his work as a form of documentary photography, and he's hoping these images will bring in potential tourists and boost Zimbabwe's economy. KANHEMA: So there's a sense of responsibility that comes with that. I'm not just here to document a place. I'm here to document the place and then go on to the next level to think - what can we do to improve the way people live in this place? MARQUARDT: We've added businesses to every block of Harare that Tawanda drove, and that's important in bringing good maps and good directions and good date night-finding to all the people in Zimbabwe. WEINER: That's Google's Stafford Marquardt again. Recently, Kanhema took Google's camera to northern Ontario. A local government hired him to capture the ice roads that connect native communities there before they're forever altered by climate change. KANHEMA: You feel this very strong sense of solitude. We've probably only met two cars. There's really not much to see besides the vast endless expanse of ice. It's the coldest place I've been to. WEINER: So far, Kanhema's mission has added 700,000 new images to Street View. Next stop, he might head to Greenland or maybe Alaska or Mozambique. And you can see everything he's seen by clicking on Google Maps. Chloee Weiner, NPR News. MICHEL MARTIN, HOST:  If you've ever used Google Street View to plan a vacation or house-hunt, then you know that you can see almost every corner in the U. S. But Africa and other places are almost completely left off the map. NPR's Chloee Weiner has the story of someone who's trying to fill in the map, starting with Zimbabwe. CHLOEE WEINER, BYLINE: Tawanda Kanhema is driving up Samora Machel Avenue, a crowded street in his hometown, Harare, the capital city of Zimbabwe. TAWANDA KANHEMA: And to my left is the Reserve Bank of Zimbabwe, which happens to be the tallest building in the city. And you can feel the energy of this commercial center and cultural center of about 2. 6 million people that live here. WEINER: If he sounds like a tour guide, he is, a tour guide for the whole world. He has a camera strapped on the roof of his car. KANHEMA: The camera kind of looks like an alien. It's got six eyes and what looks like a mouth. WEINER: It's an odd sight, Kanhema says. So he gets a lot of questions from people as he passes. KANHEMA: Is that a camera? What are you recording? What are you filming? WEINER: It's a 360-degree camera that Kanhema is using to put images of his hometown on the map - Google Street View map, that is. Kanhema spend a few thousand dollars of his own money to do it because Google didn't pay. STAFFORD MARQUARDT: We do pay them back in a lot of other ways. WEINER: That Stafford Marquardt, a product manager for Street View. He says Google's ultimate goal is to make a Street View map of the whole world. To do that, Google relies on volunteers to capture places the company hasn't prioritized. MARQUARDT: Google pays Street View car drivers through our official Google Street View fleet. When contributors decide to map the areas that are important to them, we offer a free platform to host gigabytes and terabytes of imagery and publish it to the entire world absolutely for free. KANHEMA: There's not always going to be a business case to tell the story of how people live across the world. WEINER: That's Kanhema again. He sees these gaps on Street View as a digital divide. KANHEMA: I found it quite jarring that a lot of the countries in the region were not on the map. WEINER: When Kanhema's not working as a product manager in Silicon Valley, he's a freelance photographer. He traverses Zimbabwe by foot, by car, by boat, even on a helicopter ride over Victoria Falls. KANHEMA: Imagine being able to lend a ticket to get on a helicopter tour of one of the seven natural wonders of the world and being able to bring at least a million other people with you. WEINER: He sees his work as a form of documentary photography, and he's hoping these images will bring in potential tourists and boost Zimbabwe's economy. KANHEMA: So there's a sense of responsibility that comes with that. I'm not just here to document a place. I'm here to document the place and then go on to the next level to think - what can we do to improve the way people live in this place? MARQUARDT: We've added businesses to every block of Harare that Tawanda drove, and that's important in bringing good maps and good directions and good date night-finding to all the people in Zimbabwe. WEINER: That's Google's Stafford Marquardt again. Recently, Kanhema took Google's camera to northern Ontario. A local government hired him to capture the ice roads that connect native communities there before they're forever altered by climate change. KANHEMA: You feel this very strong sense of solitude. We've probably only met two cars. There's really not much to see besides the vast endless expanse of ice. It's the coldest place I've been to. WEINER: So far, Kanhema's mission has added 700,000 new images to Street View. Next stop, he might head to Greenland or maybe Alaska or Mozambique. And you can see everything he's seen by clicking on Google Maps. Chloee Weiner, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-23-763521121": {"title": "Instagram Head Adam Mosseri Discusses App's New Features Meant To Fight Bullying : NPR", "url": "https://www.npr.org/2019/09/23/763521121/instagram-head-adam-mosseri-discusses-apps-new-features-meant-to-fight-bullying", "author": "No author found", "published_date": "2019-09-23", "content": "AUDIE CORNISH, HOST: With All Tech Considered. (SOUNDBITE OF MUSIC)CORNISH: The head of Instagram, Adam Mosseri, knows what it's like to be bullied. He also knows that today kids are likely to be bullied on Instagram. That's why he says he wants to lead the industry in the fight against online bullying. ADAM MOSSERI: I had, like, Coke bottle, Corbusier glasses at 5 years old that made my eyes as big as - I don't know - lemons. CORNISH: I can picture that. MOSSERI: I had a haircut that made me look like Harry Potter long before Harry Potter existed or was cool. It was not a good look. I was made fun of a lot. But yeah, I probably would have been made fun of on Instagram. CORNISH: Yeah, there would have been an Instagram account that was like Adam's Glasses. MOSSERI: Yeah. Oh, man. You're really bringing me back. CORNISH: (Laughter) I'm just saying. MOSSERI: (Laughter) I got to find those glasses. CORNISH: I had a rough time of it, too. And I have to admit, I look at social media; I look at things like Instagram. And I say - thank God it wasn't around when I was a kid. It would have been worse. MOSSERI: I don't quite feel that way. Obviously, I'm biased given that I'm sitting in the situation that I'm sitting in. I have two kids. They're too young to use Instagram. They're 3 and 1 right now, but they will use Instagram when they get older. I think that certain things would absolutely be worse. Other things would be better. For me right now, the way my kids appear on Instagram is they have private accounts. They don't use them; they're literally 3 and 1 years old. But I manage them, and I share basically their upbringing with my family who lives all around the world. I have family in Pittsburgh. I have family in New York. I have family in Israel. I have family in Germany. I have family in LA. And they can stay up to date on what my boys are doing and how they're growing in a way that they wouldn't have been able to do just 15 years ago. And so there's good and there's bad that comes from connecting people. Technology is not inherently good, and it's not inherently bad. For those of us who work in the industry, it's our responsibility to magnify the good and address the bad as effectively as we can. CORNISH: Mosseri took the helm of Instagram almost a year ago, coming from its big brother Facebook. Shortly after, The Atlantic published a story headlined \"Instagram Has A Massive Harassment Problem. \" People said it was too easy to set up anonymous pages just to taunt someone. They said the company wasn't responsive when they tried to sound the alarm about abuse, so now Mosseri is overseeing a rollout of new features that Instagram hopes will protect its users from bullying and keep them on the site. I asked him what he learned about bullying on the platform over the past year. MOSSERI: A few different things. One - a lot of it happens. Actually, most of it seems to happen between people who know each other in real life. Another is that the controls that we had before or have today are insufficient. So specifically, I and our researchers talked to a bunch of teens and we asked them, why don't you just block someone who is bullying you on the platform? Because you can block someone right now - they can't see you. You essentially don't exist on Instagram to them. And there were two reasons that came back. One was - often, that can actually escalate the situation. They'll figure that out. They'll know, and they will bully more, either on Instagram or elsewhere. And two is that you need, as a target of bullying, to see what the actor is actually doing. Teens would often say, like, they're talking about me. If I blocked them, I won't be able to see that. So I need to track what's actually happening. Which is why we've been developing this new control called restrict, which allows you to restrict an individual - it means that if they comment on your post, you'll see it, and it'll look to them as if they've - it's actually been commented on, but you have to approve it before anybody else sees it. It means that their messages are going to go into your other inbox, and you have to go there to see them and that they won't actually get read receipts - a whole bunch of different little nuanced ways to give a target of bullying a bit more power over the experience, which allows. . . . CORNISH: So it's not about changing their behavior. Right? It's about giving the person who's the victim a few more tools to protect themselves. MOSSERI: Yeah. Restrict is not about changing their behavior. There are other things we're doing to try and adjust people's behavior by changing incentives. CORNISH: One of them is very interesting. It's an alert that flags a mean-spirited post. Right? It essentially asks you - hey, are you sure you want to post this? MOSSERI: Yes. CORNISH: It's very polite (laughter). MOSSERI: It is. CORNISH: Why do you think that will be effective? I mean, if someone's about to say something cruel (laughter), saying think about it - like, I don't know. MOSSERI: I think it's going to be somewhat effective but not very effective. I actually think that's true of all of the work that we're going to do. All of these problems are not solved by any one solution. They are complicated. I've also seen in the actual data that a minority but a significant minority of people do rewrite their comments, and they rewrite them in a much more pleasant way. Now, do most people rewrite their comments? No - because if you have an intent that's really intense to harass an individual, a polite comment from Instagram is not going to prevent you from doing so. But sometimes people get caught up in the moment, and a lightweight reminder can actually help them rethink what they're doing or what they're saying. And so it's not supposed to be the solution that we hang our hat on for all of bullying. It's supposed to be one of many tools to try to prevent bullying from happening in the first place. CORNISH: One controversial idea that I know you are thinking about is the idea of making the number of likes private. Right? MOSSERI: Yeah. CORNISH: So when a person posts to Instagram their image, lots of people can click on a little heart that shows I love this thing. And some people get a high off of that - right? - when they - 'cause it's a big popularity contest in a way. If you make that private, doesn't that get to the heart of what Instagram is? MOSSERI: In some ways, yeah. When I said before that. . . CORNISH: That sounds bad for business. MOSSERI: It might be. But ultimately, if we make decisions that are bad for business but that keep people safe or are good for well-being more broadly, I have to believe that those are going to be good for the business over the long run, and so I'm going to make those decisions. The idea with making like counts private is to try and depressurize the experience a bit. It can sometimes feel like a popularity contest, which is why we. . . CORNISH: How likely is this to actually happen, though? Am I speculating with you right now, or is this something you're going to do? MOSSERI: I'm bullish on it. It's a big change. We're working through all the challenges. One challenge is for creators. They use Instagram to make a living. Likes are a sign of how relevant they are, so we have to figure out some way to make sure we preserve that. CORNISH: You're talking about influencers. Right? I mean, there are people who now essentially make a living off of their social media presence. And the way they show potential advertisers - look, I'm a good bet - is how many likes I get. MOSSERI: Absolutely. We've actually had a pretty mixed response from influencers. So I think - I'm optimistic to answer your question very directly. We aren't there yet. We're still iterating on the experience. But I am personally optimistic and really personally invested in making it work. CORNISH: So the boss is bullish. (Laughter) That's the take so far. MOSSERI: The boss is bullish, yes. CORNISH: Well, Adam Mosseri, thank you so much for speaking with us. MOSSERI: Absolutely. Thank you so much for the time. CORNISH: Adam Mosseri, head of Instagram. We should note Facebook, which owns Instagram, is among NPR's financial supporters. Tomorrow Mosseri tells us what his time leading Facebook's News Feed taught him about social media abuse. (SOUNDBITE OF MUSIC) AUDIE CORNISH, HOST:  With All Tech Considered. (SOUNDBITE OF MUSIC) CORNISH: The head of Instagram, Adam Mosseri, knows what it's like to be bullied. He also knows that today kids are likely to be bullied on Instagram. That's why he says he wants to lead the industry in the fight against online bullying. ADAM MOSSERI: I had, like, Coke bottle, Corbusier glasses at 5 years old that made my eyes as big as - I don't know - lemons. CORNISH: I can picture that. MOSSERI: I had a haircut that made me look like Harry Potter long before Harry Potter existed or was cool. It was not a good look. I was made fun of a lot. But yeah, I probably would have been made fun of on Instagram. CORNISH: Yeah, there would have been an Instagram account that was like Adam's Glasses. MOSSERI: Yeah. Oh, man. You're really bringing me back. CORNISH: (Laughter) I'm just saying. MOSSERI: (Laughter) I got to find those glasses. CORNISH: I had a rough time of it, too. And I have to admit, I look at social media; I look at things like Instagram. And I say - thank God it wasn't around when I was a kid. It would have been worse. MOSSERI: I don't quite feel that way. Obviously, I'm biased given that I'm sitting in the situation that I'm sitting in. I have two kids. They're too young to use Instagram. They're 3 and 1 right now, but they will use Instagram when they get older. I think that certain things would absolutely be worse. Other things would be better. For me right now, the way my kids appear on Instagram is they have private accounts. They don't use them; they're literally 3 and 1 years old. But I manage them, and I share basically their upbringing with my family who lives all around the world. I have family in Pittsburgh. I have family in New York. I have family in Israel. I have family in Germany. I have family in LA. And they can stay up to date on what my boys are doing and how they're growing in a way that they wouldn't have been able to do just 15 years ago. And so there's good and there's bad that comes from connecting people. Technology is not inherently good, and it's not inherently bad. For those of us who work in the industry, it's our responsibility to magnify the good and address the bad as effectively as we can. CORNISH: Mosseri took the helm of Instagram almost a year ago, coming from its big brother Facebook. Shortly after, The Atlantic published a story headlined \"Instagram Has A Massive Harassment Problem. \" People said it was too easy to set up anonymous pages just to taunt someone. They said the company wasn't responsive when they tried to sound the alarm about abuse, so now Mosseri is overseeing a rollout of new features that Instagram hopes will protect its users from bullying and keep them on the site. I asked him what he learned about bullying on the platform over the past year. MOSSERI: A few different things. One - a lot of it happens. Actually, most of it seems to happen between people who know each other in real life. Another is that the controls that we had before or have today are insufficient. So specifically, I and our researchers talked to a bunch of teens and we asked them, why don't you just block someone who is bullying you on the platform? Because you can block someone right now - they can't see you. You essentially don't exist on Instagram to them. And there were two reasons that came back. One was - often, that can actually escalate the situation. They'll figure that out. They'll know, and they will bully more, either on Instagram or elsewhere. And two is that you need, as a target of bullying, to see what the actor is actually doing. Teens would often say, like, they're talking about me. If I blocked them, I won't be able to see that. So I need to track what's actually happening. Which is why we've been developing this new control called restrict, which allows you to restrict an individual - it means that if they comment on your post, you'll see it, and it'll look to them as if they've - it's actually been commented on, but you have to approve it before anybody else sees it. It means that their messages are going to go into your other inbox, and you have to go there to see them and that they won't actually get read receipts - a whole bunch of different little nuanced ways to give a target of bullying a bit more power over the experience, which allows. . . . CORNISH: So it's not about changing their behavior. Right? It's about giving the person who's the victim a few more tools to protect themselves. MOSSERI: Yeah. Restrict is not about changing their behavior. There are other things we're doing to try and adjust people's behavior by changing incentives. CORNISH: One of them is very interesting. It's an alert that flags a mean-spirited post. Right? It essentially asks you - hey, are you sure you want to post this? MOSSERI: Yes. CORNISH: It's very polite (laughter). MOSSERI: It is. CORNISH: Why do you think that will be effective? I mean, if someone's about to say something cruel (laughter), saying think about it - like, I don't know. MOSSERI: I think it's going to be somewhat effective but not very effective. I actually think that's true of all of the work that we're going to do. All of these problems are not solved by any one solution. They are complicated. I've also seen in the actual data that a minority but a significant minority of people do rewrite their comments, and they rewrite them in a much more pleasant way. Now, do most people rewrite their comments? No - because if you have an intent that's really intense to harass an individual, a polite comment from Instagram is not going to prevent you from doing so. But sometimes people get caught up in the moment, and a lightweight reminder can actually help them rethink what they're doing or what they're saying. And so it's not supposed to be the solution that we hang our hat on for all of bullying. It's supposed to be one of many tools to try to prevent bullying from happening in the first place. CORNISH: One controversial idea that I know you are thinking about is the idea of making the number of likes private. Right? MOSSERI: Yeah. CORNISH: So when a person posts to Instagram their image, lots of people can click on a little heart that shows I love this thing. And some people get a high off of that - right? - when they - 'cause it's a big popularity contest in a way. If you make that private, doesn't that get to the heart of what Instagram is? MOSSERI: In some ways, yeah. When I said before that. . . CORNISH: That sounds bad for business. MOSSERI: It might be. But ultimately, if we make decisions that are bad for business but that keep people safe or are good for well-being more broadly, I have to believe that those are going to be good for the business over the long run, and so I'm going to make those decisions. The idea with making like counts private is to try and depressurize the experience a bit. It can sometimes feel like a popularity contest, which is why we. . . CORNISH: How likely is this to actually happen, though? Am I speculating with you right now, or is this something you're going to do? MOSSERI: I'm bullish on it. It's a big change. We're working through all the challenges. One challenge is for creators. They use Instagram to make a living. Likes are a sign of how relevant they are, so we have to figure out some way to make sure we preserve that. CORNISH: You're talking about influencers. Right? I mean, there are people who now essentially make a living off of their social media presence. And the way they show potential advertisers - look, I'm a good bet - is how many likes I get. MOSSERI: Absolutely. We've actually had a pretty mixed response from influencers. So I think - I'm optimistic to answer your question very directly. We aren't there yet. We're still iterating on the experience. But I am personally optimistic and really personally invested in making it work. CORNISH: So the boss is bullish. (Laughter) That's the take so far. MOSSERI: The boss is bullish, yes. CORNISH: Well, Adam Mosseri, thank you so much for speaking with us. MOSSERI: Absolutely. Thank you so much for the time. CORNISH: Adam Mosseri, head of Instagram. We should note Facebook, which owns Instagram, is among NPR's financial supporters. Tomorrow Mosseri tells us what his time leading Facebook's News Feed taught him about social media abuse. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-24-763958806": {"title": "Instagram Head Adam Mosseri On The Lessons He's Learned About Social Media Abuse : NPR", "url": "https://www.npr.org/2019/09/24/763958806/instagram-head-adam-mosseri-on-the-lessons-hes-learned-about-social-media-abuse", "author": "No author found", "published_date": "2019-09-24", "content": "AUDIE CORNISH, HOST:  Instagram is rolling out new features to stop bullying on its platform, and we spoke about it yesterday with the company's head, Adam Mosseri. He used to work at Facebook, and he says it was there that he learned how social media could become a weapon. Mosseri developed the News Feed feature that amplified misinformation and propaganda during the 2016 election. He told me how that experience changed Facebook and his own approach to developing tools for social media. ADAM MOSSERI: In the early days, we were so excited and optimistic about the value that comes from connecting people and, just quite frankly, under-focused and somewhat naive about the negative consequences of connecting people at scale. And so we were just invested way more on one side than the other. And the balance had to shift. And that's the major lesson for me and, I think, for most of us at Facebook, the company, over the last few years. And this is not just about making a decision about how many people work on one project versus another. It's an actual cultural shift, where you have to shift people's mindsets. When they have an idea that they're excited about, you need to get them to a place where they naturally not only think about all the good that can come from it but how that idea might be abused. CORNISH: Right. I mean, just for context for people, you helped design - or designed the News Feed for Facebook, right? And that was the feature, essentially, that was manipulated by trolls and foreign actors in the 2016 election. But now you're in charge at Instagram, right? So what is your lesson as a person who designs these things about, like, how a tool can be manipulated? MOSSERI: The most important lesson to me is that when you build a new feature or idea - we'll come up with something benign - let's say events on Facebook didn't exist, and we're like, oh, we're going build events. It'll be great. People will be able to schedule a time and get together and hang out in the real world. That's awesome. That's exciting. We should be enthusiastic about that, but we need to also think about how events might be misused. Could spammers use it to try and get people to - I don't know - buy fake iPads? Or could people use it to try to get people that they want to meet in real life in nefarious ways? You have to approach it with, like, an adversarial mindset. Imagine I am just a bad actor. There's all sorts of bad actors - ideologically motivated, financially motivated and otherwise. How would I abuse this thing? And then you need to try and design the experience to mitigate or address those abuses first class. And then on top of that, you try to identify the problems proactively and remove them. That is a. . . CORNISH: How big a shift was that for you? MOSSERI: It's a big shift. I mean, it is a - it makes everything you do much more complicated because, honestly, trying to build a feature that people use is difficult enough. But the people - the bad actors that you are combating at a - as a platform of our scale are really sophisticated. But that is just the way we need to run our company at this point. CORNISH: It's interesting because I think there was a time when Silicon Valley would basically say to us, like, we're just a platform making things, and it's a tool. And, like, who knows if other bad things happen? That's not really our problem. I get the sense you guys realize it's your problem. MOSSERI: We've realized that we have a lot of responsibility to understand how our platform is used and to address any abuses whenever possible. There is another tension, though, where we want to be careful about overstepping. There are certain things that we cannot and should not do, given our scale. We're not going to weigh on one ideological point of view or one religious point of view, for instance. But, yes, I think one of the big experiences that the industry as a whole has happened - has had over the last few years is to really become more aware of the negative consequences of what we do and to try and proactively embrace that responsibility and address those issues. CORNISH: Adam Mosseri, thank you so much for speaking with us. MOSSERI: Absolutely. Thank you so much for the time. CORNISH: Adam Mosseri is head of Instagram. We should note Instagram's parent company, Facebook, is among NPR's financial sponsors. (SOUNDBITE OF KIASMOS' \"SWAYED\") AUDIE CORNISH, HOST:   Instagram is rolling out new features to stop bullying on its platform, and we spoke about it yesterday with the company's head, Adam Mosseri. He used to work at Facebook, and he says it was there that he learned how social media could become a weapon. Mosseri developed the News Feed feature that amplified misinformation and propaganda during the 2016 election. He told me how that experience changed Facebook and his own approach to developing tools for social media. ADAM MOSSERI: In the early days, we were so excited and optimistic about the value that comes from connecting people and, just quite frankly, under-focused and somewhat naive about the negative consequences of connecting people at scale. And so we were just invested way more on one side than the other. And the balance had to shift. And that's the major lesson for me and, I think, for most of us at Facebook, the company, over the last few years. And this is not just about making a decision about how many people work on one project versus another. It's an actual cultural shift, where you have to shift people's mindsets. When they have an idea that they're excited about, you need to get them to a place where they naturally not only think about all the good that can come from it but how that idea might be abused. CORNISH: Right. I mean, just for context for people, you helped design - or designed the News Feed for Facebook, right? And that was the feature, essentially, that was manipulated by trolls and foreign actors in the 2016 election. But now you're in charge at Instagram, right? So what is your lesson as a person who designs these things about, like, how a tool can be manipulated? MOSSERI: The most important lesson to me is that when you build a new feature or idea - we'll come up with something benign - let's say events on Facebook didn't exist, and we're like, oh, we're going build events. It'll be great. People will be able to schedule a time and get together and hang out in the real world. That's awesome. That's exciting. We should be enthusiastic about that, but we need to also think about how events might be misused. Could spammers use it to try and get people to - I don't know - buy fake iPads? Or could people use it to try to get people that they want to meet in real life in nefarious ways? You have to approach it with, like, an adversarial mindset. Imagine I am just a bad actor. There's all sorts of bad actors - ideologically motivated, financially motivated and otherwise. How would I abuse this thing? And then you need to try and design the experience to mitigate or address those abuses first class. And then on top of that, you try to identify the problems proactively and remove them. That is a. . . CORNISH: How big a shift was that for you? MOSSERI: It's a big shift. I mean, it is a - it makes everything you do much more complicated because, honestly, trying to build a feature that people use is difficult enough. But the people - the bad actors that you are combating at a - as a platform of our scale are really sophisticated. But that is just the way we need to run our company at this point. CORNISH: It's interesting because I think there was a time when Silicon Valley would basically say to us, like, we're just a platform making things, and it's a tool. And, like, who knows if other bad things happen? That's not really our problem. I get the sense you guys realize it's your problem. MOSSERI: We've realized that we have a lot of responsibility to understand how our platform is used and to address any abuses whenever possible. There is another tension, though, where we want to be careful about overstepping. There are certain things that we cannot and should not do, given our scale. We're not going to weigh on one ideological point of view or one religious point of view, for instance. But, yes, I think one of the big experiences that the industry as a whole has happened - has had over the last few years is to really become more aware of the negative consequences of what we do and to try and proactively embrace that responsibility and address those issues. CORNISH: Adam Mosseri, thank you so much for speaking with us. MOSSERI: Absolutely. Thank you so much for the time. CORNISH: Adam Mosseri is head of Instagram. We should note Instagram's parent company, Facebook, is among NPR's financial sponsors. (SOUNDBITE OF KIASMOS' \"SWAYED\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-24-763872059": {"title": "Trump Vows To Release Ukraine Transcript As Pelosi Launches Impeachment Inquiry : NPR", "url": "https://www.npr.org/2019/09/24/763872059/trump-vows-to-release-ukraine-transcript-amid-impeachment-crescendo-in-d-c", "author": "No author found", "published_date": "2019-09-24", "content": "", "section": "National Security", "disclaimer": ""}, "2019-09-24-763857307": {"title": "'Right To Be Forgotten' Only Applies To Websites Inside EU, European Court Says : NPR", "url": "https://www.npr.org/2019/09/24/763857307/right-to-be-forgotten-only-applies-inside-eu-european-court-says", "author": "No author found", "published_date": "2019-09-24", "content": "", "section": "Technology", "disclaimer": ""}, "2019-09-25-764220454": {"title": "EBay CEO Devin Wenig Is Out As Investors Push For Growth : NPR", "url": "https://www.npr.org/2019/09/25/764220454/under-pressure-to-bid-up-growth-ebay-parts-ways-with-its-ceo", "author": "No author found", "published_date": "2019-09-25", "content": "", "section": "Business", "disclaimer": ""}, "2019-09-25-717487451": {"title": "WATCH: Scientists Look To Reverse Aging So We Can Live Decades Longer : NPR", "url": "https://www.npr.org/2019/09/25/717487451/video-what-if-aging-wasnt-inevitable-the-quest-to-slow-and-even-reverse-aging", "author": "No author found", "published_date": "2019-09-25", "content": "(SOUNDBITE OF SKINNY WILLIAMS AND STEPHEN GOODSON'S \"POP STAR EXPLOSION\")DAVID GREENE, HOST: You know, aging has its benefits, right? There's wisdom. There's experience. There's retirement - for some people, at least. And yet, there is this undying quest for the fountain of youth because what happens to our bodies is - I mean, let's just say it - not that appealing - weaker muscles, less endurance, overall slowdown, losing your hair. So what if these symptoms of getting old could be stopped or slowed or even reversed? NPR's Elise Hu has been exploring this - trying to get younger. This is part of our series Future You, which looks at emerging technologies. Hi there, Elise. ELISE HU, BYLINE: Good morning. GREENE: You look - I don't know. You don't look younger. You don't older. What - I don't know what I should say here. But. . . HU: Thanks for not stepping in it. GREENE: Yeah, I'm not going to step in it. But are you really telling me that there's some kind of treatment that treats aging? HU: Yes. And we have to be careful about getting too excited here. But what scientists have found in mice and other organisms is that they can speed up aging symptoms or - more beneficial to us - slow down and reverse them, giving the mice better blood flow, more vitality, longer endurance. And what's happening is they can take two mice born on the same day and, after a year of life, feed one an experimental molecule. And one of them who didn't get the molecule is kind of long in the tooth, right? It has gray hair. It looks older. It runs on a treadmill slowly or even falls off. The other gets the molecule - same real age as the older looking mouse - but its hair is still dark, and it's just running and running, running like it's a teenager. GREENE: But how is this possible? HU: I know. It sounds crazy, right? I met the aging researcher, David Sinclair, at Harvard Medical School, and he explained it to me. DAVID SINCLAIR: We have a molecule that we put in their water called NMN. And their muscles appear younger, and they can run further. They get new blood vessels. They have more blood flow. HU: So to make the mice physically younger, what they're doing is boost something in the cells called sirtuins. And the sirtuins are like emergency responders for cells. They repair the cellular damage that comes with aging. GREENE: So this is not just about, like, exercising and keeping yourself healthy. This is a molecule that these mice are just drinking in their water, and it's working to slow down aging. HU: Correct. GREENE: And so my next question - can I do this? HU: That is the big question. GREENE: Right. HU: So of course, you can ingest something. But will it make you feel like you're a kid again? - is the big question. They are working on it. There are clinical trials in humans that are going on now in Boston. And geneticists behind this say they're hoping to show that, in the future, humans could be living at least 10, 20 years longer. And we're not talking just life span; we're also talking health span - so the healthy kind of years where you can be present and enjoy your activities and enjoy your families. GREENE: So did you ingest some of this stuff and actually try this? HU: OK. Well, I tried a full anti-aging regimen - so not just this particular molecule - from an oncologist and a longevity specialist named Dr. Peter Attia. And just to be upfront about this, people like Attia and Dr. Sinclair, they stand to make money off of these solutions. But I wanted to know more about what they're selling. So I tried Dr. Attia's approach. It emphasizes five key things - one, more sleep and meditation to keep your stress levels low. . . GREENE: Good. HU: . . . Also, aerobic exercise most days of the week - so 45 minutes a day if possible; three, a healthier diet; and finally, this NMN molecule that's being tested now in humans. It is currently available as a supplement. And Attia chose one for me to try with a bunch of caveats. PETER ATTIA: The supplement industry is pretty loosely regulated, and therefore, you can't always be sure that what somebody says is in there is actually in there. Truthfully, my intuition is that most of them are [expletive]. I think there are others that are at least legitimately making what they're saying they're making. HU: And David, of course, like I said, these aren't the same as approved drugs, but I took the molecule anyway as instructed and did everything else - the fasting, the exercise, the sleep and the meditation. Here it goes - kind of sweet, kind of chalky. GREENE: Do you feel younger? Did you get younger? Like, I don't know how to ask this appropriately. HU: Right. And none of us know how long we're going to live. GREENE: Yeah. HU: But I got an initial blood draw before all of this. And then at the end of the six-week regimen, I got my blood drawn again. And the biomarkers in there, like liver enzyme and blood glucose, they went in the right direction - enough that an algorithm from a biohacking startup that calculates longevity based on key indicators for disease - it found that I shaved about five years off my internal or biological age, which is not bad. GREENE: So you've been covering this stuff - I mean, the future with this general time span, this marker of, like, the year 2050. So what would it mean if the world with this huge population already - if humans were at that point living even longer, if this stuff was working? HU: Right. That's a good question. And I put it to David Sinclair, and he admits it's a tricky problem. SINCLAIR: Well, if we all live forever, that's not going to work. We'll have to find a new planet. Another bad scenario is we have a lot of people around that's taking up jobs and politicians who will stay in power for a century. That's a concern. HU: But he's an optimist. He thinks that human ingenuity and the potential that could be unlocked if everyone had more health span and not just life span could knock out the scarier effects that we're talking about. GREENE: All right. Well, we'll check in maybe in 30 years - in 2050 - see how you're doing and how much younger you are or older. . . HU: Yeah. I'll be, like, 7 years old. GREENE: Yeah, that's perfect. HU: (Laughter). GREENE: Thanks, Elise. HU: You bet. GREENE: All right. You can learn a lot more about this in our video series Future You With Elise Hu. You can find that at npr. org/futureyou. It's also on NPR's YouTube channel. (SOUNDBITE OF SKINNY WILLIAMS, STEPHEN GOODSON'S \"POP STAR EXPLOSION\") (SOUNDBITE OF SKINNY WILLIAMS AND STEPHEN GOODSON'S \"POP STAR EXPLOSION\") DAVID GREENE, HOST:  You know, aging has its benefits, right? There's wisdom. There's experience. There's retirement - for some people, at least. And yet, there is this undying quest for the fountain of youth because what happens to our bodies is - I mean, let's just say it - not that appealing - weaker muscles, less endurance, overall slowdown, losing your hair. So what if these symptoms of getting old could be stopped or slowed or even reversed? NPR's Elise Hu has been exploring this - trying to get younger. This is part of our series Future You, which looks at emerging technologies. Hi there, Elise. ELISE HU, BYLINE: Good morning. GREENE: You look - I don't know. You don't look younger. You don't older. What - I don't know what I should say here. But. . . HU: Thanks for not stepping in it. GREENE: Yeah, I'm not going to step in it. But are you really telling me that there's some kind of treatment that treats aging? HU: Yes. And we have to be careful about getting too excited here. But what scientists have found in mice and other organisms is that they can speed up aging symptoms or - more beneficial to us - slow down and reverse them, giving the mice better blood flow, more vitality, longer endurance. And what's happening is they can take two mice born on the same day and, after a year of life, feed one an experimental molecule. And one of them who didn't get the molecule is kind of long in the tooth, right? It has gray hair. It looks older. It runs on a treadmill slowly or even falls off. The other gets the molecule - same real age as the older looking mouse - but its hair is still dark, and it's just running and running, running like it's a teenager. GREENE: But how is this possible? HU: I know. It sounds crazy, right? I met the aging researcher, David Sinclair, at Harvard Medical School, and he explained it to me. DAVID SINCLAIR: We have a molecule that we put in their water called NMN. And their muscles appear younger, and they can run further. They get new blood vessels. They have more blood flow. HU: So to make the mice physically younger, what they're doing is boost something in the cells called sirtuins. And the sirtuins are like emergency responders for cells. They repair the cellular damage that comes with aging. GREENE: So this is not just about, like, exercising and keeping yourself healthy. This is a molecule that these mice are just drinking in their water, and it's working to slow down aging. HU: Correct. GREENE: And so my next question - can I do this? HU: That is the big question. GREENE: Right. HU: So of course, you can ingest something. But will it make you feel like you're a kid again? - is the big question. They are working on it. There are clinical trials in humans that are going on now in Boston. And geneticists behind this say they're hoping to show that, in the future, humans could be living at least 10, 20 years longer. And we're not talking just life span; we're also talking health span - so the healthy kind of years where you can be present and enjoy your activities and enjoy your families. GREENE: So did you ingest some of this stuff and actually try this? HU: OK. Well, I tried a full anti-aging regimen - so not just this particular molecule - from an oncologist and a longevity specialist named Dr. Peter Attia. And just to be upfront about this, people like Attia and Dr. Sinclair, they stand to make money off of these solutions. But I wanted to know more about what they're selling. So I tried Dr. Attia's approach. It emphasizes five key things - one, more sleep and meditation to keep your stress levels low. . . GREENE: Good. HU: . . . Also, aerobic exercise most days of the week - so 45 minutes a day if possible; three, a healthier diet; and finally, this NMN molecule that's being tested now in humans. It is currently available as a supplement. And Attia chose one for me to try with a bunch of caveats. PETER ATTIA: The supplement industry is pretty loosely regulated, and therefore, you can't always be sure that what somebody says is in there is actually in there. Truthfully, my intuition is that most of them are [expletive]. I think there are others that are at least legitimately making what they're saying they're making. HU: And David, of course, like I said, these aren't the same as approved drugs, but I took the molecule anyway as instructed and did everything else - the fasting, the exercise, the sleep and the meditation. Here it goes - kind of sweet, kind of chalky. GREENE: Do you feel younger? Did you get younger? Like, I don't know how to ask this appropriately. HU: Right. And none of us know how long we're going to live. GREENE: Yeah. HU: But I got an initial blood draw before all of this. And then at the end of the six-week regimen, I got my blood drawn again. And the biomarkers in there, like liver enzyme and blood glucose, they went in the right direction - enough that an algorithm from a biohacking startup that calculates longevity based on key indicators for disease - it found that I shaved about five years off my internal or biological age, which is not bad. GREENE: So you've been covering this stuff - I mean, the future with this general time span, this marker of, like, the year 2050. So what would it mean if the world with this huge population already - if humans were at that point living even longer, if this stuff was working? HU: Right. That's a good question. And I put it to David Sinclair, and he admits it's a tricky problem. SINCLAIR: Well, if we all live forever, that's not going to work. We'll have to find a new planet. Another bad scenario is we have a lot of people around that's taking up jobs and politicians who will stay in power for a century. That's a concern. HU: But he's an optimist. He thinks that human ingenuity and the potential that could be unlocked if everyone had more health span and not just life span could knock out the scarier effects that we're talking about. GREENE: All right. Well, we'll check in maybe in 30 years - in 2050 - see how you're doing and how much younger you are or older. . . HU: Yeah. I'll be, like, 7 years old. GREENE: Yeah, that's perfect. HU: (Laughter). GREENE: Thanks, Elise. HU: You bet. GREENE: All right. You can learn a lot more about this in our video series Future You With Elise Hu. You can find that at npr. org/futureyou. It's also on NPR's YouTube channel. (SOUNDBITE OF SKINNY WILLIAMS, STEPHEN GOODSON'S \"POP STAR EXPLOSION\")", "section": "Video: Future You, With Elise Hu", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-26-763545811": {"title": "How The NSA And U.S. Cyber Command Hacked ISIS's Media Operation : NPR", "url": "https://www.npr.org/2019/09/26/763545811/how-the-u-s-hacked-isis", "author": "No author found", "published_date": "2019-09-26", "content": "DINA TEMPLE-RASTON (HOST): From NPR, this is I'LL BE SEEING YOU, a four-part series about the technologies that watch us. I'm Dina Temple-Raston. On August 24, 2015, a 21-year-old British hacker named TriCk stepped out of an Internet cafe in Raqqa, Syria, and climbed into his car. He didn't know it, but he'd been under surveillance for days. He pulled into a gas station, and just as he started filling the tank, a single Hellfire missile came down on him like a meteor from the sky. He was killed instantly. ED CARDON (LT GEN, US ARMY): He was the IT - one of the IT people for ISIS. He was very good. There's probably not another like him. TEMPLE-RASTON: That's General Ed Cardon, and he's a key player in a story that's never been told before. CARDON: In this space, I know one thing: talent matters. And when the talent's not there, it's not as good. TEMPLE-RASTON: This is a story about terrorist hackers and how a secretive military unit fought them in cyberspace. When TriCk was killed, ISIS was at its strongest. Just a year earlier, ISIS had surprised everyone by capturing the city of Mosul from Iraqi forces in just four days. (SOUNDBITE OF NEWS MONTAGE)UNIDENTIFIED REPORTER #1 (REPORTER): Islamic militants seized control of Iraq's second-largest city on Tuesday. UNIDENTIFIED REPORTER #2 (REPORTER): The militants have begun to impose Islamic Sharia law, hard-line Sharia. . . SCOTT PELLEY (JOURNALIST): Another major piece of what America fought for in Iraq was lost today. TEMPLE-RASTON: When all this was going on, Eric Rosenbach was the assistant secretary of defense for homeland and global security, and he was worried about the group's success on another field of battle. ERIC ROSENBACH (FORMER ASSISTANT SECRETARY OF DEFENSE FOR HOMELAND AND GLOBAL SECURITY): The center of gravity was not only their territory but their ability to use the Internet. TEMPLE-RASTON: ISIS was using the Internet in ways no other terrorist organization ever had. It created what it called the Cyber Caliphate, a division dedicated to transmitting ISIS' message out to followers around the world. They published a popular online magazine called Dabiq, which tore a page out of al-Qaida's playbook and published articles about how to launch attacks. The Cyber Caliphate had teams dedicated to posting on Facebook and Twitter; an entire media department filled with cameramen, graphic designers, and even editors; and very high-quality videos that could only be described as ISIS snuff films depicting beheadings, the burning of prisoners alive - all shot with drones and GoPro cameras. The most famous of their offerings was a full-length feature called \"Flames Of War. \"(SOUNDBITE OF ARTILLERY FIRING)TEMPLE-RASTON: Don't just sit there, the video seemed to say. Come to Syria; be part of the fight. And young men and women were listening, literally lining up at the Turkish border trying to get to Raqqa. And it was clear that the U. S. had to find a way to get ISIS off the Internet. (SOUNDBITE OF MUSIC)UNINDENTIFIED SINGERS (SINGERS): (Singing in foreign language). TEMPLE-RASTON: These days, just about all armed conflict includes cyber, whether it means tracking the digital dust of an enemy or hacking into their networks. When people talk about offensive cyber, that's what they mean - attacks in cyberspace. And probably the most famous of these attacks is one that slipped out into the wild in 2010. It was known as Stuxnet. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1 (GOVERNMENT OFFICIAL): A computer virus called Stuxnet was discovered lurking in the databanks of power plants, traffic control systems and factories around the world. TEMPLE-RASTON: Stuxnet was never supposed to be discovered, much less find its way into control systems around the world. It was designed to gum up the works in a very specific uranium enrichment plant in Iran. And even this much later, no one wants to talk about it. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2 (GOVERNMENT OFFICIAL): Because it's classified. UNIDENTIFIED PERSON #3 (GOVERNMENT OFFICIAL): Unfortunately, I can't comment. UNIDENTIFIED PERSON #4 (GOVERNMENT OFFICIAL): I do not know how to answer that. UNIDENTIFIED PERSON #5 (GOVERNMENT OFFICIAL): Two answers before you even get started. I don't know. And if I did, we wouldn't talk about it anyway. TEMPLE-RASTON: Even so, it's an open secret that back in 2007, President Obama approved a secret cyberattack that would run Iran's nuclear weapons program aground, and Stuxnet is what they came up with. The zeros and ones had the same effect a bomb might have had. The virus made the centrifuges in the enrichment plant spin too fast, and they literally blew up. ROSENBACH: You could spend years developing an option for one specific case. TEMPLE-RASTON: Like, say, a specific Iranian enrichment facility - but the U. S. had nothing in its arsenal that could be used more generally against anyone. ROSENBACH: It's evolving that way in cyberspace, but it had not been like that five or 10 years ago. TEMPLE-RASTON: When ISIS burst on the scene, Cyber Command, or Cybercom, hadn't developed ways to respond to all the new apps and new programs that gave groups like ISIS new advantages. Encryption, for example, used to be something that only Fortune 500 companies or governments had access to. Now anyone can send an encrypted message with apps like WhatsApp and Telegram. Those programs use something called public key encryption. If you encode a message using a person's public key, they can decode it using their matching private key. The keys aren't physical, of course; they're actually huge numbers generated by very complicated mathematical equations. (SOUNDBITE OF MUSIC)NEIL (RESERVIST, US MARINE CORPS): Think of it as two locks around a box. TEMPLE-RASTON: This is a guy named Neil (ph), and we're not using his last name for reasons that will become clear in a minute. NEIL: I can give a lock and a key out publicly and say anyone can lock this lock with my public key, and then I can come on the backside of that same lock and open it with my private key. And that ensures that I can - am the only one that can open it. TEMPLE-RASTON: Your private key would be almost like a master key? NEIL: A master key to messages intended for me. TEMPLE-RASTON: You can't just intercept them. You have to go to the accounts themselves where the messages began. Or alternatively, you steal those private keys so you can read them. Encrypted messaging is common practice now. WhatsApp alone has over a billion active users in over 180 countries. And Telegram, the app ISIS preferred, it's been very popular with pro-democracy protesters. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #3 (REPORTER): Protests are still raging in Hong Kong nine weeks after an extradition bill was introduced. That bill. . . TEMPLE-RASTON: According to App Annie, a mobile analytics firm that tracks downloads, Telegram was the most downloaded app in Hong Kong this summer. And that's likely because of its strong encryption. We're talking about all this now because these were the kinds of apps ISIS was using to communicate secretly four or five years ago, which says something about just how tech-savvy the group was. Its members were completely comfortable in cyberspace - in a lot of ways, more comfortable than Cybercom was. And that made leaders there start to rethink what a cyberattack really entailed. It didn't have to be so complicated. It could even be a hacker standby. CARDON: The most common way that you read about that we are all subject to. . . TEMPLE-RASTON: That we can talk about. CARDON: . . . Well, is - is phishing, right? Phishing is still very, very effective. TEMPLE-RASTON: Phishing, those emails you're not supposed to open but sometimes you do anyway. That's General Ed Cardon again. And when he was organizing the fight against ISIS, the people making decisions at Cybercom were in their 40s and 50s. iPhones were new to them. They didn't grow up with social media. The people running ISIS' cyber operations were in their early 20s or 30s. That hacker who was killed by a drone in Raqqa? He was a well-known hacktivist in the U. K. long before he'd ever heard of the Islamic State. In fact, he was kind of famous. He and his friends were part of something called TeaMp0isoN. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #4 (REPORTER): TeaMp0isoN, an anarchist hacktivist group began by. . . TEMPLE-RASTON: . . . Computer-savvy teenagers whose specialty was doing high-visibility hacks. Among other things, they rather famously launched a denial of service attack against the U. K. 's Secret Intelligence Service, MI6. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #4: And by jamming the U. K. 's counterterrorist hotline with hundreds of computer-generated calls. . . TEMPLE-RASTON: They cracked into accounts at 10 Downing Street and posted personal information from Prime Minister Tony Blair's address book. They broke into Facebook accounts and Twitter feeds. TeaMp0isoN seemed to be everywhere. (SOUNDBITE OF SONG)LYRICIST JINN (MUSICAL ARTIST): (Rapping) I linked with PoisAnoN to try and get the message across. We let them poison us, and now we getting memory loss. TEMPLE-RASTON: There was even a rap song about them. (SOUNDBITE OF SONG)\rLYRICIST JINN: (Rapping) I'm getting tired of the system, trying to break out 'cause all my life I've been a victim in this place howl (ph). TEMPLE-RASTON: So you had young hip-hackers like TriCk working for ISIS, and those were the people Cybercom had to learn to fight. (SOUNDBITE OF LYRICIST JINN SONG)STEVE DONALD (US NAVAL RESERVE OFFICER/TECHNICAL DIRECTOR, DEPUTY CHIEF TECHNOLOGY OFFICER, US ARMY CYBER COMMAND): So I'm Steve Donald. I'm a United States Naval officer. I specialize in cryptologic and cyber operations. TEMPLE-RASTON: Captain Steve Donald is a reservist. And when he's not in uniform, he's launching cybersecurity startups. And he's part of the story because in the spring of 2016, a phone call from the head of his reserve unit changed everything. DONALD: And he said, Steve, I - I need you to - I need you to come in. I said, well, I'm not in uniform. It doesn't matter. If you have - if you have a badge, come on in. I can't believe I can actually say this, but they were building a task force to conduct offensive cyber operations against ISIS. TEMPLE-RASTON: He can't believe he can say that because until now, details about the task force were classified. DONALD: And I was like, oh, muck yeah. I'm not a guy who typically yells expletives, but that day I think I did. TEMPLE-RASTON: Donald was asked to pull together a team for something called Task Force ARES. DONALD: In \"Ocean's Eleven\" parlance - right? - you know, I'm not sure I'm terribly comfortable saying that I'm the Brad Pitt guy. But. . . TEMPLE-RASTON: But he was the Brad Pitt guy - Brad Pitt in \"Ocean's Eleven. \" George Clooney plays Danny Ocean. Pitt plays his aide-de-camp. (SOUNDBITE OF FILM, \"OCEAN'S ELEVEN\")GEORGE CLOONEY (ACTOR): (As Danny Ocean) It's never been done before. BRAD PITT (ACTOR): (As Rusty Ryan) You want to knock over a casino. Three casinos? TEMPLE-RASTON: And to do the job, Pitt and Clooney had to find people with very specific expertise - pickpockets, bagmen, explosives guys. You get the idea. (SOUNDBITE OF FILM, \"OCEAN'S ELEVEN\")PITT: (As Rusty Ryan) You'd need at least a dozen guys doing a combination of cons. CLOONEY: (As Danny Ocean) Ten ought to do it, don't you think? TEMPLE-RASTON: That's kind of how Task Force ARES came together. Steve Donald had to find a team of specialists to do something that had never been done before - hack into a terrorist organization's media operation and bring it down. (SOUNDBITE OF FILM, \"OCEAN'S ELEVEN\")PITT: (As Rusty Ryan) All right. Who's in? DONALD: The vast majority of forces flowed in from joint force's headquarters. TEMPLE-RASTON: That's an Army cyber operation in Georgia. Donald brought in experts in counterterrorism who understood ISIS and had watched it evolve from a ragtag team of Iraqi Islamists to something bigger. There were operators, the people who would be at the keyboards finding key servers in ISIS' network and disabling them. He found experts in digital forensics, who knew every twist and turn of computer operating systems. DONALD: They can say this is good, this is bad, this is where the files are located that we're interested in. TEMPLE-RASTON: Files that contain things like those encryption keys we talked about earlier. (SOUNDBITE OF FILM, \"OCEAN'S ELEVEN\")CLOONEY: (As Danny Ocean) Who else is on the list? TEMPLE-RASTON: Analysts, malware experts, behavioralists - people who had spent years studying the smallest habits of key ISIS players - and they all came together with one goal. DONALD: Supporting the defeat of ISIS - right? - deny, degrade, disrupt, and manipulate ISIS' info space. TEMPLE-RASTON: Deny, degrade and disrupt - which is harder than it sounds because ISIS used encryption and remote servers. It was global, and its followers understood social media and how to abuse it. They knew how to set up dummy accounts, and they had pretty good operational security. So from a technology standpoint, it was hard enough. But to be successful, they needed to exploit a psychological component, as well. NEIL: This cyber environment involves people. It involves their habits, the way that they operate, the way that they name their accounts. . . TEMPLE-RASTON: Neil, again. NEIL: . . . When they come in during the day, when they leave, what types of apps they have on their phone. Do they click everything that comes into their inbox, or are they very tight and, you know, restrictive in what they use? All of those pieces are what we look at, not just the code. TEMPLE-RASTON: What we look at, he said. Neil has asked us to only use his first name to protect his identity because the Task Force ARES' mission against ISIS, something they called Operation Glowing Symphony, it was largely his idea. NEIL: I was the lead planner as well as the mission commander. I was the one that said go. TEMPLE-RASTON: Coming up, we go behind the scenes of Operation Glowing Symphony. This is I'LL BE SEEING YOU, a four-part series about the technologies that watch us, from NPR. I'm Dina Temple-Raston. Stay with us. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: This is I'LL BE SEEING YOU, a four-part series about the technologies that watch us, from NPR. I'm Dina Temple-Raston. On the show today, a classified offensive cyber mission called Operation Glowing Symphony. This is the first time details of this mission have been revealed publicly by the people who lived it. NEIL: We'll just go with first names, so you can just call me Neil. TEMPLE-RASTON: What's your call sign? NEIL: My call sign is Shadow Recon. That's the hacker handle that I use. TEMPLE-RASTON: Neil is a Marine reservist in his 30s. And we're only using his first name because he wasn't just the one who said go to start Operation Glowing Symphony, it was his idea. NEIL: So we were down in the basement at NSA, and we had an epiphany of how this. . . TEMPLE-RASTON: The epiphany was how ISIS distributed its propaganda. It turns out, they weren't as careful as they could have been. They did what all hackers do; they took shortcuts. They got a little lazy. Nearly all their propaganda was passing through, to use a geeky computer term, the same 10 nodes. NEIL: Every account, every IP, every domain, every. . . TEMPLE-RASTON: Think of a node as a kind of hub. You may be creating content on the laptop on your desk, but when you send it to someone else or to the outside world, you're sending it through this hub. If the node or hub is attacked, then your content can't go out. And if someone is sitting inside the node without you knowing about it, they can see and control everything. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: This was Neil's epiphany. He ran into the leadership's office, grabbed a magic marker and started drawing crazy circles and lines on a whiteboard. NEIL: Pointing everywhere and saying, it's all connected. These are the key points. Let's go. I felt like I was in \"It's Always Sunny In Philadelphia,\" when he's doing the mystery investigation for Pepe Silvia - pictures on the wall and red yarn everywhere - and nobody was understanding me. TEMPLE-RASTON: But as Neil kept explaining and drawing, this kind of bicycle tire thing started to emerge. NEIL: Bicycle tire with spokes, all of the things that were tied to that one node. And then there was another one. I said, everything's tied to these three language websites. TEMPLE-RASTON: And as what he was saying started to sink in, everything shifted. NEIL: We could take those over. We were going to win everything. It was a house of cards. TEMPLE-RASTON: And from those frantic scrawls, a plan to deny, degrade and disrupt. NEIL: We were focused on completely dismantling it in a systematic fashion so that it was in no way, shape or form recognizable from where it was that day. TEMPLE-RASTON: And while we don't understand everything they did to crack into ISIS' network, we do know that they used an old standby to start - a phishing email. CARDON: You know, they clicked on something or they did something that then allowed us to gain control and then start to move. TEMPLE-RASTON: That's General Ed Cardon again. He was Task Force ARES' first commander. In Task Force ARES, did you have like a - phishing specialty people? CARDON: We have people that are very good at mapping networks. TEMPLE-RASTON: Mapping a network is about understanding the dynamics of an organization. If you get someone to click on a phishing email, then that gives you an opportunity to explore a little bit. You can root around someone's email account to see who they talk to. Which emails get answered right away? Maybe that's a boss. Which ones just kind of sit there? Maybe that's someone they have a problem with or someone they don't respect. What do the emails say? This is the data collection you used to get with human sources that now is typically done online. CARDON: The first thing you do when you get in there is you got to get some persistence and spread out. TEMPLE-RASTON: So if you want to get into ISIS' networks, you might send them a phishing email that they can't help clicking on. And then, with a little exploring, you get yourself an administrator's account. CARDON: You can operate freely inside the network because you look like a normal IT person. TEMPLE-RASTON: And ISIS had IT people? CARDON: Oh, yes. TEMPLE-RASTON: They had a whole IT department? CARDON: Yes, they did. TEMPLE-RASTON: The spring and summer of 2016 was spent preparing for attack. It meant dropping malware on servers or looking for folders that contained things that might be helpful later, like those encryption keys we talked about before. And the deeper ARES got inside ISIS' network, the more it looked like their theory of the 10 nodes was correct. And those nodes weren't just in Syria and Iraq, they were everywhere, on servers around the world, which meant ARES had a new problem. It had to figure out how to target just the part of the server that contained ISIS material and nothing else. TIM HAUGH (MAJOR GEN, US AIR FORCE): On every server, there might be things from other commercial entities. We are only going to touch that little sliver of the adversary space and not perturb anybody else. TEMPLE-RASTON: Air Force General Tim Haugh was the first deputy commander of JTF ARES. He worked with General Cardon. And that server issue was one of the problems he had to solve. If ISIS had stored something in the cloud or in a server sitting in, say, Germany, ARES had to show that it could attack the ISIS material and leave everything else on the server unscathed. And they spent months launching small missions that showed it could be done. Then one day, General Haugh turned to a young Marine and asked out loud what everybody was thinking. HAUGH: How big can we go? And he said, sir, we can do global. I said, that's it. Write it down. We're going to take it to General Cardon. So - so that officer really set the stage. TEMPLE-RASTON: And that aggressive Marine - you may have guessed - it was Neil. He began peppering the leadership with ideas. He talked to them not just about hacking one person or ISIS in Syria and Iraq but how to take down the media operation's entire global network. NEIL: That's how these attacks work. They start very simple, and they become more complex. CARDON: Actually, a lot of them come up that way. Like, we could do this. TEMPLE-RASTON: That's General Cardon again. CARDON: Somebody says, well, we could gain access and do this to the files. Really? You can do that? Oh, yeah. Would anyone notice? Well, maybe. But the chances are low. It's like, that's interesting. Put that on the list. TEMPLE-RASTON: And there's something else going on here that's important. Neil was briefing general officers directly. That was a purposeful, organizational decision that made Task Force ARES different. Instead of a top-down traditional military hierarchy, ARES was built around trust, access, the competition of ideas and a willingness to take risks. It was organized so people like Neil wouldn't get buried. In a way, that was a revolution, too. So that's how Glowing Symphony became a global offensive cyber mission. And at the time, in 2016, that was a big deal. Remember; back then, offensive cyber operations meant doing the kind of thing North Korea was doing in 2014, when it hacked into Sony Pictures, or what the Iranians did two years earlier when they fried all the hard drives at Saudi Aramco, the state oil company. So the thought of the U. S. going on the offensive in that way - for a lot of people, it was scary. General Haugh again. HAUGH: When we think about how we all operate every day in the digital environment, we don't think a lot about the architecture that's behind it. It took a lot of explanation at all levels. TEMPLE-RASTON: In other words, a lot of people don't know how the Internet works, just that it does. And Haugh had to convince members of Congress and leaders at the Defense Department that Task Force ARES wasn't going to break the Internet. So there was a team - Joint Task Force ARES. There was a plan - Operation Glowing Symphony. There were briefings to secure approvals, right up to the president. And then, finally, a go. NEIL: The day of the operation was a very long day - coordination, final rehearsals, everybody wanted to be briefed. And then they also wanted to look me in the eye and say, are you sure this is going to work? And every time, I had to say, yes, no matter what I thought. TEMPLE-RASTON: They'd organized four separate teams at workstations set up like carols. Sergeants at the keyboard, sitting next to intelligence people, sitting next to linguists. Each station had four flat screen computer monitors on adjustable arms. And the operators sat in those huge chairs you see Internet gamers settle into before a long night. The room was packed. NEIL: I felt like there were over 80 people in the room between the teams and then everybody lining the back wall that wanted to watch. TEMPLE-RASTON: Neil was standing in a small, elevated bay at the back, from which he could see all the screens in front of him. And there weren't just glowing numbers or lines of code, he could see log-in screens - actual ISIS log-in screens - each carefully preselected and put on a target list that was so long it was on a 3 foot by 7 foot piece of paper hung on the wall. HAUGH: It was almost like a very large bingo card. TEMPLE-RASTON: General Haugh said there were numbers on that bingo card that corresponded with specific targets. Number five may have been one of the editors of the media operation, and it included all the accounts and IP addresses associated with him. Number 12 might have been the group's graphic designer. As they slept, the ISIS members had no idea that, half a world away, a room full of military cyber operators were about to take over their accounts and crash them. What follows is a pretty good approximation of what our reporting says happened that first night. It's based on interviews with half a dozen people who were there. Neil ordered the teams to start the operation. NEIL: Fire. TEMPLE-RASTON: After months of looking at static webpages and picking their way through ISIS networks, the task force started logging in as the enemy. They deleted files, changed passwords - click here, a digital forensic expert would say. We're in, the operator would respond. And then. . . NEIL: There was like six minutes where nothing was really happening because the Internet was a little slow. TEMPLE-RASTON: Yes, he just said the Internet was a little slow. NEIL: And then, you know, minute seven, eight, nine, 10, it started to flow in and I - you know, my heart started beating again. TEMPLE-RASTON: They began moving through the ISIS networks they had mapped for months. It was like watching a raid team clearing a house, except it was all online. And then - an unexpected hiccup. NEIL: We get prompted a security question. TEMPLE-RASTON: A security question - you've seen them before. What's the name of the street you grew up on? What's the first name of your best friend from childhood? The room quieted down. NEIL: And we're stuck dead in our tracks. What is the name of your pet? How could the team possibly know that? We all looked to each other and we're like, what can we do? There's no way we're going to get in. This is going to stop the 20 or 30 targets after this. And one of our best analysts stands up and he goes, sir, one-two-five-seven. And we're like, what? One-two-five-seven. We're like, how do you know that? I've been looking at this guy for a year. He does it for everything. We're like, all right. Your favorite pet - one-two-five-seven. Boom. We're in. TEMPLE-RASTON: After that, momentum started to build. One team would be taking screenshots to gather intelligence for later. Another would be logging ISIS videographers out of their own accounts. Reset successful, the screen would say - folder directory deleted. The screens they were seeing at the NSA campus were the same ones someone in Syria might have been looking at in real time - that is, until someone in Syria hit refresh. Once they did that, they'd see 404 error - destination unreadable. Target five is done, someone would yell, and someone else would walk across the room and cross the number off the big target sheet on the wall. NEIL: We're crossing names off the list. We're crossing accounts off the list. We're crossing IPs off the list. TEMPLE-RASTON: Every time a number went down, they would yell one word. NEIL: Jackpot. TEMPLE-RASTON: Neil gave us an idea of how it went. NEIL: They were running back and forth, on scratch pieces of yellow paper - the number five - five, jackpot - or a username that they had taken control of - 44, jackpot. And then we'd draw the line out. And I had stacks of paper coming up on the corner of my desk - 18, three, number six - jackpot, jackpot, jackpot, jackpot. I knew in about the first 15 minutes that we were on pace to accomplish exactly what we needed to accomplish. TEMPLE-RASTON: Once they'd taken over the 10 nodes and they blocked key people out of their accounts, they just kept chewing your way through the target list. NEIL: We spent the next five or six hours just shooting fish in a barrel. We had been waiting a long time to do that, and we had seen a lot of bad things happen. And we were happy to see them go away. TEMPLE-RASTON: And while nothing was ever going to stop random ISIS fighters from grabbing laptops and setting up new networks, there were some indications of success. Active servers were down, key accounts locked, files erased. And there were other small victories. Dabiq, the popular ISIS online magazine we mentioned before - it started missing deadlines. NEIL: These magazines had been coming out at a regular basis - like, every 28 to 30 days. And after Glowing Symphony, we saw variance. I think it was 36 days, which was the longest time between Dabiqs that ever happened. TEMPLE-RASTON: Why that happened is unclear. Was it because folders had been deleted and servers were down? Or because ISIS was under pressure on the ground, losing more and more territory in Syria and Iraq? It was impossible to tell, but the delays were eating away at the magazine's following. Dabiq eventually folded. NEIL: All these delays made it so they weren't as an effective media organization. TEMPLE-RASTON: For ISIS members, there was no ambiguity about whether they'd been attacked. It was clear. They couldn't get into accounts, couldn't use servers, lost key files. And while these insider details about how ARES hacked ISIS are new, the tools they used to do so - they are exactly the kind of thing nation-state hackers typically use. It's how the Iranians hacked into the Sands Casino in 2014. It's how the Russians got into election systems in the Midwest. And now it was how Cyber Com and the NSA were fighting ISIS. They started looking for new ways to keep the mission going - for new ways to deny, degrade and disrupt the enemy. JENNIFER BUCKNER (BRIGADIER GENERAL, US ARMY): I'm Brigadier General Jennifer Buckner. I go by Jen. I'm an Army officer. TEMPLE-RASTON: Buckner was one of the people who took the reins of Task Force ARES after Glowing Symphony had started. And after that first night, the mission shifted into a second phase. To keep pressure on ISIS, Operation Glowing Symphony began to revolve around five lines of effort. In addition to media targets, ARES wanted to make it hard for ISIS to operate on the Internet more generally. They wanted to help the forces on the ground that were pushing ISIS out of Syria and Iraq and make it harder for ISIS to raise money and attract recruits. They also started partnering more. They had people from State and DHS and Treasury actually working with the task force, and they deployed members of ARES to other countries. And because the team was based at the NSA campus at Fort Meade, getting reinforcements to do those things often just required a walk down the hall. BUCKNER: There was a lot of junior talent that we contributed to this. We pulled whoever knew - we thought could do the job and knew the mission. TEMPLE-RASTON: Best geek. BUCKNER: Absolutely, best geek. TEMPLE-RASTON: And after the first night of crossing targets off that bingo card, the mission continued, though in a more creative way. Joint Task Force ARES operators started making all those things that drive you crazy about today's technology - slow downloads, dropped connections, access denied, program glitches - they made them start happening to ISIS fighters. BUCKNER: Some of these are not sophisticated effects, but they don't need to be. The idea that, yesterday, I could get into my Instagram account and today I can't is confusing. TEMPLE-RASTON: If this sounds like something you've experienced, that's exactly the point. (SOUNDBITE OF KEYBOARD KEYS CLICKING)TEMPLE-RASTON: If you can't get into an email account, what do you do? You think, maybe I mistyped the log-in or password, so you put it in again - still doesn't work. Then you type it in more deliberately. And every time you type it, press enter and are denied, you get a little more frustrated. (SOUNDBITE OF COMPUTER BEEPING)TEMPLE-RASTON: If you're at work, you call the IT department, and you explain the issue. And then they ask you if you're sure you typed in your log-in and password correctly. It's enough to drive you nuts. It would never occur to you or to me or ISIS that this might be part of a cyberattack. BUCKNER: It just looks like I messed something up or something's wrong with the platform. TEMPLE-RASTON: That's what the follow-on phases of Operation Glowing Symphony were all about; that psychological component - eroding morale. One member of ISIS would stay up all night editing a film, and then he'd ask a fellow ISIS member to upload it. But operators with Glowing Symphony would make it so it didn't quite land at its destination. The ISIS member who stayed up all night starts asking the other ISIS member why he didn't do what he'd asked him to do. He gets angry, and so on. BUCKNER: We had to understand, how did all of that work? And so what is the best way to cause confusion online? TEMPLE-RASTON: Let's drain their cellphone batteries or insert photographs into videos that aren't supposed to be there. Task Force ARES would watch, react and adjust their plans. They'd change passwords, delete content, shut people out of their accounts; all in a way that made it look like it was just run-of-the-mill IT problems. CARDON: Pinwheels of death - the network's working really slow. People get frustrated. TEMPLE-RASTON: That's General Ed Cardon again. And he says, after more than two years on the mission, the operators had developed a kind of organizational memory. CARDON: This is built through repetition - when you do a lot - is you just sort of know what that person's going to do. TEMPLE-RASTON: Operation Glowing Symphony still isn't over. Task Force ARES, now led by General Matthew Glavy, is still sitting in ISIS networks - still driving them crazy. This is I'LL BE SEEING YOU from NPR. When we come back, something we haven't mentioned - the very dark side of offensive cyber. UNIDENTIFIED PERSON #6: This is not anything that was clean or virtual, it was something that had very lethal impacts. TEMPLE-RASTON: And a rare, exclusive interview - NSA Director General Paul Nakasone. I'm Dina Temple-Raston. Stay with us. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: This is I'LL BE SEEING YOU from NPR, a four-part series about the technologies that watch us. And today's show is about offensive cyberattacks. Specifically, we're talking about a classified mission - a mission in which the U. S. military hacked one of the most dangerous and tech-savvy terrorist organizations in the world. The military cyber arm, something called U. S. Cyber Command, and the NSA put together a special unit to knock ISIS off the Internet. They called it Joint Task Force Ares, and in the fall of 2016, something called Operation Glowing Symphony began. And it's still going on today. MATTHEW GLAVY (US GENERAL, TASK FORCE ARES COMMANDER): ISIS came in using the information domain, using Samsung phones with some HD graphics, to use information warfare in a way that's never been used before. TEMPLE-RASTON: That's General Matthew Glavy. He's the current commander of Task Force Ares. GLAVY: Atrocious, heinous, dark side - but nonetheless, they used information warfare probably better than anybody else ever has. TEMPLE-RASTON: Glavy said the five lines of effort on which Ares focused three years ago are still there. Ares still has its thumb on ISIS' media operations. The group is still having a lot of trouble operating freely on the web. Ground forces have been successful in driving them out of most of Syria and Iraq, but there are still pockets where ISIS has a presence. And they have moved to other countries like Libya, and Mali, and even the Philippines. Ares is still sitting on ISIS to prevent it from raising money and attracting recruits. Those original so-called lines of effort still apply, but they've evolved. GLAVY: They've morphed a little bit. But let's face it, we got to be ever so diligent and vigilant about the media piece. And we cannot have - for them to gain the momentum that we saw in the past. So we - we've - we have to learn that lesson. TEMPLE-RASTON: Remember that Hellfire missile that came roaring out of the Syrian sky to kill the ISIS hacker? His real name was Junaid Hussain, and he was just 21 years old when he died in Syria - a hacker deemed so dangerous the American military decided to kill him. Lorraine Murphy is a digital journalist who writes about technology and hacktivism, and she knew Hussain when he was just a teenager with a computer and a cause. She says ISIS must have seen him coming. LORRAINE MURPHY (DIGITAL JOURNALIST): He emerged at the right time. He was in the right place. He had all of the ingredients. He had connections. He had a significant social media following. TEMPLE-RASTON: By August 2014, he had traveled to Syria and began to call himself Abu Hussain al-Britani, and he joined their Cyber Caliphate. MURPHY: He was fluent in all of the things that fundamentalists tend not to be fluent in, like technology, like social media. He could be charming. He could be a gifted propagandist so I mean, they couldn't have invented a more perfect head of hacking for ISIS. TEMPLE-RASTON: You can sit at home and play \"Call Of Duty,\" one of his most famous tweets from Syria read, or you can come here and respond to the real call of duty. The choice is yours. He also rather famously hacked into U. S. Central Command's Twitter account and released a list of U. S. military personnel with names and addresses, and then called on ISIS members to kill them. It's not surprising that Cyber Command demanded a response. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #5 (REPORTER): U. S. spy drones followed and tracked notorious British-born ISIS hacker Junaid Hussain for days in the middle of heavily. . . ROSENBACH: And I know to some NPR listeners this will sound like a bitter pill. TEMPLE-RASTON: Again, former Defense Department official Eric Rosenbach. ROSENBACH: If there are a very small number of individuals in a country who know how to build a nuclear weapon and you try to think about ways to prevent those people from accomplishing their mission, that can be very effective - same thing in cyberspace. When you think about Junaid Hussain or others, we thought about ways to neuter his cybercapability to prevent them from getting online. It is really important to remember this is a war. TEMPLE-RASTON: Officials familiar with his case said that he double-clicked on a phishing email from Cybercom that allowed them to track his phone, to follow him, and eventually kill him at that gas station. ROSENBACH: For cyber problems, you can't just use cyber tools. When it's in a warzone, using kinetic physical force to address that issue - which is eventually what it came down to in that case. TEMPLE-RASTON: Even if you think Hussain's work with ISIS made him a legitimate target for attack, the problem is that this kind of thing doesn't stop there. The U. S. isn't the only country using offensive cyber this way. Remember the much more alarming case of the Washington Post journalist Jamal Khashoggi? (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #6 (REPORTER): Turkish officials have audio and video recordings of the gruesome murder of journalist Jamal Khashoggi inside the Saudi Arabian. . . UNIDENTIFIED REPORTER #7 (REPORTER): He went to the consulate to obtain paperwork to marry his fiancee and was never seen again. TEMPLE-RASTON: He was brutally murdered in a Saudi embassy late last year, and his body was never recovered. Offensive cyber appears to have played a key role in his death, too. Just as Cybercom followed Junaid Hussain by putting something on his phone, that seems to have happened in Khashoggi's case as well. RON DEIBERT (DIRECTOR, CITIZEN LAB): So when we - we talk about offensive cyber operations, I think it's important to understand that it doesn't always come in one flavor. TEMPLE-RASTON: Ron Deibert is the director of the Citizen Lab at the University of Toronto's Munk School of Global Affairs. The Citizen Lab does news-breaking research on digital security and human rights. Six years ago, it rather famously discovered that China had been hacking into the Dalai Lama's computer networks. And last year, it looked into another case of state-sponsored offensive cyber. DEIBERT: If you look at the murder of Jamal Khashoggi, I would say that a lot of the preparation for that and the lead up to it had to do with Saudi Arabia using offensive cyber weapons. TEMPLE-RASTON: Deibert's researchers dug into the case and they found offensive cyber tools tracking the journalist and his inner circle. Citizen Lab says it's figured out a way to detect if a phone has been targeted by programs that can infiltrate encrypted phones and apps. And they found just such a program in this case. A Saudi dissident who is an associate of Khashoggi's filed a lawsuit that says he found tracking spyware on his phone. He said it allowed the Saudis to secretly listen to his calls, read his messages and track his Internet history. Allegedly, it could also turn on the phone's microphone and camera. And the Saudi case isn't an outlier. In Mexico alone, Citizen Lab found 27 cases of this kind of offensive cybertool targeting political rivals, reporters and civil rights lawyers. DEIBERT: I think there's a control problem here. TEMPLE-RASTON: That's Ron Deibert again. DEIBERT: You really create conditions for an escalation of an arms race in cyberspace that really could come back to haunt the United States in the long run. TEMPLE-RASTON: Deibert says even if the United States is being careful in its use of offensive cyber, the mere fact that America is using it gives license to others to do the same. DEIBERT: There is a demonstration effect, and the equipment, the software, the methods, the capabilities proliferate. TEMPLE-RASTON: Large cyber operations like Glowing Symphony worry Deibert, too. He says offensive cyber is blurring the lines between military and civilian targets. Remember those servers with ISIS material outside of Syria and Iraq? Well, they had civilian material on them as well. UNIDENTIFIED PERSON #7: We're effectively talking about military operations that are in our common communications infrastructure. These type of operations are - are effectively maneuvering through what is essentially a public sphere on a global level. TEMPLE-RASTON: It's like using a missile in a regular city. UNIDENTIFIED PERSON #7: Exactly. TEMPLE-RASTON: Last August, President Trump signed an order that allows the secretary of Defense to conduct cyber operations more freely. Now they can launch an attack without necessarily needing presidential approval. It was meant to replace an Obama-era order known as Presidential Policy Directive 20, which set out a strict framework to keep cyber operations in check. The new directive, which has not been publicly released, is supposed to remove bureaucratic obstacles that the Defense Department thought were preventing them from fighting off cyberthreats fast enough. The subtle uses of cyber, hacks like phase two of Operation Glowing Symphony, raise other questions. What if, instead of the pinwheel of death or inserting photographs, enemies hacked into health records and changed just one thing, like, say, blood type? Imagine the damage that could cause. DEIBERT: I think there's always been a recognition of the value of offensive capabilities in cyberspace from a U. S. perspective. TEMPLE-RASTON: Ron Deibert again. DEIBERT: There may have been some reticence to deploy these widely, perhaps for legal reasons or the precedent that they set for other countries and for potential arms race in cyberspace, but those concerns seem to have lessened. And now with discussions of persistent engagement, what we're talking about is something that is more active. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Persistent engagement and defend forward - these terms come from NSA director and Cybercom Commander General Paul Nakasone, and he talked about them when he gave NPR a rare exclusive interview a few months ago at the NSA campus at Fort Meade. PAUL NAKASONE (COMMANDER GENERAL, CYBERCOM): Defend forward is part of the DOD cyber strategy that talks about acting outside our borders to ensure that we maintain contact with our adversaries in cyberspace. TEMPLE-RASTON: In other words, you don't want to wait to be attacked in cyberspace. You do things that allow you to hack back if there's an attack in the future, and that could be deploying a small team in another country that asks for help. NAKASONE: That can be hunting on a network to look for malware, or it could be, as we did in Operation Glowing Symphony, the idea of being able to impact infrastructure worldwide. TEMPLE-RASTON: Those targeted attacks that let them take down ISIS material on a server without affecting anyone else - the U. S. used to focus on defending its networks. Now it seems to be leaning more on offensive capabilities. You can draw a straight line from Task Force ARES to a new unit the NSA and Cyber Command have just started discussing publicly - something called the Russia Small Group. Just as ARES focused on ISIS, the Russia Small Group is organizing around countering Russian cyberattacks. We've known for some time that Russia has been trying to plant malware in key infrastructure targets in the U. S. The intelligence community has made clear that Russia used social media to meddle in the 2016 elections, and they expect that to continue in the 2020 elections as well. General Nakasone talked about this during our interview at the NSA. But first, there were some funny things about that interview. Because the National Security Agency is one of the most secretive organizations in the world, all the sound you hear from inside the NSA - it is from inside the building but they wouldn't let us record it for ourselves. They recorded it and then sent it to us in a file called Unclassified Sounds of The NSA. Think of it as a kind of NSA greatest hits. We heard all these things when we were there, of course, but we couldn't get them on tape. We were permitted to record General Nakasone, however. We sat down at a teak conference table that seats several dozen, and there's a kind of backbencher row of seats like in a movie theater. NAKASONE: A little bit of a big room but I thought it would be easier than probably doing it in my office, so. . . TEMPLE-RASTON: Army General Paul Nakasone has two jobs. He's the director of the nation's largest spy agency, the NSA. And he also leads U. S. Cyber Command, the military's top cyber arm. Whenever you hear about American cyberattacks, the people behind them are at Cybercom. The ones you don't hear about probably came out of the NSA. Before becoming NSA director last year, Nakasone was the head of the Army's Cyber Command, and he was in charge of Joint Task Force ARES when the cyber mission against ISIS, Operation Glowing Symphony, first started. And while he wasn't convinced the mission was a success in the first 15 minutes like Neil was, he said it was clear the mission was working from very early on. NAKASONE: Within the first 60 minutes of go, I knew we were having success. TEMPLE-RASTON: And you saw things crumble. NAKASONE: We would see the targets start to come down. It's hard to describe, but you can just sense it from being in the atmosphere that the operators, they know they're doing really well. And they're not saying that, but you're there and you know it. TEMPLE-RASTON: Nakasone said that before Ares, the fight against ISIS in cyberspace was episodic. Now it's continuous. NAKASONE: We were going to make sure that any time ISIS was going to raise money or communicate - we were going to be there. TEMPLE-RASTON: And the fact that Cybercom and Task Force Ares are there has meant that ISIS has had to change the way they operate. They aren't as strong in cyberspace as they were. They're still there but not in the same way. NAKASONE: We were seeing an adversary that was able to leverage cyber to raise a tremendous amount of money, to proselytize. And we were seeing a series of videos and posts and media products that were high-end. We haven't seen that recently. TEMPLE-RASTON: And, of course, that's good. NAKASONE: And that's one of the things that we will continue to do as ISIS shows their head or shows an ability to act. We're going to be right there. TEMPLE-RASTON: Back in June, The New York Times reported that the U. S. had cracked into Russia's electrical power grid and had planted malware there, which. . . (SOUNDBITE OF MONTAGE)UNIDENTIFIED REPORTER #8 (REPORTER): Has the potential, presumably, to take them offline. UNIDENTIFIED REPORTER #9 (REPORTER): Multiple security officials confirmed the report. UNIDENTIFIED REPORTER #10 (REPORTER): Cyber Command is gaming out what would happen if Russia attacks key states just as America goes to vote in 2020. TEMPLE-RASTON: Though Nakasone wouldn't confirm The New York Times story, it isn't hard to see how that would fit into a Russia small groups operation if it's modeled on Ares. The assumption has always been that Russia is in American networks in the event of a conflict with the U. S. in the future. Nakasone suggested that the U. S. has been doing the same, not just a response to what Russia is trying to do now but what it might attempt to do later. Nakasone said the first thing he did when he became NSA director in the spring of 2018 was to review what the Russians had done in the runup to the U. S. presidential elections. He wanted Cybercom to learn from it, to reverse engineer it and see how it works. NAKASONE: What does an adversary do? How do they try to create influence? It provided us a very, very good roadmap of what they might do in the future. TEMPLE-RASTON: Nakasone said the American people shouldn't worry about the 2020 elections because Cybercom is prepared to prevent the Russians from repeating what they did in 2016. NAKASONE: I think it's important for the American public to understand that, as with any domain - air, land, sea or space - cyberspace is the same way. Our nation has a force. We are going to make sure that we're poised, trained and ready to act when authorized. TEMPLE-RASTON: Even saying that much is new. Remember - offensive cyber not so long ago was something they didn't talk about, and now, all of a sudden, they seem to be. So why is General Nakasone talking about this now? DEIBERT: What's happening here is part of a deterrent justification. TEMPLE-RASTON: That's Ron Deibert from the Citizen Lab again. DEIBERT: You can't have cyber operations meaningly (ph) deter your adversaries unless they know that you have these capabilities and they understand what you can do with them. But what's not probably being discussed or appreciated is the extent to which there is a systemic effect of the use of these operations. Other countries take notice. Other actors take notice. TEMPLE-RASTON: At the end of Stanley Kubrick's \"Dr. Strangelove,\" there was an iconic scene in which Peter Sellers talks about the doomsday bomb as the ultimate deterrent. But it only works as a deterrent if people know it exists. (SOUNDBITE OF FILM, \"DR. STRANGELOVE OR: HOW I LEARNED TO STOP WORRYING AND LOVE THE BOMB\")PETER SELLERS (ACTOR): (As Dr. Strangelove) Deterrence is the art of producing in the mind of the enemy the fear to attack. TEMPLE-RASTON: And they come to the conclusion - if you don't tell anybody about it, what good is it? (SOUNDBITE OF FILM, \"DR. STRANGELOVE OR: HOW I LEARNED TO STOP WORRYING AND LOVE THE BOMB\")SELLERS: (As Dr. Strangelove) The whole point of the doomsday machine is lost if you keep it a secret. TEMPLE-RASTON: If you keep it a secret - you could say the same thing about American offensive cyber operations. They've been so stealthy for so long, maybe people don't realize the U. S. has them. We all hear about Russia's influence campaigns. The Chinese have been stealing intellectual property. Iranian hackers have been trolling around in our infrastructure. But we rarely hear much about the American response. This may be an effort to try to change that. The irony is that offensive cyber's richest target is us. Ron Deibert again. DEIBERT: The United States is a country most highly dependent on these technologies and arguably the most vulnerable to these sorts of attacks. I think there should be far more attention devoted to thinking about proper systems and security to defense. TEMPLE-RASTON: Doing that, of course, isn't so easy, either. This is I'LL BE SEEING YOU from NPR. Next time - an old-fashioned spy story with a high-tech twist. UNIDENTIFIED PERSON #8: I just said, it doesn't take a rocket scientist to see what's going on here because a god**** Soviet spy. TEMPLE-RASTON: The show was written and hosted by me, Dina Temple-Raston. Our producer is Adelina Lancianese, and she scored the show, too. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Special thanks to Eric Mennel for his field production, NPR Investigations, the Story Lab and Josephine Wolff at Tufts University. If you missed one of our previous shows, just go to npr. org/illbeseeingyou or find us on NPR One. I'm Dina Temple-Raston, and I'LL BE SEEING YOU. (SOUNDBITE OF MUSIC)NEIL: My friend texted me, and he said, so I hear you're doing the interview. It's General Nakasone, General Haugh, General Cardon and Neil. TEMPLE-RASTON: (Laughter). NEIL: I'm a recon Marine as well, so if you get compromised, you have to buy everybody a case of beer. TEMPLE-RASTON: I think you're going to be buying a lot of beer. NEIL: Yeah. That's OK. That's OK. DINA TEMPLE-RASTON (HOST): From NPR, this is I'LL BE SEEING YOU, a four-part series about the technologies that watch us. I'm Dina Temple-Raston. On August 24, 2015, a 21-year-old British hacker named TriCk stepped out of an Internet cafe in Raqqa, Syria, and climbed into his car. He didn't know it, but he'd been under surveillance for days. He pulled into a gas station, and just as he started filling the tank, a single Hellfire missile came down on him like a meteor from the sky. He was killed instantly. ED CARDON (LT GEN, US ARMY): He was the IT - one of the IT people for ISIS. He was very good. There's probably not another like him. TEMPLE-RASTON: That's General Ed Cardon, and he's a key player in a story that's never been told before. CARDON: In this space, I know one thing: talent matters. And when the talent's not there, it's not as good. TEMPLE-RASTON: This is a story about terrorist hackers and how a secretive military unit fought them in cyberspace. When TriCk was killed, ISIS was at its strongest. Just a year earlier, ISIS had surprised everyone by capturing the city of Mosul from Iraqi forces in just four days. (SOUNDBITE OF NEWS MONTAGE)UNIDENTIFIED REPORTER #1 (REPORTER): Islamic militants seized control of Iraq's second-largest city on Tuesday. UNIDENTIFIED REPORTER #2 (REPORTER): The militants have begun to impose Islamic Sharia law, hard-line Sharia. . . SCOTT PELLEY (JOURNALIST): Another major piece of what America fought for in Iraq was lost today. TEMPLE-RASTON: When all this was going on, Eric Rosenbach was the assistant secretary of defense for homeland and global security, and he was worried about the group's success on another field of battle. ERIC ROSENBACH (FORMER ASSISTANT SECRETARY OF DEFENSE FOR HOMELAND AND GLOBAL SECURITY): The center of gravity was not only their territory but their ability to use the Internet. TEMPLE-RASTON: ISIS was using the Internet in ways no other terrorist organization ever had. It created what it called the Cyber Caliphate, a division dedicated to transmitting ISIS' message out to followers around the world. They published a popular online magazine called Dabiq, which tore a page out of al-Qaida's playbook and published articles about how to launch attacks. The Cyber Caliphate had teams dedicated to posting on Facebook and Twitter; an entire media department filled with cameramen, graphic designers, and even editors; and very high-quality videos that could only be described as ISIS snuff films depicting beheadings, the burning of prisoners alive - all shot with drones and GoPro cameras. The most famous of their offerings was a full-length feature called \"Flames Of War. \"(SOUNDBITE OF ARTILLERY FIRING)TEMPLE-RASTON: Don't just sit there, the video seemed to say. Come to Syria; be part of the fight. And young men and women were listening, literally lining up at the Turkish border trying to get to Raqqa. And it was clear that the U. S. had to find a way to get ISIS off the Internet. (SOUNDBITE OF MUSIC)UNINDENTIFIED SINGERS (SINGERS): (Singing in foreign language). TEMPLE-RASTON: These days, just about all armed conflict includes cyber, whether it means tracking the digital dust of an enemy or hacking into their networks. When people talk about offensive cyber, that's what they mean - attacks in cyberspace. And probably the most famous of these attacks is one that slipped out into the wild in 2010. It was known as Stuxnet. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1 (GOVERNMENT OFFICIAL): A computer virus called Stuxnet was discovered lurking in the databanks of power plants, traffic control systems and factories around the world. TEMPLE-RASTON: Stuxnet was never supposed to be discovered, much less find its way into control systems around the world. It was designed to gum up the works in a very specific uranium enrichment plant in Iran. And even this much later, no one wants to talk about it. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2 (GOVERNMENT OFFICIAL): Because it's classified. UNIDENTIFIED PERSON #3 (GOVERNMENT OFFICIAL): Unfortunately, I can't comment. UNIDENTIFIED PERSON #4 (GOVERNMENT OFFICIAL): I do not know how to answer that. UNIDENTIFIED PERSON #5 (GOVERNMENT OFFICIAL): Two answers before you even get started. I don't know. And if I did, we wouldn't talk about it anyway. TEMPLE-RASTON: Even so, it's an open secret that back in 2007, President Obama approved a secret cyberattack that would run Iran's nuclear weapons program aground, and Stuxnet is what they came up with. The zeros and ones had the same effect a bomb might have had. The virus made the centrifuges in the enrichment plant spin too fast, and they literally blew up. ROSENBACH: You could spend years developing an option for one specific case. TEMPLE-RASTON: Like, say, a specific Iranian enrichment facility - but the U. S. had nothing in its arsenal that could be used more generally against anyone. ROSENBACH: It's evolving that way in cyberspace, but it had not been like that five or 10 years ago. TEMPLE-RASTON: When ISIS burst on the scene, Cyber Command, or Cybercom, hadn't developed ways to respond to all the new apps and new programs that gave groups like ISIS new advantages. Encryption, for example, used to be something that only Fortune 500 companies or governments had access to. Now anyone can send an encrypted message with apps like WhatsApp and Telegram. Those programs use something called public key encryption. If you encode a message using a person's public key, they can decode it using their matching private key. The keys aren't physical, of course; they're actually huge numbers generated by very complicated mathematical equations. (SOUNDBITE OF MUSIC)NEIL (RESERVIST, US MARINE CORPS): Think of it as two locks around a box. TEMPLE-RASTON: This is a guy named Neil (ph), and we're not using his last name for reasons that will become clear in a minute. NEIL: I can give a lock and a key out publicly and say anyone can lock this lock with my public key, and then I can come on the backside of that same lock and open it with my private key. And that ensures that I can - am the only one that can open it. TEMPLE-RASTON: Your private key would be almost like a master key? NEIL: A master key to messages intended for me. TEMPLE-RASTON: You can't just intercept them. You have to go to the accounts themselves where the messages began. Or alternatively, you steal those private keys so you can read them. Encrypted messaging is common practice now. WhatsApp alone has over a billion active users in over 180 countries. And Telegram, the app ISIS preferred, it's been very popular with pro-democracy protesters. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #3 (REPORTER): Protests are still raging in Hong Kong nine weeks after an extradition bill was introduced. That bill. . . TEMPLE-RASTON: According to App Annie, a mobile analytics firm that tracks downloads, Telegram was the most downloaded app in Hong Kong this summer. And that's likely because of its strong encryption. We're talking about all this now because these were the kinds of apps ISIS was using to communicate secretly four or five years ago, which says something about just how tech-savvy the group was. Its members were completely comfortable in cyberspace - in a lot of ways, more comfortable than Cybercom was. And that made leaders there start to rethink what a cyberattack really entailed. It didn't have to be so complicated. It could even be a hacker standby. CARDON: The most common way that you read about that we are all subject to. . . TEMPLE-RASTON: That we can talk about. CARDON: . . . Well, is - is phishing, right? Phishing is still very, very effective. TEMPLE-RASTON: Phishing, those emails you're not supposed to open but sometimes you do anyway. That's General Ed Cardon again. And when he was organizing the fight against ISIS, the people making decisions at Cybercom were in their 40s and 50s. iPhones were new to them. They didn't grow up with social media. The people running ISIS' cyber operations were in their early 20s or 30s. That hacker who was killed by a drone in Raqqa? He was a well-known hacktivist in the U. K. long before he'd ever heard of the Islamic State. In fact, he was kind of famous. He and his friends were part of something called TeaMp0isoN. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #4 (REPORTER): TeaMp0isoN, an anarchist hacktivist group began by. . . TEMPLE-RASTON: . . . Computer-savvy teenagers whose specialty was doing high-visibility hacks. Among other things, they rather famously launched a denial of service attack against the U. K. 's Secret Intelligence Service, MI6. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #4: And by jamming the U. K. 's counterterrorist hotline with hundreds of computer-generated calls. . . TEMPLE-RASTON: They cracked into accounts at 10 Downing Street and posted personal information from Prime Minister Tony Blair's address book. They broke into Facebook accounts and Twitter feeds. TeaMp0isoN seemed to be everywhere. (SOUNDBITE OF SONG)LYRICIST JINN (MUSICAL ARTIST): (Rapping) I linked with PoisAnoN to try and get the message across. We let them poison us, and now we getting memory loss. TEMPLE-RASTON: There was even a rap song about them. (SOUNDBITE OF SONG)\rLYRICIST JINN: (Rapping) I'm getting tired of the system, trying to break out 'cause all my life I've been a victim in this place howl (ph). TEMPLE-RASTON: So you had young hip-hackers like TriCk working for ISIS, and those were the people Cybercom had to learn to fight. (SOUNDBITE OF LYRICIST JINN SONG)STEVE DONALD (US NAVAL RESERVE OFFICER/TECHNICAL DIRECTOR, DEPUTY CHIEF TECHNOLOGY OFFICER, US ARMY CYBER COMMAND): So I'm Steve Donald. I'm a United States Naval officer. I specialize in cryptologic and cyber operations. TEMPLE-RASTON: Captain Steve Donald is a reservist. And when he's not in uniform, he's launching cybersecurity startups. And he's part of the story because in the spring of 2016, a phone call from the head of his reserve unit changed everything. DONALD: And he said, Steve, I - I need you to - I need you to come in. I said, well, I'm not in uniform. It doesn't matter. If you have - if you have a badge, come on in. I can't believe I can actually say this, but they were building a task force to conduct offensive cyber operations against ISIS. TEMPLE-RASTON: He can't believe he can say that because until now, details about the task force were classified. DONALD: And I was like, oh, muck yeah. I'm not a guy who typically yells expletives, but that day I think I did. TEMPLE-RASTON: Donald was asked to pull together a team for something called Task Force ARES. DONALD: In \"Ocean's Eleven\" parlance - right? - you know, I'm not sure I'm terribly comfortable saying that I'm the Brad Pitt guy. But. . . TEMPLE-RASTON: But he was the Brad Pitt guy - Brad Pitt in \"Ocean's Eleven. \" George Clooney plays Danny Ocean. Pitt plays his aide-de-camp. (SOUNDBITE OF FILM, \"OCEAN'S ELEVEN\")GEORGE CLOONEY (ACTOR): (As Danny Ocean) It's never been done before. BRAD PITT (ACTOR): (As Rusty Ryan) You want to knock over a casino. Three casinos? TEMPLE-RASTON: And to do the job, Pitt and Clooney had to find people with very specific expertise - pickpockets, bagmen, explosives guys. You get the idea. (SOUNDBITE OF FILM, \"OCEAN'S ELEVEN\")PITT: (As Rusty Ryan) You'd need at least a dozen guys doing a combination of cons. CLOONEY: (As Danny Ocean) Ten ought to do it, don't you think? TEMPLE-RASTON: That's kind of how Task Force ARES came together. Steve Donald had to find a team of specialists to do something that had never been done before - hack into a terrorist organization's media operation and bring it down. (SOUNDBITE OF FILM, \"OCEAN'S ELEVEN\")PITT: (As Rusty Ryan) All right. Who's in? DONALD: The vast majority of forces flowed in from joint force's headquarters. TEMPLE-RASTON: That's an Army cyber operation in Georgia. Donald brought in experts in counterterrorism who understood ISIS and had watched it evolve from a ragtag team of Iraqi Islamists to something bigger. There were operators, the people who would be at the keyboards finding key servers in ISIS' network and disabling them. He found experts in digital forensics, who knew every twist and turn of computer operating systems. DONALD: They can say this is good, this is bad, this is where the files are located that we're interested in. TEMPLE-RASTON: Files that contain things like those encryption keys we talked about earlier. (SOUNDBITE OF FILM, \"OCEAN'S ELEVEN\")CLOONEY: (As Danny Ocean) Who else is on the list? TEMPLE-RASTON: Analysts, malware experts, behavioralists - people who had spent years studying the smallest habits of key ISIS players - and they all came together with one goal. DONALD: Supporting the defeat of ISIS - right? - deny, degrade, disrupt, and manipulate ISIS' info space. TEMPLE-RASTON: Deny, degrade and disrupt - which is harder than it sounds because ISIS used encryption and remote servers. It was global, and its followers understood social media and how to abuse it. They knew how to set up dummy accounts, and they had pretty good operational security. So from a technology standpoint, it was hard enough. But to be successful, they needed to exploit a psychological component, as well. NEIL: This cyber environment involves people. It involves their habits, the way that they operate, the way that they name their accounts. . . TEMPLE-RASTON: Neil, again. NEIL: . . . When they come in during the day, when they leave, what types of apps they have on their phone. Do they click everything that comes into their inbox, or are they very tight and, you know, restrictive in what they use? All of those pieces are what we look at, not just the code. TEMPLE-RASTON: What we look at, he said. Neil has asked us to only use his first name to protect his identity because the Task Force ARES' mission against ISIS, something they called Operation Glowing Symphony, it was largely his idea. NEIL: I was the lead planner as well as the mission commander. I was the one that said go. TEMPLE-RASTON: Coming up, we go behind the scenes of Operation Glowing Symphony. This is I'LL BE SEEING YOU, a four-part series about the technologies that watch us, from NPR. I'm Dina Temple-Raston. Stay with us. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: This is I'LL BE SEEING YOU, a four-part series about the technologies that watch us, from NPR. I'm Dina Temple-Raston. On the show today, a classified offensive cyber mission called Operation Glowing Symphony. This is the first time details of this mission have been revealed publicly by the people who lived it. NEIL: We'll just go with first names, so you can just call me Neil. TEMPLE-RASTON: What's your call sign? NEIL: My call sign is Shadow Recon. That's the hacker handle that I use. TEMPLE-RASTON: Neil is a Marine reservist in his 30s. And we're only using his first name because he wasn't just the one who said go to start Operation Glowing Symphony, it was his idea. NEIL: So we were down in the basement at NSA, and we had an epiphany of how this. . . TEMPLE-RASTON: The epiphany was how ISIS distributed its propaganda. It turns out, they weren't as careful as they could have been. They did what all hackers do; they took shortcuts. They got a little lazy. Nearly all their propaganda was passing through, to use a geeky computer term, the same 10 nodes. NEIL: Every account, every IP, every domain, every. . . TEMPLE-RASTON: Think of a node as a kind of hub. You may be creating content on the laptop on your desk, but when you send it to someone else or to the outside world, you're sending it through this hub. If the node or hub is attacked, then your content can't go out. And if someone is sitting inside the node without you knowing about it, they can see and control everything. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: This was Neil's epiphany. He ran into the leadership's office, grabbed a magic marker and started drawing crazy circles and lines on a whiteboard. NEIL: Pointing everywhere and saying, it's all connected. These are the key points. Let's go. I felt like I was in \"It's Always Sunny In Philadelphia,\" when he's doing the mystery investigation for Pepe Silvia - pictures on the wall and red yarn everywhere - and nobody was understanding me. TEMPLE-RASTON: But as Neil kept explaining and drawing, this kind of bicycle tire thing started to emerge. NEIL: Bicycle tire with spokes, all of the things that were tied to that one node. And then there was another one. I said, everything's tied to these three language websites. TEMPLE-RASTON: And as what he was saying started to sink in, everything shifted. NEIL: We could take those over. We were going to win everything. It was a house of cards. TEMPLE-RASTON: And from those frantic scrawls, a plan to deny, degrade and disrupt. NEIL: We were focused on completely dismantling it in a systematic fashion so that it was in no way, shape or form recognizable from where it was that day. TEMPLE-RASTON: And while we don't understand everything they did to crack into ISIS' network, we do know that they used an old standby to start - a phishing email. CARDON: You know, they clicked on something or they did something that then allowed us to gain control and then start to move. TEMPLE-RASTON: That's General Ed Cardon again. He was Task Force ARES' first commander. In Task Force ARES, did you have like a - phishing specialty people? CARDON: We have people that are very good at mapping networks. TEMPLE-RASTON: Mapping a network is about understanding the dynamics of an organization. If you get someone to click on a phishing email, then that gives you an opportunity to explore a little bit. You can root around someone's email account to see who they talk to. Which emails get answered right away? Maybe that's a boss. Which ones just kind of sit there? Maybe that's someone they have a problem with or someone they don't respect. What do the emails say? This is the data collection you used to get with human sources that now is typically done online. CARDON: The first thing you do when you get in there is you got to get some persistence and spread out. TEMPLE-RASTON: So if you want to get into ISIS' networks, you might send them a phishing email that they can't help clicking on. And then, with a little exploring, you get yourself an administrator's account. CARDON: You can operate freely inside the network because you look like a normal IT person. TEMPLE-RASTON: And ISIS had IT people? CARDON: Oh, yes. TEMPLE-RASTON: They had a whole IT department? CARDON: Yes, they did. TEMPLE-RASTON: The spring and summer of 2016 was spent preparing for attack. It meant dropping malware on servers or looking for folders that contained things that might be helpful later, like those encryption keys we talked about before. And the deeper ARES got inside ISIS' network, the more it looked like their theory of the 10 nodes was correct. And those nodes weren't just in Syria and Iraq, they were everywhere, on servers around the world, which meant ARES had a new problem. It had to figure out how to target just the part of the server that contained ISIS material and nothing else. TIM HAUGH (MAJOR GEN, US AIR FORCE): On every server, there might be things from other commercial entities. We are only going to touch that little sliver of the adversary space and not perturb anybody else. TEMPLE-RASTON: Air Force General Tim Haugh was the first deputy commander of JTF ARES. He worked with General Cardon. And that server issue was one of the problems he had to solve. If ISIS had stored something in the cloud or in a server sitting in, say, Germany, ARES had to show that it could attack the ISIS material and leave everything else on the server unscathed. And they spent months launching small missions that showed it could be done. Then one day, General Haugh turned to a young Marine and asked out loud what everybody was thinking. HAUGH: How big can we go? And he said, sir, we can do global. I said, that's it. Write it down. We're going to take it to General Cardon. So - so that officer really set the stage. TEMPLE-RASTON: And that aggressive Marine - you may have guessed - it was Neil. He began peppering the leadership with ideas. He talked to them not just about hacking one person or ISIS in Syria and Iraq but how to take down the media operation's entire global network. NEIL: That's how these attacks work. They start very simple, and they become more complex. CARDON: Actually, a lot of them come up that way. Like, we could do this. TEMPLE-RASTON: That's General Cardon again. CARDON: Somebody says, well, we could gain access and do this to the files. Really? You can do that? Oh, yeah. Would anyone notice? Well, maybe. But the chances are low. It's like, that's interesting. Put that on the list. TEMPLE-RASTON: And there's something else going on here that's important. Neil was briefing general officers directly. That was a purposeful, organizational decision that made Task Force ARES different. Instead of a top-down traditional military hierarchy, ARES was built around trust, access, the competition of ideas and a willingness to take risks. It was organized so people like Neil wouldn't get buried. In a way, that was a revolution, too. So that's how Glowing Symphony became a global offensive cyber mission. And at the time, in 2016, that was a big deal. Remember; back then, offensive cyber operations meant doing the kind of thing North Korea was doing in 2014, when it hacked into Sony Pictures, or what the Iranians did two years earlier when they fried all the hard drives at Saudi Aramco, the state oil company. So the thought of the U. S. going on the offensive in that way - for a lot of people, it was scary. General Haugh again. HAUGH: When we think about how we all operate every day in the digital environment, we don't think a lot about the architecture that's behind it. It took a lot of explanation at all levels. TEMPLE-RASTON: In other words, a lot of people don't know how the Internet works, just that it does. And Haugh had to convince members of Congress and leaders at the Defense Department that Task Force ARES wasn't going to break the Internet. So there was a team - Joint Task Force ARES. There was a plan - Operation Glowing Symphony. There were briefings to secure approvals, right up to the president. And then, finally, a go. NEIL: The day of the operation was a very long day - coordination, final rehearsals, everybody wanted to be briefed. And then they also wanted to look me in the eye and say, are you sure this is going to work? And every time, I had to say, yes, no matter what I thought. TEMPLE-RASTON: They'd organized four separate teams at workstations set up like carols. Sergeants at the keyboard, sitting next to intelligence people, sitting next to linguists. Each station had four flat screen computer monitors on adjustable arms. And the operators sat in those huge chairs you see Internet gamers settle into before a long night. The room was packed. NEIL: I felt like there were over 80 people in the room between the teams and then everybody lining the back wall that wanted to watch. TEMPLE-RASTON: Neil was standing in a small, elevated bay at the back, from which he could see all the screens in front of him. And there weren't just glowing numbers or lines of code, he could see log-in screens - actual ISIS log-in screens - each carefully preselected and put on a target list that was so long it was on a 3 foot by 7 foot piece of paper hung on the wall. HAUGH: It was almost like a very large bingo card. TEMPLE-RASTON: General Haugh said there were numbers on that bingo card that corresponded with specific targets. Number five may have been one of the editors of the media operation, and it included all the accounts and IP addresses associated with him. Number 12 might have been the group's graphic designer. As they slept, the ISIS members had no idea that, half a world away, a room full of military cyber operators were about to take over their accounts and crash them. What follows is a pretty good approximation of what our reporting says happened that first night. It's based on interviews with half a dozen people who were there. Neil ordered the teams to start the operation. NEIL: Fire. TEMPLE-RASTON: After months of looking at static webpages and picking their way through ISIS networks, the task force started logging in as the enemy. They deleted files, changed passwords - click here, a digital forensic expert would say. We're in, the operator would respond. And then. . . NEIL: There was like six minutes where nothing was really happening because the Internet was a little slow. TEMPLE-RASTON: Yes, he just said the Internet was a little slow. NEIL: And then, you know, minute seven, eight, nine, 10, it started to flow in and I - you know, my heart started beating again. TEMPLE-RASTON: They began moving through the ISIS networks they had mapped for months. It was like watching a raid team clearing a house, except it was all online. And then - an unexpected hiccup. NEIL: We get prompted a security question. TEMPLE-RASTON: A security question - you've seen them before. What's the name of the street you grew up on? What's the first name of your best friend from childhood? The room quieted down. NEIL: And we're stuck dead in our tracks. What is the name of your pet? How could the team possibly know that? We all looked to each other and we're like, what can we do? There's no way we're going to get in. This is going to stop the 20 or 30 targets after this. And one of our best analysts stands up and he goes, sir, one-two-five-seven. And we're like, what? One-two-five-seven. We're like, how do you know that? I've been looking at this guy for a year. He does it for everything. We're like, all right. Your favorite pet - one-two-five-seven. Boom. We're in. TEMPLE-RASTON: After that, momentum started to build. One team would be taking screenshots to gather intelligence for later. Another would be logging ISIS videographers out of their own accounts. Reset successful, the screen would say - folder directory deleted. The screens they were seeing at the NSA campus were the same ones someone in Syria might have been looking at in real time - that is, until someone in Syria hit refresh. Once they did that, they'd see 404 error - destination unreadable. Target five is done, someone would yell, and someone else would walk across the room and cross the number off the big target sheet on the wall. NEIL: We're crossing names off the list. We're crossing accounts off the list. We're crossing IPs off the list. TEMPLE-RASTON: Every time a number went down, they would yell one word. NEIL: Jackpot. TEMPLE-RASTON: Neil gave us an idea of how it went. NEIL: They were running back and forth, on scratch pieces of yellow paper - the number five - five, jackpot - or a username that they had taken control of - 44, jackpot. And then we'd draw the line out. And I had stacks of paper coming up on the corner of my desk - 18, three, number six - jackpot, jackpot, jackpot, jackpot. I knew in about the first 15 minutes that we were on pace to accomplish exactly what we needed to accomplish. TEMPLE-RASTON: Once they'd taken over the 10 nodes and they blocked key people out of their accounts, they just kept chewing your way through the target list. NEIL: We spent the next five or six hours just shooting fish in a barrel. We had been waiting a long time to do that, and we had seen a lot of bad things happen. And we were happy to see them go away. TEMPLE-RASTON: And while nothing was ever going to stop random ISIS fighters from grabbing laptops and setting up new networks, there were some indications of success. Active servers were down, key accounts locked, files erased. And there were other small victories. Dabiq, the popular ISIS online magazine we mentioned before - it started missing deadlines. NEIL: These magazines had been coming out at a regular basis - like, every 28 to 30 days. And after Glowing Symphony, we saw variance. I think it was 36 days, which was the longest time between Dabiqs that ever happened. TEMPLE-RASTON: Why that happened is unclear. Was it because folders had been deleted and servers were down? Or because ISIS was under pressure on the ground, losing more and more territory in Syria and Iraq? It was impossible to tell, but the delays were eating away at the magazine's following. Dabiq eventually folded. NEIL: All these delays made it so they weren't as an effective media organization. TEMPLE-RASTON: For ISIS members, there was no ambiguity about whether they'd been attacked. It was clear. They couldn't get into accounts, couldn't use servers, lost key files. And while these insider details about how ARES hacked ISIS are new, the tools they used to do so - they are exactly the kind of thing nation-state hackers typically use. It's how the Iranians hacked into the Sands Casino in 2014. It's how the Russians got into election systems in the Midwest. And now it was how Cyber Com and the NSA were fighting ISIS. They started looking for new ways to keep the mission going - for new ways to deny, degrade and disrupt the enemy. JENNIFER BUCKNER (BRIGADIER GENERAL, US ARMY): I'm Brigadier General Jennifer Buckner. I go by Jen. I'm an Army officer. TEMPLE-RASTON: Buckner was one of the people who took the reins of Task Force ARES after Glowing Symphony had started. And after that first night, the mission shifted into a second phase. To keep pressure on ISIS, Operation Glowing Symphony began to revolve around five lines of effort. In addition to media targets, ARES wanted to make it hard for ISIS to operate on the Internet more generally. They wanted to help the forces on the ground that were pushing ISIS out of Syria and Iraq and make it harder for ISIS to raise money and attract recruits. They also started partnering more. They had people from State and DHS and Treasury actually working with the task force, and they deployed members of ARES to other countries. And because the team was based at the NSA campus at Fort Meade, getting reinforcements to do those things often just required a walk down the hall. BUCKNER: There was a lot of junior talent that we contributed to this. We pulled whoever knew - we thought could do the job and knew the mission. TEMPLE-RASTON: Best geek. BUCKNER: Absolutely, best geek. TEMPLE-RASTON: And after the first night of crossing targets off that bingo card, the mission continued, though in a more creative way. Joint Task Force ARES operators started making all those things that drive you crazy about today's technology - slow downloads, dropped connections, access denied, program glitches - they made them start happening to ISIS fighters. BUCKNER: Some of these are not sophisticated effects, but they don't need to be. The idea that, yesterday, I could get into my Instagram account and today I can't is confusing. TEMPLE-RASTON: If this sounds like something you've experienced, that's exactly the point. (SOUNDBITE OF KEYBOARD KEYS CLICKING)TEMPLE-RASTON: If you can't get into an email account, what do you do? You think, maybe I mistyped the log-in or password, so you put it in again - still doesn't work. Then you type it in more deliberately. And every time you type it, press enter and are denied, you get a little more frustrated. (SOUNDBITE OF COMPUTER BEEPING)TEMPLE-RASTON: If you're at work, you call the IT department, and you explain the issue. And then they ask you if you're sure you typed in your log-in and password correctly. It's enough to drive you nuts. It would never occur to you or to me or ISIS that this might be part of a cyberattack. BUCKNER: It just looks like I messed something up or something's wrong with the platform. TEMPLE-RASTON: That's what the follow-on phases of Operation Glowing Symphony were all about; that psychological component - eroding morale. One member of ISIS would stay up all night editing a film, and then he'd ask a fellow ISIS member to upload it. But operators with Glowing Symphony would make it so it didn't quite land at its destination. The ISIS member who stayed up all night starts asking the other ISIS member why he didn't do what he'd asked him to do. He gets angry, and so on. BUCKNER: We had to understand, how did all of that work? And so what is the best way to cause confusion online? TEMPLE-RASTON: Let's drain their cellphone batteries or insert photographs into videos that aren't supposed to be there. Task Force ARES would watch, react and adjust their plans. They'd change passwords, delete content, shut people out of their accounts; all in a way that made it look like it was just run-of-the-mill IT problems. CARDON: Pinwheels of death - the network's working really slow. People get frustrated. TEMPLE-RASTON: That's General Ed Cardon again. And he says, after more than two years on the mission, the operators had developed a kind of organizational memory. CARDON: This is built through repetition - when you do a lot - is you just sort of know what that person's going to do. TEMPLE-RASTON: Operation Glowing Symphony still isn't over. Task Force ARES, now led by General Matthew Glavy, is still sitting in ISIS networks - still driving them crazy. This is I'LL BE SEEING YOU from NPR. When we come back, something we haven't mentioned - the very dark side of offensive cyber. UNIDENTIFIED PERSON #6: This is not anything that was clean or virtual, it was something that had very lethal impacts. TEMPLE-RASTON: And a rare, exclusive interview - NSA Director General Paul Nakasone. I'm Dina Temple-Raston. Stay with us. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: This is I'LL BE SEEING YOU from NPR, a four-part series about the technologies that watch us. And today's show is about offensive cyberattacks. Specifically, we're talking about a classified mission - a mission in which the U. S. military hacked one of the most dangerous and tech-savvy terrorist organizations in the world. The military cyber arm, something called U. S. Cyber Command, and the NSA put together a special unit to knock ISIS off the Internet. They called it Joint Task Force Ares, and in the fall of 2016, something called Operation Glowing Symphony began. And it's still going on today. MATTHEW GLAVY (US GENERAL, TASK FORCE ARES COMMANDER): ISIS came in using the information domain, using Samsung phones with some HD graphics, to use information warfare in a way that's never been used before. TEMPLE-RASTON: That's General Matthew Glavy. He's the current commander of Task Force Ares. GLAVY: Atrocious, heinous, dark side - but nonetheless, they used information warfare probably better than anybody else ever has. TEMPLE-RASTON: Glavy said the five lines of effort on which Ares focused three years ago are still there. Ares still has its thumb on ISIS' media operations. The group is still having a lot of trouble operating freely on the web. Ground forces have been successful in driving them out of most of Syria and Iraq, but there are still pockets where ISIS has a presence. And they have moved to other countries like Libya, and Mali, and even the Philippines. Ares is still sitting on ISIS to prevent it from raising money and attracting recruits. Those original so-called lines of effort still apply, but they've evolved. GLAVY: They've morphed a little bit. But let's face it, we got to be ever so diligent and vigilant about the media piece. And we cannot have - for them to gain the momentum that we saw in the past. So we - we've - we have to learn that lesson. TEMPLE-RASTON: Remember that Hellfire missile that came roaring out of the Syrian sky to kill the ISIS hacker? His real name was Junaid Hussain, and he was just 21 years old when he died in Syria - a hacker deemed so dangerous the American military decided to kill him. Lorraine Murphy is a digital journalist who writes about technology and hacktivism, and she knew Hussain when he was just a teenager with a computer and a cause. She says ISIS must have seen him coming. LORRAINE MURPHY (DIGITAL JOURNALIST): He emerged at the right time. He was in the right place. He had all of the ingredients. He had connections. He had a significant social media following. TEMPLE-RASTON: By August 2014, he had traveled to Syria and began to call himself Abu Hussain al-Britani, and he joined their Cyber Caliphate. MURPHY: He was fluent in all of the things that fundamentalists tend not to be fluent in, like technology, like social media. He could be charming. He could be a gifted propagandist so I mean, they couldn't have invented a more perfect head of hacking for ISIS. TEMPLE-RASTON: You can sit at home and play \"Call Of Duty,\" one of his most famous tweets from Syria read, or you can come here and respond to the real call of duty. The choice is yours. He also rather famously hacked into U. S. Central Command's Twitter account and released a list of U. S. military personnel with names and addresses, and then called on ISIS members to kill them. It's not surprising that Cyber Command demanded a response. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #5 (REPORTER): U. S. spy drones followed and tracked notorious British-born ISIS hacker Junaid Hussain for days in the middle of heavily. . . ROSENBACH: And I know to some NPR listeners this will sound like a bitter pill. TEMPLE-RASTON: Again, former Defense Department official Eric Rosenbach. ROSENBACH: If there are a very small number of individuals in a country who know how to build a nuclear weapon and you try to think about ways to prevent those people from accomplishing their mission, that can be very effective - same thing in cyberspace. When you think about Junaid Hussain or others, we thought about ways to neuter his cybercapability to prevent them from getting online. It is really important to remember this is a war. TEMPLE-RASTON: Officials familiar with his case said that he double-clicked on a phishing email from Cybercom that allowed them to track his phone, to follow him, and eventually kill him at that gas station. ROSENBACH: For cyber problems, you can't just use cyber tools. When it's in a warzone, using kinetic physical force to address that issue - which is eventually what it came down to in that case. TEMPLE-RASTON: Even if you think Hussain's work with ISIS made him a legitimate target for attack, the problem is that this kind of thing doesn't stop there. The U. S. isn't the only country using offensive cyber this way. Remember the much more alarming case of the Washington Post journalist Jamal Khashoggi? (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #6 (REPORTER): Turkish officials have audio and video recordings of the gruesome murder of journalist Jamal Khashoggi inside the Saudi Arabian. . . UNIDENTIFIED REPORTER #7 (REPORTER): He went to the consulate to obtain paperwork to marry his fiancee and was never seen again. TEMPLE-RASTON: He was brutally murdered in a Saudi embassy late last year, and his body was never recovered. Offensive cyber appears to have played a key role in his death, too. Just as Cybercom followed Junaid Hussain by putting something on his phone, that seems to have happened in Khashoggi's case as well. RON DEIBERT (DIRECTOR, CITIZEN LAB): So when we - we talk about offensive cyber operations, I think it's important to understand that it doesn't always come in one flavor. TEMPLE-RASTON: Ron Deibert is the director of the Citizen Lab at the University of Toronto's Munk School of Global Affairs. The Citizen Lab does news-breaking research on digital security and human rights. Six years ago, it rather famously discovered that China had been hacking into the Dalai Lama's computer networks. And last year, it looked into another case of state-sponsored offensive cyber. DEIBERT: If you look at the murder of Jamal Khashoggi, I would say that a lot of the preparation for that and the lead up to it had to do with Saudi Arabia using offensive cyber weapons. TEMPLE-RASTON: Deibert's researchers dug into the case and they found offensive cyber tools tracking the journalist and his inner circle. Citizen Lab says it's figured out a way to detect if a phone has been targeted by programs that can infiltrate encrypted phones and apps. And they found just such a program in this case. A Saudi dissident who is an associate of Khashoggi's filed a lawsuit that says he found tracking spyware on his phone. He said it allowed the Saudis to secretly listen to his calls, read his messages and track his Internet history. Allegedly, it could also turn on the phone's microphone and camera. And the Saudi case isn't an outlier. In Mexico alone, Citizen Lab found 27 cases of this kind of offensive cybertool targeting political rivals, reporters and civil rights lawyers. DEIBERT: I think there's a control problem here. TEMPLE-RASTON: That's Ron Deibert again. DEIBERT: You really create conditions for an escalation of an arms race in cyberspace that really could come back to haunt the United States in the long run. TEMPLE-RASTON: Deibert says even if the United States is being careful in its use of offensive cyber, the mere fact that America is using it gives license to others to do the same. DEIBERT: There is a demonstration effect, and the equipment, the software, the methods, the capabilities proliferate. TEMPLE-RASTON: Large cyber operations like Glowing Symphony worry Deibert, too. He says offensive cyber is blurring the lines between military and civilian targets. Remember those servers with ISIS material outside of Syria and Iraq? Well, they had civilian material on them as well. UNIDENTIFIED PERSON #7: We're effectively talking about military operations that are in our common communications infrastructure. These type of operations are - are effectively maneuvering through what is essentially a public sphere on a global level. TEMPLE-RASTON: It's like using a missile in a regular city. UNIDENTIFIED PERSON #7: Exactly. TEMPLE-RASTON: Last August, President Trump signed an order that allows the secretary of Defense to conduct cyber operations more freely. Now they can launch an attack without necessarily needing presidential approval. It was meant to replace an Obama-era order known as Presidential Policy Directive 20, which set out a strict framework to keep cyber operations in check. The new directive, which has not been publicly released, is supposed to remove bureaucratic obstacles that the Defense Department thought were preventing them from fighting off cyberthreats fast enough. The subtle uses of cyber, hacks like phase two of Operation Glowing Symphony, raise other questions. What if, instead of the pinwheel of death or inserting photographs, enemies hacked into health records and changed just one thing, like, say, blood type? Imagine the damage that could cause. DEIBERT: I think there's always been a recognition of the value of offensive capabilities in cyberspace from a U. S. perspective. TEMPLE-RASTON: Ron Deibert again. DEIBERT: There may have been some reticence to deploy these widely, perhaps for legal reasons or the precedent that they set for other countries and for potential arms race in cyberspace, but those concerns seem to have lessened. And now with discussions of persistent engagement, what we're talking about is something that is more active. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Persistent engagement and defend forward - these terms come from NSA director and Cybercom Commander General Paul Nakasone, and he talked about them when he gave NPR a rare exclusive interview a few months ago at the NSA campus at Fort Meade. PAUL NAKASONE (COMMANDER GENERAL, CYBERCOM): Defend forward is part of the DOD cyber strategy that talks about acting outside our borders to ensure that we maintain contact with our adversaries in cyberspace. TEMPLE-RASTON: In other words, you don't want to wait to be attacked in cyberspace. You do things that allow you to hack back if there's an attack in the future, and that could be deploying a small team in another country that asks for help. NAKASONE: That can be hunting on a network to look for malware, or it could be, as we did in Operation Glowing Symphony, the idea of being able to impact infrastructure worldwide. TEMPLE-RASTON: Those targeted attacks that let them take down ISIS material on a server without affecting anyone else - the U. S. used to focus on defending its networks. Now it seems to be leaning more on offensive capabilities. You can draw a straight line from Task Force ARES to a new unit the NSA and Cyber Command have just started discussing publicly - something called the Russia Small Group. Just as ARES focused on ISIS, the Russia Small Group is organizing around countering Russian cyberattacks. We've known for some time that Russia has been trying to plant malware in key infrastructure targets in the U. S. The intelligence community has made clear that Russia used social media to meddle in the 2016 elections, and they expect that to continue in the 2020 elections as well. General Nakasone talked about this during our interview at the NSA. But first, there were some funny things about that interview. Because the National Security Agency is one of the most secretive organizations in the world, all the sound you hear from inside the NSA - it is from inside the building but they wouldn't let us record it for ourselves. They recorded it and then sent it to us in a file called Unclassified Sounds of The NSA. Think of it as a kind of NSA greatest hits. We heard all these things when we were there, of course, but we couldn't get them on tape. We were permitted to record General Nakasone, however. We sat down at a teak conference table that seats several dozen, and there's a kind of backbencher row of seats like in a movie theater. NAKASONE: A little bit of a big room but I thought it would be easier than probably doing it in my office, so. . . TEMPLE-RASTON: Army General Paul Nakasone has two jobs. He's the director of the nation's largest spy agency, the NSA. And he also leads U. S. Cyber Command, the military's top cyber arm. Whenever you hear about American cyberattacks, the people behind them are at Cybercom. The ones you don't hear about probably came out of the NSA. Before becoming NSA director last year, Nakasone was the head of the Army's Cyber Command, and he was in charge of Joint Task Force ARES when the cyber mission against ISIS, Operation Glowing Symphony, first started. And while he wasn't convinced the mission was a success in the first 15 minutes like Neil was, he said it was clear the mission was working from very early on. NAKASONE: Within the first 60 minutes of go, I knew we were having success. TEMPLE-RASTON: And you saw things crumble. NAKASONE: We would see the targets start to come down. It's hard to describe, but you can just sense it from being in the atmosphere that the operators, they know they're doing really well. And they're not saying that, but you're there and you know it. TEMPLE-RASTON: Nakasone said that before Ares, the fight against ISIS in cyberspace was episodic. Now it's continuous. NAKASONE: We were going to make sure that any time ISIS was going to raise money or communicate - we were going to be there. TEMPLE-RASTON: And the fact that Cybercom and Task Force Ares are there has meant that ISIS has had to change the way they operate. They aren't as strong in cyberspace as they were. They're still there but not in the same way. NAKASONE: We were seeing an adversary that was able to leverage cyber to raise a tremendous amount of money, to proselytize. And we were seeing a series of videos and posts and media products that were high-end. We haven't seen that recently. TEMPLE-RASTON: And, of course, that's good. NAKASONE: And that's one of the things that we will continue to do as ISIS shows their head or shows an ability to act. We're going to be right there. TEMPLE-RASTON: Back in June, The New York Times reported that the U. S. had cracked into Russia's electrical power grid and had planted malware there, which. . . (SOUNDBITE OF MONTAGE)UNIDENTIFIED REPORTER #8 (REPORTER): Has the potential, presumably, to take them offline. UNIDENTIFIED REPORTER #9 (REPORTER): Multiple security officials confirmed the report. UNIDENTIFIED REPORTER #10 (REPORTER): Cyber Command is gaming out what would happen if Russia attacks key states just as America goes to vote in 2020. TEMPLE-RASTON: Though Nakasone wouldn't confirm The New York Times story, it isn't hard to see how that would fit into a Russia small groups operation if it's modeled on Ares. The assumption has always been that Russia is in American networks in the event of a conflict with the U. S. in the future. Nakasone suggested that the U. S. has been doing the same, not just a response to what Russia is trying to do now but what it might attempt to do later. Nakasone said the first thing he did when he became NSA director in the spring of 2018 was to review what the Russians had done in the runup to the U. S. presidential elections. He wanted Cybercom to learn from it, to reverse engineer it and see how it works. NAKASONE: What does an adversary do? How do they try to create influence? It provided us a very, very good roadmap of what they might do in the future. TEMPLE-RASTON: Nakasone said the American people shouldn't worry about the 2020 elections because Cybercom is prepared to prevent the Russians from repeating what they did in 2016. NAKASONE: I think it's important for the American public to understand that, as with any domain - air, land, sea or space - cyberspace is the same way. Our nation has a force. We are going to make sure that we're poised, trained and ready to act when authorized. TEMPLE-RASTON: Even saying that much is new. Remember - offensive cyber not so long ago was something they didn't talk about, and now, all of a sudden, they seem to be. So why is General Nakasone talking about this now? DEIBERT: What's happening here is part of a deterrent justification. TEMPLE-RASTON: That's Ron Deibert from the Citizen Lab again. DEIBERT: You can't have cyber operations meaningly (ph) deter your adversaries unless they know that you have these capabilities and they understand what you can do with them. But what's not probably being discussed or appreciated is the extent to which there is a systemic effect of the use of these operations. Other countries take notice. Other actors take notice. TEMPLE-RASTON: At the end of Stanley Kubrick's \"Dr. Strangelove,\" there was an iconic scene in which Peter Sellers talks about the doomsday bomb as the ultimate deterrent. But it only works as a deterrent if people know it exists. (SOUNDBITE OF FILM, \"DR. STRANGELOVE OR: HOW I LEARNED TO STOP WORRYING AND LOVE THE BOMB\")PETER SELLERS (ACTOR): (As Dr. Strangelove) Deterrence is the art of producing in the mind of the enemy the fear to attack. TEMPLE-RASTON: And they come to the conclusion - if you don't tell anybody about it, what good is it? (SOUNDBITE OF FILM, \"DR. STRANGELOVE OR: HOW I LEARNED TO STOP WORRYING AND LOVE THE BOMB\")SELLERS: (As Dr. Strangelove) The whole point of the doomsday machine is lost if you keep it a secret. TEMPLE-RASTON: If you keep it a secret - you could say the same thing about American offensive cyber operations. They've been so stealthy for so long, maybe people don't realize the U. S. has them. We all hear about Russia's influence campaigns. The Chinese have been stealing intellectual property. Iranian hackers have been trolling around in our infrastructure. But we rarely hear much about the American response. This may be an effort to try to change that. The irony is that offensive cyber's richest target is us. Ron Deibert again. DEIBERT: The United States is a country most highly dependent on these technologies and arguably the most vulnerable to these sorts of attacks. I think there should be far more attention devoted to thinking about proper systems and security to defense. TEMPLE-RASTON: Doing that, of course, isn't so easy, either. This is I'LL BE SEEING YOU from NPR. Next time - an old-fashioned spy story with a high-tech twist. UNIDENTIFIED PERSON #8: I just said, it doesn't take a rocket scientist to see what's going on here because a god**** Soviet spy. TEMPLE-RASTON: The show was written and hosted by me, Dina Temple-Raston. Our producer is Adelina Lancianese, and she scored the show, too. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Special thanks to Eric Mennel for his field production, NPR Investigations, the Story Lab and Josephine Wolff at Tufts University. If you missed one of our previous shows, just go to npr. org/illbeseeingyou or find us on NPR One. I'm Dina Temple-Raston, and I'LL BE SEEING YOU. (SOUNDBITE OF MUSIC)NEIL: My friend texted me, and he said, so I hear you're doing the interview. It's General Nakasone, General Haugh, General Cardon and Neil. TEMPLE-RASTON: (Laughter). NEIL: I'm a recon Marine as well, so if you get compromised, you have to buy everybody a case of beer. TEMPLE-RASTON: I think you're going to be buying a lot of beer. NEIL: Yeah. That's OK. That's OK.", "section": "I'll Be Seeing You", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-09-30-765834528": {"title": "Inside The Movement To Improve Access To High-Speed Internet In Rural Areas : NPR", "url": "https://www.npr.org/2019/09/30/765834528/inside-the-movement-to-improve-access-to-high-speed-internet-in-rural-areas", "author": "No author found", "published_date": "2019-09-30", "content": "AUDIE CORNISH, HOST: It's time now for All Tech Considered. (SOUNDBITE OF MUSIC)CORNISH: Like clean water or electricity, the Internet is now a must in most people's lives, but the federal government says more than 21 million people can't get broadband. Many of them live in rural areas. Now, if they had Internet access, it might slow the brain drain, spur innovation in farming and breathe new life into local economies. Paul Flahive of Texas Public Radio visited one rural community that's turned to an old playbook to connect the disconnected. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: Electricity - about time we got it around here. PAUL FLAHIVE, BYLINE: The same reasons advocates use to promote broadband today were used 80 years ago to power rural America. This film, funded by the Rural Electrification Administration, shows some of the same obstacles exist. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: But the power company says you can see their lines go cross-country, see them in the sky. But they don't bring the power down to the farm - say it costs too much, say a lot of things. FLAHIVE: Federal loan dollars helped create these rural electrical cooperatives to reach millions of homes with electricity. Now these same co-ops are taking on broadband in small towns across Texas. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PEOPLE: (Chanting) Let's go, bulldogs. Let's go. Let's go, bulldogs. Let's go. FLAHIVE: In downtown Bandera, a homecoming parade crosses Main Street. Students cheer and throw candy to kids lining the route. JERRY HOLLINGSWORTH: We offer a small-town flavor of life that you can't get in generic suburbs. FLAHIVE: Jerry Hollingsworth is superintendent of the school district. Bandera calls itself the cowboy capital of the world, hosting shootouts and reenactments most Saturday afternoons. A horse-crossing sign greets people on the highway entering the town from the rolling Texas Hill Country - hills filled with turkey and deer, but not a lot of people. The nearest big city is San Antonio, an hour east. Hollingsworth says some students travel 50 miles to school in a district that spans hundreds of square miles. Given its sparse population, Hollingsworth says students often don't have Internet at home. HOLLINGSWORTH: We're basically saying to an entire group of kids, an entire group of families that you're not as important as somebody who's living in a suburb where it's easily accessible. FLAHIVE: But that is changing. (SOUNDBITE OF DRILL)FLAHIVE: On a quiet country road, contractors for Bandera Electric Cooperative, like Jonathan Leyva, hang steel lashing wire from one of their poles. Eventually, it will carry high-speed Internet to the county. JONATHAN LEYVA: Give me the end of that strand, and I'll put it up there real quick. FLAHIVE: Crews like this have hung a thousand miles of fiber-optic cable. Bandera CEO Bill Hetherington plans to offer broadband to most of its 30,000 members in three years. BILL HETHERINGTON: It's not about making money. You're not doing this to make money. You're doing this to allow your communities to survive and to be here 20 years from now. FLAHIVE: Hetherington says broadband service is important to the co-op's future. Without it, businesses and people may not move here. He says the town's economic growth has hit double digits since they began to offer the high-speed Internet. Other Texas electrical co-ops are paying attention. HETHERINGTON: A lot of co-ops come to us and say - to give us the secret sauce. FLAHIVE: There's limited federal money for these projects, and it may not work everywhere. But rural co-ops already own the power lines and poles to homes and may well be the key to granting broadband access to millions of rural Americans. For NPR News, I'm Paul Flahive in Bandera, Texas. AUDIE CORNISH, HOST:  It's time now for All Tech Considered. (SOUNDBITE OF MUSIC) CORNISH: Like clean water or electricity, the Internet is now a must in most people's lives, but the federal government says more than 21 million people can't get broadband. Many of them live in rural areas. Now, if they had Internet access, it might slow the brain drain, spur innovation in farming and breathe new life into local economies. Paul Flahive of Texas Public Radio visited one rural community that's turned to an old playbook to connect the disconnected. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: Electricity - about time we got it around here. PAUL FLAHIVE, BYLINE: The same reasons advocates use to promote broadband today were used 80 years ago to power rural America. This film, funded by the Rural Electrification Administration, shows some of the same obstacles exist. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: But the power company says you can see their lines go cross-country, see them in the sky. But they don't bring the power down to the farm - say it costs too much, say a lot of things. FLAHIVE: Federal loan dollars helped create these rural electrical cooperatives to reach millions of homes with electricity. Now these same co-ops are taking on broadband in small towns across Texas. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PEOPLE: (Chanting) Let's go, bulldogs. Let's go. Let's go, bulldogs. Let's go. FLAHIVE: In downtown Bandera, a homecoming parade crosses Main Street. Students cheer and throw candy to kids lining the route. JERRY HOLLINGSWORTH: We offer a small-town flavor of life that you can't get in generic suburbs. FLAHIVE: Jerry Hollingsworth is superintendent of the school district. Bandera calls itself the cowboy capital of the world, hosting shootouts and reenactments most Saturday afternoons. A horse-crossing sign greets people on the highway entering the town from the rolling Texas Hill Country - hills filled with turkey and deer, but not a lot of people. The nearest big city is San Antonio, an hour east. Hollingsworth says some students travel 50 miles to school in a district that spans hundreds of square miles. Given its sparse population, Hollingsworth says students often don't have Internet at home. HOLLINGSWORTH: We're basically saying to an entire group of kids, an entire group of families that you're not as important as somebody who's living in a suburb where it's easily accessible. FLAHIVE: But that is changing. (SOUNDBITE OF DRILL) FLAHIVE: On a quiet country road, contractors for Bandera Electric Cooperative, like Jonathan Leyva, hang steel lashing wire from one of their poles. Eventually, it will carry high-speed Internet to the county. JONATHAN LEYVA: Give me the end of that strand, and I'll put it up there real quick. FLAHIVE: Crews like this have hung a thousand miles of fiber-optic cable. Bandera CEO Bill Hetherington plans to offer broadband to most of its 30,000 members in three years. BILL HETHERINGTON: It's not about making money. You're not doing this to make money. You're doing this to allow your communities to survive and to be here 20 years from now. FLAHIVE: Hetherington says broadband service is important to the co-op's future. Without it, businesses and people may not move here. He says the town's economic growth has hit double digits since they began to offer the high-speed Internet. Other Texas electrical co-ops are paying attention. HETHERINGTON: A lot of co-ops come to us and say - to give us the secret sauce. FLAHIVE: There's limited federal money for these projects, and it may not work everywhere. But rural co-ops already own the power lines and poles to homes and may well be the key to granting broadband access to millions of rural Americans. For NPR News, I'm Paul Flahive in Bandera, Texas.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-03-766424733": {"title": "Robots, Tech, and Big Data on Google | Hidden Brain : NPR", "url": "https://www.npr.org/2019/10/03/766424733/i-robot-our-changing-relationship-with-technology", "author": "No author found", "published_date": "2019-10-03", "content": "", "section": "Hidden Brain", "disclaimer": ""}, "2019-10-03-766861700": {"title": "Uber's New Uber Works App Connects Gig Workers To Temporary Jobs : NPR", "url": "https://www.npr.org/2019/10/03/766861700/uber-launches-an-app-to-connect-job-seekers-with-gig-work", "author": "No author found", "published_date": "2019-10-03", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-03-766507832": {"title": "Instagram Rolls Out 'Restrict' Feature to Control Bullies' Comments : NPR", "url": "https://www.npr.org/2019/10/03/766507832/instagram-now-lets-you-control-your-bullys-comments", "author": "No author found", "published_date": "2019-10-03", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-04-767445740": {"title": "New Tesla Software Draws Attention of Consumers and Safety Regulators : NPR", "url": "https://www.npr.org/2019/10/04/767445740/tesla-smart-summon-software-under-government-scrutiny-for-possible-safety-proble", "author": "No author found", "published_date": "2019-10-04", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-04-767274042": {"title": "Microsoft Says Iran-Linked Hackers Targeted U.S. Presidential Campaign : NPR", "url": "https://www.npr.org/2019/10/04/767274042/microsoft-says-iranians-tried-to-hack-u-s-presidential-campaign", "author": "No author found", "published_date": "2019-10-04", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-04-767214364": {"title": "Law Enforcement Officials Try To Warn Facebook Off Its Encryption Plans : NPR", "url": "https://www.npr.org/2019/10/04/767214364/law-enforcement-officials-try-to-warn-facebook-off-its-encryption-plans", "author": "No author found", "published_date": "2019-10-04", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-06-767636934": {"title": "You Can Now Read Books On Instagram : NPR", "url": "https://www.npr.org/2019/10/06/767636934/you-can-now-read-books-on-instagram", "author": "No author found", "published_date": "2019-10-06", "content": "LULU GARCIA-NAVARRO, HOST: More than 500 million people log on to Instagram each day. The social media app now allows you to scroll through image after image with a function called Stories, and that gave an idea to people at an institution with plenty of stories. LYNN LOBASH: Many of the people that I talked to when this first launched in 2018 were very surprised. Like, what is the library doing on Instagram? GARCIA-NAVARRO: That's Lynn Lobash, manager of reader services at the New York Public Library. She says her venerable institution has been innovating to attract readers for more than a hundred years. Now it's posted five works of literature - stories, novellas and a poem - on Instagram. LOBASH: \"Alice In Wonderland,\" \"The Yellow Wallpaper,\" \"The Raven,\" \"Metamorphosis\" and \"A Christmas Carol. \"GARCIA-NAVARRO: The Insta Novel texts are the same as what you'd read in a musty volume from the public library stacks. But thanks to design firm Mother NYC, they also offer a visual reward. Wax rolls down a candlestick on the screen as readers page through the Charles Dickens Christmas classic. The famous nevermore flashes in bold face as the verses of Edgar Allen Poe's \"The Raven\" fly by. British blogger Cara Curtis says Insta Novels validated her social media addiction. One of our staffers reads her blog post here. UNIDENTIFIED PERSON: (Reading) Even though I might not read a whole book via Insta Novels, I did feel inspired to pick up a physical book. Reading the snippet from Lewis Carroll's \"Alice In Wonderland\" reminded me of how much I enjoy getting absorbed into a book, which might have been NYPL's plan all along. GARCIA-NAVARRO: It was exactly what Lynn Lobash was hoping for. LOBASH: The thing that I like the most about this project is that it would produce this moment where you would, you know, be looking at your feed and say, oh, yeah - and sort of sink into the book and be like, I remember - I really like reading. Like, I remember this feeling. Maybe I hadn't done it in a while. And then that surprise would lead you to, you know, read something else. And so this, like, catching fire of the habit again was my favorite part of the project. GARCIA-NAVARRO: Lobash says Insta Novels have gotten 300,000 reads so far. This summer, the designers won Webby Awards for best feature and the best use of Instagram Stories. But sorry, if you find \"Ulysses\" or \"War And Peace\" a long slog, no plans yet to transform them into Insta Novels. LULU GARCIA-NAVARRO, HOST:  More than 500 million people log on to Instagram each day. The social media app now allows you to scroll through image after image with a function called Stories, and that gave an idea to people at an institution with plenty of stories. LYNN LOBASH: Many of the people that I talked to when this first launched in 2018 were very surprised. Like, what is the library doing on Instagram? GARCIA-NAVARRO: That's Lynn Lobash, manager of reader services at the New York Public Library. She says her venerable institution has been innovating to attract readers for more than a hundred years. Now it's posted five works of literature - stories, novellas and a poem - on Instagram. LOBASH: \"Alice In Wonderland,\" \"The Yellow Wallpaper,\" \"The Raven,\" \"Metamorphosis\" and \"A Christmas Carol. \" GARCIA-NAVARRO: The Insta Novel texts are the same as what you'd read in a musty volume from the public library stacks. But thanks to design firm Mother NYC, they also offer a visual reward. Wax rolls down a candlestick on the screen as readers page through the Charles Dickens Christmas classic. The famous nevermore flashes in bold face as the verses of Edgar Allen Poe's \"The Raven\" fly by. British blogger Cara Curtis says Insta Novels validated her social media addiction. One of our staffers reads her blog post here. UNIDENTIFIED PERSON: (Reading) Even though I might not read a whole book via Insta Novels, I did feel inspired to pick up a physical book. Reading the snippet from Lewis Carroll's \"Alice In Wonderland\" reminded me of how much I enjoy getting absorbed into a book, which might have been NYPL's plan all along. GARCIA-NAVARRO: It was exactly what Lynn Lobash was hoping for. LOBASH: The thing that I like the most about this project is that it would produce this moment where you would, you know, be looking at your feed and say, oh, yeah - and sort of sink into the book and be like, I remember - I really like reading. Like, I remember this feeling. Maybe I hadn't done it in a while. And then that surprise would lead you to, you know, read something else. And so this, like, catching fire of the habit again was my favorite part of the project. GARCIA-NAVARRO: Lobash says Insta Novels have gotten 300,000 reads so far. This summer, the designers won Webby Awards for best feature and the best use of Instagram Stories. But sorry, if you find \"Ulysses\" or \"War And Peace\" a long slog, no plans yet to transform them into Insta Novels.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-07-768032896": {"title": "Lessons Learned From The Microsoft Anti-Trust Case That Began In The 1990s : NPR", "url": "https://www.npr.org/2019/10/07/768032896/lessons-learned-from-the-microsoft-anti-trust-case-that-began-in-the-1990s", "author": "No author found", "published_date": "2019-10-07", "content": "MARY LOUISE KELLY, HOST: This month in All Tech Considered, why everyone wants to break up big tech. From federal regulators to Congress to state attorneys general, everyone seems to be trying to figure out whether major tech companies have gotten too big, too powerful and maybe broken antitrust laws. For clues about what Google and Facebook and Amazon and Apple might face, we're going to take a look back at one of the biggest antitrust cases of the past few decades. (SOUNDBITE OF ARCHIVED NPR BROADCAST)UNIDENTIFIED PERSON: After weeks of mounting tension, today the Justice Department and a coalition of 20 states filed a pair of broad antitrust lawsuits against the world's leading software company Microsoft. KELLY: Now, that is from our show on May 18, 1998. The case had actually started to take shape almost a decade earlier, and the end of it wouldn't come until years later. Well, here to explain the lessons that today's big tech giants can take from the Microsoft case is reporter Adi Robertson of The Verge. Adi Robertson, welcome. ADI ROBERTSON: Hi. KELLY: Let's start with that moment - 1998. Walk me through the landscape of what was going on in terms of Microsoft and federal regulators starting to think very hard about reining it in then. ROBERTSON: Microsoft had been skirmishing with the Department of Justice and the Federal Trade Commission for almost a decade at that point over, how did it achieve massive dominance in operating systems? And then in 1995, Bill Gates realized that he had missed the Internet. KELLY: What does that mean, he had missed the Internet? ROBERTSON: Microsoft had focused so much on making Windows important that it had missed that the Internet could make operating systems almost obsolete. And there was this one company making this one browser, Netscape Navigator, that had achieved a really strong presence already. And Microsoft realized that they needed to stop them. KELLY: Right around this time, Bill Gates ended up testifying before Congress. (SOUNDBITE OF ARCHIVED RECORDING)BILL GATES: The products that Microsoft makes have a very short lifetime in terms of their attractiveness to customers. In the span of the term of a senator, we create a product; it becomes a very popular product; and then that product has no demand whatsoever. KELLY: Adi Robertson, if - when you go back and listen through to all of that testimony, the thrust of it is Bill Gates mounting a vigorous defense of Microsoft, saying it's not a monopoly. Was that a - an argument that regulators and state attorneys general bought? ROBERTSON: Ultimately, no. Ultimately, they decided that Microsoft had been incredibly aggressive at trying to drive Netscape out of business, which they ultimately did. They would pressure companies not to work with them, and they were also giving Internet Explorer away for free. So they were just massively undercutting this product that was partially paid by using their incredibly popular operating system and integrating this browser right into it. KELLY: What do you see as lessons learned, as similarities between how the Microsoft case unfolded and what companies Apple, Amazon, Google, Facebook are facing today? ROBERTSON: Maybe the clearest point of comparison is between Microsoft and Google. You have a company that controls this whole stack of technology. It runs an incredibly popular operating system. It runs incredibly popular apps on top of that operating system, and it uses those things to push other companies out, arguably. One of the lessons is that this genuinely is an issue, even if it is sort of giving these things away for free, even if it is saying, oh, well, you can uninstall this; you can go and take your own Chrome alternative and install it. The law has said that these things have an effect, that these things can still be damaging. KELLY: I suppose - worth pointing out that Microsoft is still around, is actually doing pretty well. Is there a lesson there for these companies facing potential litigation that, you know, they might come out of it and be doing just fine a decade, two decades from now? ROBERTSON: Yeah, I think the big argument around Microsoft was that the point was not to kill the company, that the point was to make the company sort of more circumspect about shutting out competition. So when a company like Google shows up and starts working in the Web, Microsoft realizes that it can operate and it can work without mercilessly destroying them. KELLY: That is reporter Adi Robertson of The Verge. Thanks very much. ROBERTSON: Thank you. MARY LOUISE KELLY, HOST:  This month in All Tech Considered, why everyone wants to break up big tech. From federal regulators to Congress to state attorneys general, everyone seems to be trying to figure out whether major tech companies have gotten too big, too powerful and maybe broken antitrust laws. For clues about what Google and Facebook and Amazon and Apple might face, we're going to take a look back at one of the biggest antitrust cases of the past few decades. (SOUNDBITE OF ARCHIVED NPR BROADCAST) UNIDENTIFIED PERSON: After weeks of mounting tension, today the Justice Department and a coalition of 20 states filed a pair of broad antitrust lawsuits against the world's leading software company Microsoft. KELLY: Now, that is from our show on May 18, 1998. The case had actually started to take shape almost a decade earlier, and the end of it wouldn't come until years later. Well, here to explain the lessons that today's big tech giants can take from the Microsoft case is reporter Adi Robertson of The Verge. Adi Robertson, welcome. ADI ROBERTSON: Hi. KELLY: Let's start with that moment - 1998. Walk me through the landscape of what was going on in terms of Microsoft and federal regulators starting to think very hard about reining it in then. ROBERTSON: Microsoft had been skirmishing with the Department of Justice and the Federal Trade Commission for almost a decade at that point over, how did it achieve massive dominance in operating systems? And then in 1995, Bill Gates realized that he had missed the Internet. KELLY: What does that mean, he had missed the Internet? ROBERTSON: Microsoft had focused so much on making Windows important that it had missed that the Internet could make operating systems almost obsolete. And there was this one company making this one browser, Netscape Navigator, that had achieved a really strong presence already. And Microsoft realized that they needed to stop them. KELLY: Right around this time, Bill Gates ended up testifying before Congress. (SOUNDBITE OF ARCHIVED RECORDING) BILL GATES: The products that Microsoft makes have a very short lifetime in terms of their attractiveness to customers. In the span of the term of a senator, we create a product; it becomes a very popular product; and then that product has no demand whatsoever. KELLY: Adi Robertson, if - when you go back and listen through to all of that testimony, the thrust of it is Bill Gates mounting a vigorous defense of Microsoft, saying it's not a monopoly. Was that a - an argument that regulators and state attorneys general bought? ROBERTSON: Ultimately, no. Ultimately, they decided that Microsoft had been incredibly aggressive at trying to drive Netscape out of business, which they ultimately did. They would pressure companies not to work with them, and they were also giving Internet Explorer away for free. So they were just massively undercutting this product that was partially paid by using their incredibly popular operating system and integrating this browser right into it. KELLY: What do you see as lessons learned, as similarities between how the Microsoft case unfolded and what companies Apple, Amazon, Google, Facebook are facing today? ROBERTSON: Maybe the clearest point of comparison is between Microsoft and Google. You have a company that controls this whole stack of technology. It runs an incredibly popular operating system. It runs incredibly popular apps on top of that operating system, and it uses those things to push other companies out, arguably. One of the lessons is that this genuinely is an issue, even if it is sort of giving these things away for free, even if it is saying, oh, well, you can uninstall this; you can go and take your own Chrome alternative and install it. The law has said that these things have an effect, that these things can still be damaging. KELLY: I suppose - worth pointing out that Microsoft is still around, is actually doing pretty well. Is there a lesson there for these companies facing potential litigation that, you know, they might come out of it and be doing just fine a decade, two decades from now? ROBERTSON: Yeah, I think the big argument around Microsoft was that the point was not to kill the company, that the point was to make the company sort of more circumspect about shutting out competition. So when a company like Google shows up and starts working in the Web, Microsoft realizes that it can operate and it can work without mercilessly destroying them. KELLY: That is reporter Adi Robertson of The Verge. Thanks very much. ROBERTSON: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-08-767116408": {"title": "VR At Work: Employers Embrace Virtual Reality For Workplace Training : NPR", "url": "https://www.npr.org/2019/10/08/767116408/virtual-reality-goes-to-work-helping-train-employees", "author": "No author found", "published_date": "2019-10-08", "content": "DAVID GREENE, HOST:  On-the-job training in virtual reality - for millions of workers, it is increasingly becoming an important learning tool. In the VR world, cashiers are taught to show greater empathy, mechanics learn to repair planes and employees are shown how to deal with an active shooter. This year, companies are really embracing this high-tech trend, as NPR's Yuki Noguchi reports. YUKI NOGUCHI, BYLINE: Derek Belch is the CEO and founder of Silicon Valley startup STRIVR, one of several companies using virtual reality for workplace training. He hands me a goggle-like headset that encircles my eyes and ears. But I feel like I should just try it. DEREK BELCH: You want to do a demo to start? NOGUCHI: Yeah. BELCH: Yeah. Let's do it. NOGUCHI: Inside this alternate world, I'm a Verizon store manager. I walk through a parking lot and then unlock the store, unaware of a masked robber right behind me. (SOUNDBITE OF VR SIMULATION)UNIDENTIFIED ACTOR #1: (As Robber #1) All right. Stop. Don't move. NOGUCHI: As I react, the technology gauges my reaction - how I move, where I look. (SOUNDBITE OF VR SIMULATION)UNIDENTIFIED ACTOR #1: (As Robber #1) Come on. Let's go. UNIDENTIFIED ACTOR #2: (As Robber #2) Come on. UNIDENTIFIED ACTOR #3: (As Robber #3) Give us the inventory. NOGUCHI: I'm flustered and make a mistake. I turn to face the gunman. (SOUNDBITE OF VR SIMULATION)UNIDENTIFIED ACTOR #2: (As Robber #2) Move. UNIDENTIFIED ACTOR #4: (As employee, wailing). UNIDENTIFIED ACTOR #1: (As Robber #1) What are you doing? Turn around. NOGUCHI: An employee wails in front of me. I feel paralyzed, like I probably would be in real life. This is the teachable moment employers want. A screen pops up prompting me to make choices - unlock the safe, hand over merchandise. It's learning in real time under duress. NOGUCHI: All right. So what do I do? BELCH: All right. So he just pointed the gun in your face. I can see you're a little freaked out right now. NOGUCHI: Yeah. BELCH: I can see the goose bumps on your arm. NOGUCHI: Right. I feel like I'm sweating on my palms. BELCH: Yeah, yeah, yeah, yeah. NOGUCHI: The stress feels real. Lou Tedrick says that's the point. Tedrick heads Verizon's learning and development. She says employees who survived robberies say the video is true to life. LOU TEDRICK: The emotions that they felt during the robbery experience, they feel during the VR experience. NOGUCHI: That realism, she says, makes the employee better prepared and more likely to remember how to react and not escalate an already dangerous situation. TEDRICK: By the end of the experience, they feel like they've been robbed three times. And by the third time, their confidence is significantly higher. NOGUCHI: This is the power and potential of virtual reality. While consumers haven't embraced it, employers from Walmart to airlines, food processors and professional sports teams are using it. It's now used to teach cashiers to treat customers with greater empathy. Warehouse workers learn how to stack pallets safely. Derek Belch of STRIVR says the basic premise is that people learn best through experience. BELCH: As you saw, like, your heart rate went up. Right? You were like - oh, my God - what do I do? It's all about emotional preparedness. NOGUCHI: Belch says this could be applied in many different workplace situations, including harassment or unconscious bias training. The benefit of the technology isn't just practice using a realistic substitute; it's also a far cheaper and easier way to train people across a big organization. This year, Walmart plans to train over a million of its workers on VR across 4,700 of its locations. The retailer's head of learning is a man appropriately named Andy Trainor. ANDY TRAINOR: Must've been destiny. NOGUCHI: For example, Walmart recently rolled out new machines used for consumers picking up online orders. TRAINOR: Previously, we had to send three to four people to the store and to train how to set it up, how to maintain it, how to interact with customers with it. NOGUCHI: That took days and thousands of man-hours. TRAINOR: Now we just send a pair of VR goggles. We have VR modules that teach them how to do all of that stuff without any human intervention. NOGUCHI: It isn't just physical skills like mechanics and reaction time. Walmart and others are increasingly using VR to train employees in soft skills, things like handling customer disputes or creating a more inclusive workplace. TRAINOR: With all the data you get from VR, you can see where they look; you can see how they move and how they react. You could do an interview in VR and, based on the way they answer the questions, you can preselect whether or not they'd be a good fit for that role or not. NOGUCHI: But that is something Walmart is still testing. Using VR to match candidates with specific jobs is still the new frontier. Current software isn't all that good at responding to subtle things, like how an employee rolls his eyes or raises his voice. So it's possible virtual reality could someday be programmed to understand the complexities of human interaction and help us make better decisions. But right now there isn't enough research to say where those limits lie. Yuki Noguchi, NPR News, Menlo Park, Calif. (SOUNDBITE OF HANDBOOK'S \"UNKNOWN DESIRE\") DAVID GREENE, HOST:   On-the-job training in virtual reality - for millions of workers, it is increasingly becoming an important learning tool. In the VR world, cashiers are taught to show greater empathy, mechanics learn to repair planes and employees are shown how to deal with an active shooter. This year, companies are really embracing this high-tech trend, as NPR's Yuki Noguchi reports. YUKI NOGUCHI, BYLINE: Derek Belch is the CEO and founder of Silicon Valley startup STRIVR, one of several companies using virtual reality for workplace training. He hands me a goggle-like headset that encircles my eyes and ears. But I feel like I should just try it. DEREK BELCH: You want to do a demo to start? NOGUCHI: Yeah. BELCH: Yeah. Let's do it. NOGUCHI: Inside this alternate world, I'm a Verizon store manager. I walk through a parking lot and then unlock the store, unaware of a masked robber right behind me. (SOUNDBITE OF VR SIMULATION) UNIDENTIFIED ACTOR #1: (As Robber #1) All right. Stop. Don't move. NOGUCHI: As I react, the technology gauges my reaction - how I move, where I look. (SOUNDBITE OF VR SIMULATION) UNIDENTIFIED ACTOR #1: (As Robber #1) Come on. Let's go. UNIDENTIFIED ACTOR #2: (As Robber #2) Come on. UNIDENTIFIED ACTOR #3: (As Robber #3) Give us the inventory. NOGUCHI: I'm flustered and make a mistake. I turn to face the gunman. (SOUNDBITE OF VR SIMULATION) UNIDENTIFIED ACTOR #2: (As Robber #2) Move. UNIDENTIFIED ACTOR #4: (As employee, wailing). UNIDENTIFIED ACTOR #1: (As Robber #1) What are you doing? Turn around. NOGUCHI: An employee wails in front of me. I feel paralyzed, like I probably would be in real life. This is the teachable moment employers want. A screen pops up prompting me to make choices - unlock the safe, hand over merchandise. It's learning in real time under duress. NOGUCHI: All right. So what do I do? BELCH: All right. So he just pointed the gun in your face. I can see you're a little freaked out right now. NOGUCHI: Yeah. BELCH: I can see the goose bumps on your arm. NOGUCHI: Right. I feel like I'm sweating on my palms. BELCH: Yeah, yeah, yeah, yeah. NOGUCHI: The stress feels real. Lou Tedrick says that's the point. Tedrick heads Verizon's learning and development. She says employees who survived robberies say the video is true to life. LOU TEDRICK: The emotions that they felt during the robbery experience, they feel during the VR experience. NOGUCHI: That realism, she says, makes the employee better prepared and more likely to remember how to react and not escalate an already dangerous situation. TEDRICK: By the end of the experience, they feel like they've been robbed three times. And by the third time, their confidence is significantly higher. NOGUCHI: This is the power and potential of virtual reality. While consumers haven't embraced it, employers from Walmart to airlines, food processors and professional sports teams are using it. It's now used to teach cashiers to treat customers with greater empathy. Warehouse workers learn how to stack pallets safely. Derek Belch of STRIVR says the basic premise is that people learn best through experience. BELCH: As you saw, like, your heart rate went up. Right? You were like - oh, my God - what do I do? It's all about emotional preparedness. NOGUCHI: Belch says this could be applied in many different workplace situations, including harassment or unconscious bias training. The benefit of the technology isn't just practice using a realistic substitute; it's also a far cheaper and easier way to train people across a big organization. This year, Walmart plans to train over a million of its workers on VR across 4,700 of its locations. The retailer's head of learning is a man appropriately named Andy Trainor. ANDY TRAINOR: Must've been destiny. NOGUCHI: For example, Walmart recently rolled out new machines used for consumers picking up online orders. TRAINOR: Previously, we had to send three to four people to the store and to train how to set it up, how to maintain it, how to interact with customers with it. NOGUCHI: That took days and thousands of man-hours. TRAINOR: Now we just send a pair of VR goggles. We have VR modules that teach them how to do all of that stuff without any human intervention. NOGUCHI: It isn't just physical skills like mechanics and reaction time. Walmart and others are increasingly using VR to train employees in soft skills, things like handling customer disputes or creating a more inclusive workplace. TRAINOR: With all the data you get from VR, you can see where they look; you can see how they move and how they react. You could do an interview in VR and, based on the way they answer the questions, you can preselect whether or not they'd be a good fit for that role or not. NOGUCHI: But that is something Walmart is still testing. Using VR to match candidates with specific jobs is still the new frontier. Current software isn't all that good at responding to subtle things, like how an employee rolls his eyes or raises his voice. So it's possible virtual reality could someday be programmed to understand the complexities of human interaction and help us make better decisions. But right now there isn't enough research to say where those limits lie. Yuki Noguchi, NPR News, Menlo Park, Calif. (SOUNDBITE OF HANDBOOK'S \"UNKNOWN DESIRE\")", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-08-768150426": {"title": "U.S. Blacklists Chinese Tech Firms Over Treatment Of Uighurs : NPR", "url": "https://www.npr.org/2019/10/08/768150426/u-s-blacklists-chinese-tech-firms-over-treatment-of-uighurs", "author": "No author found", "published_date": "2019-10-08", "content": "", "section": "Asia", "disclaimer": ""}, "2019-10-08-767293251": {"title": "Book Review: In 'Mindf*ck,' Cambridge Analytica Whistleblower Stops Short Of A Full Mea Culpa  : NPR", "url": "https://www.npr.org/2019/10/08/767293251/in-new-book-cambridge-analytica-whistleblower-stops-short-of-a-full-mea-culpa", "author": "No author found", "published_date": "2019-10-08", "content": "", "section": "Book Reviews", "disclaimer": ""}, "2019-10-10-768841864": {"title": "After China Objects, Apple Removes App Used By Hong Kong Protesters : NPR", "url": "https://www.npr.org/2019/10/10/768841864/after-china-objects-apple-removes-app-used-by-hong-kong-protesters", "author": "No author found", "published_date": "2019-10-10", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-10-768646968": {"title": "Twitter Analysis Shows How Trump Tweets Differently About Nonwhite Lawmakers : NPR", "url": "https://www.npr.org/2019/10/10/768646968/as-summer-heated-up-trumps-tweets-about-non-white-democrats-intensified", "author": "No author found", "published_date": "2019-10-10", "content": "STEVE INSKEEP, HOST: When President Trump holds a campaign rally today in Minneapolis, Minn. , he will be inside the congressional district of a lawmaker he often criticizes. It is the district of Democrat Ilhan Omar. NPR's Ayesha Rascoe has been looking at the president's Twitter feed, where he has criticized Omar, among many other people. She found the president's attacks on lawmakers are growing more personal, especially against people of color. AYESHA RASCOE, BYLINE: President Trump's tweets can set the wheels of government in motion and set the tone for his supporters. Back in July, Trump was eagerly attacking Ilhan Omar at a rally in Greenville, N. C. (SOUNDBITE OF ARCHIVED RECORDING)PRESIDENT DONALD TRUMP: Omar has a history of launching vicious anti-Semitic screeds. RASCOE: In response, the crowd chanted, send her back. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED TRUMP SUPPORTERS: (Chanting) Send her back. Send her back. . . RASCOE: Omar was born in Somalia but is an American citizen. The chants were prompted by Trump's tweets a few days earlier. Using racist language, Trump tweeted that Omar and three other Democrats - all women of color - known as the squad should go back to the, quote, \"totally broken and crime-infested places from which they came. \"Trump has always used Twitter as a weapon against his political opponents. But these tweets and the dozens that would come in the weeks following against the squad and against African American Congressman Elijah Cummings represented an escalation of the president's attacks. (SOUNDBITE OF MONTAGE)TRUMP: These are people that, in my opinion, hate our country. And AOC plus 3, that's what I call it. Elijah Cummings has not helped the people. If you're not happy here, then you can leave. RASCOE: NPR examined Trump's tweets about members of Congress through October 4. Since becoming president, Trump has tweeted more than 700 times about current lawmakers. While Trump hasn't been restrained about insults, his negative tweets have increased in volume. And the nature of insults have become more extreme since July. In the case of the squad, which includes Omar, Ayanna Pressley, Alexandria Ocasio-Cortez and Rashida Tlaib, Trump has tweeted about them collectively nearly 20 times in about three months. That's more negative tweets than Trump has aimed at presidential candidates Elizabeth Warren or Bernie Sanders. Among other things, he's called the squad a very racist group of troublemakers. That language speaks to some white Trump supporters who may be worried about their place in America, says Niambi Carter. She's a political science professor at Howard University. NIAMBI CARTER: When he says make America great again and points to these women as the sort of embodiment of everything that America isn't - it's not black, it's not a woman, it's not Muslim, it's not Latina - these women become perfect vehicles for all of that anxiety that these white voters feel. RASCOE: The only lawmakers with more negative tweets than the squad and Cummings are top Democrats - House Speaker Nancy Pelosi, Senate Minority Leader Chuck Schumer and House Intelligence Committee Chairman Adam Schiff. But his tweets about them are more general criticisms about the Democratic Party. BRIAN OTT: The president chooses his targets pretty carefully, but he doesn't always choose them for the same reason. RASCOE: That's Brian Ott, a communications professor at Texas Tech University and co-author of \"The Twitter Presidency. \" He says Trump uses Twitter to discredit politicians he views as a threat and to distract from other matters. OTT: Then there's this other motivation, which is trying to define the opposition. And it's clear that he takes tremendous pleasure and is particularly vitriolic when it comes to characterizing women and ethnic minorities on Twitter. RASCOE: On Twitter, Trump says the face of the Democratic Party is the squad, Pelosi and Congresswoman Maxine Waters, who is African American. He never says white men are the face of the Democratic Party. The White House didn't respond to interview requests, but Trump's supporters argue he's a counterpuncher. Paris Dennard is a prominent African American conservative who backs Trump. He said it's not race or gender that motivates the president. PARIS DENNARD: If you are a worthy opponent - meaning you have a large social media following, meaning you can attract a lot of people to whatever it is you're talking about - he is going to engage with them because he thinks it's worth the fight. RASCOE: Trump has targeted both Cummings, who is chair of the House Oversight Committee, and Schiff with a flood of tweets. But while the tweets against Schiff have focused on his actions as chairman, Trump's tweets against Cummings focus on the mostly black district he represents in Baltimore. Ott says the rhetoric about Cummings in particular is meant to set him apart as different. OTT: The president's discussion of Baltimore as rat-infested is really meant to convey that he's not quite human. RASCOE: With an impeachment probe in full swing, the president has tweeted about the squad less. Cummings hasn't gotten a mention in weeks. But Trump hasn't forgotten about these lawmakers. He told reporters on Friday that AOC plus three, as he likes to call the squad, are the real leaders of the Democratic Party. Ayesha Rascoe, NPR News, Washington. (SOUNDBITE OF THRUPENCE'S \"FOREST ON THE SUN\") STEVE INSKEEP, HOST:  When President Trump holds a campaign rally today in Minneapolis, Minn. , he will be inside the congressional district of a lawmaker he often criticizes. It is the district of Democrat Ilhan Omar. NPR's Ayesha Rascoe has been looking at the president's Twitter feed, where he has criticized Omar, among many other people. She found the president's attacks on lawmakers are growing more personal, especially against people of color. AYESHA RASCOE, BYLINE: President Trump's tweets can set the wheels of government in motion and set the tone for his supporters. Back in July, Trump was eagerly attacking Ilhan Omar at a rally in Greenville, N. C. (SOUNDBITE OF ARCHIVED RECORDING) PRESIDENT DONALD TRUMP: Omar has a history of launching vicious anti-Semitic screeds. RASCOE: In response, the crowd chanted, send her back. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED TRUMP SUPPORTERS: (Chanting) Send her back. Send her back. . . RASCOE: Omar was born in Somalia but is an American citizen. The chants were prompted by Trump's tweets a few days earlier. Using racist language, Trump tweeted that Omar and three other Democrats - all women of color - known as the squad should go back to the, quote, \"totally broken and crime-infested places from which they came. \" Trump has always used Twitter as a weapon against his political opponents. But these tweets and the dozens that would come in the weeks following against the squad and against African American Congressman Elijah Cummings represented an escalation of the president's attacks. (SOUNDBITE OF MONTAGE) TRUMP: These are people that, in my opinion, hate our country. And AOC plus 3, that's what I call it. Elijah Cummings has not helped the people. If you're not happy here, then you can leave. RASCOE: NPR examined Trump's tweets about members of Congress through October 4. Since becoming president, Trump has tweeted more than 700 times about current lawmakers. While Trump hasn't been restrained about insults, his negative tweets have increased in volume. And the nature of insults have become more extreme since July. In the case of the squad, which includes Omar, Ayanna Pressley, Alexandria Ocasio-Cortez and Rashida Tlaib, Trump has tweeted about them collectively nearly 20 times in about three months. That's more negative tweets than Trump has aimed at presidential candidates Elizabeth Warren or Bernie Sanders. Among other things, he's called the squad a very racist group of troublemakers. That language speaks to some white Trump supporters who may be worried about their place in America, says Niambi Carter. She's a political science professor at Howard University. NIAMBI CARTER: When he says make America great again and points to these women as the sort of embodiment of everything that America isn't - it's not black, it's not a woman, it's not Muslim, it's not Latina - these women become perfect vehicles for all of that anxiety that these white voters feel. RASCOE: The only lawmakers with more negative tweets than the squad and Cummings are top Democrats - House Speaker Nancy Pelosi, Senate Minority Leader Chuck Schumer and House Intelligence Committee Chairman Adam Schiff. But his tweets about them are more general criticisms about the Democratic Party. BRIAN OTT: The president chooses his targets pretty carefully, but he doesn't always choose them for the same reason. RASCOE: That's Brian Ott, a communications professor at Texas Tech University and co-author of \"The Twitter Presidency. \" He says Trump uses Twitter to discredit politicians he views as a threat and to distract from other matters. OTT: Then there's this other motivation, which is trying to define the opposition. And it's clear that he takes tremendous pleasure and is particularly vitriolic when it comes to characterizing women and ethnic minorities on Twitter. RASCOE: On Twitter, Trump says the face of the Democratic Party is the squad, Pelosi and Congresswoman Maxine Waters, who is African American. He never says white men are the face of the Democratic Party. The White House didn't respond to interview requests, but Trump's supporters argue he's a counterpuncher. Paris Dennard is a prominent African American conservative who backs Trump. He said it's not race or gender that motivates the president. PARIS DENNARD: If you are a worthy opponent - meaning you have a large social media following, meaning you can attract a lot of people to whatever it is you're talking about - he is going to engage with them because he thinks it's worth the fight. RASCOE: Trump has targeted both Cummings, who is chair of the House Oversight Committee, and Schiff with a flood of tweets. But while the tweets against Schiff have focused on his actions as chairman, Trump's tweets against Cummings focus on the mostly black district he represents in Baltimore. Ott says the rhetoric about Cummings in particular is meant to set him apart as different. OTT: The president's discussion of Baltimore as rat-infested is really meant to convey that he's not quite human. RASCOE: With an impeachment probe in full swing, the president has tweeted about the squad less. Cummings hasn't gotten a mention in weeks. But Trump hasn't forgotten about these lawmakers. He told reporters on Friday that AOC plus three, as he likes to call the squad, are the real leaders of the Democratic Party. Ayesha Rascoe, NPR News, Washington. (SOUNDBITE OF THRUPENCE'S \"FOREST ON THE SUN\")", "section": "Politics", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-11-769609684": {"title": "Safety Experts Slam Boeing And FAA For Design And Approval Of 737 Max Jets : NPR", "url": "https://www.npr.org/2019/10/11/769609684/safety-experts-slam-boeing-and-faa-for-design-and-approval-of-737-max-jets", "author": "No author found", "published_date": "2019-10-11", "content": "", "section": "National", "disclaimer": ""}, "2019-10-11-769371114": {"title": "Uber To Buy Cornershop, An Online Grocery Delivery Service : NPR", "url": "https://www.npr.org/2019/10/11/769371114/uber-to-buy-latin-american-online-grocery-delivery-service-cornershop", "author": "No author found", "published_date": "2019-10-11", "content": "", "section": "Business", "disclaimer": ""}, "2019-10-13-769946841": {"title": "Troll Watch: Elizabeth Warren's Facebook Ad : NPR", "url": "https://www.npr.org/2019/10/13/769946841/troll-watch-elizabeth-warrens-facebook-ad", "author": "No author found", "published_date": "2019-10-13", "content": "SACHA PFEIFFER, HOST: Facebook and its CEO, Mark Zuckerberg, have not endorsed President Trump for reelection. So why is Elizabeth Warren spreading that falsehood in a new ad running on Facebook? She says she did it to protest the social network's political advertising policies, and she accuses Facebook of profiting from spreading lies. We're going to look at this deliberate ploy by Warren's campaign in our regular segment called Troll Watch. (SOUNDBITE OF MUSIC)PFEIFFER: NPR's tech correspondent Shannon Bond joins us from San Francisco. Hi, Shannon. SHANNON BOND, BYLINE: Hi, Sacha. PFEIFFER: So if anyone's seen this ad, you realize, if you read beyond the headline, that it's not true. But if you only read the headline, you might think it's true. So describe this ad for us. BOND: Sure. It shows a picture of President Trump shaking hands with Mark Zuckerberg in the Oval Office. And it says breaking news - Mark Zuckerberg and Facebook just endorsed Donald Trump for reelection. You're probably shocked. And you might be thinking - how could this possibly be true? And then the ad goes on to say, well, it's not. Sorry. So we should reiterate here. Facebook and Zuckerberg have not endorsed the president or any other candidate. But the Warren campaign - they started running this ad on Thursday. It's already been shown to a lot of people all over the country. PFEIFFER: And Facebook actually approved the ad, you know. As you said, it's been up for a few days, even though it's gotten a lot of publicity. So this didn't just accidentally slip through some automated approval system. Now, Warren says that proves her point, although I've seen some comments on Facebook saying it's not really a lie, it's just a parody, and Facebook is fine with parodies. But explain to us. Why is she running this ad? BOND: Yeah, I mean, Elizabeth Warren has been a relentless critic of Facebook. And she says she's made an ad that's deliberately false to highlight this fact that Facebook allows political candidates to essentially lie in their ads. So this actually came up because of another misleading political ad. The Trump campaign was running this ad across social media and on TV, making false claims about Joe Biden. And the Biden campaign complained, but Facebook didn't take it down. It says it doesn't fact-check political speech as a matter of course. That's the problem that Warren is raising. She says by taking money for these kinds of ads, Facebook is choosing profits over, quote, \"protecting democracy. \"PFEIFFER: When I was getting ready to talk to you, I was reading about Facebook's policy on when it will or won't reject ads or downplay ads. And it's confusing, and it's controversial. What are the rules governing this area? BOND: Yeah, it is confusing. So what Facebook says is, we aren't actually doing anything different than what broadcast television does. There's actually an FCC rule that says broadcast stations have to air political ads. They can't block them based on what they say. The difference is for cable networks, and people might have seen that CNN refused to air the Trump ad. It sets its own policies for what it airs. Facebook has that discretion, but they're saying they consider themselves to be like the broadcast network. And so they're not going to censor political ads. PFEIFFER: It seems like Facebook is framing this as a free speech issue. But how much of this is driven by ad revenue? And is Facebook possibly looking the other way because advertising is so lucrative? BOND: I mean, that's a good question. Now, Facebook says political ads are actually just a drop in the bucket for them. I mean, that company sold $55 billion worth of ads last year, and political ads are just a single digit percentage of their total. But it's also clearly reluctant to drop political advertising, altogether. So I think this is really about Facebook feeling it can't win if it starts policing political speech. PFEIFFER: The presidential election is, obviously, more than a year away. How do you see this playing out over the coming year and beyond? BOND: Well, candidates are spending more money online. We know this. And Facebook is, you know, particularly important for them as a place to advertise. You can reach so many people there. Facebook's made very clear, though, it's not going to tell politicians what they can or can't say. And it's going to keep selling these ads. So I think there's clearly a continued risk that it'll be, once again, a place where misinformation is spreading. PFEIFFER: That's NPR's tech correspondent Shannon Bond. Shannon, thank you. BOND: Thanks. PFEIFFER: And we want to note that Facebook is among NPR's recent financial supporters. SACHA PFEIFFER, HOST:  Facebook and its CEO, Mark Zuckerberg, have not endorsed President Trump for reelection. So why is Elizabeth Warren spreading that falsehood in a new ad running on Facebook? She says she did it to protest the social network's political advertising policies, and she accuses Facebook of profiting from spreading lies. We're going to look at this deliberate ploy by Warren's campaign in our regular segment called Troll Watch. (SOUNDBITE OF MUSIC) PFEIFFER: NPR's tech correspondent Shannon Bond joins us from San Francisco. Hi, Shannon. SHANNON BOND, BYLINE: Hi, Sacha. PFEIFFER: So if anyone's seen this ad, you realize, if you read beyond the headline, that it's not true. But if you only read the headline, you might think it's true. So describe this ad for us. BOND: Sure. It shows a picture of President Trump shaking hands with Mark Zuckerberg in the Oval Office. And it says breaking news - Mark Zuckerberg and Facebook just endorsed Donald Trump for reelection. You're probably shocked. And you might be thinking - how could this possibly be true? And then the ad goes on to say, well, it's not. Sorry. So we should reiterate here. Facebook and Zuckerberg have not endorsed the president or any other candidate. But the Warren campaign - they started running this ad on Thursday. It's already been shown to a lot of people all over the country. PFEIFFER: And Facebook actually approved the ad, you know. As you said, it's been up for a few days, even though it's gotten a lot of publicity. So this didn't just accidentally slip through some automated approval system. Now, Warren says that proves her point, although I've seen some comments on Facebook saying it's not really a lie, it's just a parody, and Facebook is fine with parodies. But explain to us. Why is she running this ad? BOND: Yeah, I mean, Elizabeth Warren has been a relentless critic of Facebook. And she says she's made an ad that's deliberately false to highlight this fact that Facebook allows political candidates to essentially lie in their ads. So this actually came up because of another misleading political ad. The Trump campaign was running this ad across social media and on TV, making false claims about Joe Biden. And the Biden campaign complained, but Facebook didn't take it down. It says it doesn't fact-check political speech as a matter of course. That's the problem that Warren is raising. She says by taking money for these kinds of ads, Facebook is choosing profits over, quote, \"protecting democracy. \" PFEIFFER: When I was getting ready to talk to you, I was reading about Facebook's policy on when it will or won't reject ads or downplay ads. And it's confusing, and it's controversial. What are the rules governing this area? BOND: Yeah, it is confusing. So what Facebook says is, we aren't actually doing anything different than what broadcast television does. There's actually an FCC rule that says broadcast stations have to air political ads. They can't block them based on what they say. The difference is for cable networks, and people might have seen that CNN refused to air the Trump ad. It sets its own policies for what it airs. Facebook has that discretion, but they're saying they consider themselves to be like the broadcast network. And so they're not going to censor political ads. PFEIFFER: It seems like Facebook is framing this as a free speech issue. But how much of this is driven by ad revenue? And is Facebook possibly looking the other way because advertising is so lucrative? BOND: I mean, that's a good question. Now, Facebook says political ads are actually just a drop in the bucket for them. I mean, that company sold $55 billion worth of ads last year, and political ads are just a single digit percentage of their total. But it's also clearly reluctant to drop political advertising, altogether. So I think this is really about Facebook feeling it can't win if it starts policing political speech. PFEIFFER: The presidential election is, obviously, more than a year away. How do you see this playing out over the coming year and beyond? BOND: Well, candidates are spending more money online. We know this. And Facebook is, you know, particularly important for them as a place to advertise. You can reach so many people there. Facebook's made very clear, though, it's not going to tell politicians what they can or can't say. And it's going to keep selling these ads. So I think there's clearly a continued risk that it'll be, once again, a place where misinformation is spreading. PFEIFFER: That's NPR's tech correspondent Shannon Bond. Shannon, thank you. BOND: Thanks. PFEIFFER: And we want to note that Facebook is among NPR's recent financial supporters.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-15-770318763": {"title": "Fortnite Unveils New Chapter After Two-Day 'Black Hole' Shutdown : NPR", "url": "https://www.npr.org/2019/10/15/770318763/fortnite-unveils-new-chapter-after-two-day-black-hole-shutdown", "author": "No author found", "published_date": "2019-10-15", "content": "", "section": "Pop Culture", "disclaimer": ""}, "2019-10-17-771095259": {"title": "Facebook CEO Mark Zuckerberg Discusses Free Speech With Georgetown Students : NPR", "url": "https://www.npr.org/2019/10/17/771095259/facebook-ceo-mark-zuckerberg-discusses-free-speech-with-georgetown-students", "author": "No author found", "published_date": "2019-10-17", "content": "AUDIE CORNISH, HOST: It's rare that Facebook CEO Mark Zuckerberg makes public speeches, but he did so today at Georgetown University. It was an attempt to tamp down some of the criticism Facebook has received about what it chooses to leave up on its site and what it takes down. Zuckerberg defended free speech. He said he didn't want Facebook to be the judge of what's allowed on its platform. NPR's tech correspondent Shannon Bond listened in on Facebook, and she joins us now with the details. And, Shannon, to start, we know the headline. Can you give us a little more detail about what Mark Zuckerberg had to say? SHANNON BOND, BYLINE: Yeah. He gave a strong defense of Facebook's role in promoting free speech as he sees it. And the way Zuckerberg talked about it, the risk that Facebook is confronting is too much restriction about what people can and can't post on Facebook or even just on the Internet in general. And he says the political divisions that we're seeing are encouraging people to call for shutting down speech that they disagree with. But that's really dangerous in Facebook's view. Zuckerberg framed this as not about political fights in the U. S. but a question, really, with global implications given the rise of China. He noted in the speech that six of the top 10 Internet platforms today are Chinese. Those platforms are subject to censorship from Beijing. And in contrast, Zuckerberg says that Facebook represents American values of free speech. So here's what he said about that. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: So this question of which nation's values are going to determine what speech is allowed for decades to come really puts into perspective our debates about the content issues of the day. BOND: And he's saying here that Facebook critics really need to take the worldview instead of just thinking about what's happening here at home. CORNISH: Now, Zuckerberg, as I said earlier, he doesn't speak a lot at public events like this. So what do you think is behind this decision to do it now? BOND: Well, he's coming back to Washington next week to testify before Congress, and he's probably going to face some tough questions from lawmakers on both sides. So I think, to some degree, this is about getting ahead of that. And Facebook's becoming this central concern in the 2020 election. You know, we all remember what happened in 2016, and there are questions whether Facebook has done enough to correct the problems it faced. Just last week, Democratic presidential candidate Elizabeth Warren attacked Facebook for allowing a Trump ad that made false claims. And so the questions people are asking is, what is Facebook's responsibility? You know, this is a private company. It gets to set the rules for what can and can't appear on its platform, but it doesn't want to be the arbiter of truth. It doesn't want to be a censor. Zuckerberg addressed exactly this today, and he stressed that there are no easy answers. Here's what he said. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: I believe that when it's not absolutely clear what to do, then we should err on the side of greater expression. BOND: And so, you know, Zuckerberg says that Facebook actually considered banning political ads altogether - just, you know, saying we're not going to take any of these. And it's not a big part of their revenue stream. But he didn't. He said that his concerns were that that would tip the scales, just doing that itself. And he says that people should be able to hear politicians and make up their own minds. So this is really this doubling down on this position that Facebook and other tech companies are taking; they're not going to be the ref here. CORNISH: Right. And Facebook isn't alone in dealing with this issue. So you've got these other social media platforms trying to figure out when they leave speech up, when they suspend accounts. How are other people dealing with it? BOND: That's right. I mean, Twitter is also struggling with this. Like Facebook, it said that its rules that ban things like bullying and threatening language don't apply in all cases to political leaders like President Trump. You know, he uses Twitter all the time. And they're going to err on the side of allowing that. So I think despite all of this criticism they're facing, we're seeing big tech companies saying, when it comes to political speech, free expression wins out over those rules that affect the average user like you and me. CORNISH: I also want to note that Facebook is among NPR's financial supporters. I want to thank NPR's Shannon Bond for your reporting. BOND: Thanks, Audie. AUDIE CORNISH, HOST:  It's rare that Facebook CEO Mark Zuckerberg makes public speeches, but he did so today at Georgetown University. It was an attempt to tamp down some of the criticism Facebook has received about what it chooses to leave up on its site and what it takes down. Zuckerberg defended free speech. He said he didn't want Facebook to be the judge of what's allowed on its platform. NPR's tech correspondent Shannon Bond listened in on Facebook, and she joins us now with the details. And, Shannon, to start, we know the headline. Can you give us a little more detail about what Mark Zuckerberg had to say? SHANNON BOND, BYLINE: Yeah. He gave a strong defense of Facebook's role in promoting free speech as he sees it. And the way Zuckerberg talked about it, the risk that Facebook is confronting is too much restriction about what people can and can't post on Facebook or even just on the Internet in general. And he says the political divisions that we're seeing are encouraging people to call for shutting down speech that they disagree with. But that's really dangerous in Facebook's view. Zuckerberg framed this as not about political fights in the U. S. but a question, really, with global implications given the rise of China. He noted in the speech that six of the top 10 Internet platforms today are Chinese. Those platforms are subject to censorship from Beijing. And in contrast, Zuckerberg says that Facebook represents American values of free speech. So here's what he said about that. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: So this question of which nation's values are going to determine what speech is allowed for decades to come really puts into perspective our debates about the content issues of the day. BOND: And he's saying here that Facebook critics really need to take the worldview instead of just thinking about what's happening here at home. CORNISH: Now, Zuckerberg, as I said earlier, he doesn't speak a lot at public events like this. So what do you think is behind this decision to do it now? BOND: Well, he's coming back to Washington next week to testify before Congress, and he's probably going to face some tough questions from lawmakers on both sides. So I think, to some degree, this is about getting ahead of that. And Facebook's becoming this central concern in the 2020 election. You know, we all remember what happened in 2016, and there are questions whether Facebook has done enough to correct the problems it faced. Just last week, Democratic presidential candidate Elizabeth Warren attacked Facebook for allowing a Trump ad that made false claims. And so the questions people are asking is, what is Facebook's responsibility? You know, this is a private company. It gets to set the rules for what can and can't appear on its platform, but it doesn't want to be the arbiter of truth. It doesn't want to be a censor. Zuckerberg addressed exactly this today, and he stressed that there are no easy answers. Here's what he said. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: I believe that when it's not absolutely clear what to do, then we should err on the side of greater expression. BOND: And so, you know, Zuckerberg says that Facebook actually considered banning political ads altogether - just, you know, saying we're not going to take any of these. And it's not a big part of their revenue stream. But he didn't. He said that his concerns were that that would tip the scales, just doing that itself. And he says that people should be able to hear politicians and make up their own minds. So this is really this doubling down on this position that Facebook and other tech companies are taking; they're not going to be the ref here. CORNISH: Right. And Facebook isn't alone in dealing with this issue. So you've got these other social media platforms trying to figure out when they leave speech up, when they suspend accounts. How are other people dealing with it? BOND: That's right. I mean, Twitter is also struggling with this. Like Facebook, it said that its rules that ban things like bullying and threatening language don't apply in all cases to political leaders like President Trump. You know, he uses Twitter all the time. And they're going to err on the side of allowing that. So I think despite all of this criticism they're facing, we're seeing big tech companies saying, when it comes to political speech, free expression wins out over those rules that affect the average user like you and me. CORNISH: I also want to note that Facebook is among NPR's financial supporters. I want to thank NPR's Shannon Bond for your reporting. BOND: Thanks, Audie.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-18-771451904": {"title": "Boeing Pilots Detected 737 Max Flight Control Glitch 2 Years Before Deadly Crash : NPR", "url": "https://www.npr.org/2019/10/18/771451904/boeing-pilots-detected-737-max-flight-control-glitch-two-years-before-deadly-cra", "author": "No author found", "published_date": "2019-10-18", "content": "", "section": "National", "disclaimer": ""}, "2019-10-18-770898952": {"title": "Drone Delivery Is One Step Closer To Reality : NPR", "url": "https://www.npr.org/2019/10/18/770898952/drone-delivery-is-one-step-closer-to-reality", "author": "No author found", "published_date": "2019-10-18", "content": "DAVID GREENE, HOST: All right. So drones are not quite ready to compete with Santa Claus delivering toys to your home for Christmas morning, but that dream may be closer to reality. Walgreens is now testing on-demand drone delivery in Virginia. And UPS just won approval to operate a drone delivery airline. Here's NPR's David Schaper(SOUNDBITE OF HELICOPTER WHIRRING)DAVID SCHAPER, BYLINE: Sounding like a huge swarm of angry bees or maybe a hedge trimmer on steroids, a small quadcopter lifts up in front of the hospital building on the WakeMed campus in Raleigh, N. C. (SOUNDBITE OF HELICOPTER WHIRRING)SCHAPER: Attached underneath is a metal box smaller than a shoe box with vials of blood samples heading to the lab. Dr. Stuart Ginn says by ground carrier, this used to take close to an hour. But now, with the drone. . . STUART GINN: We've seen that dropped to about 10 minutes, and that's really door to door. The actual flight time one-way on the route is about three minutes. SCHAPER: Now, WakeMed's partner in this endeavor, UPS, has federal approval to expand operations, so it can fly multiple aircraft in multiple locations over longer distances. Dr. Ginn says that will allow WakeMed to speed delivery of samples and supplies between its other health care facilities across the region. GINN: We anticipate being able to connect those hospitals together and those health-plexes back to the hospitals. SCHAPER: But its longer range drones that could eventually be a game-changer in places where doctors and patients, medications and supplies may be miles apart. Bala Ganesh is vice president of the advanced technology group at UPS. BALA GANESH: What we're doing is we're opening up a third dimension that wasn't there. We were thinking in 2D. And now, we're starting to think in the third dimension. SCHAPER: Ganesh says GPS and other technologies allow the unmanned drones to fly to precise locations. And collision avoidance technology will help prevent them from crashing into obstacles, like trees, power lines, buildings or, for that matter, even other drones. GANESH: We are moving forward into a future that does not exist today. So it's an amazing, amazing thing. SCHAPER: Drones are already being used commercially for photography and film, for inspecting crops, buildings, bridges and railroads. And first responders use them in search and rescue operations and to survey damage from disasters. But those drones operate on short leashes. The new federal Part 135 certification will eventually allow UPS to make drone deliveries beyond the operator's line of sight, at night and over populated areas. And now the company is expanding drone delivery to the University of Utah's medical campus, among others, and partnering with CVS to home deliver prescriptions by drone. Jacob Reed heads the unmanned aircraft systems program at Lewis University outside of Chicago. JACOB REED: Everybody understands the value in this, right? And everybody understands the demands that consumers have with wanting their goods and wanting their goods faster. SCHAPER: And drones could do that much more easily than delivery vans and trucks. But Reed says there are still obstacles. REED: The biggest thing is safety. It's not only the safety of everybody on the ground that the aircraft may be flying over, but it's the safety of other manned aircraft that are in the skies. SCHAPER: There are also concerns about privacy and drone noise and security concerns over whether they could be hacked and steered off course. And then there's just plain old human curiosity. Reed imagines a drone delivering to his house on a warm summer day. REED: And this big rotor craft comes and lands maybe on my driveway or my doorstep to drop off a package. Well, now, there's kids in the area that are off to school. And they're - come by to check out this cool aircraft that just delivered something. SCHAPER: Will the drone know kids are close by and could be injured by restarting the rotors? The FAA and drone developers are working to address those and other concerns. And it is becoming more likely that drone deliveries to our homes will soon take off. David Schaper, NPR News, Chicago. DAVID GREENE, HOST:  All right. So drones are not quite ready to compete with Santa Claus delivering toys to your home for Christmas morning, but that dream may be closer to reality. Walgreens is now testing on-demand drone delivery in Virginia. And UPS just won approval to operate a drone delivery airline. Here's NPR's David Schaper (SOUNDBITE OF HELICOPTER WHIRRING) DAVID SCHAPER, BYLINE: Sounding like a huge swarm of angry bees or maybe a hedge trimmer on steroids, a small quadcopter lifts up in front of the hospital building on the WakeMed campus in Raleigh, N. C. (SOUNDBITE OF HELICOPTER WHIRRING) SCHAPER: Attached underneath is a metal box smaller than a shoe box with vials of blood samples heading to the lab. Dr. Stuart Ginn says by ground carrier, this used to take close to an hour. But now, with the drone. . . STUART GINN: We've seen that dropped to about 10 minutes, and that's really door to door. The actual flight time one-way on the route is about three minutes. SCHAPER: Now, WakeMed's partner in this endeavor, UPS, has federal approval to expand operations, so it can fly multiple aircraft in multiple locations over longer distances. Dr. Ginn says that will allow WakeMed to speed delivery of samples and supplies between its other health care facilities across the region. GINN: We anticipate being able to connect those hospitals together and those health-plexes back to the hospitals. SCHAPER: But its longer range drones that could eventually be a game-changer in places where doctors and patients, medications and supplies may be miles apart. Bala Ganesh is vice president of the advanced technology group at UPS. BALA GANESH: What we're doing is we're opening up a third dimension that wasn't there. We were thinking in 2D. And now, we're starting to think in the third dimension. SCHAPER: Ganesh says GPS and other technologies allow the unmanned drones to fly to precise locations. And collision avoidance technology will help prevent them from crashing into obstacles, like trees, power lines, buildings or, for that matter, even other drones. GANESH: We are moving forward into a future that does not exist today. So it's an amazing, amazing thing. SCHAPER: Drones are already being used commercially for photography and film, for inspecting crops, buildings, bridges and railroads. And first responders use them in search and rescue operations and to survey damage from disasters. But those drones operate on short leashes. The new federal Part 135 certification will eventually allow UPS to make drone deliveries beyond the operator's line of sight, at night and over populated areas. And now the company is expanding drone delivery to the University of Utah's medical campus, among others, and partnering with CVS to home deliver prescriptions by drone. Jacob Reed heads the unmanned aircraft systems program at Lewis University outside of Chicago. JACOB REED: Everybody understands the value in this, right? And everybody understands the demands that consumers have with wanting their goods and wanting their goods faster. SCHAPER: And drones could do that much more easily than delivery vans and trucks. But Reed says there are still obstacles. REED: The biggest thing is safety. It's not only the safety of everybody on the ground that the aircraft may be flying over, but it's the safety of other manned aircraft that are in the skies. SCHAPER: There are also concerns about privacy and drone noise and security concerns over whether they could be hacked and steered off course. And then there's just plain old human curiosity. Reed imagines a drone delivering to his house on a warm summer day. REED: And this big rotor craft comes and lands maybe on my driveway or my doorstep to drop off a package. Well, now, there's kids in the area that are off to school. And they're - come by to check out this cool aircraft that just delivered something. SCHAPER: Will the drone know kids are close by and could be injured by restarting the rotors? The FAA and drone developers are working to address those and other concerns. And it is becoming more likely that drone deliveries to our homes will soon take off. David Schaper, NPR News, Chicago.", "section": "National", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-20-771755317": {"title": "Kara Swisher's Take On Mark Zuckerberg's 'Free Speech' Speech : NPR", "url": "https://www.npr.org/2019/10/20/771755317/kara-swisher-s-take-on-mark-zuckerberg-s-free-speech-speech", "author": "No author found", "published_date": "2019-10-20", "content": "MICHEL MARTIN, HOST: We're going to revisit a story we've covered regularly on this program - Facebook and how it handles false or misleading content. Critics from around the world have become increasingly vocal about this, saying Facebook has become a vehicle for the rapid dissemination of lies and needs more regulation. Despite this, Facebook has reaffirmed that it will continue to exempt politicians from fact-checking, allowing them to make false statements in their paid advertisements. On Thursday, though, Facebook founder Mark Zuckerberg decided to address the matter further with a nearly 40-minute speech at Georgetown University on the value of free expression. (SOUNDBITE OF SPEECH)MARK ZUCKERBERG: Giving more people a voice gives power to the powerless, and it pushes society to get better over time. MARTIN: To talk more about this, we've called on Kara Swisher. She is the co-founder and editor at large of Recode. That is a media outlet that covers the digital world. Kara, thank you so much for joining us. KARA SWISHER: Thanks so much. MARTIN: And I do want to disclose here that Facebook is among NPR's recent financial supporters. Having said that, what was the importance of this speech, Kara? Was any news made there? SWISHER: Well, I'm sort of trying to figure it out still. He's sort of on a PR offensive again. And right now, it's around free speech and trying to defend what has happened on Facebook as being sort of a binary choice between free speech or, I guess, China. I can't really quite figure it out I actually thought the speech was pretty thin intellectually on an incredibly complicated topic. MARTIN: The New York Times posted a piece this weekend that talks about the overwhelming financial advantage that the Trump campaign. . . SWISHER: Yeah. MARTIN: . . . Has. And even as some of the broadcast networks are refusing to air certain ads, Facebook is taking advantage of the fact that the platform has said it will not subject politicians' statements to fact-checking. And so they're posting ads that say things that have been completely debunked. Did he directly address that? SWISHER: Well, he did. He - there was a question about that. And he said he had thought about removing political advertising from Facebook, which was a - it's not really news because he didn't do it. But it's very clear he could remove people who are running for office. You know, and I don't know what he would do about super PACs. But I think the issue was is, this has been something that's been actually in place - is that Facebook and Twitter - and not just Facebook but Twitter and YouTube - they allow all kinds of egregious lying to go on. And especially when they're newsworthy figures on Twitter - they use the term newsworthy to allow, say, Donald Trump to violate its terms daily, essentially. And so they're saying because we want all the voices to speak, we're not going to be the ones that are arbiters of what politicians say. We'll let the public at large decide even if they're lying and that there's a mechanism in place, which is called the press, that will say, these are lies. The problem is, once these lies get out there, they get the same amount of attention that it's hard to pull them apart. And that's the one thing they don't realize - they're not like television, which has certain rules around what it's allowed to broadcast or any other medium because it's so pervasive. It's so hard to understand what's real and what's not. And it's everywhere around us. And I think that's the part he missed out. He was trying to compare himself to radio or TV, and it's not - it's just simply not the same thing. MARTIN: Before we let you go, Zuckerberg is going to testify for the second time in front of Congress. He's expected to speak before the House Financial Services Committee next Wednesday. What is that about? SWISHER: Well, it was supposed to be about Libra, which is this currency that Facebook - (laughter) they're moving into dating and currency, which, what could go wrong? So they have this currency called Libra that they're in a consortium with. A number of big payment providers pulled out of the consortium last week. And so it's going to be very hard for them to talk about it because now it's not quite the same thing as they initially introduced. They're trying to do this in a partnership style, as they should. But I think people are worried about Facebook having any control over money. And so I'm not sure what he's going to say because there's not a lot he can say about what's going on. So I suspect they'll be asking all kinds of different questions about Facebook's intentions around payments. It could be a big player, or it could be - and it could be a dangerous player. So we'll see. MARTIN: That was Kara Swisher, co-founder and editor-at-large of Recode. Kara, thanks so much for talking with us. SWISHER: Thank you so much MICHEL MARTIN, HOST:  We're going to revisit a story we've covered regularly on this program - Facebook and how it handles false or misleading content. Critics from around the world have become increasingly vocal about this, saying Facebook has become a vehicle for the rapid dissemination of lies and needs more regulation. Despite this, Facebook has reaffirmed that it will continue to exempt politicians from fact-checking, allowing them to make false statements in their paid advertisements. On Thursday, though, Facebook founder Mark Zuckerberg decided to address the matter further with a nearly 40-minute speech at Georgetown University on the value of free expression. (SOUNDBITE OF SPEECH) MARK ZUCKERBERG: Giving more people a voice gives power to the powerless, and it pushes society to get better over time. MARTIN: To talk more about this, we've called on Kara Swisher. She is the co-founder and editor at large of Recode. That is a media outlet that covers the digital world. Kara, thank you so much for joining us. KARA SWISHER: Thanks so much. MARTIN: And I do want to disclose here that Facebook is among NPR's recent financial supporters. Having said that, what was the importance of this speech, Kara? Was any news made there? SWISHER: Well, I'm sort of trying to figure it out still. He's sort of on a PR offensive again. And right now, it's around free speech and trying to defend what has happened on Facebook as being sort of a binary choice between free speech or, I guess, China. I can't really quite figure it out I actually thought the speech was pretty thin intellectually on an incredibly complicated topic. MARTIN: The New York Times posted a piece this weekend that talks about the overwhelming financial advantage that the Trump campaign. . . SWISHER: Yeah. MARTIN: . . . Has. And even as some of the broadcast networks are refusing to air certain ads, Facebook is taking advantage of the fact that the platform has said it will not subject politicians' statements to fact-checking. And so they're posting ads that say things that have been completely debunked. Did he directly address that? SWISHER: Well, he did. He - there was a question about that. And he said he had thought about removing political advertising from Facebook, which was a - it's not really news because he didn't do it. But it's very clear he could remove people who are running for office. You know, and I don't know what he would do about super PACs. But I think the issue was is, this has been something that's been actually in place - is that Facebook and Twitter - and not just Facebook but Twitter and YouTube - they allow all kinds of egregious lying to go on. And especially when they're newsworthy figures on Twitter - they use the term newsworthy to allow, say, Donald Trump to violate its terms daily, essentially. And so they're saying because we want all the voices to speak, we're not going to be the ones that are arbiters of what politicians say. We'll let the public at large decide even if they're lying and that there's a mechanism in place, which is called the press, that will say, these are lies. The problem is, once these lies get out there, they get the same amount of attention that it's hard to pull them apart. And that's the one thing they don't realize - they're not like television, which has certain rules around what it's allowed to broadcast or any other medium because it's so pervasive. It's so hard to understand what's real and what's not. And it's everywhere around us. And I think that's the part he missed out. He was trying to compare himself to radio or TV, and it's not - it's just simply not the same thing. MARTIN: Before we let you go, Zuckerberg is going to testify for the second time in front of Congress. He's expected to speak before the House Financial Services Committee next Wednesday. What is that about? SWISHER: Well, it was supposed to be about Libra, which is this currency that Facebook - (laughter) they're moving into dating and currency, which, what could go wrong? So they have this currency called Libra that they're in a consortium with. A number of big payment providers pulled out of the consortium last week. And so it's going to be very hard for them to talk about it because now it's not quite the same thing as they initially introduced. They're trying to do this in a partnership style, as they should. But I think people are worried about Facebook having any control over money. And so I'm not sure what he's going to say because there's not a lot he can say about what's going on. So I suspect they'll be asking all kinds of different questions about Facebook's intentions around payments. It could be a big player, or it could be - and it could be a dangerous player. So we'll see. MARTIN: That was Kara Swisher, co-founder and editor-at-large of Recode. Kara, thanks so much for talking with us. SWISHER: Thank you so much", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-21-770280447": {"title": "A Soccer Team In Denmark Is Using Facial Recognition To Stop Unruly Fans : NPR", "url": "https://www.npr.org/2019/10/21/770280447/a-soccer-team-in-denmark-is-using-facial-recognition-to-stop-unruly-fans", "author": "No author found", "published_date": "2019-10-21", "content": "AUDIE CORNISH, HOST: This month on All Tech Considered, we're looking at how easy it's become to track people. And one reason - facial recognition technology. (SOUNDBITE OF MUSIC)CORNISH: They've been called hooligans, though they're officially known as active supporters. Either way, diehard fans of Denmark's Brondby soccer team have a reputation for being rowdy, sometimes even violent. So team officials are using facial recognition software to cut down on stadium brawls. Sidsel Overgaard reports from just outside Copenhagen. SIDSEL OVERGAARD, BYLINE: It's a cold but sunny day on the outskirts of Copenhagen, and dozens of men dressed in black - a few with face masks - are gathered outside Brondby Stadium, fists in the air, shouting about how their team will soon beat - and beat up - the arch enemy, FC Copenhagen. UNIDENTIFIED BRONDBY ACTIVE FANS: (Chanting in foreign language). OVERGAARD: My attempted photo of the group gets a gloved hand in the face and a security guard scurrying over with concern. UNIDENTIFIED SECURITY GUARD: (Foreign language spoken). OVERGAARD: OK. \"Don't film them,\" she says. \"It'll end badly. They don't want to be recognized. \"Ironically, this would-be anonymous group has become the target of one of the first large-scale facial recognition systems approved for private use under Europe's strict data protection laws. Chant over, the group moves towards the stadium entrance where they, along with 21,000 other fans, will be asked to remove masks, hats and glasses so a computer can scan their faces, compare it to a list of banned troublemakers and determine whether they can get in. Since being launched a few months ago, the system has caught four people on the watchlist. MARTIN LUND: I think it's great. I think it's OK. OVERGAARD: Martin Lund is here with his kids. M LUND: We're here to enjoy the game. We're here to have a nice time. We're here to shout at the opponent. But we're not here to fight. And if people want to fight, they shouldn't come to the game. OVERGAARD: Lund says that while the idea of facial recognition technology gives him pause, he trusts that here it's being used well. M LUND: You can't do anything in Denmark without getting the proper approval. So it's not being misused. You can't do that in Denmark. OVERGAARD: Brondby's security chief, Mickel Lauritsen, says getting this system approved did involve government regulators, team lawyers, the software company and fan input. MICKEL LAURITSEN: It was a long process. It took us about 2 1/2, three years to get in place. OVERGAARD: And now that it's here, Lauritsen says he's very careful to stay within the prescribed boundaries. That means pictures of those on the watchlist are entered into the system on game day and deleted again at the end of the day. The system is not connected to the Internet. And there's a cross-check to avoid false positives. Lauritsen says at one point, the police asked him to enter a suspect's picture into the system to help with an investigation, and he said no because that is not part of the deal. LAURITSEN: I know if I misuse the system, I'm not allowed to do anything with it going forward. And then we'll be restricted in what we can do even further than we are now. So I'm not going to misuse the system or misuse the trust we've been given. OVERGAARD: Still, that's not enough reassurance for everyone. Jesper Lund is chairman of the IT-Political Association of Denmark. He says it's a slippery slope from one facial recognition system to the next. And he believes it's a tool that should be reserved for rare situations involving terrorism or serious crime. JESPER LUND: Using this very invasive and error-prone technology for something like making sure that persons on a banned list cannot go to a football match is really not proportionate. So in my opinion, this should never have been allowed by the Danish DPA. OVERGAARD: But he acknowledges this is a tough fight. Everyone I talked to at the Brondby Stadium expressed some version of it's inevitable. And in that light, says IT law professor Henrik Udsen, the best thing any country can do is have the conversation. HENRIK UDSEN: The important part here is that we have these discussions and we are taking both advantages and risks into account when we decide what to do because there are no necessary right and wrong answers here. OVERGAARD: In the U. S. , it's a conversation that so far has been happening on the local and state level and is usually about how government uses the technology, not so much the private sector. But now there are several bills pending in Congress to regulate facial recognition, and some have bipartisan support. For NPR News, I'm Sidsel Overgaard in Denmark. AUDIE CORNISH, HOST:  This month on All Tech Considered, we're looking at how easy it's become to track people. And one reason - facial recognition technology. (SOUNDBITE OF MUSIC) CORNISH: They've been called hooligans, though they're officially known as active supporters. Either way, diehard fans of Denmark's Brondby soccer team have a reputation for being rowdy, sometimes even violent. So team officials are using facial recognition software to cut down on stadium brawls. Sidsel Overgaard reports from just outside Copenhagen. SIDSEL OVERGAARD, BYLINE: It's a cold but sunny day on the outskirts of Copenhagen, and dozens of men dressed in black - a few with face masks - are gathered outside Brondby Stadium, fists in the air, shouting about how their team will soon beat - and beat up - the arch enemy, FC Copenhagen. UNIDENTIFIED BRONDBY ACTIVE FANS: (Chanting in foreign language). OVERGAARD: My attempted photo of the group gets a gloved hand in the face and a security guard scurrying over with concern. UNIDENTIFIED SECURITY GUARD: (Foreign language spoken). OVERGAARD: OK. \"Don't film them,\" she says. \"It'll end badly. They don't want to be recognized. \" Ironically, this would-be anonymous group has become the target of one of the first large-scale facial recognition systems approved for private use under Europe's strict data protection laws. Chant over, the group moves towards the stadium entrance where they, along with 21,000 other fans, will be asked to remove masks, hats and glasses so a computer can scan their faces, compare it to a list of banned troublemakers and determine whether they can get in. Since being launched a few months ago, the system has caught four people on the watchlist. MARTIN LUND: I think it's great. I think it's OK. OVERGAARD: Martin Lund is here with his kids. M LUND: We're here to enjoy the game. We're here to have a nice time. We're here to shout at the opponent. But we're not here to fight. And if people want to fight, they shouldn't come to the game. OVERGAARD: Lund says that while the idea of facial recognition technology gives him pause, he trusts that here it's being used well. M LUND: You can't do anything in Denmark without getting the proper approval. So it's not being misused. You can't do that in Denmark. OVERGAARD: Brondby's security chief, Mickel Lauritsen, says getting this system approved did involve government regulators, team lawyers, the software company and fan input. MICKEL LAURITSEN: It was a long process. It took us about 2 1/2, three years to get in place. OVERGAARD: And now that it's here, Lauritsen says he's very careful to stay within the prescribed boundaries. That means pictures of those on the watchlist are entered into the system on game day and deleted again at the end of the day. The system is not connected to the Internet. And there's a cross-check to avoid false positives. Lauritsen says at one point, the police asked him to enter a suspect's picture into the system to help with an investigation, and he said no because that is not part of the deal. LAURITSEN: I know if I misuse the system, I'm not allowed to do anything with it going forward. And then we'll be restricted in what we can do even further than we are now. So I'm not going to misuse the system or misuse the trust we've been given. OVERGAARD: Still, that's not enough reassurance for everyone. Jesper Lund is chairman of the IT-Political Association of Denmark. He says it's a slippery slope from one facial recognition system to the next. And he believes it's a tool that should be reserved for rare situations involving terrorism or serious crime. JESPER LUND: Using this very invasive and error-prone technology for something like making sure that persons on a banned list cannot go to a football match is really not proportionate. So in my opinion, this should never have been allowed by the Danish DPA. OVERGAARD: But he acknowledges this is a tough fight. Everyone I talked to at the Brondby Stadium expressed some version of it's inevitable. And in that light, says IT law professor Henrik Udsen, the best thing any country can do is have the conversation. HENRIK UDSEN: The important part here is that we have these discussions and we are taking both advantages and risks into account when we decide what to do because there are no necessary right and wrong answers here. OVERGAARD: In the U. S. , it's a conversation that so far has been happening on the local and state level and is usually about how government uses the technology, not so much the private sector. But now there are several bills pending in Congress to regulate facial recognition, and some have bipartisan support. For NPR News, I'm Sidsel Overgaard in Denmark.", "section": "World", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-21-772035601": {"title": "Facebook Rolls Out New Election Security Measures : NPR", "url": "https://www.npr.org/2019/10/21/772035601/we-have-a-big-responsibility-facebook-rolls-out-new-election-security-measures", "author": "No author found", "published_date": "2019-10-21", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-21-772049931": {"title": "What Breaking Up Big Tech Might Look Like : NPR", "url": "https://www.npr.org/2019/10/21/772049931/what-breaking-up-big-tech-might-look-like", "author": "No author found", "published_date": "2019-10-21", "content": "AILSA CHANG, HOST: This month in All Tech Considered, we're looking at why everyone wants to break up big tech. The Department of Justice, the Federal Trade Commission and state attorneys general from across the country have all begun to investigate the tech giants like Facebook and Google, and with Massachusetts Senator Elizabeth Warren leading the charge, Democratic presidential hopefuls now have to stake out their own positions. (SOUNDBITE OF MONTAGE)ELIZABETH WARREN: I'm not willing to give up and let a handful of monopolists dominate our economy and our democracy. TOM STEYER: I agree with Senator Warren that, in fact, monopolies have to be dealt with. They either have to be broken up or regulated, and that's part of it. CORY BOOKER: Anybody who does not think that we have a massive crisis in our democracy with the way these tech companies are being used not just in terms of anti-competitive practices, but also to undermine our democracy. . . CHANG: That's Cory Booker, and before him, Tom Steyer. To explain why this has become a leading political issue and what breaking up big tech might even look like, I'm joined now by someone who has spent years thinking about it, Columbia law professor Tim Wu. Welcome. TIM WU: Pleasure to be here. CHANG: So I know you've explained this to lawmakers on the Hill already, but I want you to explain it to us now. What do you think the harm is that these companies are creating? Because they do offer free services, so we're not talking about, like, price gouging. Is this more about, say, elimination of choices? WU: Yeah. It's the harms that come with monopolization, which is the ability of a company to get away with stuff. So if you take a company like Facebook facing very little competition, they've been able to decrease the privacy protections that they offer people. If you look at a company like Google with not much competition in search, they've been raising and raising their advertising rates. So it's less in the old-fashioned price-fixing kind of conspiracy but more about users having less choice, less places to go and, therefore, companies being able to get away with more. CHANG: So do you think the Facebooks or the Googles, Amazons of the world are actually violating current antitrust law? WU: You know, it depends on the company and the case, but I think there's enough evidence to look hard at the question. I've spent the most time looking at Facebook. They bought out a lot of their most dangerous rivals in the early 2010s. It's against the law to buy your competitors. Instagram and WhatsApp or two of their competitors they bought. So I think there's at least, you know, probable cause that they've done something. The case against Google is a little more complicated, but some people believe that there is enough there to take a look as well. CHANG: Now, Elizabeth Warren, she has said that big tech companies - specifically Amazon, Google and Facebook - should be broken up. She says Amazon should not own Whole Foods, for example - should not own Zappos, the shoe company. Google should spin off Google Search - you know, things like that. So let me ask you, is breaking up these companies the best approach? WU: It depends. I mean, I think sometimes, it is. Returning to Facebook, I think their acquisitions of WhatsApp and Instagram were illegal and anti-competitive, and so if they were asked to spin off those companies under the acquisitions, it would be a form of breakup, but one I think that would reintroduce competition. I think breakups or undoing of mergers are actually called for more than we have appreciated in the last few decades, but you don't want to create new problems. CHANG: What kind of problems do you foresee? WU: In any action, there's unintended consequences. If you imagine, for example, breaking up Apple into three little mini-Apples, there might be more competition, but it might be that people's phones don't work as well or, you know, they lose whatever magic mojo they had. So you don't want to actually make things worse for people, but that said, I think breakups have the positive effect of rebooting an industry, starting things afresh and frankly, historically, have often been better for people in the long run than anyone predicted. CHANG: That's Tim Wu of Columbia Law School. He's the author of \"The Curse Of Bigness: Antitrust In The New Gilded Age. \" Thanks very much for joining us today. WU: My pleasure. AILSA CHANG, HOST:  This month in All Tech Considered, we're looking at why everyone wants to break up big tech. The Department of Justice, the Federal Trade Commission and state attorneys general from across the country have all begun to investigate the tech giants like Facebook and Google, and with Massachusetts Senator Elizabeth Warren leading the charge, Democratic presidential hopefuls now have to stake out their own positions. (SOUNDBITE OF MONTAGE) ELIZABETH WARREN: I'm not willing to give up and let a handful of monopolists dominate our economy and our democracy. TOM STEYER: I agree with Senator Warren that, in fact, monopolies have to be dealt with. They either have to be broken up or regulated, and that's part of it. CORY BOOKER: Anybody who does not think that we have a massive crisis in our democracy with the way these tech companies are being used not just in terms of anti-competitive practices, but also to undermine our democracy. . . CHANG: That's Cory Booker, and before him, Tom Steyer. To explain why this has become a leading political issue and what breaking up big tech might even look like, I'm joined now by someone who has spent years thinking about it, Columbia law professor Tim Wu. Welcome. TIM WU: Pleasure to be here. CHANG: So I know you've explained this to lawmakers on the Hill already, but I want you to explain it to us now. What do you think the harm is that these companies are creating? Because they do offer free services, so we're not talking about, like, price gouging. Is this more about, say, elimination of choices? WU: Yeah. It's the harms that come with monopolization, which is the ability of a company to get away with stuff. So if you take a company like Facebook facing very little competition, they've been able to decrease the privacy protections that they offer people. If you look at a company like Google with not much competition in search, they've been raising and raising their advertising rates. So it's less in the old-fashioned price-fixing kind of conspiracy but more about users having less choice, less places to go and, therefore, companies being able to get away with more. CHANG: So do you think the Facebooks or the Googles, Amazons of the world are actually violating current antitrust law? WU: You know, it depends on the company and the case, but I think there's enough evidence to look hard at the question. I've spent the most time looking at Facebook. They bought out a lot of their most dangerous rivals in the early 2010s. It's against the law to buy your competitors. Instagram and WhatsApp or two of their competitors they bought. So I think there's at least, you know, probable cause that they've done something. The case against Google is a little more complicated, but some people believe that there is enough there to take a look as well. CHANG: Now, Elizabeth Warren, she has said that big tech companies - specifically Amazon, Google and Facebook - should be broken up. She says Amazon should not own Whole Foods, for example - should not own Zappos, the shoe company. Google should spin off Google Search - you know, things like that. So let me ask you, is breaking up these companies the best approach? WU: It depends. I mean, I think sometimes, it is. Returning to Facebook, I think their acquisitions of WhatsApp and Instagram were illegal and anti-competitive, and so if they were asked to spin off those companies under the acquisitions, it would be a form of breakup, but one I think that would reintroduce competition. I think breakups or undoing of mergers are actually called for more than we have appreciated in the last few decades, but you don't want to create new problems. CHANG: What kind of problems do you foresee? WU: In any action, there's unintended consequences. If you imagine, for example, breaking up Apple into three little mini-Apples, there might be more competition, but it might be that people's phones don't work as well or, you know, they lose whatever magic mojo they had. So you don't want to actually make things worse for people, but that said, I think breakups have the positive effect of rebooting an industry, starting things afresh and frankly, historically, have often been better for people in the long run than anyone predicted. CHANG: That's Tim Wu of Columbia Law School. He's the author of \"The Curse Of Bigness: Antitrust In The New Gilded Age. \" Thanks very much for joining us today. WU: My pleasure.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-22-769403296": {"title": "WATCH: The Military Discovered A Way To Boost Memories, And We Tried It : NPR", "url": "https://www.npr.org/2019/10/22/769403296/video-the-military-discovered-a-way-to-boost-soldiers-memories-and-we-tried-it", "author": "No author found", "published_date": "2019-10-22", "content": "DAVID GREENE, HOST: You know, many of us would just jump at the chance to improve our memories, right? That would be amazing. Our ability to recall information can be really important to our performance in so many parts of life, everything from tests in school to the ability to master a new task at work. Well, so scientists are experimenting with ways to get our minds to better hold on to the things that we have just learned. And NPR's Elise Hu got to try one of these experiments for our series about emerging technologies called Future You. Hi, Elise. ELISE HU, BYLINE: Hey. Good morning. GREENE: So what exactly did you do here? HU: Well, the idea behind this, David, is that there's something going on in our brains while we're sleeping that helps us hang onto our memories. Our brains essentially create narratives for what happened during the day, or new ideas that we were exposed to, while we're in deep sleep. So this experiment, what it does is it stimulates the brains of subjects they're studying while they're asleep. Now, so far, this research is still in the labs, and it's not commercially available, but the results have been really promising. The experiment involves applying noninvasive electrical stimulation to our brains. GREENE: Noninvasive electrical stimulation - what is that? HU: So it's kind of a brain cap that you wear while you're sleeping. I tried this out at the University of New Mexico, where I went to visit the sleep lab. GREENE: OK. HU: The researchers there introduced me to this experiment. I have to learn how to play a custom video game. And then at night, I would sleep. I'd wake up and then play the video game again. And the researchers test to see how much my performance improves. In other words, they test your memory of how to play a game. GREENE: And why did they say video games is the best way to work on your memory? HU: This all has to do with why it was designed in the first place. The U. S. military's research arm DARPA funded this study with the original purpose of wanting to improve soldiers' performance on the battlefield. So the task I learned was a VR game where you wear that VR headset that involves spotting and shooting targets. Just to review, if I get it, it'll turn green, and then if I miss a human target, then it grunts. UNIDENTIFIED RESEARCHER: Yep. HU: So I played this game, and I was tested after one night of sleep without any sort of brain stimulation. And then the second night, they zapped my brain while I was asleep, and then I played the game again. GREENE: OK. Can we talk about zapping? What is zapping? HU: OK. So the researchers put a brain cap on my head. GREENE: Shower cap with wires coming off. . . HU: Exactly. And I've worn a series of these in the Future You series. But I'm wearing that this time when I'm sleeping. And when I reach that deep stage of sleep, which is when your mind starts really consolidating memories and constructing narratives for what happened to you, the research then tracked my brainwaves and then sent electricity at the same speed as my own brainwaves back to my brain. And this is a closed loop that's supposed to improve that process of memory consolidation that already naturally occurs. So the next morning, when tested again, many subjects showed as much as a 30% improvement in recognizing and remembering targets in the VR game. GREENE: But how do they know that you're not just getting better at the game because you're playing the game more? HU: Exactly. Well, the researchers adjust the scores to account for any improvements just from practice. GREENE: OK. So what are the implications of this? I mean, it's just one kind of study with a video game. Your series, Future You, is all about imagining how all this technology advancement could change our lives in 30 years. So if they can successfully start zapping our brains when we're asleep and it improves our memory, I mean, that feels like it could be a big deal. HU: I asked exactly that question to the lead psychologist on this research at the University of New Mexico. His name is Vince Clarke. VINCE CLARKE: I think we'll get better at being able to enhance people's ability to learn, also things like pay attention, doing sports, and medical treatments like reducing pain. HU: But since we are imagining the future here, I also asked him what the downside of this kind of technology might be. GREENE: Sure. HU: For example, if your memories can be boosted, could they also be erased? And here's what Vince Clark said about that. CLARKE: You could force someone to have a memory that wasn't really true somehow, although it would take a lot more work than - what we're doing now is just enhancing a natural process. We're not really manipulating details about what you learned and all. GREENE: That sounds bad. HU: I had the same reaction. But the researchers tell me that we don't have to worry about this for a long while because it's like that quote that's often attributed to the sci-fi writer William Gibson. The future has already arrived, it's just not evenly distributed. GREENE: That's a perfect quote with which to say goodbye to Elise Hu. Thanks, Elise. HU: You bet. GREENE: And you can see this full experiment involving brain zapping and everything else for yourself as part of our original video series Future You with Elise Hu. You can find it at npr. org/futureyou, or just go to NPR's YouTube channel. DAVID GREENE, HOST:  You know, many of us would just jump at the chance to improve our memories, right? That would be amazing. Our ability to recall information can be really important to our performance in so many parts of life, everything from tests in school to the ability to master a new task at work. Well, so scientists are experimenting with ways to get our minds to better hold on to the things that we have just learned. And NPR's Elise Hu got to try one of these experiments for our series about emerging technologies called Future You. Hi, Elise. ELISE HU, BYLINE: Hey. Good morning. GREENE: So what exactly did you do here? HU: Well, the idea behind this, David, is that there's something going on in our brains while we're sleeping that helps us hang onto our memories. Our brains essentially create narratives for what happened during the day, or new ideas that we were exposed to, while we're in deep sleep. So this experiment, what it does is it stimulates the brains of subjects they're studying while they're asleep. Now, so far, this research is still in the labs, and it's not commercially available, but the results have been really promising. The experiment involves applying noninvasive electrical stimulation to our brains. GREENE: Noninvasive electrical stimulation - what is that? HU: So it's kind of a brain cap that you wear while you're sleeping. I tried this out at the University of New Mexico, where I went to visit the sleep lab. GREENE: OK. HU: The researchers there introduced me to this experiment. I have to learn how to play a custom video game. And then at night, I would sleep. I'd wake up and then play the video game again. And the researchers test to see how much my performance improves. In other words, they test your memory of how to play a game. GREENE: And why did they say video games is the best way to work on your memory? HU: This all has to do with why it was designed in the first place. The U. S. military's research arm DARPA funded this study with the original purpose of wanting to improve soldiers' performance on the battlefield. So the task I learned was a VR game where you wear that VR headset that involves spotting and shooting targets. Just to review, if I get it, it'll turn green, and then if I miss a human target, then it grunts. UNIDENTIFIED RESEARCHER: Yep. HU: So I played this game, and I was tested after one night of sleep without any sort of brain stimulation. And then the second night, they zapped my brain while I was asleep, and then I played the game again. GREENE: OK. Can we talk about zapping? What is zapping? HU: OK. So the researchers put a brain cap on my head. GREENE: Shower cap with wires coming off. . . HU: Exactly. And I've worn a series of these in the Future You series. But I'm wearing that this time when I'm sleeping. And when I reach that deep stage of sleep, which is when your mind starts really consolidating memories and constructing narratives for what happened to you, the research then tracked my brainwaves and then sent electricity at the same speed as my own brainwaves back to my brain. And this is a closed loop that's supposed to improve that process of memory consolidation that already naturally occurs. So the next morning, when tested again, many subjects showed as much as a 30% improvement in recognizing and remembering targets in the VR game. GREENE: But how do they know that you're not just getting better at the game because you're playing the game more? HU: Exactly. Well, the researchers adjust the scores to account for any improvements just from practice. GREENE: OK. So what are the implications of this? I mean, it's just one kind of study with a video game. Your series, Future You, is all about imagining how all this technology advancement could change our lives in 30 years. So if they can successfully start zapping our brains when we're asleep and it improves our memory, I mean, that feels like it could be a big deal. HU: I asked exactly that question to the lead psychologist on this research at the University of New Mexico. His name is Vince Clarke. VINCE CLARKE: I think we'll get better at being able to enhance people's ability to learn, also things like pay attention, doing sports, and medical treatments like reducing pain. HU: But since we are imagining the future here, I also asked him what the downside of this kind of technology might be. GREENE: Sure. HU: For example, if your memories can be boosted, could they also be erased? And here's what Vince Clark said about that. CLARKE: You could force someone to have a memory that wasn't really true somehow, although it would take a lot more work than - what we're doing now is just enhancing a natural process. We're not really manipulating details about what you learned and all. GREENE: That sounds bad. HU: I had the same reaction. But the researchers tell me that we don't have to worry about this for a long while because it's like that quote that's often attributed to the sci-fi writer William Gibson. The future has already arrived, it's just not evenly distributed. GREENE: That's a perfect quote with which to say goodbye to Elise Hu. Thanks, Elise. HU: You bet. GREENE: And you can see this full experiment involving brain zapping and everything else for yourself as part of our original video series Future You with Elise Hu. You can find it at npr. org/futureyou, or just go to NPR's YouTube channel.", "section": "Video: Future You, With Elise Hu", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-22-772082415": {"title": "Book Review: Former Cambridge Analytica Director Says She Saw Company Techniques 'As Savvy' : NPR", "url": "https://www.npr.org/2019/10/22/772082415/former-cambridge-analytica-director-says-she-saw-company-techniques-as-savvy", "author": "No author found", "published_date": "2019-10-22", "content": "", "section": "Book Reviews", "disclaimer": ""}, "2019-10-23-772710977": {"title": "Quantum Supremacy Achieved, Says Google; IBM Pushes Back : NPR", "url": "https://www.npr.org/2019/10/23/772710977/google-claims-to-achieve-quantum-supremacy-ibm-pushes-back", "author": "No author found", "published_date": "2019-10-23", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-23-772024008": {"title": "CubeSats Are Small Yet Mighty Important In Space Exploration : NPR", "url": "https://www.npr.org/2019/10/23/772024008/itty-bitty-satellites-take-on-big-time-science-missions", "author": "No author found", "published_date": "2019-10-23", "content": "NOEL KING, HOST: Tiny satellites called CubeSats are starting to play a big part in exploring space. CubeSats are really small. They're just about twice as big as a Rubik's Cube, but electronics are getting ever more mini, and it's possible to pack a sophisticated mission into a tiny satellite. NPR's Joe Palca has more. JOE PALCA, BYLINE: Aerospace giants use spiffy advertising to attract top engineering talent. With CubeSats, it was different - at least at the start. HANNAH GOLDBERG: I saw a flyer on a bus stop that said, want to build a satellite? PALCA: Hannah Goldberg saw that flyer when she was an undergraduate engineering major in 1999. Now she works at GomSpace, a Danish satellite company making CubeSats for the European Space Agency. GOLDBERG: In the beginning, the early days of CubeSats, they kind of had a bad reputation. People didn't think you could do much science or much engineering benefit with them. PALCA: CubeSats are basically a box with fixed dimensions. You can put anything you like inside. And since they were initially intended for students learning the principles of aerospace engineering, no big deal if they broke after a few months in space. But Goldberg says with the advent of smartphones, engineers started getting really good at packing a bunch of electronics into a small space. CubeSats started getting more sophisticated, and scientists started to take notice. Barbara Cohen is a planetary scientist at NASA's Goddard Space Flight Center. She says, at first, scientists were cautious about a CubeSat mission. BARBARA COHEN: We want to get science out of it. We don't want it to be throwing something into space for the sake of in three months becoming space debris. PALCA: Now she thinks CubeSats are ready for prime time. COHEN: It's a miniature spacecraft. That's actually the way we think about it. PALCA: Cohen is working on a mission called Lunar Flashlight. It's actually made up of six CubeSats joined together, about as big as a family-sized cereal box. COHEN: Lunar Flashlight is designed to look for exposed water frost in the permanently shadowed regions of the moon. PALCA: The spacecraft will shine a laser into those regions' deep craters at the moon's south pole. One of the reasons CubeSats can be so small and light is they can hitch a ride on other missions' rockets. But Cohen says for the moon or some other spot outside Earth orbit, you still need to be able to change course. COHEN: And about one-third of our satellite is dedicated to our propulsion system. PALCA: Lunar Flashlight uses fairly standard thrusters to change course, but Tiffany Russell Lockett says there are a lot of ways to propel yourself through space. TIFFANY RUSSELL LOCKETT: We are using a solar sail as our primary propulsion system. PALCA: Lockett is an engineer at NASA's Marshall Space Flight Center. LOCKETT: A solar sail is a large, thin film, reflective surface. Think of, like, a sailboat or a large kite. But instead of using wind to propel itself, it uses sunlight. PALCA: The sail is folded up for launch and unfurls once in space. Lockett's CubeSat is headed for an asteroid. Once it gets there. . . LOCKETT: We take pictures. PALCA: One reason scientists like CubeSat missions is they're relatively cheap, so you can have them all to yourself. Evgenya Shkolnik is an astronomer at Arizona State University. For now, she's using the Hubble Space Telescope to study stars that might have planets around them. EVGENYA SHKOLNIK: Hubble is shared by hundreds of people every year. And even though our team has a large - what's considered a large program on Hubble, it's still only about a week of time. PALCA: That's why Shkolnik is working on a CubeSat. SHKOLNIK: This way, if you build what you need for one very clear experiment, then you can have the full year to do one experiment really well. PALCA: Shkolnik thinks more scientists will be turning to miniature satellites in the future. Joe Palca, NPR News. (SOUNDBITE OF CERULEAN SKIES' \"THEY DO EXIST\") NOEL KING, HOST:  Tiny satellites called CubeSats are starting to play a big part in exploring space. CubeSats are really small. They're just about twice as big as a Rubik's Cube, but electronics are getting ever more mini, and it's possible to pack a sophisticated mission into a tiny satellite. NPR's Joe Palca has more. JOE PALCA, BYLINE: Aerospace giants use spiffy advertising to attract top engineering talent. With CubeSats, it was different - at least at the start. HANNAH GOLDBERG: I saw a flyer on a bus stop that said, want to build a satellite? PALCA: Hannah Goldberg saw that flyer when she was an undergraduate engineering major in 1999. Now she works at GomSpace, a Danish satellite company making CubeSats for the European Space Agency. GOLDBERG: In the beginning, the early days of CubeSats, they kind of had a bad reputation. People didn't think you could do much science or much engineering benefit with them. PALCA: CubeSats are basically a box with fixed dimensions. You can put anything you like inside. And since they were initially intended for students learning the principles of aerospace engineering, no big deal if they broke after a few months in space. But Goldberg says with the advent of smartphones, engineers started getting really good at packing a bunch of electronics into a small space. CubeSats started getting more sophisticated, and scientists started to take notice. Barbara Cohen is a planetary scientist at NASA's Goddard Space Flight Center. She says, at first, scientists were cautious about a CubeSat mission. BARBARA COHEN: We want to get science out of it. We don't want it to be throwing something into space for the sake of in three months becoming space debris. PALCA: Now she thinks CubeSats are ready for prime time. COHEN: It's a miniature spacecraft. That's actually the way we think about it. PALCA: Cohen is working on a mission called Lunar Flashlight. It's actually made up of six CubeSats joined together, about as big as a family-sized cereal box. COHEN: Lunar Flashlight is designed to look for exposed water frost in the permanently shadowed regions of the moon. PALCA: The spacecraft will shine a laser into those regions' deep craters at the moon's south pole. One of the reasons CubeSats can be so small and light is they can hitch a ride on other missions' rockets. But Cohen says for the moon or some other spot outside Earth orbit, you still need to be able to change course. COHEN: And about one-third of our satellite is dedicated to our propulsion system. PALCA: Lunar Flashlight uses fairly standard thrusters to change course, but Tiffany Russell Lockett says there are a lot of ways to propel yourself through space. TIFFANY RUSSELL LOCKETT: We are using a solar sail as our primary propulsion system. PALCA: Lockett is an engineer at NASA's Marshall Space Flight Center. LOCKETT: A solar sail is a large, thin film, reflective surface. Think of, like, a sailboat or a large kite. But instead of using wind to propel itself, it uses sunlight. PALCA: The sail is folded up for launch and unfurls once in space. Lockett's CubeSat is headed for an asteroid. Once it gets there. . . LOCKETT: We take pictures. PALCA: One reason scientists like CubeSat missions is they're relatively cheap, so you can have them all to yourself. Evgenya Shkolnik is an astronomer at Arizona State University. For now, she's using the Hubble Space Telescope to study stars that might have planets around them. EVGENYA SHKOLNIK: Hubble is shared by hundreds of people every year. And even though our team has a large - what's considered a large program on Hubble, it's still only about a week of time. PALCA: That's why Shkolnik is working on a CubeSat. SHKOLNIK: This way, if you build what you need for one very clear experiment, then you can have the full year to do one experiment really well. PALCA: Shkolnik thinks more scientists will be turning to miniature satellites in the future. Joe Palca, NPR News. (SOUNDBITE OF CERULEAN SKIES' \"THEY DO EXIST\")", "section": "Joe's Big Idea", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-24-773060596": {"title": "BBC Launches Tor Mirror Site To Thwart Media Censorship : NPR", "url": "https://www.npr.org/2019/10/24/773060596/bbc-launches-tor-mirror-site-to-thwart-media-censorship", "author": "No author found", "published_date": "2019-10-24", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-24-759902041": {"title": "China's Tech Giant Huawei Spans Much Of The Globe Despite U.S. Efforts To Ban It : NPR", "url": "https://www.npr.org/2019/10/24/759902041/chinas-tech-giant-huawei-spans-much-of-the-globe-despite-u-s-efforts-to-ban-it", "author": "No author found", "published_date": "2019-10-24", "content": "", "section": "World", "disclaimer": ""}, "2019-10-24-772798717": {"title": "FACT CHECK: Do Robots Or Trade Threaten American Workers More? : NPR", "url": "https://www.npr.org/2019/10/24/772798717/fact-check-do-robots-or-trade-threaten-american-workers-more", "author": "No author found", "published_date": "2019-10-24", "content": "", "section": "Politics", "disclaimer": ""}, "2019-10-25-773199525": {"title": "Using Cell Phone Numbers As A Secondary ID Can Pose Security Risks, Experts Say : NPR", "url": "https://www.npr.org/2019/10/25/773199525/sim-swap-scams-expose-risks-of-using-phones-for-secondary-i-d", "author": "No author found", "published_date": "2019-10-25", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-25-773186206": {"title": "Trump Ads On Russian Propaganda Sites: It's Not Collusion. It's The Algorithm  : NPR", "url": "https://www.npr.org/2019/10/25/773186206/trump-ads-alongside-russian-propaganda-its-not-collusion-it-s-an-algorithm", "author": "No author found", "published_date": "2019-10-25", "content": "", "section": "Politics", "disclaimer": ""}, "2019-10-25-773331294": {"title": "Facebook Launches News Site, With Hand-Picked Stories For Users : NPR", "url": "https://www.npr.org/2019/10/25/773331294/facebook-news-app-will-offer-hand-picked-stories-from-npr-other-outlets", "author": "No author found", "published_date": "2019-10-25", "content": "", "section": "Media", "disclaimer": ""}, "2019-10-25-772325133": {"title": "As President Trump Tweets And Deletes, The Historical Record Takes Shape : NPR", "url": "https://www.npr.org/2019/10/25/772325133/as-president-trump-tweets-and-deletes-the-historical-record-takes-shape", "author": "No author found", "published_date": "2019-10-25", "content": "", "section": "National", "disclaimer": ""}, "2019-10-25-760487476": {"title": "Researchers Are Using Artificial Intelligence To Stop African Elephant Poachers : NPR", "url": "https://www.npr.org/2019/10/25/760487476/elephants-under-attack-have-an-unlikely-ally-artificial-intelligence", "author": "No author found", "published_date": "2019-10-25", "content": "DINA TEMPLE-RASTON (HOST): From NPR, this is I'LL BE SEEING YOU, a four-part series about the technologies that watch us. I'm Dina Temple-Raston. A few years ago, there was huge news for elephant researchers. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED RESEARCH TEAM MEMBER #1 (RESEARCH TEAM MEMBER): A new study, the Great Elephant Census, suggests a failure to protect the world's largest land mammals, elephants. TEMPLE-RASTON: And the census found that in just seven years, one-third of the savanna elephant population had disappeared. But there was one kind of elephant that they left out. PETER WREGE (CORNELL UNIVERSITY): The Great Elephant Census depended very heavily on fixed-wing small aircraft because, in the savanna, you can fly over it and count herds of elephants. That is not possible to do in the rainforest because they're under the canopy. TEMPLE-RASTON: The rainforest - where the forest elephant lives. And the forest elephant happens to be the very species that Peter Wrege from Cornell University specializes in. WREGE: Of the two African elephants, they're very much more endangered, and this is really because their ivory is the most prized of any ivory. It's denser than savanna elephant ivory, and it has a pinkish tinge to it. TEMPLE-RASTON: Now, Wrege knew that the forest elephants were at extraordinary risk. But what he didn't know was - how fast were they being killed? Where were they being killed? And if he didn't know how many forest elephants there actually were, how could he protect them? So Wrege and his team decided to do a kind of great forest elephant census, which is actually even harder than it sounds because the forest where they live is incredibly dense. WREGE: Sometimes you see them, let's say, 15 meters away from you, and they move 5 meters into the forest, and you can't see them. Somehow, they just disappear. TEMPLE-RASTON: For a long time, they felt that the best way to try to count these elephants was simply staking off a part of the forest and counting dung piles. WREGE: There's an estimate of - how often does an elephant poop? And how long does a pile of dung last? TEMPLE-RASTON: You have to really love elephants to do that one. And then there's also a DNA method, where you test the poop to try to identify individual elephants and count them that way. None of this was very efficient - or very accurate. So researchers from Cornell University decided to try something new - to just listen for them. WREGE: If we know how often an elephant gives a vocalization and we can record it, then we can spread recorders over a big area and record their vocalizations and use those numbers to count them. TEMPLE-RASTON: Wrege had 50 custom audio recorders made, divided the rainforest up into a grid and then headed to Central Africa. Every 5 square kilometers in the rainforest, they placed an audio recorder. WREGE: We put recorders 7 to 10 meters up in a tree, hanging from a tree limb. TEMPLE-RASTON: Thirty feet in the air happens to be just a little higher than an elephant standing on its hind legs can reach with its trunk. WREGE: They've destroyed them once in a while. They've stuck a tusk through them. TEMPLE-RASTON: So Wrege's team climbs these trees, straps these audio recorders in place. And then they just hit record. (SOUNDBITE OF RAINFOREST AMBIENCE)TEMPLE-RASTON: This is the actual audio from one of those recorders. WREGE: We record anything that makes an acoustic signature, including things that are not vocalizations, like the pounding on a tree buttress by chimpanzees. (SOUNDBITE OF CHIMPANZEES SCREECHING, POUNDING ON TREES)TEMPLE-RASTON: So these tape recorders are designed to record for about three months, after which Wrege sends the teams back into the forest to climb the trees and. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED RESEARCH TEAM MEMBER #1: (Vocalizing). WREGE: Bring the recording units back down, change batteries, change the SD card so we have the recordings and put them back up in the trees. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED RESEARCH TEAM MEMBER #1: (Vocalizing)UNIDENTIFIED RESEARCH TEAM MEMBER #2 (RESEARCH TEAM MEMBER): (Foreign language spoken). TEMPLE-RASTON: Wrege says that every time they get one of these recordings back, it's exciting. WREGE: It's a combination of excitement and kind of apprehension because 50 recorders recording 24 hours a day for three months is a lot of stuff to get through. TEMPLE-RASTON: A lot of chimps and frogs and birds, just to hear what he really wanted - recorded elephants. (SOUNDBITE OF ELEPHANT TRUMPETING)WREGE: Just like - whoa, we've got it. (SOUNDBITE OF ELEPHANT TRUMPETING)TEMPLE-RASTON: Wrege was incredibly excited that his giant acoustic census might just work. And then he's thinking, how can I possibly get through everything I've recorded? Wrege had a classic big-data dilemma. In AI circles, he was having something called a cocktail party problem. (SOUNDBITE OF FILM, \"BREAKFAST AT TIFFANY'S\")AUDREY HEPBURN (ACTOR): (As Holly Golightly) Mike, darling, I tried reaching you all day long. TEMPLE-RASTON: He needed to find specific things in a huge dataset, and that happens to be a perfect job for artificial intelligence. The human brain has this amazing way of focusing on a specific person's voice and then magically amplifying it. (SOUNDBITE OF FILM, \"BREAKFAST AT TIFFANY'S\")GEORGE PEPPARD (ACTOR): (As Paul Varjak) Well, he did mentioned something about calling the police. TEMPLE-RASTON: Wrege needed to find a way to have a computer do that. And it turns out that a subset of AI, something called neural networks, is really good at that. All you have to do is turn the sound you're trying to analyze into a picture. (SOUNDBITE OF FILM, \"BREAKFAST AT TIFFANY'S\")PEPPARD: (As Paul Varjak) OK. The party's over. Out. TEMPLE-RASTON: Now, there's something called a spectrogram, which is exactly that - a ghostly little picture of a soundwave. So the AI analyzes the spectrogram, recognizes the characteristics of the soundwave, focuses in on it, amplifies it. A researcher at University of California, Santa Cruz was having a cocktail party problem of his own. He had collected huge volumes of bird songs. MATTHEW MCKOWN (BEHAVIORAL ECOLOGIST, UNIVERSITY OF CALIFORNIA, SANTA CRUZ): You know, sitting on mountaintops with tape recorders. (SOUNDBITE OF BIRDS CHIRPING)TEMPLE-RASTON: That's behavioral ecologist Matthew McKown. He studies obscure birds. MCKOWN: I was one of the few people in the world, I think, that used MiniDiscs. I was super excited about MiniDisc recorders. TEMPLE-RASTON: And he recorded all this information and then thought to himself, now what? MCKOWN: So I started to look for ways that we could process the data more easily. TEMPLE-RASTON: And that's when he and a colleague started talking about neural networks. So neural networks are called neural networks because they process things like the brain does. The easiest way to think about it is in terms of layers, one on top of another. Neurons in the first layer of the network would likely recognize something simple, like pitch or modulation; something that might characterize a bird or an elephant or. . . (SOUNDBITE OF MUSIC)TEMPLE-RASTON: . . . For the purposes of our discussion, a particular instrument. . . (SOUNDBITE OF MUSIC)TEMPLE-RASTON: . . . Say, violins. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: And to find violins in an orchestral piece of music, it might have to start to look for that modulation. The next layer of neurons might look for a range of notes that it's learned is associated with violins. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: And it thinks, hm, I recognize some notes that a violin might play. And then one of these neurons lights up. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Then middle layers would build on that, focusing in on the other qualities that might be associated with the violin, like the presence of strings. It might pick out a cello or a piano for their string-like sound. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: There can be hundreds and hundreds of layers, each getting progressively more refined. Some, perhaps, filtering out things that the network decides definitely are not violins, like, say, percussion. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: It says to itself, this is not a violin. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: And all this gets melded together with one layer after another, taking the network closer and closer to its goal - recognizing the violin. . . (SOUNDBITE OF MUSIC)TEMPLE-RASTON: . . . Until, at the end, the network looks at all the information that it used to filter down to the violin, and it makes a statistical calculation. What's the likelihood that this pattern I've identified is a violin or an elephant or a bird - 50%; 80% - until it gets what it wants. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Violins. (SOUNDBITE OF MUSIC)MCKOWN: This whole field is called deep learning. TEMPLE-RASTON: That's Matthew McKown again. MCKOWN: Because of the advent of sort of modern computing technology, you can make these neural networks that have many, many layers of neurons. And so you can start to recognize real fine-scale patterns. TEMPLE-RASTON: McKown ended up starting a company called Conservation Metrics to help people like Peter Wrege measure various things in conservation. MCKOWN: And specifically, what we do is turn that information into actionable information for people on the ground. TEMPLE-RASTON: So Wrege's 50 recorders running for three months produce about 100,000 hours of audio. McKown says the neural network can rip through that in about four hours. Thank you, cloud computing. And then it finds something like this. (SOUNDBITE OF ELEPHANTS RUMBLING)WREGE: That, actually, is a fantastic example of two females who are performing what we call a greeting ceremony. TEMPLE-RASTON: Again, Peter Wrege - and he says this sequence of sounds - they're known as rumbles - happen when elephants who have been separated for some time. . . (SOUNDBITE OF ELEPHANTS RUMBLING)TEMPLE-RASTON: . . . Come together again. WREGE: I think it's very much like if you run into a friend on the street, you know, that you haven't seen for a while - whoa, how are you? And, oh, I'm OK. What about you? Oh, it's not so good. I've lost my job. Oh, my God. You know, who knows what they're really saying, but it's that - perhaps that kind of thing. TEMPLE-RASTON: But remember; the reason Wrege wanted to listen to the forest elephants was because he wanted to protect them. And while he was listening to months and months of rain forest audio, he also heard something rather shocking - gunshots. (SOUNDBITE OF GUNSHOT)TEMPLE-RASTON: Now, it's impossible to know if an elephant died from these particular gunshots. But Wrege decided that if his team was going to count elephant rumbles, they should also find a way to count the gunshots because they could be a pretty good proxy for poaching attempts. So he has McKown building another neural network that will learn how to tell the difference between the sound of a gunshot. . . (SOUNDBITE OF GUNSHOT)TEMPLE-RASTON: . . . And the sound of a branch breaking. (SOUNDBITE OF BRANCH BREAKING)TEMPLE-RASTON: The great acoustic forest elephant count is still going on. The neural network is still training, and getting an accurate count depends on lots of things - weather, the season, where in the forest they're listening. So there isn't a precise number yet, but the Elephant Listening Project has already started learning things that can help fight poaching. For example, Wrege's team has found that elephants don't go to some parts of the forest during specific times of the year. WREGE: You can say, OK, we know that elephants are not using this huge part of this park for these seven months. We don't need to send any anti-poaching teams there because no poachers are going to find an elephant anyway. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: And every time they get a new set of recordings and feed it into the neural network, they get closer to an accurate count. Wrege imagines the network learning to distinguish elephant calls and to be able to tell if they're in distress or danger. WREGE: We have two gunshots here or we have AK-47 shots here. If that can be fed out of the rainforest in real time, then the anti-poaching people know where to go to intercept that poacher that just killed an elephant. TEMPLE-RASTON: Do you think that AI is going to save the elephants? WREGE: I actually do. It definitely is going to be our salvation. TEMPLE-RASTON: This is I'LL BE SEEING YOU from NPR, a series about the technologies that watch us. When we come back, how artificial intelligence is helping rangers stay one step ahead of poachers. And we go to a park in Malawi where they're starting to use algorithms to predict where poachers will go before they even fire a shot. I'm Dina Temple-Raston. Stay with us. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: From NPR, this is I'LL BE SEEING YOU, a show about the technologies that watch us. I'm Dina Temple-Raston. And on today's show, we've been looking at how artificial intelligence is saving the elephant. AI is helping environmentalists and researchers process huge amounts of data and mine it for information because AI doesn't get tired like humans do, and it may be better at teasing out patterns in near real time. They're testing this theory in southern Malawi. And we went to see for ourselves. (SOUNDBITE OF ENGINE REVVING)TEMPLE-RASTON: Liwonde National Park is about 200 square miles of fields and forests that run alongside the Sheri River. And the night we arrived, the first thing we did was go out on the water. CRAIG REID (MANAGER, LIWONDE NATIONAL PARK): All right, just a quick safety thing for everybody, please. TEMPLE-RASTON: Craig Reid is the manager of Liwonde National Park. REID: There are lots of crocodiles here in this river. TEMPLE-RASTON: We go out on the river at sunset, and we're skimming along the water in a metal-bottom boat. REID: And people are very much on the menu, so don't get tempted to put your hand out and run your fingers through the water or anything romantic like that - not a good idea. TEMPLE-RASTON: And the river is glassy, and we cut through reeds close to the shore and watch all the animals coming out. Oh, look; there are hippos there. So as the boat's going by, they're just dropping into the water, holding their breaths and then coming up and snorting all the air out. There are some elephants on the opposite bank, splashing in the water. Liwonde has a huge number of elephants. And to get close to one, you really have to go on land and use that old-fashioned tracking method we talked about earlier in the show. We saw the fresh dung piles along the road and followed those and found him in this clearing. An enormous elephant is right in front of us. You'd think an elephant is so huge that you'd hear him walking all the time. But what they say is the only time you hear an elephant is when he's coming and he breaks a branch, or when he's eating. And what we just got was an elephant eating. Liwonde National Park is full of scenes just like that. In addition to hippos and elephants, it has a rare black rhino sanctuary, baboons of every variety, gazelles, water bucks, warthogs. They recently reintroduced lions into the park, and they have some cheetahs. Seeing all this, it's hard to believe that just four years ago, Liwonde was a failed park. Here's park manager Craig Reid again. REID: So I always described Liwonde as we found it as being in a state of terminal decline. TEMPLE-RASTON: The buildings, roads, infrastructure - they were all falling apart. REID: So effectively, what would have happened had we not intervened would be a total elimination of all wildlife over the 10-year period following. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Four years ago, a nonprofit called African Parks wanted to try to bring Liwonde back from the brink. And it so happens that around that same time, one of the founders of Microsoft, Paul Allen, was just wrapping up the Great Elephant Census in effort to count all the elephants in Africa. Allen started a company whose goal was to find ways to use technology to save the elephants. He called the company Vulcan and hired people who love elephants, like Pawan Nrisimha. PAWAN NRISIMHA (DIRECTOR OF PRODUCT MANAGEMENT, VULCAN): Who doesn't love elephants, right? I mean, I love elephants. TEMPLE-RASTON: Nrisimha was a coder. Now he's the director of product management at Vulcan. He and the technologists there started building an AI solution for this elephant problem; a solution that would take all the information that park rangers, like Craig Reid, had both in their heads and in daily reports and then make it smarter. They called the program EarthRanger, Nrisimha explains. NRISIMHA: EarthRanger was entirely customer driven. And when I say customers here, it's essentially the park rangers. TEMPLE-RASTON: The Craig Reids - the idea was to feed all this information into a very accessible AI and machine learning platform. NRISIMHA: So it's a real-time visualization of where all the park assets are. And when I say park assets, it's the rangers. It's the animals. It's information from various sensors. TEMPLE-RASTON: Things like security cameras, GPS locations of vehicles, motion-activated sensors on park gates - so that was the first part; a real-time data visualization of the entire park. NRISIMHA: The second part is providing them analytics tools to be able to manage their patrols better. So these are things like heat maps that helps them - OK, these are the areas where most of the snaring or animal traps are happening, or these are the areas where there are lots fence breaks happening. TEMPLE-RASTON: That allows them to respond to security problems in the park just by clicking a mouse on the EarthRanger screen and running those incidents back, kind of like playing an interactive video game. And then there is an artificial intelligence piece. NRISIMHA: Gathering all this data and then we are trying to see how we can really let this artificial intelligence or predictive analytics to proactively tell the park management how to do the patrols and manage the security effort better. TEMPLE-RASTON: So in other words, take all this information they're gathering so a computer can learn about the rhythms of Liwonde Park and eventually offer suggestions on how to keep it safe. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Liwonde National Park has been using EarthRanger for the past two years. And we went to see it in action in a little brick building behind the ranger headquarters. (SOUNDBITE OF WALKIE TALKIE BEEPING)UNIDENTIFIED PERSON #2: (Foreign language spoken). TEMPLE-RASTON: If you've ever been in a command center of a small police department, this is a little like that. There are flat screens on the wall, closed circuit television monitors and, on two long tables, a series of computers analyzing and categorizing information coming into headquarters. Liwonde's operations manager is a man named Lawrence Munro, and part of his job is to plan and schedule ranger patrols. He's been using EarthRanger to help him figure out where to deploy them. LAWRENCE MUNRO (OPERATIONS MANAGER, LIWONDE NATIONAL PARK): If you can kill the (unintelligible) for this circle. TEMPLE-RASTON: To get an idea of what I'm seeing, there's a real-time satellite image of the park up on a flat screen. It shows the river, clearings, forests. MUNRO: Message (unintelligible). TEMPLE-RASTON: And there are little elephant icons tracking GPS location signals from collared elephants and little rhino icons in a sanctuary at the center of the park. MUNRO: Because we will use these guys for some operations that are coming up in this circle. TEMPLE-RASTON: Munro and enforcement chief Paul Chidyera move back and forth between a whiteboard at the back of the room, and they populate the EarthRanger screen with GPS locations of snares and footprints rangers had found in the park in the past month. MUNRO: You can have a look on the spreadsheet to just have a look what areas are worse affected (unintelligible). TEMPLE-RASTON: Munro starts clicking a mouse through various EarthRanger menus. MUNRO: I'd say we're going to look across the previous moon phase. TEMPLE-RASTON: He moves through an animation of suspicious activity in the park. MUNRO: And then if I can ask you to run the snares from the 15th of March, maybe just a tad. . . TEMPLE-RASTON: You can see a concentration of snares. The little icons look like little Western lassos, and there are a bunch of them right off the main road. MUNRO: So you can see those snares are out there (unintelligible). TEMPLE-RASTON: Munro and Chidyera decide to set up new checkpoints right where EarthRanger recorded a huge number of snares. If nothing else, that would discourage whoever put them there from setting more of them. MUNRO: So the old system was a map on the wall. I still have it in my office. And a similar tough process, but you wouldn't be able to see exactly where all your assets are. TEMPLE-RASTON: Assets - men, planes, jeeps, helicopters. MUNRO: You have to rely a lot more on memory, a lot more radio traffic. Here, you can do a lot more pre-emptive stuff because you can see the picture. TEMPLE-RASTON: Pre-emptive stuff - what he means is anticipating where a poacher might show up in the same way that a police department might send extra patrols into a high-crime area. And while this is standard operating procedure in policing, it wasn't really a focus in conservation until recently. And just like police departments have started trying to use computer analytics to find crime patterns, rangers like Munro are doing that, too. But it turns out that poaching in Liwonde is affected by lots of different things - weather patterns, animal movements. . . MUNRO: Things like religious holidays, the payday of the government sector. TEMPLE-RASTON: Bushmeat is considered a delicacy Malawi, so people put in their orders right after they get paid. MUNRO: We pick up trends. We pick up patterns. TEMPLE-RASTON: Munro said it used to take a month to start to see patterns emerging because they had to rifle through written reports. Now they can respond to threats faster. MUNRO: We study it intently every Wednesday because we want to deploy out our guys accordingly. But you can do it daily. That's the difference. It's basically the speed at which you can strategize. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Earlier this year, the head of law enforcement of Liwonde, Paul Chidyera, got an alert on his cellphone. A sensor linked to EarthRanger detected some suspicious movement in the eastern part of the park, so he and his team set up something called a poacher cam. It's basically a motion-activated camera. PAUL CHIDYERA (HEAD OF LAW ENFORCEMENT, LIWONDE NATIONAL PARK): There was an area way back when. . . TEMPLE-RASTON: Chidyera said there had been an area of the park rangers had found a lot of human tracks. So you can imagine if you just put up a motion-activated camera in a game preserve, you'll be getting lots of pictures of animals that go by. But these cameras have an algorithm built inside that helps a camera determine whether whatever's going past has a human shape or an animal one. If it looks human, it snaps a picture and sends it with GPS coordinates to EarthRanger and staff cellphones. The poacher cam got a photo of someone coming in, and Chidyera's team went to a nearby village and found him. CHIDYERA: When he was arrested, he was confronted. And he revealed what he has been doing. TEMPLE-RASTON: As fate would have it, he was a well-known poacher in the area. In fact, as they scrolled through EarthRanger's database, they found other pictures of him coming into the park with weapons. Those pictures were submitted as evidence in his trial. Each photo was used to support a different criminal count of trespass and weapons charges. In the end, the poacher was sentenced to 27 years as a repeat offender. As a tourist, elephants may seem magical, but it's important to understand that to people who live right next to them, they can be a nuisance or even a threat. And I got a real sense of this walking the fence line along Liwonde Park. Workers put this up? CHIDYERA: Yes. TEMPLE-RASTON: So they're all from villages around here? CHIDYERA: From the villages. TEMPLE-RASTON: I'm getting a tour of a relatively new fence at Liwonde Park when we run into this local guy. Can we ask you, sir? Can we talk? His name is Various Donzani. You live close by? VARIOUS DONZANI (LIWONDE RESIDENT): Yes, my house is 50 meters from the fence. TEMPLE-RASTON: Fifty meters from the fence? DONZANI: Fifty meters. TEMPLE-RASTON: So before the fence, were there elephants that came around your house? DONZANI: Oh, very much, especially when they come through a garden where there is food. We had problems. TEMPLE-RASTON: Donzani is a school teacher. But like a lot of people on the edge of the park, he relies on his own garden for food, which attracts elephants. One elephant can wipe out a village garden in a couple of hours. Do you have an experience with an elephant. . . DONZANI: Yes. \rTEMPLE-RASTON: . . . That you can tell us about? DONZANI: It's fortunate that I'm not killed. TEMPLE-RASTON: These elephant-human conflicts - that's what they're called - are really dangerous. After hippos, elephants kill more humans in Africa than any other animal. And Donzani remembers one horrible day before the fence was up that a herd of elephants came stampeding through his village. DONZANI: I tell you, it was a disaster. Seven people were killed that day. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: This is where poaching can get complicated. It often begins with locals slipping into the park just to hunt meat to eat - a Thomson's gazelle here, a water buck there. And then it's gales up. Criminal syndicates move in and recruit locals to help them find elephants or a black rhino. And the financial incentive is hard to resist. The tusks can fetch literally hundreds of thousands of dollars apiece on the black market. The human-elephant conflict plays into this as well. Smugglers and international cartels will often find local farmers who have just had their crops destroyed and then ask them for help to kill an elephant. They will say they heard they just lost everything. But if you help us find an elephant, we'll give you enough money to buy food for your family. It's easy to see where the incentive might come from. (SOUNDBITE OF ELEPHANT TRUMPETING)TEMPLE-RASTON: This is actual sound from a video of an elephant charging people in Liwonde. If someone tries to shoo an elephant off their fields, this is what they face. (SOUNDBITE OF ELEPHANT TRUMPETING)TEMPLE-RASTON: And being killed by an elephant is a horrifying thing. An adult female elephant weighs over three tons; a bull elephant is typically over six - park manager Craig Reid. REID: These big bulls will charge a person. And if they manage to catch them, which is not that difficult considering that an elephant can run about 60 kilometers an hour, which is. . . TEMPLE-RASTON: That's about twice as fast as the fastest human being. REID: Most people are relatively catchable. And when people are caught by an elephant, they're normally - using their trunk, knock them to the ground and then kneel on them, crushing them either with their knees or with the base of their trunk, which, at that point, is all curled up and almost like a fist. TEMPLE-RASTON: Lawrence Munro remembers that just a few years ago, elephants were leaving the park all the time, and locals would call in sightings. MUNRO: They would see elephants - repeat offenders, so to speak. They can identify the animals by a nick in their ear or whatever the case is. And then they would tell us - they'd say, that elephant there breaks out often. Soon as it's got a collar on it, we can watch it from here. And then we can make informed decisions regarding that elephant. We don't want elephants outside the park. They damage crops. They injure people. We want those people on our side. TEMPLE-RASTON: Because EarthRanger is tracking the elephants, it's easier to chase them away from the fence line before they get out of the park. And that has already started changing their behavior. They used to break out of the park any time they felt like it. Now they do it at night, when the ranger staff might be smaller. And they're back in the park by daylight. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: So how is EarthRanger doing? In the past two years, poaching in Liwonde has plummeted. The park hasn't lost a single high-value animal in 30 months. Those elephant trips outside the park - those are also way down. And while all this has been going on, EarthRanger's machine learning algorithm has been training. By the end of the year, Nrisimha thinks the program will have ingested enough data to start what they call pre-bang enforcement, something conservationists think will change poaching as we know it. NRISIMHA: Post-bang is you see a poacher come in and fired a shot, and then you hear it maybe because you have some sensors there, or maybe one of your camera traps on the field saw a poacher come in. TEMPLE-RASTON: You remember Nrisimha, the elephant lover who works for the company that created EarthRanger. NRISIMHA: So that's post-bang. You might be successful in that case to intercept the poacher, but the animal would have been hurt. TEMPLE-RASTON: Pre-bang is just like it sounds. It's about using behavioral analysis, artificial intelligence and predictive analytics to intercept poachers before they even get into the park. Among other things, EarthRanger's program will automatically generate the patrol routes that we heard Munro and Chidyera putting together in the control room. And instead of just focusing on places where there's already been criminal activity, the routes will be anticipated based on patterns EarthRanger's recognized. It will allow rangers to intercept bad guys pre-bang. Poachers will go to where they believe the elephants will be, and they'll find rangers there instead. NRISIMHA: The biggest difficulty here is really gathering up that data to train up that artificial intelligence model to know what to predict. TEMPLE-RASTON: It takes a long time for AI to learn. Now that EarthRanger has been deployed in Liwonde for two years, it's getting close to having enough data to start making predictions. NRISIMHA: If I had to give a number, I would say in about a year or two, we would start seeing solid results and parks actually using this data to help manage their parks or manage their patrols. TEMPLE-RASTON: But one thing that always looms over these high-tech solutions - what if they get hacked? One of the most vulnerable technologies is this. (SOUNDBITE OF RADIO STATIC)TEMPLE-RASTON: Don't adjust your radio. That's actually something called radio telemetry at work. The tracking collars they use to keep tabs on animals - the transmitter they use often sends out a signal in the form of a radio wave. And you use a receiver to zero-in on that signal. REID: Andrea is just flicking through the various frequencies. And she's now trying to get a response from one of the collars. TEMPLE-RASTON: That's Craig Reid again. And his wife, Andrea, is in charge of special projects there, including keeping track of the animals. ANDREA REID (PROJECT MANAGER, NATIONAL PARK): A lot of the collars actually have different beeps to them. TEMPLE-RASTON: The receiver has a directional antenna. And then all you do is listen. REID: So on the telemetry device here, you have a button that's called the gain. And when you turn the gain down and the beep is quite loud, it means that you're getting closer to the animal. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: The problem is hacking a transponder is fairly easy. Poachers are known to buy their own antennas and receivers and then do exactly what Andrea is doing. You can imagine, if a rhino horn or elephant tusk can fetch hundreds of thousands of dollars on the black market, there's a huge incentive for crime syndicates to come up with creative ways to locate high-value animals. REID: There's evidence of poachers now actually using the same technologies which are used by conservation officials. In a sense, we're involved in a bit of an arms race with poachers and poaching syndicates. TEMPLE-RASTON: From NPR, this is I'LL BE SEEING YOU, a four-part series about the technologies that watch us. I'm Dina Temple-Raston. And on today's show, we're looking at how AI is helping the elephant. Coming up, what if all this technology was hacked? DAWN SONG (PROFESSOR, UC BERKELEY): Even though deep neural networks has made tremendous advancements, it can be fooled very easily. TEMPLE-RASTON: Stay with us. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: From NPR, this is I'LL BE SEEING YOU, a four-part series about the technologies that watch us. I'm Dina Temple-Raston. And on today's show, we've been looking at how AI has been helping the elephant and the humans who live near them. And with all these high-tech solutions, people are now starting to worry about something else - hacking. (SOUNDBITE OF MUSIC)FRANK POPE (CEO, SAVE THE ELEPHANTS): Hello? TEMPLE-RASTON: Hello. Are you there? POPE: Hello. Is that Dina? TEMPLE-RASTON: It is. Hi. Is that Frank Pope? POPE: I've only just landed, after using some artificial intelligence to help elephants. TEMPLE-RASTON: (Laughter). Frank Pope is the CEO of an organization called Save the Elephants. You might have heard of it. It's been at the forefront of elephant conservation for decades. He was in Kenya when I called and had just finished flying over a park to check on elephants wearing special tracking collars. And I asked him whether Save the Elephants ever worried about hackers. POPE: That was one of the prime motives for diving into this project. TEMPLE-RASTON: He means the EarthRanger project we were talking about before. And he says the danger of hacking into tracking collars varies depending on whether you have elephants spread out over a large area or have them all bunched up. POPE: I've just spent three hours flying around. We think we saw about 400 elephants, and there was one collar among them. TEMPLE-RASTON: So in Kenya, where Pope is, going to the trouble of hacking into the tracking system doesn't make much sense. But if you're in South Sudan, hacking into tracking collars could help you find a huge number of the national herd. The Sudanese government keeps their elephants in close proximity to each other, so hacking a tracking collar. . . POPE: That could be catastrophic. So we have to be careful in some circumstances, but we don't have to be so careful in others. TEMPLE-RASTON: And Pope said the Save the Elephants tracking system is being run through programs like EarthRanger precisely because of the concern about hacking. To be clear, Pope said he's not just concerned about technology; he's worried about carbon units. You know them as humans. They're the biggest threat to keeping the animals safe. POPE: The weakness of all these security systems is, of course, the users. If you've got a radio room or an operations control room, the people you have in there have to be trusted, and you have to have layers of security. It's impossible to pay rangers enough to ensure that they won't be tempted by the staggering amounts of money you could get for ratting on a rhino and selling it to them if that rhino moves close to a fence, for example. And that's not about hacking so much as making sure that the right people have access to the data. TEMPLE-RASTON: That's something Craig Reid at Liwonde National Park in Malawi understood from the start. He's placed closed-circuit television cameras all around the ranger headquarters where the EarthRanger system is running. REID: That's all part of the process of trying to protect the staff that are working with information and also trying to make sure that the information doesn't leave the control room. TEMPLE-RASTON: But the dream of AI goes far beyond saving elephants. AI is everywhere. It's starting to read our X-rays and drive our cars. And researchers are just beginning to understand AI and its vulnerabilities. If that sounds familiar, it should. New technologies have always developed this way. Remember the movie \"WarGames? \"(SOUNDBITE OF FILM, \"WARGAMES\")UNIDENTIFIED PERSON #1 (VOICEOVER): He found the right code word to play the game. TEMPLE-RASTON: It came out in 1983. (SOUNDBITE OF FILM, \"WARGAMES\")UNIDENTIFIED PERSON #1: But it was the wrong computer. UNIDENTIFIED ACTOR #1 (ACTOR): (As character) Shall we play a game? UNIDENTIFIED ACTOR #2 (ACTOR): (As character) How can it ask you that? \rMATTHEW BRODERICK (ACTOR): (As David) How about global thermonuclear war? TEMPLE-RASTON: Matthew Broderick hacks into the U. S. military's threat detection system thinking he's playing a new computer game, and he almost starts a war. Crazy? Well, President Ronald Reagan was rather famously freaked out by the movie, and one of his generals told him that something like that could actually happen. It led to America's first federal law against computer hacking, the Computer Fraud and Abuse Act. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Trying to anticipate how adversaries might hack into technology is something people at the Defense Advanced Research Projects Agency, or DARPA, think a lot about. DARPA has been a kind of high-tech incubator for the military since 1958. Stealth technology - that came out of DARPA. Tank simulators, the M16 rifle - you can thank DARPA for those, too. On the creepier side, it is said to be developing what they call a bio-enhanced soldier, or supersoldier. The thing that DARPA is really famous for, though, is artificial intelligence. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: Now, back to its very beginning, DARPA has been at the forefront of support, advocacy and leadership in artificial intelligence. TEMPLE-RASTON: This is from one of their promotional videos. And of course, artificial intelligence led to the driverless car. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #3 (DARPA): The DARPA autonomous land vehicle project, which started in 1984, led to. . . TEMPLE-RASTON: DARPA's long history with AI is part of the reason why researchers there have been so focused on what can go wrong with it. The agency launched a major project earlier this year to try to make AI more resilient because right now it is alarmingly easy to confuse an AI program. UNIDENTIFIED PERSON #4 (ARTIFICIAL INTELLIGENCE): Thirteenth floor. TEMPLE-RASTON: So I went to DARPA to meet with Hava Siegelmann. I'm Dina Temple-Raston. HAVA SIEGELMANN (PROJECT MANAGER, DARPA): Hey, Dina. I'm Hava. TEMPLE-RASTON: Thank you for making the time. She's the director of something called the GARD project. Guard stands for Guaranteeing AI Robustness Against Deception. It's a program that's trying to find ways to make artificial intelligence more hack-proof. SIEGELMANN: So when people started with AI, they were going and asking professionals how do you do this and how do you do that and try to write it as rules. TEMPLE-RASTON: Siegelmann is originally from Israel, which accounts for her accent, and she spends hours every day looking at how machines can be tricked into doing the wrong thing; not necessarily just going against its programming, but instead being taught something by adversaries that it is not supposed to learn. SIEGELMANN: When you design your machine learning computer network, you always come with the best data set that you can. TEMPLE-RASTON: The best data set that you can - so in the case of Peter Wrege, our Elephant Listening Project director, he had all those hours and hours of recordings. (SOUNDBITE OF BIRDS CHIRPING)TEMPLE-RASTON: Liwonde had reports rangers had been feeding into EarthRanger. REID: The red is for checks going in, and that amber color's for checks going out. TEMPLE-RASTON: But it turns out because AI learns in ways we don't quite understand, its decision-making process is a bit of a black box. It can be easy to fool. So earlier in the show, we talked about a subset of AI called neural networks. And we explained that it was fed millions and millions of data points to begin to understand the plucking sound of a violin. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: The tone, the rhythm, the notes - and it learned enough to be able to recognize some characteristics of violins even though they're buried in this. . . (SOUNDBITE OF MUSIC)TEMPLE-RASTON: . . . A slightly addictive, happy song. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: But let's say we want the neural network to do something a little different. We want it to recognize a specific genre of music. This time, it isn't trained to find its violins, but it's trained to tell us whether what we're hearing is orchestral music or disco. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: The neural network went through all its training and decided on maybe a dozen qualities that, in all its analysis, helps it tell orchestral music apart from disco. And maybe one of those things is an oboe. It has looked for an oboe in classical music, and it's always there - hard to find but always there. And then it listened to disco music, and it comes to the conclusion most disco music has some Barry Gibb of the Bee Gees and a host of other things but never an oboe. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: The important thing to understand is that the neural network isn't taking in all the music to figure that out. It's just zeroing in on these qualities it learned to identify. And that's the rub. If you actually know what the qualities are that it's looking for, you can fool it. You can bury something in the music that the human ear can't hear but the network might see - something like an oboe carefully placed inside the disco beat. And the network is fooled. Can you hear the oboe we snuck in there? (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Know what the AI or neural network is focusing on, tweak that and you're done. Humans don't make that mistake because they take in the totality. AI systems are easy to nudge into making the wrong inference. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: One of my favorite examples of this came from computer scientists at Carnegie Mellon University. And it's a fairly basic experiment. The CMU researchers trained a computer to use facial recognition to identify different people. The computer dutifully ingested lots of photographs of the people and identified them correctly every time. But then they put big, colorful glasses on a subject who hadn't had glasses on before, and the computer confidently misidentified him. Fooling AI can be that easy. This is the kind of thing Siegelmann thinks a lot about. SIEGELMANN: So there are examples on images that if you put dominance patch in an image, then the network will attend to this patch and will tell you. . . TEMPLE-RASTON: A dominance patch - think of it as the equivalent of a bright, shiny object that catches your attention or big, bright glasses that get the computer to focus on that instead of the bigger picture. Siegelmann says this could become a problem on the battlefield. She is with DARPA, after all. SIEGELMANN: So perhaps I'm just imagining. People can put some patch on their clothes. . . TEMPLE-RASTON: A patch on their clothes, instead of those glasses the CMU researchers added to one of their subjects - that patch could fool an AI system into thinking someone is a member of your platoon when, in fact, it's the enemy. I'm here to see Dawn Song on the 34th floor. So there's a pretty famous experiment about this conducted by UC Berkeley professor Dawn Song. Wow. This is quite a view. SONG: Yeah, the view is nice. TEMPLE-RASTON: We met in her office. And from her conference room, you can see the entire San Francisco Bay. SONG: You can actually see the Golden Gate from here. TEMPLE-RASTON: Alcatraz, the Golden Gate - Song is a long way from where she grew up in Northeast China, and one of the things that brought her to this high-rise with a great view of the bay is a short video about AI that went viral, which she made with some of her colleagues at Berkeley. SONG: Let me start playing the video. TEMPLE-RASTON: The video doesn't have any sound, and it's less than a minute long. But it has rocked the AI community because it showed just how vulnerable AI and driverless cars can be. SONG: So first let me explain what you will see in the video. So in the video, you'll see two frames side by side. TEMPLE-RASTON: Think split screen. SONG: In both frames, you'll see the vehicle is driving towards the end of the road, where there's a person holding a stop sign. TEMPLE-RASTON: The split screens are subtitled so you can see how the AI - and specifically something called image classification - is making decisions inside the autonomous car. SONG: You'll see the prediction given by the image classification system to try to predict what the traffic sign is. TEMPLE-RASTON: So sort of like the car starting to think, hm, a sign is coming. I'm going to have to make a decision. SONG: Right. TEMPLE-RASTON: So the key part of the experiment is that Song put some strategically placed stickers on one of the stop signs. There's one sticker below the S and the other above the O in stop. The other stop sign hasn't been changed. And as the car gets closer to the sign, the subtitles on the screen are showing how the AI system is interpreting what it's seeing. The AI reads the regular sign just fine. But the one with the stickers - it thinks it says, speed limit 45 miles an hour. And the car blows right through the intersection. Two carefully placed stickers was all it took to make a self-driving car run a stop sign. So you were expecting it to misread the sign. And then it did, and you were happy about that. SONG: In some sense, yes, because we created the adversarial example. So it is surprising still, given that - how well it worked. TEMPLE-RASTON: It worked so well that people who were developing driverless cars tapped the brakes, and it helped launch a movement to see how vulnerable AI really is. Now, to be fair, Song's team didn't just throw some stickers onto a sign and fool an AI system. It was a very involved and sophisticated process. They knew exactly how the AI's image classification system worked. They knew which pixels of the sign to manipulate to fool it. So you might ask, a speed limit sign is a totally different shape than a stop sign; how could two stickers fool the AI system so completely? Song says it's because AI isn't seeing the sign the way we might see a sign with our own eyes. AI sees the sign as a mathematical equation not a shape. SONG: At the end of the day, the human vision system is also a function. TEMPLE-RASTON: A function or a calculation. SONG: But it's a much more sophisticated function. And through the evolution and through learning, the human vision system is working really, really well and has learned to learn the right things. And even though deep neural networks has made tremendous advancements - and for certain vision tasks, it actually has achieved a human level of performance. But it has these issues that it can be fooled very easily. TEMPLE-RASTON: Song says there have been huge strides in AI, but it's still in its infancy. SONG: We need to understand that the machine - like, the machine learning system is not - actually, it's not as powerful as what people think. We still have a lot of work to do. TEMPLE-RASTON: Decades before there's a safe self-driving car? SONG: We do really need new and more breakthroughs before we can really get there. TEMPLE-RASTON: So would you ride in a driverless car? SONG: Not today. (LAUGHTER)SONG: I mean, I would enjoy having a test drive. But. . . (LAUGHTER)TEMPLE-RASTON: Hava Siegelmann over at DARPA has similar reservations. SIEGELMANN: If you can do things with, like, a stop sign - think that the person, or there's a group, is driving a tank, and they put the particular sticker on it. And because that sticker that has particular color - we think that this tank is actually an ambulance. And immediately, we open the gates to let the ambulance go in. TEMPLE-RASTON: The reason to study all of this isn't to scare us about AI, although it does that, too. The reason Song and Siegelmann are trying to see the limits of AI is so they can fix it, kind of like old-fashioned hackers who used to call up software companies and let them know about flaws in their coding so they could send out patches. SIEGELMANN: So the field started a few years ago when people said, wow, there is a really interesting feature of the neural network; that it's really not robust. TEMPLE-RASTON: And you know that stop sign that launched a thousand doubts about AI? It's now hanging up at the London Museum of Science and Technology. It's part of an exhibit about our driverless future. (SOUNDBITE OF MUSIC)UNIDENTIFIED SINGERS (SINGERS): (Singing) We've got to slow down, slow down. TEMPLE-RASTON: This is from a musical short General Motors produced for an auto expo in 1956, and it's all about the future of driving. (SOUNDBITE OF MUSIC)UNIDENTIFIED SINGER #1 (SINGER): (Singing) You can bet your high compression we're going to be late. TEMPLE-RASTON: And in the video, the car becomes a time machine, taking the four people in the car into the future. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTOR #3 (ACTOR): (As character) Hey, I wonder what we'd hear if I turn on the switch, and we're driving along in 1976. TEMPLE-RASTON: 1976 - modern machinery. In this case, think \"The Jetsons\" - a driverless car that looks a lot like a spaceship. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTOR #4 (ACTOR): (As character) Well done, Firebird 2. You're now under automatic control; hands-off steering. TEMPLE-RASTON: This all goes on for almost nine minutes, and 1976 came and went without a driverless car. And so did 2018. Most experts will tell you that driverless cars are more than a decade away. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTOR #5 (ACTOR): (As character) Here we go on the high-speed safety lane. TEMPLE-RASTON: The sticking point has always come down to artificial intelligence - using neural networks to identify elephant songs. . . (SOUNDBITE OF ELEPHANT RUMBLING)TEMPLE-RASTON: . . . Or to anticipate where poachers might be. . . REID: The old system was a map on the wall. TEMPLE-RASTON: . . . Allows AI to be wrong sometimes without life-threatening consequences. One of the things that researchers are looking at is whether AI can be trained to explain to us how it's making decisions. Until we understand that, researchers will tell you that AI really shouldn't be trusted with the car keys. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTOR #6 (ACTOR): (As character) Ah, this is the life - safe, cool, comfortable. Mind if I smoke a cigar? UNIDENTIFIED ACTOR #7 (ACTOR): (As character) Oh, not with this wonderful air conditioning. TEMPLE-RASTON: This is I'LL BE SEEING YOU from NPR. The show was written and hosted by me, Dina Temple-Raston. And our producer is Adelina Lancianese, and she scored the show too. Field production by Michael May. Our theme music is composed by Ramtin Arablouei. And special thanks to NPR investigations, the Story Lab, Kenny Malone, John Keefe of the Quartz AI Studio and Josephine Wolff at Tufts University. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTOR #5: (As character) The safe, easy way to make a turn. TEMPLE-RASTON: And if you missed one of our previous shows, just go to npr. org/illbeseeingyou or find us on NPR One. Next time, we get exclusive access to the U. S. military unit fighting the most secretive and deadly terrorist organization in the world. And their weapon is a computer keyboard. UNIDENTIFIED PERSON #5: You could take those over. We were going to win everything. It was a house of cards. UNIDENTIFIED PERSON #6: So within the first 60 minutes of go, I knew we were having success. And that continued, obviously, well into the night. TEMPLE-RASTON: I'm Dina Temple-Raston, and I'll be seeing you. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED SINGER #2 (SINGER): (Singing) Roger, Firebird, I'll swing you to the right; hands-on steering. Firebird, good night. DINA TEMPLE-RASTON (HOST): From NPR, this is I'LL BE SEEING YOU, a four-part series about the technologies that watch us. I'm Dina Temple-Raston. A few years ago, there was huge news for elephant researchers. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED RESEARCH TEAM MEMBER #1 (RESEARCH TEAM MEMBER): A new study, the Great Elephant Census, suggests a failure to protect the world's largest land mammals, elephants. TEMPLE-RASTON: And the census found that in just seven years, one-third of the savanna elephant population had disappeared. But there was one kind of elephant that they left out. PETER WREGE (CORNELL UNIVERSITY): The Great Elephant Census depended very heavily on fixed-wing small aircraft because, in the savanna, you can fly over it and count herds of elephants. That is not possible to do in the rainforest because they're under the canopy. TEMPLE-RASTON: The rainforest - where the forest elephant lives. And the forest elephant happens to be the very species that Peter Wrege from Cornell University specializes in. WREGE: Of the two African elephants, they're very much more endangered, and this is really because their ivory is the most prized of any ivory. It's denser than savanna elephant ivory, and it has a pinkish tinge to it. TEMPLE-RASTON: Now, Wrege knew that the forest elephants were at extraordinary risk. But what he didn't know was - how fast were they being killed? Where were they being killed? And if he didn't know how many forest elephants there actually were, how could he protect them? So Wrege and his team decided to do a kind of great forest elephant census, which is actually even harder than it sounds because the forest where they live is incredibly dense. WREGE: Sometimes you see them, let's say, 15 meters away from you, and they move 5 meters into the forest, and you can't see them. Somehow, they just disappear. TEMPLE-RASTON: For a long time, they felt that the best way to try to count these elephants was simply staking off a part of the forest and counting dung piles. WREGE: There's an estimate of - how often does an elephant poop? And how long does a pile of dung last? TEMPLE-RASTON: You have to really love elephants to do that one. And then there's also a DNA method, where you test the poop to try to identify individual elephants and count them that way. None of this was very efficient - or very accurate. So researchers from Cornell University decided to try something new - to just listen for them. WREGE: If we know how often an elephant gives a vocalization and we can record it, then we can spread recorders over a big area and record their vocalizations and use those numbers to count them. TEMPLE-RASTON: Wrege had 50 custom audio recorders made, divided the rainforest up into a grid and then headed to Central Africa. Every 5 square kilometers in the rainforest, they placed an audio recorder. WREGE: We put recorders 7 to 10 meters up in a tree, hanging from a tree limb. TEMPLE-RASTON: Thirty feet in the air happens to be just a little higher than an elephant standing on its hind legs can reach with its trunk. WREGE: They've destroyed them once in a while. They've stuck a tusk through them. TEMPLE-RASTON: So Wrege's team climbs these trees, straps these audio recorders in place. And then they just hit record. (SOUNDBITE OF RAINFOREST AMBIENCE)TEMPLE-RASTON: This is the actual audio from one of those recorders. WREGE: We record anything that makes an acoustic signature, including things that are not vocalizations, like the pounding on a tree buttress by chimpanzees. (SOUNDBITE OF CHIMPANZEES SCREECHING, POUNDING ON TREES)TEMPLE-RASTON: So these tape recorders are designed to record for about three months, after which Wrege sends the teams back into the forest to climb the trees and. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED RESEARCH TEAM MEMBER #1: (Vocalizing). WREGE: Bring the recording units back down, change batteries, change the SD card so we have the recordings and put them back up in the trees. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED RESEARCH TEAM MEMBER #1: (Vocalizing)UNIDENTIFIED RESEARCH TEAM MEMBER #2 (RESEARCH TEAM MEMBER): (Foreign language spoken). TEMPLE-RASTON: Wrege says that every time they get one of these recordings back, it's exciting. WREGE: It's a combination of excitement and kind of apprehension because 50 recorders recording 24 hours a day for three months is a lot of stuff to get through. TEMPLE-RASTON: A lot of chimps and frogs and birds, just to hear what he really wanted - recorded elephants. (SOUNDBITE OF ELEPHANT TRUMPETING)WREGE: Just like - whoa, we've got it. (SOUNDBITE OF ELEPHANT TRUMPETING)TEMPLE-RASTON: Wrege was incredibly excited that his giant acoustic census might just work. And then he's thinking, how can I possibly get through everything I've recorded? Wrege had a classic big-data dilemma. In AI circles, he was having something called a cocktail party problem. (SOUNDBITE OF FILM, \"BREAKFAST AT TIFFANY'S\")AUDREY HEPBURN (ACTOR): (As Holly Golightly) Mike, darling, I tried reaching you all day long. TEMPLE-RASTON: He needed to find specific things in a huge dataset, and that happens to be a perfect job for artificial intelligence. The human brain has this amazing way of focusing on a specific person's voice and then magically amplifying it. (SOUNDBITE OF FILM, \"BREAKFAST AT TIFFANY'S\")GEORGE PEPPARD (ACTOR): (As Paul Varjak) Well, he did mentioned something about calling the police. TEMPLE-RASTON: Wrege needed to find a way to have a computer do that. And it turns out that a subset of AI, something called neural networks, is really good at that. All you have to do is turn the sound you're trying to analyze into a picture. (SOUNDBITE OF FILM, \"BREAKFAST AT TIFFANY'S\")PEPPARD: (As Paul Varjak) OK. The party's over. Out. TEMPLE-RASTON: Now, there's something called a spectrogram, which is exactly that - a ghostly little picture of a soundwave. So the AI analyzes the spectrogram, recognizes the characteristics of the soundwave, focuses in on it, amplifies it. A researcher at University of California, Santa Cruz was having a cocktail party problem of his own. He had collected huge volumes of bird songs. MATTHEW MCKOWN (BEHAVIORAL ECOLOGIST, UNIVERSITY OF CALIFORNIA, SANTA CRUZ): You know, sitting on mountaintops with tape recorders. (SOUNDBITE OF BIRDS CHIRPING)TEMPLE-RASTON: That's behavioral ecologist Matthew McKown. He studies obscure birds. MCKOWN: I was one of the few people in the world, I think, that used MiniDiscs. I was super excited about MiniDisc recorders. TEMPLE-RASTON: And he recorded all this information and then thought to himself, now what? MCKOWN: So I started to look for ways that we could process the data more easily. TEMPLE-RASTON: And that's when he and a colleague started talking about neural networks. So neural networks are called neural networks because they process things like the brain does. The easiest way to think about it is in terms of layers, one on top of another. Neurons in the first layer of the network would likely recognize something simple, like pitch or modulation; something that might characterize a bird or an elephant or. . . (SOUNDBITE OF MUSIC)TEMPLE-RASTON: . . . For the purposes of our discussion, a particular instrument. . . (SOUNDBITE OF MUSIC)TEMPLE-RASTON: . . . Say, violins. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: And to find violins in an orchestral piece of music, it might have to start to look for that modulation. The next layer of neurons might look for a range of notes that it's learned is associated with violins. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: And it thinks, hm, I recognize some notes that a violin might play. And then one of these neurons lights up. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Then middle layers would build on that, focusing in on the other qualities that might be associated with the violin, like the presence of strings. It might pick out a cello or a piano for their string-like sound. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: There can be hundreds and hundreds of layers, each getting progressively more refined. Some, perhaps, filtering out things that the network decides definitely are not violins, like, say, percussion. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: It says to itself, this is not a violin. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: And all this gets melded together with one layer after another, taking the network closer and closer to its goal - recognizing the violin. . . (SOUNDBITE OF MUSIC)TEMPLE-RASTON: . . . Until, at the end, the network looks at all the information that it used to filter down to the violin, and it makes a statistical calculation. What's the likelihood that this pattern I've identified is a violin or an elephant or a bird - 50%; 80% - until it gets what it wants. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Violins. (SOUNDBITE OF MUSIC)MCKOWN: This whole field is called deep learning. TEMPLE-RASTON: That's Matthew McKown again. MCKOWN: Because of the advent of sort of modern computing technology, you can make these neural networks that have many, many layers of neurons. And so you can start to recognize real fine-scale patterns. TEMPLE-RASTON: McKown ended up starting a company called Conservation Metrics to help people like Peter Wrege measure various things in conservation. MCKOWN: And specifically, what we do is turn that information into actionable information for people on the ground. TEMPLE-RASTON: So Wrege's 50 recorders running for three months produce about 100,000 hours of audio. McKown says the neural network can rip through that in about four hours. Thank you, cloud computing. And then it finds something like this. (SOUNDBITE OF ELEPHANTS RUMBLING)WREGE: That, actually, is a fantastic example of two females who are performing what we call a greeting ceremony. TEMPLE-RASTON: Again, Peter Wrege - and he says this sequence of sounds - they're known as rumbles - happen when elephants who have been separated for some time. . . (SOUNDBITE OF ELEPHANTS RUMBLING)TEMPLE-RASTON: . . . Come together again. WREGE: I think it's very much like if you run into a friend on the street, you know, that you haven't seen for a while - whoa, how are you? And, oh, I'm OK. What about you? Oh, it's not so good. I've lost my job. Oh, my God. You know, who knows what they're really saying, but it's that - perhaps that kind of thing. TEMPLE-RASTON: But remember; the reason Wrege wanted to listen to the forest elephants was because he wanted to protect them. And while he was listening to months and months of rain forest audio, he also heard something rather shocking - gunshots. (SOUNDBITE OF GUNSHOT)TEMPLE-RASTON: Now, it's impossible to know if an elephant died from these particular gunshots. But Wrege decided that if his team was going to count elephant rumbles, they should also find a way to count the gunshots because they could be a pretty good proxy for poaching attempts. So he has McKown building another neural network that will learn how to tell the difference between the sound of a gunshot. . . (SOUNDBITE OF GUNSHOT)TEMPLE-RASTON: . . . And the sound of a branch breaking. (SOUNDBITE OF BRANCH BREAKING)TEMPLE-RASTON: The great acoustic forest elephant count is still going on. The neural network is still training, and getting an accurate count depends on lots of things - weather, the season, where in the forest they're listening. So there isn't a precise number yet, but the Elephant Listening Project has already started learning things that can help fight poaching. For example, Wrege's team has found that elephants don't go to some parts of the forest during specific times of the year. WREGE: You can say, OK, we know that elephants are not using this huge part of this park for these seven months. We don't need to send any anti-poaching teams there because no poachers are going to find an elephant anyway. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: And every time they get a new set of recordings and feed it into the neural network, they get closer to an accurate count. Wrege imagines the network learning to distinguish elephant calls and to be able to tell if they're in distress or danger. WREGE: We have two gunshots here or we have AK-47 shots here. If that can be fed out of the rainforest in real time, then the anti-poaching people know where to go to intercept that poacher that just killed an elephant. TEMPLE-RASTON: Do you think that AI is going to save the elephants? WREGE: I actually do. It definitely is going to be our salvation. TEMPLE-RASTON: This is I'LL BE SEEING YOU from NPR, a series about the technologies that watch us. When we come back, how artificial intelligence is helping rangers stay one step ahead of poachers. And we go to a park in Malawi where they're starting to use algorithms to predict where poachers will go before they even fire a shot. I'm Dina Temple-Raston. Stay with us. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: From NPR, this is I'LL BE SEEING YOU, a show about the technologies that watch us. I'm Dina Temple-Raston. And on today's show, we've been looking at how artificial intelligence is saving the elephant. AI is helping environmentalists and researchers process huge amounts of data and mine it for information because AI doesn't get tired like humans do, and it may be better at teasing out patterns in near real time. They're testing this theory in southern Malawi. And we went to see for ourselves. (SOUNDBITE OF ENGINE REVVING)TEMPLE-RASTON: Liwonde National Park is about 200 square miles of fields and forests that run alongside the Sheri River. And the night we arrived, the first thing we did was go out on the water. CRAIG REID (MANAGER, LIWONDE NATIONAL PARK): All right, just a quick safety thing for everybody, please. TEMPLE-RASTON: Craig Reid is the manager of Liwonde National Park. REID: There are lots of crocodiles here in this river. TEMPLE-RASTON: We go out on the river at sunset, and we're skimming along the water in a metal-bottom boat. REID: And people are very much on the menu, so don't get tempted to put your hand out and run your fingers through the water or anything romantic like that - not a good idea. TEMPLE-RASTON: And the river is glassy, and we cut through reeds close to the shore and watch all the animals coming out. Oh, look; there are hippos there. So as the boat's going by, they're just dropping into the water, holding their breaths and then coming up and snorting all the air out. There are some elephants on the opposite bank, splashing in the water. Liwonde has a huge number of elephants. And to get close to one, you really have to go on land and use that old-fashioned tracking method we talked about earlier in the show. We saw the fresh dung piles along the road and followed those and found him in this clearing. An enormous elephant is right in front of us. You'd think an elephant is so huge that you'd hear him walking all the time. But what they say is the only time you hear an elephant is when he's coming and he breaks a branch, or when he's eating. And what we just got was an elephant eating. Liwonde National Park is full of scenes just like that. In addition to hippos and elephants, it has a rare black rhino sanctuary, baboons of every variety, gazelles, water bucks, warthogs. They recently reintroduced lions into the park, and they have some cheetahs. Seeing all this, it's hard to believe that just four years ago, Liwonde was a failed park. Here's park manager Craig Reid again. REID: So I always described Liwonde as we found it as being in a state of terminal decline. TEMPLE-RASTON: The buildings, roads, infrastructure - they were all falling apart. REID: So effectively, what would have happened had we not intervened would be a total elimination of all wildlife over the 10-year period following. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Four years ago, a nonprofit called African Parks wanted to try to bring Liwonde back from the brink. And it so happens that around that same time, one of the founders of Microsoft, Paul Allen, was just wrapping up the Great Elephant Census in effort to count all the elephants in Africa. Allen started a company whose goal was to find ways to use technology to save the elephants. He called the company Vulcan and hired people who love elephants, like Pawan Nrisimha. PAWAN NRISIMHA (DIRECTOR OF PRODUCT MANAGEMENT, VULCAN): Who doesn't love elephants, right? I mean, I love elephants. TEMPLE-RASTON: Nrisimha was a coder. Now he's the director of product management at Vulcan. He and the technologists there started building an AI solution for this elephant problem; a solution that would take all the information that park rangers, like Craig Reid, had both in their heads and in daily reports and then make it smarter. They called the program EarthRanger, Nrisimha explains. NRISIMHA: EarthRanger was entirely customer driven. And when I say customers here, it's essentially the park rangers. TEMPLE-RASTON: The Craig Reids - the idea was to feed all this information into a very accessible AI and machine learning platform. NRISIMHA: So it's a real-time visualization of where all the park assets are. And when I say park assets, it's the rangers. It's the animals. It's information from various sensors. TEMPLE-RASTON: Things like security cameras, GPS locations of vehicles, motion-activated sensors on park gates - so that was the first part; a real-time data visualization of the entire park. NRISIMHA: The second part is providing them analytics tools to be able to manage their patrols better. So these are things like heat maps that helps them - OK, these are the areas where most of the snaring or animal traps are happening, or these are the areas where there are lots fence breaks happening. TEMPLE-RASTON: That allows them to respond to security problems in the park just by clicking a mouse on the EarthRanger screen and running those incidents back, kind of like playing an interactive video game. And then there is an artificial intelligence piece. NRISIMHA: Gathering all this data and then we are trying to see how we can really let this artificial intelligence or predictive analytics to proactively tell the park management how to do the patrols and manage the security effort better. TEMPLE-RASTON: So in other words, take all this information they're gathering so a computer can learn about the rhythms of Liwonde Park and eventually offer suggestions on how to keep it safe. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Liwonde National Park has been using EarthRanger for the past two years. And we went to see it in action in a little brick building behind the ranger headquarters. (SOUNDBITE OF WALKIE TALKIE BEEPING)UNIDENTIFIED PERSON #2: (Foreign language spoken). TEMPLE-RASTON: If you've ever been in a command center of a small police department, this is a little like that. There are flat screens on the wall, closed circuit television monitors and, on two long tables, a series of computers analyzing and categorizing information coming into headquarters. Liwonde's operations manager is a man named Lawrence Munro, and part of his job is to plan and schedule ranger patrols. He's been using EarthRanger to help him figure out where to deploy them. LAWRENCE MUNRO (OPERATIONS MANAGER, LIWONDE NATIONAL PARK): If you can kill the (unintelligible) for this circle. TEMPLE-RASTON: To get an idea of what I'm seeing, there's a real-time satellite image of the park up on a flat screen. It shows the river, clearings, forests. MUNRO: Message (unintelligible). TEMPLE-RASTON: And there are little elephant icons tracking GPS location signals from collared elephants and little rhino icons in a sanctuary at the center of the park. MUNRO: Because we will use these guys for some operations that are coming up in this circle. TEMPLE-RASTON: Munro and enforcement chief Paul Chidyera move back and forth between a whiteboard at the back of the room, and they populate the EarthRanger screen with GPS locations of snares and footprints rangers had found in the park in the past month. MUNRO: You can have a look on the spreadsheet to just have a look what areas are worse affected (unintelligible). TEMPLE-RASTON: Munro starts clicking a mouse through various EarthRanger menus. MUNRO: I'd say we're going to look across the previous moon phase. TEMPLE-RASTON: He moves through an animation of suspicious activity in the park. MUNRO: And then if I can ask you to run the snares from the 15th of March, maybe just a tad. . . TEMPLE-RASTON: You can see a concentration of snares. The little icons look like little Western lassos, and there are a bunch of them right off the main road. MUNRO: So you can see those snares are out there (unintelligible). TEMPLE-RASTON: Munro and Chidyera decide to set up new checkpoints right where EarthRanger recorded a huge number of snares. If nothing else, that would discourage whoever put them there from setting more of them. MUNRO: So the old system was a map on the wall. I still have it in my office. And a similar tough process, but you wouldn't be able to see exactly where all your assets are. TEMPLE-RASTON: Assets - men, planes, jeeps, helicopters. MUNRO: You have to rely a lot more on memory, a lot more radio traffic. Here, you can do a lot more pre-emptive stuff because you can see the picture. TEMPLE-RASTON: Pre-emptive stuff - what he means is anticipating where a poacher might show up in the same way that a police department might send extra patrols into a high-crime area. And while this is standard operating procedure in policing, it wasn't really a focus in conservation until recently. And just like police departments have started trying to use computer analytics to find crime patterns, rangers like Munro are doing that, too. But it turns out that poaching in Liwonde is affected by lots of different things - weather patterns, animal movements. . . MUNRO: Things like religious holidays, the payday of the government sector. TEMPLE-RASTON: Bushmeat is considered a delicacy Malawi, so people put in their orders right after they get paid. MUNRO: We pick up trends. We pick up patterns. TEMPLE-RASTON: Munro said it used to take a month to start to see patterns emerging because they had to rifle through written reports. Now they can respond to threats faster. MUNRO: We study it intently every Wednesday because we want to deploy out our guys accordingly. But you can do it daily. That's the difference. It's basically the speed at which you can strategize. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Earlier this year, the head of law enforcement of Liwonde, Paul Chidyera, got an alert on his cellphone. A sensor linked to EarthRanger detected some suspicious movement in the eastern part of the park, so he and his team set up something called a poacher cam. It's basically a motion-activated camera. PAUL CHIDYERA (HEAD OF LAW ENFORCEMENT, LIWONDE NATIONAL PARK): There was an area way back when. . . TEMPLE-RASTON: Chidyera said there had been an area of the park rangers had found a lot of human tracks. So you can imagine if you just put up a motion-activated camera in a game preserve, you'll be getting lots of pictures of animals that go by. But these cameras have an algorithm built inside that helps a camera determine whether whatever's going past has a human shape or an animal one. If it looks human, it snaps a picture and sends it with GPS coordinates to EarthRanger and staff cellphones. The poacher cam got a photo of someone coming in, and Chidyera's team went to a nearby village and found him. CHIDYERA: When he was arrested, he was confronted. And he revealed what he has been doing. TEMPLE-RASTON: As fate would have it, he was a well-known poacher in the area. In fact, as they scrolled through EarthRanger's database, they found other pictures of him coming into the park with weapons. Those pictures were submitted as evidence in his trial. Each photo was used to support a different criminal count of trespass and weapons charges. In the end, the poacher was sentenced to 27 years as a repeat offender. As a tourist, elephants may seem magical, but it's important to understand that to people who live right next to them, they can be a nuisance or even a threat. And I got a real sense of this walking the fence line along Liwonde Park. Workers put this up? CHIDYERA: Yes. TEMPLE-RASTON: So they're all from villages around here? CHIDYERA: From the villages. TEMPLE-RASTON: I'm getting a tour of a relatively new fence at Liwonde Park when we run into this local guy. Can we ask you, sir? Can we talk? His name is Various Donzani. You live close by? VARIOUS DONZANI (LIWONDE RESIDENT): Yes, my house is 50 meters from the fence. TEMPLE-RASTON: Fifty meters from the fence? DONZANI: Fifty meters. TEMPLE-RASTON: So before the fence, were there elephants that came around your house? DONZANI: Oh, very much, especially when they come through a garden where there is food. We had problems. TEMPLE-RASTON: Donzani is a school teacher. But like a lot of people on the edge of the park, he relies on his own garden for food, which attracts elephants. One elephant can wipe out a village garden in a couple of hours. Do you have an experience with an elephant. . . DONZANI: Yes. \rTEMPLE-RASTON: . . . That you can tell us about? DONZANI: It's fortunate that I'm not killed. TEMPLE-RASTON: These elephant-human conflicts - that's what they're called - are really dangerous. After hippos, elephants kill more humans in Africa than any other animal. And Donzani remembers one horrible day before the fence was up that a herd of elephants came stampeding through his village. DONZANI: I tell you, it was a disaster. Seven people were killed that day. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: This is where poaching can get complicated. It often begins with locals slipping into the park just to hunt meat to eat - a Thomson's gazelle here, a water buck there. And then it's gales up. Criminal syndicates move in and recruit locals to help them find elephants or a black rhino. And the financial incentive is hard to resist. The tusks can fetch literally hundreds of thousands of dollars apiece on the black market. The human-elephant conflict plays into this as well. Smugglers and international cartels will often find local farmers who have just had their crops destroyed and then ask them for help to kill an elephant. They will say they heard they just lost everything. But if you help us find an elephant, we'll give you enough money to buy food for your family. It's easy to see where the incentive might come from. (SOUNDBITE OF ELEPHANT TRUMPETING)TEMPLE-RASTON: This is actual sound from a video of an elephant charging people in Liwonde. If someone tries to shoo an elephant off their fields, this is what they face. (SOUNDBITE OF ELEPHANT TRUMPETING)TEMPLE-RASTON: And being killed by an elephant is a horrifying thing. An adult female elephant weighs over three tons; a bull elephant is typically over six - park manager Craig Reid. REID: These big bulls will charge a person. And if they manage to catch them, which is not that difficult considering that an elephant can run about 60 kilometers an hour, which is. . . TEMPLE-RASTON: That's about twice as fast as the fastest human being. REID: Most people are relatively catchable. And when people are caught by an elephant, they're normally - using their trunk, knock them to the ground and then kneel on them, crushing them either with their knees or with the base of their trunk, which, at that point, is all curled up and almost like a fist. TEMPLE-RASTON: Lawrence Munro remembers that just a few years ago, elephants were leaving the park all the time, and locals would call in sightings. MUNRO: They would see elephants - repeat offenders, so to speak. They can identify the animals by a nick in their ear or whatever the case is. And then they would tell us - they'd say, that elephant there breaks out often. Soon as it's got a collar on it, we can watch it from here. And then we can make informed decisions regarding that elephant. We don't want elephants outside the park. They damage crops. They injure people. We want those people on our side. TEMPLE-RASTON: Because EarthRanger is tracking the elephants, it's easier to chase them away from the fence line before they get out of the park. And that has already started changing their behavior. They used to break out of the park any time they felt like it. Now they do it at night, when the ranger staff might be smaller. And they're back in the park by daylight. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: So how is EarthRanger doing? In the past two years, poaching in Liwonde has plummeted. The park hasn't lost a single high-value animal in 30 months. Those elephant trips outside the park - those are also way down. And while all this has been going on, EarthRanger's machine learning algorithm has been training. By the end of the year, Nrisimha thinks the program will have ingested enough data to start what they call pre-bang enforcement, something conservationists think will change poaching as we know it. NRISIMHA: Post-bang is you see a poacher come in and fired a shot, and then you hear it maybe because you have some sensors there, or maybe one of your camera traps on the field saw a poacher come in. TEMPLE-RASTON: You remember Nrisimha, the elephant lover who works for the company that created EarthRanger. NRISIMHA: So that's post-bang. You might be successful in that case to intercept the poacher, but the animal would have been hurt. TEMPLE-RASTON: Pre-bang is just like it sounds. It's about using behavioral analysis, artificial intelligence and predictive analytics to intercept poachers before they even get into the park. Among other things, EarthRanger's program will automatically generate the patrol routes that we heard Munro and Chidyera putting together in the control room. And instead of just focusing on places where there's already been criminal activity, the routes will be anticipated based on patterns EarthRanger's recognized. It will allow rangers to intercept bad guys pre-bang. Poachers will go to where they believe the elephants will be, and they'll find rangers there instead. NRISIMHA: The biggest difficulty here is really gathering up that data to train up that artificial intelligence model to know what to predict. TEMPLE-RASTON: It takes a long time for AI to learn. Now that EarthRanger has been deployed in Liwonde for two years, it's getting close to having enough data to start making predictions. NRISIMHA: If I had to give a number, I would say in about a year or two, we would start seeing solid results and parks actually using this data to help manage their parks or manage their patrols. TEMPLE-RASTON: But one thing that always looms over these high-tech solutions - what if they get hacked? One of the most vulnerable technologies is this. (SOUNDBITE OF RADIO STATIC)TEMPLE-RASTON: Don't adjust your radio. That's actually something called radio telemetry at work. The tracking collars they use to keep tabs on animals - the transmitter they use often sends out a signal in the form of a radio wave. And you use a receiver to zero-in on that signal. REID: Andrea is just flicking through the various frequencies. And she's now trying to get a response from one of the collars. TEMPLE-RASTON: That's Craig Reid again. And his wife, Andrea, is in charge of special projects there, including keeping track of the animals. ANDREA REID (PROJECT MANAGER, NATIONAL PARK): A lot of the collars actually have different beeps to them. TEMPLE-RASTON: The receiver has a directional antenna. And then all you do is listen. REID: So on the telemetry device here, you have a button that's called the gain. And when you turn the gain down and the beep is quite loud, it means that you're getting closer to the animal. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: The problem is hacking a transponder is fairly easy. Poachers are known to buy their own antennas and receivers and then do exactly what Andrea is doing. You can imagine, if a rhino horn or elephant tusk can fetch hundreds of thousands of dollars on the black market, there's a huge incentive for crime syndicates to come up with creative ways to locate high-value animals. REID: There's evidence of poachers now actually using the same technologies which are used by conservation officials. In a sense, we're involved in a bit of an arms race with poachers and poaching syndicates. TEMPLE-RASTON: From NPR, this is I'LL BE SEEING YOU, a four-part series about the technologies that watch us. I'm Dina Temple-Raston. And on today's show, we're looking at how AI is helping the elephant. Coming up, what if all this technology was hacked? DAWN SONG (PROFESSOR, UC BERKELEY): Even though deep neural networks has made tremendous advancements, it can be fooled very easily. TEMPLE-RASTON: Stay with us. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: From NPR, this is I'LL BE SEEING YOU, a four-part series about the technologies that watch us. I'm Dina Temple-Raston. And on today's show, we've been looking at how AI has been helping the elephant and the humans who live near them. And with all these high-tech solutions, people are now starting to worry about something else - hacking. (SOUNDBITE OF MUSIC)FRANK POPE (CEO, SAVE THE ELEPHANTS): Hello? TEMPLE-RASTON: Hello. Are you there? POPE: Hello. Is that Dina? TEMPLE-RASTON: It is. Hi. Is that Frank Pope? POPE: I've only just landed, after using some artificial intelligence to help elephants. TEMPLE-RASTON: (Laughter). Frank Pope is the CEO of an organization called Save the Elephants. You might have heard of it. It's been at the forefront of elephant conservation for decades. He was in Kenya when I called and had just finished flying over a park to check on elephants wearing special tracking collars. And I asked him whether Save the Elephants ever worried about hackers. POPE: That was one of the prime motives for diving into this project. TEMPLE-RASTON: He means the EarthRanger project we were talking about before. And he says the danger of hacking into tracking collars varies depending on whether you have elephants spread out over a large area or have them all bunched up. POPE: I've just spent three hours flying around. We think we saw about 400 elephants, and there was one collar among them. TEMPLE-RASTON: So in Kenya, where Pope is, going to the trouble of hacking into the tracking system doesn't make much sense. But if you're in South Sudan, hacking into tracking collars could help you find a huge number of the national herd. The Sudanese government keeps their elephants in close proximity to each other, so hacking a tracking collar. . . POPE: That could be catastrophic. So we have to be careful in some circumstances, but we don't have to be so careful in others. TEMPLE-RASTON: And Pope said the Save the Elephants tracking system is being run through programs like EarthRanger precisely because of the concern about hacking. To be clear, Pope said he's not just concerned about technology; he's worried about carbon units. You know them as humans. They're the biggest threat to keeping the animals safe. POPE: The weakness of all these security systems is, of course, the users. If you've got a radio room or an operations control room, the people you have in there have to be trusted, and you have to have layers of security. It's impossible to pay rangers enough to ensure that they won't be tempted by the staggering amounts of money you could get for ratting on a rhino and selling it to them if that rhino moves close to a fence, for example. And that's not about hacking so much as making sure that the right people have access to the data. TEMPLE-RASTON: That's something Craig Reid at Liwonde National Park in Malawi understood from the start. He's placed closed-circuit television cameras all around the ranger headquarters where the EarthRanger system is running. REID: That's all part of the process of trying to protect the staff that are working with information and also trying to make sure that the information doesn't leave the control room. TEMPLE-RASTON: But the dream of AI goes far beyond saving elephants. AI is everywhere. It's starting to read our X-rays and drive our cars. And researchers are just beginning to understand AI and its vulnerabilities. If that sounds familiar, it should. New technologies have always developed this way. Remember the movie \"WarGames? \"(SOUNDBITE OF FILM, \"WARGAMES\")UNIDENTIFIED PERSON #1 (VOICEOVER): He found the right code word to play the game. TEMPLE-RASTON: It came out in 1983. (SOUNDBITE OF FILM, \"WARGAMES\")UNIDENTIFIED PERSON #1: But it was the wrong computer. UNIDENTIFIED ACTOR #1 (ACTOR): (As character) Shall we play a game? UNIDENTIFIED ACTOR #2 (ACTOR): (As character) How can it ask you that? \rMATTHEW BRODERICK (ACTOR): (As David) How about global thermonuclear war? TEMPLE-RASTON: Matthew Broderick hacks into the U. S. military's threat detection system thinking he's playing a new computer game, and he almost starts a war. Crazy? Well, President Ronald Reagan was rather famously freaked out by the movie, and one of his generals told him that something like that could actually happen. It led to America's first federal law against computer hacking, the Computer Fraud and Abuse Act. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Trying to anticipate how adversaries might hack into technology is something people at the Defense Advanced Research Projects Agency, or DARPA, think a lot about. DARPA has been a kind of high-tech incubator for the military since 1958. Stealth technology - that came out of DARPA. Tank simulators, the M16 rifle - you can thank DARPA for those, too. On the creepier side, it is said to be developing what they call a bio-enhanced soldier, or supersoldier. The thing that DARPA is really famous for, though, is artificial intelligence. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: Now, back to its very beginning, DARPA has been at the forefront of support, advocacy and leadership in artificial intelligence. TEMPLE-RASTON: This is from one of their promotional videos. And of course, artificial intelligence led to the driverless car. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #3 (DARPA): The DARPA autonomous land vehicle project, which started in 1984, led to. . . TEMPLE-RASTON: DARPA's long history with AI is part of the reason why researchers there have been so focused on what can go wrong with it. The agency launched a major project earlier this year to try to make AI more resilient because right now it is alarmingly easy to confuse an AI program. UNIDENTIFIED PERSON #4 (ARTIFICIAL INTELLIGENCE): Thirteenth floor. TEMPLE-RASTON: So I went to DARPA to meet with Hava Siegelmann. I'm Dina Temple-Raston. HAVA SIEGELMANN (PROJECT MANAGER, DARPA): Hey, Dina. I'm Hava. TEMPLE-RASTON: Thank you for making the time. She's the director of something called the GARD project. Guard stands for Guaranteeing AI Robustness Against Deception. It's a program that's trying to find ways to make artificial intelligence more hack-proof. SIEGELMANN: So when people started with AI, they were going and asking professionals how do you do this and how do you do that and try to write it as rules. TEMPLE-RASTON: Siegelmann is originally from Israel, which accounts for her accent, and she spends hours every day looking at how machines can be tricked into doing the wrong thing; not necessarily just going against its programming, but instead being taught something by adversaries that it is not supposed to learn. SIEGELMANN: When you design your machine learning computer network, you always come with the best data set that you can. TEMPLE-RASTON: The best data set that you can - so in the case of Peter Wrege, our Elephant Listening Project director, he had all those hours and hours of recordings. (SOUNDBITE OF BIRDS CHIRPING)TEMPLE-RASTON: Liwonde had reports rangers had been feeding into EarthRanger. REID: The red is for checks going in, and that amber color's for checks going out. TEMPLE-RASTON: But it turns out because AI learns in ways we don't quite understand, its decision-making process is a bit of a black box. It can be easy to fool. So earlier in the show, we talked about a subset of AI called neural networks. And we explained that it was fed millions and millions of data points to begin to understand the plucking sound of a violin. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: The tone, the rhythm, the notes - and it learned enough to be able to recognize some characteristics of violins even though they're buried in this. . . (SOUNDBITE OF MUSIC)TEMPLE-RASTON: . . . A slightly addictive, happy song. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: But let's say we want the neural network to do something a little different. We want it to recognize a specific genre of music. This time, it isn't trained to find its violins, but it's trained to tell us whether what we're hearing is orchestral music or disco. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: The neural network went through all its training and decided on maybe a dozen qualities that, in all its analysis, helps it tell orchestral music apart from disco. And maybe one of those things is an oboe. It has looked for an oboe in classical music, and it's always there - hard to find but always there. And then it listened to disco music, and it comes to the conclusion most disco music has some Barry Gibb of the Bee Gees and a host of other things but never an oboe. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: The important thing to understand is that the neural network isn't taking in all the music to figure that out. It's just zeroing in on these qualities it learned to identify. And that's the rub. If you actually know what the qualities are that it's looking for, you can fool it. You can bury something in the music that the human ear can't hear but the network might see - something like an oboe carefully placed inside the disco beat. And the network is fooled. Can you hear the oboe we snuck in there? (SOUNDBITE OF MUSIC)TEMPLE-RASTON: Know what the AI or neural network is focusing on, tweak that and you're done. Humans don't make that mistake because they take in the totality. AI systems are easy to nudge into making the wrong inference. (SOUNDBITE OF MUSIC)TEMPLE-RASTON: One of my favorite examples of this came from computer scientists at Carnegie Mellon University. And it's a fairly basic experiment. The CMU researchers trained a computer to use facial recognition to identify different people. The computer dutifully ingested lots of photographs of the people and identified them correctly every time. But then they put big, colorful glasses on a subject who hadn't had glasses on before, and the computer confidently misidentified him. Fooling AI can be that easy. This is the kind of thing Siegelmann thinks a lot about. SIEGELMANN: So there are examples on images that if you put dominance patch in an image, then the network will attend to this patch and will tell you. . . TEMPLE-RASTON: A dominance patch - think of it as the equivalent of a bright, shiny object that catches your attention or big, bright glasses that get the computer to focus on that instead of the bigger picture. Siegelmann says this could become a problem on the battlefield. She is with DARPA, after all. SIEGELMANN: So perhaps I'm just imagining. People can put some patch on their clothes. . . TEMPLE-RASTON: A patch on their clothes, instead of those glasses the CMU researchers added to one of their subjects - that patch could fool an AI system into thinking someone is a member of your platoon when, in fact, it's the enemy. I'm here to see Dawn Song on the 34th floor. So there's a pretty famous experiment about this conducted by UC Berkeley professor Dawn Song. Wow. This is quite a view. SONG: Yeah, the view is nice. TEMPLE-RASTON: We met in her office. And from her conference room, you can see the entire San Francisco Bay. SONG: You can actually see the Golden Gate from here. TEMPLE-RASTON: Alcatraz, the Golden Gate - Song is a long way from where she grew up in Northeast China, and one of the things that brought her to this high-rise with a great view of the bay is a short video about AI that went viral, which she made with some of her colleagues at Berkeley. SONG: Let me start playing the video. TEMPLE-RASTON: The video doesn't have any sound, and it's less than a minute long. But it has rocked the AI community because it showed just how vulnerable AI and driverless cars can be. SONG: So first let me explain what you will see in the video. So in the video, you'll see two frames side by side. TEMPLE-RASTON: Think split screen. SONG: In both frames, you'll see the vehicle is driving towards the end of the road, where there's a person holding a stop sign. TEMPLE-RASTON: The split screens are subtitled so you can see how the AI - and specifically something called image classification - is making decisions inside the autonomous car. SONG: You'll see the prediction given by the image classification system to try to predict what the traffic sign is. TEMPLE-RASTON: So sort of like the car starting to think, hm, a sign is coming. I'm going to have to make a decision. SONG: Right. TEMPLE-RASTON: So the key part of the experiment is that Song put some strategically placed stickers on one of the stop signs. There's one sticker below the S and the other above the O in stop. The other stop sign hasn't been changed. And as the car gets closer to the sign, the subtitles on the screen are showing how the AI system is interpreting what it's seeing. The AI reads the regular sign just fine. But the one with the stickers - it thinks it says, speed limit 45 miles an hour. And the car blows right through the intersection. Two carefully placed stickers was all it took to make a self-driving car run a stop sign. So you were expecting it to misread the sign. And then it did, and you were happy about that. SONG: In some sense, yes, because we created the adversarial example. So it is surprising still, given that - how well it worked. TEMPLE-RASTON: It worked so well that people who were developing driverless cars tapped the brakes, and it helped launch a movement to see how vulnerable AI really is. Now, to be fair, Song's team didn't just throw some stickers onto a sign and fool an AI system. It was a very involved and sophisticated process. They knew exactly how the AI's image classification system worked. They knew which pixels of the sign to manipulate to fool it. So you might ask, a speed limit sign is a totally different shape than a stop sign; how could two stickers fool the AI system so completely? Song says it's because AI isn't seeing the sign the way we might see a sign with our own eyes. AI sees the sign as a mathematical equation not a shape. SONG: At the end of the day, the human vision system is also a function. TEMPLE-RASTON: A function or a calculation. SONG: But it's a much more sophisticated function. And through the evolution and through learning, the human vision system is working really, really well and has learned to learn the right things. And even though deep neural networks has made tremendous advancements - and for certain vision tasks, it actually has achieved a human level of performance. But it has these issues that it can be fooled very easily. TEMPLE-RASTON: Song says there have been huge strides in AI, but it's still in its infancy. SONG: We need to understand that the machine - like, the machine learning system is not - actually, it's not as powerful as what people think. We still have a lot of work to do. TEMPLE-RASTON: Decades before there's a safe self-driving car? SONG: We do really need new and more breakthroughs before we can really get there. TEMPLE-RASTON: So would you ride in a driverless car? SONG: Not today. (LAUGHTER)SONG: I mean, I would enjoy having a test drive. But. . . (LAUGHTER)TEMPLE-RASTON: Hava Siegelmann over at DARPA has similar reservations. SIEGELMANN: If you can do things with, like, a stop sign - think that the person, or there's a group, is driving a tank, and they put the particular sticker on it. And because that sticker that has particular color - we think that this tank is actually an ambulance. And immediately, we open the gates to let the ambulance go in. TEMPLE-RASTON: The reason to study all of this isn't to scare us about AI, although it does that, too. The reason Song and Siegelmann are trying to see the limits of AI is so they can fix it, kind of like old-fashioned hackers who used to call up software companies and let them know about flaws in their coding so they could send out patches. SIEGELMANN: So the field started a few years ago when people said, wow, there is a really interesting feature of the neural network; that it's really not robust. TEMPLE-RASTON: And you know that stop sign that launched a thousand doubts about AI? It's now hanging up at the London Museum of Science and Technology. It's part of an exhibit about our driverless future. (SOUNDBITE OF MUSIC)UNIDENTIFIED SINGERS (SINGERS): (Singing) We've got to slow down, slow down. TEMPLE-RASTON: This is from a musical short General Motors produced for an auto expo in 1956, and it's all about the future of driving. (SOUNDBITE OF MUSIC)UNIDENTIFIED SINGER #1 (SINGER): (Singing) You can bet your high compression we're going to be late. TEMPLE-RASTON: And in the video, the car becomes a time machine, taking the four people in the car into the future. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTOR #3 (ACTOR): (As character) Hey, I wonder what we'd hear if I turn on the switch, and we're driving along in 1976. TEMPLE-RASTON: 1976 - modern machinery. In this case, think \"The Jetsons\" - a driverless car that looks a lot like a spaceship. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTOR #4 (ACTOR): (As character) Well done, Firebird 2. You're now under automatic control; hands-off steering. TEMPLE-RASTON: This all goes on for almost nine minutes, and 1976 came and went without a driverless car. And so did 2018. Most experts will tell you that driverless cars are more than a decade away. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTOR #5 (ACTOR): (As character) Here we go on the high-speed safety lane. TEMPLE-RASTON: The sticking point has always come down to artificial intelligence - using neural networks to identify elephant songs. . . (SOUNDBITE OF ELEPHANT RUMBLING)TEMPLE-RASTON: . . . Or to anticipate where poachers might be. . . REID: The old system was a map on the wall. TEMPLE-RASTON: . . . Allows AI to be wrong sometimes without life-threatening consequences. One of the things that researchers are looking at is whether AI can be trained to explain to us how it's making decisions. Until we understand that, researchers will tell you that AI really shouldn't be trusted with the car keys. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTOR #6 (ACTOR): (As character) Ah, this is the life - safe, cool, comfortable. Mind if I smoke a cigar? UNIDENTIFIED ACTOR #7 (ACTOR): (As character) Oh, not with this wonderful air conditioning. TEMPLE-RASTON: This is I'LL BE SEEING YOU from NPR. The show was written and hosted by me, Dina Temple-Raston. And our producer is Adelina Lancianese, and she scored the show too. Field production by Michael May. Our theme music is composed by Ramtin Arablouei. And special thanks to NPR investigations, the Story Lab, Kenny Malone, John Keefe of the Quartz AI Studio and Josephine Wolff at Tufts University. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTOR #5: (As character) The safe, easy way to make a turn. TEMPLE-RASTON: And if you missed one of our previous shows, just go to npr. org/illbeseeingyou or find us on NPR One. Next time, we get exclusive access to the U. S. military unit fighting the most secretive and deadly terrorist organization in the world. And their weapon is a computer keyboard. UNIDENTIFIED PERSON #5: You could take those over. We were going to win everything. It was a house of cards. UNIDENTIFIED PERSON #6: So within the first 60 minutes of go, I knew we were having success. And that continued, obviously, well into the night. TEMPLE-RASTON: I'm Dina Temple-Raston, and I'll be seeing you. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED SINGER #2 (SINGER): (Singing) Roger, Firebird, I'll swing you to the right; hands-on steering. Firebird, good night.", "section": "I'll Be Seeing You", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-25-773305389": {"title": "How Hijacked Cellphone Numbers Can Be Security Risks : NPR", "url": "https://www.npr.org/2019/10/25/773305389/how-hijacked-cellphone-numbers-can-be-security-risks", "author": "No author found", "published_date": "2019-10-25", "content": "DAVID GREENE, HOST: All right. Think of all the ways your cellphone has become your electronic ID card. The bank knows you by your phone number. Secure websites send key codes by text message. But phone numbers can be hijacked, and a recent wave of phone-based identity theft raises concerns that wireless carriers are just not doing enough to protect customers. NPR's Martin Kaste reports. MARTIN KASTE, BYLINE: Greg Bennett (ph) had already heard of people's phones getting hijacked. He's an entrepreneur in Bellevue, Wash. , and he knows a thing or two about tech. So he's a little embarrassed when he remembers the day in April when it happened to him. GREG BENNETT: (Laughter) Well, I'm sitting here with my son and I was having trouble getting into my email account. And then all of a sudden, my phone went dead. And I'm looking at my phone, and there's no signal. And I go, oh, no, something's happened here. KASTE: It was a SIM swap. That's when scammers get your phone company to switch your number to their phone. Sometimes the scammers fool the phone company into thinking they're you. Sometimes a phone company staffer is in on the scam. Once they have your number, they can get your text messages, including password reset confirmation codes. Bennett says that's how they got into his email accounts, and from there, it was open season. BENNETT: Oh, they got into my Amazon account, my Evernote account, my Starbucks account. They were kind of messing with me (laughter). KASTE: But the big prize was his bitcoin account. It's not clear exactly how they used his phone number to get in there, but once they were, he says they stole about half a million dollars' worth. BENNETT: A hundred bitcoin all in a matter of minutes. KASTE: And that's what's new here. SIM swapping has been around for a while, but now there's just so much more at stake. ALLISON NIXON: The reason why SIM swapping is such a problem is because phone numbers have suddenly become valuable. KASTE: Allison Nixon is director of security research at Flashpoint, a company that tracks cybercrime. She says cell numbers have become an irresistible target for scammers because so many companies now force us to use our phone numbers as a form of ID. NIXON: Financial, health care, social media, email - all of these different companies, by policy, they require a phone number from you. And that's what creates the vulnerability. KASTE: As scams go, SIM swapping is labor intensive. Thieves have to research their victims first, and they look for rich targets, like the cryptocurrency investor in California who says he lost $24 million to SIM swappers last year. But Nixon says the scammers are also starting to aim lower. NIXON: Eventually, you're going to run out of rich people - right? - and you've got to start targeting middle-class people, upper-middle-class people. I know people that have been SIM swapped that have no clear indication as to why aside from the fact that they get paid, and they have a retirement account. KASTE: Experts have suggested various ideas for improving security. For instance, carriers could require that phone number transfers always happen in person at a store. Senator Ron Wyden, Democrat from Oregon, has been looking at what the carriers could do. He won't get into details about behind-the-scenes discussions, but he's not optimistic. RON WYDEN: The industry is not exactly exerting itself in order to better protect the consumer from these SIM swap scams. KASTE: The wireless companies refer questions about SIM swapping to their industry association, the CTIA, but it also would not do an interview with NPR. It pointed, instead, to a blog post with tips for avoiding SIM swaps. For instance, they say you should keep your personal details off social media so you're harder to impersonate. Critics counter that it should be on the industry to keep these phone numbers safe. Though, Allison Nixon says she can understand why the phone companies might be reluctant to erect higher security barriers to SIM swaps. NIXON: It would make the purchase process for the average legitimate customer a little bit more difficult, a little bit slower, and multiply that by however many millions of sales that they make, it probably adds up to a decent amount of money. KASTE: Another solution might just be to wean Americans from using their phone numbers for authentication. Federal regulators have noted the vulnerabilities of text message codes compared to more secure two-factor methods like apps. But the wireless industry is pushing back on that. In a letter to the FTC in August, the industry defended text messages as, quote, \"easily accessible and trusted. \" But they're not trusted anymore by Greg Bennett back in Washington state. BENNETT: People who are using phones as their only source of two-factor authentication are kind of inviting identity theft. KASTE: He now does two-factor authentication with apps or a hardware key. When he's forced to use text message codes, he uses a second phone number, which he keeps secret. He's in arbitration with AT&T, which wouldn't talk to NPR about his case. And he says the company is stonewalling on the details of just how he got SIM swapped. But he suspects that he was victimized by somebody on the East Coast. How does he know? BENNETT: When I finally recovered my phone, I got a text message asking how my service was at the AT&T store in Boston (laughter). KASTE: Martin Kaste, NPR News, Seattle. (SOUNDBITE OF SINJIN HAWKE AND ZORA JONES' \"SOURCE OF CONFLICT\")GREENE: Now, for tips on protecting yourself from SIM swaps, take a look at the web version of this story at npr. org. DAVID GREENE, HOST:  All right. Think of all the ways your cellphone has become your electronic ID card. The bank knows you by your phone number. Secure websites send key codes by text message. But phone numbers can be hijacked, and a recent wave of phone-based identity theft raises concerns that wireless carriers are just not doing enough to protect customers. NPR's Martin Kaste reports. MARTIN KASTE, BYLINE: Greg Bennett (ph) had already heard of people's phones getting hijacked. He's an entrepreneur in Bellevue, Wash. , and he knows a thing or two about tech. So he's a little embarrassed when he remembers the day in April when it happened to him. GREG BENNETT: (Laughter) Well, I'm sitting here with my son and I was having trouble getting into my email account. And then all of a sudden, my phone went dead. And I'm looking at my phone, and there's no signal. And I go, oh, no, something's happened here. KASTE: It was a SIM swap. That's when scammers get your phone company to switch your number to their phone. Sometimes the scammers fool the phone company into thinking they're you. Sometimes a phone company staffer is in on the scam. Once they have your number, they can get your text messages, including password reset confirmation codes. Bennett says that's how they got into his email accounts, and from there, it was open season. BENNETT: Oh, they got into my Amazon account, my Evernote account, my Starbucks account. They were kind of messing with me (laughter). KASTE: But the big prize was his bitcoin account. It's not clear exactly how they used his phone number to get in there, but once they were, he says they stole about half a million dollars' worth. BENNETT: A hundred bitcoin all in a matter of minutes. KASTE: And that's what's new here. SIM swapping has been around for a while, but now there's just so much more at stake. ALLISON NIXON: The reason why SIM swapping is such a problem is because phone numbers have suddenly become valuable. KASTE: Allison Nixon is director of security research at Flashpoint, a company that tracks cybercrime. She says cell numbers have become an irresistible target for scammers because so many companies now force us to use our phone numbers as a form of ID. NIXON: Financial, health care, social media, email - all of these different companies, by policy, they require a phone number from you. And that's what creates the vulnerability. KASTE: As scams go, SIM swapping is labor intensive. Thieves have to research their victims first, and they look for rich targets, like the cryptocurrency investor in California who says he lost $24 million to SIM swappers last year. But Nixon says the scammers are also starting to aim lower. NIXON: Eventually, you're going to run out of rich people - right? - and you've got to start targeting middle-class people, upper-middle-class people. I know people that have been SIM swapped that have no clear indication as to why aside from the fact that they get paid, and they have a retirement account. KASTE: Experts have suggested various ideas for improving security. For instance, carriers could require that phone number transfers always happen in person at a store. Senator Ron Wyden, Democrat from Oregon, has been looking at what the carriers could do. He won't get into details about behind-the-scenes discussions, but he's not optimistic. RON WYDEN: The industry is not exactly exerting itself in order to better protect the consumer from these SIM swap scams. KASTE: The wireless companies refer questions about SIM swapping to their industry association, the CTIA, but it also would not do an interview with NPR. It pointed, instead, to a blog post with tips for avoiding SIM swaps. For instance, they say you should keep your personal details off social media so you're harder to impersonate. Critics counter that it should be on the industry to keep these phone numbers safe. Though, Allison Nixon says she can understand why the phone companies might be reluctant to erect higher security barriers to SIM swaps. NIXON: It would make the purchase process for the average legitimate customer a little bit more difficult, a little bit slower, and multiply that by however many millions of sales that they make, it probably adds up to a decent amount of money. KASTE: Another solution might just be to wean Americans from using their phone numbers for authentication. Federal regulators have noted the vulnerabilities of text message codes compared to more secure two-factor methods like apps. But the wireless industry is pushing back on that. In a letter to the FTC in August, the industry defended text messages as, quote, \"easily accessible and trusted. \" But they're not trusted anymore by Greg Bennett back in Washington state. BENNETT: People who are using phones as their only source of two-factor authentication are kind of inviting identity theft. KASTE: He now does two-factor authentication with apps or a hardware key. When he's forced to use text message codes, he uses a second phone number, which he keeps secret. He's in arbitration with AT&T, which wouldn't talk to NPR about his case. And he says the company is stonewalling on the details of just how he got SIM swapped. But he suspects that he was victimized by somebody on the East Coast. How does he know? BENNETT: When I finally recovered my phone, I got a text message asking how my service was at the AT&T store in Boston (laughter). KASTE: Martin Kaste, NPR News, Seattle. (SOUNDBITE OF SINJIN HAWKE AND ZORA JONES' \"SOURCE OF CONFLICT\") GREENE: Now, for tips on protecting yourself from SIM swaps, take a look at the web version of this story at npr. org.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-26-773706177": {"title": "Pentagon Picks Microsoft Over Amazon For $10 Billion Contract : NPR", "url": "https://www.npr.org/2019/10/26/773706177/pentagon-awards-10-billion-contract-to-microsoft-over-front-runner-amazon", "author": "No author found", "published_date": "2019-10-26", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-26-773675386": {"title": "The Importance Of Preserving Videos Of War Crimes : NPR", "url": "https://www.npr.org/2019/10/26/773675386/the-importance-of-preserving-videos-of-war-crimes", "author": "No author found", "published_date": "2019-10-26", "content": "SCOTT SIMON, HOST: When a traumatic event occurs today, it's often followed by the quick posting of a video - mass shootings or alleged police misconduct or use of chemical weapons. They're often uploaded to platforms like YouTube and Facebook, which are then under pressure to remove videos that contain violence or graphic imagery. Videos of human rights violations can be used as evidence of war crimes in places like Syria, Yemen and Myanmar. Does deleting them help keep war crimes unreported and undocumented and war criminals free? Witness is an organization that helps people use that technology. Dia Kayyali is a program manager there and joins us via Skype from London. Thanks so much for being with us. DIA KAYYALI: Thank you for having me. SIMON: You recently helped produce a video op-ed for The New York Times on this. Can you give us an example of a video that's been taken down that, in your view, really should be seen? KAYYALI: Yes. And I will say that we work very closely with Syrian Archive, and they're really working on the ground and seeing a lot of this content get taken down. So just to give you a very recent example, there was a Facebook video from June that was showing a protest in Sudan. And that's just one video that we've seen get taken down, among many, that are actually just documenting protests. SIMON: And this is being done by algorithms, not human beings. KAYYALI: That's right. Content moderation these days still relies on some human flagging, but it also relies on content moderation algorithms. SIMON: And how do you believe this is preventing human rights violations from being reported and documented? KAYYALI: Well, unfortunately, what we're seeing is that these algorithms appear to really be focused on content that is coming from the Arabic-speaking world or from places with large Muslim communities. And these are videos we're seeing coming from places, as you mentioned, like Syria, that may contain content showing, for example, a barrel bombing. It shouldn't be in violation of the standards of either Facebook or YouTube, but we still see videos like that get taken down. And these are videos that groups like Syrian Archive would take and verify and put into a data collection about a barrel bombing or about a specific chemical attack. SIMON: Now, I have made it my business to see a number of those videos. And it seems to me that you immediately see on some platforms like Twitter people saying, oh, that's fake, or, oh, that's from three or four years ago. Doesn't that indicate that maybe the platform is best to take it down sometimes? KAYYALI: That is exactly why groups like Syrian Archive are so important because they are going through and verifying the videos using a number of different methods. So, of course, they're using geolocation. I think they make great use of Google Earth, zooming in and looking for buildings and geographical features. They're also doing things like cross-verifying with weather information, looking at different dialects that are spoken and comparing with different sources, collecting very large sets of videos that are showing the different - same events from different angles. So there is actually a way to determine whether or not a video is fake. SIMON: I am aware of the fact that when some other websites complain that the video we're seeing is old or faked, they might be trying to pull a fast one on people, too. KAYYALI: Oh, absolutely. You know, one of the quotes that I like to show people when I'm explaining these videos is directly from Assad. And he's speaking about a video of a chemical attack. And he's saying, you know, these videos are probably fake; you can't know if a video is faked today. And that's what he would like for you to believe. But, really, there is pretty clear and verified evidence of a lot of these incidents. SIMON: What would you like some of these social media platforms to do? KAYYALI: One of the really important suggestions that we have for social media companies is they need to be retaining this evidence. They can't just delete it and then have it be gone. They need to have some sort of collection that human rights researchers can still access so that it's still available for the International Criminal Court, for the United Nations, for people who are trying to understand what's happened in Syria. You know, this is, really, history that is being deleted. And then second, of course, you know, algorithms are being used all over on platforms now, but they're being done in a totally intransparent (ph) way. We really don't know what data they're basing these algorithms on. So just being a little bit more transparent. SIMON: Dia Kayyali, program manager for Witness, thank you so much for speaking with us. KAYYALI: Thank you for having me. SCOTT SIMON, HOST:  When a traumatic event occurs today, it's often followed by the quick posting of a video - mass shootings or alleged police misconduct or use of chemical weapons. They're often uploaded to platforms like YouTube and Facebook, which are then under pressure to remove videos that contain violence or graphic imagery. Videos of human rights violations can be used as evidence of war crimes in places like Syria, Yemen and Myanmar. Does deleting them help keep war crimes unreported and undocumented and war criminals free? Witness is an organization that helps people use that technology. Dia Kayyali is a program manager there and joins us via Skype from London. Thanks so much for being with us. DIA KAYYALI: Thank you for having me. SIMON: You recently helped produce a video op-ed for The New York Times on this. Can you give us an example of a video that's been taken down that, in your view, really should be seen? KAYYALI: Yes. And I will say that we work very closely with Syrian Archive, and they're really working on the ground and seeing a lot of this content get taken down. So just to give you a very recent example, there was a Facebook video from June that was showing a protest in Sudan. And that's just one video that we've seen get taken down, among many, that are actually just documenting protests. SIMON: And this is being done by algorithms, not human beings. KAYYALI: That's right. Content moderation these days still relies on some human flagging, but it also relies on content moderation algorithms. SIMON: And how do you believe this is preventing human rights violations from being reported and documented? KAYYALI: Well, unfortunately, what we're seeing is that these algorithms appear to really be focused on content that is coming from the Arabic-speaking world or from places with large Muslim communities. And these are videos we're seeing coming from places, as you mentioned, like Syria, that may contain content showing, for example, a barrel bombing. It shouldn't be in violation of the standards of either Facebook or YouTube, but we still see videos like that get taken down. And these are videos that groups like Syrian Archive would take and verify and put into a data collection about a barrel bombing or about a specific chemical attack. SIMON: Now, I have made it my business to see a number of those videos. And it seems to me that you immediately see on some platforms like Twitter people saying, oh, that's fake, or, oh, that's from three or four years ago. Doesn't that indicate that maybe the platform is best to take it down sometimes? KAYYALI: That is exactly why groups like Syrian Archive are so important because they are going through and verifying the videos using a number of different methods. So, of course, they're using geolocation. I think they make great use of Google Earth, zooming in and looking for buildings and geographical features. They're also doing things like cross-verifying with weather information, looking at different dialects that are spoken and comparing with different sources, collecting very large sets of videos that are showing the different - same events from different angles. So there is actually a way to determine whether or not a video is fake. SIMON: I am aware of the fact that when some other websites complain that the video we're seeing is old or faked, they might be trying to pull a fast one on people, too. KAYYALI: Oh, absolutely. You know, one of the quotes that I like to show people when I'm explaining these videos is directly from Assad. And he's speaking about a video of a chemical attack. And he's saying, you know, these videos are probably fake; you can't know if a video is faked today. And that's what he would like for you to believe. But, really, there is pretty clear and verified evidence of a lot of these incidents. SIMON: What would you like some of these social media platforms to do? KAYYALI: One of the really important suggestions that we have for social media companies is they need to be retaining this evidence. They can't just delete it and then have it be gone. They need to have some sort of collection that human rights researchers can still access so that it's still available for the International Criminal Court, for the United Nations, for people who are trying to understand what's happened in Syria. You know, this is, really, history that is being deleted. And then second, of course, you know, algorithms are being used all over on platforms now, but they're being done in a totally intransparent (ph) way. We really don't know what data they're basing these algorithms on. So just being a little bit more transparent. SIMON: Dia Kayyali, program manager for Witness, thank you so much for speaking with us. KAYYALI: Thank you for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-28-774178653": {"title": "U.S. Campaign Ads Are Popping Up On Russian Propaganda YouTube Channels : NPR", "url": "https://www.npr.org/2019/10/28/774178653/u-s-campaign-ads-are-popping-up-on-russian-propaganda-youtube-channels", "author": "No author found", "published_date": "2019-10-28", "content": "ARI SHAPIRO, HOST: Political advertising is surging online, even though the election is still over a year away. Campaigns are already spending tens of millions of dollars to promote their candidates on platforms like Facebook and Google, but some of those ads may be showing up in unintended places. NPR's Geoff Brumfiel has more. GEOFF BRUMFIEL, BYLINE: So a couple of days ago, I was tooling around on YouTube, looking at Russia's main propaganda channel, known as RT. (SOUNDBITE OF ARCHIVED RECORDING)ILYA PETRENKO: Kremlin talking points is what I was talking about, so why waste time on. . . BRUMFIEL: But then something really unexpected happened. In between videos, this ad popped up for President Trump. It was from the Trump Make America Great Again Committee, which is part of his 2020 re-election campaign. The ad showed up again and again before a video accusing the U. S. of meddling in the United Nations. . . (SOUNDBITE OF YOUTUBE VIDEO, \"EP. 806: WHISTLEBLOWER KATHARINE GUN- HOW THE US SPIED+BLACKMAILED TO GET SUPPORT FOR THE IRAQ WAR\")AFSHIN RATTANSI: I mean, you believe the plan was to spy on the U. N. Security Council members and then potentially blackmail them. BRUMFIEL: . . . And another one showing a Russian fighter jet confronting a NATO rival. In an email, Trump campaign communications director Tim Murtaugh told NPR, quote, \"we have not targeted those channels at all. \" So what's going on? LAURA EDELSON: The Trump campaign is a big advertiser. BRUMFIEL: Laura Edelson is a graduate student studying online political advertising at New York University. The Trump campaign has already spent over $12 million advertising on Google platforms, including YouTube. She says the type of ad I saw is designed to draw new supporters. It's probably being spread over a large swath of YouTube. A spokesperson for the company told me advertisers do have the option to prevent their ads from appearing on certain channels, but Edelson says the Trump campaign may just not have checked that box. EDELSON: They could do something like say, I want to advertise against news channels, and they just didn't exclude RT. That is a possibility. BRUMFIEL: Murtaugh didn't say whether the campaign had excluded RT, but he did say its main strategy is to target the user. That means advertising follows people the campaign wants to reach no matter what they're viewing on YouTube. Shannon Kowalczyk is with the progressive group ACRONYM, which is tracking online advertising this election season. She says going after viewers is pretty common. SHANNON KOWALCZYK: You know, a political advertiser can go in and define that they want to target more of a Democratic audience or more of a Republican-leaning audience. So if you're on RT, it may happen there. If you're on a cooking channel, it may happen there, et cetera. BRUMFIEL: If all this sounds complicated, well, it is. And like all software, it's constantly being updated, too. KOWALCZYK: Platforms like Facebook and Google and others are basically changing the way that the buying of political advertising works on their platforms what seems like every day. BRUMFIEL: The rules, the technology, the strategy - it's all constantly evolving, and that could lead to more unexpected matchups. KOWALCZYK: As things change, yeah, I'm sure some weird stuff could happen. BRUMFIEL: Stuff voters will ultimately have to adjust to as America heads into a big election year online. Geoff Brumfiel, NPR News, Washington. ARI SHAPIRO, HOST:  Political advertising is surging online, even though the election is still over a year away. Campaigns are already spending tens of millions of dollars to promote their candidates on platforms like Facebook and Google, but some of those ads may be showing up in unintended places. NPR's Geoff Brumfiel has more. GEOFF BRUMFIEL, BYLINE: So a couple of days ago, I was tooling around on YouTube, looking at Russia's main propaganda channel, known as RT. (SOUNDBITE OF ARCHIVED RECORDING) ILYA PETRENKO: Kremlin talking points is what I was talking about, so why waste time on. . . BRUMFIEL: But then something really unexpected happened. In between videos, this ad popped up for President Trump. It was from the Trump Make America Great Again Committee, which is part of his 2020 re-election campaign. The ad showed up again and again before a video accusing the U. S. of meddling in the United Nations. . . (SOUNDBITE OF YOUTUBE VIDEO, \"EP. 806: WHISTLEBLOWER KATHARINE GUN- HOW THE US SPIED+BLACKMAILED TO GET SUPPORT FOR THE IRAQ WAR\") AFSHIN RATTANSI: I mean, you believe the plan was to spy on the U. N. Security Council members and then potentially blackmail them. BRUMFIEL: . . . And another one showing a Russian fighter jet confronting a NATO rival. In an email, Trump campaign communications director Tim Murtaugh told NPR, quote, \"we have not targeted those channels at all. \" So what's going on? LAURA EDELSON: The Trump campaign is a big advertiser. BRUMFIEL: Laura Edelson is a graduate student studying online political advertising at New York University. The Trump campaign has already spent over $12 million advertising on Google platforms, including YouTube. She says the type of ad I saw is designed to draw new supporters. It's probably being spread over a large swath of YouTube. A spokesperson for the company told me advertisers do have the option to prevent their ads from appearing on certain channels, but Edelson says the Trump campaign may just not have checked that box. EDELSON: They could do something like say, I want to advertise against news channels, and they just didn't exclude RT. That is a possibility. BRUMFIEL: Murtaugh didn't say whether the campaign had excluded RT, but he did say its main strategy is to target the user. That means advertising follows people the campaign wants to reach no matter what they're viewing on YouTube. Shannon Kowalczyk is with the progressive group ACRONYM, which is tracking online advertising this election season. She says going after viewers is pretty common. SHANNON KOWALCZYK: You know, a political advertiser can go in and define that they want to target more of a Democratic audience or more of a Republican-leaning audience. So if you're on RT, it may happen there. If you're on a cooking channel, it may happen there, et cetera. BRUMFIEL: If all this sounds complicated, well, it is. And like all software, it's constantly being updated, too. KOWALCZYK: Platforms like Facebook and Google and others are basically changing the way that the buying of political advertising works on their platforms what seems like every day. BRUMFIEL: The rules, the technology, the strategy - it's all constantly evolving, and that could lead to more unexpected matchups. KOWALCZYK: As things change, yeah, I'm sure some weird stuff could happen. BRUMFIEL: Stuff voters will ultimately have to adjust to as America heads into a big election year online. Geoff Brumfiel, NPR News, Washington.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-29-774400675": {"title": "Australia Accuses Google Of Misleading Consumers Over Location Data : NPR", "url": "https://www.npr.org/2019/10/29/774400675/australia-accuses-google-of-misleading-consumers-over-location-data", "author": "No author found", "published_date": "2019-10-29", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-29-774306250": {"title": "Report: More Than Half Of U.S. Children Now Own A Smartphone By Age 11 : NPR", "url": "https://www.npr.org/2019/10/29/774306250/report-more-than-half-of-u-s-children-now-own-a-smartphone-by-age-11", "author": "No author found", "published_date": "2019-10-29", "content": "DAVID GREENE, HOST:  OK. Here's a fact: More than half of American children now own a smartphone by the age of 11. That is according to a new national survey of kids' media use. So what are the implications of that? Well, NPR's Anya Kamenetz covers education, kids and tech. And she's been digging into these numbers and joins me. Hi, Anya. ANYA KAMENETZ, BYLINE: Hi, David. GREENE: Am I wrong to be a little shocked by this? Eleven years old, more than half of American kids have smartphones. What - well, tell me about this survey. KAMENETZ: You know, it's really interesting. So if you look around you, it may not be that surprising. But the survey was conducted by Common Sense Media, and they've been looking into kids and media use since 2003. And yes, we are seeing that a majority of kids are getting their first phones younger and younger. So in 2015, the last time they did this survey, most kids had a phone by age 14. And now it's just over half have their first phone by age 11. And about 1 in 5 have one by the age of 8 years old. GREENE: Eight years old? KAMENETZ: Yeah. GREENE: I mean, that - what does that even tell us about children now? KAMENETZ: Well, you know, it could be a lot of different things. And it truly depends on the family here, so I'm not necessarily hitting the panic button. Like, sometimes a kid might need to go between Dad's and Mom's house - right? - and they need the phone for communication. GREENE: Sure. KAMENETZ: Obviously, a phone, you know, is a connection to inappropriate content, cyberbullying, a lot of scary stuff. And some researchers say, you know, there could be a silver lining in that if your kid starts out with a phone earlier, as long as there's a strong parental hand guiding them and looking over their shoulder and taking the phone away, you know, certainly at bedtime and at other designated times, maybe that gives you longer to model healthy habits instead of kind of throwing them the phone at age 15, let's say, where they're really not listening to Mom and Dad. GREENE: OK. So beyond a safety necessity in some cases, like, did you learn about what kids are actually doing online when they have these things? KAMENETZ: So one piece of good news is we're actually not seeing kids spend a lot more time with media overall compared with 2015. What we have seen is they're watching less TV; they're watching more videos online. They're also spending more time using screens for homework, and that's a real challenge given the, you know, dangers of multitasking, which is not great for their brain. GREENE: So does the survey break it down in terms of which kids might have phones and which might not? KAMENETZ: Yeah. And there's actually some really interesting points there. For example, ownership of phones is very, very high across the board, and the lowest-income young people reported spending almost two hours per day more using screen media. And that is a really big difference compared to the most affluent youth. And first of all, this is a sign that the digital divide has a very different definition than it might have just five years ago. And there's so much that we don't know. I mean, are they spending more time unsupervised? What are they actually doing on the phones? And the big one is, is this a positive or a negative - right? - this class divide? We don't know. GREENE: Were there other divides that kind of struck you? KAMENETZ: Well, you might not be surprised that boys play more video games. Girls enjoy listening to music and reading more than boys do. They also say that they like social media a lot more, and they use it more often. And another really intriguing area is racial and ethnic divides. So black and Hispanic teens report that they spend much more time on social media. They like social media more. And that's unexpected, but it's also really intriguing because there's other research that suggests that young people, people of color as a group are more likely to value social media and possibly even use it as a bridge to civic engagement. So there's a lot more to dig into here. GREENE: Such interesting stuff. NPR education reporter Anya Kamenetz. Thanks, Anya. KAMENETZ: Thanks, David. DAVID GREENE, HOST:   OK. Here's a fact: More than half of American children now own a smartphone by the age of 11. That is according to a new national survey of kids' media use. So what are the implications of that? Well, NPR's Anya Kamenetz covers education, kids and tech. And she's been digging into these numbers and joins me. Hi, Anya. ANYA KAMENETZ, BYLINE: Hi, David. GREENE: Am I wrong to be a little shocked by this? Eleven years old, more than half of American kids have smartphones. What - well, tell me about this survey. KAMENETZ: You know, it's really interesting. So if you look around you, it may not be that surprising. But the survey was conducted by Common Sense Media, and they've been looking into kids and media use since 2003. And yes, we are seeing that a majority of kids are getting their first phones younger and younger. So in 2015, the last time they did this survey, most kids had a phone by age 14. And now it's just over half have their first phone by age 11. And about 1 in 5 have one by the age of 8 years old. GREENE: Eight years old? KAMENETZ: Yeah. GREENE: I mean, that - what does that even tell us about children now? KAMENETZ: Well, you know, it could be a lot of different things. And it truly depends on the family here, so I'm not necessarily hitting the panic button. Like, sometimes a kid might need to go between Dad's and Mom's house - right? - and they need the phone for communication. GREENE: Sure. KAMENETZ: Obviously, a phone, you know, is a connection to inappropriate content, cyberbullying, a lot of scary stuff. And some researchers say, you know, there could be a silver lining in that if your kid starts out with a phone earlier, as long as there's a strong parental hand guiding them and looking over their shoulder and taking the phone away, you know, certainly at bedtime and at other designated times, maybe that gives you longer to model healthy habits instead of kind of throwing them the phone at age 15, let's say, where they're really not listening to Mom and Dad. GREENE: OK. So beyond a safety necessity in some cases, like, did you learn about what kids are actually doing online when they have these things? KAMENETZ: So one piece of good news is we're actually not seeing kids spend a lot more time with media overall compared with 2015. What we have seen is they're watching less TV; they're watching more videos online. They're also spending more time using screens for homework, and that's a real challenge given the, you know, dangers of multitasking, which is not great for their brain. GREENE: So does the survey break it down in terms of which kids might have phones and which might not? KAMENETZ: Yeah. And there's actually some really interesting points there. For example, ownership of phones is very, very high across the board, and the lowest-income young people reported spending almost two hours per day more using screen media. And that is a really big difference compared to the most affluent youth. And first of all, this is a sign that the digital divide has a very different definition than it might have just five years ago. And there's so much that we don't know. I mean, are they spending more time unsupervised? What are they actually doing on the phones? And the big one is, is this a positive or a negative - right? - this class divide? We don't know. GREENE: Were there other divides that kind of struck you? KAMENETZ: Well, you might not be surprised that boys play more video games. Girls enjoy listening to music and reading more than boys do. They also say that they like social media a lot more, and they use it more often. And another really intriguing area is racial and ethnic divides. So black and Hispanic teens report that they spend much more time on social media. They like social media more. And that's unexpected, but it's also really intriguing because there's other research that suggests that young people, people of color as a group are more likely to value social media and possibly even use it as a bridge to civic engagement. So there's a lot more to dig into here. GREENE: Such interesting stuff. NPR education reporter Anya Kamenetz. Thanks, Anya. KAMENETZ: Thanks, David.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-30-774865522": {"title": "Twitter To Halt Political Ads, In Contrast To Facebook : NPR", "url": "https://www.npr.org/2019/10/30/774865522/twitter-to-halt-political-ads-in-contrast-to-facebook", "author": "No author found", "published_date": "2019-10-30", "content": "AILSA CHANG, HOST:  A political message should be earned, not bought. That comes from Twitter CEO Jack Dorsey via tweet, of course, in his announcement that Twitter will take the drastic step of no longer allowing political advertising. The decision comes as Facebook has been in the spotlight for refusing to hold politicians accountable for misleading or factually inaccurate ads. NPR's tech correspondent Shannon Bond has the story and joins us now. Hey, Shannon. SHANNON BOND, BYLINE: Hi. CHANG: So what's going on here? I mean, how did Dorsey explain why Twitter would no longer be taking any political ads? BOND: Yeah, so he says they're not going to take political ads anywhere in the world starting next month. And he says it's not about free speech or free expression. It's about whether politicians and campaigns should be allowed to pay to reach Twitter's hundreds of millions of users. Dorsey's argument here is that Internet ads are really different than other kinds of ads that we've seen before because the speed and the scale at which social media in particular lets misleading information spread and 'cause of the targeting that's allowed there, right? Those messages can end up in front of really susceptible people. And so he's also calling for more regulation of political ads that takes into account these differences and just how powerful the Internet is as a way to reach people. CHANG: I mean, this is a really major decision at a time when there is this ongoing big conversation about the role of social media and political advertising. BOND: That's right. Twitter's breaking, really, here with Facebook. I mean, most of all - I mean, that's - Facebook is sort of the elephant in the room here. CHANG: Yeah. BOND: There's this big debate going on about political ads, and Facebook in particular has taken this really hands-off approach. So the latest uproar in the past couple weeks about this was kicked off by this ad that the Trump campaign ran that contained false allegations about Joe Biden. And they ran that ad on Twitter, on Facebook, on YouTube. The Biden campaign complained, but the platforms all said that the ad didn't violate their policies. And that really maps with what these tech companies have been saying recently. They don't want to regulate speech, particularly political speech. And it's mostly that they don't want to be accused of bias by deciding what's true and what's not true and kind of being that judge. CHANG: I can't help but wonder - I mean, it can't be a coincidence that Mark Zuckerberg at Facebook was asked about all of this just last week when he was speaking at public events. How has Facebook been responding to all the scrutiny over political ads? BOND: Yeah, you're right. Zuckerberg has been all over the place talking about this. He gave a speech at Georgetown University. He was at Capitol Hill. And he's been defending this policy. So just a reminder - Facebook has said it's not going to fact-check political ads, even though it fact-checks all other kinds of ads. CHANG: Right. BOND: And that's left the door open for politicians to lie. People have been testing Facebook. Elizabeth Warren ran a deliberately false ad. And, you know, after Zuckerberg gave that speech at Georgetown, Dorsey went on the attack. You know, he said that there was a major flaw in Zuckerberg's argument that Facebook's just, you know, promoting free speech here. So in some ways, this isn't a super surprising change. And in the tweets, you know, today laying out this policy, Dorsey referred pretty clearly to Zuckerberg. You know, he said, you kind of can't say we don't want to spread misleading information, but it's OK if people want to pay us to spread misleading information. And he followed that up with a winking emoji. I think it's a pretty clear reference. CHANG: Still, how might this decision by Twitter to not take any more political ads affect profits there? BOND: It'll have some financial impact on Twitter. It's a popular place for politicians to advertise. And, you know, Twitter doesn't break out how much money it makes from political ads, but it says it's not a substantial portion of the total advertising sales. But it's not nothing. And, you know, the Trump campaign has already responded. And it said Twitter just walked away from hundreds of millions of dollars and called the decision a very dumb move. But ultimately, I think this means people are going to be asking, really, what it means for Facebook, the social media platform that's a lot bigger than Twitter and runs a lot more political ads. So the ball's now in Facebook's court. Are they going to follow suit? CHANG: That's NPR tech correspondent Shannon Bond. Thanks so much, Shannon. BOND: Thanks for having me. AILSA CHANG, HOST:   A political message should be earned, not bought. That comes from Twitter CEO Jack Dorsey via tweet, of course, in his announcement that Twitter will take the drastic step of no longer allowing political advertising. The decision comes as Facebook has been in the spotlight for refusing to hold politicians accountable for misleading or factually inaccurate ads. NPR's tech correspondent Shannon Bond has the story and joins us now. Hey, Shannon. SHANNON BOND, BYLINE: Hi. CHANG: So what's going on here? I mean, how did Dorsey explain why Twitter would no longer be taking any political ads? BOND: Yeah, so he says they're not going to take political ads anywhere in the world starting next month. And he says it's not about free speech or free expression. It's about whether politicians and campaigns should be allowed to pay to reach Twitter's hundreds of millions of users. Dorsey's argument here is that Internet ads are really different than other kinds of ads that we've seen before because the speed and the scale at which social media in particular lets misleading information spread and 'cause of the targeting that's allowed there, right? Those messages can end up in front of really susceptible people. And so he's also calling for more regulation of political ads that takes into account these differences and just how powerful the Internet is as a way to reach people. CHANG: I mean, this is a really major decision at a time when there is this ongoing big conversation about the role of social media and political advertising. BOND: That's right. Twitter's breaking, really, here with Facebook. I mean, most of all - I mean, that's - Facebook is sort of the elephant in the room here. CHANG: Yeah. BOND: There's this big debate going on about political ads, and Facebook in particular has taken this really hands-off approach. So the latest uproar in the past couple weeks about this was kicked off by this ad that the Trump campaign ran that contained false allegations about Joe Biden. And they ran that ad on Twitter, on Facebook, on YouTube. The Biden campaign complained, but the platforms all said that the ad didn't violate their policies. And that really maps with what these tech companies have been saying recently. They don't want to regulate speech, particularly political speech. And it's mostly that they don't want to be accused of bias by deciding what's true and what's not true and kind of being that judge. CHANG: I can't help but wonder - I mean, it can't be a coincidence that Mark Zuckerberg at Facebook was asked about all of this just last week when he was speaking at public events. How has Facebook been responding to all the scrutiny over political ads? BOND: Yeah, you're right. Zuckerberg has been all over the place talking about this. He gave a speech at Georgetown University. He was at Capitol Hill. And he's been defending this policy. So just a reminder - Facebook has said it's not going to fact-check political ads, even though it fact-checks all other kinds of ads. CHANG: Right. BOND: And that's left the door open for politicians to lie. People have been testing Facebook. Elizabeth Warren ran a deliberately false ad. And, you know, after Zuckerberg gave that speech at Georgetown, Dorsey went on the attack. You know, he said that there was a major flaw in Zuckerberg's argument that Facebook's just, you know, promoting free speech here. So in some ways, this isn't a super surprising change. And in the tweets, you know, today laying out this policy, Dorsey referred pretty clearly to Zuckerberg. You know, he said, you kind of can't say we don't want to spread misleading information, but it's OK if people want to pay us to spread misleading information. And he followed that up with a winking emoji. I think it's a pretty clear reference. CHANG: Still, how might this decision by Twitter to not take any more political ads affect profits there? BOND: It'll have some financial impact on Twitter. It's a popular place for politicians to advertise. And, you know, Twitter doesn't break out how much money it makes from political ads, but it says it's not a substantial portion of the total advertising sales. But it's not nothing. And, you know, the Trump campaign has already responded. And it said Twitter just walked away from hundreds of millions of dollars and called the decision a very dumb move. But ultimately, I think this means people are going to be asking, really, what it means for Facebook, the social media platform that's a lot bigger than Twitter and runs a lot more political ads. So the ball's now in Facebook's court. Are they going to follow suit? CHANG: That's NPR tech correspondent Shannon Bond. Thanks so much, Shannon. BOND: Thanks for having me.", "section": "Politics", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-30-774691659": {"title": "'Mr. Robot' Creator Sam Esmail Says Anxiety And Hacking Inspired The Show : NPR", "url": "https://www.npr.org/2019/10/30/774691659/mr-robot-creator-says-his-own-anxiety-and-hacking-helped-inspire-the-show", "author": "No author found", "published_date": "2019-10-30", "content": "TERRY GROSS, HOST: This is FRESH AIR. I'm Terry Gross. My guest Sam Esmail is the creator and lead writer and director of the TV series \"Mr. Robot,\" which is in its fourth and final season on USA Network. \"Mr. Robot\" stars Rami Malek as a hacker who's tried to withdraw from the world while, at the same time, he's trying to control it through his ability to hack into a giant corporate computer network. There's a blurry line between his attempts to be a force for good and his attempts to just burn the system down. Shadowy networks he's up against have made it hard for him to tell if he's the one in control or if he's being manipulated. He also doesn't know what's real and what's a figment of his imagination, and neither do we because he has dissociative identity disorder, DID, a mental health diagnosis previously known as multiple personality disorder. Director Sam Esmail is pretty knowledgeable about computer networks and hacking. In college, he was put on academic probation because of hacking. When he was 20, back in the dial-up era, before broadband, he created a startup designed to give easier dial-up access to the Internet. But his real ambition was always to make movies. Let's start with the opening scene of Season 1 of \"Mr. Robot. \" Elliot, Rami Malek's character, is a hacker who, at this point in the story, also works as a cybersecurity expert. He's just walked into a cafe called Ron's and has sat down at a table opposite the owner. (SOUNDBITE OF TV SHOW, \"MR. ROBOT\")RAMI MALEK: (As Elliot) You're Ron. But your real name's Rohit Mehta (ph). You changed it to Ron when you bought your first Ron's Coffee Shop six years ago. Now you got 17 of them, with eight more coming next quarter. SAMRAT CHAKRABARTI: (As Ron) Can I help you with something? MALEK: (As Elliot) I like coming here because your Wi-Fi was fast. I mean, you're one of the few spots that has a fiber connection with gigabit speed. It's good - so good it scratched that part of my mind, the Part that doesn't allow good to exist without conditions. So I started intercepting all the traffic on your network. That's when I noticed something strange. And I decided to hack you. CHAKRABARTI: (As Ron) Hack. . . MALEK: (As Elliot) I know you run a website called Plato's Boys (ph). CHAKRABARTI: (As Ron) Pardon me? MALEK: (As Elliot) You're using Tor networking to keep the servers anonymous. You made it really hard for anyone to see it. But I saw it. The onion routing protocol, it's not as anonymous as you think it is. Whoever's in control of the exit nodes is also in control of the traffic, which makes me the one in control. CHAKRABARTI: (As Ron) I must ask you to kindly leave. MALEK: (As Elliot) I own everything - all your emails, all your files, all your pics. CHAKRABARTI: (As Ron) Get out of here right now, or I'll call the. . . MALEK: (As Elliot) Police? Do you want them to find out about the 100 terabytes of child pornography you serve to your 400,000 users? Personally, man, I was hoping it was just going to be some BDSM stuff. You realize how much simpler that would have been? CHAKRABARTI: (As Ron) I did not hurt anyone - never did. That's my personal life. MALEK: (As Elliot) I understand what it's like to be different. I'm very different, too. I don't know how to talk to people. My dad was the only one I could talk to. But he died. GROSS: Sam Esmail, welcome to FRESH AIR. That is one of the great opening scenes in TV history. SAM ESMAIL: Oh, wow. Thank you. GROSS: (Laughter) It's so - it's really so good. And you know, it's funny because so much of what we hear in that scene comes in one way or another from your life. I mean, it's not autobiographical, but you can draw on what you know from it. I mean, so you did some hacking when you were in college. ESMAIL: (Laughter). GROSS: You edited porn, so you know something about the porn world - when you were scratching around for a living. ESMAIL: (Laughter) Right. GROSS: You know about being different because you grew up Egyptian American and Muslim in New Jersey. So, like - so many of the talking points in that opening scene I'm sure you could relate to. But you took what you know and then you drove it to a totally different place that I really think you did not know (laughter). ESMAIL: Yes, for sure. Honestly, it's - that outsider mentality is what I think the Elliot character and my life sort of overlap, and it has a lot to do with growing up in an Egyptian American household while trying to assimilate into American culture and that sort of tension, you know, because not only was I born in Jersey, then my parents took me down to South Carolina, which was (laughter) somewhat cruel for an Egyptian to be raised in, and then North Carolina. And yeah, I've always kind of constantly had to deal with this concept of this idea of being very different than the others. GROSS: What was your initial idea for \"Mr. Robot? \"ESMAIL: Well, I think, you know - I always say there's three factors that kind of led to creating \"Mr. Robot. \" No. 1 - you know, growing up being really into programming and kind of being a little fascinated with the hacker culture, I wanted to tell a movie or a television show or some sort of story that authentically represented that world because a lot of the movies that came out of Hollywood, especially in the late '90s and even then in the 2000s, were very corny and had nothing to do with, realistically, what hacker subculture was all about. I'd say the second and third things - because it kind of happened very close to one another - was the financial collapse in 2008 and then the Arab Spring that happened a few years after that. Those two things - this concept of a revolution, which is so tied into, again, the hacker subculture, those two things sort of kind of helped me develop the character of Elliot and why he wanted to be a hacker, why he wanted to go into this subculture, why he had this sort of grandiose vision of starting a revolution and changing the world. Because I was so moved - I was so horrified by the financial collapse and how sort of the top 1% were kind of doing these really criminal activities on a large scale and getting away with it and then seeing how technology helped people in the Arab countries rise up and have a voice and start a revolution. And so all those things sort of coalesced into the character of Elliot. And it started there, and then from there, the story of \"Mr. Robot\" kind of spun out. GROSS: Elliot is mentally ill. He has dissociative identity disorder, which used to be known as multiple personality disorder. So parts of himself he's kind of fragmented into other people who he thinks he's talking to or seeing. And we, the audience, it took us a very long time to learn that Elliot's father was actually one of - you know, a part of Elliot's personality, that Elliot's father is actually long dead. Why did you want Elliot to have dissociative identity disorder? ESMAIL: I think when we started - when I started piecing together who Elliot as a character is, I wanted to really represent his loneliness in a very authentic way. And because his isolationism is part of what drives him to hack people - I mean, that was the sort of irony or twist in his characters, is that he's so alone, but yet he's able to access sort of the most intimate details of everyone around him, to stay true to that kind of person, that kind of extreme that Elliot goes to. Dissociative identity disorder sort of fit what Elliot was sort of experiencing because he wasn't able to essentially connect to people. And sort of the contrast to that is that he just dissociates from them. And in fact, there's also this very profound fear that he has in even just talking to them. And you kind of see that in the pilot when he can't even bring himself to go into his friend's birthday party. So DID was just something that really fit, I think, what Elliot's journey was ultimately going to be about across the whole series, which is about this young man who cannot - through this deep fear and this sort of deep isolation, can't find a way to connect with other people. GROSS: Part of what we heard the Elliot character say - Rami Malek's character say, in that opening scene was the fantastic Wi-Fi at this coffee shop - he says, it scratched the part of my mind that doesn't allow good to exist without condition, which is a beautiful way of saying too good to be true. (LAUGHTER)GROSS: But I really like the words you chose to. . . ESMAIL: Thanks. GROSS: . . . Describe that. But does that describe you, that you're always thinking, this is too good to be true? This - that your mind doesn't allow good to exist without condition? ESMAIL: I think that - I mean, you know, I suffered a lot from social anxiety disorder. I still work through that even now. I had a deep fear of the world around me, and I think that's part of the journey we wanted to tell about Elliot. Honestly, again, that's something that we drew from hacker culture. There's a lot of paranoia; not only paranoia but, kind of beyond that, just a lot of distrust in what's going on around us. And I think that's where that line came from, is that can we fully allow ourselves to feel good about something without feeling like the other shoe is going to drop? And I think that's a constant anxiety that's just prevalent in society right now. GROSS: Well, let's take a short break here, and then we'll talk some more. If you're just joining us, my guest is Sam Esmail, who is the creator and lead director and writer of the series \"Mr. Robot,\" which is in its fourth and final season. We'll be right back. This is FRESH AIR. (SOUNDBITE OF MUSIC)GROSS: This is FRESH AIR. And if you're just joining us, my guest is Sam Esmail. He's the creator and lead writer and director of the USA Network series \"Mr. Robot,\" which is in its fourth and final season. And he was also the showrunner for the TV series \"Homecoming,\" which was adapted from a podcast of the same name. For the character of Elliot, the social anxiety and the dissociative identity disorder go hand in hand with him being a hacker because, really, the only way he can connect with people is through technology, and the only way he can learn about who people are is through hacking them. For you, did social anxiety issues connect with your - I don't want to say obsession. I don't know how - if you were obsessed or not, but connect with how involved you were with technology when you were in college? I mean, you created a startup. You worked in the computer lab. You did some hacking. ESMAIL: Yes. The short answer is yes. You know, I had social anxiety, which meant I did not want to go out a lot because I was afraid to go out. I was afraid to go to parties. I was afraid of embarrassing myself. So I stayed in my dorm room and - or I went to the computer lab, and instead, I tried to talk to people online. It felt safer. And when you are in that position where you're just behind a computer and essentially replacing a social life with that. So that when you're behind that computer and you have the sort of wherewithal to sort of pick up and try and learn how to get into people's email accounts or people's social media accounts - when I was in college, it was specifically email accounts - then that becomes your form of socializing with people. I think that's honestly the kind of more common route that hackers find themselves in. It's sort of the way hackers sort of - at least in my experience, the hackers that I knew growing up, that's how they became - that's how they started going down that road, is that - finding that replacement of connection with people. GROSS: And was that true for you, like, in your limited career as a hacker? (LAUGHTER)GROSS: Did you feel like you were replacing intimacy with hacking people's emails? ESMAIL: Oh, yeah. Yeah, yeah, yeah. And you know - I think I've spoken about this, and I'm not exactly proud about this - but I definitely hacked my girlfriend. Or you know, at the time, we were sort of off and on again, off again back in college. And then - I don't know. I hacked her college and sent out this email. And I was talking - you know, I was trying to essentially show off to her. And I sent out this blast email to the entire campus of her college and then got busted. And again, I'm not a very good hacker, and I got busted pretty easily because I used a - I worked at the computer lab at NYU, and they essentially traced it back to the employees' computer at the lab. And you know, it was time-stamped so - at the time when I was working there, and so they found out. They caught me pretty easily, and I got fired and put on probation. So that was sort of my - that was the time where I was like, OK, well, I'm going to give up on this hacking pursuit (laughter). It's just not for me. GROSS: Until you were caught, did it feel good to hack your girlfriend and her college? Or did it feel like you were violating her? ESMAIL: It - I'm trying to - I want to be honest - right? - because I think I should say, no, it felt terrible, and I shouldn't do this. But in the moment, it felt like the only way to know the truth. I guess that's the only way I can say it, is that I wanted to know how she actually felt about me because I didn't trust if she - if what she was telling me was true. And so I wanted to know if she really liked me. I thought maybe she didn't know or maybe she liked somebody else or - you know. And I don't know. I had - probably had 20 theories back then about what it was that we weren't connecting on. GROSS: And did you find out what you wanted to know? ESMAIL: No. No, I think it - ultimately, I think we weren't right for each other. I mean, I think that's the answer, right? I think I was not in a very healthy place with my social anxiety. It was way worse than it is today. I wasn't coping with it at all; I was indulging it. I was probably getting worse. And I think it was probably very difficult to be in any kind of relationship with me, not just romantic, friendship or any sort of one-on-one connection because I was - I had this real deep insecurity and deep fear of other people. GROSS: And now, I mean, like, you're a director. You have to not only connect with people; you have to give them advice about what the emotions are that their character is experiencing. You have to hold everything together and make sure that relationships are good on the set. They're hopefully good on the set (laughter). So how do you deal with your social anxiety in your job as director? ESMAIL: Well, I think my wife really - if it wasn't for my wife, I don't know if I would be able to handle my social anxiety as adequately, I will say (laughter), as I do right now, which is to say that - you know, on set, though, I will - you know, to me, the work is the work, right? I'm there to make - to tell the story as best I can. And I don't really have those same fears as if I'm at a party, where the whole point, the whole purpose, is to socialize and to chitchat and to share - you know, to share, open up about your private life and to ask people about their private life. And that's where my wife really, really was able to hold my hand. You know, I couldn't even go to any of the awards shows that first year - with \"Mr. Robot\" - without her being there holding my hand the entire time. GROSS: I want to ask you about casting Rami Malek. He's so good in the series. And man, his eyes are just so expressive. You can tell just by looking at his eyes what his state of mind is, like how paranoid he is at that moment, how worried he is. How did you realize through auditions how good he was? How did you choose him? ESMAIL: Well, I got to be honest with you. It was - in that sense, it was easy because when we were auditioning people - and we must have seen, I would say, close to 100 guys, if not more - the scene that we auditioned with was that coffee shop scene, and there was also this scene later on with his therapist where he kind of goes on this rant about society. And everybody - and again, we had great actors coming in. So it wasn't them. I always thought it was the script because they would come in and they would do those scenes, and they would do these sort of beautiful interpretations of the scene, but the character just came off very cold, very obnoxious. And I was almost, you know, going to tell USA, let's not do this. This doesn't make sense, or I got to rewrite this. It's just - I don't - I think this guy is annoying, and I don't want to - I don't think anybody's going to want to spend every week with this person. And then Rami came in. And when he did the scene, he added this vulnerability. It's almost - like, I can hear it in his voice just in that clip that you played, where it doesn't come off commanding or egotistical, even though the words are that. He added this subtext that it was coming from a place of real pain and real vulnerability and real wanting to connect. And that was the spark that really made that character come to life. GROSS: Why don't we hear the scene that you just mentioned, where he's talking to the psychiatrist? ESMAIL: OK. GROSS: And she basically wants to know, like, why are you so angry? Why are you so afraid? Like, what's bothering you? And he won't tell her anything, but what we're hearing is what he's thinking. This is Rami Malek from \"Mr. Robot. \"(SOUNDBITE OF TV SHOW, \"MR. ROBOT\")MALEK: (As Elliot) Oh, I don't know. Is it that we collectively thought Steve Jobs was a great man, even when we knew he made billions off the backs of children? Or maybe it's that it feels like all our heroes are counterfeits, the world itself just one big hoax, spamming each other with our running commentary [expletive] masquerading as insight, are social media faking intimacy? Or is it that we voted for this, not with our rigged elections but with our things, our property, our money? I'm not saying anything new. We all know why we do this; not because \"Hunger Games\" books makes us happy, but because we want to be sedated, because it's painful not to pretend, because we're cowards. GROSS: That's a scene from \"Mr. Robot. \" My guest is Sam Esmail, the creator and lead writer and director of the series. \"Mr. Robot\" is in its fourth and final season on the USA Network. After a break, we'll talk about growing up in an Egyptian Muslim family in New Jersey and South Carolina and Esmail's first film job editing porn. I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF MUSIC)GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to my interview with Sam Esmail, the creator and lead writer and director of the TV series \"Mr. Robot,\" which is in its fourth and final season on the USA Network. \"Mr. Robot\" stars Rami Malek as Elliot, a hacker who can't connect with people and feels uncomfortable around them. He's tried to withdraw from the world while, at the same time, he's tried to control it through his ability to hack into a giant corporate computer network. We see the world through Elliot's eyes, but we can't always trust what we see because he has dissociative identity disorder, or DID, a mental illness that used to be called multiple personality disorder. Rami Malek wears a hoodie in the character of Elliot. And, I mean, he wants to hide from the world. He doesn't want to be seen. He wants to look as invisible as he kind of feels. And when I interviewed Rami Malek in November of 2018, I asked him about wearing the hoodie. And I want to play for you what he had to say about the hoodie. (SOUNDBITE OF ARCHIVED NPR BROADCAST)MALEK: Before we did the pilot, I just walked around the streets of New York with my head down and the hoodie on. And I said to our costume designer early on, I want an outfit that if you are surveilling him from above or anywhere, he can put his head down and blend into the concrete. And that was the way I approached preparing for him. At some points, I would go into elevators, and I remembered trying to identify where the cameras were. And for days, I wanted to see if I could go undetected and not have any human contact with anyone. And that takes quite a toll on you, but it does make you aware of how much we are being watched in the world. GROSS: So that's Rami Malek. And Sam Esmail, I'm wondering what it was like for you to shoot him in a hoodie so much of the time because, as he said, he wants to be invisible from surveillance cameras. But you are a camera. I mean, (laughter) like. . . ESMAIL: (Laughter). GROSS: Your job is to train the camera on him. So as the director, what were the pros and cons of the hoodie? ESMAIL: Why I. . . GROSS: Of course, he takes it off, you know, often, like, in conversation. ESMAIL: Right, right, right. Well, I don't really know if there are any cons, honestly, because one thing about him wearing a hoodie in every scene is you don't ever (laughter) have to worry about continuity, and you can kind of always, like, you know, swap scenes around and not have to worry that he wasn't wearing that in the prior scene. He's always wearing that hoodie. But, you know, to me, I - that - so this is something directly lifted from my life. I wore a hoodie every day. And for me, that was easy to visualize. I'd visualize it just with myself walking down the street, knowing where to put the camera. And I loved that you could see that he was hiding. Even though I couldn't see his face at all times, it was the fact that we could see a piece of him, framing it that way. To me, just because you're - to - it's not about capturing someone's face. It's about capturing that person, that character, and always trying to tell a story with however you put - wherever you put the camera on that person. So it's not about just getting both eyes and having it symmetrical and - you know? We wanted the frame and always express what Elliot is doing, who he is. And so it was easy to - it was - it made - that made it easier because the limitations of where you can put the camera when Rami was in that hoodie made us just closer to who Elliot was. GROSS: So one of the characters in \"Mr. Robot\" now is a trans woman named Whiterose, who heads a group called the Dark Army, played by B. D. Wong. And publicly, she's known as Zhang, the male Chinese minister of state security. I want you to describe the character and why you wanted to create a trans woman character for this series. ESMAIL: Well, you know, DID is all about identity. And so we - so just starting with Elliot, here's a guy that wants to hide from the world, that wants to hide his identity, that wants to make it as anonymous as possible by hiding in a hoodie, blending into the concrete, like Rami said. And then to kind of then think about the opposite of that person, the ultimate sort of antagonist to that person, nemesis to that person, is this person that knows who she is, knows her identity. And it's the rest of the world that forces her to hide it. And I just thought that was such an interesting kind of counterpoint to what Elliot's emotional journey is about. Here is a woman who knows who she is but is told by society that she can't be that, and so she has to then find a way to still kind of hold on to that identity and stay true to that identity in spite of that. And so in that way, she's stronger than Elliot because she wants to define that person that she knows she is, that she believes she is. And so I love when a protagonist, antagonist really mirror each other like that in, really, the most, you know, sort of opposite extremes because it's that contrast that, I think, really kind of makes the most interesting conflict between the two. GROSS: Well, let's take a short break here, and then we'll talk some more. If you're just joining us, my guest is Sam Esmail, who is the creator and lead director and writer of the series \"Mr. Robot,\" which is in its fourth and final season. We'll be right back. This is FRESH AIR. (SOUNDBITE OF MUSIC)GROSS: This is FRESH AIR. If you're just joining us, my guest is Sam Esmail. He's the creator and lead director and writer of the TV series \"Mr. Robot,\" which is in its fourth and final season on USA Network. A heads up to parents - the next chapter includes a section that might not be suitable for young children. We'll be talking not explicitly about his first film job, which was editing porn. I think your first job working in film was editing porn. And was that your first? ESMAIL: Yep. GROSS: OK. ESMAIL: That would be my first. GROSS: So we got to talk about it (laughter). ESMAIL: Yeah, let's do it. GROSS: So, first of all, I mean, your parents are Egyptian and Muslim. So I - did they know that you were editing porn? ESMAIL: No. And, I mean, honestly, they didn't know - I didn't tell them - I got away with a lot because they were sort of closed off to American culture. And they weren't, honestly, that interested in my life. I mean, I wanted to be a filmmaker since I was 8. They kept being - I told them that, but they were sort of in denial about that for a long time and to the point where when I went to NYU, I just lied and told them I was going to - because they weren't going to pay for it if I went to - if I said I was going to go to film school. So I just told them I was in computer programming. And so - yeah. So I kept a lot of my sort of ambition into filmmaking, you know, a secret from them. And in fact, even now, I don't even - I mean, my mom kind of knows I have a TV show, but it's such a vague concept to her. She doesn't watch the show. She doesn't - I don't think she really sort of, you know, wraps her mind around it. GROSS: OK. So when you were editing porn, did you get turned on while you were editing? ESMAIL: No. GROSS: No. ESMAIL: No, I got turned off. I mean, you know, it's a lot of close-ups of human anatomy and not necessarily, like you know, the most hygienic human (laughter) anatomy shots. And, you know, it's - and it's a lot of - you get to see a lot of, like, the mechanics of it in terms of, you know, OK, stop, now do this and, you know, and there's a pause. I don't know how graphic you want to get, but, like, there's a lot pauses of, you know, people needing to - a refresher to get back into the mood. There's a lot of that going on. Yeah, it's not a turn-on at all. It's the opposite, I would say. GROSS: You probably can't describe it. I'm really wondering what the outtakes were like (laughter). ESMAIL: I mean, again, it's a lot of soft penises. . . (LAUGHTER)ESMAIL: . . . If you want me to be honest. It's a lot of guys freaking out and having to - 'cause, you know, the porn that I was editing was, you know, a lot - amateur porn. So it wasn't, like, big porn stars. So it was a lot of guys doing it for the first or second or third time. And, you know, they would freak out and a lot of times - you know, because I was also on the shoots, too, sometimes. And a lot of times, we would have to cancel it because it wasn't going to happen, you know? GROSS: So how did you make a salary editing amateur porn? 'Cause I always assume amateur porn is edited by the amateurs. ESMAIL: Oh, no, no, no. I mean, when I say amateur porn, I mean that they're not necessarily porn stars. GROSS: Oh, I see. ESMAIL: They're still - the production's still, you know, a porn. . . GROSS: Like amateur B films (laughter). ESMAIL: Exactly, exactly. Like - yes, the Roger Corman of porn. Yeah. I mean, honestly, that - it was - yeah, it still paid extremely well. I mean, at least, you know, being 26 and just having graduated film school, it paid my rent and then some. So I was kind of - you know, it was kind of, like, not necessarily happy to do it, but for me, because editing was, you know, came easy and it was on a computer and I kind of understood tech really well and I could figure out how to use Final Cut pretty easily, it was pretty easy for me - easy money for me. GROSS: And are we talking about the Internet porn era or is this still for theaters? ESMAIL: No, no, no. This was Internet porn for sure. Yeah. GROSS: OK. So titles - there are some great porn titles that are puns or plays on the titles of real movies or TV shows. Do you have any favorites? ESMAIL: No. The ones that I made were spinoffs of reality shows because reality shows had started to get really popular. And so what porn did was kind of jump on that. So I think the porn that I ended up editing a lot of was this one called \"Blind Sex Dates\" because at the time, there was a show called - I believe it was just called \"Blind Date\" and it was a reality show. It was obviously super fake, but it was a reality show where two people were on a blind date, and they followed them around. And so the spin on this was, like, you know, they were on the date for two minutes in the episode and then they went home and had sex for, you know, 45 minutes. And that was the \"Blind Sex Dates\" show. So that was the one that I worked on the most. GROSS: But then you ended up working on actual reality shows, right? ESMAIL: Yeah, which I got to say is not that different. It's just essentially the same thing minus the sex. GROSS: Can you mention the shows you worked on? ESMAIL: I worked on a reality show called \"I Married A Princess. . . \"(LAUGHTER)ESMAIL: . . . Which was about Casper Van Dien, who I was a huge fan of because of \"Starship Troopers. \" And he, I guess, married a princess, and they have this beautiful family. And the show was just about sort of their life - their home life. And then I briefly worked on a reality show for HBO called \"Tourgasm,\" which was Dane Cook's. It was, like, a one-off reality show where Dane Cook went around on tour. And it was just sort of behind the scenes of that. Then the DVD boom kind of happened. And that was back in the day when everyone was, like, buying box sets and special deluxe editions of every movie. And so we were editing all the making-of documentaries, you know, like, the behind-the-scenes footage of those movies. And that was really cool because, obviously, you know, wanting to be a filmmaker, I was able to kind of, you know, glimpse into the sort of - what the set life was like on all these big studio films. GROSS: So you grew up mostly in New Jersey, briefly lived in South Carolina. Your parents emigrated to the U. S. from Egypt. They're Muslim. Were you bullied or feeling left out because you were different when you were growing up in Jersey and South Carolina? ESMAIL: Oh, constantly. I remember when I was 5 I think I was called sand nigger so much that I didn't even know that that was a pejorative until, I think, I said it out loud in front of adults. My parents didn't even know 'cause I would say it in front of my parents and they wouldn't even know. Yeah. I got beat up. I remember the first day of kindergarten at - my older sister Nancy, she was born mentally challenged. And so she would get picked on a lot. And therefore, I would get sort of residual bullying from that. And so they were picking on her, I remember, on the first day of kindergarten at the bus stop and so, of course, I tried to stand up for her. And then I just got beat up by, you know, about five kids from the neighborhood. One of them actually ended up being one of my good friends, ironically enough, after that. But yeah, I probably got into more fights than I can remember all through elementary school and middle school. GROSS: You know, you mentioned you have a sister who's mentally challenged, and I'm wondering if that figured into your decision to create the Rami Malek character in \"Mr. Robot\" as somebody who is cognitively different. ESMAIL: I think so. I think - ultimately, it's more about being different. It's more about - I think it's more about that and that all - it's cumulative, right? It's a piece of that. It's a piece of just maybe being just not able to easily fit in with other people and being socially awkward. And it's - a lot of it had to do with being Egyptian and just being different. You know, my real name isn't Sam, and I changed it into Sam when I was 8 because that's how much I got bullied about it and made fun of at school. So. . . GROSS: Can I ask you what your name was? ESMAIL: I don't actually tell people. GROSS: Well, that's fine. ESMAIL: Sorry, Terry. GROSS: Yeah, that's fine. Yeah, I respect that. ESMAIL: Yeah. And, I mean, honestly, that's why - and that's how traumatizing I think it was for me growing up. So it's really about that. It was really about the fact that I was sort of pushed to - cast aside a lot. And I think it was all cumulative. I think it was all - the kind of being different, not being able to connect with other people and all of those things, all the details, my sister, my parents and the way I grew up in South Carolina - all that kind of fed into who Elliot became. GROSS: Well, let's take a break here, and then we'll talk some more. If you're just joining us, my guest is Sam Esmail, and he's the creator and lead writer and director of the USA Network series \"Mr. Robot,\" which is in its fourth and final season. We'll be right back. This is FRESH AIR. (SOUNDBITE OF MAC QUAYLE'S \"3. 0 6-WEDID17. TMP\")GROSS: This is FRESH AIR, and if you're just joining us, my guest is Sam Email, and he's the creator and lead writer and director of the USA Network series \"Mr. Robot,\" which is in its fourth and final season. You love movies. ESMAIL: Yes. GROSS: Did not seeing yourself represented in movies make you feel left out or distanced from what you loved? ESMAIL: No, it did the opposite. I thought I was not normal and the people in the movies were. And it just made me aspire to be them. So the reason I think my avenue (ph) - the reason why I love movies so much is that I could escape into that world and pretend I am those people, people that were normal, people that were more accepted, good looking people who were connected with other good looking people and who were powerful and successful and charismatic and could talk to each other. And it wasn't until later when, you know, I started watching Kubrick films that I realized, oh, you could actually talk about outsiders, too. You could actually channel the person that I knew I was deep down as well. And that's when I started to kind of realize the power of what - in terms of, like, what I could bring to it as a storyteller and how I can bring my experiences to the storytelling. Because, initially, it was just all about escapism. And then it became about something - channeling something more real inside me. GROSS: When you were watching movies and aspiring to be the attractive people who were friends with other attractive people, who were some of the people you were aspiring to be like? ESMAIL: Michael J. Fox, \"Back To The Future;\" Ralph Macchio, \"The Karate Kid,\" getting Elisabeth Shue. I mean, actually, weirdly, Elisabeth Shue was also in \"Back To The Future. \" Well, she was in the sequel, \"Back To The Future Part II\" - or Bill Murray in \"Ghostbusters,\" you know, and then I would watch - like, \"The French Connection\" I was obsessed with, and I was obsessed with Gene Hackman. And even though I - you know, my concept of good looking wasn't - it wasn't necessarily about that, but it was just about the energy, right? Gene Hackman was a badass in that movie. And he was going to go get the bad guys in these awesome car chases and, you know - so, yeah, those are the people that - or even, I'll tell you this, \"A Few Good Men,\" weirdly enough. I was 12 years old when I saw that movie. I didn't understand all the courtroom lingo, but I was obsessed with that movie. And Tom Cruise, you know, the way he sort of approached - his personality - I don't know if you remember the film, but his personality was so outspoken, so charming. He was cocky but in this fun way, in this endearing way. And I just remember so desperately wanting to be like him in that film. GROSS: What about Jack Nicholson saying you can't handle the truth? ESMAIL: Well - but then Tom Cruise, you know, brings him down and it was - so I was more - I mean, as much as I love Jack Nicholson and, you know - and he's obviously, you know, someone I admired in his own right, it was Tom Cruise being on the good side. I was never actually - it's weird because I think of Elliot as this morally ambiguous character. It's not quite the hero. He's, you know, described as an antihero. And I do love that about that character. But I think as a kid growing up watching the movies, I was always on the side of the hero. I was always on the side of the person who wanted to do good even if they didn't realize it. And so, yeah, I was - yeah, I was more on Tom Cruise's side or Daniel Kaffee I guess is his character's name. GROSS: Was it frustrating for you that your parents didn't like movies - they didn't care about movies? I don't know if they liked them or not, but they didn't care. They didn't go. You didn't go with them. So that was - like, something you were obsessed with was something you couldn't really share with them. ESMAIL: It was incredibly frustrating. I mean, now that - you know, that doesn't even - you got to remember; they didn't care about anything related to American culture. So yeah, movies, they would drop me off at noon at the movie theater. I would buy one ticket, and then I would stay until 10 p. m. And I would theater hop and, you know, sneak into other movies. GROSS: Oh, at the multiplex? ESMAIL: Yeah. I would see four movies. And then they'd pick me up at 10 p. m. at night. And I was 8, 9 when I was doing this, 10. They wouldn't even come into the movies, you know, with their son who was that young. And they would just - rather than spend the money, they would just pay for my ticket and leave me there. GROSS: So did you get to see adult things that you wouldn't have been able to see? ESMAIL: Oh, yeah. I mean, I remember I saw \"Robocop\" when I was really young. I think I was nine. And I don't know if you've ever seen \"RoboCop,\" but. . . GROSS: I have. ESMAIL: . . . There is a scene where they just torture Peter Weller's character and shooting limbs off with a gun and kind of laughing about it, and it's horrific. And yeah, I was 9 years old. And I think I bought a ticket to \"Karate Kid Part II,\" saw \"Karate Kid Part II,\" and then snuck into \"RoboCop. \"GROSS: So what impact did that violence have on your young mind? ESMAIL: Well, you got to - you know, prior to that, I was obsessed with slasher films, too - so \"Friday The 13th,\" \"Nightmare On Elm Street. \" So I saw gore. And I don't - see, to me - and this is the weird thing about people I know who are scared of watching horror films, you know, adults that are scared of watching horror films - I never took the violence in films that seriously. They never scared me, you know, to that extent. It was almost - I loved the visceral experience of it, the sort of high stakes of it. But it was never something that actually permeated me past that into some deep fear or anything like that. I just remember loving the impact that that could have on someone. And so, of course, then as me just being a young person who was aspiring to be a filmmaker, thought how do I use that as a storyteller to give off that same experience that I'm experiencing right now? GROSS: So was there a pivot point where you thought not only do I love movies, I'm going to make them? ESMAIL: It was \"E. T. \" I hate to bring this story up because it sounds like I'm dissing on Steven Spielberg, and I don't want him to think that. But if I'm being honest - and again, I'm a huge fan; let me just preface this by saying I'm a huge fan of Spielberg. And I love a lot of his films. And I would - but I would say that when I saw \"E. T. ,\" and I remember - it was the first movie I saw in the movie theater. And I remember everyone in school talking about it. And I remember just wanting - like, you know, I had not been allowed to go to the movie theater. I remember being like - and because back then, movies weren't coming out on video for a long time. So it was going to be years before I could see it on VHS. So I finally convinced my parents to let me go see it. And I was so excited. I mean, again, it was about - I remember hearing what it was about. It was about aliens. It was about a little boy who encounters aliens, and I just - and it was this action-adventure film. And so I was really excited. And then I saw it, and I remember being really bored. Again, this is the first time in the movie theater, and I was bored because I did not feel that visceral experience that I had watching \"French Connection\" or any of the bad slasher films I would watch. I didn't - it was more of a drama. I mean, you know, \"E. T. \" is more of this sort of lovely friendship between a boy and an alien. And it was - it wasn't - it was sort of not what I was expecting. But I do remember just being this egotistical little kid. I remember walking out of the movie theater disappointed and thinking, I can do better than that. And I remember that was the first thought I had about being a filmmaker. GROSS: Have you met Spielberg? ESMAIL: I have not (laughter). GROSS: You should prepare for the meeting. ESMAIL: I hope to God he's not offended. I know, right? GROSS: Figure out what you're going to say (laughter). ESMAIL: But you got to - I mean, look; I'm a huge \"Jurassic Park\" fan. I mean, that was a ride that I still remember and I still go on. Every once in a while, I'll pop that movie on. \"Schindler's List\" is obviously a masterpiece. So, Spielberg, if you're listening, I'm a fan. GROSS: Sam Esmail, it's just been great to talk with you. Thank you so much. ESMAIL: Likewise. Thank you. GROSS: Sam Esmail is the creator and the lead writer and director of the series \"Mr. Robot,\" which is in its fourth and final season on USA Network. Tomorrow on FRESH AIR, as the impeachment inquiry gains momentum, our guest will be the New York Times correspondent in Ukraine Andrew Kramer. He'll talk about the impact of President Trump's dealings with Ukraine on that country, the difficult balancing act facing Ukraine's new president, and why President Trump says Ukrainian officials tried to sabotage his campaign. I hope you'll join us. (SOUNDBITE OF GUILLERMO KLEIN'S \"MELODIA DE ARRABAL\")GROSS: FRESH AIR's executive producer is Danny Miller. Our technical director and engineer is Audrey Bentham. Our associate producer for digital media is Molly Seavy-Nesper. Roberta Shorrock directs the show. I'm Terry Gross. (SOUNDBITE OF GUILLERMO KLEIN'S \"MELODIA DE ARRABAL\") TERRY GROSS, HOST:  This is FRESH AIR. I'm Terry Gross. My guest Sam Esmail is the creator and lead writer and director of the TV series \"Mr. Robot,\" which is in its fourth and final season on USA Network. \"Mr. Robot\" stars Rami Malek as a hacker who's tried to withdraw from the world while, at the same time, he's trying to control it through his ability to hack into a giant corporate computer network. There's a blurry line between his attempts to be a force for good and his attempts to just burn the system down. Shadowy networks he's up against have made it hard for him to tell if he's the one in control or if he's being manipulated. He also doesn't know what's real and what's a figment of his imagination, and neither do we because he has dissociative identity disorder, DID, a mental health diagnosis previously known as multiple personality disorder. Director Sam Esmail is pretty knowledgeable about computer networks and hacking. In college, he was put on academic probation because of hacking. When he was 20, back in the dial-up era, before broadband, he created a startup designed to give easier dial-up access to the Internet. But his real ambition was always to make movies. Let's start with the opening scene of Season 1 of \"Mr. Robot. \" Elliot, Rami Malek's character, is a hacker who, at this point in the story, also works as a cybersecurity expert. He's just walked into a cafe called Ron's and has sat down at a table opposite the owner. (SOUNDBITE OF TV SHOW, \"MR. ROBOT\") RAMI MALEK: (As Elliot) You're Ron. But your real name's Rohit Mehta (ph). You changed it to Ron when you bought your first Ron's Coffee Shop six years ago. Now you got 17 of them, with eight more coming next quarter. SAMRAT CHAKRABARTI: (As Ron) Can I help you with something? MALEK: (As Elliot) I like coming here because your Wi-Fi was fast. I mean, you're one of the few spots that has a fiber connection with gigabit speed. It's good - so good it scratched that part of my mind, the Part that doesn't allow good to exist without conditions. So I started intercepting all the traffic on your network. That's when I noticed something strange. And I decided to hack you. CHAKRABARTI: (As Ron) Hack. . . MALEK: (As Elliot) I know you run a website called Plato's Boys (ph). CHAKRABARTI: (As Ron) Pardon me? MALEK: (As Elliot) You're using Tor networking to keep the servers anonymous. You made it really hard for anyone to see it. But I saw it. The onion routing protocol, it's not as anonymous as you think it is. Whoever's in control of the exit nodes is also in control of the traffic, which makes me the one in control. CHAKRABARTI: (As Ron) I must ask you to kindly leave. MALEK: (As Elliot) I own everything - all your emails, all your files, all your pics. CHAKRABARTI: (As Ron) Get out of here right now, or I'll call the. . . MALEK: (As Elliot) Police? Do you want them to find out about the 100 terabytes of child pornography you serve to your 400,000 users? Personally, man, I was hoping it was just going to be some BDSM stuff. You realize how much simpler that would have been? CHAKRABARTI: (As Ron) I did not hurt anyone - never did. That's my personal life. MALEK: (As Elliot) I understand what it's like to be different. I'm very different, too. I don't know how to talk to people. My dad was the only one I could talk to. But he died. GROSS: Sam Esmail, welcome to FRESH AIR. That is one of the great opening scenes in TV history. SAM ESMAIL: Oh, wow. Thank you. GROSS: (Laughter) It's so - it's really so good. And you know, it's funny because so much of what we hear in that scene comes in one way or another from your life. I mean, it's not autobiographical, but you can draw on what you know from it. I mean, so you did some hacking when you were in college. ESMAIL: (Laughter). GROSS: You edited porn, so you know something about the porn world - when you were scratching around for a living. ESMAIL: (Laughter) Right. GROSS: You know about being different because you grew up Egyptian American and Muslim in New Jersey. So, like - so many of the talking points in that opening scene I'm sure you could relate to. But you took what you know and then you drove it to a totally different place that I really think you did not know (laughter). ESMAIL: Yes, for sure. Honestly, it's - that outsider mentality is what I think the Elliot character and my life sort of overlap, and it has a lot to do with growing up in an Egyptian American household while trying to assimilate into American culture and that sort of tension, you know, because not only was I born in Jersey, then my parents took me down to South Carolina, which was (laughter) somewhat cruel for an Egyptian to be raised in, and then North Carolina. And yeah, I've always kind of constantly had to deal with this concept of this idea of being very different than the others. GROSS: What was your initial idea for \"Mr. Robot? \" ESMAIL: Well, I think, you know - I always say there's three factors that kind of led to creating \"Mr. Robot. \" No. 1 - you know, growing up being really into programming and kind of being a little fascinated with the hacker culture, I wanted to tell a movie or a television show or some sort of story that authentically represented that world because a lot of the movies that came out of Hollywood, especially in the late '90s and even then in the 2000s, were very corny and had nothing to do with, realistically, what hacker subculture was all about. I'd say the second and third things - because it kind of happened very close to one another - was the financial collapse in 2008 and then the Arab Spring that happened a few years after that. Those two things - this concept of a revolution, which is so tied into, again, the hacker subculture, those two things sort of kind of helped me develop the character of Elliot and why he wanted to be a hacker, why he wanted to go into this subculture, why he had this sort of grandiose vision of starting a revolution and changing the world. Because I was so moved - I was so horrified by the financial collapse and how sort of the top 1% were kind of doing these really criminal activities on a large scale and getting away with it and then seeing how technology helped people in the Arab countries rise up and have a voice and start a revolution. And so all those things sort of coalesced into the character of Elliot. And it started there, and then from there, the story of \"Mr. Robot\" kind of spun out. GROSS: Elliot is mentally ill. He has dissociative identity disorder, which used to be known as multiple personality disorder. So parts of himself he's kind of fragmented into other people who he thinks he's talking to or seeing. And we, the audience, it took us a very long time to learn that Elliot's father was actually one of - you know, a part of Elliot's personality, that Elliot's father is actually long dead. Why did you want Elliot to have dissociative identity disorder? ESMAIL: I think when we started - when I started piecing together who Elliot as a character is, I wanted to really represent his loneliness in a very authentic way. And because his isolationism is part of what drives him to hack people - I mean, that was the sort of irony or twist in his characters, is that he's so alone, but yet he's able to access sort of the most intimate details of everyone around him, to stay true to that kind of person, that kind of extreme that Elliot goes to. Dissociative identity disorder sort of fit what Elliot was sort of experiencing because he wasn't able to essentially connect to people. And sort of the contrast to that is that he just dissociates from them. And in fact, there's also this very profound fear that he has in even just talking to them. And you kind of see that in the pilot when he can't even bring himself to go into his friend's birthday party. So DID was just something that really fit, I think, what Elliot's journey was ultimately going to be about across the whole series, which is about this young man who cannot - through this deep fear and this sort of deep isolation, can't find a way to connect with other people. GROSS: Part of what we heard the Elliot character say - Rami Malek's character say, in that opening scene was the fantastic Wi-Fi at this coffee shop - he says, it scratched the part of my mind that doesn't allow good to exist without condition, which is a beautiful way of saying too good to be true. (LAUGHTER) GROSS: But I really like the words you chose to. . . ESMAIL: Thanks. GROSS: . . . Describe that. But does that describe you, that you're always thinking, this is too good to be true? This - that your mind doesn't allow good to exist without condition? ESMAIL: I think that - I mean, you know, I suffered a lot from social anxiety disorder. I still work through that even now. I had a deep fear of the world around me, and I think that's part of the journey we wanted to tell about Elliot. Honestly, again, that's something that we drew from hacker culture. There's a lot of paranoia; not only paranoia but, kind of beyond that, just a lot of distrust in what's going on around us. And I think that's where that line came from, is that can we fully allow ourselves to feel good about something without feeling like the other shoe is going to drop? And I think that's a constant anxiety that's just prevalent in society right now. GROSS: Well, let's take a short break here, and then we'll talk some more. If you're just joining us, my guest is Sam Esmail, who is the creator and lead director and writer of the series \"Mr. Robot,\" which is in its fourth and final season. We'll be right back. This is FRESH AIR. (SOUNDBITE OF MUSIC) GROSS: This is FRESH AIR. And if you're just joining us, my guest is Sam Esmail. He's the creator and lead writer and director of the USA Network series \"Mr. Robot,\" which is in its fourth and final season. And he was also the showrunner for the TV series \"Homecoming,\" which was adapted from a podcast of the same name. For the character of Elliot, the social anxiety and the dissociative identity disorder go hand in hand with him being a hacker because, really, the only way he can connect with people is through technology, and the only way he can learn about who people are is through hacking them. For you, did social anxiety issues connect with your - I don't want to say obsession. I don't know how - if you were obsessed or not, but connect with how involved you were with technology when you were in college? I mean, you created a startup. You worked in the computer lab. You did some hacking. ESMAIL: Yes. The short answer is yes. You know, I had social anxiety, which meant I did not want to go out a lot because I was afraid to go out. I was afraid to go to parties. I was afraid of embarrassing myself. So I stayed in my dorm room and - or I went to the computer lab, and instead, I tried to talk to people online. It felt safer. And when you are in that position where you're just behind a computer and essentially replacing a social life with that. So that when you're behind that computer and you have the sort of wherewithal to sort of pick up and try and learn how to get into people's email accounts or people's social media accounts - when I was in college, it was specifically email accounts - then that becomes your form of socializing with people. I think that's honestly the kind of more common route that hackers find themselves in. It's sort of the way hackers sort of - at least in my experience, the hackers that I knew growing up, that's how they became - that's how they started going down that road, is that - finding that replacement of connection with people. GROSS: And was that true for you, like, in your limited career as a hacker? (LAUGHTER) GROSS: Did you feel like you were replacing intimacy with hacking people's emails? ESMAIL: Oh, yeah. Yeah, yeah, yeah. And you know - I think I've spoken about this, and I'm not exactly proud about this - but I definitely hacked my girlfriend. Or you know, at the time, we were sort of off and on again, off again back in college. And then - I don't know. I hacked her college and sent out this email. And I was talking - you know, I was trying to essentially show off to her. And I sent out this blast email to the entire campus of her college and then got busted. And again, I'm not a very good hacker, and I got busted pretty easily because I used a - I worked at the computer lab at NYU, and they essentially traced it back to the employees' computer at the lab. And you know, it was time-stamped so - at the time when I was working there, and so they found out. They caught me pretty easily, and I got fired and put on probation. So that was sort of my - that was the time where I was like, OK, well, I'm going to give up on this hacking pursuit (laughter). It's just not for me. GROSS: Until you were caught, did it feel good to hack your girlfriend and her college? Or did it feel like you were violating her? ESMAIL: It - I'm trying to - I want to be honest - right? - because I think I should say, no, it felt terrible, and I shouldn't do this. But in the moment, it felt like the only way to know the truth. I guess that's the only way I can say it, is that I wanted to know how she actually felt about me because I didn't trust if she - if what she was telling me was true. And so I wanted to know if she really liked me. I thought maybe she didn't know or maybe she liked somebody else or - you know. And I don't know. I had - probably had 20 theories back then about what it was that we weren't connecting on. GROSS: And did you find out what you wanted to know? ESMAIL: No. No, I think it - ultimately, I think we weren't right for each other. I mean, I think that's the answer, right? I think I was not in a very healthy place with my social anxiety. It was way worse than it is today. I wasn't coping with it at all; I was indulging it. I was probably getting worse. And I think it was probably very difficult to be in any kind of relationship with me, not just romantic, friendship or any sort of one-on-one connection because I was - I had this real deep insecurity and deep fear of other people. GROSS: And now, I mean, like, you're a director. You have to not only connect with people; you have to give them advice about what the emotions are that their character is experiencing. You have to hold everything together and make sure that relationships are good on the set. They're hopefully good on the set (laughter). So how do you deal with your social anxiety in your job as director? ESMAIL: Well, I think my wife really - if it wasn't for my wife, I don't know if I would be able to handle my social anxiety as adequately, I will say (laughter), as I do right now, which is to say that - you know, on set, though, I will - you know, to me, the work is the work, right? I'm there to make - to tell the story as best I can. And I don't really have those same fears as if I'm at a party, where the whole point, the whole purpose, is to socialize and to chitchat and to share - you know, to share, open up about your private life and to ask people about their private life. And that's where my wife really, really was able to hold my hand. You know, I couldn't even go to any of the awards shows that first year - with \"Mr. Robot\" - without her being there holding my hand the entire time. GROSS: I want to ask you about casting Rami Malek. He's so good in the series. And man, his eyes are just so expressive. You can tell just by looking at his eyes what his state of mind is, like how paranoid he is at that moment, how worried he is. How did you realize through auditions how good he was? How did you choose him? ESMAIL: Well, I got to be honest with you. It was - in that sense, it was easy because when we were auditioning people - and we must have seen, I would say, close to 100 guys, if not more - the scene that we auditioned with was that coffee shop scene, and there was also this scene later on with his therapist where he kind of goes on this rant about society. And everybody - and again, we had great actors coming in. So it wasn't them. I always thought it was the script because they would come in and they would do those scenes, and they would do these sort of beautiful interpretations of the scene, but the character just came off very cold, very obnoxious. And I was almost, you know, going to tell USA, let's not do this. This doesn't make sense, or I got to rewrite this. It's just - I don't - I think this guy is annoying, and I don't want to - I don't think anybody's going to want to spend every week with this person. And then Rami came in. And when he did the scene, he added this vulnerability. It's almost - like, I can hear it in his voice just in that clip that you played, where it doesn't come off commanding or egotistical, even though the words are that. He added this subtext that it was coming from a place of real pain and real vulnerability and real wanting to connect. And that was the spark that really made that character come to life. GROSS: Why don't we hear the scene that you just mentioned, where he's talking to the psychiatrist? ESMAIL: OK. GROSS: And she basically wants to know, like, why are you so angry? Why are you so afraid? Like, what's bothering you? And he won't tell her anything, but what we're hearing is what he's thinking. This is Rami Malek from \"Mr. Robot. \" (SOUNDBITE OF TV SHOW, \"MR. ROBOT\") MALEK: (As Elliot) Oh, I don't know. Is it that we collectively thought Steve Jobs was a great man, even when we knew he made billions off the backs of children? Or maybe it's that it feels like all our heroes are counterfeits, the world itself just one big hoax, spamming each other with our running commentary [expletive] masquerading as insight, are social media faking intimacy? Or is it that we voted for this, not with our rigged elections but with our things, our property, our money? I'm not saying anything new. We all know why we do this; not because \"Hunger Games\" books makes us happy, but because we want to be sedated, because it's painful not to pretend, because we're cowards. GROSS: That's a scene from \"Mr. Robot. \" My guest is Sam Esmail, the creator and lead writer and director of the series. \"Mr. Robot\" is in its fourth and final season on the USA Network. After a break, we'll talk about growing up in an Egyptian Muslim family in New Jersey and South Carolina and Esmail's first film job editing porn. I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF MUSIC) GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to my interview with Sam Esmail, the creator and lead writer and director of the TV series \"Mr. Robot,\" which is in its fourth and final season on the USA Network. \"Mr. Robot\" stars Rami Malek as Elliot, a hacker who can't connect with people and feels uncomfortable around them. He's tried to withdraw from the world while, at the same time, he's tried to control it through his ability to hack into a giant corporate computer network. We see the world through Elliot's eyes, but we can't always trust what we see because he has dissociative identity disorder, or DID, a mental illness that used to be called multiple personality disorder. Rami Malek wears a hoodie in the character of Elliot. And, I mean, he wants to hide from the world. He doesn't want to be seen. He wants to look as invisible as he kind of feels. And when I interviewed Rami Malek in November of 2018, I asked him about wearing the hoodie. And I want to play for you what he had to say about the hoodie. (SOUNDBITE OF ARCHIVED NPR BROADCAST) MALEK: Before we did the pilot, I just walked around the streets of New York with my head down and the hoodie on. And I said to our costume designer early on, I want an outfit that if you are surveilling him from above or anywhere, he can put his head down and blend into the concrete. And that was the way I approached preparing for him. At some points, I would go into elevators, and I remembered trying to identify where the cameras were. And for days, I wanted to see if I could go undetected and not have any human contact with anyone. And that takes quite a toll on you, but it does make you aware of how much we are being watched in the world. GROSS: So that's Rami Malek. And Sam Esmail, I'm wondering what it was like for you to shoot him in a hoodie so much of the time because, as he said, he wants to be invisible from surveillance cameras. But you are a camera. I mean, (laughter) like. . . ESMAIL: (Laughter). GROSS: Your job is to train the camera on him. So as the director, what were the pros and cons of the hoodie? ESMAIL: Why I. . . GROSS: Of course, he takes it off, you know, often, like, in conversation. ESMAIL: Right, right, right. Well, I don't really know if there are any cons, honestly, because one thing about him wearing a hoodie in every scene is you don't ever (laughter) have to worry about continuity, and you can kind of always, like, you know, swap scenes around and not have to worry that he wasn't wearing that in the prior scene. He's always wearing that hoodie. But, you know, to me, I - that - so this is something directly lifted from my life. I wore a hoodie every day. And for me, that was easy to visualize. I'd visualize it just with myself walking down the street, knowing where to put the camera. And I loved that you could see that he was hiding. Even though I couldn't see his face at all times, it was the fact that we could see a piece of him, framing it that way. To me, just because you're - to - it's not about capturing someone's face. It's about capturing that person, that character, and always trying to tell a story with however you put - wherever you put the camera on that person. So it's not about just getting both eyes and having it symmetrical and - you know? We wanted the frame and always express what Elliot is doing, who he is. And so it was easy to - it was - it made - that made it easier because the limitations of where you can put the camera when Rami was in that hoodie made us just closer to who Elliot was. GROSS: So one of the characters in \"Mr. Robot\" now is a trans woman named Whiterose, who heads a group called the Dark Army, played by B. D. Wong. And publicly, she's known as Zhang, the male Chinese minister of state security. I want you to describe the character and why you wanted to create a trans woman character for this series. ESMAIL: Well, you know, DID is all about identity. And so we - so just starting with Elliot, here's a guy that wants to hide from the world, that wants to hide his identity, that wants to make it as anonymous as possible by hiding in a hoodie, blending into the concrete, like Rami said. And then to kind of then think about the opposite of that person, the ultimate sort of antagonist to that person, nemesis to that person, is this person that knows who she is, knows her identity. And it's the rest of the world that forces her to hide it. And I just thought that was such an interesting kind of counterpoint to what Elliot's emotional journey is about. Here is a woman who knows who she is but is told by society that she can't be that, and so she has to then find a way to still kind of hold on to that identity and stay true to that identity in spite of that. And so in that way, she's stronger than Elliot because she wants to define that person that she knows she is, that she believes she is. And so I love when a protagonist, antagonist really mirror each other like that in, really, the most, you know, sort of opposite extremes because it's that contrast that, I think, really kind of makes the most interesting conflict between the two. GROSS: Well, let's take a short break here, and then we'll talk some more. If you're just joining us, my guest is Sam Esmail, who is the creator and lead director and writer of the series \"Mr. Robot,\" which is in its fourth and final season. We'll be right back. This is FRESH AIR. (SOUNDBITE OF MUSIC) GROSS: This is FRESH AIR. If you're just joining us, my guest is Sam Esmail. He's the creator and lead director and writer of the TV series \"Mr. Robot,\" which is in its fourth and final season on USA Network. A heads up to parents - the next chapter includes a section that might not be suitable for young children. We'll be talking not explicitly about his first film job, which was editing porn. I think your first job working in film was editing porn. And was that your first? ESMAIL: Yep. GROSS: OK. ESMAIL: That would be my first. GROSS: So we got to talk about it (laughter). ESMAIL: Yeah, let's do it. GROSS: So, first of all, I mean, your parents are Egyptian and Muslim. So I - did they know that you were editing porn? ESMAIL: No. And, I mean, honestly, they didn't know - I didn't tell them - I got away with a lot because they were sort of closed off to American culture. And they weren't, honestly, that interested in my life. I mean, I wanted to be a filmmaker since I was 8. They kept being - I told them that, but they were sort of in denial about that for a long time and to the point where when I went to NYU, I just lied and told them I was going to - because they weren't going to pay for it if I went to - if I said I was going to go to film school. So I just told them I was in computer programming. And so - yeah. So I kept a lot of my sort of ambition into filmmaking, you know, a secret from them. And in fact, even now, I don't even - I mean, my mom kind of knows I have a TV show, but it's such a vague concept to her. She doesn't watch the show. She doesn't - I don't think she really sort of, you know, wraps her mind around it. GROSS: OK. So when you were editing porn, did you get turned on while you were editing? ESMAIL: No. GROSS: No. ESMAIL: No, I got turned off. I mean, you know, it's a lot of close-ups of human anatomy and not necessarily, like you know, the most hygienic human (laughter) anatomy shots. And, you know, it's - and it's a lot of - you get to see a lot of, like, the mechanics of it in terms of, you know, OK, stop, now do this and, you know, and there's a pause. I don't know how graphic you want to get, but, like, there's a lot pauses of, you know, people needing to - a refresher to get back into the mood. There's a lot of that going on. Yeah, it's not a turn-on at all. It's the opposite, I would say. GROSS: You probably can't describe it. I'm really wondering what the outtakes were like (laughter). ESMAIL: I mean, again, it's a lot of soft penises. . . (LAUGHTER) ESMAIL: . . . If you want me to be honest. It's a lot of guys freaking out and having to - 'cause, you know, the porn that I was editing was, you know, a lot - amateur porn. So it wasn't, like, big porn stars. So it was a lot of guys doing it for the first or second or third time. And, you know, they would freak out and a lot of times - you know, because I was also on the shoots, too, sometimes. And a lot of times, we would have to cancel it because it wasn't going to happen, you know? GROSS: So how did you make a salary editing amateur porn? 'Cause I always assume amateur porn is edited by the amateurs. ESMAIL: Oh, no, no, no. I mean, when I say amateur porn, I mean that they're not necessarily porn stars. GROSS: Oh, I see. ESMAIL: They're still - the production's still, you know, a porn. . . GROSS: Like amateur B films (laughter). ESMAIL: Exactly, exactly. Like - yes, the Roger Corman of porn. Yeah. I mean, honestly, that - it was - yeah, it still paid extremely well. I mean, at least, you know, being 26 and just having graduated film school, it paid my rent and then some. So I was kind of - you know, it was kind of, like, not necessarily happy to do it, but for me, because editing was, you know, came easy and it was on a computer and I kind of understood tech really well and I could figure out how to use Final Cut pretty easily, it was pretty easy for me - easy money for me. GROSS: And are we talking about the Internet porn era or is this still for theaters? ESMAIL: No, no, no. This was Internet porn for sure. Yeah. GROSS: OK. So titles - there are some great porn titles that are puns or plays on the titles of real movies or TV shows. Do you have any favorites? ESMAIL: No. The ones that I made were spinoffs of reality shows because reality shows had started to get really popular. And so what porn did was kind of jump on that. So I think the porn that I ended up editing a lot of was this one called \"Blind Sex Dates\" because at the time, there was a show called - I believe it was just called \"Blind Date\" and it was a reality show. It was obviously super fake, but it was a reality show where two people were on a blind date, and they followed them around. And so the spin on this was, like, you know, they were on the date for two minutes in the episode and then they went home and had sex for, you know, 45 minutes. And that was the \"Blind Sex Dates\" show. So that was the one that I worked on the most. GROSS: But then you ended up working on actual reality shows, right? ESMAIL: Yeah, which I got to say is not that different. It's just essentially the same thing minus the sex. GROSS: Can you mention the shows you worked on? ESMAIL: I worked on a reality show called \"I Married A Princess. . . \" (LAUGHTER) ESMAIL: . . . Which was about Casper Van Dien, who I was a huge fan of because of \"Starship Troopers. \" And he, I guess, married a princess, and they have this beautiful family. And the show was just about sort of their life - their home life. And then I briefly worked on a reality show for HBO called \"Tourgasm,\" which was Dane Cook's. It was, like, a one-off reality show where Dane Cook went around on tour. And it was just sort of behind the scenes of that. Then the DVD boom kind of happened. And that was back in the day when everyone was, like, buying box sets and special deluxe editions of every movie. And so we were editing all the making-of documentaries, you know, like, the behind-the-scenes footage of those movies. And that was really cool because, obviously, you know, wanting to be a filmmaker, I was able to kind of, you know, glimpse into the sort of - what the set life was like on all these big studio films. GROSS: So you grew up mostly in New Jersey, briefly lived in South Carolina. Your parents emigrated to the U. S. from Egypt. They're Muslim. Were you bullied or feeling left out because you were different when you were growing up in Jersey and South Carolina? ESMAIL: Oh, constantly. I remember when I was 5 I think I was called sand nigger so much that I didn't even know that that was a pejorative until, I think, I said it out loud in front of adults. My parents didn't even know 'cause I would say it in front of my parents and they wouldn't even know. Yeah. I got beat up. I remember the first day of kindergarten at - my older sister Nancy, she was born mentally challenged. And so she would get picked on a lot. And therefore, I would get sort of residual bullying from that. And so they were picking on her, I remember, on the first day of kindergarten at the bus stop and so, of course, I tried to stand up for her. And then I just got beat up by, you know, about five kids from the neighborhood. One of them actually ended up being one of my good friends, ironically enough, after that. But yeah, I probably got into more fights than I can remember all through elementary school and middle school. GROSS: You know, you mentioned you have a sister who's mentally challenged, and I'm wondering if that figured into your decision to create the Rami Malek character in \"Mr. Robot\" as somebody who is cognitively different. ESMAIL: I think so. I think - ultimately, it's more about being different. It's more about - I think it's more about that and that all - it's cumulative, right? It's a piece of that. It's a piece of just maybe being just not able to easily fit in with other people and being socially awkward. And it's - a lot of it had to do with being Egyptian and just being different. You know, my real name isn't Sam, and I changed it into Sam when I was 8 because that's how much I got bullied about it and made fun of at school. So. . . GROSS: Can I ask you what your name was? ESMAIL: I don't actually tell people. GROSS: Well, that's fine. ESMAIL: Sorry, Terry. GROSS: Yeah, that's fine. Yeah, I respect that. ESMAIL: Yeah. And, I mean, honestly, that's why - and that's how traumatizing I think it was for me growing up. So it's really about that. It was really about the fact that I was sort of pushed to - cast aside a lot. And I think it was all cumulative. I think it was all - the kind of being different, not being able to connect with other people and all of those things, all the details, my sister, my parents and the way I grew up in South Carolina - all that kind of fed into who Elliot became. GROSS: Well, let's take a break here, and then we'll talk some more. If you're just joining us, my guest is Sam Esmail, and he's the creator and lead writer and director of the USA Network series \"Mr. Robot,\" which is in its fourth and final season. We'll be right back. This is FRESH AIR. (SOUNDBITE OF MAC QUAYLE'S \"3. 0 6-WEDID17. TMP\") GROSS: This is FRESH AIR, and if you're just joining us, my guest is Sam Email, and he's the creator and lead writer and director of the USA Network series \"Mr. Robot,\" which is in its fourth and final season. You love movies. ESMAIL: Yes. GROSS: Did not seeing yourself represented in movies make you feel left out or distanced from what you loved? ESMAIL: No, it did the opposite. I thought I was not normal and the people in the movies were. And it just made me aspire to be them. So the reason I think my avenue (ph) - the reason why I love movies so much is that I could escape into that world and pretend I am those people, people that were normal, people that were more accepted, good looking people who were connected with other good looking people and who were powerful and successful and charismatic and could talk to each other. And it wasn't until later when, you know, I started watching Kubrick films that I realized, oh, you could actually talk about outsiders, too. You could actually channel the person that I knew I was deep down as well. And that's when I started to kind of realize the power of what - in terms of, like, what I could bring to it as a storyteller and how I can bring my experiences to the storytelling. Because, initially, it was just all about escapism. And then it became about something - channeling something more real inside me. GROSS: When you were watching movies and aspiring to be the attractive people who were friends with other attractive people, who were some of the people you were aspiring to be like? ESMAIL: Michael J. Fox, \"Back To The Future;\" Ralph Macchio, \"The Karate Kid,\" getting Elisabeth Shue. I mean, actually, weirdly, Elisabeth Shue was also in \"Back To The Future. \" Well, she was in the sequel, \"Back To The Future Part II\" - or Bill Murray in \"Ghostbusters,\" you know, and then I would watch - like, \"The French Connection\" I was obsessed with, and I was obsessed with Gene Hackman. And even though I - you know, my concept of good looking wasn't - it wasn't necessarily about that, but it was just about the energy, right? Gene Hackman was a badass in that movie. And he was going to go get the bad guys in these awesome car chases and, you know - so, yeah, those are the people that - or even, I'll tell you this, \"A Few Good Men,\" weirdly enough. I was 12 years old when I saw that movie. I didn't understand all the courtroom lingo, but I was obsessed with that movie. And Tom Cruise, you know, the way he sort of approached - his personality - I don't know if you remember the film, but his personality was so outspoken, so charming. He was cocky but in this fun way, in this endearing way. And I just remember so desperately wanting to be like him in that film. GROSS: What about Jack Nicholson saying you can't handle the truth? ESMAIL: Well - but then Tom Cruise, you know, brings him down and it was - so I was more - I mean, as much as I love Jack Nicholson and, you know - and he's obviously, you know, someone I admired in his own right, it was Tom Cruise being on the good side. I was never actually - it's weird because I think of Elliot as this morally ambiguous character. It's not quite the hero. He's, you know, described as an antihero. And I do love that about that character. But I think as a kid growing up watching the movies, I was always on the side of the hero. I was always on the side of the person who wanted to do good even if they didn't realize it. And so, yeah, I was - yeah, I was more on Tom Cruise's side or Daniel Kaffee I guess is his character's name. GROSS: Was it frustrating for you that your parents didn't like movies - they didn't care about movies? I don't know if they liked them or not, but they didn't care. They didn't go. You didn't go with them. So that was - like, something you were obsessed with was something you couldn't really share with them. ESMAIL: It was incredibly frustrating. I mean, now that - you know, that doesn't even - you got to remember; they didn't care about anything related to American culture. So yeah, movies, they would drop me off at noon at the movie theater. I would buy one ticket, and then I would stay until 10 p. m. And I would theater hop and, you know, sneak into other movies. GROSS: Oh, at the multiplex? ESMAIL: Yeah. I would see four movies. And then they'd pick me up at 10 p. m. at night. And I was 8, 9 when I was doing this, 10. They wouldn't even come into the movies, you know, with their son who was that young. And they would just - rather than spend the money, they would just pay for my ticket and leave me there. GROSS: So did you get to see adult things that you wouldn't have been able to see? ESMAIL: Oh, yeah. I mean, I remember I saw \"Robocop\" when I was really young. I think I was nine. And I don't know if you've ever seen \"RoboCop,\" but. . . GROSS: I have. ESMAIL: . . . There is a scene where they just torture Peter Weller's character and shooting limbs off with a gun and kind of laughing about it, and it's horrific. And yeah, I was 9 years old. And I think I bought a ticket to \"Karate Kid Part II,\" saw \"Karate Kid Part II,\" and then snuck into \"RoboCop. \" GROSS: So what impact did that violence have on your young mind? ESMAIL: Well, you got to - you know, prior to that, I was obsessed with slasher films, too - so \"Friday The 13th,\" \"Nightmare On Elm Street. \" So I saw gore. And I don't - see, to me - and this is the weird thing about people I know who are scared of watching horror films, you know, adults that are scared of watching horror films - I never took the violence in films that seriously. They never scared me, you know, to that extent. It was almost - I loved the visceral experience of it, the sort of high stakes of it. But it was never something that actually permeated me past that into some deep fear or anything like that. I just remember loving the impact that that could have on someone. And so, of course, then as me just being a young person who was aspiring to be a filmmaker, thought how do I use that as a storyteller to give off that same experience that I'm experiencing right now? GROSS: So was there a pivot point where you thought not only do I love movies, I'm going to make them? ESMAIL: It was \"E. T. \" I hate to bring this story up because it sounds like I'm dissing on Steven Spielberg, and I don't want him to think that. But if I'm being honest - and again, I'm a huge fan; let me just preface this by saying I'm a huge fan of Spielberg. And I love a lot of his films. And I would - but I would say that when I saw \"E. T. ,\" and I remember - it was the first movie I saw in the movie theater. And I remember everyone in school talking about it. And I remember just wanting - like, you know, I had not been allowed to go to the movie theater. I remember being like - and because back then, movies weren't coming out on video for a long time. So it was going to be years before I could see it on VHS. So I finally convinced my parents to let me go see it. And I was so excited. I mean, again, it was about - I remember hearing what it was about. It was about aliens. It was about a little boy who encounters aliens, and I just - and it was this action-adventure film. And so I was really excited. And then I saw it, and I remember being really bored. Again, this is the first time in the movie theater, and I was bored because I did not feel that visceral experience that I had watching \"French Connection\" or any of the bad slasher films I would watch. I didn't - it was more of a drama. I mean, you know, \"E. T. \" is more of this sort of lovely friendship between a boy and an alien. And it was - it wasn't - it was sort of not what I was expecting. But I do remember just being this egotistical little kid. I remember walking out of the movie theater disappointed and thinking, I can do better than that. And I remember that was the first thought I had about being a filmmaker. GROSS: Have you met Spielberg? ESMAIL: I have not (laughter). GROSS: You should prepare for the meeting. ESMAIL: I hope to God he's not offended. I know, right? GROSS: Figure out what you're going to say (laughter). ESMAIL: But you got to - I mean, look; I'm a huge \"Jurassic Park\" fan. I mean, that was a ride that I still remember and I still go on. Every once in a while, I'll pop that movie on. \"Schindler's List\" is obviously a masterpiece. So, Spielberg, if you're listening, I'm a fan. GROSS: Sam Esmail, it's just been great to talk with you. Thank you so much. ESMAIL: Likewise. Thank you. GROSS: Sam Esmail is the creator and the lead writer and director of the series \"Mr. Robot,\" which is in its fourth and final season on USA Network. Tomorrow on FRESH AIR, as the impeachment inquiry gains momentum, our guest will be the New York Times correspondent in Ukraine Andrew Kramer. He'll talk about the impact of President Trump's dealings with Ukraine on that country, the difficult balancing act facing Ukraine's new president, and why President Trump says Ukrainian officials tried to sabotage his campaign. I hope you'll join us. (SOUNDBITE OF GUILLERMO KLEIN'S \"MELODIA DE ARRABAL\") GROSS: FRESH AIR's executive producer is Danny Miller. Our technical director and engineer is Audrey Bentham. Our associate producer for digital media is Molly Seavy-Nesper. Roberta Shorrock directs the show. I'm Terry Gross. (SOUNDBITE OF GUILLERMO KLEIN'S \"MELODIA DE ARRABAL\")", "section": "Television", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-30-774749376": {"title": "Facebook Pays $643,000 Fine For Role In Cambridge Analytica Scandal : NPR", "url": "https://www.npr.org/2019/10/30/774749376/facebook-pays-643-000-fine-for-role-in-cambridge-analytica-scandal", "author": "No author found", "published_date": "2019-10-30", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-30-774726286": {"title": "Facebook Suspends Russian-Backed Accounts For Meddling In Africa : NPR", "url": "https://www.npr.org/2019/10/30/774726286/facebook-takes-down-accounts-tied-to-russian-businessman-behind-troll-factory", "author": "No author found", "published_date": "2019-10-30", "content": "", "section": "Technology", "disclaimer": ""}, "2019-10-30-774646244": {"title": "Facebook Feels Pressure On Political Ad Policy : NPR", "url": "https://www.npr.org/2019/10/30/774646244/facebook-feels-pressure-on-political-ad-policy", "author": "No author found", "published_date": "2019-10-30", "content": "STEVE INSKEEP, HOST: The pressure on Facebook to change the way it handles political ads includes some pressure from Facebook employees. They're joining lawmakers and civil rights groups who want the social network to revise its policy of letting politicians say almost anything in ads, even things that are not true. We should note that Facebook is among NPR's recent financial supporters. NPR tech correspondent Shannon Bond has more. SHANNON BOND, BYLINE: What can you say in a Facebook ad? That depends on what you're talking about and who you are. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: Our policy is that we do not fact-check politicians' speech, and the reason for that is that we believe that in a democracy, it is important that people can see for themselves what politicians are saying. BOND: That's Mark Zuckerberg at a congressional hearing last week. Facebook doesn't want to censor political speech, but he's now finding out how complicated that decision can be. The policy is under attack by critics who say the company gives politicians free rein to lie and makes it easy to spread those lies. And some of those critics work at Facebook. Workers posted an open letter to the company's internal message board. They say the policy lets politicians weaponize the platform. They want Facebook to hold political ads to the same standards as other ads, including being fact-checked. The letter got 250 signatures - not a huge amount in a company with almost 40,000 employees but. . . JEFF BERMAN: The employees at Facebook are notoriously quiet in challenging their leadership. And so for hundreds of them to come forward together is a big deal on its own. BOND: That's Jeff Berman, a tech executive who used to run policy and advertising for MySpace. Scrutiny of Facebook's influence on politics has been heating up with the 2020 election around the corner. And while workers are debating inside Facebook, there is also pressure from the outside. (SOUNDBITE OF ARCHIVED RECORDING)ALEXANDRIA OCASIO-CORTEZ: Would I be able to run advertisements on Facebook targeting Republicans in primaries saying that they voted for the Green New Deal? BOND: Democratic Congresswoman Alexandria Ocasio-Cortez of New York tried to pin down Zuckerberg at the hearing about whether she could run an ad with false information. Just days later, a political action committee followed through to make a point. It made an ad that appeared to show Republican Senator Lindsey Graham endorsing the Green New Deal. (SOUNDBITE OF AD)LINDSEY GRAHAM: From a Republican point of view, I think we need to look at the science, admit that climate change is real. Simply put - we believe in the Green New Deal. BOND: Graham never said that. The ad just edited videos to make it seem like he did. But here's the twist - Facebook took that ad down because it came from a political group, not a politician. But the challenges keep coming. The activists who made the Green New Deal ad registered as a candidate for California governor. Adriel Hampton says he did it to qualify as a politician in Facebook's eyes and to be allowed to run fake ads. ADRIEL HAMPTON: I spent $19 and, you know, half of a video editor's day and, you know, obviously, not a lot of PR but a very low level of effort to get this big of a reaction. BOND: But Facebook now says that because his candidacy is an attempt to get around its policies, his ads will be fact-checked. Critics say the problem is not just that Facebook lets politicians lie. It's about how those lies spread thanks to the way Facebook ads can be targeted to small groups. Here's Berman, the tech executive. BERMAN: The ability to microtarget to pick exactly the people with the right backgrounds, the right interests, where they live, who they engage with, et cetera, you now go from phishing with a dragnet to phishing with a spear. BOND: That means politicians can create specific versions of ads to target the people most susceptible to their messages, even if they're not true. Facebook says it's not changing its approach. It's resisted pressure from Washington so far. The question is whether it will resist its own employees. Shannon Bond, NPR News, San Francisco. (SOUNDBITE OF MARLEY CARROLL'S \"STARLINGS\") STEVE INSKEEP, HOST:  The pressure on Facebook to change the way it handles political ads includes some pressure from Facebook employees. They're joining lawmakers and civil rights groups who want the social network to revise its policy of letting politicians say almost anything in ads, even things that are not true. We should note that Facebook is among NPR's recent financial supporters. NPR tech correspondent Shannon Bond has more. SHANNON BOND, BYLINE: What can you say in a Facebook ad? That depends on what you're talking about and who you are. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: Our policy is that we do not fact-check politicians' speech, and the reason for that is that we believe that in a democracy, it is important that people can see for themselves what politicians are saying. BOND: That's Mark Zuckerberg at a congressional hearing last week. Facebook doesn't want to censor political speech, but he's now finding out how complicated that decision can be. The policy is under attack by critics who say the company gives politicians free rein to lie and makes it easy to spread those lies. And some of those critics work at Facebook. Workers posted an open letter to the company's internal message board. They say the policy lets politicians weaponize the platform. They want Facebook to hold political ads to the same standards as other ads, including being fact-checked. The letter got 250 signatures - not a huge amount in a company with almost 40,000 employees but. . . JEFF BERMAN: The employees at Facebook are notoriously quiet in challenging their leadership. And so for hundreds of them to come forward together is a big deal on its own. BOND: That's Jeff Berman, a tech executive who used to run policy and advertising for MySpace. Scrutiny of Facebook's influence on politics has been heating up with the 2020 election around the corner. And while workers are debating inside Facebook, there is also pressure from the outside. (SOUNDBITE OF ARCHIVED RECORDING) ALEXANDRIA OCASIO-CORTEZ: Would I be able to run advertisements on Facebook targeting Republicans in primaries saying that they voted for the Green New Deal? BOND: Democratic Congresswoman Alexandria Ocasio-Cortez of New York tried to pin down Zuckerberg at the hearing about whether she could run an ad with false information. Just days later, a political action committee followed through to make a point. It made an ad that appeared to show Republican Senator Lindsey Graham endorsing the Green New Deal. (SOUNDBITE OF AD) LINDSEY GRAHAM: From a Republican point of view, I think we need to look at the science, admit that climate change is real. Simply put - we believe in the Green New Deal. BOND: Graham never said that. The ad just edited videos to make it seem like he did. But here's the twist - Facebook took that ad down because it came from a political group, not a politician. But the challenges keep coming. The activists who made the Green New Deal ad registered as a candidate for California governor. Adriel Hampton says he did it to qualify as a politician in Facebook's eyes and to be allowed to run fake ads. ADRIEL HAMPTON: I spent $19 and, you know, half of a video editor's day and, you know, obviously, not a lot of PR but a very low level of effort to get this big of a reaction. BOND: But Facebook now says that because his candidacy is an attempt to get around its policies, his ads will be fact-checked. Critics say the problem is not just that Facebook lets politicians lie. It's about how those lies spread thanks to the way Facebook ads can be targeted to small groups. Here's Berman, the tech executive. BERMAN: The ability to microtarget to pick exactly the people with the right backgrounds, the right interests, where they live, who they engage with, et cetera, you now go from phishing with a dragnet to phishing with a spear. BOND: That means politicians can create specific versions of ads to target the people most susceptible to their messages, even if they're not true. Facebook says it's not changing its approach. It's resisted pressure from Washington so far. The question is whether it will resist its own employees. Shannon Bond, NPR News, San Francisco. (SOUNDBITE OF MARLEY CARROLL'S \"STARLINGS\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-10-31-774838891": {"title": "New Survey Examines Phone, Media Use Among U.S. Teens And Children : NPR", "url": "https://www.npr.org/2019/10/31/774838891/its-a-smartphone-life-more-than-half-of-u-s-children-now-have-one", "author": "No author found", "published_date": "2019-10-31", "content": "", "section": "Education", "disclaimer": ""}, "2019-10-31-774985413": {"title": "Twitter Bans Political Ads : NPR", "url": "https://www.npr.org/2019/10/31/774985413/twitter-bans-political-ads", "author": "No author found", "published_date": "2019-10-31", "content": "STEVE INSKEEP, HOST: Jack Dorsey says his company has had enough of political ads. Dorsey is the CEO of Twitter. And in a post on Twitter, he explains why the company will no longer accept that advertising. Here is part of what he writes - quote, \"the Internet political ads present entirely new challenges to civic discourse - machine-learning-based optimization of messaging and microtargeting, unchecked misleading information and deep fakes, all at increasing velocity, sophistication and overwhelming scale. \" There's a lot in there. That was a single tweet, 280 characters or less. NPR tech correspondent Shannon Bond will help us explain all that. She's in San Francisco. Good morning. SHANNON BOND, BYLINE: Hi, Steve. INSKEEP: I guess we should note we've heard in recent days from Facebook's Mark Zuckerberg, who says, hey, let's have the political debate, and even if candidates go out there and lie, we should see them lie and know that they lie. Why is Dorsey so different? BOND: I think Dorsey just has a really different worldview from Zuckerberg, as epitomized in that tweet that you read. I mean, he says that he thinks Internet ads for political campaigns and issues are too much for our democratic institutions to handle and that when they allow politicians to pay for these kinds of ads which show up in your Twitter feed as promoted tweets, that's forcing messages on people who didn't seek them out, the people who get targeted. That's very different than when people choose to follow a politician or see a retweet from someone else they follow. And this comes as Twitter has been struggling with how people use its platform and misuse its platform. This is the biggest step Dorsey's taken to try to fix things, you know, ranging from hate speech to bullying to disinformation. This is the most dramatic move. And on the other hand, you know, Zuckerberg has been defending Facebook's hands-off approach by saying it's really protecting free expression. INSKEEP: I want to make sure I understand how this works because, you know, I go on Twitter a fair amount, and I read other people's tweets, but I don't see things that are obvious ads. The Twitter timeline doesn't stop for a Budweiser ad or something. So what is it exactly we're talking about when we talk about Twitter ads? BOND: Right. It's not like TV. These aren't commercial breaks. You have seen ads, I'm almost sure, but you may not have noticed them. So they show up just like a regular tweet, and it just says in small letters at the bottom, promoted. I mean, that's what Twitter advertising is. It very much in, you know, in that feed and in the platform looks very similar. That's why it's really powerful - right? - because it's just part of the experience of scrolling through. INSKEEP: It's like native advertising, as they would say. It's just right there. It's part of this, but someone is paid to get it in front of my eyes, and it just looks like somebody's tweet. BOND: Right. And it may not be from somebody that you, yourself, actually have chosen to follow. INSKEEP: Well, let's talk about someone who has a lot of political content on Twitter, the president of the United States - many millions of followers, lots of political content there. How does this affect the president and political players like him? And what does he think about it? BOND: Well, Trump's 66 million Twitter followers will still be able to read all of his tweets and retweet them, and so we won't miss out on anything. INSKEEP: OK. BOND: What the difference is - he - well, he and other candidates for office and other politicians won't be able to pay to promote tweets that are about the election or other political issues. INSKEEP: Oh, so the fact - and we don't exactly know everything, but the fact that the president has so many millions of followers partly is because he makes news but partly is because he's pushing his messages out there with money. BOND: Yes, that's - I think that's a fair guess. INSKEEP: And what does his campaign think about the losing the ability to do that? BOND: Well, you know, they say it's a mistake. The - you know, his campaign manager said this is a big mistake and that Twitter was walking away from a lot of money here. You know, we should note, though, it's actually a small portion of Twitter's revenue. INSKEEP: Do Trump's Democratic rivals, who also, we should note, spend money on Twitter, think this is a good idea? BOND: Yeah. He's been applauded by many of them, including Hillary Clinton. INSKEEP: Shannon, thanks so much. BOND: Thank you. INSKEEP: That is NPR's Shannon Bond in San Francisco. STEVE INSKEEP, HOST:  Jack Dorsey says his company has had enough of political ads. Dorsey is the CEO of Twitter. And in a post on Twitter, he explains why the company will no longer accept that advertising. Here is part of what he writes - quote, \"the Internet political ads present entirely new challenges to civic discourse - machine-learning-based optimization of messaging and microtargeting, unchecked misleading information and deep fakes, all at increasing velocity, sophistication and overwhelming scale. \" There's a lot in there. That was a single tweet, 280 characters or less. NPR tech correspondent Shannon Bond will help us explain all that. She's in San Francisco. Good morning. SHANNON BOND, BYLINE: Hi, Steve. INSKEEP: I guess we should note we've heard in recent days from Facebook's Mark Zuckerberg, who says, hey, let's have the political debate, and even if candidates go out there and lie, we should see them lie and know that they lie. Why is Dorsey so different? BOND: I think Dorsey just has a really different worldview from Zuckerberg, as epitomized in that tweet that you read. I mean, he says that he thinks Internet ads for political campaigns and issues are too much for our democratic institutions to handle and that when they allow politicians to pay for these kinds of ads which show up in your Twitter feed as promoted tweets, that's forcing messages on people who didn't seek them out, the people who get targeted. That's very different than when people choose to follow a politician or see a retweet from someone else they follow. And this comes as Twitter has been struggling with how people use its platform and misuse its platform. This is the biggest step Dorsey's taken to try to fix things, you know, ranging from hate speech to bullying to disinformation. This is the most dramatic move. And on the other hand, you know, Zuckerberg has been defending Facebook's hands-off approach by saying it's really protecting free expression. INSKEEP: I want to make sure I understand how this works because, you know, I go on Twitter a fair amount, and I read other people's tweets, but I don't see things that are obvious ads. The Twitter timeline doesn't stop for a Budweiser ad or something. So what is it exactly we're talking about when we talk about Twitter ads? BOND: Right. It's not like TV. These aren't commercial breaks. You have seen ads, I'm almost sure, but you may not have noticed them. So they show up just like a regular tweet, and it just says in small letters at the bottom, promoted. I mean, that's what Twitter advertising is. It very much in, you know, in that feed and in the platform looks very similar. That's why it's really powerful - right? - because it's just part of the experience of scrolling through. INSKEEP: It's like native advertising, as they would say. It's just right there. It's part of this, but someone is paid to get it in front of my eyes, and it just looks like somebody's tweet. BOND: Right. And it may not be from somebody that you, yourself, actually have chosen to follow. INSKEEP: Well, let's talk about someone who has a lot of political content on Twitter, the president of the United States - many millions of followers, lots of political content there. How does this affect the president and political players like him? And what does he think about it? BOND: Well, Trump's 66 million Twitter followers will still be able to read all of his tweets and retweet them, and so we won't miss out on anything. INSKEEP: OK. BOND: What the difference is - he - well, he and other candidates for office and other politicians won't be able to pay to promote tweets that are about the election or other political issues. INSKEEP: Oh, so the fact - and we don't exactly know everything, but the fact that the president has so many millions of followers partly is because he makes news but partly is because he's pushing his messages out there with money. BOND: Yes, that's - I think that's a fair guess. INSKEEP: And what does his campaign think about the losing the ability to do that? BOND: Well, you know, they say it's a mistake. The - you know, his campaign manager said this is a big mistake and that Twitter was walking away from a lot of money here. You know, we should note, though, it's actually a small portion of Twitter's revenue. INSKEEP: Do Trump's Democratic rivals, who also, we should note, spend money on Twitter, think this is a good idea? BOND: Yeah. He's been applauded by many of them, including Hillary Clinton. INSKEEP: Shannon, thanks so much. BOND: Thank you. INSKEEP: That is NPR's Shannon Bond in San Francisco.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-01-775366588": {"title": "Russia 'Sovereign Internet' Law Gives Kremlin Sweeping Control Over Web : NPR", "url": "https://www.npr.org/2019/11/01/775366588/russian-law-takes-effect-that-gives-government-sweeping-power-over-internet", "author": "No author found", "published_date": "2019-11-01", "content": "", "section": "Technology", "disclaimer": ""}, "2019-11-01-775419131": {"title": "Google Buys Fitbit For $2.1 Billion, Pledges To Protect Health Data : NPR", "url": "https://www.npr.org/2019/11/01/775419131/google-buys-fitbit-for-2-1-billion-pledges-to-protect-health-data", "author": "No author found", "published_date": "2019-11-01", "content": "", "section": "Business", "disclaimer": ""}, "2019-11-01-774781133": {"title": "Game Review: 'Death Stranding' Is A Mess Worth Playing : NPR", "url": "https://www.npr.org/2019/11/01/774781133/new-game-death-stranding-is-a-compelling-mess", "author": "No author found", "published_date": "2019-11-01", "content": "MICHEL MARTIN, HOST: It's become something of an article of faith to say we live in divided times, especially politically. But we're going to spend the next part of the program focusing on efforts to bridge divides. A new video game by visionary game designer Hideo Kojima was released yesterday for the PlayStation 4. It's called Death Stranding. And it tasks players with reconnecting a divided America. NPR's Vincent Acovino says the game is frustrating but compelling. VINCENT ACOVINO, BYLINE: The opening images are of extinction - whales washed up on a beach, birds struck down by acid rain, former bustling cities reduced to rubble, all thanks to a not-so-far-off environmental disaster called the death stranding. (SOUNDBITE OF VIDEO GAME, \"DEATH STRANDING\")NORMAN REEDUS: (As Sam Porter Bridges) Once there was an explosion, a bang which gave rise to life as we know it. And then came the next explosion. ACOVINO: That's Sam Porter Bridges, played by \"The Walking Dead\" actor Norman Reedus. By trade, Sam is a courier or a delivery man. He's also part of an organization trying to reconnect an America torn apart by natural disaster and domestic terrorism. He does so with the help of a baby strapped to his chest that helps him fight interdimensional spirits. (SOUNDBITE OF BABY CRYING)ACOVINO: Death Stranding is outlandish. The games of Hideo Kojima are obtuse, cinematic, eccentric. His Metal Gear Solid series is famous for having hourlong cut scenes, B-movie dialogue and postmodern gameplay mechanics. Death Stranding bears all the marks of a Hideo Kojima title, for better and for worse. It has over-the-top characters like Heartman, who dies every 21 minutes. (SOUNDBITE OF VIDEO GAME, \"DEATH STRANDING\")WINDING REFN: (As Heartman) Defecation, pollution, nutrition - most of life's basic functions fit rather easily into a 21-minute time slot. ACOVINO: And as a game, Death Stranding is intentionally mundane. The simple act of moving around, which is a breeze in most video games, is treated like a puzzle in Death Stranding. You travel across jagged terrain while tending to various logistical details like balancing the weight of the cargo on your back or scanning a stretch of terrain to see if a cliff is too steep to scale or water too deep to walk across. Death Stranding is far from perfect. It has overwritten dialogue and serious pacing issues, but it's an earnest creative expression from a talented game designer. In an interview with the BBC, Kojima said that during a time when, quote, \"Trump is building a wall and the U. K. is leaving the EU,\" he wanted to make a game about using bridges to connect things. Games often pit us against one another. And just weeks after yet another Call Of Duty release, there's something to admire about Death Stranding's refusal to do so. Vincent Acovino, NPR News. MICHEL MARTIN, HOST:  It's become something of an article of faith to say we live in divided times, especially politically. But we're going to spend the next part of the program focusing on efforts to bridge divides. A new video game by visionary game designer Hideo Kojima was released yesterday for the PlayStation 4. It's called Death Stranding. And it tasks players with reconnecting a divided America. NPR's Vincent Acovino says the game is frustrating but compelling. VINCENT ACOVINO, BYLINE: The opening images are of extinction - whales washed up on a beach, birds struck down by acid rain, former bustling cities reduced to rubble, all thanks to a not-so-far-off environmental disaster called the death stranding. (SOUNDBITE OF VIDEO GAME, \"DEATH STRANDING\") NORMAN REEDUS: (As Sam Porter Bridges) Once there was an explosion, a bang which gave rise to life as we know it. And then came the next explosion. ACOVINO: That's Sam Porter Bridges, played by \"The Walking Dead\" actor Norman Reedus. By trade, Sam is a courier or a delivery man. He's also part of an organization trying to reconnect an America torn apart by natural disaster and domestic terrorism. He does so with the help of a baby strapped to his chest that helps him fight interdimensional spirits. (SOUNDBITE OF BABY CRYING) ACOVINO: Death Stranding is outlandish. The games of Hideo Kojima are obtuse, cinematic, eccentric. His Metal Gear Solid series is famous for having hourlong cut scenes, B-movie dialogue and postmodern gameplay mechanics. Death Stranding bears all the marks of a Hideo Kojima title, for better and for worse. It has over-the-top characters like Heartman, who dies every 21 minutes. (SOUNDBITE OF VIDEO GAME, \"DEATH STRANDING\") WINDING REFN: (As Heartman) Defecation, pollution, nutrition - most of life's basic functions fit rather easily into a 21-minute time slot. ACOVINO: And as a game, Death Stranding is intentionally mundane. The simple act of moving around, which is a breeze in most video games, is treated like a puzzle in Death Stranding. You travel across jagged terrain while tending to various logistical details like balancing the weight of the cargo on your back or scanning a stretch of terrain to see if a cliff is too steep to scale or water too deep to walk across. Death Stranding is far from perfect. It has overwritten dialogue and serious pacing issues, but it's an earnest creative expression from a talented game designer. In an interview with the BBC, Kojima said that during a time when, quote, \"Trump is building a wall and the U. K. is leaving the EU,\" he wanted to make a game about using bridges to connect things. Games often pit us against one another. And just weeks after yet another Call Of Duty release, there's something to admire about Death Stranding's refusal to do so. Vincent Acovino, NPR News.", "section": "Games & Humor", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-05-776578139": {"title": "FCC Approves T-Mobile/Sprint Merger, But 10 State AGs Still Object : NPR", "url": "https://www.npr.org/2019/11/05/776578139/fcc-clears-t-mobile-sprint-merger-deal", "author": "No author found", "published_date": "2019-11-05", "content": "", "section": "Business", "disclaimer": ""}, "2019-11-05-776488326": {"title": "FTC Tells Influencers How To Properly Disclose Ads On Social Media : NPR", "url": "https://www.npr.org/2019/11/05/776488326/ftc-issues-rules-for-disclosure-of-ads-by-social-media-influencers", "author": "No author found", "published_date": "2019-11-05", "content": "", "section": "Business", "disclaimer": ""}, "2019-11-05-774943933": {"title": "Facebook, Twitter, Google Under Pressure To Prep For Census Disinformation : NPR", "url": "https://www.npr.org/2019/11/05/774943933/social-media-sites-under-pressure-to-prep-for-census-trolls-and-interference", "author": "No author found", "published_date": "2019-11-05", "content": "", "section": "National", "disclaimer": ""}, "2019-11-06-777098293": {"title": "2 Former Twitter Employees Charged With Spying For Saudi Arabia : NPR", "url": "https://www.npr.org/2019/11/06/777098293/2-former-twitter-employees-charged-with-spying-for-saudi-arabia", "author": "No author found", "published_date": "2019-11-06", "content": "", "section": "National Security", "disclaimer": ""}, "2019-11-06-776840260": {"title": "Video Game Regulations For Chinese Minors Are Aimed At Reducing Addiction : NPR", "url": "https://www.npr.org/2019/11/06/776840260/china-introduces-restrictions-on-video-games-for-minors", "author": "No author found", "published_date": "2019-11-06", "content": "", "section": "Asia", "disclaimer": ""}, "2019-11-06-776905980": {"title": "Facebook Stalls California Investigation Into How The Company Handled User Data : NPR", "url": "https://www.npr.org/2019/11/06/776905980/california-attorney-general-seeks-court-order-to-force-facebook-to-turn-over-rec", "author": "No author found", "published_date": "2019-11-06", "content": "", "section": "Technology", "disclaimer": ""}, "2019-11-07-777438412": {"title": "Self-Driving Uber SUV Didn't Recognize Jaywalking Pedestrian In Fatal Crash : NPR", "url": "https://www.npr.org/2019/11/07/777438412/feds-say-self-driving-uber-suv-did-not-recognize-jaywalking-pedestrian-in-fatal-", "author": "No author found", "published_date": "2019-11-07", "content": "", "section": "Technology", "disclaimer": ""}, "2019-11-07-776834206": {"title": "Book Review: A Conspiracy Theory Is At The Heart Of 'The Mysterious Affair at Olivetti'  : NPR", "url": "https://www.npr.org/2019/11/07/776834206/the-mysterious-affair-at-olivetti-attempts-to-find-a-cold-war-conspiracy", "author": "No author found", "published_date": "2019-11-07", "content": "", "section": "Book Reviews", "disclaimer": ""}, "2019-11-07-776403310": {"title": "Internet Voting Is Becoming A Reality In Some States, Despite Cyber Fears : NPR", "url": "https://www.npr.org/2019/11/07/776403310/in-2020-some-americans-will-vote-on-their-phones-is-that-the-future", "author": "No author found", "published_date": "2019-11-07", "content": "", "section": "Politics", "disclaimer": ""}, "2019-11-09-777747209": {"title": "More Builders Are Selling Homes Wired For Tech But Data Privacy Is At Stake : NPR", "url": "https://www.npr.org/2019/11/09/777747209/a-smart-home-neighborhood-residents-find-it-enjoyably-convenient-or-a-bit-creepy", "author": "No author found", "published_date": "2019-11-09", "content": "STEVE INSKEEP, HOST: There's a new kind of community about an hour south of Amazon's headquarters in Seattle, Wash. What makes it a new kind of community is all the Amazon gear. The neighborhood in the town of Black Diamond is stuffed with so much technology you could call it a smart home laboratory. Its residents are learning what it's like to live in what may be the home of the future. They're finding smart homes offer convenience but also feed tech companies incredibly detailed information about their lives. KUOW's Joshua McNichols takes us there. JOSHUA MCNICHOLS, BYLINE: Is one of these the Amazon house? BRITTNEY SVACH: Yeah. So the one that has the sign in front of it that says Amazon Experience Center - that's the one that we're going to tour. MCNICHOLS: Brittney Svach works for Lennar, one of the nation's largest home builders, and she's been making the same sales pitch over and over to potential homebuyers in this Seattle suburb - control everything from your window blinds to your door locks using just your voice. ALEXA: The front door is locked. MCNICHOLS: Adjust the mood lighting or tell Roomba to clean up. (SOUNDBITE OF ROOMBA RUNNING)MCNICHOLS: Call up the feed from one of the countless video cameras on the smart television. SVACH: Alexa, show me the backyard. ALEXA: OK. SVACH: And now we can spy on whoever is having a drink on the back patio (laughter). MCNICHOLS: Cameras and convenience - that's the pitch. Drew Holmes (ph) wasn't looking for a smart home when he shopped for a house here. But the technologies came with the house. DREW HOLMES: Now I, like, would not live without them. MCNICHOLS: His favorite feature is a Ring Doorbell that logs visitors. HOLMES: I have a teenager. It's nice to confirm when they come home and I can - I have proof of it. MCNICHOLS: One time, Holmes was away on a business trip, and his stepdaughter forgot her key and couldn't get in. HOLMES: So she just texted me, hey, can you open the door? And I opened the door from Oregon, and so that was nice - problem solved. MCNICHOLS: There are other neighborhoods like this bubbling up outside tech-savvy cities like Miami and Denver. And I should mention Amazon is an NPR sponsor. In this neighborhood, Alexa's in every room. She adjusts the thermostat and reports on people's commutes when they roll out of bed. And she's getting better at it because she's watching and learning what people need. But that data collection worries Therron Smith (ph), and he went out of his way to buy a home here that does not include smart home technology. He didn't want the cloud to know every time his kids flip a light switch. THERRON SMITH: That data's not just sitting there just empty. Like, somebody's going to look at it and leverage it to try to turn a profit or try to create an ad or try to create some revenue. MCNICHOLS: That might be one reason smart homes aren't more popular already. A Zillow survey found people are just far more interested in air conditioning and ample storage than smart home technology. On the other hand, Dave Garland thinks the technology will take off once people try it. He's with Second Century Ventures, an investment arm of the National Association of Realtors. DAVE GARLAND: There is a new narrative when it comes to what home means. MCNICHOLS: It means a personalized environment where technology responds to your every need. Maybe it means giving up some privacy. These families are trying out that compromise. Fifteen-year-old Macey Ferguson (ph) says she likes it. She uses Alexa alarms - one for cheerleading practice and one for homework - to help her manage her busy life. MACEY FERGUSON: I just feel really fancy because I feel like she's my little, like, servant or butler - I don't (laughter) - butler. MCNICHOLS: But her mom, Kelli (ph), is more cautious. KELLI FERGUSON: If I'm walking on our street, I walk on the other side of the street. . . MCNICHOLS: The side without the smart homes. . . FERGUSON: Just because I don't feel like being on everyone's cameras. MCNICHOLS: And that's something we'll all have to learn how to navigate if this technology becomes standard in more neighborhoods. For NPR News, I'm Joshua McNichols in Seattle. (SOUNDBITE OF FREE THE ROBOTS' \"TURKISH VOODOO\") STEVE INSKEEP, HOST:  There's a new kind of community about an hour south of Amazon's headquarters in Seattle, Wash. What makes it a new kind of community is all the Amazon gear. The neighborhood in the town of Black Diamond is stuffed with so much technology you could call it a smart home laboratory. Its residents are learning what it's like to live in what may be the home of the future. They're finding smart homes offer convenience but also feed tech companies incredibly detailed information about their lives. KUOW's Joshua McNichols takes us there. JOSHUA MCNICHOLS, BYLINE: Is one of these the Amazon house? BRITTNEY SVACH: Yeah. So the one that has the sign in front of it that says Amazon Experience Center - that's the one that we're going to tour. MCNICHOLS: Brittney Svach works for Lennar, one of the nation's largest home builders, and she's been making the same sales pitch over and over to potential homebuyers in this Seattle suburb - control everything from your window blinds to your door locks using just your voice. ALEXA: The front door is locked. MCNICHOLS: Adjust the mood lighting or tell Roomba to clean up. (SOUNDBITE OF ROOMBA RUNNING) MCNICHOLS: Call up the feed from one of the countless video cameras on the smart television. SVACH: Alexa, show me the backyard. ALEXA: OK. SVACH: And now we can spy on whoever is having a drink on the back patio (laughter). MCNICHOLS: Cameras and convenience - that's the pitch. Drew Holmes (ph) wasn't looking for a smart home when he shopped for a house here. But the technologies came with the house. DREW HOLMES: Now I, like, would not live without them. MCNICHOLS: His favorite feature is a Ring Doorbell that logs visitors. HOLMES: I have a teenager. It's nice to confirm when they come home and I can - I have proof of it. MCNICHOLS: One time, Holmes was away on a business trip, and his stepdaughter forgot her key and couldn't get in. HOLMES: So she just texted me, hey, can you open the door? And I opened the door from Oregon, and so that was nice - problem solved. MCNICHOLS: There are other neighborhoods like this bubbling up outside tech-savvy cities like Miami and Denver. And I should mention Amazon is an NPR sponsor. In this neighborhood, Alexa's in every room. She adjusts the thermostat and reports on people's commutes when they roll out of bed. And she's getting better at it because she's watching and learning what people need. But that data collection worries Therron Smith (ph), and he went out of his way to buy a home here that does not include smart home technology. He didn't want the cloud to know every time his kids flip a light switch. THERRON SMITH: That data's not just sitting there just empty. Like, somebody's going to look at it and leverage it to try to turn a profit or try to create an ad or try to create some revenue. MCNICHOLS: That might be one reason smart homes aren't more popular already. A Zillow survey found people are just far more interested in air conditioning and ample storage than smart home technology. On the other hand, Dave Garland thinks the technology will take off once people try it. He's with Second Century Ventures, an investment arm of the National Association of Realtors. DAVE GARLAND: There is a new narrative when it comes to what home means. MCNICHOLS: It means a personalized environment where technology responds to your every need. Maybe it means giving up some privacy. These families are trying out that compromise. Fifteen-year-old Macey Ferguson (ph) says she likes it. She uses Alexa alarms - one for cheerleading practice and one for homework - to help her manage her busy life. MACEY FERGUSON: I just feel really fancy because I feel like she's my little, like, servant or butler - I don't (laughter) - butler. MCNICHOLS: But her mom, Kelli (ph), is more cautious. KELLI FERGUSON: If I'm walking on our street, I walk on the other side of the street. . . MCNICHOLS: The side without the smart homes. . . FERGUSON: Just because I don't feel like being on everyone's cameras. MCNICHOLS: And that's something we'll all have to learn how to navigate if this technology becomes standard in more neighborhoods. For NPR News, I'm Joshua McNichols in Seattle. (SOUNDBITE OF FREE THE ROBOTS' \"TURKISH VOODOO\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-09-777906177": {"title": "Instagram Will Test Hiding 'Likes' On Some U.S. Accounts Starting Next Week : NPR", "url": "https://www.npr.org/2019/11/09/777906177/instagram-will-test-hiding-likes-on-some-u-s-accounts-starting-next-week", "author": "No author found", "published_date": "2019-11-09", "content": "", "section": "Technology", "disclaimer": ""}, "2019-11-09-777887951": {"title": "Long-Lost Texts Arrive : NPR", "url": "https://www.npr.org/2019/11/09/777887951/long-lost-texts-arrive", "author": "No author found", "published_date": "2019-11-09", "content": "SCOTT SIMON, HOST:  Waiting for a text message from a lost love or word you've been nominated for an Oscar? Well, maybe it's not too late. Almost 170,000 text messages from last Valentine's Day, February 14, just finally arrived this past Thursday night because of a server issue - they call it an internal maintenance cycle - at Syniverse, a company that helps deliver texts for major U. S. carriers. Marissa Figueroa of Turlock, Calif. , got a text from an ex she doesn't talk to anymore. It just was not good for me and my mental health, she said. I like to think the text message sent to me I never saw was from the people who give MacArthur Genius Grants. Let me check now. (SOUNDBITE OF MUSIC) SCOTT SIMON, HOST:   Waiting for a text message from a lost love or word you've been nominated for an Oscar? Well, maybe it's not too late. Almost 170,000 text messages from last Valentine's Day, February 14, just finally arrived this past Thursday night because of a server issue - they call it an internal maintenance cycle - at Syniverse, a company that helps deliver texts for major U. S. carriers. Marissa Figueroa of Turlock, Calif. , got a text from an ex she doesn't talk to anymore. It just was not good for me and my mental health, she said. I like to think the text message sent to me I never saw was from the people who give MacArthur Genius Grants. Let me check now. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-14-777585675": {"title": "Amazon Appeals Pentagon's $10 Billion JEDI Cloud Contract Award To Microsoft : NPR", "url": "https://www.npr.org/2019/11/14/777585675/amazon-appeals-pentagons-choice-of-microsoft-for-10-billion-cloud-contract", "author": "No author found", "published_date": "2019-11-14", "content": "", "section": "Business", "disclaimer": ""}, "2019-11-14-779208282": {"title": "Google Health Data Project Under Scrutiny : NPR", "url": "https://www.npr.org/2019/11/14/779208282/google-health-data-project-under-scrutiny", "author": "No author found", "published_date": "2019-11-14", "content": "DAVID GREENE, HOST: So what are tech companies doing with your data? Google is handling the medical records of millions of Americans as part of a big push into the health care industry. A project between the tech giant and one of the country's largest hospital chains is under scrutiny by federal regulators and lawmakers who are asking if Google can really be trusted. This is just the latest example of big tech companies making forays into health care and also raising questions about patient privacy. And just a note here, Google is among NPR's financial supporters. I want to bring in NPR tech correspondent Shannon Bond. Good morning, Shannon. SHANNON BOND, BYLINE: Morning, David. GREENE: OK. So Google has a project that's called Project Nightingale. What is it? BOND: This is a deal that Google has struck with Ascension, which is a big Catholic health care system that operates hospitals and doctors' offices across 20 states and D. C. It was first reported about by the Wall Street Journal this week, and Google has since confirmed it. It says it has a contract to manage Ascension's clinical data. So that's test results and hospitalization records, your treatment records - you know, your whole health history. GREENE: Sure. BOND: The companies say they want to improve patient care, and they're working on artificial intelligence tools to help get there. And as anyone who's dealt with a doctor's office or, you know, a hospital knows, a lot of these systems are really fragmented. So one doctor might not have any idea what, you know, other parts of the system are doing. All these health care providers are looking to move electronic medical records into the cloud. And for tech companies, they - there's this massive business opportunity to do that, not just for Google, but the other tech giants too, like Apple and Amazon and Microsoft. GREENE: So all of this could be good for patients, I mean, if it organizes their medical records, if it can make recommendations for treatment and so forth. So take me to the other side of this, which is the worry. BOND: Yeah. I think it really comes down to privacy. Google already knows so much about all of us. Now we're talking about medical history, health information. This is sensitive stuff. And so critics are asking, what else might Google do with this data? It comes on the heels of Google's recent announcement that it's buying Fitbit, which makes fitness trackers. And it's spending $2 billion on that deal. You know, so there - these are the questions that are being raised. I spoke with Tiffany Li, a visiting technology law professor at Boston University. And she says these aggressive moves into health care by Google are going to increase the scrutiny the company is already under. TIFFANY LI: So any sort of acquisition or any sort of partnership that gives them more data will only give Google more power and more ability to be influential in the health care sector. BOND: Ascension and Google say Google's not allowed to use this data for its own advertising or research. The data's encrypted. It's on Google's server but in a space exclusively for Ascension. But Google does say that some Google employees working on the project have access to the data. And there are strict federal laws about who gets to handle patients' health information, and Google does say it's abiding by those laws. GREENE: And so all of that you just said, is that satisfying lawmakers and regulators? Or are they going to keep asking a lot of questions here? BOND: Well, this week, the U. S. Department of Health and Human Services says it wants more information about how the companies are collecting these medical records to make sure they are following the law. And Senator Mark Warner of Virginia said that Google and Ascension should stop this project while it's being reviewed. He told CNBC he's concerned about whether patients have been notified properly about this, you know, about their data going into Google's systems. Some critics are also saying that Google just doesn't have a great track record on privacy, especially when it comes to health data. They're - the company is being sued over how it handled health data for a research project with the University of Chicago. The company and the university hospital were accused of leaving identifiable information in the data. Now, Google's denied those accusations. And, you know, when I talk to people at the company, they say, look, there's a lot of competition. There's this race among tech companies to get into health care, and they don't want to let anything slow them down here. GREENE: NPR tech correspondent Shannon Bond. Thanks, Shannon. BOND: Thanks, David. DAVID GREENE, HOST:  So what are tech companies doing with your data? Google is handling the medical records of millions of Americans as part of a big push into the health care industry. A project between the tech giant and one of the country's largest hospital chains is under scrutiny by federal regulators and lawmakers who are asking if Google can really be trusted. This is just the latest example of big tech companies making forays into health care and also raising questions about patient privacy. And just a note here, Google is among NPR's financial supporters. I want to bring in NPR tech correspondent Shannon Bond. Good morning, Shannon. SHANNON BOND, BYLINE: Morning, David. GREENE: OK. So Google has a project that's called Project Nightingale. What is it? BOND: This is a deal that Google has struck with Ascension, which is a big Catholic health care system that operates hospitals and doctors' offices across 20 states and D. C. It was first reported about by the Wall Street Journal this week, and Google has since confirmed it. It says it has a contract to manage Ascension's clinical data. So that's test results and hospitalization records, your treatment records - you know, your whole health history. GREENE: Sure. BOND: The companies say they want to improve patient care, and they're working on artificial intelligence tools to help get there. And as anyone who's dealt with a doctor's office or, you know, a hospital knows, a lot of these systems are really fragmented. So one doctor might not have any idea what, you know, other parts of the system are doing. All these health care providers are looking to move electronic medical records into the cloud. And for tech companies, they - there's this massive business opportunity to do that, not just for Google, but the other tech giants too, like Apple and Amazon and Microsoft. GREENE: So all of this could be good for patients, I mean, if it organizes their medical records, if it can make recommendations for treatment and so forth. So take me to the other side of this, which is the worry. BOND: Yeah. I think it really comes down to privacy. Google already knows so much about all of us. Now we're talking about medical history, health information. This is sensitive stuff. And so critics are asking, what else might Google do with this data? It comes on the heels of Google's recent announcement that it's buying Fitbit, which makes fitness trackers. And it's spending $2 billion on that deal. You know, so there - these are the questions that are being raised. I spoke with Tiffany Li, a visiting technology law professor at Boston University. And she says these aggressive moves into health care by Google are going to increase the scrutiny the company is already under. TIFFANY LI: So any sort of acquisition or any sort of partnership that gives them more data will only give Google more power and more ability to be influential in the health care sector. BOND: Ascension and Google say Google's not allowed to use this data for its own advertising or research. The data's encrypted. It's on Google's server but in a space exclusively for Ascension. But Google does say that some Google employees working on the project have access to the data. And there are strict federal laws about who gets to handle patients' health information, and Google does say it's abiding by those laws. GREENE: And so all of that you just said, is that satisfying lawmakers and regulators? Or are they going to keep asking a lot of questions here? BOND: Well, this week, the U. S. Department of Health and Human Services says it wants more information about how the companies are collecting these medical records to make sure they are following the law. And Senator Mark Warner of Virginia said that Google and Ascension should stop this project while it's being reviewed. He told CNBC he's concerned about whether patients have been notified properly about this, you know, about their data going into Google's systems. Some critics are also saying that Google just doesn't have a great track record on privacy, especially when it comes to health data. They're - the company is being sued over how it handled health data for a research project with the University of Chicago. The company and the university hospital were accused of leaving identifiable information in the data. Now, Google's denied those accusations. And, you know, when I talk to people at the company, they say, look, there's a lot of competition. There's this race among tech companies to get into health care, and they don't want to let anything slow them down here. GREENE: NPR tech correspondent Shannon Bond. Thanks, Shannon. BOND: Thanks, David.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-16-779720295": {"title": "Lawyer Carrie Goldberg: Revenge Porn 'Isn't Speech' : NPR", "url": "https://www.npr.org/2019/11/16/779720295/this-isnt-speech-attorney-carrie-goldberg-on-revenge-porn", "author": "No author found", "published_date": "2019-11-16", "content": "MICHEL MARTIN, HOST: When Carrie Goldberg broke up with her boyfriend of a few months, frightening things started happening. He sent her hundreds of threatening messages, contacted friends, family and even work colleagues on Facebook to spread vicious lies about her. And that wasn't all. One night, she opened her laptop to find email after email containing intimate pictures of her, including a graphic video filmed without her consent. Goldberg, a lawyer, went to the police and was told there was nothing they could do. Thousands of dollars in legal fees and a restraining order later, Carrie Goldberg turned her traumatic episode into a career. She started her own firm to represent people who, like her, had been the targets of this kind of abuse. That's the focus of her new book, \"Nobody's Victim: Fighting Psychos, Stalkers, Pervs And Trolls. \" And Carrie Goldberg is with us now in our studios in New York. Carrie Goldberg, welcome. Thank you so much for joining us. CARRIE GOLDBERG: I'm so happy to be here. MARTIN: So there are so many eye-popping stories in your book, including your own. It's hard to know where to start. I mean, could you just give us a sense, as briefly as you can, of the range of things we are talking about here. GOLDBERG: This book covers stories that range from revenge porn to young people who've been sextorted to a high school student who was sexually assaulted outside of school. MARTIN: There is also a man you write about in the book who was also subjected to vicious behavior by a vengeful ex. Just tell us briefly what happened to him. GOLDBERG: So when Matthew (ph) first came to me, he told me about when it first started. Somebody had approached him while he was outside smoking a cigarette and walking his dog and recognized him. And Matthew had no idea who this man was and came to learn that he'd been sent through the gay dating app, Grindr. And Matthew's like, I don't even have an account on Grindr. There must be some mistake. And then a few minutes later, somebody else came. Over the next few months, it continued happening almost a thousand times. It turned out that Matthew's ex-boyfriend was impersonating him on Grindr. And Matthew did everything that any one of us would do. He made a dozen police reports. He got an order of protection against his ex. He flagged these profiles and nothing stopped it. MARTIN: So I wanted did to ask you about that. I mean, one of the points you make in the book is that the legal system hasn't caught up to the technologies that a lot of people use. For example, like, why is this site allowed to continue posting his image and his address even after he's contacted them and told them that it isn't him and it's being used to harass him? How is that possible? GOLDBERG: Well, we ended up getting a restraining order against Grindr, demanding that they exclude this particular user. And when their lawyers came to court, they told us that Grindr didn't have the technology to exclude a user. And that was flabbergasting to us. So we ended up bringing a consumer protection lawsuit, basically saying that Grindr had released into the stream of commerce a defectively designed and produced product. And turns out that there's a 1996 law called the Communications Decency Act, which has been interpreted so broadly over the last 23 years that Grindr basically had immunity from liability. And so this law, Section 230, is a protection that so many social media companies, websites, platforms hide behind. Basically, it gives them this false sense of confidence that they are outside the reach of our courts. So there's an argument that our laws cannot keep up with the technology. But, in actuality, we've had stalking and hacking laws. And now nonconsensual porn laws are in 46 states. We have the laws. But, sometimes, they're just really horribly interpreted. And that's the case with the Communications Decency Act. MARTIN: What about people, though, who do, as I understand it, continually make the free speech argument? And what do you say to that? GOLDBERG: This isn't about speech. Matthew's case - we weren't suing for anything that his offender said. We were suing for a products liability issue, you know? This was about conduct. I mean, they used to make this argument with nonconsensual porn, you know, that if we criminalize that, then we would lose all speech on the Internet. And now, you know, we have 46 states that have revenge porn laws. And I think, you know, speech is still pretty robust. MARTIN: How do you feel now that - you know? You went through this terrible experience. You turned your worst moment into a career where you can advocate for others who've gone through it. But now this book - I mean, I know that you're very well-known in legal circles, you know, for example. And you're certainly well-known in - among the community of people who've experienced this who have found, you know, legal counsel. But now you've written this book, and your story is there, you know, in the world. How does it make you feel to tell your story in this way? GOLDBERG: It's empowering to have told my story. But I have to admit that there are moments of confusion and fear and, oh, my God. Did I really put all of that in that book? And now, you know, when people tell me that they've read it, you know, there are parts of the book that I hadn't really talked about with almost anybody and something that I talk about in the conclusion, which was a really personal and frightening experience for me. And it's like, oh, my God, you know? Like, I've never even talked to my parents about that they know about it from the book. But, you know, there is still some discomfort with the fact that I've put all of this and all of these personal stories into the book. But I'm proud of it. And I feel that it's important. And one of the reasons I did it is because my clients give me everything. They tell me the most personal details of their stories. And then I put it into legal complaints, and those become public documents. So, you know, if they're being so willing and so trusting to expose themselves in order to advance their case and advance the law, then what am I doing if I'm not following that same model? MARTIN: Well, your book is very brave. And I think it will be very helpful. So thank you so much for writing it. Thank you so much for your important work. GOLDBERG: Thank you, Michel. MARTIN: That's Carrie Goldberg. Her book is called \"Nobody's Victim: Fighting Psychos, Stalkers, Pervs And Trolls. \" Carrie Goldberg, thank you so much for talking with us. GOLDBERG: Thank you. MICHEL MARTIN, HOST:  When Carrie Goldberg broke up with her boyfriend of a few months, frightening things started happening. He sent her hundreds of threatening messages, contacted friends, family and even work colleagues on Facebook to spread vicious lies about her. And that wasn't all. One night, she opened her laptop to find email after email containing intimate pictures of her, including a graphic video filmed without her consent. Goldberg, a lawyer, went to the police and was told there was nothing they could do. Thousands of dollars in legal fees and a restraining order later, Carrie Goldberg turned her traumatic episode into a career. She started her own firm to represent people who, like her, had been the targets of this kind of abuse. That's the focus of her new book, \"Nobody's Victim: Fighting Psychos, Stalkers, Pervs And Trolls. \" And Carrie Goldberg is with us now in our studios in New York. Carrie Goldberg, welcome. Thank you so much for joining us. CARRIE GOLDBERG: I'm so happy to be here. MARTIN: So there are so many eye-popping stories in your book, including your own. It's hard to know where to start. I mean, could you just give us a sense, as briefly as you can, of the range of things we are talking about here. GOLDBERG: This book covers stories that range from revenge porn to young people who've been sextorted to a high school student who was sexually assaulted outside of school. MARTIN: There is also a man you write about in the book who was also subjected to vicious behavior by a vengeful ex. Just tell us briefly what happened to him. GOLDBERG: So when Matthew (ph) first came to me, he told me about when it first started. Somebody had approached him while he was outside smoking a cigarette and walking his dog and recognized him. And Matthew had no idea who this man was and came to learn that he'd been sent through the gay dating app, Grindr. And Matthew's like, I don't even have an account on Grindr. There must be some mistake. And then a few minutes later, somebody else came. Over the next few months, it continued happening almost a thousand times. It turned out that Matthew's ex-boyfriend was impersonating him on Grindr. And Matthew did everything that any one of us would do. He made a dozen police reports. He got an order of protection against his ex. He flagged these profiles and nothing stopped it. MARTIN: So I wanted did to ask you about that. I mean, one of the points you make in the book is that the legal system hasn't caught up to the technologies that a lot of people use. For example, like, why is this site allowed to continue posting his image and his address even after he's contacted them and told them that it isn't him and it's being used to harass him? How is that possible? GOLDBERG: Well, we ended up getting a restraining order against Grindr, demanding that they exclude this particular user. And when their lawyers came to court, they told us that Grindr didn't have the technology to exclude a user. And that was flabbergasting to us. So we ended up bringing a consumer protection lawsuit, basically saying that Grindr had released into the stream of commerce a defectively designed and produced product. And turns out that there's a 1996 law called the Communications Decency Act, which has been interpreted so broadly over the last 23 years that Grindr basically had immunity from liability. And so this law, Section 230, is a protection that so many social media companies, websites, platforms hide behind. Basically, it gives them this false sense of confidence that they are outside the reach of our courts. So there's an argument that our laws cannot keep up with the technology. But, in actuality, we've had stalking and hacking laws. And now nonconsensual porn laws are in 46 states. We have the laws. But, sometimes, they're just really horribly interpreted. And that's the case with the Communications Decency Act. MARTIN: What about people, though, who do, as I understand it, continually make the free speech argument? And what do you say to that? GOLDBERG: This isn't about speech. Matthew's case - we weren't suing for anything that his offender said. We were suing for a products liability issue, you know? This was about conduct. I mean, they used to make this argument with nonconsensual porn, you know, that if we criminalize that, then we would lose all speech on the Internet. And now, you know, we have 46 states that have revenge porn laws. And I think, you know, speech is still pretty robust. MARTIN: How do you feel now that - you know? You went through this terrible experience. You turned your worst moment into a career where you can advocate for others who've gone through it. But now this book - I mean, I know that you're very well-known in legal circles, you know, for example. And you're certainly well-known in - among the community of people who've experienced this who have found, you know, legal counsel. But now you've written this book, and your story is there, you know, in the world. How does it make you feel to tell your story in this way? GOLDBERG: It's empowering to have told my story. But I have to admit that there are moments of confusion and fear and, oh, my God. Did I really put all of that in that book? And now, you know, when people tell me that they've read it, you know, there are parts of the book that I hadn't really talked about with almost anybody and something that I talk about in the conclusion, which was a really personal and frightening experience for me. And it's like, oh, my God, you know? Like, I've never even talked to my parents about that they know about it from the book. But, you know, there is still some discomfort with the fact that I've put all of this and all of these personal stories into the book. But I'm proud of it. And I feel that it's important. And one of the reasons I did it is because my clients give me everything. They tell me the most personal details of their stories. And then I put it into legal complaints, and those become public documents. So, you know, if they're being so willing and so trusting to expose themselves in order to advance their case and advance the law, then what am I doing if I'm not following that same model? MARTIN: Well, your book is very brave. And I think it will be very helpful. So thank you so much for writing it. Thank you so much for your important work. GOLDBERG: Thank you, Michel. MARTIN: That's Carrie Goldberg. Her book is called \"Nobody's Victim: Fighting Psychos, Stalkers, Pervs And Trolls. \" Carrie Goldberg, thank you so much for talking with us. GOLDBERG: Thank you.", "section": "Author Interviews", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-18-780473704": {"title": "U.S. Grants Huawei Third Reprieve From Ban : NPR", "url": "https://www.npr.org/2019/11/18/780473704/u-s-firms-get-90-day-extension-to-work-with-huawei-on-rural-networks", "author": "No author found", "published_date": "2019-11-18", "content": "", "section": "Technology", "disclaimer": ""}, "2019-11-18-778894491": {"title": "Cybercriminals Are Hacking Human Nature To Steal Billions From Us : NPR", "url": "https://www.npr.org/2019/11/18/778894491/cybercrime-booms-as-scammers-hack-human-nature-to-steal-billions", "author": "No author found", "published_date": "2019-11-18", "content": "AUDIE CORNISH, HOST: By now, cybercrime is a routine danger. But as ordinary as it seems, it's still really bad for businesses. A company can lose thousands, even millions to a single deceptive email to an employee. And as NPR's Martin Kaste reports, business has never been better for the scammers. (SOUNDBITE OF PHONE RINGING)MARTIN KASTE, BYLINE: This latest wave of cybercrime against American businesses really got rolling about two years ago, and this is one of the earliest victims. It's a real estate company in Seattle. One of the owners here is named Mark. He'd rather we don't give his last name. MARK: We're somewhat experienced businesspeople. The idea that we've been duped makes you feel pretty stupid. KASTE: That reluctance to talk about this? More about that in a minute. But first, the scam. Mark had been wrapping up a project and emailing with an investment partner. What they didn't realize was someone had hacked into their email traffic. MARK: It was clear that they had studied our conversation. KASTE: Because the scammers knew just the right moment to insert themselves into that conversation. MARK: The cadence and the timing and the email was so normal that it wasn't suspicious at all. It was just like we were continuing to have a conversation, but I just wasn't having it with the person I thought I was. KASTE: That person had picked up on the fact that Mark was about to send his partner some money. So, pretending to be the partner, the scammers sent him wiring instructions to a different account at his usual bank. Mark didn't think twice. A little later, he texted his partner to see if he'd got the money. MARK: And there was an immediate reaction and response from him, you know, question mark, what wire? And, oh, it was a cold sweat. KASTE: The $50,000 he'd wired was gone, already rewired from the American bank to an account overseas. Mark was a victim of a growing category of cybercrime that's called business email compromise or BEC. But don't let that bland name fool you. PATRICK PETERSON: What we've seen in 2019 is that the wave that's breaking is primarily focused around social engineering. KASTE: Patrick Peterson is CEO of Agari, a company that specializes in protecting corporate email systems. And he sees a lot of these scams up close. When he says social engineering, what he means is hacks that are based not so much on breaking into software, but rather on fooling people. PETERSON: It's not so much having the most sophisticated evil technology, it's using our own trust and desire to communicate with others against us. KASTE: He says these schemes are usually run by international networks - you know, those Nigerian prince emails in the early days of the Internet. It's still similar groups, but now they're more focused on researching their victims. When they break into a company's email, they're patient. They just lurk there for a while. PETERSON: And then they can sit there and watch the email go back and forth. And they can see this person pays a lot of invoices or sends a lot of accounts payable. And at the right time, we'll send one that has our payable instructions. KASTE: And given the sums that businesses move around on a daily basis, the payoff can be enormous. JAMES ABBOTT: In 2016, we had business email compromise schemes at $361 million. KASTE: This is James Abbott. He's a supervisory special agent with the FBI, specializing in BEC fraud. ABBOTT: 2017, that number jumped to 676 million. In 2018, we're at nearly 1. 2 billion. But the thing to keep in mind with these statistics is this is just what we're aware of. KASTE: Millions of dollars in fraud goes unreported because embarrassed businesses prefer to keep their losses quiet. Investigators say this kind of secrecy helps the scammers because it keeps their tricks less visible. The FBI's Abbott says businesses are also too quick to assume that the culprits are all overseas and untouchable. ABBOTT: That is absolutely not the case. There are many times where the victim is sending their money to what we consider a money mule located right in their backyard or another part of the United States. KASTE: Money mules are people here in the country who set up bank accounts to receive the diverted funds. The foreign scammers need these American accounts because overseas bank accounts would raise suspicions. Nayib Hassan is the friend and lawyer of one of these money mules, a man named Alfredo Veloso. NAYIB HASSAN: Alfredo's just your run-of-the-mill individual that you see anywhere else. I mean, he's not going to be your Nigerian. He's not going to be your - from anywhere else. He's just trying to make it, trying to survive here in Miami. KASTE: Veloso is serving a federal sentence and didn't want to talk to NPR, but Hassan says his friend is basically a decent guy who was offered easy money to sort of lend his bank account to people who needed some help moving their money. HASSAN: In his mind, when it first got presented to him, it sounded possibly legitimate because they don't want their loved one or they don't want this individual stealing this money. But then at some point you understand that it's fraudulent, and he understood it. KASTE: It's people like this who are most likely to get caught. Just in September, the FBI announced the arrest of 74 people here in the U. S. connected to business email compromise, alleged money mules and other enablers for overseas scammers. Meanwhile, this sort of scam is spreading, and it's not targeting just businesses anymore. Nick Selby is director of cyber intelligence and investigations for the New York Police Department. NICK SELBY: In New York, we have seen over the past year a notable increase in the number of individuals who are receiving these kinds of emails because they fall for it, too. KASTE: Selby says you have to keep in mind that these business scams are all about researching certain people at companies to figure out what might fool them. SELBY: And if you think about that, it doesn't take much to imagine how this could work on individuals. KASTE: So the question is, as this more sophisticated research-based cybercrime spreads, can American law enforcement keep up? When Mark in Seattle was conned out of his $50,000, he says talking to the FBI just left him feeling hopeless. MARK: They basically said, we're really sorry, but we're going after this same fraud but in the millions and millions and millions of dollars. And so, you know, it's not enough to go after. KASTE: The banks weren't much help, either. Since he was the one who gave the scammers the account number, they saw this as his responsibility. He has learned one thing - never again trust wiring instructions that are sent by email. He says people in his business now insist on voice calls before sending money. And some colleagues actually put account numbers down on paper to be delivered by hand. Martin Kaste, NPR News. AUDIE CORNISH, HOST:  By now, cybercrime is a routine danger. But as ordinary as it seems, it's still really bad for businesses. A company can lose thousands, even millions to a single deceptive email to an employee. And as NPR's Martin Kaste reports, business has never been better for the scammers. (SOUNDBITE OF PHONE RINGING) MARTIN KASTE, BYLINE: This latest wave of cybercrime against American businesses really got rolling about two years ago, and this is one of the earliest victims. It's a real estate company in Seattle. One of the owners here is named Mark. He'd rather we don't give his last name. MARK: We're somewhat experienced businesspeople. The idea that we've been duped makes you feel pretty stupid. KASTE: That reluctance to talk about this? More about that in a minute. But first, the scam. Mark had been wrapping up a project and emailing with an investment partner. What they didn't realize was someone had hacked into their email traffic. MARK: It was clear that they had studied our conversation. KASTE: Because the scammers knew just the right moment to insert themselves into that conversation. MARK: The cadence and the timing and the email was so normal that it wasn't suspicious at all. It was just like we were continuing to have a conversation, but I just wasn't having it with the person I thought I was. KASTE: That person had picked up on the fact that Mark was about to send his partner some money. So, pretending to be the partner, the scammers sent him wiring instructions to a different account at his usual bank. Mark didn't think twice. A little later, he texted his partner to see if he'd got the money. MARK: And there was an immediate reaction and response from him, you know, question mark, what wire? And, oh, it was a cold sweat. KASTE: The $50,000 he'd wired was gone, already rewired from the American bank to an account overseas. Mark was a victim of a growing category of cybercrime that's called business email compromise or BEC. But don't let that bland name fool you. PATRICK PETERSON: What we've seen in 2019 is that the wave that's breaking is primarily focused around social engineering. KASTE: Patrick Peterson is CEO of Agari, a company that specializes in protecting corporate email systems. And he sees a lot of these scams up close. When he says social engineering, what he means is hacks that are based not so much on breaking into software, but rather on fooling people. PETERSON: It's not so much having the most sophisticated evil technology, it's using our own trust and desire to communicate with others against us. KASTE: He says these schemes are usually run by international networks - you know, those Nigerian prince emails in the early days of the Internet. It's still similar groups, but now they're more focused on researching their victims. When they break into a company's email, they're patient. They just lurk there for a while. PETERSON: And then they can sit there and watch the email go back and forth. And they can see this person pays a lot of invoices or sends a lot of accounts payable. And at the right time, we'll send one that has our payable instructions. KASTE: And given the sums that businesses move around on a daily basis, the payoff can be enormous. JAMES ABBOTT: In 2016, we had business email compromise schemes at $361 million. KASTE: This is James Abbott. He's a supervisory special agent with the FBI, specializing in BEC fraud. ABBOTT: 2017, that number jumped to 676 million. In 2018, we're at nearly 1. 2 billion. But the thing to keep in mind with these statistics is this is just what we're aware of. KASTE: Millions of dollars in fraud goes unreported because embarrassed businesses prefer to keep their losses quiet. Investigators say this kind of secrecy helps the scammers because it keeps their tricks less visible. The FBI's Abbott says businesses are also too quick to assume that the culprits are all overseas and untouchable. ABBOTT: That is absolutely not the case. There are many times where the victim is sending their money to what we consider a money mule located right in their backyard or another part of the United States. KASTE: Money mules are people here in the country who set up bank accounts to receive the diverted funds. The foreign scammers need these American accounts because overseas bank accounts would raise suspicions. Nayib Hassan is the friend and lawyer of one of these money mules, a man named Alfredo Veloso. NAYIB HASSAN: Alfredo's just your run-of-the-mill individual that you see anywhere else. I mean, he's not going to be your Nigerian. He's not going to be your - from anywhere else. He's just trying to make it, trying to survive here in Miami. KASTE: Veloso is serving a federal sentence and didn't want to talk to NPR, but Hassan says his friend is basically a decent guy who was offered easy money to sort of lend his bank account to people who needed some help moving their money. HASSAN: In his mind, when it first got presented to him, it sounded possibly legitimate because they don't want their loved one or they don't want this individual stealing this money. But then at some point you understand that it's fraudulent, and he understood it. KASTE: It's people like this who are most likely to get caught. Just in September, the FBI announced the arrest of 74 people here in the U. S. connected to business email compromise, alleged money mules and other enablers for overseas scammers. Meanwhile, this sort of scam is spreading, and it's not targeting just businesses anymore. Nick Selby is director of cyber intelligence and investigations for the New York Police Department. NICK SELBY: In New York, we have seen over the past year a notable increase in the number of individuals who are receiving these kinds of emails because they fall for it, too. KASTE: Selby says you have to keep in mind that these business scams are all about researching certain people at companies to figure out what might fool them. SELBY: And if you think about that, it doesn't take much to imagine how this could work on individuals. KASTE: So the question is, as this more sophisticated research-based cybercrime spreads, can American law enforcement keep up? When Mark in Seattle was conned out of his $50,000, he says talking to the FBI just left him feeling hopeless. MARK: They basically said, we're really sorry, but we're going after this same fraud but in the millions and millions and millions of dollars. And so, you know, it's not enough to go after. KASTE: The banks weren't much help, either. Since he was the one who gave the scammers the account number, they saw this as his responsibility. He has learned one thing - never again trust wiring instructions that are sent by email. He says people in his business now insist on voice calls before sending money. And some colleagues actually put account numbers down on paper to be delivered by hand. Martin Kaste, NPR News.", "section": "National", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-18-780372971": {"title": "Unless Deadline Is Extended, U.S. Firms Must Stop Working With Huawei : NPR", "url": "https://www.npr.org/2019/11/18/780372971/unless-deadline-is-extended-u-s-firms-must-stop-working-with-huawei", "author": "No author found", "published_date": "2019-11-18", "content": "DAVID GREENE, HOST: Chinese tech giant Huawei is blacklisted in the U. S. for national security reasons. But the company has been able to keep doing business with American companies thanks to a temporary reprieve. That reprieve, though, expires today. Several media outlets are reporting that it's going to be extended. Let's sort this out with NPR's tech correspondent Shannon Bond, who is with us. Hey, Shannon. SHANNON BOND, BYLINE: Hey, David. GREENE: Take us back, if you can, just for the beginning here. How did Huawei get in the crosshairs of the U. S. government? BOND: Well, Huawei is one of the world's biggest makers of smartphones and telecoms equipment. And it relies on a lot of American companies to supply things like processing chips and, in the case of its phones, Google's Android software. But the administration says it's worried that Huawei and other big Chinese company could - companies could be spying for Beijing or stealing intellectual property from American companies. The Pentagon, for example, banned purchases of Huawei phones on military bases last year. Huawei maintains that the U. S. has given no evidence of spying. But as you said, back in May, the Trump administration put Huawei on this entity list, which means U. S. companies can't sell products to Huawei without government approval. GREENE: But I mean, is this really about suspicion of spying? Or - I mean, this is not happening in a vacuum. We've got a big trade war between these two countries happening. BOND: That's right. Huawei's being used for leverage. Back in June, when President Trump met with Chinese President Xi, they agreed to a cease-fire in the trade war while they continued negotiations. And at the same meeting, Trump said some U. S. companies would be able to do business with Huawei while those talks continued. . . GREENE: OK. BOND: . . . They're right in the middle of this. GREENE: So there was that reprieve. How has that played out for U. S. companies, who must be watching this wondering what the result's going to be? BOND: Right. I mean, so this - I - the reprieve was to help these companies do some work for a limited period of time to minimize disruption - so providing software updates to Huawei phones. And it was particularly aimed at some smaller, rural cellphone and internet providers in the U. S. that rely on Huawei for their networking equipment. Here is Commerce Secretary Wilbur Ross explaining this on Fox Business last week. (SOUNDBITE OF FOX BUSINESS NETWORK BROADCAST)WILBUR ROSS: There are enough problems with telephone service in the rural communities. We don't want to knock them out, so one of the main purposes of the temporary general licenses is to let those rural guys continue to operate. BOND: And as you said at the top, this reprieve has been extended twice already. It expires at the end of the day today. Several other publications are reporting that it'll be extended for a third time, though NPR hasn't confirmed this. GREENE: But I mean, this is a big deal for American companies - right? - if they get this reprieve extended. BOND: Well, it would be another temporary extension. And what companies say they really want is to understand how they'll be able to deal with Huawei over the long term. The Trump administration says it's also working towards allowing some businesses to be able to sell to Huawei if they're not in really sensitive national security areas. Huawei says there are 200 companies - chip-makers and others - that want to sell them things. And there are 40,000 jobs in the U. S. that could depend on those decisions. GREENE: How has Huawei responded to being kind of caught up in all this? BOND: Yeah. Well, Huawei, as you can imagine, denies all accusations about these national security issues or spying. They think, fundamentally, they've been pulled into a geopolitical fight, you know, with the trade war. And you know, they point to some of these examples, like what Trump has said. Huawei has made great strides in the meantime in finding alternatives to U. S. chips, even creating its own mobile operating system. It started selling phones in China that don't contain any chips made by U. S. companies. So this U. S. government fight with Huawei may actually be creating a stronger and more independent tech company in China. GREENE: Oh, interesting. NPR's tech correspondent Shannon Bond with us. Shannon, thanks. BOND: Thank you. DAVID GREENE, HOST:  Chinese tech giant Huawei is blacklisted in the U. S. for national security reasons. But the company has been able to keep doing business with American companies thanks to a temporary reprieve. That reprieve, though, expires today. Several media outlets are reporting that it's going to be extended. Let's sort this out with NPR's tech correspondent Shannon Bond, who is with us. Hey, Shannon. SHANNON BOND, BYLINE: Hey, David. GREENE: Take us back, if you can, just for the beginning here. How did Huawei get in the crosshairs of the U. S. government? BOND: Well, Huawei is one of the world's biggest makers of smartphones and telecoms equipment. And it relies on a lot of American companies to supply things like processing chips and, in the case of its phones, Google's Android software. But the administration says it's worried that Huawei and other big Chinese company could - companies could be spying for Beijing or stealing intellectual property from American companies. The Pentagon, for example, banned purchases of Huawei phones on military bases last year. Huawei maintains that the U. S. has given no evidence of spying. But as you said, back in May, the Trump administration put Huawei on this entity list, which means U. S. companies can't sell products to Huawei without government approval. GREENE: But I mean, is this really about suspicion of spying? Or - I mean, this is not happening in a vacuum. We've got a big trade war between these two countries happening. BOND: That's right. Huawei's being used for leverage. Back in June, when President Trump met with Chinese President Xi, they agreed to a cease-fire in the trade war while they continued negotiations. And at the same meeting, Trump said some U. S. companies would be able to do business with Huawei while those talks continued. . . GREENE: OK. BOND: . . . They're right in the middle of this. GREENE: So there was that reprieve. How has that played out for U. S. companies, who must be watching this wondering what the result's going to be? BOND: Right. I mean, so this - I - the reprieve was to help these companies do some work for a limited period of time to minimize disruption - so providing software updates to Huawei phones. And it was particularly aimed at some smaller, rural cellphone and internet providers in the U. S. that rely on Huawei for their networking equipment. Here is Commerce Secretary Wilbur Ross explaining this on Fox Business last week. (SOUNDBITE OF FOX BUSINESS NETWORK BROADCAST) WILBUR ROSS: There are enough problems with telephone service in the rural communities. We don't want to knock them out, so one of the main purposes of the temporary general licenses is to let those rural guys continue to operate. BOND: And as you said at the top, this reprieve has been extended twice already. It expires at the end of the day today. Several other publications are reporting that it'll be extended for a third time, though NPR hasn't confirmed this. GREENE: But I mean, this is a big deal for American companies - right? - if they get this reprieve extended. BOND: Well, it would be another temporary extension. And what companies say they really want is to understand how they'll be able to deal with Huawei over the long term. The Trump administration says it's also working towards allowing some businesses to be able to sell to Huawei if they're not in really sensitive national security areas. Huawei says there are 200 companies - chip-makers and others - that want to sell them things. And there are 40,000 jobs in the U. S. that could depend on those decisions. GREENE: How has Huawei responded to being kind of caught up in all this? BOND: Yeah. Well, Huawei, as you can imagine, denies all accusations about these national security issues or spying. They think, fundamentally, they've been pulled into a geopolitical fight, you know, with the trade war. And you know, they point to some of these examples, like what Trump has said. Huawei has made great strides in the meantime in finding alternatives to U. S. chips, even creating its own mobile operating system. It started selling phones in China that don't contain any chips made by U. S. companies. So this U. S. government fight with Huawei may actually be creating a stronger and more independent tech company in China. GREENE: Oh, interesting. NPR's tech correspondent Shannon Bond with us. Shannon, thanks. BOND: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-18-779386167": {"title": "The Language Of Cybercrime : NPR", "url": "https://www.npr.org/2019/11/18/779386167/the-language-of-cybercrime", "author": "No author found", "published_date": "2019-11-18", "content": "RACHEL MARTIN, HOST: Romance scams are the type of cybercrimes most people are sure they would never fall for. But the FBI says victims reported losing $211 million in 2017. And last year, losses jumped to over $350 million. As NPR's Martin Kaste reports, the scammers have turned manipulating people into a science. MARTIN KASTE, BYLINE: It's an old grift - older than the Internet. You make a long-distance love connection, but soon you're being asked for money. To understand why it still works, it helps to talk to a victim. TEXAS MOM: Hi, I'm a single, working mom in Texas. KASTE: And for our purposes here today, that's what we're going to call her - Texas Mom. Few people in her life know this story, and she wants to keep it that way. It started about three years ago, after her divorce, when she tried online dating. She found a match, a guy with a nice picture and an Austin area code. There was just one thing. TEXAS MOM: Before I ever spoke to him, he said, well, I have an accent. And I'm like, well, you know, that's OK. And then he told me, you know, the Italian story. KASTE: He said he'd grown up in Milan. And she could look past the accent, given the way he talked to her. TEXAS MOM: Just very complimentary, understanding and someone who had a real interest in me, which was new to me. KASTE: Now, it's important to point out here that Texas Mom knew very well that this could be fake. She was a little suspicious when a job took him out of the country right before they could meet. And her suspicion grew. And he contacted her from South Africa. He said he'd been hacked, couldn't get access to his money and had an emergency expense at work. Could she help him? TEXAS MOM: I started crying, and I said to him, I've heard stories about scammers. And he's like, oh, no, no, no. Forget it. Forget I even asked. And he really tried to make me feel better, but I was very upset. KASTE: And yet she didn't cut things off. She eventually sent him money - again and again. He had unexpected travel expenses. He was being fined by abusive government officials. It was one thing after another. TEXAS MOM: It's that gambling scenario. It's like, OK, now, if I give him a little bit more, I can get that back. That's what you keep telling yourself, and that's what he keeps telling you. JOHN WILSON: It's hard to comprehend. And yet, when she explains it, it starts to make sense as to how somebody could fall for this. KASTE: John Wilson is field chief technology officer with Agari, a cybersecurity company. And he's impressed by the amount of research that goes into these romance scams. WILSON: The folks that get roped into these things are very carefully selected. They know, demographically, the people that are going to be most susceptible. They have these playbooks that we've seen. KASTE: The playbooks are scripts, conversational gambits for a variety of situations that might come up in conversations with victims, which scammers have refined over time. When a certain kind of sweet talk works, they make a note of it and share it in their network. Looking back, Texas Mom seems almost fascinated by what happened to her. TEXAS MOM: You know, the best way I could describe it is that you have two brains when you have this excitement or these feelings of love or passion - because you know it's wrong, and you've read stories about it, and people are telling you. You would tell your best friend, you're crazy, don't do it. But then, you do it. KASTE: And there's another piece to this manipulation. The victims start to lie to friends and family to cover what's happening. They start to participate in the deception. Texas Mom remembers how she went from bank to bank, borrowing money to send to South Africa. One of the bank's suspected she was being conned and made her talk to a fraud investigator. TEXAS MOM: The fraud investigator on the phone said I would strongly advise you not to do this. And I sat there and I said, OK, I won't do it. And they're like, OK. Wow, you're really smart. And then I went somewhere else and did it. KASTE: And when the victims start covering for the scammers like this, there's a danger that they'll take the next step and become accomplices. NOWELL AGENT: We have found several individuals who started off with a romance scam. KASTE: This is the FBI supervisory special agent in charge of the criminal cyber squad in Houston. His name is Nowell Agent (ph). AGENT: So Agent is my last name. KASTE: Which makes him special agent Agent - just to get that distracting detail out of the way. Anyway, Agent spends a lot of time tracking cybercrime networks. And that often means tracking money mules. Those are the people here in the U. S. who set up American bank accounts for foreign scammers. Basically, they're helping them to move money stolen from other Americans. Agent says, sometimes, the accounts are owned by victims of romance scams, who take a chance doing this to try to get back some of the money they lost. AGENT: They're going to get a percentage of the money that goes through their account. So even though they were suspicious, and even though they felt that there was something malicious going on, they were also making money. KASTE: Other times, the romance scam victims are blackmailed into being money mules. Here's John Wilson, again, with Agari. WILSON: Very often, the victim has perhaps sent compromising photographs, may have moved money once or twice or something. When they say they want to get out, that's when they may be reminded, hey, I have pictures of you. You moved this money through your bank account. You're part of this now. KASTE: But in the case of Texas Mom, this did not happen. She bucked the trend and never became a money mule. Instead, she got a warning. Late last year, Wilson's company, Agari, was investigating a gang of scammers based in South Africa. And they saw that the gang was talking to a woman in Texas. Wilson says they had to step in. WILSON: You know, we took a big risk. There was a very good chance that this woman was simply going to tell her, quote-unquote, \"boyfriend,\" hey, I got this really weird call today. There's this company that thinks you're so-and-so. And, you know, what do you make of that? KASTE: But Texas Mom believed them. She says these strangers - these cybersecurity guys provided the confirmation that she needed. TEXAS MOM: I had to know that they were a scammer. And this was finally the evidence that proved that to me. KASTE: And then, a rarity in these cases, some justice was done. Agari connected her with the Secret Service, which alerted the South African authorities, and they set up a sting. She sent her fake boyfriend one last loan, and the police there arrested a group of Nigerians as they picked up the money. Still, the damage was done. All told, she says she sent her fictional friend nearly half a million dollars. TEXAS MOM: The person really did break my heart because I believed everything that they told me. I had to change the way I thought about myself. KASTE: The money's probably not recoverable, and she's mired in debt. Uncomfortable as it is for her to talk about what happened, she decided she should because she figures other people are being conned right now with the same playbook. And they're probably also hiding the truth from themselves and others the same way she did. Martin Kaste, NPR News. SOUNDBITE OF DJ KRUSH'S \"DUST TRAIL\") RACHEL MARTIN, HOST:  Romance scams are the type of cybercrimes most people are sure they would never fall for. But the FBI says victims reported losing $211 million in 2017. And last year, losses jumped to over $350 million. As NPR's Martin Kaste reports, the scammers have turned manipulating people into a science. MARTIN KASTE, BYLINE: It's an old grift - older than the Internet. You make a long-distance love connection, but soon you're being asked for money. To understand why it still works, it helps to talk to a victim. TEXAS MOM: Hi, I'm a single, working mom in Texas. KASTE: And for our purposes here today, that's what we're going to call her - Texas Mom. Few people in her life know this story, and she wants to keep it that way. It started about three years ago, after her divorce, when she tried online dating. She found a match, a guy with a nice picture and an Austin area code. There was just one thing. TEXAS MOM: Before I ever spoke to him, he said, well, I have an accent. And I'm like, well, you know, that's OK. And then he told me, you know, the Italian story. KASTE: He said he'd grown up in Milan. And she could look past the accent, given the way he talked to her. TEXAS MOM: Just very complimentary, understanding and someone who had a real interest in me, which was new to me. KASTE: Now, it's important to point out here that Texas Mom knew very well that this could be fake. She was a little suspicious when a job took him out of the country right before they could meet. And her suspicion grew. And he contacted her from South Africa. He said he'd been hacked, couldn't get access to his money and had an emergency expense at work. Could she help him? TEXAS MOM: I started crying, and I said to him, I've heard stories about scammers. And he's like, oh, no, no, no. Forget it. Forget I even asked. And he really tried to make me feel better, but I was very upset. KASTE: And yet she didn't cut things off. She eventually sent him money - again and again. He had unexpected travel expenses. He was being fined by abusive government officials. It was one thing after another. TEXAS MOM: It's that gambling scenario. It's like, OK, now, if I give him a little bit more, I can get that back. That's what you keep telling yourself, and that's what he keeps telling you. JOHN WILSON: It's hard to comprehend. And yet, when she explains it, it starts to make sense as to how somebody could fall for this. KASTE: John Wilson is field chief technology officer with Agari, a cybersecurity company. And he's impressed by the amount of research that goes into these romance scams. WILSON: The folks that get roped into these things are very carefully selected. They know, demographically, the people that are going to be most susceptible. They have these playbooks that we've seen. KASTE: The playbooks are scripts, conversational gambits for a variety of situations that might come up in conversations with victims, which scammers have refined over time. When a certain kind of sweet talk works, they make a note of it and share it in their network. Looking back, Texas Mom seems almost fascinated by what happened to her. TEXAS MOM: You know, the best way I could describe it is that you have two brains when you have this excitement or these feelings of love or passion - because you know it's wrong, and you've read stories about it, and people are telling you. You would tell your best friend, you're crazy, don't do it. But then, you do it. KASTE: And there's another piece to this manipulation. The victims start to lie to friends and family to cover what's happening. They start to participate in the deception. Texas Mom remembers how she went from bank to bank, borrowing money to send to South Africa. One of the bank's suspected she was being conned and made her talk to a fraud investigator. TEXAS MOM: The fraud investigator on the phone said I would strongly advise you not to do this. And I sat there and I said, OK, I won't do it. And they're like, OK. Wow, you're really smart. And then I went somewhere else and did it. KASTE: And when the victims start covering for the scammers like this, there's a danger that they'll take the next step and become accomplices. NOWELL AGENT: We have found several individuals who started off with a romance scam. KASTE: This is the FBI supervisory special agent in charge of the criminal cyber squad in Houston. His name is Nowell Agent (ph). AGENT: So Agent is my last name. KASTE: Which makes him special agent Agent - just to get that distracting detail out of the way. Anyway, Agent spends a lot of time tracking cybercrime networks. And that often means tracking money mules. Those are the people here in the U. S. who set up American bank accounts for foreign scammers. Basically, they're helping them to move money stolen from other Americans. Agent says, sometimes, the accounts are owned by victims of romance scams, who take a chance doing this to try to get back some of the money they lost. AGENT: They're going to get a percentage of the money that goes through their account. So even though they were suspicious, and even though they felt that there was something malicious going on, they were also making money. KASTE: Other times, the romance scam victims are blackmailed into being money mules. Here's John Wilson, again, with Agari. WILSON: Very often, the victim has perhaps sent compromising photographs, may have moved money once or twice or something. When they say they want to get out, that's when they may be reminded, hey, I have pictures of you. You moved this money through your bank account. You're part of this now. KASTE: But in the case of Texas Mom, this did not happen. She bucked the trend and never became a money mule. Instead, she got a warning. Late last year, Wilson's company, Agari, was investigating a gang of scammers based in South Africa. And they saw that the gang was talking to a woman in Texas. Wilson says they had to step in. WILSON: You know, we took a big risk. There was a very good chance that this woman was simply going to tell her, quote-unquote, \"boyfriend,\" hey, I got this really weird call today. There's this company that thinks you're so-and-so. And, you know, what do you make of that? KASTE: But Texas Mom believed them. She says these strangers - these cybersecurity guys provided the confirmation that she needed. TEXAS MOM: I had to know that they were a scammer. And this was finally the evidence that proved that to me. KASTE: And then, a rarity in these cases, some justice was done. Agari connected her with the Secret Service, which alerted the South African authorities, and they set up a sting. She sent her fake boyfriend one last loan, and the police there arrested a group of Nigerians as they picked up the money. Still, the damage was done. All told, she says she sent her fictional friend nearly half a million dollars. TEXAS MOM: The person really did break my heart because I believed everything that they told me. I had to change the way I thought about myself. KASTE: The money's probably not recoverable, and she's mired in debt. Uncomfortable as it is for her to talk about what happened, she decided she should because she figures other people are being conned right now with the same playbook. And they're probably also hiding the truth from themselves and others the same way she did. Martin Kaste, NPR News. SOUNDBITE OF DJ KRUSH'S \"DUST TRAIL\")", "section": "National", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-20-717487360": {"title": "VIDEO: Elon Musk's Next Quest Is A Mind-Machine Meld. Let's Consider The Implications : NPR", "url": "https://www.npr.org/2019/11/20/717487360/video-elon-musks-next-quest-is-a-mind-machine-meld-let-s-consider-the-implicatio", "author": "No author found", "published_date": "2019-11-20", "content": "DAVID GREENE, HOST: Elon Musk, the co-founder of Tesla, is known for these audacious business plans that grab a whole lot of attention. Right? He's been getting a lot of attention lately for unveiling a new electric pickup truck that's been dubbed the Cybertruck. Well, a few months ago, he was hawking another one of his companies called Neuralink. He said the company's goal is to stitch computer wires - he called them threads - straight into human brains. (SOUNDBITE OF ARCHIVED RECORDING)ELON MUSK: This, I think, has a very good purpose, which is to cure important diseases and ultimately to help secure humanity's future. GREENE: OK - high expectations there. Now, if you're wondering about curing diseases by connecting our brains to computers and whether this is possible, you would not be alone. But Musk's company is part of a fledgling industry that is trying to make this the wave of the future. And NPR's Elise Hu has been reporting on this in her series we call Future You. Hi, Elise. ELISE HU, BYLINE: Good morning. GREENE: All right. So this is the final report in your series, and you've been looking at ethical questions raised by businesses that are promoting this brain-computer interface. And we should say, Musk is not the only one who's spending a whole lot of money on this kind of thing. HU: No, he's not. Mark Zuckerberg from Facebook is another one. Then there's a guy named Bryan Johnson; he's a billionaire who founded the backbone to the payment system Venmo. And these tech moguls - and others - believe that if you can unlock the mysteries of our minds, then computers would be better able to read them and then better work inside them. GREENE: OK. So what case are all these people making for why this is a good investment? HU: I interviewed Bryan Johnson, whose company is working on this. It is called Kernel, and he's just as expansive as Elon Musk - or maybe even more so - in his vision of solving every problem known to humanity. BRYAN JOHNSON: If we are to address climate change, the risk of pandemics, of wars, terrorism, of mental illness, of everything we care to solve in the world, there's one thing they all have in common - our minds. My hope is that when we build tools with Kernel and that we can begin evolving our cognition - that if we do that, it's the fastest possible path to addressing every other problem we care about. HU: And Kernel is starting with a non-invasive cap - so a way to do this without stitching anything into our heads. But this is big talk. It envisions that everyone would be connected to computers at some point in the future and kind of rewiring ourselves to solve humanity's problems. GREENE: I mean, I love dreaming big - obviously. But I mean, this is a huge leap. I mean, we don't even know if these devices are going to work like they say they're going to work. And I would assume there'd be some regulatory stuff that would have to happen before we can actually be used. Right? HU: Right. And right now we're discussing a lot of the ethical questions here. And I've been looking at whether this kind of thing works at all over the course of this series. I've tried on a couple devices to connect with other machines; I moved two different robots just by thinking about it. So in this reporting, I have seen that science, and it's been making tremendous leaps forward. GREENE: But I also like that you've really boiled this down to one very important question, which is - should we even be trying to upgrade ourselves, upgrade people with computers? HU: And that's a key question. And I spoke with the tech ethicist Tristan Harris, who used to be at Google. He went on to found a thing called the Center for Humane Technology. And he told me about some of the things he worries about, beginning with the fact that, at this moment in history, you might say that many of us have been cognitively downgraded by our technology because we are so addicted to and distracted by our phones. And those aren't even connected to our bodies. TRISTAN HARRIS: What got us into the present situation where our attention spans are 40 seconds on any computer screen? What got us there wasn't - let's make our attention spans short. What got us there is - let's give ourselves superpowers. And we didn't know ourselves well enough that when we gave ourselves superpowers, we debased our way of making attention. HU: Tristan Harris says we need to assess where we are now before we introduce the kind of mind-computer connections that some of these entrepreneurs are talking about. HARRIS: If we don't even have a balance sheet of - these are the harms - and we're just eager to just race into the future and get to that future milestone, this is why - it's not about focusing on the present so much as it's about understanding carefully, like, where is this good and where is it in control and where is it happening with wisdom when we're making technology? HU: Just some of the considerations as this brain-machine interface industry becomes, by one estimate, a multibillion-dollar industry over the next few years. GREENE: All right. Elise's series Future You, with Elise Hu wraps up with an animated episode on neuro ethics and the future. You can catch it on npr. org and also on NPR's YouTube channel. Elise, I just want to say I'm going to miss this series. Thanks so much for all the great stuff. HU: Me, too. Thank you. DAVID GREENE, HOST:  Elon Musk, the co-founder of Tesla, is known for these audacious business plans that grab a whole lot of attention. Right? He's been getting a lot of attention lately for unveiling a new electric pickup truck that's been dubbed the Cybertruck. Well, a few months ago, he was hawking another one of his companies called Neuralink. He said the company's goal is to stitch computer wires - he called them threads - straight into human brains. (SOUNDBITE OF ARCHIVED RECORDING) ELON MUSK: This, I think, has a very good purpose, which is to cure important diseases and ultimately to help secure humanity's future. GREENE: OK - high expectations there. Now, if you're wondering about curing diseases by connecting our brains to computers and whether this is possible, you would not be alone. But Musk's company is part of a fledgling industry that is trying to make this the wave of the future. And NPR's Elise Hu has been reporting on this in her series we call Future You. Hi, Elise. ELISE HU, BYLINE: Good morning. GREENE: All right. So this is the final report in your series, and you've been looking at ethical questions raised by businesses that are promoting this brain-computer interface. And we should say, Musk is not the only one who's spending a whole lot of money on this kind of thing. HU: No, he's not. Mark Zuckerberg from Facebook is another one. Then there's a guy named Bryan Johnson; he's a billionaire who founded the backbone to the payment system Venmo. And these tech moguls - and others - believe that if you can unlock the mysteries of our minds, then computers would be better able to read them and then better work inside them. GREENE: OK. So what case are all these people making for why this is a good investment? HU: I interviewed Bryan Johnson, whose company is working on this. It is called Kernel, and he's just as expansive as Elon Musk - or maybe even more so - in his vision of solving every problem known to humanity. BRYAN JOHNSON: If we are to address climate change, the risk of pandemics, of wars, terrorism, of mental illness, of everything we care to solve in the world, there's one thing they all have in common - our minds. My hope is that when we build tools with Kernel and that we can begin evolving our cognition - that if we do that, it's the fastest possible path to addressing every other problem we care about. HU: And Kernel is starting with a non-invasive cap - so a way to do this without stitching anything into our heads. But this is big talk. It envisions that everyone would be connected to computers at some point in the future and kind of rewiring ourselves to solve humanity's problems. GREENE: I mean, I love dreaming big - obviously. But I mean, this is a huge leap. I mean, we don't even know if these devices are going to work like they say they're going to work. And I would assume there'd be some regulatory stuff that would have to happen before we can actually be used. Right? HU: Right. And right now we're discussing a lot of the ethical questions here. And I've been looking at whether this kind of thing works at all over the course of this series. I've tried on a couple devices to connect with other machines; I moved two different robots just by thinking about it. So in this reporting, I have seen that science, and it's been making tremendous leaps forward. GREENE: But I also like that you've really boiled this down to one very important question, which is - should we even be trying to upgrade ourselves, upgrade people with computers? HU: And that's a key question. And I spoke with the tech ethicist Tristan Harris, who used to be at Google. He went on to found a thing called the Center for Humane Technology. And he told me about some of the things he worries about, beginning with the fact that, at this moment in history, you might say that many of us have been cognitively downgraded by our technology because we are so addicted to and distracted by our phones. And those aren't even connected to our bodies. TRISTAN HARRIS: What got us into the present situation where our attention spans are 40 seconds on any computer screen? What got us there wasn't - let's make our attention spans short. What got us there is - let's give ourselves superpowers. And we didn't know ourselves well enough that when we gave ourselves superpowers, we debased our way of making attention. HU: Tristan Harris says we need to assess where we are now before we introduce the kind of mind-computer connections that some of these entrepreneurs are talking about. HARRIS: If we don't even have a balance sheet of - these are the harms - and we're just eager to just race into the future and get to that future milestone, this is why - it's not about focusing on the present so much as it's about understanding carefully, like, where is this good and where is it in control and where is it happening with wisdom when we're making technology? HU: Just some of the considerations as this brain-machine interface industry becomes, by one estimate, a multibillion-dollar industry over the next few years. GREENE: All right. Elise's series Future You, with Elise Hu wraps up with an animated episode on neuro ethics and the future. You can catch it on npr. org and also on NPR's YouTube channel. Elise, I just want to say I'm going to miss this series. Thanks so much for all the great stuff. HU: Me, too. Thank you.", "section": "Video: Future You, With Elise Hu", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-21-781633158": {"title": "Twitter Adds 'Hide Reply' Function To Try To Improve Online Conversation : NPR", "url": "https://www.npr.org/2019/11/21/781633158/twitter-adds-hide-reply-function-to-try-to-improve-online-conversation", "author": "No author found", "published_date": "2019-11-21", "content": "", "section": "Technology", "disclaimer": ""}, "2019-11-22-782052992": {"title": "Cybertruck: Elon Musk Unveils Tesla's Truck With A Polarizing Wedge Shape : NPR", "url": "https://www.npr.org/2019/11/22/782052992/elon-musk-unveils-teslas-cybertruck-with-a-polarizing-wedge-shape", "author": "No author found", "published_date": "2019-11-22", "content": "", "section": "Business", "disclaimer": ""}, "2019-11-24-779136094": {"title": "Using Virtual Reality To Drive Home Climate Change Impacts : NPR", "url": "https://www.npr.org/2019/11/24/779136094/climate-planners-turn-to-virtual-reality-and-hope-seeing-is-believing", "author": "No author found", "published_date": "2019-11-24", "content": "", "section": "Environment And Energy Collaborative", "disclaimer": ""}, "2019-11-25-782732974": {"title": "How Much Should Big Tech Know About Our Personal Health Data And History? : NPR", "url": "https://www.npr.org/2019/11/25/782732974/how-much-should-big-tech-know-about-our-personal-health-data-and-history", "author": "No author found", "published_date": "2019-11-25", "content": "MARY LOUISE KELLY, HOST: Big tech knows more and more these days about your health. Take Facebook, which has been rolling out a new health tab connecting you to doctors, reminding you to get your annual physical. Or take Google, which announced this month it is acquiring Fitbit. So how much control do we have? How much control should we have over how much of our personal health data ends up in the hands of tech giants? It's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")KELLY: I am joined by Ben Moskowitz. He's director of Consumer Reports' Digital Lab. Hi, Ben. BEN MOSKOWITZ: Hello. KELLY: So I mentioned Facebook. I mentioned Google. Give us a sense of the landscape. What other companies have access into this particular slice of my life these days? MOSKOWITZ: Five of the top six companies by market cap are tech companies. So we're talking about Microsoft, Google, Apple, Facebook, Amazon. And, you know, they need to continue growing. And so if you look across the marketplace, where is there great, tremendous potential for growth? It's health care. KELLY: And what is the impact of this? Let's start with the examples that I just listed. Google buying Fitbit, for example, which I should mention for those of us not tethered to our Fitbits, it knows how many steps we walk. It knows how many hours we sleep. It knows, for women, when we're getting our periods and so forth. What are the pros and cons of Google knowing these kinds of things about us? MOSKOWITZ: Well, from Google's perspective, you know, they want to create a product ecosystem that makes life easy for people just in terms of the functionality they get from these products. You know, in a certain light, the more data that can be collected about a person, the more valuable the insight about that person that can be sold to advertisers. KELLY: So that's the bottom line is they want to collect this data so that they can sell it. MOSKOWITZ: That's right. KELLY: Sum up for me - so what is the downside here? From my point of view, if all of my data is going into my Apple Watch or my now about to be Google Fitbit or whatever it is, what's the risk? MOSKOWITZ: You don't want people knowing everything about you the same way that when you go out in public, you wear clothes or you present in a certain way. Another harm is manipulation. So in the abstract here, you know, Google's a supercomputer. And if it's a supercomputer that knows all about who you are and how you think and what you want, imagine that supercomputer targeted at you and, you know, knowing that at a certain time of day, this is the kind of appeal that is going to land right. KELLY: What about - I'm thinking of this form that I have to sign every time I go to the doctor - the HIPAA privacy rule, which is supposed to regulate things like exactly this, I thought. How much of my personal medical data is being shared amongst companies and providers? Does that apply when I download some fitness tracker on my cellphone? MOSKOWITZ: No, it does not. And that's precisely the issue is that if you were to go to a hospital and create a health record, they would be governed under HIPAA and so they would have strict limitations on what they could do with that data. If you download a random app out of the App Store, the Google Play Store, then you wouldn't. And you could read - you know, they might say - they might claim to be HIPAA-compliant, but unless you read that privacy policy super closely, you know, you might not realize that they're sharing it for all kinds of purposes. KELLY: I'm also thinking about a lot of the apps I download, I pick the free version, but there's a version that I could pay for. Do I have more privacy protections if I pay for it? In other words, are we starting to think about medical privacy as this expensive add-on that I have to pay for? MOSKOWITZ: It depends, and it all goes down to what does the privacy policy say? But even, you know, if you read the privacy policy, it might be written in an overly broad way. It might be written in a way that says something very vague, like, you know, we may, in order to provide you with this service, share the information with X party. And beyond that, you just wouldn't know. KELLY: Have we reached a tipping point, though, do you think, where we have all become so reliant on technology and our gadgets that we're willing to trade away a lot of our privacy for the convenience that they provide, whether it's health care or anything else? MOSKOWITZ: What we know is that Americans really care about privacy. They feel like they don't have control over their personal information. In fact, just 9% of people in a nationally representative survey believed that they had a lot of control over the information. But the vast majority, though, think it's very important to be in control of who can get information - 74%. KELLY: Yeah, we care but we feel powerless. MOSKOWITZ: That's right. This is why I think it's a collective action problem. A lot of companies now are going out of their way to advertise and say, we provide you with granular privacy controls. And that's certainly a step in the right direction, but that's not going to solve the problem because not every company is going to be aboveboard in that way. And so it's going to require us to, you know, demand - well, first to define, then to demand the privacy laws that we think, you know, we deserve in this country. KELLY: I suppose the other collective action that could be taken is just swearing off your Fitbit or your Apple Watch or whatever it is and not inputting all your data, doing it the old-fashioned way. MOSKOWITZ: I don't think that the right answer is to say, let's just become Luddites and not use any of this stuff. You know, we need to demand rules so that we can get the benefits of the modern digital world and not have to sacrifice our privacy. KELLY: Ben Moskowitz, thank you. MOSKOWITZ: Thank you. KELLY: He is director of Consumer Reports' Digital Lab. And we should note Facebook and Google are among NPR's financial supporters. (SOUNDBITE OF GRACE JONES SONG, \"PRIVATE LIFE\") MARY LOUISE KELLY, HOST:  Big tech knows more and more these days about your health. Take Facebook, which has been rolling out a new health tab connecting you to doctors, reminding you to get your annual physical. Or take Google, which announced this month it is acquiring Fitbit. So how much control do we have? How much control should we have over how much of our personal health data ends up in the hands of tech giants? It's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") KELLY: I am joined by Ben Moskowitz. He's director of Consumer Reports' Digital Lab. Hi, Ben. BEN MOSKOWITZ: Hello. KELLY: So I mentioned Facebook. I mentioned Google. Give us a sense of the landscape. What other companies have access into this particular slice of my life these days? MOSKOWITZ: Five of the top six companies by market cap are tech companies. So we're talking about Microsoft, Google, Apple, Facebook, Amazon. And, you know, they need to continue growing. And so if you look across the marketplace, where is there great, tremendous potential for growth? It's health care. KELLY: And what is the impact of this? Let's start with the examples that I just listed. Google buying Fitbit, for example, which I should mention for those of us not tethered to our Fitbits, it knows how many steps we walk. It knows how many hours we sleep. It knows, for women, when we're getting our periods and so forth. What are the pros and cons of Google knowing these kinds of things about us? MOSKOWITZ: Well, from Google's perspective, you know, they want to create a product ecosystem that makes life easy for people just in terms of the functionality they get from these products. You know, in a certain light, the more data that can be collected about a person, the more valuable the insight about that person that can be sold to advertisers. KELLY: So that's the bottom line is they want to collect this data so that they can sell it. MOSKOWITZ: That's right. KELLY: Sum up for me - so what is the downside here? From my point of view, if all of my data is going into my Apple Watch or my now about to be Google Fitbit or whatever it is, what's the risk? MOSKOWITZ: You don't want people knowing everything about you the same way that when you go out in public, you wear clothes or you present in a certain way. Another harm is manipulation. So in the abstract here, you know, Google's a supercomputer. And if it's a supercomputer that knows all about who you are and how you think and what you want, imagine that supercomputer targeted at you and, you know, knowing that at a certain time of day, this is the kind of appeal that is going to land right. KELLY: What about - I'm thinking of this form that I have to sign every time I go to the doctor - the HIPAA privacy rule, which is supposed to regulate things like exactly this, I thought. How much of my personal medical data is being shared amongst companies and providers? Does that apply when I download some fitness tracker on my cellphone? MOSKOWITZ: No, it does not. And that's precisely the issue is that if you were to go to a hospital and create a health record, they would be governed under HIPAA and so they would have strict limitations on what they could do with that data. If you download a random app out of the App Store, the Google Play Store, then you wouldn't. And you could read - you know, they might say - they might claim to be HIPAA-compliant, but unless you read that privacy policy super closely, you know, you might not realize that they're sharing it for all kinds of purposes. KELLY: I'm also thinking about a lot of the apps I download, I pick the free version, but there's a version that I could pay for. Do I have more privacy protections if I pay for it? In other words, are we starting to think about medical privacy as this expensive add-on that I have to pay for? MOSKOWITZ: It depends, and it all goes down to what does the privacy policy say? But even, you know, if you read the privacy policy, it might be written in an overly broad way. It might be written in a way that says something very vague, like, you know, we may, in order to provide you with this service, share the information with X party. And beyond that, you just wouldn't know. KELLY: Have we reached a tipping point, though, do you think, where we have all become so reliant on technology and our gadgets that we're willing to trade away a lot of our privacy for the convenience that they provide, whether it's health care or anything else? MOSKOWITZ: What we know is that Americans really care about privacy. They feel like they don't have control over their personal information. In fact, just 9% of people in a nationally representative survey believed that they had a lot of control over the information. But the vast majority, though, think it's very important to be in control of who can get information - 74%. KELLY: Yeah, we care but we feel powerless. MOSKOWITZ: That's right. This is why I think it's a collective action problem. A lot of companies now are going out of their way to advertise and say, we provide you with granular privacy controls. And that's certainly a step in the right direction, but that's not going to solve the problem because not every company is going to be aboveboard in that way. And so it's going to require us to, you know, demand - well, first to define, then to demand the privacy laws that we think, you know, we deserve in this country. KELLY: I suppose the other collective action that could be taken is just swearing off your Fitbit or your Apple Watch or whatever it is and not inputting all your data, doing it the old-fashioned way. MOSKOWITZ: I don't think that the right answer is to say, let's just become Luddites and not use any of this stuff. You know, we need to demand rules so that we can get the benefits of the modern digital world and not have to sacrifice our privacy. KELLY: Ben Moskowitz, thank you. MOSKOWITZ: Thank you. KELLY: He is director of Consumer Reports' Digital Lab. And we should note Facebook and Google are among NPR's financial supporters. (SOUNDBITE OF GRACE JONES SONG, \"PRIVATE LIFE\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-25-782732967": {"title": "Uber Stripped Of Its License To Operate In London : NPR", "url": "https://www.npr.org/2019/11/25/782732967/uber-stripped-of-its-license-to-operate-in-london", "author": "No author found", "published_date": "2019-11-25", "content": "AILSA CHANG, HOST: Uber has been stripped of its license to operate in one of its most important cities, London. The city's transportation agency says Uber put passengers at risk through a pattern of failures. The decision is another big blow for the company in what has already been a difficult year. NPR's tech correspondent Shannon Bond reports. SHANNON BOND, BYLINE: London is one of Uber's biggest markets. It's one of five cities around the world where the ride-hailing company takes in a quarter of its fares from customers. DANIEL IVES: London's the heart and lungs of its European operations, biggest city in Europe. BOND: Daniel Ives is an analyst at Wedbush Securities. By his estimate, the British capital accounts for 3- to 5% of Uber's total ride-hailing sales. But that's now at risk of vanishing. Transport for London, the city's transit agency, says it won't renew the license Uber needs to run its car service there. The British agency says Uber doesn't meet its standard of being a, quote, \"fit and proper\" company. The agency says unauthorized drivers manipulated Uber systems to upload their own photos to other drivers' accounts. That resulted in 14,000 uninsured trips where passengers had no idea their driver had not been vetted by Uber. In at least one case, a driver whose license had been revoked was still able to drive for Uber. Uber says it has fixed the flaw that allowed this to happen. It says it's introducing facial matching in London to confirm drivers' identities. But Uber has had safety issues for years as it has raced to grow quickly. Ives, the analyst, says the British agency's action against Uber reflects a big problem. IVES: Safety is the lifeblood of Uber. If consumers don't feel safe within the platform, there's much broader issues. BOND: For Uber, what's at risk is not just the money it makes in London. It's the precedent that could be followed by other big cities. Bradley Tusk is a former adviser to Uber who helped the company fight regulations in New York in its early days. He still owns Uber shares. BRADLEY TUSK: For a company that's already struggling financially, this is yet another difficult blow. But beyond that, London is one of the most widely seen cities in the world, and what happens in London is noticed everywhere. BOND: The London license denial is just the latest black eye for Uber this year. For example, the company has had to limit how many drivers it has in New York after losing a legal battle with the city. Uber is already losing billions of dollars a year, and its stock price has fallen sharply from when the company started trading publicly in May. Ives, the analyst, says it's tough to be an Uber shareholder. IVES: Since the IPO, it's really been a horror movie. And I think this is something that investors are starting to get more and more frustrated with the company, and this latest London issue is another overhang now over the Uber stock. BOND: Uber says it will appeal the decision. In the meantime, its 45,000 London drivers will still be picking up passengers while the company fights to stay in the city. Shannon Bond, NPR News. AILSA CHANG, HOST:  Uber has been stripped of its license to operate in one of its most important cities, London. The city's transportation agency says Uber put passengers at risk through a pattern of failures. The decision is another big blow for the company in what has already been a difficult year. NPR's tech correspondent Shannon Bond reports. SHANNON BOND, BYLINE: London is one of Uber's biggest markets. It's one of five cities around the world where the ride-hailing company takes in a quarter of its fares from customers. DANIEL IVES: London's the heart and lungs of its European operations, biggest city in Europe. BOND: Daniel Ives is an analyst at Wedbush Securities. By his estimate, the British capital accounts for 3- to 5% of Uber's total ride-hailing sales. But that's now at risk of vanishing. Transport for London, the city's transit agency, says it won't renew the license Uber needs to run its car service there. The British agency says Uber doesn't meet its standard of being a, quote, \"fit and proper\" company. The agency says unauthorized drivers manipulated Uber systems to upload their own photos to other drivers' accounts. That resulted in 14,000 uninsured trips where passengers had no idea their driver had not been vetted by Uber. In at least one case, a driver whose license had been revoked was still able to drive for Uber. Uber says it has fixed the flaw that allowed this to happen. It says it's introducing facial matching in London to confirm drivers' identities. But Uber has had safety issues for years as it has raced to grow quickly. Ives, the analyst, says the British agency's action against Uber reflects a big problem. IVES: Safety is the lifeblood of Uber. If consumers don't feel safe within the platform, there's much broader issues. BOND: For Uber, what's at risk is not just the money it makes in London. It's the precedent that could be followed by other big cities. Bradley Tusk is a former adviser to Uber who helped the company fight regulations in New York in its early days. He still owns Uber shares. BRADLEY TUSK: For a company that's already struggling financially, this is yet another difficult blow. But beyond that, London is one of the most widely seen cities in the world, and what happens in London is noticed everywhere. BOND: The London license denial is just the latest black eye for Uber this year. For example, the company has had to limit how many drivers it has in New York after losing a legal battle with the city. Uber is already losing billions of dollars a year, and its stock price has fallen sharply from when the company started trading publicly in May. Ives, the analyst, says it's tough to be an Uber shareholder. IVES: Since the IPO, it's really been a horror movie. And I think this is something that investors are starting to get more and more frustrated with the company, and this latest London issue is another overhang now over the Uber stock. BOND: Uber says it will appeal the decision. In the meantime, its 45,000 London drivers will still be picking up passengers while the company fights to stay in the city. Shannon Bond, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-25-782712176": {"title": "Islamic State 'Not Present On The Internet Anymore' Following European Operation : NPR", "url": "https://www.npr.org/2019/11/25/782712176/islamic-state-not-present-on-the-internet-anymore-following-european-operation", "author": "No author found", "published_date": "2019-11-25", "content": "", "section": "World", "disclaimer": ""}, "2019-11-25-782556399": {"title": "Tiny Tech Tips: Which Of The New, 'Hi-Fi' Smart Speakers Sounds The Best? : NPR", "url": "https://www.npr.org/2019/11/25/782556399/tiny-tech-tips-which-of-the-new-hi-fi-smart-speakers-sounds-the-best", "author": "No author found", "published_date": "2019-11-25", "content": "", "section": "Tiny Tech Tips", "disclaimer": ""}, "2019-11-25-778546287": {"title": "Instacart Workers Feel Squeezed By Constantly Changing App : NPR", "url": "https://www.npr.org/2019/11/25/778546287/at-the-mercy-of-an-app-workers-feel-the-instacart-squeeze", "author": "No author found", "published_date": "2019-11-25", "content": "AILSA CHANG, HOST: Instacart, the grocery delivery app, has a lot of unhappy workers. Their concerns about pay are an eerie parallel to complaints from Uber drivers and others making living in the app-driven economy. NPR's Alina Selyukh reports. ALINA SELYUKH, BYLINE: At first, Instacart was fun. Michaellita Fortier says it felt like her dream of being on a speed-shopping TV show. And she could make 16- or $20 per delivery in West Michigan. MICHAELLITA FORTIER: My mindset was, I'm helping a family, or, I'm helping a customer 'cause they can't go out. SELYUKH: But then she kept getting offered 7- or $9 per order, meaning go to the store, shop for someone's groceries and drive maybe 10 or 15 miles. FORTIER: Like, I'm looking at a order right now. It was $8. 19 with zero tip. Once I kept seeing those come through, I thought, now, listen; this is less than minimum wage you want me to drive. SELYUKH: Fortier quit last month, but millions of Americans like her still count on apps - Instacart, DoorDash, Uber, Lyft - not just as a service, but a job. These workers find themselves at the mercy of an algorithm, no assurance of a minimum wage, rules and pay that constantly change, the smallest tweak of the app capable of upending their livelihood. Here's Liz Temkin, who's delivered groceries for Instacart in Los Angeles for over four years. LIZ TEMKIN: It was extremely clear before how much you would be paid per item, per order. And then they came up with a completely different what we call block-box algorithm. SELYUKH: Instacart has recently faced a strike over pay. The app relies on more than 130,000 contractors, most of them women. NPR connected with more than 40 of them, and their grievances are extensive. The strike was over changes to tipping, a hefty chunk of workers' income. Instacart used to set a default tip of 10% per order. Now it's half that, and it's on top of two other fees that go straight to Instacart. These days, Instacart has new minimum pay - 7- to $10 per gig. Sometimes a gig means two or three orders and multiple deliveries - several payments to Instacart, one to the worker - though Instacart says its pay does account for the total, size, distance and other factors. If all this sounds overwhelming, a lot of workers feel that way. MERCEDES MALTESE: We used to get $10 to go to the store and 40 cents an item. Now, no matter how many items and how many units we get, they pay us $7. Maybe there's a tip; maybe there's not. SELYUKH: Mercedes Maltese says she took up Instacart in a Detroit suburb after losing a full-time and a part-time job in succession. She says her options are other minimum wage jobs. And with Instacart, she can at least try to work as many hours as possible, though much of her shift is often just sitting in her car, unpaid, waiting for a decent order. MALTESE: And I work 15 hours a day, seven days a week. I just have to work and live in my car like a homeless person in some kind of hopes of scraping up some kind of money in order to pay my bills. SELYUKH: Right after the recent strike, Instacart made another change. It stopped paying a quality bonus, $3 for each five-star review from a customer. The strikers claim retaliation. Instacart says not at all. The $3 bonus was a test, and it did not meaningfully improve the quality of work. These tests and sudden changes are the reality of modern gig work. In August, a study that surveyed almost a thousand food delivery workers found Instacart stood out among the apps for just how much it pushed people to commit long hours and accept lower-paying gigs. The researchers called it algorithmic despotism. Here's lead author Kathleen Griesbach of Columbia University. KATHLEEN GRIESBACH: Instacart, we found in particular, really did control the labor process or control workers' experiences and limited their autonomy over their time and over the work they could do in a significant way. SELYUKH: The study received some union funding and surveyed workers at apps like Instacart, DoorDash and GrubHub. Instacart dismissed the research. It argued the study used an outdated version of the app, which has since added a more flexible option for workers. Instead of having to sign up for two-hour shifts, now workers can also grab assignments whenever as orders come in in real time. Most of the 40-plus workers I contacted said they worked both the old way and the new way. And either way, being a faster, nicer, more experienced worker did not guarantee better pay. Their bottom line - they are the face of the company but feel disposable. Alina Selyukh, NPR News. AILSA CHANG, HOST:  Instacart, the grocery delivery app, has a lot of unhappy workers. Their concerns about pay are an eerie parallel to complaints from Uber drivers and others making living in the app-driven economy. NPR's Alina Selyukh reports. ALINA SELYUKH, BYLINE: At first, Instacart was fun. Michaellita Fortier says it felt like her dream of being on a speed-shopping TV show. And she could make 16- or $20 per delivery in West Michigan. MICHAELLITA FORTIER: My mindset was, I'm helping a family, or, I'm helping a customer 'cause they can't go out. SELYUKH: But then she kept getting offered 7- or $9 per order, meaning go to the store, shop for someone's groceries and drive maybe 10 or 15 miles. FORTIER: Like, I'm looking at a order right now. It was $8. 19 with zero tip. Once I kept seeing those come through, I thought, now, listen; this is less than minimum wage you want me to drive. SELYUKH: Fortier quit last month, but millions of Americans like her still count on apps - Instacart, DoorDash, Uber, Lyft - not just as a service, but a job. These workers find themselves at the mercy of an algorithm, no assurance of a minimum wage, rules and pay that constantly change, the smallest tweak of the app capable of upending their livelihood. Here's Liz Temkin, who's delivered groceries for Instacart in Los Angeles for over four years. LIZ TEMKIN: It was extremely clear before how much you would be paid per item, per order. And then they came up with a completely different what we call block-box algorithm. SELYUKH: Instacart has recently faced a strike over pay. The app relies on more than 130,000 contractors, most of them women. NPR connected with more than 40 of them, and their grievances are extensive. The strike was over changes to tipping, a hefty chunk of workers' income. Instacart used to set a default tip of 10% per order. Now it's half that, and it's on top of two other fees that go straight to Instacart. These days, Instacart has new minimum pay - 7- to $10 per gig. Sometimes a gig means two or three orders and multiple deliveries - several payments to Instacart, one to the worker - though Instacart says its pay does account for the total, size, distance and other factors. If all this sounds overwhelming, a lot of workers feel that way. MERCEDES MALTESE: We used to get $10 to go to the store and 40 cents an item. Now, no matter how many items and how many units we get, they pay us $7. Maybe there's a tip; maybe there's not. SELYUKH: Mercedes Maltese says she took up Instacart in a Detroit suburb after losing a full-time and a part-time job in succession. She says her options are other minimum wage jobs. And with Instacart, she can at least try to work as many hours as possible, though much of her shift is often just sitting in her car, unpaid, waiting for a decent order. MALTESE: And I work 15 hours a day, seven days a week. I just have to work and live in my car like a homeless person in some kind of hopes of scraping up some kind of money in order to pay my bills. SELYUKH: Right after the recent strike, Instacart made another change. It stopped paying a quality bonus, $3 for each five-star review from a customer. The strikers claim retaliation. Instacart says not at all. The $3 bonus was a test, and it did not meaningfully improve the quality of work. These tests and sudden changes are the reality of modern gig work. In August, a study that surveyed almost a thousand food delivery workers found Instacart stood out among the apps for just how much it pushed people to commit long hours and accept lower-paying gigs. The researchers called it algorithmic despotism. Here's lead author Kathleen Griesbach of Columbia University. KATHLEEN GRIESBACH: Instacart, we found in particular, really did control the labor process or control workers' experiences and limited their autonomy over their time and over the work they could do in a significant way. SELYUKH: The study received some union funding and surveyed workers at apps like Instacart, DoorDash and GrubHub. Instacart dismissed the research. It argued the study used an outdated version of the app, which has since added a more flexible option for workers. Instead of having to sign up for two-hour shifts, now workers can also grab assignments whenever as orders come in in real time. Most of the 40-plus workers I contacted said they worked both the old way and the new way. And either way, being a faster, nicer, more experienced worker did not guarantee better pay. Their bottom line - they are the face of the company but feel disposable. Alina Selyukh, NPR News.", "section": "Profiles Of America In Full Employment", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-27-783385093": {"title": "Twitter Will Allow Users To 'Memorialize' Accounts Of The Deceased : NPR", "url": "https://www.npr.org/2019/11/27/783385093/following-backlash-twitter-offers-to-memorialize-accounts-of-the-deceased", "author": "No author found", "published_date": "2019-11-27", "content": "", "section": "Technology", "disclaimer": ""}, "2019-11-27-783036724": {"title": "Touch-Screen Self-Serve Taps Take Off As Costs Rise For Bars : NPR", "url": "https://www.npr.org/2019/11/27/783036724/99-bottles-of-beer-on-the-touch-screen-the-spread-of-self-serve-taps", "author": "No author found", "published_date": "2019-11-27", "content": "RACHEL MARTIN, HOST: Labor does not come cheap in cities with astronomical rents, even in the service industry. So some restaurant and bar owners are cutting costs with technology, like devices that allow patrons to serve their own drinks. NPR's Neda Ulaby reports. NEDA ULABY, BYLINE: Walters Sports Bar in Washington, D. C. , is a gleaming new pub just blocks from the city's largest stadium. It's industrial chic and spacious, with room for hundreds of customers. Tonight, one of them is approaching a stainless steel wall lined with beer taps and slots for cards where you pay. CHRIS PORCARO: Honestly - very easy. ULABY: That's Chris Porcaro describing what it's like to pour himself a yeasty IPA. PORCARO: This one was a bit foamy. So I have to say I was a little surprised by how foamy it was. ULABY: You can pour yourself 23 different kinds of beer at this bar - 24 if you count Bud Light. That joke's from owner Jeremy Gifford. He says the technology will cut you off after a few drinks. For more, you must get approved by the staff. JEREMY GIFFORD: We make sure that you still got your pants on and everybody's still doing OK. ULABY: Laws that regulate pour-your-own alcohol vary from state to state. This self-serve beer wall cost Gifford $100,000, but he says it'll pay for itself because it means employing fewer bartenders. GIFFORD: Labor is one of the largest costs in running a restaurant. ULABY: Self-serve taps are exploding in popularity thanks to increasing staffing expenses, such as mandatory minimum wage, says Josh Goodman. He runs a company called PourMyBeer. JOSH GOODMAN: If you have 50 self-serve taps, then you essentially have 50 employees that you don't have to pay to service your customers. ULABY: Goodman's company sells self-serve tap systems. It sold fewer than 200 taps in 2015, but in the last four years, he says it's sold more than 5,000 to establishments across the United States. GOODMAN: The big initiative now is to get them all communicating to one centralized mothership database so then we're gathering all this really unique information about products being dispensed. ULABY: Big data meet your beer and your wine and your cocktails - self-serve wine bars are popping up around the country and automated self-serve cocktail taps for exactly the same drink every time. Josh Goodman says his company's working with Whole Foods, the U. S. military, even companies that do not serve alcohol at all. GOODMAN: Dunkin' Donuts and Starbucks and cold-brew coffee and kombucha. I guess anywhere you see a line, we see an opportunity. ULABY: At a similar company called Table Tap, business has shot up sixfold since 2016 according to founder Jeff Libby. He points to all the pour-your-own taps popping up at co-working spaces, apartment lobbies and. . . JEFF LIBBY: Senior living communities love self-serve beer. We put one in a senior living community and they are crushing it, and they're actually having to cut people off (laughter). ULABY: Libby admits that cutting out servers in the service industry might feel a little harsh. LIBBY: I am sure that there are many bartenders that don't like self-serve beer. I know that for a fact. ULABY: One is here at Walters Sports Bar. But this former bartender, John Murphy, used a self-serve tap to pour a frosty beer. JOHN MURPHY: I have mixed feelings about it. I'm not going to lie, but I have mixed feelings about it. But I do enjoy being able to just walk over and quench my thirst. And I'm always thirsty (laughter). ULABY: Cheers, Murphy says, to the machines. Neda Ulaby, NPR News. (SOUNDBITE OF MUSIC) RACHEL MARTIN, HOST:  Labor does not come cheap in cities with astronomical rents, even in the service industry. So some restaurant and bar owners are cutting costs with technology, like devices that allow patrons to serve their own drinks. NPR's Neda Ulaby reports. NEDA ULABY, BYLINE: Walters Sports Bar in Washington, D. C. , is a gleaming new pub just blocks from the city's largest stadium. It's industrial chic and spacious, with room for hundreds of customers. Tonight, one of them is approaching a stainless steel wall lined with beer taps and slots for cards where you pay. CHRIS PORCARO: Honestly - very easy. ULABY: That's Chris Porcaro describing what it's like to pour himself a yeasty IPA. PORCARO: This one was a bit foamy. So I have to say I was a little surprised by how foamy it was. ULABY: You can pour yourself 23 different kinds of beer at this bar - 24 if you count Bud Light. That joke's from owner Jeremy Gifford. He says the technology will cut you off after a few drinks. For more, you must get approved by the staff. JEREMY GIFFORD: We make sure that you still got your pants on and everybody's still doing OK. ULABY: Laws that regulate pour-your-own alcohol vary from state to state. This self-serve beer wall cost Gifford $100,000, but he says it'll pay for itself because it means employing fewer bartenders. GIFFORD: Labor is one of the largest costs in running a restaurant. ULABY: Self-serve taps are exploding in popularity thanks to increasing staffing expenses, such as mandatory minimum wage, says Josh Goodman. He runs a company called PourMyBeer. JOSH GOODMAN: If you have 50 self-serve taps, then you essentially have 50 employees that you don't have to pay to service your customers. ULABY: Goodman's company sells self-serve tap systems. It sold fewer than 200 taps in 2015, but in the last four years, he says it's sold more than 5,000 to establishments across the United States. GOODMAN: The big initiative now is to get them all communicating to one centralized mothership database so then we're gathering all this really unique information about products being dispensed. ULABY: Big data meet your beer and your wine and your cocktails - self-serve wine bars are popping up around the country and automated self-serve cocktail taps for exactly the same drink every time. Josh Goodman says his company's working with Whole Foods, the U. S. military, even companies that do not serve alcohol at all. GOODMAN: Dunkin' Donuts and Starbucks and cold-brew coffee and kombucha. I guess anywhere you see a line, we see an opportunity. ULABY: At a similar company called Table Tap, business has shot up sixfold since 2016 according to founder Jeff Libby. He points to all the pour-your-own taps popping up at co-working spaces, apartment lobbies and. . . JEFF LIBBY: Senior living communities love self-serve beer. We put one in a senior living community and they are crushing it, and they're actually having to cut people off (laughter). ULABY: Libby admits that cutting out servers in the service industry might feel a little harsh. LIBBY: I am sure that there are many bartenders that don't like self-serve beer. I know that for a fact. ULABY: One is here at Walters Sports Bar. But this former bartender, John Murphy, used a self-serve tap to pour a frosty beer. JOHN MURPHY: I have mixed feelings about it. I'm not going to lie, but I have mixed feelings about it. But I do enjoy being able to just walk over and quench my thirst. And I'm always thirsty (laughter). ULABY: Cheers, Murphy says, to the machines. Neda Ulaby, NPR News. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-11-30-783819900": {"title": "Is Crimea Russian? U.S. And EU Say No, Apple Says Kremlin Can See What It Wants To : NPR", "url": "https://www.npr.org/2019/11/30/783819900/is-crimea-russian-u-s-and-eu-say-no-apple-says-kremlin-can-see-what-it-wants-to", "author": "No author found", "published_date": "2019-11-30", "content": "SCOTT SIMON, HOST: European Union and United States refuse to recognize Russia's 2014 seizure of Crimea from Ukraine. But Apple seems to oblige Russia in this. When the company's apps are used inside of Russia, they now show Crimea as part of that country, not Ukraine. Our situation with Apple has now been resolved, Vasily Piskaryov (ph) of Russia's Duma said this week. We see that everything has happened the way we wanted it. News accounts have noted that Apple also already accommodates the Chinese government by removing apps of foreign news outlets. Apple's chief, Tim Cook, has said each country in the world decides their laws and their regulations. And so your choice is, do you participate, or do you stand on the sideline and yell at how things should be? Yesterday, a company spokesperson said, we are taking a deeper look at how we handle disputed borders in our services and may make changes in the future as a result. Does that include an app for free speech? (SOUNDBITE OF MUSIC) SCOTT SIMON, HOST:  European Union and United States refuse to recognize Russia's 2014 seizure of Crimea from Ukraine. But Apple seems to oblige Russia in this. When the company's apps are used inside of Russia, they now show Crimea as part of that country, not Ukraine. Our situation with Apple has now been resolved, Vasily Piskaryov (ph) of Russia's Duma said this week. We see that everything has happened the way we wanted it. News accounts have noted that Apple also already accommodates the Chinese government by removing apps of foreign news outlets. Apple's chief, Tim Cook, has said each country in the world decides their laws and their regulations. And so your choice is, do you participate, or do you stand on the sideline and yell at how things should be? Yesterday, a company spokesperson said, we are taking a deeper look at how we handle disputed borders in our services and may make changes in the future as a result. Does that include an app for free speech? (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-01-784005860": {"title": "Troll Watch: 'Juice Jacking' : NPR", "url": "https://www.npr.org/2019/12/01/784005860/troll-watch-juice-jacking", "author": "No author found", "published_date": "2019-12-01", "content": "DON GONYEA, HOST: The holiday season comes with lots of airport travel and lots of time waiting at airports. So, naturally, worn-out travelers look for distractions while they wait out that massive layover. You might want to plug in your favorite show on the iPad. But, unfortunately, the battery's dead, and you need to charge. If you're lucky, there are some USB ports around. But hold on, you might not be as lucky as you think. Some hackers could be using those free public USB ports to steal your data in a process experts are calling juice jacking. Perfect topic for our recurring Troll Watch segment. (SOUNDBITE OF MUSIC)GONYEA: Joining us now to talk about this trend is Luke Sisak. He's a deputy district attorney for Los Angeles County and a cybercrimes prosecutor. Luke, thanks for joining us. LUKE SISAK: Thanks for having me, Don. GONYEA: What exactly is juice jacking? How does it work? SISAK: Juice jacking is a way for a criminal to get your personal information out of your phone through your power port. It can work a variety of different ways. But the end result is that by plugging into a USB socket somewhere, a criminal has either downloaded your information or actually uploaded malware into your phone that will then send it to him or her wherever they are. And it isn't always the same way. But it's the same result, which is they have your information, and you've been compromised. GONYEA: So give us some examples of how they might actually get access to the information on my phone or my iPad or whatever. I'm sitting there, and there's the USB port just asking me to use it. SISAK: Well, the simplest way to think about it is when you plug your, let's say, iPhone into your computer, you'll get the little warning or little question box. It says, do you want to trust this computer? And if you press yes, you can see your photos and other things on your computer screen that are actually on your phone. They can build in a version of that that allows them to do the same thing. They don't really care about the photos. What they want are credit card numbers that are saved, Apple Pay accounts, even things like your address or your Social Security number. If they've been entered into a webpage or some other app, they potentially have the chance to get those. GONYEA: Is it that the actual USB port has been compromised in some way? Or is it a question of them, say, plugging in a handy USB cable there that you just think is there for you to use? SISAK: No, it's not the cable. It's the port itself. And, really, what it is is what's behind the port. For all intents and purposes, it's a little computer. It's got a little data socket - a USB socket. And it's got software installed in it, and there's a variety of ways to install that software, a variety of things it can do to your phone. GONYEA: Is there a visual thing we should notice as we look at a port to see if it has, perhaps, been compromised? Or should we just stay away from them, completely? SISAK: It's like anything else. The less reputable it looks, that's usually a good sign that it's a problem. What we tell everybody, though, is that, again, it's not common. But because it is possible, if you can avoid using just a USB socket for power and find an actual wall outlet, you're far better off. GONYEA: So the wall outlet is OK, probably OK. SISAK: So the wall outlet is going to be OK for sure because the wall outlet only transmits power. GONYEA: Do airports regularly check these ports to see if they've been compromised? SISAK: I don't know. And, obviously, within airports, each airport is different. But I don't know that airport staff would necessarily even know what to look for. And the toughest part about this issue is that people don't know they've been victimized. So it's part of the reason why it's hard for us to catch scammers or hackers in the act. And it's part of the reason why it's hard for airports or anywhere else to know because no one's complaining. Oh, a guy out there just took my purse. That's easy. But people may be in Omaha, Neb. , or Alaska or wherever by the time they realize they got their information taken. And they'll have no idea where it happened. GONYEA: So let's close by having you offer just your quick advice to people. SISAK: What I do is I bring a wall charger with me. That covers you most of the time. In the event that there is no wall charger, I usually - especially when I travel - will have a battery pack with me. That way, I know where the power's coming from. I know what I'm plugged into. But the biggest thing is to just be aware of what you're plugging into and that there is a potential risk there. And, especially, if your phone or tablet says do you want to trust this computer, the answer's pretty much always no unless you're at your home. GONYEA: I will never look at a USB port the same way again. SISAK: And that's really the only point because it is not the most common thing. But it's just, like, well, you know, just so you know that it's out there. And just be aware. GONYEA: That was Luke Sisak, deputy DA for LA County and a cybercrimes prosecutor. Luke, thanks much. SISAK: Thanks, Don. Happy Thanksgiving. DON GONYEA, HOST:  The holiday season comes with lots of airport travel and lots of time waiting at airports. So, naturally, worn-out travelers look for distractions while they wait out that massive layover. You might want to plug in your favorite show on the iPad. But, unfortunately, the battery's dead, and you need to charge. If you're lucky, there are some USB ports around. But hold on, you might not be as lucky as you think. Some hackers could be using those free public USB ports to steal your data in a process experts are calling juice jacking. Perfect topic for our recurring Troll Watch segment. (SOUNDBITE OF MUSIC) GONYEA: Joining us now to talk about this trend is Luke Sisak. He's a deputy district attorney for Los Angeles County and a cybercrimes prosecutor. Luke, thanks for joining us. LUKE SISAK: Thanks for having me, Don. GONYEA: What exactly is juice jacking? How does it work? SISAK: Juice jacking is a way for a criminal to get your personal information out of your phone through your power port. It can work a variety of different ways. But the end result is that by plugging into a USB socket somewhere, a criminal has either downloaded your information or actually uploaded malware into your phone that will then send it to him or her wherever they are. And it isn't always the same way. But it's the same result, which is they have your information, and you've been compromised. GONYEA: So give us some examples of how they might actually get access to the information on my phone or my iPad or whatever. I'm sitting there, and there's the USB port just asking me to use it. SISAK: Well, the simplest way to think about it is when you plug your, let's say, iPhone into your computer, you'll get the little warning or little question box. It says, do you want to trust this computer? And if you press yes, you can see your photos and other things on your computer screen that are actually on your phone. They can build in a version of that that allows them to do the same thing. They don't really care about the photos. What they want are credit card numbers that are saved, Apple Pay accounts, even things like your address or your Social Security number. If they've been entered into a webpage or some other app, they potentially have the chance to get those. GONYEA: Is it that the actual USB port has been compromised in some way? Or is it a question of them, say, plugging in a handy USB cable there that you just think is there for you to use? SISAK: No, it's not the cable. It's the port itself. And, really, what it is is what's behind the port. For all intents and purposes, it's a little computer. It's got a little data socket - a USB socket. And it's got software installed in it, and there's a variety of ways to install that software, a variety of things it can do to your phone. GONYEA: Is there a visual thing we should notice as we look at a port to see if it has, perhaps, been compromised? Or should we just stay away from them, completely? SISAK: It's like anything else. The less reputable it looks, that's usually a good sign that it's a problem. What we tell everybody, though, is that, again, it's not common. But because it is possible, if you can avoid using just a USB socket for power and find an actual wall outlet, you're far better off. GONYEA: So the wall outlet is OK, probably OK. SISAK: So the wall outlet is going to be OK for sure because the wall outlet only transmits power. GONYEA: Do airports regularly check these ports to see if they've been compromised? SISAK: I don't know. And, obviously, within airports, each airport is different. But I don't know that airport staff would necessarily even know what to look for. And the toughest part about this issue is that people don't know they've been victimized. So it's part of the reason why it's hard for us to catch scammers or hackers in the act. And it's part of the reason why it's hard for airports or anywhere else to know because no one's complaining. Oh, a guy out there just took my purse. That's easy. But people may be in Omaha, Neb. , or Alaska or wherever by the time they realize they got their information taken. And they'll have no idea where it happened. GONYEA: So let's close by having you offer just your quick advice to people. SISAK: What I do is I bring a wall charger with me. That covers you most of the time. In the event that there is no wall charger, I usually - especially when I travel - will have a battery pack with me. That way, I know where the power's coming from. I know what I'm plugged into. But the biggest thing is to just be aware of what you're plugging into and that there is a potential risk there. And, especially, if your phone or tablet says do you want to trust this computer, the answer's pretty much always no unless you're at your home. GONYEA: I will never look at a USB port the same way again. SISAK: And that's really the only point because it is not the most common thing. But it's just, like, well, you know, just so you know that it's out there. And just be aware. GONYEA: That was Luke Sisak, deputy DA for LA County and a cybercrimes prosecutor. Luke, thanks much. SISAK: Thanks, Don. Happy Thanksgiving.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-02-784225426": {"title": "What You Should Consider When Thinking About Gifting Smart Speakers This Season : NPR", "url": "https://www.npr.org/2019/12/02/784225426/what-you-should-consider-when-thinking-about-gifting-smart-speakers-this-season", "author": "No author found", "published_date": "2019-12-02", "content": "AILSA CHANG, HOST: Whether you hit the stores on Black Friday or browsed Cyber Monday deals today, let's face it. Holiday shopping season is already in full swing. But before you click purchase on that smart speaker or fitness tracker, Lily Hay Newman wants you to think twice. She says you should know what you're getting people into before they unwrap the box. Newman is the security reporter at Wired, and I asked her what made her want to issue such a warning. LILY HAY NEWMAN: I think it was kind of a cumulative thing over time. I've written a ton of stories and done a lot of reporting about Internet of Things security and privacy. CHANG: Internet of Things - you mean like Internet-connected devices? NEWMAN: Internet-connected - exactly. And I've also received a lot of Internet-connected devices and kind of felt stuck with them and never knew what to do with them because I didn't want to use them myself. So it's like, do I try to return it? Do I tell the person? Do I regift it? CHANG: It's so awkward. NEWMAN: Yeah. You're kind of in a bind. CHANG: OK, so you warn people it's not just cheap, off-brand devices that we're talking about that could end up gathering all kinds of information about you. It's stuff made by huge companies like Google, Apple, Amazon. Let me just ask you, how clearly do they say to people what they're going to be doing with the data they collect? NEWMAN: Well, a big part of the problem is that it is pretty unclear, and there have been a lot of different examples of big companies and small companies using data in ways that ends up being surprising to the users. For example, virtually all of the big smart assistant manufacturers got caught in a situation where consumers didn't really realize that they were using audio snippets of people talking to their devices to check how the machine learning that works on the device was doing and make it better, which, in a way, is a good service. But it involves human-to-human - you know, someone listening to what you were saying. . . CHANG: Right. NEWMAN: . . . Which is exactly the concern. And the companies mostly said, well, this is just how machine learning works at this point. It involves some of this human input. And the customers were like, OK, well, we didn't know that, you know? So. . . CHANG: Why are you using my voice to feed. . . NEWMAN: Right. CHANG: . . . Your machine's brain? Well, what about stuff that kids use? Like, I could see Internet-connected toys or Fitbits end up on some children's wish lists. In fact, I am hearing already from colleagues who are seeing this on their children's wish lists already. What sort of precautions could you take to protect kids' privacy? NEWMAN: There are some Internet-connected devices that are tailor-made for kids. They incorporate additional privacy and security protections, so there are limits on the type of data the device can collect. But I also think there's a big issue with gifting these devices to kids because kids aren't necessarily in a position to weigh the pros and cons of using the device. And if adults in their lives are promoting the use of these devices, then they're going to think it's safe. CHANG: So what is the best etiquette around giving someone one of these devices? I mean, do you think I should ask first before I actually give someone one of these devices, or should I just include a gift receipt? NEWMAN: Yeah. I like all of those options. I mean, I think with a gift receipt, they can choose whether they actually want it - or asking them or, you know, someone close to them, well, which one did they want? Which model? What are the features they're looking for? Things like that. CHANG: Or else they'll just end up regifting it or throwing it out anyway. NEWMAN: Yeah, and, I mean. . . CHANG: Who wants that? NEWMAN: I'm ashamed to say that I've regifted Internet-connected devices. I'm swearing it off now. CHANG: You are making a pledge on public radio. NEWMAN: I never - yeah. No more. CHANG: Lily Hay Newman covers information security, digital privacy and hacking for Wired. Thanks very much. NEWMAN: Thanks for having me. AILSA CHANG, HOST:  Whether you hit the stores on Black Friday or browsed Cyber Monday deals today, let's face it. Holiday shopping season is already in full swing. But before you click purchase on that smart speaker or fitness tracker, Lily Hay Newman wants you to think twice. She says you should know what you're getting people into before they unwrap the box. Newman is the security reporter at Wired, and I asked her what made her want to issue such a warning. LILY HAY NEWMAN: I think it was kind of a cumulative thing over time. I've written a ton of stories and done a lot of reporting about Internet of Things security and privacy. CHANG: Internet of Things - you mean like Internet-connected devices? NEWMAN: Internet-connected - exactly. And I've also received a lot of Internet-connected devices and kind of felt stuck with them and never knew what to do with them because I didn't want to use them myself. So it's like, do I try to return it? Do I tell the person? Do I regift it? CHANG: It's so awkward. NEWMAN: Yeah. You're kind of in a bind. CHANG: OK, so you warn people it's not just cheap, off-brand devices that we're talking about that could end up gathering all kinds of information about you. It's stuff made by huge companies like Google, Apple, Amazon. Let me just ask you, how clearly do they say to people what they're going to be doing with the data they collect? NEWMAN: Well, a big part of the problem is that it is pretty unclear, and there have been a lot of different examples of big companies and small companies using data in ways that ends up being surprising to the users. For example, virtually all of the big smart assistant manufacturers got caught in a situation where consumers didn't really realize that they were using audio snippets of people talking to their devices to check how the machine learning that works on the device was doing and make it better, which, in a way, is a good service. But it involves human-to-human - you know, someone listening to what you were saying. . . CHANG: Right. NEWMAN: . . . Which is exactly the concern. And the companies mostly said, well, this is just how machine learning works at this point. It involves some of this human input. And the customers were like, OK, well, we didn't know that, you know? So. . . CHANG: Why are you using my voice to feed. . . NEWMAN: Right. CHANG: . . . Your machine's brain? Well, what about stuff that kids use? Like, I could see Internet-connected toys or Fitbits end up on some children's wish lists. In fact, I am hearing already from colleagues who are seeing this on their children's wish lists already. What sort of precautions could you take to protect kids' privacy? NEWMAN: There are some Internet-connected devices that are tailor-made for kids. They incorporate additional privacy and security protections, so there are limits on the type of data the device can collect. But I also think there's a big issue with gifting these devices to kids because kids aren't necessarily in a position to weigh the pros and cons of using the device. And if adults in their lives are promoting the use of these devices, then they're going to think it's safe. CHANG: So what is the best etiquette around giving someone one of these devices? I mean, do you think I should ask first before I actually give someone one of these devices, or should I just include a gift receipt? NEWMAN: Yeah. I like all of those options. I mean, I think with a gift receipt, they can choose whether they actually want it - or asking them or, you know, someone close to them, well, which one did they want? Which model? What are the features they're looking for? Things like that. CHANG: Or else they'll just end up regifting it or throwing it out anyway. NEWMAN: Yeah, and, I mean. . . CHANG: Who wants that? NEWMAN: I'm ashamed to say that I've regifted Internet-connected devices. I'm swearing it off now. CHANG: You are making a pledge on public radio. NEWMAN: I never - yeah. No more. CHANG: Lily Hay Newman covers information security, digital privacy and hacking for Wired. Thanks very much. NEWMAN: Thanks for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-03-784570156": {"title": "Google Founders Sergey Brin, Larry Page Step Down; Pichai Becomes Alphabet CEO : NPR", "url": "https://www.npr.org/2019/12/03/784570156/google-founders-brin-page-step-down-pichai-takes-over-as-alphabet-ceo", "author": "No author found", "published_date": "2019-12-03", "content": "MARY LOUISE KELLY, HOST: It is a historic moment for one of the most influential companies of our time, Google. Its two co-founders are stepping down. Here to talk about Larry Page and Sergey Brin and their move is NPR's technology correspondent Shannon Bond. Hi, Shannon. SHANNON BOND, BYLINE: Hi, Mary Louise. KELLY: So tell me more. What happened today? BOND: Yeah. Well, it's the end of an era at Google. Larry and Sergey founded the company in 1998, when they were Stanford University students. And. . . KELLY: I love that you're on a first-name basis with them, by the way. (LAUGHTER)KELLY: Go on. BOND: Well, I think a lot of people in the valley are. KELLY: Yeah. BOND: You know, think about them that way - they - you know, they made Google into one of the largest companies in the world. It dominates online search and digital advertising and video. Just a few moments ago, they announced they're leading - leaving their leadership roles. Now, they had already been playing less of an active role in the past few years. Larry Page in particular hasn't really been publicly, you know, present at Google. KELLY: OK. BOND: But they say they'll still be active board members but no longer calling the shots. That's going to be Google's current CEO, Sundar Pichai. He will be CEO of both Google and its parent company, Alphabet. KELLY: But so why? Why would they step away from this company that's their baby? BOND: Well, right. This is a company that they founded and that they have seen through a lot. I think, you know, part of it is the company has really changed over time. It's not that sort of idealistic place that they started. They've made a lot of money. They've been focusing on other things. And I think they say it's now time, you know, for new management to reflect where the company currently is. KELLY: And where is the current state of Google? I mean, what kind of shape will they be leaving it in as they step away? BOND: Well, it is a very turbulent time right now at Google, maybe the most turbulent in its short history. You know, Google is, of course, extremely profitable, but it's facing a lot of challenges, including from within. There are employees who are really, really unhappy. There've been a lot of protests over a range of issues - contracts with the military, contracts with immigration agencies. One day in November last year, 20,000 Google workers walked out over sexual harassment and bad behavior by executives that they said was tolerated. Google's always been known for this very open, freewheeling culture. Employees were encouraged to speak out, but that's been really cracked down on lately. Just last week, four employees who were involved in protests were fired. There's also external pressure from regulators who have been looking into just how dominant Google is in search and advertising. Some people even want the company to be broken up. Now, there's no indication that their stepping down is related to those issues. But I think it's just another sign just how far Google has come from 1998 when they started it. KELLY: Right. I mean, it sounds like a fascinating moment for a company that was famous for - wasn't the motto, don't be evil? BOND: Yeah. I mean, these were very idealistic guys. You know, they founded this company in their dorm room at Stanford. It was built around this vision of helping people find information. But it's grown. It's not just the biggest search engine with a 90% market share. It's an advertising behemoth. It's developing artificial intelligence. There are self-driving Google cars on the streets in Phoenix. And they acknowledged this change in a letter to employees. They said it's evolved and matured. KELLY: And time to turn the page - that is NPR's Shannon Bond. Thank you, Shannon. BOND: Thank you. MARY LOUISE KELLY, HOST:  It is a historic moment for one of the most influential companies of our time, Google. Its two co-founders are stepping down. Here to talk about Larry Page and Sergey Brin and their move is NPR's technology correspondent Shannon Bond. Hi, Shannon. SHANNON BOND, BYLINE: Hi, Mary Louise. KELLY: So tell me more. What happened today? BOND: Yeah. Well, it's the end of an era at Google. Larry and Sergey founded the company in 1998, when they were Stanford University students. And. . . KELLY: I love that you're on a first-name basis with them, by the way. (LAUGHTER) KELLY: Go on. BOND: Well, I think a lot of people in the valley are. KELLY: Yeah. BOND: You know, think about them that way - they - you know, they made Google into one of the largest companies in the world. It dominates online search and digital advertising and video. Just a few moments ago, they announced they're leading - leaving their leadership roles. Now, they had already been playing less of an active role in the past few years. Larry Page in particular hasn't really been publicly, you know, present at Google. KELLY: OK. BOND: But they say they'll still be active board members but no longer calling the shots. That's going to be Google's current CEO, Sundar Pichai. He will be CEO of both Google and its parent company, Alphabet. KELLY: But so why? Why would they step away from this company that's their baby? BOND: Well, right. This is a company that they founded and that they have seen through a lot. I think, you know, part of it is the company has really changed over time. It's not that sort of idealistic place that they started. They've made a lot of money. They've been focusing on other things. And I think they say it's now time, you know, for new management to reflect where the company currently is. KELLY: And where is the current state of Google? I mean, what kind of shape will they be leaving it in as they step away? BOND: Well, it is a very turbulent time right now at Google, maybe the most turbulent in its short history. You know, Google is, of course, extremely profitable, but it's facing a lot of challenges, including from within. There are employees who are really, really unhappy. There've been a lot of protests over a range of issues - contracts with the military, contracts with immigration agencies. One day in November last year, 20,000 Google workers walked out over sexual harassment and bad behavior by executives that they said was tolerated. Google's always been known for this very open, freewheeling culture. Employees were encouraged to speak out, but that's been really cracked down on lately. Just last week, four employees who were involved in protests were fired. There's also external pressure from regulators who have been looking into just how dominant Google is in search and advertising. Some people even want the company to be broken up. Now, there's no indication that their stepping down is related to those issues. But I think it's just another sign just how far Google has come from 1998 when they started it. KELLY: Right. I mean, it sounds like a fascinating moment for a company that was famous for - wasn't the motto, don't be evil? BOND: Yeah. I mean, these were very idealistic guys. You know, they founded this company in their dorm room at Stanford. It was built around this vision of helping people find information. But it's grown. It's not just the biggest search engine with a 90% market share. It's an advertising behemoth. It's developing artificial intelligence. There are self-driving Google cars on the streets in Phoenix. And they acknowledged this change in a letter to employees. They said it's evolved and matured. KELLY: And time to turn the page - that is NPR's Shannon Bond. Thank you, Shannon. BOND: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-03-783002964": {"title": "Fair Count Sets Up Internet Hotspots Around Georgia Ahead Of Mainly Online Count : NPR", "url": "https://www.npr.org/2019/12/03/783002964/installing-free-wi-fi-to-help-count-rural-communities-of-color-in-2020-census", "author": "No author found", "published_date": "2019-12-03", "content": "", "section": "National", "disclaimer": ""}, "2019-12-04-784884508": {"title": "Instagram Now Requires Users To Provide Their Age : NPR", "url": "https://www.npr.org/2019/12/04/784884508/instagram-now-requires-users-to-provide-their-age", "author": "No author found", "published_date": "2019-12-04", "content": "", "section": "Technology", "disclaimer": ""}, "2019-12-05-785037245": {"title": "Uber Received Nearly 6,000 U.S. Sexual Assault Claims In Past 2 Years : NPR", "url": "https://www.npr.org/2019/12/05/785037245/uber-received-nearly-6-000-u-s-sexual-assault-claims-in-past-2-years", "author": "No author found", "published_date": "2019-12-05", "content": "AILSA CHANG, HOST: How safe is your Uber ride? That question has dogged the company for years as complaints have piled up from passengers and drivers alleging they were sexually assaulted inside an Uber. Now Uber is revealing the scale of these complaints for the first time. A new report tallies the number of sexual assaults, deaths and fatal crashes that happened during Uber rides in the U. S. in 2017 and 2018. NPR's tech correspondent Shannon Bond has more. SHANNON BOND, BYLINE: The numbers and the details are disturbing. In two years, Uber received 5,981 reports of serious sexual assault in the U. S. They range from allegations of unwanted touching and kissing to rape. The report covers rides in the U. S. only in 2017 and 2018. As shocking as the numbers are, the incidents make up far less than 1% of the 2. 3 billion rides people in the U. S. took on Uber in those two years. TONY WEST: At the scale that Uber operates, we're going to see both the good and the bad that happens in society because we're operating so many trips every single day. BOND: Tony West is Uber's chief legal officer. He says Uber is so big, the problems it's facing are not unique to Uber. WEST: And one of the unfortunate-but-sad truths is that sexual assault, sexual violence is far more prevalent in American society than a lot of people recognize. BOND: The report is a window into a longstanding problem for ride-hailing companies from Uber and Lyft in the U. S. to DiDi in China and Ola in India. And we should note - Uber is among NPR's financial sponsors. These apps have made it really easy to get around cities, but they've also come under intense scrutiny for how they screen their drivers, handle complaints of harm and remove offenders from their platforms. Uber's data shows that both passengers and drivers are at risk. In fact, passengers were accused of sexual assault in nearly half the claims. The report doesn't present a complete picture. It doesn't count other types of sexual misconduct like masturbation, asking for sex or verbal threats of assault. However, in the two years the report covers, the rate of incidents per ride went down in every category. Tony West says that's encouraging. WEST: So I feel comfortable that Uber is a very safe mode of transportation. I think the data in this report actually indicates that as well. BOND: The report also covers deaths involving Uber rides. During those two years, 19 people were killed in physical assaults in connection with an Uber ride, and 107 people died in crashes involving Uber cars. By comparison, 74,000 people in total died on U. S. roads in the same period, according to federal data. With scrutiny of its safety record intensifying, Uber has rolled out new features to prevent problems and make it easier to report them. There is now a panic button in its app that passengers and drivers can use to call 911. And Uber's testing out additional safety measures such as audio or video recording rides. And the company wants to share the names of drivers it has banned for serious safety violations with other companies. That could help prevent a driver accused of groping, for example, from simply switching over to driving for Lyft. Cindy Southworth of the National Network to End Domestic Violence has been advising Uber about safety since 2015. She says sharing names of banned drivers will help address a big concern for victims. CINDY SOUTHWORTH: I think what most survivors want is this not to happen to someone else. If somebody comes forward and tells their story and it's determined that the rider or driver is not safe to be part of the Uber platform, then they can be removed. BOND: But sharing those names could violate drivers' privacy and raise questions of fairness. Uber will have to weigh that against its pledge to become safer. Shannon Bond, NPR News, Washington. AILSA CHANG, HOST:  How safe is your Uber ride? That question has dogged the company for years as complaints have piled up from passengers and drivers alleging they were sexually assaulted inside an Uber. Now Uber is revealing the scale of these complaints for the first time. A new report tallies the number of sexual assaults, deaths and fatal crashes that happened during Uber rides in the U. S. in 2017 and 2018. NPR's tech correspondent Shannon Bond has more. SHANNON BOND, BYLINE: The numbers and the details are disturbing. In two years, Uber received 5,981 reports of serious sexual assault in the U. S. They range from allegations of unwanted touching and kissing to rape. The report covers rides in the U. S. only in 2017 and 2018. As shocking as the numbers are, the incidents make up far less than 1% of the 2. 3 billion rides people in the U. S. took on Uber in those two years. TONY WEST: At the scale that Uber operates, we're going to see both the good and the bad that happens in society because we're operating so many trips every single day. BOND: Tony West is Uber's chief legal officer. He says Uber is so big, the problems it's facing are not unique to Uber. WEST: And one of the unfortunate-but-sad truths is that sexual assault, sexual violence is far more prevalent in American society than a lot of people recognize. BOND: The report is a window into a longstanding problem for ride-hailing companies from Uber and Lyft in the U. S. to DiDi in China and Ola in India. And we should note - Uber is among NPR's financial sponsors. These apps have made it really easy to get around cities, but they've also come under intense scrutiny for how they screen their drivers, handle complaints of harm and remove offenders from their platforms. Uber's data shows that both passengers and drivers are at risk. In fact, passengers were accused of sexual assault in nearly half the claims. The report doesn't present a complete picture. It doesn't count other types of sexual misconduct like masturbation, asking for sex or verbal threats of assault. However, in the two years the report covers, the rate of incidents per ride went down in every category. Tony West says that's encouraging. WEST: So I feel comfortable that Uber is a very safe mode of transportation. I think the data in this report actually indicates that as well. BOND: The report also covers deaths involving Uber rides. During those two years, 19 people were killed in physical assaults in connection with an Uber ride, and 107 people died in crashes involving Uber cars. By comparison, 74,000 people in total died on U. S. roads in the same period, according to federal data. With scrutiny of its safety record intensifying, Uber has rolled out new features to prevent problems and make it easier to report them. There is now a panic button in its app that passengers and drivers can use to call 911. And Uber's testing out additional safety measures such as audio or video recording rides. And the company wants to share the names of drivers it has banned for serious safety violations with other companies. That could help prevent a driver accused of groping, for example, from simply switching over to driving for Lyft. Cindy Southworth of the National Network to End Domestic Violence has been advising Uber about safety since 2015. She says sharing names of banned drivers will help address a big concern for victims. CINDY SOUTHWORTH: I think what most survivors want is this not to happen to someone else. If somebody comes forward and tells their story and it's determined that the rider or driver is not safe to be part of the Uber platform, then they can be removed. BOND: But sharing those names could violate drivers' privacy and raise questions of fairness. Uber will have to weigh that against its pledge to become safer. Shannon Bond, NPR News, Washington.", "section": "Business", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-05-785253931": {"title": "American Citizen Arrested After Giving Talk On Cryptocurrency In North Korea : NPR", "url": "https://www.npr.org/2019/12/05/785253931/american-citizen-arrested-after-giving-talk-on-cryptocurrency-in-north-korea", "author": "No author found", "published_date": "2019-12-05", "content": "MARY LOUISE KELLY, HOST: Last Thursday - so Thanksgiving Day, a 36-year-old American man named Virgil Griffith stepped off a plane at Los Angeles International Airport and was arrested. The Justice Department has charged Griffith with helping North Korea evade sanctions. Griffith is a computer scientist, and he had given a talk earlier this year at a Pyongyang conference on cryptocurrency. Authorities say he discussed how to launder money and dodge sanctions. I'm joined now by Jason Brett, who wrote about this for forbes. com. Thanks for coming in. JASON BRETT: Thank you for having me. KELLY: So Virgil Griffith, I gather, is something of a well-known guy in technology circles. Tell me about him. BRETT: Yeah. Virgil Griffith is a well-known hacker, and he's a bit of a cypherpunk (ph). That's cypher like C-Y-P-H-E-R punk. . . KELLY: I was about to ask you, yeah. BRETT: Mmm hmm, yep - which is people who really believe in this cryptography that's really behind the cryptocurrency and blockchain and kind of have an ethos of they'll go wherever in the world they can go to teach it 'cause they really believe it's something that can change the world. KELLY: What is his interest in North Korea? BRETT: So Virgil Griffith is currently a computer scientist that works at the Ethereum Foundation, which is one of the largest cryptocurrencies, second only to bitcoin. His interest isn't clear. However, it did appear that he told people back in 2018 he wanted to travel there to teach them cryptocurrency. And the title of the talk that he made when he was in North Korea was called Blockchain and Peace. So it appears he believed he was trying to help North Korea. However, in text messages that the Department of Justice received - when he asked, what does North Korea want to know about this cryptocurrency? - he said, perhaps evading sanctions. Who knows? KELLY: What does the Justice Department allege here? I mean, give me a little bit more detail in terms of what they've charged him with. BRETT: Sure, sure. So they've charged him with breaking the IEEPA, which is the international protection act that started with George Bush in 2008. In that executive order, it said that there's unusual circumstances where North Korea may act in a way of, you know, firing nuclear missiles at the United States, and there was questions about how they were raising money for it. And then in 2016, Obama added to OFAC's regime, saying if you provide any products or services or technology to North Korea, that's also a violation of sanctions. KELLY: OFAC being the U. S. government agency that enforces sanctions? BRETT: Mmm hmm, yes. The problem with cryptocurrency is that cryptocurrency is probably used now by North Korea as a way of evading international banking sanctions. And so that is a way that they are using that money to build their nuclear arms. KELLY: Give me some sense of how this fits more broadly into U. S. concerns about cryptocurrency. I mean, it's obviously not just North Korea that the U. S. is tracking closely but other governments and how they might use this and what risks that might pose. BRETT: Yeah, absolutely. So the U. S. government has been looking at cryptocurrency for a long time now. And it's an ongoing issue for them with this new technology because you're really talking about weakening the U. S. dollar's ability to impose sanctions on countries. And so for the United States, it's a problem that they're facing 'cause this new technology is very disruptive. KELLY: What might happen next for Virgil Griffith? BRETT: So Virgil Griffith is - has an excellent lawyer representing him who's represented others in the cryptocurrency space before in these situations. You know, he is facing up to 20 years in prison for violating the sanctions. I think that it's going to be definitely an interesting trial because it's going to have a lot to do with his motives. But it's also - for many people in the community, such as myself, we don't want to see the technology go on trial. KELLY: Jason Brett, contributor to forbes. com. A fascinating reporting - thanks for sharing it. BRETT: Thank you. (SOUNDBITE OF MUSIC) MARY LOUISE KELLY, HOST:  Last Thursday - so Thanksgiving Day, a 36-year-old American man named Virgil Griffith stepped off a plane at Los Angeles International Airport and was arrested. The Justice Department has charged Griffith with helping North Korea evade sanctions. Griffith is a computer scientist, and he had given a talk earlier this year at a Pyongyang conference on cryptocurrency. Authorities say he discussed how to launder money and dodge sanctions. I'm joined now by Jason Brett, who wrote about this for forbes. com. Thanks for coming in. JASON BRETT: Thank you for having me. KELLY: So Virgil Griffith, I gather, is something of a well-known guy in technology circles. Tell me about him. BRETT: Yeah. Virgil Griffith is a well-known hacker, and he's a bit of a cypherpunk (ph). That's cypher like C-Y-P-H-E-R punk. . . KELLY: I was about to ask you, yeah. BRETT: Mmm hmm, yep - which is people who really believe in this cryptography that's really behind the cryptocurrency and blockchain and kind of have an ethos of they'll go wherever in the world they can go to teach it 'cause they really believe it's something that can change the world. KELLY: What is his interest in North Korea? BRETT: So Virgil Griffith is currently a computer scientist that works at the Ethereum Foundation, which is one of the largest cryptocurrencies, second only to bitcoin. His interest isn't clear. However, it did appear that he told people back in 2018 he wanted to travel there to teach them cryptocurrency. And the title of the talk that he made when he was in North Korea was called Blockchain and Peace. So it appears he believed he was trying to help North Korea. However, in text messages that the Department of Justice received - when he asked, what does North Korea want to know about this cryptocurrency? - he said, perhaps evading sanctions. Who knows? KELLY: What does the Justice Department allege here? I mean, give me a little bit more detail in terms of what they've charged him with. BRETT: Sure, sure. So they've charged him with breaking the IEEPA, which is the international protection act that started with George Bush in 2008. In that executive order, it said that there's unusual circumstances where North Korea may act in a way of, you know, firing nuclear missiles at the United States, and there was questions about how they were raising money for it. And then in 2016, Obama added to OFAC's regime, saying if you provide any products or services or technology to North Korea, that's also a violation of sanctions. KELLY: OFAC being the U. S. government agency that enforces sanctions? BRETT: Mmm hmm, yes. The problem with cryptocurrency is that cryptocurrency is probably used now by North Korea as a way of evading international banking sanctions. And so that is a way that they are using that money to build their nuclear arms. KELLY: Give me some sense of how this fits more broadly into U. S. concerns about cryptocurrency. I mean, it's obviously not just North Korea that the U. S. is tracking closely but other governments and how they might use this and what risks that might pose. BRETT: Yeah, absolutely. So the U. S. government has been looking at cryptocurrency for a long time now. And it's an ongoing issue for them with this new technology because you're really talking about weakening the U. S. dollar's ability to impose sanctions on countries. And so for the United States, it's a problem that they're facing 'cause this new technology is very disruptive. KELLY: What might happen next for Virgil Griffith? BRETT: So Virgil Griffith is - has an excellent lawyer representing him who's represented others in the cryptocurrency space before in these situations. You know, he is facing up to 20 years in prison for violating the sanctions. I think that it's going to be definitely an interesting trial because it's going to have a lot to do with his motives. But it's also - for many people in the community, such as myself, we don't want to see the technology go on trial. KELLY: Jason Brett, contributor to forbes. com. A fascinating reporting - thanks for sharing it. BRETT: Thank you. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-05-785034567": {"title": "Justice Department Indicts Russian Hacking Group Over Alleged Bank Fraud : NPR", "url": "https://www.npr.org/2019/12/05/785034567/russian-hacking-group-evil-corp-charged-by-federal-prosecutors-in-alleged-bank-f", "author": "No author found", "published_date": "2019-12-05", "content": "", "section": "Technology", "disclaimer": ""}, "2019-12-05-783164944": {"title": "Delivery Only: The Rise Of Restaurants With No Diners As Apps Take Orders : NPR", "url": "https://www.npr.org/2019/12/05/783164944/delivery-only-the-rise-of-restaurants-with-no-diners-as-apps-take-orders", "author": "No author found", "published_date": "2019-12-05", "content": "", "section": "Business", "disclaimer": ""}, "2019-12-07-785804847": {"title": "NATO Targets Disinformation Efforts : NPR", "url": "https://www.npr.org/2019/12/07/785804847/nato-targets-disinformation-efforts", "author": "No author found", "published_date": "2019-12-07", "content": "SCOTT SIMON, HOST: Of course, NATO was created as a Western military alliance. But when NATO-sponsored experts met in Riga, Latvia this week, it was to examine a non-military adversary of increasing concern to them - social media manipulation. The prevalence of what experts refer to as malign influence campaigns - the rest of us call it disinformation - is both growing and spreading. Janis Sarts heads the NATO Strategic Communications Center of Excellence in Riga. Mr. Sarts, thanks so much for being with us. JANIS SARTS: Thank you. SIMON: Is disinformation, in your mind, the equivalent of a military threat? SARTS: Certainly it is a national security threat. And particularly, it is increasing because of the way technology has transformed the overall information landscape. And that allows for also the new ways of manipulating disinformation and using disinformation to affect our cognitive spaces. And that is obviously increasingly a national security threat. SIMON: And how would it be a threat to national security? SARTS: Well, because that undermines the internal societal discussion processes. These used the systems to actually create distraction on the certain sets of fake news. And in these cases, that is meant to undermine the stability within the given society. SIMON: Can you give us a for instance that you were able to notice? SARTS: There was, a few years ago, a referendum for the Catalonian independence. And then suddenly, the bots, which seemed to be Venezuelan bots, had started to spread this highly divisive images of the atrocities of the police, most of them inaccurate. And they were meant to deepen the crisis which was unfolding and thus achieving this for agility in Spain and using this opportunity to divide the society. SIMON: This isn't just happening in Europe, is it? SARTS: No, no, no, no. Actually, Europe is a small place. But, of course, in United States, in Asia, in India, Mexico, Brazil - it's all over place. The interesting twist, though, is core of this manipulation infrastructure is actually Russian-owned. SIMON: And who does it? Is it foreign intelligence services? SARTS: We know there is a very, very vibrant, vast ecosystem that sells the SERPs (ph). We estimate that most of that is commercial activity, but we believe some within that commercial activity, you would have also other state-related actors. All of that is for sale. And to give you the context, we bought 50,000 engagements during EU election for 300 euros. So it's really very accessible. And it is cheap. SIMON: Social media companies have to bear special responsibility for this? SARTS: Well, obviously you see that what this whole ecosystem is doing, it is undermining the credibility of what is taking place within these platforms. So I would believe that is not in their business interest. But you clearly see their kind of practice is not sufficient enough to confront that risk. But I think also what would be important that there is some kind of ability for - from a public perspective - to get some transparency. Is what these social media companies saying what they're doing, is it the real deal? Does it deliver? Because for most of the case, we have to take their word for granted. SIMON: I think a lot of us have heard about obviously trolling and deepfakes. What technology particularly concerns you right now? SARTS: Well, right now, what I'm really worried is the way big data, AI and increased knowledge of how human mind operates and behavioral sciences are marrying together. You know, we recently run an experiment together, one of the allied militaries, where, in a military exercise, we tried to see whether by scraping the open source data we could indicate who are the soldiers participating in the exercise, and based on these data sets that were openly available, whether we can influence what soldiers do in a military exercise. And what we were able to accomplish was we were able to make soldiers disobey orders, leave the positions they are supposed to defend during a military exercise, which to me indicates the kind of trajectory of the risk because the kind of skilled actors and the amount of data that we are leaving as people in the digital space. . . SIMON: Yeah. SARTS: . . . With a good AI capability, can deliver very significant ability to manipulate the behavioral outcomes. And that is something that we have to really start very, very quickly starting to solve from a national security perspective but also from a individual privacy and individual security perspective because there are, of course, many ways one can use that for malign purposes. SIMON: Janis Sarts heads the native Strategic Communications Center of Excellence in Riga, Latvia. Thanks so much for being with us, Mr. Sarts. SARTS: Thank you very much. SCOTT SIMON, HOST:  Of course, NATO was created as a Western military alliance. But when NATO-sponsored experts met in Riga, Latvia this week, it was to examine a non-military adversary of increasing concern to them - social media manipulation. The prevalence of what experts refer to as malign influence campaigns - the rest of us call it disinformation - is both growing and spreading. Janis Sarts heads the NATO Strategic Communications Center of Excellence in Riga. Mr. Sarts, thanks so much for being with us. JANIS SARTS: Thank you. SIMON: Is disinformation, in your mind, the equivalent of a military threat? SARTS: Certainly it is a national security threat. And particularly, it is increasing because of the way technology has transformed the overall information landscape. And that allows for also the new ways of manipulating disinformation and using disinformation to affect our cognitive spaces. And that is obviously increasingly a national security threat. SIMON: And how would it be a threat to national security? SARTS: Well, because that undermines the internal societal discussion processes. These used the systems to actually create distraction on the certain sets of fake news. And in these cases, that is meant to undermine the stability within the given society. SIMON: Can you give us a for instance that you were able to notice? SARTS: There was, a few years ago, a referendum for the Catalonian independence. And then suddenly, the bots, which seemed to be Venezuelan bots, had started to spread this highly divisive images of the atrocities of the police, most of them inaccurate. And they were meant to deepen the crisis which was unfolding and thus achieving this for agility in Spain and using this opportunity to divide the society. SIMON: This isn't just happening in Europe, is it? SARTS: No, no, no, no. Actually, Europe is a small place. But, of course, in United States, in Asia, in India, Mexico, Brazil - it's all over place. The interesting twist, though, is core of this manipulation infrastructure is actually Russian-owned. SIMON: And who does it? Is it foreign intelligence services? SARTS: We know there is a very, very vibrant, vast ecosystem that sells the SERPs (ph). We estimate that most of that is commercial activity, but we believe some within that commercial activity, you would have also other state-related actors. All of that is for sale. And to give you the context, we bought 50,000 engagements during EU election for 300 euros. So it's really very accessible. And it is cheap. SIMON: Social media companies have to bear special responsibility for this? SARTS: Well, obviously you see that what this whole ecosystem is doing, it is undermining the credibility of what is taking place within these platforms. So I would believe that is not in their business interest. But you clearly see their kind of practice is not sufficient enough to confront that risk. But I think also what would be important that there is some kind of ability for - from a public perspective - to get some transparency. Is what these social media companies saying what they're doing, is it the real deal? Does it deliver? Because for most of the case, we have to take their word for granted. SIMON: I think a lot of us have heard about obviously trolling and deepfakes. What technology particularly concerns you right now? SARTS: Well, right now, what I'm really worried is the way big data, AI and increased knowledge of how human mind operates and behavioral sciences are marrying together. You know, we recently run an experiment together, one of the allied militaries, where, in a military exercise, we tried to see whether by scraping the open source data we could indicate who are the soldiers participating in the exercise, and based on these data sets that were openly available, whether we can influence what soldiers do in a military exercise. And what we were able to accomplish was we were able to make soldiers disobey orders, leave the positions they are supposed to defend during a military exercise, which to me indicates the kind of trajectory of the risk because the kind of skilled actors and the amount of data that we are leaving as people in the digital space. . . SIMON: Yeah. SARTS: . . . With a good AI capability, can deliver very significant ability to manipulate the behavioral outcomes. And that is something that we have to really start very, very quickly starting to solve from a national security perspective but also from a individual privacy and individual security perspective because there are, of course, many ways one can use that for malign purposes. SIMON: Janis Sarts heads the native Strategic Communications Center of Excellence in Riga, Latvia. Thanks so much for being with us, Mr. Sarts. SARTS: Thank you very much.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-07-785804791": {"title": "Uighurs And Genetic Surveillance In China : NPR", "url": "https://www.npr.org/2019/12/07/785804791/uighurs-and-genetic-surveillance-in-china", "author": "No author found", "published_date": "2019-12-07", "content": "SCOTT SIMON, HOST: DNA data has been used to track and identify alleged criminals for decades, but what happens when China begins to use that technology to identify and detain people based on their ethnicity, especially ethnic minorities like Uighur Muslims, in the name of national security? Yves Moreau is an engineer and professor at the Catholic University in Leuven, Belgium. He studies human genetics and the ethics involved in that. Mr. Moreau, thanks so much for being with us. YVES MOREAU: Thank you very much for the invitation. SIMON: How exactly is China collecting DNA and then using that information to identify people? MOREAU: So overall, in the whole of China, the technique that is used to, indeed, investigate criminals and crime scenes has been rolled out on a very large scale. And what we have seen is that this technology is being rolled out in particular in the west of China. And in 2016, 2017, blood samples from essentially the entire population, people 12 to 65 in Xinjiang, was collected and potentially put in that database. And it can be part of a broader system of what we call total surveillance. SIMON: So can this technology be used, for example, to survey a crowd and pick out Uighurs or, let's say, Tibetans, Miao people, Yao people, other minorities or their relatives? MOREAU: So defining ethnicity is extremely, extremely messy. So actually, ethnicities is first a social and cultural concept. And now suddenly, we're talking about genes. However, it makes it possible to tomorrow decide that someone does belong or does not belong to a certain population. I'm extremely concerned about this because in history, actually, if you look back in the first half of the 20th century, German and then Belgian colonists in Rwanda and Burundi actually went there, and they were using pseudoscientific ideas about race and assigned people to a particular ethnicity. That actually was a significant factor in genocides. And the risk for this in the midterm is actually really worrying. SIMON: Are American companies, European companies contributing to this? MOREAU: So the technology that is needed to do these DNA studies requires, on the one hand, a device, a DNA sequencer. And it requires very specialized chemical reagents. And there is very significant involvement of American and European companies in this market. SIMON: You've also called on scientific journals and publications to be careful about what they publish. MOREAU: Yes, exactly. There is a huge level of activity - I mean up to some kind of obsession - I mean, it's very surprising - of studying the genetics of different populations across China. Tibetans are studied 40 times more intensely than the Hans, and the Uighurs are studied 30 times more intensely than the Hans. In the last eight years, out of over 500 population that did this genetic characterization of ethnic populations across China, half of all publications had a co-author from the Chinese police, the military, the judiciary or some such government institution. And I think that means that this kind of research isn't acceptable and that publishers - mostly Western publishers - should not have published all that literature. SIMON: Is this a losing struggle, Professor Moreau? I mean, DNA databases grow every week. MOREAU: If you look, for example, in Europe and the United States, there have been already strong battles that have been fought to actually put safeguards on this technology. And those are not perfect, but they've made a huge difference. I think that it's close to midnight. It's been two minutes before midnight, but I'm not desperate. And I think that it's really possible to still do something. But the battle is going to be very arduous. SIMON: Yves Moreau is an engineer and professor at the Catholic University in Leuven, Belgium. He spoke with us via Skype. Thanks so much for being with us. MOREAU: Thank you very much for your time. (SOUNDBITE OF THE ALBUM LEAF'S \"FALSE DAWN\") SCOTT SIMON, HOST:  DNA data has been used to track and identify alleged criminals for decades, but what happens when China begins to use that technology to identify and detain people based on their ethnicity, especially ethnic minorities like Uighur Muslims, in the name of national security? Yves Moreau is an engineer and professor at the Catholic University in Leuven, Belgium. He studies human genetics and the ethics involved in that. Mr. Moreau, thanks so much for being with us. YVES MOREAU: Thank you very much for the invitation. SIMON: How exactly is China collecting DNA and then using that information to identify people? MOREAU: So overall, in the whole of China, the technique that is used to, indeed, investigate criminals and crime scenes has been rolled out on a very large scale. And what we have seen is that this technology is being rolled out in particular in the west of China. And in 2016, 2017, blood samples from essentially the entire population, people 12 to 65 in Xinjiang, was collected and potentially put in that database. And it can be part of a broader system of what we call total surveillance. SIMON: So can this technology be used, for example, to survey a crowd and pick out Uighurs or, let's say, Tibetans, Miao people, Yao people, other minorities or their relatives? MOREAU: So defining ethnicity is extremely, extremely messy. So actually, ethnicities is first a social and cultural concept. And now suddenly, we're talking about genes. However, it makes it possible to tomorrow decide that someone does belong or does not belong to a certain population. I'm extremely concerned about this because in history, actually, if you look back in the first half of the 20th century, German and then Belgian colonists in Rwanda and Burundi actually went there, and they were using pseudoscientific ideas about race and assigned people to a particular ethnicity. That actually was a significant factor in genocides. And the risk for this in the midterm is actually really worrying. SIMON: Are American companies, European companies contributing to this? MOREAU: So the technology that is needed to do these DNA studies requires, on the one hand, a device, a DNA sequencer. And it requires very specialized chemical reagents. And there is very significant involvement of American and European companies in this market. SIMON: You've also called on scientific journals and publications to be careful about what they publish. MOREAU: Yes, exactly. There is a huge level of activity - I mean up to some kind of obsession - I mean, it's very surprising - of studying the genetics of different populations across China. Tibetans are studied 40 times more intensely than the Hans, and the Uighurs are studied 30 times more intensely than the Hans. In the last eight years, out of over 500 population that did this genetic characterization of ethnic populations across China, half of all publications had a co-author from the Chinese police, the military, the judiciary or some such government institution. And I think that means that this kind of research isn't acceptable and that publishers - mostly Western publishers - should not have published all that literature. SIMON: Is this a losing struggle, Professor Moreau? I mean, DNA databases grow every week. MOREAU: If you look, for example, in Europe and the United States, there have been already strong battles that have been fought to actually put safeguards on this technology. And those are not perfect, but they've made a huge difference. I think that it's close to midnight. It's been two minutes before midnight, but I'm not desperate. And I think that it's really possible to still do something. But the battle is going to be very arduous. SIMON: Yves Moreau is an engineer and professor at the Catholic University in Leuven, Belgium. He spoke with us via Skype. Thanks so much for being with us. MOREAU: Thank you very much for your time. (SOUNDBITE OF THE ALBUM LEAF'S \"FALSE DAWN\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-08-786039738": {"title": "Managing Misinformation On Reddit : NPR", "url": "https://www.npr.org/2019/12/08/786039738/managing-misinformation-on-reddit", "author": "No author found", "published_date": "2019-12-08", "content": "LULU GARCIA-NAVARRO, HOST: The discussion and news aggregation site Reddit sees some of the most web traffic in the United States, ranking higher than both Netflix and Instagram. In Reddit's forums, called subreddits, users find cooking advice, book recommendations, thriving TV discussions, all sorts of topics - including politics. Our next guest moderates the main political subreddit, r/politics. We're not calling him by his name because threats of physical harm and doxxing are so common on Reddit. He goes by the username Qu1nlan, and he told us that he sees mostly talk about impeachment these days. And though it has a slight leftward lean politically, there are users on Reddit from all over the political spectrum. QU1NLAN: We see a lot of really high-level, good faith discussion with people citing links, citing sources, talking about why they think the way they do. But we also see some people who are just saying Trump 2020 or impeach him; lock him up. But we do see people changing their minds about lower-level, less divisive things, such like, do you support gender-inclusive restrooms, or do you support the Keystone XL pipeline? - things like that. GARCIA-NAVARRO: We here at NPR have been covering, quote, unquote, \"fake news\" in its original sense - you know, disinformation, misleading information masked as journalism or facts - for years now. How do you and your team of moderators deal with that? QU1NLAN: In short, we don't. We are a team of about 50 moderators on a subreddit of 5. 6 million people. We don't have resources that a team like Facebook or Twitter might have with tons and tons of paid fact-checkers. So generally, when we see fake news that we know is not factually correct, we'll go ahead and encourage our user base to go ahead and fact-check it themselves and also comment - say, well, here's why this news is fake, and here is the sources to back it up. And here's why you shouldn't believe everything that you see on the Internet. GARCIA-NAVARRO: It sounds like a pretty unregulated community. Is that what you like about it? QU1NLAN: Well, I think that unregulated is not the word that I would necessarily use for it. But I think that. . . GARCIA-NAVARRO: Self-regulated, I think, would be a better word. QU1NLAN: I think self-regulated is a great word for this. And people are using civility and discussing things in a fashion that would not necessarily be used on other forums in the Internet with more anonymity, such as places like 4chan or even any other places, any other online forums with the requirement to create a username rather than use your real name. GARCIA-NAVARRO: But I must ask you - you're unpaid, right? You don't get paid to do this. You volunteer your time. But the reason we're not referring to you by name is because of concerns about doxxing and users actually threatening you. That doesn't sound very civil. QU1NLAN: Thankfully, it's rare that people will come in and threaten us with things like doxxing, things like personal harm. But when that does happen, it's very serious, and we will try to get protection from ourselves. Personally, I have been doxxed in the past. So I got my name in the Reddit-wide filter, which means that my full real name cannot actually be set on Reddit, that it will get automatically removed by the robots. So I'm very thankful that the Reddit admins also take it seriously. GARCIA-NAVARRO: You've been doing this for four or five years now. What have you seen change? QU1NLAN: I've seen a lot change. Back in the Obama administration, we had a lot of quiet times. And that was really nice, to be honest, as a moderator. We had people just generally discussing the issues. But as we've progressed through the 2016 election and the actual Trump presidency, I personally have seen incivility shoot up. I have seen things like hate speech shoot up. I have seen a lot of people a little bit more afraid to accept sources and, in general, a lot more divisiveness. GARCIA-NAVARRO: I guess my last question is, do moderators ever change beats? QU1NLAN: So I am very thankful that politics is not my only subreddit. I'm also a big geek personally, and I moderate the \"Star Wars\" subreddit. So whenever politics becomes a little bit too much for me - I've had too many slurs for one day or I've had too many sources that I have to read in one day - I'll go ahead, and I'll read about baby Yoda. And that'll make me feel better. GARCIA-NAVARRO: (Laughter) Hear, hear - although I have to tell you \"Star Wars\" can also get pretty contentious. QU1NLAN: Oh, I should tell you sometime about \"The Last Jedi\" movie release. That was something. GARCIA-NAVARRO: That's the Reddit moderator who goes by the username Qu1nlan. Thank you so much. QU1NLAN: Thank you, Lulu. LULU GARCIA-NAVARRO, HOST:  The discussion and news aggregation site Reddit sees some of the most web traffic in the United States, ranking higher than both Netflix and Instagram. In Reddit's forums, called subreddits, users find cooking advice, book recommendations, thriving TV discussions, all sorts of topics - including politics. Our next guest moderates the main political subreddit, r/politics. We're not calling him by his name because threats of physical harm and doxxing are so common on Reddit. He goes by the username Qu1nlan, and he told us that he sees mostly talk about impeachment these days. And though it has a slight leftward lean politically, there are users on Reddit from all over the political spectrum. QU1NLAN: We see a lot of really high-level, good faith discussion with people citing links, citing sources, talking about why they think the way they do. But we also see some people who are just saying Trump 2020 or impeach him; lock him up. But we do see people changing their minds about lower-level, less divisive things, such like, do you support gender-inclusive restrooms, or do you support the Keystone XL pipeline? - things like that. GARCIA-NAVARRO: We here at NPR have been covering, quote, unquote, \"fake news\" in its original sense - you know, disinformation, misleading information masked as journalism or facts - for years now. How do you and your team of moderators deal with that? QU1NLAN: In short, we don't. We are a team of about 50 moderators on a subreddit of 5. 6 million people. We don't have resources that a team like Facebook or Twitter might have with tons and tons of paid fact-checkers. So generally, when we see fake news that we know is not factually correct, we'll go ahead and encourage our user base to go ahead and fact-check it themselves and also comment - say, well, here's why this news is fake, and here is the sources to back it up. And here's why you shouldn't believe everything that you see on the Internet. GARCIA-NAVARRO: It sounds like a pretty unregulated community. Is that what you like about it? QU1NLAN: Well, I think that unregulated is not the word that I would necessarily use for it. But I think that. . . GARCIA-NAVARRO: Self-regulated, I think, would be a better word. QU1NLAN: I think self-regulated is a great word for this. And people are using civility and discussing things in a fashion that would not necessarily be used on other forums in the Internet with more anonymity, such as places like 4chan or even any other places, any other online forums with the requirement to create a username rather than use your real name. GARCIA-NAVARRO: But I must ask you - you're unpaid, right? You don't get paid to do this. You volunteer your time. But the reason we're not referring to you by name is because of concerns about doxxing and users actually threatening you. That doesn't sound very civil. QU1NLAN: Thankfully, it's rare that people will come in and threaten us with things like doxxing, things like personal harm. But when that does happen, it's very serious, and we will try to get protection from ourselves. Personally, I have been doxxed in the past. So I got my name in the Reddit-wide filter, which means that my full real name cannot actually be set on Reddit, that it will get automatically removed by the robots. So I'm very thankful that the Reddit admins also take it seriously. GARCIA-NAVARRO: You've been doing this for four or five years now. What have you seen change? QU1NLAN: I've seen a lot change. Back in the Obama administration, we had a lot of quiet times. And that was really nice, to be honest, as a moderator. We had people just generally discussing the issues. But as we've progressed through the 2016 election and the actual Trump presidency, I personally have seen incivility shoot up. I have seen things like hate speech shoot up. I have seen a lot of people a little bit more afraid to accept sources and, in general, a lot more divisiveness. GARCIA-NAVARRO: I guess my last question is, do moderators ever change beats? QU1NLAN: So I am very thankful that politics is not my only subreddit. I'm also a big geek personally, and I moderate the \"Star Wars\" subreddit. So whenever politics becomes a little bit too much for me - I've had too many slurs for one day or I've had too many sources that I have to read in one day - I'll go ahead, and I'll read about baby Yoda. And that'll make me feel better. GARCIA-NAVARRO: (Laughter) Hear, hear - although I have to tell you \"Star Wars\" can also get pretty contentious. QU1NLAN: Oh, I should tell you sometime about \"The Last Jedi\" movie release. That was something. GARCIA-NAVARRO: That's the Reddit moderator who goes by the username Qu1nlan. Thank you so much. QU1NLAN: Thank you, Lulu.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-10-786835248": {"title": "As TikTok Grows In Popularity, It's Also Setting Off Alarms In Silicon Valley : NPR", "url": "https://www.npr.org/2019/12/10/786835248/as-tiktok-grows-in-popularity-its-also-setting-off-alarms-in-silicon-valley", "author": "No author found", "published_date": "2019-12-10", "content": "AUDIE CORNISH, HOST: Now time for All Tech Considered. (SOUNDBITE OF MUSIC)CORNISH: And when it comes to tech, 2019 may go down in history as the year of TikTok. In 2019, the app has spawned memes, dance contests, even minted hit songs. (SOUNDBITE OF SONG, \"THE GIT UP\")BLANCO BROWN: (Singing) Going to do the two step then cowboy boogie. Grab your sweetheart and spin out with them. (SOUNDBITE OF SONG, \"PROM DRESS\")MXMTOON: (Singing) I'm sitting here, crying in my prom dress. I'd be the prom queen if crying was a contest. (SOUNDBITE OF SONG, \"OLD TOWN ROAD\")LIL NAS X: (Singing) Riding on a horse, you can whip your Porsche. CORNISH: It also set off alarm bells in Washington and Silicon Valley. NPR's tech correspondent Shannon Bond joins us now to talk about how the video sharing app went from teen fad to cultural juggernaut to possible national security threat. Shannon, welcome back. SHANNON BOND, BYLINE: Thanks for having me. CORNISH: How big is TikTok at this point? How many users? BOND: Well, it's kind of crazy. It's not even a year and a half old, and it's been downloaded 1. 5 billion times. So that means something like 1 in 5 people around the world have put this app on their phones. And teenagers and 20-somethings in particular have embraced it. I spoke with a TikToker - that's what they're called. Her name is Alexia Del Valle. ALEXIA DEL VALLE: It's, like, really exciting to know that people are enjoying something that I thought of, you know, yesterday, and I just readily made a video about. BOND: Del Valle is a 21-year-old student. She goes to college in Miami. And she makes these videos where she plays different characters. This summer, she made a parody of the Netflix show \"Stranger Things. \" It's called \"If Latina Moms Were In Stranger Things. \" Del Valle's Puerto Rican, and she drew on that for the video. (SOUNDBITE OF ARCHIVED RECORDING)DEL VALLE: Do you think you're going to walk inside my house with your feet like that? Let me tell you something, Mr. Demogorgon. I'm going to need you to go wash those little feet of yours because you are not going to come inside my house all dirty. I'm not doing that. BOND: That video blew up on TikTok, and Del Valle now has more than 330,000 followers. She says it's really changed her life. She has fans now. She's actually been stopped in the street by people asking for pictures. CORNISH: So what you're describing are, like, skits. I know it's a big place for dance routines, and some pop songs have even been launched there. It all seems innocuous. Why is Congress raising the alarm about TikTok? BOND: Well, what's gotten the attention of lawmakers is who owns TikTok. And that's a Chinese company called ByteDance. Actually, the head of TikTok was supposed to come meet with lawmakers in Washington to talk about some of these concerns this week. He's now rescheduled those meetings for after the holidays. But when he does come, he's probably going to get questions about whether, because of its Chinese ownership, TikTok is censoring content that might upset the Chinese government. Just last month, the app banned an American user who made a video criticizing China's treatment of Uighur Muslims. TikTok did apologize and reinstated the video soon after. But there are worries about how any Chinese-owned company handles the data of American users because of censorship and surveillance by the Chinese government. Here's Republican Senator Josh Hawley of Missouri. He was talking about TikTok at a hearing in November. (SOUNDBITE OF ARCHIVED RECORDING)JOSH HAWLEY: A company compromised by the Chinese Communist Party knows where your children are, knows what they look like, what their voices sound like, what they're watching and what they share with each other. BOND: TikTok says it stores all American user data in the U. S. and Singapore, not in China. And I'm not sure that many TikTok users in the U. S. are particularly worried about this yet, but political pressure on the company is only going to rise. There's reportedly a national security review of ByteDance's ownership of TikTok. CORNISH: Are Silicon Valley leaders worried about the rise of TikTok? What are you hearing from companies there? BOND: Yeah, the most alarm out here is coming from Facebook. CEO Mark Zuckerberg has really seized on TikTok as a foil. So he touts Facebook as an American company with American values versus TikTok as a company that reflects China's interests. Here's what he said about that in October. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: While our services like WhatsApp are used by protesters and activists everywhere due to strong encryption and privacy protections, on TikTok, the Chinese app growing quickly around the world, mentions of these same protests are censored, even here in the U. S. Is that the Internet that we want? BOND: But, you know, the interesting thing, Audie, is that Facebook is actually trying to copy TikTok. It's testing out these tools that allow people to make TikTok-like videos. It's hoping to capture its popularity, especially with young people. But the bigger picture, I think, here is that there've been a lot of successful Chinese social media companies, but they've all been in China. TikTok's the first Chinese social app that has broken out globally, and that scares Facebook. And it scares the U. S. government. CORNISH: That's NPR's Shannon Bond. Shannon, thank you. BOND: Thanks for having me. (SOUNDBITE OF KHRUANGBIN SONG, \"PEOPLE EVERYWHERE (STILL ALIVE)\") AUDIE CORNISH, HOST:  Now time for All Tech Considered. (SOUNDBITE OF MUSIC) CORNISH: And when it comes to tech, 2019 may go down in history as the year of TikTok. In 2019, the app has spawned memes, dance contests, even minted hit songs. (SOUNDBITE OF SONG, \"THE GIT UP\") BLANCO BROWN: (Singing) Going to do the two step then cowboy boogie. Grab your sweetheart and spin out with them. (SOUNDBITE OF SONG, \"PROM DRESS\") MXMTOON: (Singing) I'm sitting here, crying in my prom dress. I'd be the prom queen if crying was a contest. (SOUNDBITE OF SONG, \"OLD TOWN ROAD\") LIL NAS X: (Singing) Riding on a horse, you can whip your Porsche. CORNISH: It also set off alarm bells in Washington and Silicon Valley. NPR's tech correspondent Shannon Bond joins us now to talk about how the video sharing app went from teen fad to cultural juggernaut to possible national security threat. Shannon, welcome back. SHANNON BOND, BYLINE: Thanks for having me. CORNISH: How big is TikTok at this point? How many users? BOND: Well, it's kind of crazy. It's not even a year and a half old, and it's been downloaded 1. 5 billion times. So that means something like 1 in 5 people around the world have put this app on their phones. And teenagers and 20-somethings in particular have embraced it. I spoke with a TikToker - that's what they're called. Her name is Alexia Del Valle. ALEXIA DEL VALLE: It's, like, really exciting to know that people are enjoying something that I thought of, you know, yesterday, and I just readily made a video about. BOND: Del Valle is a 21-year-old student. She goes to college in Miami. And she makes these videos where she plays different characters. This summer, she made a parody of the Netflix show \"Stranger Things. \" It's called \"If Latina Moms Were In Stranger Things. \" Del Valle's Puerto Rican, and she drew on that for the video. (SOUNDBITE OF ARCHIVED RECORDING) DEL VALLE: Do you think you're going to walk inside my house with your feet like that? Let me tell you something, Mr. Demogorgon. I'm going to need you to go wash those little feet of yours because you are not going to come inside my house all dirty. I'm not doing that. BOND: That video blew up on TikTok, and Del Valle now has more than 330,000 followers. She says it's really changed her life. She has fans now. She's actually been stopped in the street by people asking for pictures. CORNISH: So what you're describing are, like, skits. I know it's a big place for dance routines, and some pop songs have even been launched there. It all seems innocuous. Why is Congress raising the alarm about TikTok? BOND: Well, what's gotten the attention of lawmakers is who owns TikTok. And that's a Chinese company called ByteDance. Actually, the head of TikTok was supposed to come meet with lawmakers in Washington to talk about some of these concerns this week. He's now rescheduled those meetings for after the holidays. But when he does come, he's probably going to get questions about whether, because of its Chinese ownership, TikTok is censoring content that might upset the Chinese government. Just last month, the app banned an American user who made a video criticizing China's treatment of Uighur Muslims. TikTok did apologize and reinstated the video soon after. But there are worries about how any Chinese-owned company handles the data of American users because of censorship and surveillance by the Chinese government. Here's Republican Senator Josh Hawley of Missouri. He was talking about TikTok at a hearing in November. (SOUNDBITE OF ARCHIVED RECORDING) JOSH HAWLEY: A company compromised by the Chinese Communist Party knows where your children are, knows what they look like, what their voices sound like, what they're watching and what they share with each other. BOND: TikTok says it stores all American user data in the U. S. and Singapore, not in China. And I'm not sure that many TikTok users in the U. S. are particularly worried about this yet, but political pressure on the company is only going to rise. There's reportedly a national security review of ByteDance's ownership of TikTok. CORNISH: Are Silicon Valley leaders worried about the rise of TikTok? What are you hearing from companies there? BOND: Yeah, the most alarm out here is coming from Facebook. CEO Mark Zuckerberg has really seized on TikTok as a foil. So he touts Facebook as an American company with American values versus TikTok as a company that reflects China's interests. Here's what he said about that in October. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: While our services like WhatsApp are used by protesters and activists everywhere due to strong encryption and privacy protections, on TikTok, the Chinese app growing quickly around the world, mentions of these same protests are censored, even here in the U. S. Is that the Internet that we want? BOND: But, you know, the interesting thing, Audie, is that Facebook is actually trying to copy TikTok. It's testing out these tools that allow people to make TikTok-like videos. It's hoping to capture its popularity, especially with young people. But the bigger picture, I think, here is that there've been a lot of successful Chinese social media companies, but they've all been in China. TikTok's the first Chinese social app that has broken out globally, and that scares Facebook. And it scares the U. S. government. CORNISH: That's NPR's Shannon Bond. Shannon, thank you. BOND: Thanks for having me. (SOUNDBITE OF KHRUANGBIN SONG, \"PEOPLE EVERYWHERE (STILL ALIVE)\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-11-787165948": {"title": "YouTube Announces New Anti-Harassment Policy To Fight Racial, Gender, LGBTQ Abuse : NPR", "url": "https://www.npr.org/2019/12/11/787165948/youtube-announces-new-anti-harassment-policy-to-fight-racial-gender-lgbtq-abuse", "author": "No author found", "published_date": "2019-12-11", "content": "", "section": "Technology", "disclaimer": ""}, "2019-12-12-787646809": {"title": "FAA Chief Pushes Back On Boeing Pressure To Return 737 Max Jets To Service : NPR", "url": "https://www.npr.org/2019/12/12/787646809/faa-chief-pushes-back-on-boeing-pressure-to-return-737-max-jets-to-service", "author": "No author found", "published_date": "2019-12-12", "content": "", "section": "Business", "disclaimer": ""}, "2019-12-12-787441031": {"title": "Universal Product Code Designer George Laurer Dies At 94 : NPR", "url": "https://www.npr.org/2019/12/12/787441031/ibm-engineer-who-designed-the-universal-bar-code-dies-at-94", "author": "No author found", "published_date": "2019-12-12", "content": "", "section": "Technology", "disclaimer": ""}, "2019-12-15-788261168": {"title": "Troll Watch: How To Be An Online Troll : NPR", "url": "https://www.npr.org/2019/12/15/788261168/troll-watch-how-to-be-an-online-troll", "author": "No author found", "published_date": "2019-12-15", "content": "MICHEL MARTIN, HOST: Social media has become a minefield of false news stories. Media outlets and tech companies have been struggling to give people tools to separate fact from fiction. But a couple of people have taken the issue into their own hands by developing games that show how fake news spreads as a way to inoculate people against it. We'll hear more about that as part of our regular segment called Troll Watch. (SOUNDBITE OF MUSIC)MARTIN: NPR's tech correspondent Shannon Bond tried out some of the games and has this report. SHANNON BOND, BYLINE: Internet trolls exploit our emotions and gin up outrage with fake stories. But instead of just telling you that, what if I showed you how trolling works? I'm playing a game where the goal is to spread disinformation. My instructions arrived by text message from a mysterious contact only identified as Boss. JARNO KOPONEN: You become an Internet troll that uses malicious memes, fake news, targeted advertising, botnets and conspiracy theories to spread fear, hate and distrust. BOND: Jarno Koponen is the brains behind the web-based game called Troll Factory. He's head of AI and personalization at YLE, Finland's public broadcaster. We spoke on Skype. Troll Factory is sort of a choose-your-own adventure. You win by becoming the most malicious online troll. It's a simulated smartphone screen. At the top, a scoreboard counts my imaginary followers and racks up how many times my posts are shared. KOPONEN: The better you succeed, the more influential you become in the game. So it's very literal. BOND: My task is to whip up anger against immigrants. I'm not actually putting hateful content on the Internet, of course. But the posts and images I'm choosing from in the game are real examples from social media. I'm cautious at first, choosing posts that are provocative but not the most offensive. But I only gain a handful of followers. My boss in the game urges me to do better. So when I'm asked to exploit a real-life tragedy, I pick the fire at Notre Dame Cathedral in Paris. Here's my NPR colleague Eleanor Beardsley with a real report on the devastating blaze back in April. (SOUNDBITE OF ARCHIVED NPR BROADCAST)ELEANOR BEARDSLEY: Although Notre Dame is a Catholic cathedral, Parisians of all faiths and backgrounds embraced it. Taxi driver Bashir Arbouuli brought me to Notre Dame this morning. This Muslim, who's originally from Tunisia, says he can hardly bear to look at Notre Dame today. BOND: But in the game, the fake story I post claims that Muslims celebrate it as the church burned. Of course, that's not true. But I realized that the more I dial up the outrage, the more successful I become as a troll. The fake story really gets things going. My score surges. When I finish the game, I've hit 1,500 followers. They shared my post 14,000 times. I win the title director of disorder. But it makes me wonder - isn't it wrong to expose people to such hateful material, even if it's just a game? Koponen says that showing people how fake news spreads can make them more skeptical in real life. And there's evidence supporting that. SANDER VAN DER LINDEN: Maybe when you inject people with a small dose, a weakened dose of the fake news, people can build up cognitive or mental antibodies and become more resistant to it. BOND: That's Sander van der Linden. He's a psychology professor at the University of Cambridge in the U. K. We also spoke on Skype. As a research project, he and his colleagues made their own game called Bad News. Here, too, players post conspiracy theories and memes to gain followers. More than a million people have played the game, and the researchers found it helped. Van der Linden says that after players use the tools themselves, they were more likely to say fake news was not reliable. VAN DER LINDEN: So that, then, when we go out in testing phase and expose people to, you know, a range of fake news, they're going to be inoculated because they can spot the techniques embedded in fake news articles. BOND: Fake news and trolling are proving hard to stamp out entirely from social media. These experimental gains suggests that increasing our own resistance could be a better solution. Shannon Bond, NPR News, San Francisco. MICHEL MARTIN, HOST:  Social media has become a minefield of false news stories. Media outlets and tech companies have been struggling to give people tools to separate fact from fiction. But a couple of people have taken the issue into their own hands by developing games that show how fake news spreads as a way to inoculate people against it. We'll hear more about that as part of our regular segment called Troll Watch. (SOUNDBITE OF MUSIC) MARTIN: NPR's tech correspondent Shannon Bond tried out some of the games and has this report. SHANNON BOND, BYLINE: Internet trolls exploit our emotions and gin up outrage with fake stories. But instead of just telling you that, what if I showed you how trolling works? I'm playing a game where the goal is to spread disinformation. My instructions arrived by text message from a mysterious contact only identified as Boss. JARNO KOPONEN: You become an Internet troll that uses malicious memes, fake news, targeted advertising, botnets and conspiracy theories to spread fear, hate and distrust. BOND: Jarno Koponen is the brains behind the web-based game called Troll Factory. He's head of AI and personalization at YLE, Finland's public broadcaster. We spoke on Skype. Troll Factory is sort of a choose-your-own adventure. You win by becoming the most malicious online troll. It's a simulated smartphone screen. At the top, a scoreboard counts my imaginary followers and racks up how many times my posts are shared. KOPONEN: The better you succeed, the more influential you become in the game. So it's very literal. BOND: My task is to whip up anger against immigrants. I'm not actually putting hateful content on the Internet, of course. But the posts and images I'm choosing from in the game are real examples from social media. I'm cautious at first, choosing posts that are provocative but not the most offensive. But I only gain a handful of followers. My boss in the game urges me to do better. So when I'm asked to exploit a real-life tragedy, I pick the fire at Notre Dame Cathedral in Paris. Here's my NPR colleague Eleanor Beardsley with a real report on the devastating blaze back in April. (SOUNDBITE OF ARCHIVED NPR BROADCAST) ELEANOR BEARDSLEY: Although Notre Dame is a Catholic cathedral, Parisians of all faiths and backgrounds embraced it. Taxi driver Bashir Arbouuli brought me to Notre Dame this morning. This Muslim, who's originally from Tunisia, says he can hardly bear to look at Notre Dame today. BOND: But in the game, the fake story I post claims that Muslims celebrate it as the church burned. Of course, that's not true. But I realized that the more I dial up the outrage, the more successful I become as a troll. The fake story really gets things going. My score surges. When I finish the game, I've hit 1,500 followers. They shared my post 14,000 times. I win the title director of disorder. But it makes me wonder - isn't it wrong to expose people to such hateful material, even if it's just a game? Koponen says that showing people how fake news spreads can make them more skeptical in real life. And there's evidence supporting that. SANDER VAN DER LINDEN: Maybe when you inject people with a small dose, a weakened dose of the fake news, people can build up cognitive or mental antibodies and become more resistant to it. BOND: That's Sander van der Linden. He's a psychology professor at the University of Cambridge in the U. K. We also spoke on Skype. As a research project, he and his colleagues made their own game called Bad News. Here, too, players post conspiracy theories and memes to gain followers. More than a million people have played the game, and the researchers found it helped. Van der Linden says that after players use the tools themselves, they were more likely to say fake news was not reliable. VAN DER LINDEN: So that, then, when we go out in testing phase and expose people to, you know, a range of fake news, they're going to be inoculated because they can spot the techniques embedded in fake news articles. BOND: Fake news and trolling are proving hard to stamp out entirely from social media. These experimental gains suggests that increasing our own resistance could be a better solution. Shannon Bond, NPR News, San Francisco.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-15-788195185": {"title": "How The Team Behind 'The Irishman' Made Actors Look Decades Younger : NPR", "url": "https://www.npr.org/2019/12/15/788195185/how-the-team-behind-the-irishman-made-actors-look-decades-younger", "author": "No author found", "published_date": "2019-12-15", "content": "LULU GARCIA-NAVARRO, HOST: In Martin Scorsese's new film \"The Irishman,\" we are first introduced to Robert De Niro as an old man in a nursing home. But just a few minutes later, he's dropped a good 30 to 40 years, alongside actor Joe Pesci, when they meet up over a broken-down truck at a gas station. (SOUNDBITE OF FILM, \"THE IRISHMAN\")JOE PESCI: (As Russell Bufalino) What's the problem, kid? ROBERT DE NIRO: (As Frank Sheeran) I don't know. It sounds funny. It stops and starts and loses power. PESCI: (As Russell Bufalino) Let me see if I can give you a hand. GARCIA-NAVARRO: It's this latter transformation we want to talk about today of De Niro and Pesci and Al Pacino, too, who plays murdered Teamster boss Jimmy Hoffa in \"The Irishman. \" New technology has developed to de-age these septuagenarian actors on-screen. And we're joined now by the man who headed the team that did it. Pablo Helman is an Oscar-nominated visual effects supervisor at Industrial Light and Magic, the studio founded by George Lucas. And he joins us now from member station KQED in San Francisco. Hi, there. PABLO HELMAN: Hi. How are you? GARCIA-NAVARRO: I'm great. This is an extraordinary bit of technology. And, you know, we've gotten so used to seeing actors made younger on-screen, but with makeup and CGI. Can you explain for the layperson what you did? HELMAN: Main reason - the main difference is that we did not use any markers on the actors' face. GARCIA-NAVARRO: Those are, like, the CGI green dots - right? - the markers. HELMAN: Yeah, yeah, something like that. And that was explicit request from Bob De Niro and Marty Scorsese. So he emailed me the script overnight. And overnight, I read it because when Marty sends you a script, you read it right away. GARCIA-NAVARRO: Yeah, you read it right away. HELMAN: Right away. GARCIA-NAVARRO: You don't just leave it in your inbox (laughter). HELMAN: No. And so I read it overnight. And in the morning, I said, you know, I'm in. It was \"'The Irishman. \" And he said, you know, be careful what you wish for because Bob De Niro is not going to want to wear any markers on his face, helmet cam with little cameras in front of him. And. . . GARCIA-NAVARRO: And this is how usually this is done - right? - with CGI. HELMAN: Right. Yeah. If you think about four years ago, everybody was doing that and - including myself. GARCIA-NAVARRO: And the way it works is with cameras. HELMAN: Yes. We had to develop the software, first of all. If you don't have markers in your face, then we need to work with whatever is left. And whatever is left is basically lighting and textures of the actor who is in front of the camera. Now, to do that, we came up with a rig that has three cameras. And then the software takes a look at all that information and makes sense out of that. And it creates a 3D feature of whatever is in front of them. GARCIA-NAVARRO: I mean, there are limits to the technology, though, right? The younger Robert De Niro of \"The Irishman\" does not look like the younger De Niro of, say, \"Goodfellas. \" His face is wider. HELMAN: Right. Well, I think that it has a lot to do with the design of the characters. You see, Marty Scorsese didn't want to rewind 30 years and see Jimmy Conway from \"Goodfellas,\" right? He wanted to design a character that was a younger version of Frank Sheeran. And when you see him first on the screen, you don't say, oh, wow, he doesn't look like \"Deer Hunter\" - you know what I mean. . . GARCIA-NAVARRO: Right. HELMAN: . . . Or \"Taxi Driver. \" He looks like the character that he's being portray, Frank Sheeran. GARCIA-NAVARRO: Obviously, when you're trying a new technology and you're doing it on such a marquee film, I mean, it must've been pretty nerve-wracking. I mean, were you sure it was going to work? HELMAN: Yes, I. . . GARCIA-NAVARRO: (Laughter) Yes. HELMAN: I kind of did. Well, the thing about it is I've been in Ireland for 24 years and been doing visual effects for 30. And I remember talking to - I don't know if you know Dennis Muren, but he has eight Oscars in visual effects. And his office is right next to mine in Ireland. And he's been a great mentor. And so I brought the script to him. And I said, look at what we got, you know, just making Robert De Niro and Al Pacino and Joe Pesci, you know, 30 years younger and for 3 1/2 hours. And he says, don't do it. GARCIA-NAVARRO: (Laughter). HELMAN: And so I say, well, do you remember when you did \"Jurassic Park? \" And then he just got quiet and said, you're right; we should do this. GARCIA-NAVARRO: (Laughter) What was the hardest thing? HELMAN: Well, I mean, we knew that the whole purpose of doing this was to get the technology away from the actors - natural thing, no markers. If you don't have markers, then every one of your pores and your imperfections and your, you know, skin becomes a marker. So now all of a sudden, you have thousands of markers. So now you start getting things that you couldn't get before, like mouth - every time we, say, finding a consonant, everything reverberates. And you can't really tell what it is. But if you don't have it, you do feel it. The differences between, you know, a smile and a wince is very, very small. And how Bob De Niro goes from a smile into the frown is what makes him who he is. So the behavior or likeness of him has to be caught by the computer, or else he doesn't look like himself. GARCIA-NAVARRO: And what you're describing is a technology that's only going to get better and better, which I think brings up some ethical issues because as the technology gets more seamless and commonplace and those likenesses that you've just described get more subtle, could we end up doing away with the actual actor altogether? I mean, could it come to a point where a studio owns the digital image of an actor and just uses that instead of the real thing? HELMAN: I don't think so 'cause the performance has to come from somewhere, and that has to be the actor. And so just think about what it'll take for a computer to do what Robert De Niro does. You need to train the computer - right? - to do those kinds of things. And basically, if you think about the behavior or likeness of somebody, how do you become yourself? You become yourself by living, you know, by having a bunch of experiences. And then you also have all the connections that are made in your face, the way you smile, all the cultural things that you live. So if you want a computer to act like Robert De Niro, you need to train the computer like Robert De Niro. And then you spend a lifetime, you know, basically training the computer. And for that, you might as well just use Robert De Niro, you know? GARCIA-NAVARRO: But will now Robert De Niro would be living forever? - because I also read a criticism that said if actors are made forever young, it just keeps the studios using their bankable stars over and over again and perhaps sort of impeding the way for the next Robert De Niro. HELMAN: I think you have to be careful what you could do because just because you could, that doesn't mean that you should do it, number one. The other thing about it is that you have to be careful about why you're doing what you're doing and whether you're servicing the story or not. And now I understand that the actors need this truth, you know, that they're all looking for. And I'm more than willing to give it to them because - you know why? Because that means that the performances are going to be great. GARCIA-NAVARRO: Visual effects artist Pablo Helman, thank you so much. HELMAN: Thank you, Lulu. (SOUNDBITE OF MUSIC) LULU GARCIA-NAVARRO, HOST:  In Martin Scorsese's new film \"The Irishman,\" we are first introduced to Robert De Niro as an old man in a nursing home. But just a few minutes later, he's dropped a good 30 to 40 years, alongside actor Joe Pesci, when they meet up over a broken-down truck at a gas station. (SOUNDBITE OF FILM, \"THE IRISHMAN\") JOE PESCI: (As Russell Bufalino) What's the problem, kid? ROBERT DE NIRO: (As Frank Sheeran) I don't know. It sounds funny. It stops and starts and loses power. PESCI: (As Russell Bufalino) Let me see if I can give you a hand. GARCIA-NAVARRO: It's this latter transformation we want to talk about today of De Niro and Pesci and Al Pacino, too, who plays murdered Teamster boss Jimmy Hoffa in \"The Irishman. \" New technology has developed to de-age these septuagenarian actors on-screen. And we're joined now by the man who headed the team that did it. Pablo Helman is an Oscar-nominated visual effects supervisor at Industrial Light and Magic, the studio founded by George Lucas. And he joins us now from member station KQED in San Francisco. Hi, there. PABLO HELMAN: Hi. How are you? GARCIA-NAVARRO: I'm great. This is an extraordinary bit of technology. And, you know, we've gotten so used to seeing actors made younger on-screen, but with makeup and CGI. Can you explain for the layperson what you did? HELMAN: Main reason - the main difference is that we did not use any markers on the actors' face. GARCIA-NAVARRO: Those are, like, the CGI green dots - right? - the markers. HELMAN: Yeah, yeah, something like that. And that was explicit request from Bob De Niro and Marty Scorsese. So he emailed me the script overnight. And overnight, I read it because when Marty sends you a script, you read it right away. GARCIA-NAVARRO: Yeah, you read it right away. HELMAN: Right away. GARCIA-NAVARRO: You don't just leave it in your inbox (laughter). HELMAN: No. And so I read it overnight. And in the morning, I said, you know, I'm in. It was \"'The Irishman. \" And he said, you know, be careful what you wish for because Bob De Niro is not going to want to wear any markers on his face, helmet cam with little cameras in front of him. And. . . GARCIA-NAVARRO: And this is how usually this is done - right? - with CGI. HELMAN: Right. Yeah. If you think about four years ago, everybody was doing that and - including myself. GARCIA-NAVARRO: And the way it works is with cameras. HELMAN: Yes. We had to develop the software, first of all. If you don't have markers in your face, then we need to work with whatever is left. And whatever is left is basically lighting and textures of the actor who is in front of the camera. Now, to do that, we came up with a rig that has three cameras. And then the software takes a look at all that information and makes sense out of that. And it creates a 3D feature of whatever is in front of them. GARCIA-NAVARRO: I mean, there are limits to the technology, though, right? The younger Robert De Niro of \"The Irishman\" does not look like the younger De Niro of, say, \"Goodfellas. \" His face is wider. HELMAN: Right. Well, I think that it has a lot to do with the design of the characters. You see, Marty Scorsese didn't want to rewind 30 years and see Jimmy Conway from \"Goodfellas,\" right? He wanted to design a character that was a younger version of Frank Sheeran. And when you see him first on the screen, you don't say, oh, wow, he doesn't look like \"Deer Hunter\" - you know what I mean. . . GARCIA-NAVARRO: Right. HELMAN: . . . Or \"Taxi Driver. \" He looks like the character that he's being portray, Frank Sheeran. GARCIA-NAVARRO: Obviously, when you're trying a new technology and you're doing it on such a marquee film, I mean, it must've been pretty nerve-wracking. I mean, were you sure it was going to work? HELMAN: Yes, I. . . GARCIA-NAVARRO: (Laughter) Yes. HELMAN: I kind of did. Well, the thing about it is I've been in Ireland for 24 years and been doing visual effects for 30. And I remember talking to - I don't know if you know Dennis Muren, but he has eight Oscars in visual effects. And his office is right next to mine in Ireland. And he's been a great mentor. And so I brought the script to him. And I said, look at what we got, you know, just making Robert De Niro and Al Pacino and Joe Pesci, you know, 30 years younger and for 3 1/2 hours. And he says, don't do it. GARCIA-NAVARRO: (Laughter). HELMAN: And so I say, well, do you remember when you did \"Jurassic Park? \" And then he just got quiet and said, you're right; we should do this. GARCIA-NAVARRO: (Laughter) What was the hardest thing? HELMAN: Well, I mean, we knew that the whole purpose of doing this was to get the technology away from the actors - natural thing, no markers. If you don't have markers, then every one of your pores and your imperfections and your, you know, skin becomes a marker. So now all of a sudden, you have thousands of markers. So now you start getting things that you couldn't get before, like mouth - every time we, say, finding a consonant, everything reverberates. And you can't really tell what it is. But if you don't have it, you do feel it. The differences between, you know, a smile and a wince is very, very small. And how Bob De Niro goes from a smile into the frown is what makes him who he is. So the behavior or likeness of him has to be caught by the computer, or else he doesn't look like himself. GARCIA-NAVARRO: And what you're describing is a technology that's only going to get better and better, which I think brings up some ethical issues because as the technology gets more seamless and commonplace and those likenesses that you've just described get more subtle, could we end up doing away with the actual actor altogether? I mean, could it come to a point where a studio owns the digital image of an actor and just uses that instead of the real thing? HELMAN: I don't think so 'cause the performance has to come from somewhere, and that has to be the actor. And so just think about what it'll take for a computer to do what Robert De Niro does. You need to train the computer - right? - to do those kinds of things. And basically, if you think about the behavior or likeness of somebody, how do you become yourself? You become yourself by living, you know, by having a bunch of experiences. And then you also have all the connections that are made in your face, the way you smile, all the cultural things that you live. So if you want a computer to act like Robert De Niro, you need to train the computer like Robert De Niro. And then you spend a lifetime, you know, basically training the computer. And for that, you might as well just use Robert De Niro, you know? GARCIA-NAVARRO: But will now Robert De Niro would be living forever? - because I also read a criticism that said if actors are made forever young, it just keeps the studios using their bankable stars over and over again and perhaps sort of impeding the way for the next Robert De Niro. HELMAN: I think you have to be careful what you could do because just because you could, that doesn't mean that you should do it, number one. The other thing about it is that you have to be careful about why you're doing what you're doing and whether you're servicing the story or not. And now I understand that the actors need this truth, you know, that they're all looking for. And I'm more than willing to give it to them because - you know why? Because that means that the performances are going to be great. GARCIA-NAVARRO: Visual effects artist Pablo Helman, thank you so much. HELMAN: Thank you, Lulu. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-16-788490509": {"title": "Congress Allocates $425 Million Toward Election Security, But Questions Remain : NPR", "url": "https://www.npr.org/2019/12/16/788490509/congress-allocates-425-million-for-election-security-in-new-legislation", "author": "No author found", "published_date": "2019-12-16", "content": "", "section": "Politics", "disclaimer": ""}, "2019-12-16-788597818": {"title": "How China Is Using Facial Recognition Technology : NPR", "url": "https://www.npr.org/2019/12/16/788597818/how-china-is-using-facial-recognition-technology", "author": "No author found", "published_date": "2019-12-16", "content": "ARI SHAPIRO, HOST: Facial recognition technology has been around for a while, but this year it became a facet of life in China. We take a look in All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")SHAPIRO: China has built a vast network of cameras all across the country that enable facial recognition technology. To find out just how common it's become and whether it works, NPR's Emily Feng took a walk around her neighborhood in Beijing. EMILY FENG, BYLINE: So right now, Amy Cheng, NPR's producer here in Beijing, and I are walking down just another street in Beijing. We normally wouldn't be out. It's hovering right above freezing. The smog is pretty choking today. But we're on the lookout for facial recognition cameras, and we've just spotted what looks like one near a parking lot. (SOUNDBITE OF CAR HORNS HONKING)FENG: I think I see it. Do you see that white bulbous thing hanging off the pole? China has some 200 million cameras by its own estimate, but we're looking for a very specific set of facial recognition cameras that feed into a surveillance dataset on hundreds of people in Beijing discovered this spring. JOHN WETHINGTON: You can just call me a security researcher at Condition:Black. That's our organization. And my name is John Wethington. FENG: Wethington was on duty the night his firm found the dataset. WETHINGTON: And as we started digging deeper into it, we realized people were effectively being watched. We had latitude and longitude for cameras. We had IP addresses. We had identity cards. FENG: The data came from hundreds, if not thousands, of cameras across Beijing, some of which Amy and I were now tracking down. WETHINGTON: And they were doing comparisons of facial recognition data that they were picking up off of these cameras with facial recognition data that had been collected by police surveillance. FENG: In the dataset Wethington found, people were indexed by information, like their criminal history, with facial recognition data, like if they were bearded or wearing a mask, and even what ethnicity they were, Han, the ethnic majority here in China, or Uighur, a predominantly Muslim ethnic minority China has detained by the hundreds of thousands in the region of Xinjiang in the name of anti-terrorism. Xinjiang has seen a rapid buildup in facial recognition technologies in cameras, all part of an effort to monitor millions of ethnic minorities there in the name of national security. Those methods are now becoming mainstream across China, including cities like Beijing. STEVEN FELDSTEIN: Oh, I think China is absolutely unique. FENG: Steven Feldstein is an associate professor at Boise State University and a fellow at the Carnegie Endowment. FELDSTEIN: I think it's at the cutting edge of not only integrating new forms of AI surveillance capabilities, but tying those into a very repressive police state. FENG: Feldstein has created an index that measures the degree to which countries use artificial technology surveillance, including facial recognition, and where that technology comes from. FELDSTEIN: It's really proliferating around the world in all regions. And it also - I mean, it goes to autocracies, hybrid regimes, democracies. So it really is something that is spreading pretty evenly around the world. FENG: But while the accuracy of facial recognition is quickly rising, it's still dependent on things like weather conditions and lighting. China has also shown itself willing to surveil broad categories of people. In Xinjiang, for example, where an estimated 1 million or more Uighurs and other minorities have been detained at some point, facial recognition has not been used to precisely pinpoint Muslims. It's used as a blunt instrument of intimidation. Back on the streets of Beijing, I ask Amy, a native Beijinger and NPR's producer, do you ever have a feeling that you're being watched? AMY CHENG, BYLINE: I think surveillance cameras are those things that you start noticing them, you can't un-notice them. FENG: So far, the feeling of being surveilled constantly seems to be more of a deterrent than the facial recognition technology itself. Emily Feng, NPR News, Beijing. (SOUNDBITE OF MUSIC) ARI SHAPIRO, HOST:  Facial recognition technology has been around for a while, but this year it became a facet of life in China. We take a look in All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") SHAPIRO: China has built a vast network of cameras all across the country that enable facial recognition technology. To find out just how common it's become and whether it works, NPR's Emily Feng took a walk around her neighborhood in Beijing. EMILY FENG, BYLINE: So right now, Amy Cheng, NPR's producer here in Beijing, and I are walking down just another street in Beijing. We normally wouldn't be out. It's hovering right above freezing. The smog is pretty choking today. But we're on the lookout for facial recognition cameras, and we've just spotted what looks like one near a parking lot. (SOUNDBITE OF CAR HORNS HONKING) FENG: I think I see it. Do you see that white bulbous thing hanging off the pole? China has some 200 million cameras by its own estimate, but we're looking for a very specific set of facial recognition cameras that feed into a surveillance dataset on hundreds of people in Beijing discovered this spring. JOHN WETHINGTON: You can just call me a security researcher at Condition:Black. That's our organization. And my name is John Wethington. FENG: Wethington was on duty the night his firm found the dataset. WETHINGTON: And as we started digging deeper into it, we realized people were effectively being watched. We had latitude and longitude for cameras. We had IP addresses. We had identity cards. FENG: The data came from hundreds, if not thousands, of cameras across Beijing, some of which Amy and I were now tracking down. WETHINGTON: And they were doing comparisons of facial recognition data that they were picking up off of these cameras with facial recognition data that had been collected by police surveillance. FENG: In the dataset Wethington found, people were indexed by information, like their criminal history, with facial recognition data, like if they were bearded or wearing a mask, and even what ethnicity they were, Han, the ethnic majority here in China, or Uighur, a predominantly Muslim ethnic minority China has detained by the hundreds of thousands in the region of Xinjiang in the name of anti-terrorism. Xinjiang has seen a rapid buildup in facial recognition technologies in cameras, all part of an effort to monitor millions of ethnic minorities there in the name of national security. Those methods are now becoming mainstream across China, including cities like Beijing. STEVEN FELDSTEIN: Oh, I think China is absolutely unique. FENG: Steven Feldstein is an associate professor at Boise State University and a fellow at the Carnegie Endowment. FELDSTEIN: I think it's at the cutting edge of not only integrating new forms of AI surveillance capabilities, but tying those into a very repressive police state. FENG: Feldstein has created an index that measures the degree to which countries use artificial technology surveillance, including facial recognition, and where that technology comes from. FELDSTEIN: It's really proliferating around the world in all regions. And it also - I mean, it goes to autocracies, hybrid regimes, democracies. So it really is something that is spreading pretty evenly around the world. FENG: But while the accuracy of facial recognition is quickly rising, it's still dependent on things like weather conditions and lighting. China has also shown itself willing to surveil broad categories of people. In Xinjiang, for example, where an estimated 1 million or more Uighurs and other minorities have been detained at some point, facial recognition has not been used to precisely pinpoint Muslims. It's used as a blunt instrument of intimidation. Back on the streets of Beijing, I ask Amy, a native Beijinger and NPR's producer, do you ever have a feeling that you're being watched? AMY CHENG, BYLINE: I think surveillance cameras are those things that you start noticing them, you can't un-notice them. FENG: So far, the feeling of being surveilled constantly seems to be more of a deterrent than the facial recognition technology itself. Emily Feng, NPR News, Beijing. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-17-787486571": {"title": "Lizzo, Taylor Swift, And Life Tips: The 20 Top Stories On NPR In 2019 : NPR", "url": "https://www.npr.org/2019/12/17/787486571/the-20-top-stories-on-npr-in-2019", "author": "No author found", "published_date": "2019-12-17", "content": "", "section": "National", "disclaimer": ""}, "2019-12-17-788222302": {"title": "'Wattam' Is Keita Takahashi's Latest Video Game Of Radical Joy : NPR", "url": "https://www.npr.org/2019/12/17/788222302/meet-wattam-the-newest-absurd-video-game-playground-from-keita-takahashi", "author": "No author found", "published_date": "2019-12-17", "content": "STEVE INSKEEP, HOST:  The point of most video games is to defeat your competition. But a new game out today called Wattam has a different intention, aiming to bring people together. It's the creation of a renowned video game designer, Keita Takahashi. And we have a report this morning from NPR's Vincent Acovino. VINCENT ACOVINO, BYLINE: The year 2004 was not unusual for video games. The bestselling games of that year were full of familiar titles like Grand Theft Auto, Madden and Pokemon. But one game that year was unlike any other. (SOUNDBITE OF SONG, \"KATAMARI ON THE ROCKS\")UNIDENTIFIED SINGER: (Vocalizing). ACOVINO: That game was Katamari Damacy. It's about a god named the King of All Cosmos who, while drunk, accidentally destroys the stars in the sky. His son, The Prince, is left to clean up the mess by rolling up objects on Earth into big, sticky balls that grow so large they become stars. Robin Hunicke is the co-founder of Funomena, the studio that developed Wattam. But she's long been a fan of Takahashi's work. Back in 2004, she helped exhibit Katamari at the Game Developers Conference in San Francisco for a workshop on experimental games. Keita Takahashi gave a presentation that blew her away. ROBIN HUNICKE: He showed the gameplay. But then he went all the way to the credit screen where you can roll up all the countries on Earth. And so you're making a katamari of all of us. And Keita said, in a very heartfelt way, that he had been really destroyed by the idea that we were going to just keep fighting and killing each other. ACOVINO: Katamari Damacy was a surprise commercial hit in North America. It was also a critical success and is one of 14 video games that helped establish a new category of art in the Museum of Modern Art's permanent collection. Keita Takahashi's games are funny and absurd at the surface but kind and earnest at their core. Although Takahashi originally studied to be a sculptor, he became upset when his classmates were throwing out their work after getting grades on their projects. KEITA TAKAHASHI: I was very shocked about that. We have environment crisis. I don't want make more garbage. ACOVINO: Takahashi aimed to make work that was useful and made people laugh. It's an ethos that has carried over to all of his projects, including his latest game, Wattam. HARRY DELORME: It's full of very strange things - a character that's a mouth that eats other characters and excretes them as poop. ACOVINO: That's Harry DeLorme, who helped curate a mid-career survey of Takahashi's work at the Telfair Museums in Savannah, Ga. He says that Wattam is funny, but it's also a game about joy and kindness. (SOUNDBITE OF MUSIC)DELORME: It starts off with a character who is utterly alone. ACOVINO: A green cube with a mouth, a mustache and a top hat sits on top of a rock. It's crying but then starts to walk around. DELORME: Gradually, he starts discovering that there are indeed other beings around. ACOVINO: More and more characters join Wattam's world. They interact together in simple and joyful ways - climbing on top of one another, spinning in circles, holding hands. (LAUGHTER)ACOVINO: Takahashi said he came up with the idea for Wattam when he moved to Vancouver and was living in a place with a diverse population. TAKAHASHI: We still have some issue or problem or conflict in this world because of the - like, a different perspective or different religion or different skin color. I believe differences make a good, deep culture. ACOVINO: A good, deep culture, one where people are brought together by playing games. TAKAHASHI: Somehow people get over the differences by having fun. ACOVINO: Vincent Acovino, NPR News. STEVE INSKEEP, HOST:   The point of most video games is to defeat your competition. But a new game out today called Wattam has a different intention, aiming to bring people together. It's the creation of a renowned video game designer, Keita Takahashi. And we have a report this morning from NPR's Vincent Acovino. VINCENT ACOVINO, BYLINE: The year 2004 was not unusual for video games. The bestselling games of that year were full of familiar titles like Grand Theft Auto, Madden and Pokemon. But one game that year was unlike any other. (SOUNDBITE OF SONG, \"KATAMARI ON THE ROCKS\") UNIDENTIFIED SINGER: (Vocalizing). ACOVINO: That game was Katamari Damacy. It's about a god named the King of All Cosmos who, while drunk, accidentally destroys the stars in the sky. His son, The Prince, is left to clean up the mess by rolling up objects on Earth into big, sticky balls that grow so large they become stars. Robin Hunicke is the co-founder of Funomena, the studio that developed Wattam. But she's long been a fan of Takahashi's work. Back in 2004, she helped exhibit Katamari at the Game Developers Conference in San Francisco for a workshop on experimental games. Keita Takahashi gave a presentation that blew her away. ROBIN HUNICKE: He showed the gameplay. But then he went all the way to the credit screen where you can roll up all the countries on Earth. And so you're making a katamari of all of us. And Keita said, in a very heartfelt way, that he had been really destroyed by the idea that we were going to just keep fighting and killing each other. ACOVINO: Katamari Damacy was a surprise commercial hit in North America. It was also a critical success and is one of 14 video games that helped establish a new category of art in the Museum of Modern Art's permanent collection. Keita Takahashi's games are funny and absurd at the surface but kind and earnest at their core. Although Takahashi originally studied to be a sculptor, he became upset when his classmates were throwing out their work after getting grades on their projects. KEITA TAKAHASHI: I was very shocked about that. We have environment crisis. I don't want make more garbage. ACOVINO: Takahashi aimed to make work that was useful and made people laugh. It's an ethos that has carried over to all of his projects, including his latest game, Wattam. HARRY DELORME: It's full of very strange things - a character that's a mouth that eats other characters and excretes them as poop. ACOVINO: That's Harry DeLorme, who helped curate a mid-career survey of Takahashi's work at the Telfair Museums in Savannah, Ga. He says that Wattam is funny, but it's also a game about joy and kindness. (SOUNDBITE OF MUSIC) DELORME: It starts off with a character who is utterly alone. ACOVINO: A green cube with a mouth, a mustache and a top hat sits on top of a rock. It's crying but then starts to walk around. DELORME: Gradually, he starts discovering that there are indeed other beings around. ACOVINO: More and more characters join Wattam's world. They interact together in simple and joyful ways - climbing on top of one another, spinning in circles, holding hands. (LAUGHTER) ACOVINO: Takahashi said he came up with the idea for Wattam when he moved to Vancouver and was living in a place with a diverse population. TAKAHASHI: We still have some issue or problem or conflict in this world because of the - like, a different perspective or different religion or different skin color. I believe differences make a good, deep culture. ACOVINO: A good, deep culture, one where people are brought together by playing games. TAKAHASHI: Somehow people get over the differences by having fun. ACOVINO: Vincent Acovino, NPR News.", "section": "Art & Design", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-19-789949239": {"title": "Uber To Pay $4.4 Million To Employees Who Were Sexually Harassed At Work : NPR", "url": "https://www.npr.org/2019/12/19/789949239/uber-to-pay-4-4-million-to-employees-who-were-sexually-harassed-at-work", "author": "No author found", "published_date": "2019-12-19", "content": "ARI SHAPIRO, HOST: Uber will pay $4. 4 million to employees who were sexually harassed at work. The settlement resolves an investigation by the Equal Employment Opportunity Commission that found Uber allowed a culture of sexual harassment and retaliation. The investigation came after a series of scandals that led to the ouster of Uber's co-founder and CEO Travis Kalanick in 2017. More than two years later, Uber is still trying to clean up its workplace culture. NPR tech correspondent Shannon Bond is here to discuss it. Hi, Shannon. SHANNON BOND, BYLINE: Hi, Ari. SHAPIRO: Tell us more about the settlement. BOND: Yeah, this is just the latest chapter in these long-running issues with Uber's culture, in this case about sexual harassment and retaliation for reporting that harassment. So in this settlement, Uber will pay $4. 4 million into a fund for current and former employees with complaints dating back to 2014. And that amount might not be big, but Uber doesn't come out looking particularly good from this. It's agreed to have its workplace be monitored by a former EEOC commissioner for three years. And the message that this federal agency is sending is really to the entire tech industry. EEOC officials say the settlement will hold Uber accountable, but they also hope it will encourage other workers to speak up about sexism in tech. SHAPIRO: Explain more about why the EEOC was investigating Uber in the first place. BOND: Yeah. This goes back to early 2017. A woman named Susan Fowler, who is an engineer who had recently left Uber, wrote this really shocking blog post describing a toxic work environment there. She said, right when she first started, her manager propositioned her, and she later heard from other women this wasn't the first time that had happened. When she complained, she says HR and management protected the manager, who they said was a high performer. And that post really set off this chain reaction. There was an internal investigation. Twenty employees were fired for bad behavior. And this and other scandals ultimately culminated in the ouster of Travis Kalanick, the CEO, that summer, and federal regulators, including the EEOC, began looking into Uber. SHAPIRO: And what did that EEOC investigation find? BOND: Well, the EEOC says it found, quote, \"reasonable cause to believe that Uber permitted a culture of sexual harassment and retaliation. \" And that matches what other investigations over the years about Uber have found. You know, Dara Khosrowshahi, who replaced Kalanick as CEO, has pledged to change Uber's ways, and he has made some changes. They got rid of a requirement that sexual harassment and assault complaints be settled in arbitration. Those claims can now be taken to court. SHAPIRO: How significant is this settlement in the context of a company like Uber? Four-point-four million dollars doesn't sound like a whole lot of money. BOND: Yeah, it's not much money at all, especially here in Silicon Valley. I mean, remember - Uber is a company that brought in $11 billion in revenue last year. That's less than a tenth - the amount - the settlement is less than a tenth of the pay that CEO Dara Khosrowshahi took home last year. And Uber's paid other, bigger settlements. Like, last year, it paid $10 million to settle claims of race and gender discrimination. So I asked this question to Adrienne Lawrence, who's a lawyer and has a book coming out about sexual harassment in the workplace. I asked her what she made of it, and here's what she said. ADRIENNE LAWRENCE: So when we consider how much this $4. 4 million settlement fund is, it's really just a drop in the bucket. BOND: But she says the settlement also sends an important message to employees and other tech companies. LAWRENCE: The possibility of facing workplace penalties, fines and continuing to really be subject to government monitoring, that is something that Uber and a lot of tech companies, they don't really want. BOND: And we know sexual harassment is a pervasive problem in Silicon Valley, not just at Uber. So the hope is this will put other companies on notice, too. SHAPIRO: That's NPR tech correspondent Shannon Bond. Thank you. BOND: Thanks, Ari. ARI SHAPIRO, HOST:  Uber will pay $4. 4 million to employees who were sexually harassed at work. The settlement resolves an investigation by the Equal Employment Opportunity Commission that found Uber allowed a culture of sexual harassment and retaliation. The investigation came after a series of scandals that led to the ouster of Uber's co-founder and CEO Travis Kalanick in 2017. More than two years later, Uber is still trying to clean up its workplace culture. NPR tech correspondent Shannon Bond is here to discuss it. Hi, Shannon. SHANNON BOND, BYLINE: Hi, Ari. SHAPIRO: Tell us more about the settlement. BOND: Yeah, this is just the latest chapter in these long-running issues with Uber's culture, in this case about sexual harassment and retaliation for reporting that harassment. So in this settlement, Uber will pay $4. 4 million into a fund for current and former employees with complaints dating back to 2014. And that amount might not be big, but Uber doesn't come out looking particularly good from this. It's agreed to have its workplace be monitored by a former EEOC commissioner for three years. And the message that this federal agency is sending is really to the entire tech industry. EEOC officials say the settlement will hold Uber accountable, but they also hope it will encourage other workers to speak up about sexism in tech. SHAPIRO: Explain more about why the EEOC was investigating Uber in the first place. BOND: Yeah. This goes back to early 2017. A woman named Susan Fowler, who is an engineer who had recently left Uber, wrote this really shocking blog post describing a toxic work environment there. She said, right when she first started, her manager propositioned her, and she later heard from other women this wasn't the first time that had happened. When she complained, she says HR and management protected the manager, who they said was a high performer. And that post really set off this chain reaction. There was an internal investigation. Twenty employees were fired for bad behavior. And this and other scandals ultimately culminated in the ouster of Travis Kalanick, the CEO, that summer, and federal regulators, including the EEOC, began looking into Uber. SHAPIRO: And what did that EEOC investigation find? BOND: Well, the EEOC says it found, quote, \"reasonable cause to believe that Uber permitted a culture of sexual harassment and retaliation. \" And that matches what other investigations over the years about Uber have found. You know, Dara Khosrowshahi, who replaced Kalanick as CEO, has pledged to change Uber's ways, and he has made some changes. They got rid of a requirement that sexual harassment and assault complaints be settled in arbitration. Those claims can now be taken to court. SHAPIRO: How significant is this settlement in the context of a company like Uber? Four-point-four million dollars doesn't sound like a whole lot of money. BOND: Yeah, it's not much money at all, especially here in Silicon Valley. I mean, remember - Uber is a company that brought in $11 billion in revenue last year. That's less than a tenth - the amount - the settlement is less than a tenth of the pay that CEO Dara Khosrowshahi took home last year. And Uber's paid other, bigger settlements. Like, last year, it paid $10 million to settle claims of race and gender discrimination. So I asked this question to Adrienne Lawrence, who's a lawyer and has a book coming out about sexual harassment in the workplace. I asked her what she made of it, and here's what she said. ADRIENNE LAWRENCE: So when we consider how much this $4. 4 million settlement fund is, it's really just a drop in the bucket. BOND: But she says the settlement also sends an important message to employees and other tech companies. LAWRENCE: The possibility of facing workplace penalties, fines and continuing to really be subject to government monitoring, that is something that Uber and a lot of tech companies, they don't really want. BOND: And we know sexual harassment is a pervasive problem in Silicon Valley, not just at Uber. So the hope is this will put other companies on notice, too. SHAPIRO: That's NPR tech correspondent Shannon Bond. Thank you. BOND: Thanks, Ari.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-19-789609572": {"title": "Facebook Releases 2020 Census Policy To Prevent Interference Ahead of Count : NPR", "url": "https://www.npr.org/2019/12/19/789609572/on-census-facebook-and-instagram-to-ban-disinformation-and-false-ads", "author": "No author found", "published_date": "2019-12-19", "content": "", "section": "National", "disclaimer": ""}, "2019-12-20-790098677": {"title": "In Uncrewed Test, Boeing's Starliner Capsule Fails To Reach Space Station : NPR", "url": "https://www.npr.org/2019/12/20/790098677/in-uncrewed-test-boeings-starliner-capsule-fails-to-reach-space-station", "author": "No author found", "published_date": "2019-12-20", "content": "", "section": "Science", "disclaimer": ""}, "2019-12-22-790588715": {"title": "Boeing's Starliner Lands Safely Back To Earth After Aborted Space Mission : NPR", "url": "https://www.npr.org/2019/12/22/790588715/boeings-starliner-lands-safely-back-to-earth-after-aborted-space-station-mission", "author": "No author found", "published_date": "2019-12-22", "content": "", "section": "Science", "disclaimer": ""}, "2019-12-22-790553860": {"title": "Home Security And Hacking : NPR", "url": "https://www.npr.org/2019/12/22/790553860/home-security-and-hacking", "author": "No author found", "published_date": "2019-12-22", "content": "LULU GARCIA-NAVARRO, HOST:  There have been many reports in recent months about people's home security cameras and smart appliances getting hacked, with hackers taunting small children over cameras or blasting vulgar music. We asked Rachel Cericola, who reports on smart homes for Wirecutter, how the hacks actually happen. RACHEL CERICOLA: Well, in this case, we talk a lot about hacking, but hacking sort of implies that someone found a hole in the system or a backdoor entry. That's not really what's going on here. For the most part, people have poor passwords. They might be reusing them - the same passwords they used for their banking accounts or a store rewards account. And they're probably compromised elsewhere. And when that happens, they're out there on the Web. People find them, and they can use them to access people's cameras. GARCIA-NAVARRO: You wrote that there will be over 42 million smart homes by the end of 2019. Any idea how many of these homes will be vulnerable to data breaches? I mean, it seems like this is happening more and more as these homes and appliances become more popular. CERICOLA: I guess I should say that every smart device is vulnerable to data breach. You have to protect yourself by using strong, unique passwords for every single device. GARCIA-NAVARRO: Is one type of device more vulnerable than others? I mean, Target was hacked through its HVAC system in 2014. CERICOLA: Right. I can't say that one is stronger than the other. I do think that it seems like cameras are the most popular devices that we hear about these incidents happening. You know, a lot of these devices store your Wi-Fi credentials, your password. They might even store where you're located, your home address or at least your location. GARCIA-NAVARRO: Motherboard just reported that automated software for cracking Ring passwords was being sold online for all of $6. Is trying to keep your home secure kind of hopeless? CERICOLA: It might seem that way, but I think there are simple precautions that people can take. Unfortunately, right now, there's no government regulation regarding security and privacy. That doesn't mean that manufacturers shouldn't be making some of these safety features a requirement or even better educating consumers about what they can do. But currently, a lot of it falls back on the user not to just enable the safe practices but know what they are. GARCIA-NAVARRO: What have the companies, like Ring and Google - which owns Nest - said in response to these hacks? CERICOLA: Typically, they've been pretty good about letting consumers know what the deal is, that things have been compromised. And both Nest and Ring have another safety feature called two-factor authentication. They're telling people to enable that. I think this is relatively new that companies are reaching out to tell consumers to enable these safety practices. And I hope to see more of that in the future. GARCIA-NAVARRO: Do you use any of these home security services or smart devices in your home? CERICOLA: Oh, so many of them. GARCIA-NAVARRO: Really? CERICOLA: My job is reviewing smart home devices. So I have experience with most of these brands and many others. GARCIA-NAVARRO: And what do you do to keep safe? CERICOLA: Like I said, I really - I use the strong passwords. I also use a guest network for all of my smart devices so it's not connected to the same computer that I use for home use and that my family uses. And I also try to encourage people to stick with a popular or good brands. Typically, bigger brands will have the infrastructure to handle problems when they arise like this. GARCIA-NAVARRO: That's Rachel Cericola. Thank you so much for speaking with us. CERICOLA: Oh, thank you for having me. LULU GARCIA-NAVARRO, HOST:   There have been many reports in recent months about people's home security cameras and smart appliances getting hacked, with hackers taunting small children over cameras or blasting vulgar music. We asked Rachel Cericola, who reports on smart homes for Wirecutter, how the hacks actually happen. RACHEL CERICOLA: Well, in this case, we talk a lot about hacking, but hacking sort of implies that someone found a hole in the system or a backdoor entry. That's not really what's going on here. For the most part, people have poor passwords. They might be reusing them - the same passwords they used for their banking accounts or a store rewards account. And they're probably compromised elsewhere. And when that happens, they're out there on the Web. People find them, and they can use them to access people's cameras. GARCIA-NAVARRO: You wrote that there will be over 42 million smart homes by the end of 2019. Any idea how many of these homes will be vulnerable to data breaches? I mean, it seems like this is happening more and more as these homes and appliances become more popular. CERICOLA: I guess I should say that every smart device is vulnerable to data breach. You have to protect yourself by using strong, unique passwords for every single device. GARCIA-NAVARRO: Is one type of device more vulnerable than others? I mean, Target was hacked through its HVAC system in 2014. CERICOLA: Right. I can't say that one is stronger than the other. I do think that it seems like cameras are the most popular devices that we hear about these incidents happening. You know, a lot of these devices store your Wi-Fi credentials, your password. They might even store where you're located, your home address or at least your location. GARCIA-NAVARRO: Motherboard just reported that automated software for cracking Ring passwords was being sold online for all of $6. Is trying to keep your home secure kind of hopeless? CERICOLA: It might seem that way, but I think there are simple precautions that people can take. Unfortunately, right now, there's no government regulation regarding security and privacy. That doesn't mean that manufacturers shouldn't be making some of these safety features a requirement or even better educating consumers about what they can do. But currently, a lot of it falls back on the user not to just enable the safe practices but know what they are. GARCIA-NAVARRO: What have the companies, like Ring and Google - which owns Nest - said in response to these hacks? CERICOLA: Typically, they've been pretty good about letting consumers know what the deal is, that things have been compromised. And both Nest and Ring have another safety feature called two-factor authentication. They're telling people to enable that. I think this is relatively new that companies are reaching out to tell consumers to enable these safety practices. And I hope to see more of that in the future. GARCIA-NAVARRO: Do you use any of these home security services or smart devices in your home? CERICOLA: Oh, so many of them. GARCIA-NAVARRO: Really? CERICOLA: My job is reviewing smart home devices. So I have experience with most of these brands and many others. GARCIA-NAVARRO: And what do you do to keep safe? CERICOLA: Like I said, I really - I use the strong passwords. I also use a guest network for all of my smart devices so it's not connected to the same computer that I use for home use and that my family uses. And I also try to encourage people to stick with a popular or good brands. Typically, bigger brands will have the infrastructure to handle problems when they arise like this. GARCIA-NAVARRO: That's Rachel Cericola. Thank you so much for speaking with us. CERICOLA: Oh, thank you for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-22-790553909": {"title": "Snopes, Facebook And Fake Accounts : NPR", "url": "https://www.npr.org/2019/12/22/790553909/snopes-facebook-and-fake-accounts", "author": "No author found", "published_date": "2019-12-22", "content": ": [Editor's Note on Oct. 18, 2020: Since the airing of this story, we received a request from The Epoch Times to respond to the allegations in this story. It provided this statement: \"The introduction to this interview states that The Epoch Times was behind this 'sophisticated misinformation campaign. ' The Epoch Times vehemently denies that it was involved in this misinformation campaign. The Epoch Times has no connection with The Beauty of Life (The BL), and the companies are in no way affiliated with one another. The BL was founded by a former employee and employs some of our former employees. However, that some of our former employees work for The BL is not evidence of any connection between the two organizations. \"]LULU GARCIA-NAVARRO, HOST:  On Friday, Facebook announced it removed more than 900 fake accounts, groups and pages, many with AI-generated profiles. Some of the groups had names like Americans for President Trump, America Needs President Trump and even Hispanos por Trump. Behind the sophisticated misinformation campaign - the group Epoch Times. It's pro-Trump, but it is also linked to the Falun Gong, which opposes the Chinese government. Snopes, the fact-checking website, reported on The Epoch Times' misleading use of Facebook and Instagram last fall. And we're joined now by Snopes' VP of operations, Vinny Green. Hello. VINNY GREEN: Hi. How are you? GARCIA-NAVARRO: And reporter Jordan Liles. Welcome to the program. JORDAN LILES: Hi. Thank you for having us. GARCIA-NAVARRO: So Vinny, I'm going to start with you. This investigation centers around a group called the BL, or Beauty of Life. And you found that it has extensive links to The Epoch Times. So remind us where The Epoch Times sort of lives in the media ecosystem. GREEN: Yeah. Well, it's really far-reaching. Not only do they have a print edition that distributes pretty widely, but they have a very dominant online presence. They've really - have established a vast multimedia, multiplatform distribution channel and artificially, through advertising, boost their prominence. And it - so it's widely read and widely distributed. But it is this very fringe publication. And we've discovered that it's got some other tentacles that are reaching out into the media landscape and - one of which was this vast network called the BL. GARCIA-NAVARRO: Yeah. I mean, their main page has more than 6 million followers on Facebook, and they have ties to the Falun Gong movement, which is a spiritual practice which the Chinese government calls a cult. And these fake pages and profiles had a lot of pro-Trump content but also anti-Chinese government, too. What were they pushing out? GREEN: What we saw was an extreme amount of pro-Trump content. Almost exclusively what we were looking at on the BL was the amplification of pro-Trump media, usually created out of whole cloth. We saw some really peculiar things around creating fan pages for the individual family members in the Trump family - you know, fawning coverage, creating memes and video clips. And that content plays well on Facebook. GARCIA-NAVARRO: Some people have called this this sort of industrial-scale misinformation. Jordan, you reported on this extensively. What made you look at the BL group of accounts? LILES: When we first started looking at the BL, the way that we actually even found it was the same account that we had been following, who previously led us to a Ukraine-run page, also one day early in October shared a post for the BL. And I had never heard of it, so I clicked. And then on every Facebook page, you can scroll down. And there's a page transparency button that lets you see where the page managers are that actually operate the page. And my first reaction to seeing that most of them were in Vietnam and then one of them was in Syria and other countries was that this very pro-America page was being operated largely outside of the United States. GARCIA-NAVARRO: Vinny, Facebook shut down The Epoch Times' mass ad buying this summer, but they didn't remove these groups and fake accounts even after your reporting went public. You pushed them on this multiple times. What happened, and how did they respond? GREEN: Well, you know, this is probably the most disheartening of all of this because it really, I think, is emblematic of what's wrong with Facebook in particular and these platforms in general - is their inability to combat this stuff in a timely manner or kind of acknowledge it's even happening. We identified these accounts in early October. And all of that time, as good reporters do, we are knocking on Facebook's door, saying, what is going on here? They said they spotted this network in July. There is resolution in December. What are they going to do about this type of election interference when people are trying to head to the ballot box? It's really concerning, and something has to change immediately. GARCIA-NAVARRO: Vinny, just briefly, Snopes and Facebook were partners with their third-party fact-checking program, but you left in February. Why was that? GREEN: Well, there was a resounding response from our newsroom in late last year that it was no longer tenable to work with an organization that didn't share our values. From what we've seen, especially what we've seen since the last election, Facebook is not a partner of the news media. I do not believe that despite their outreach and efforts that it is nothing more than a PR strategy. We didn't think that we could best serve our readers by continuing a relationship that was what we believed ineffectual. GARCIA-NAVARRO: That's Vinny Green and Jordan Liles from Snopes. Thank you so much. GREEN: Thank you for having us. We really greatly appreciate it. : [Editor's Note on Oct. 18, 2020: Since the airing of this story, we received a request from The Epoch Times to respond to the allegations in this story. It provided this statement: \"The introduction to this interview states that The Epoch Times was behind this 'sophisticated misinformation campaign. ' The Epoch Times vehemently denies that it was involved in this misinformation campaign. The Epoch Times has no connection with The Beauty of Life (The BL), and the companies are in no way affiliated with one another. The BL was founded by a former employee and employs some of our former employees. However, that some of our former employees work for The BL is not evidence of any connection between the two organizations. \"] LULU GARCIA-NAVARRO, HOST:   On Friday, Facebook announced it removed more than 900 fake accounts, groups and pages, many with AI-generated profiles. Some of the groups had names like Americans for President Trump, America Needs President Trump and even Hispanos por Trump. Behind the sophisticated misinformation campaign - the group Epoch Times. It's pro-Trump, but it is also linked to the Falun Gong, which opposes the Chinese government. Snopes, the fact-checking website, reported on The Epoch Times' misleading use of Facebook and Instagram last fall. And we're joined now by Snopes' VP of operations, Vinny Green. Hello. VINNY GREEN: Hi. How are you? GARCIA-NAVARRO: And reporter Jordan Liles. Welcome to the program. JORDAN LILES: Hi. Thank you for having us. GARCIA-NAVARRO: So Vinny, I'm going to start with you. This investigation centers around a group called the BL, or Beauty of Life. And you found that it has extensive links to The Epoch Times. So remind us where The Epoch Times sort of lives in the media ecosystem. GREEN: Yeah. Well, it's really far-reaching. Not only do they have a print edition that distributes pretty widely, but they have a very dominant online presence. They've really - have established a vast multimedia, multiplatform distribution channel and artificially, through advertising, boost their prominence. And it - so it's widely read and widely distributed. But it is this very fringe publication. And we've discovered that it's got some other tentacles that are reaching out into the media landscape and - one of which was this vast network called the BL. GARCIA-NAVARRO: Yeah. I mean, their main page has more than 6 million followers on Facebook, and they have ties to the Falun Gong movement, which is a spiritual practice which the Chinese government calls a cult. And these fake pages and profiles had a lot of pro-Trump content but also anti-Chinese government, too. What were they pushing out? GREEN: What we saw was an extreme amount of pro-Trump content. Almost exclusively what we were looking at on the BL was the amplification of pro-Trump media, usually created out of whole cloth. We saw some really peculiar things around creating fan pages for the individual family members in the Trump family - you know, fawning coverage, creating memes and video clips. And that content plays well on Facebook. GARCIA-NAVARRO: Some people have called this this sort of industrial-scale misinformation. Jordan, you reported on this extensively. What made you look at the BL group of accounts? LILES: When we first started looking at the BL, the way that we actually even found it was the same account that we had been following, who previously led us to a Ukraine-run page, also one day early in October shared a post for the BL. And I had never heard of it, so I clicked. And then on every Facebook page, you can scroll down. And there's a page transparency button that lets you see where the page managers are that actually operate the page. And my first reaction to seeing that most of them were in Vietnam and then one of them was in Syria and other countries was that this very pro-America page was being operated largely outside of the United States. GARCIA-NAVARRO: Vinny, Facebook shut down The Epoch Times' mass ad buying this summer, but they didn't remove these groups and fake accounts even after your reporting went public. You pushed them on this multiple times. What happened, and how did they respond? GREEN: Well, you know, this is probably the most disheartening of all of this because it really, I think, is emblematic of what's wrong with Facebook in particular and these platforms in general - is their inability to combat this stuff in a timely manner or kind of acknowledge it's even happening. We identified these accounts in early October. And all of that time, as good reporters do, we are knocking on Facebook's door, saying, what is going on here? They said they spotted this network in July. There is resolution in December. What are they going to do about this type of election interference when people are trying to head to the ballot box? It's really concerning, and something has to change immediately. GARCIA-NAVARRO: Vinny, just briefly, Snopes and Facebook were partners with their third-party fact-checking program, but you left in February. Why was that? GREEN: Well, there was a resounding response from our newsroom in late last year that it was no longer tenable to work with an organization that didn't share our values. From what we've seen, especially what we've seen since the last election, Facebook is not a partner of the news media. I do not believe that despite their outreach and efforts that it is nothing more than a PR strategy. We didn't think that we could best serve our readers by continuing a relationship that was what we believed ineffectual. GARCIA-NAVARRO: That's Vinny Green and Jordan Liles from Snopes. Thank you so much. GREEN: Thank you for having us. We really greatly appreciate it.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-22-790281142": {"title": "Scathing Report Puts Secret FISA Court Into The Spotlight. Will Congress Act? : NPR", "url": "https://www.npr.org/2019/12/22/790281142/scathing-report-puts-secret-fisa-court-into-the-spotlight-will-congress-act", "author": "No author found", "published_date": "2019-12-22", "content": "LULU GARCIA-NAVARRO, HOST:  Even by the secretive standards of U. S. national security, the Foreign Intelligence Surveillance Court is cloaked in mystery. It oversees government surveillance on American soil in terrorism and espionage investigations. But a recent report from the Justice Department's inspector general has raised questions about how the court protects the rights of Americans and whether it needs to be reformed. NPR justice correspondent Ryan Lucas brings us this report. RYAN LUCAS, BYLINE: When the FBI wants to wiretap an American suspected of spying for a foreign power or belonging to a terrorist group, it needs to get court approval. That comes from the Foreign Intelligence Surveillance Court, also known as the FISA Court. That approval process plays out in secret behind closed doors. There are now growing calls to change how this works, including from Chairman of the Senate Judiciary Committee Lindsey Graham. (SOUNDBITE OF ARCHIVED RECORDING)LINDSEY GRAHAM: I'd hate to lose the ability of the FISA Court to operate at a time, probably, when we need it the most. But after your report, I have serious concerns about whether the FISA Court can continue unless there is fundamental reform. LUCAS: The report Graham refers to there is Justice Department Inspector General Michael Horowitz's review of the early days of the Russia investigation. Horowitz's report came out last week. And it focuses, in part, on the FBI surveillance of former Trump campaign adviser Carter Page. What Horowitz discovered was 17 significant inaccuracies and omissions in the FBI's applications to wiretap Page. (SOUNDBITE OF ARCHIVED RECORDING)MICHAEL HOROWITZ: We found that investigators failed to meet their basic obligations of ensuring that the FISA applications were scrupulously accurate. LUCAS: Information that supported the FBI's case was given to the court, while information that undercut it was left out. For longtime critics of the FISA process, the problems the inspector general unearthed point to what they say are deeper issues with national security surveillance. Here's Elizabeth Goitein of the Brennan Center for Justice. ELIZABETH GOITEIN: The way the system is built enables this kind of one-sided presentation that results in violations of the privacy of Americans. LUCAS: The problems with the Page case have generated blowback, including from the FISA Court itself. The court's chief judge, Rosemary Collyer, issued a rare public order this week that sharply rebuked the FBI. She said the bureau has an obligation to be fully forthcoming with the court, and in the case of Page, it most definitely was not. She has ordered the government to tell the court by January 10 what it plans to do to make sure inaccuracies and omissions don't happen again. There was a line tucked inside her order that pointed to a broader issue, as well. She said the frequency of the FBI's misrepresentations raises questions about the accuracy of its other surveillance requests. It's a point that lawmakers, including Republican Senator Ben Sasse, have also made. (SOUNDBITE OF ARCHIVED RECORDING)BEN SASSE: If the American people hear this and they say this can happen against a campaign for the presidency of the United States, what happens in an ordinary FISA case? LUCAS: For civil liberties advocates, such questions about FISA are not new. They have long expressed concerns about the government's use of intrusive surveillance on, say, Muslim Americans. But the Page case has registered with congressional hawks in a way those concerns never did. Sasse and other lawmakers who have defended national security surveillance in the past now say Congress may need to make changes to FISA. And lawmakers will soon have a chance to do so. Three parts of FISA are set to expire in March. The Brennan Center's Goitein says that gives members of Congress a chance to address the problems raised by the inspector general's report. GOITEIN: So we'll see if they've had a change of heart, whether they're actually going to build in greater civil liberties protections or whether all of this talk about FISA reform was just political posturing. LUCAS: For all the talk of reform, those who have used FISA to track foreign spies and terrorists say it is, in many ways, irreplaceable. STEPHANIE DOUGLAS: FISA is an incredibly important tool. . . LUCAS: That's Stephanie Douglas. She used to be in charge of the FBI's national security branch. DOUGLAS: . . . Not just in counterintelligence investigations but also a very important tool in counterterrorism investigations and even some cyber investigations. LUCAS: Douglas, who now works for Guidepost Solutions, says the process for getting FISA orders may need to be tightened. But this kind of surveillance power shouldn't be tossed out entirely. That's the line from FBI Director Christopher Wray, as well. He says he's ordered corrective steps to fix the FBI's FISA process. The goal, he says, is to make sure that the information presented to the court is verified, reviewed and accurate. Ryan Lucas, NPR News, Washington. LULU GARCIA-NAVARRO, HOST:   Even by the secretive standards of U. S. national security, the Foreign Intelligence Surveillance Court is cloaked in mystery. It oversees government surveillance on American soil in terrorism and espionage investigations. But a recent report from the Justice Department's inspector general has raised questions about how the court protects the rights of Americans and whether it needs to be reformed. NPR justice correspondent Ryan Lucas brings us this report. RYAN LUCAS, BYLINE: When the FBI wants to wiretap an American suspected of spying for a foreign power or belonging to a terrorist group, it needs to get court approval. That comes from the Foreign Intelligence Surveillance Court, also known as the FISA Court. That approval process plays out in secret behind closed doors. There are now growing calls to change how this works, including from Chairman of the Senate Judiciary Committee Lindsey Graham. (SOUNDBITE OF ARCHIVED RECORDING) LINDSEY GRAHAM: I'd hate to lose the ability of the FISA Court to operate at a time, probably, when we need it the most. But after your report, I have serious concerns about whether the FISA Court can continue unless there is fundamental reform. LUCAS: The report Graham refers to there is Justice Department Inspector General Michael Horowitz's review of the early days of the Russia investigation. Horowitz's report came out last week. And it focuses, in part, on the FBI surveillance of former Trump campaign adviser Carter Page. What Horowitz discovered was 17 significant inaccuracies and omissions in the FBI's applications to wiretap Page. (SOUNDBITE OF ARCHIVED RECORDING) MICHAEL HOROWITZ: We found that investigators failed to meet their basic obligations of ensuring that the FISA applications were scrupulously accurate. LUCAS: Information that supported the FBI's case was given to the court, while information that undercut it was left out. For longtime critics of the FISA process, the problems the inspector general unearthed point to what they say are deeper issues with national security surveillance. Here's Elizabeth Goitein of the Brennan Center for Justice. ELIZABETH GOITEIN: The way the system is built enables this kind of one-sided presentation that results in violations of the privacy of Americans. LUCAS: The problems with the Page case have generated blowback, including from the FISA Court itself. The court's chief judge, Rosemary Collyer, issued a rare public order this week that sharply rebuked the FBI. She said the bureau has an obligation to be fully forthcoming with the court, and in the case of Page, it most definitely was not. She has ordered the government to tell the court by January 10 what it plans to do to make sure inaccuracies and omissions don't happen again. There was a line tucked inside her order that pointed to a broader issue, as well. She said the frequency of the FBI's misrepresentations raises questions about the accuracy of its other surveillance requests. It's a point that lawmakers, including Republican Senator Ben Sasse, have also made. (SOUNDBITE OF ARCHIVED RECORDING) BEN SASSE: If the American people hear this and they say this can happen against a campaign for the presidency of the United States, what happens in an ordinary FISA case? LUCAS: For civil liberties advocates, such questions about FISA are not new. They have long expressed concerns about the government's use of intrusive surveillance on, say, Muslim Americans. But the Page case has registered with congressional hawks in a way those concerns never did. Sasse and other lawmakers who have defended national security surveillance in the past now say Congress may need to make changes to FISA. And lawmakers will soon have a chance to do so. Three parts of FISA are set to expire in March. The Brennan Center's Goitein says that gives members of Congress a chance to address the problems raised by the inspector general's report. GOITEIN: So we'll see if they've had a change of heart, whether they're actually going to build in greater civil liberties protections or whether all of this talk about FISA reform was just political posturing. LUCAS: For all the talk of reform, those who have used FISA to track foreign spies and terrorists say it is, in many ways, irreplaceable. STEPHANIE DOUGLAS: FISA is an incredibly important tool. . . LUCAS: That's Stephanie Douglas. She used to be in charge of the FBI's national security branch. DOUGLAS: . . . Not just in counterintelligence investigations but also a very important tool in counterterrorism investigations and even some cyber investigations. LUCAS: Douglas, who now works for Guidepost Solutions, says the process for getting FISA orders may need to be tightened. But this kind of surveillance power shouldn't be tossed out entirely. That's the line from FBI Director Christopher Wray, as well. He says he's ordered corrective steps to fix the FBI's FISA process. The goal, he says, is to make sure that the information presented to the court is verified, reviewed and accurate. Ryan Lucas, NPR News, Washington.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-23-790929788": {"title": "Apple Pulls ToTok App After Report That Emirati Government Uses It For Surveillance : NPR", "url": "https://www.npr.org/2019/12/23/790929788/apple-pulls-totok-app-after-report-that-emirati-government-uses-it-for-surveilla", "author": "No author found", "published_date": "2019-12-23", "content": "ARI SHAPIRO, HOST: Last week, Apple and Google removed a new messaging app from their app stores. It's called ToTok, and it's been growing in the Middle East and the U. S. The government of the United Arab Emirates is accused of using ToTok as a spying tool. That's according to a new report from The New York Times which cites American intelligence officials. NPR tech correspondent Shannon Bond joins us now. Hi, Shannon. SHANNON BOND, BYLINE: Hi, Ari. SHAPIRO: I confess, when I saw the name of this app, ToTok, I thought it was a typo and people meant TikTok. That's not what it is. What is ToTok? BOND: That's right. I think a lot of people probably made that mistake. So ToTok is a messaging app. It allows people to chat by video or text sort of like Facebook Messenger or WhatsApp. And it's now, as you said, unavailable from Apple and Google in their app stores. That's because of this story in The New York Times that ToTok is being used by the Emirati government to spy on people. Now, NPR hasn't independently confirmed this, and ToTok hasn't responded to the allegations. It says the app is temporarily unavailable from Google and Apple because of a technical issue right now. SHAPIRO: How popular is it? BOND: Well, it's new but very popular. It was launched in August. Since then, it has been downloaded by about 8 million people globally, according to App Annie, a mobile analytics company. It's been especially popular in the Middle East and India, but it's also been getting downloads here in the U. S. Around 240,000 people here have downloaded it. In the UAE, it seems to have really benefited from the fact that the government there heavily restricts communications apps, so there's lots of limits on using other apps you might think of for voice and video calls like FaceTime, Skype, WhatsApp. And ToTok's emerged as an attractive alternative, and as the more people started using it, that drove more and more downloads in the UAE and elsewhere. SHAPIRO: I think a lot of people are used to their devices spying on them. What are intelligence officials specifically worried about with ToTok? BOND: Right. So like many apps, ToTok collects a lot of information about its users, and this wasn't about malicious code or spyware. These were standard permissions that people were agreeing to. Its privacy policy says it may share data, and we know people tend to agree to those policies without reading them. But like I said, people don't necessarily have access to other apps that might be - have encrypted communications, so this has been popular. I spoke to Patrick Wardle. He's a security researcher at a company called Jamf, and he looked into the app for the Times. He found that ToTok asked for access to location, photos, calendars, contacts. PATRICK WARDLE: So for example, when you launch the app for the first time, it asks you if you want to be connected with your friends. If you click allow, the application will access your address book, collect everyone in your address book and attempt to send that out to a server. BOND: And not only can it see who you're talking to. It can see the contents of those messages, so it can see sort of, you know, the contents of your conversation. SHAPIRO: So does that mean that if - I don't know - democracy activists were planning to meet up or LGBTQ people were connecting, that the government would know about that in the UAE? Is that the concern? BOND: Yeah, that is the concern. The Times reports the Emirati government is basically behind the app and is getting this data, and it's known for cracking down on dissent. According to Human Rights Watch, it regularly surveils dissidents and their families. And Wardle says the kind of data that ToTok is collecting is a powerful tool. WARDLE: You're able to push out an application that becomes very popular, that millions of people are going to download and start sharing their contacts and chatting. I mean, that's really a goldmine from a surveillance and intelligence operation point of view. BOND: So I think this is a reminder to all of us. As we're going around every day using our smartphones, we're leaving trails of data behind about ourselves. And we know companies might use this data to target us with ads, but here's an example of how governments may be using it to watch us as well. SHAPIRO: That is NPR tech correspondent Shannon Bond. Thanks, Shannon. BOND: Thanks, Ari. ARI SHAPIRO, HOST:  Last week, Apple and Google removed a new messaging app from their app stores. It's called ToTok, and it's been growing in the Middle East and the U. S. The government of the United Arab Emirates is accused of using ToTok as a spying tool. That's according to a new report from The New York Times which cites American intelligence officials. NPR tech correspondent Shannon Bond joins us now. Hi, Shannon. SHANNON BOND, BYLINE: Hi, Ari. SHAPIRO: I confess, when I saw the name of this app, ToTok, I thought it was a typo and people meant TikTok. That's not what it is. What is ToTok? BOND: That's right. I think a lot of people probably made that mistake. So ToTok is a messaging app. It allows people to chat by video or text sort of like Facebook Messenger or WhatsApp. And it's now, as you said, unavailable from Apple and Google in their app stores. That's because of this story in The New York Times that ToTok is being used by the Emirati government to spy on people. Now, NPR hasn't independently confirmed this, and ToTok hasn't responded to the allegations. It says the app is temporarily unavailable from Google and Apple because of a technical issue right now. SHAPIRO: How popular is it? BOND: Well, it's new but very popular. It was launched in August. Since then, it has been downloaded by about 8 million people globally, according to App Annie, a mobile analytics company. It's been especially popular in the Middle East and India, but it's also been getting downloads here in the U. S. Around 240,000 people here have downloaded it. In the UAE, it seems to have really benefited from the fact that the government there heavily restricts communications apps, so there's lots of limits on using other apps you might think of for voice and video calls like FaceTime, Skype, WhatsApp. And ToTok's emerged as an attractive alternative, and as the more people started using it, that drove more and more downloads in the UAE and elsewhere. SHAPIRO: I think a lot of people are used to their devices spying on them. What are intelligence officials specifically worried about with ToTok? BOND: Right. So like many apps, ToTok collects a lot of information about its users, and this wasn't about malicious code or spyware. These were standard permissions that people were agreeing to. Its privacy policy says it may share data, and we know people tend to agree to those policies without reading them. But like I said, people don't necessarily have access to other apps that might be - have encrypted communications, so this has been popular. I spoke to Patrick Wardle. He's a security researcher at a company called Jamf, and he looked into the app for the Times. He found that ToTok asked for access to location, photos, calendars, contacts. PATRICK WARDLE: So for example, when you launch the app for the first time, it asks you if you want to be connected with your friends. If you click allow, the application will access your address book, collect everyone in your address book and attempt to send that out to a server. BOND: And not only can it see who you're talking to. It can see the contents of those messages, so it can see sort of, you know, the contents of your conversation. SHAPIRO: So does that mean that if - I don't know - democracy activists were planning to meet up or LGBTQ people were connecting, that the government would know about that in the UAE? Is that the concern? BOND: Yeah, that is the concern. The Times reports the Emirati government is basically behind the app and is getting this data, and it's known for cracking down on dissent. According to Human Rights Watch, it regularly surveils dissidents and their families. And Wardle says the kind of data that ToTok is collecting is a powerful tool. WARDLE: You're able to push out an application that becomes very popular, that millions of people are going to download and start sharing their contacts and chatting. I mean, that's really a goldmine from a surveillance and intelligence operation point of view. BOND: So I think this is a reminder to all of us. As we're going around every day using our smartphones, we're leaving trails of data behind about ourselves. And we know companies might use this data to target us with ads, but here's an example of how governments may be using it to watch us as well. SHAPIRO: That is NPR tech correspondent Shannon Bond. Thanks, Shannon. BOND: Thanks, Ari.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-25-791351673": {"title": "What Might Happen In Tech Over The Next Decade : NPR", "url": "https://www.npr.org/2019/12/25/791351673/what-might-happen-in-tech-over-the-next-decade", "author": "No author found", "published_date": "2019-12-25", "content": "ARI SHAPIRO, HOST: Well, our next guest is someone who's done a lot of work on how to detect that last problem of deepfake videos. We have asked Hany Farid of the University of California, Berkeley to join us now to discuss how technology may develop over the next decade. Welcome back to All Tech Considered. HANY FARID: Good to be here, Ari. SHAPIRO: We're ending this decade on a note of uncertainty and skepticism about technology that might not have been there 10 years ago for whatever reason. Can I just begin by getting a quick sense of how you are feeling about technology heading into 2020? FARID: I think you're right that the last decade has been disappointing, and I think we are frustrated and worried about the direction we are moving in in the industry and how technology is affecting our lives day to day. I think we're reaching the low point. I don't think we're quite at it. But I hope that after that low point, we start to climb back uphill and start to take advantage of all the wonderful things about technology while mitigating many of the awful things that we've seen over the last. . . SHAPIRO: So you're an optimist. FARID: I am an optimist, and I'm a technologist. And I want technology to be great aspects of our lives. I think now the tables have turned a little bit, and I think that we are seeing more downside than upside. But I think that that's fixable with some thought and some regulation and some corporate changes and some new technologies. SHAPIRO: I realize that when I ask you to imagine the next decade in technology, we could be talking about anything from social media. . . FARID: Yeah. SHAPIRO: . . . To health, manufacturing, vehicles, clean energy. Just to get specific, I'd like to hear about one thing that makes you really optimistic and excited and one thing that makes you feel a little bit of dread. Start by telling us something that makes you hopeful. SHAPIRO: Yeah. Where I'm hopeful is the use of technology in medicine. I think one of the most exciting things we are seeing with the advances in machine learning and deep learning and AI is in rapid advances in everything from early detection of cancer to treatments because of the big data. And I think that's really exciting, and I think it has real potential in the next decade. For the dread, I mean, I can go on and on and on. FARID: (Laughter). FARID: But let me. . . SHAPIRO: Give us one thing that. . . FARID: Let me tell you why. SHAPIRO: Yeah. FARID: The thing that worries me most is the dis- and misinformation campaigns - is the sort of the polluting of the online ecosystem of information with absolute garbage. And I think these are real existential threats to our democracy. And I think we are far, far from being ready to deal with it from a technological perspective and from a legislative and regulatory perspective. And I'm very worried about the upcoming elections and the next round and the next round. SHAPIRO: What about the physical, human relationship to technology? Do you think that in the next 10 years, the line between ourselves and the technology that we use will begin to blur? FARID: I would argue it's already blurring. I mean, the cell phone, the mobile device is now ubiquitous. If you go out and - you go to an airport, for example, 99% of the people are glued to a device. They are now part of our lives for better or worse. And what's next? Probably the next step in the next decade is going to be augmented reality, where we all have glasses where all the information is coming in simultaneously and will start to intermingle with the real world in an interesting and maybe dystopian way. I think after that, it's hard to say because, you know, that next step, that next 10 years, is almost impossible to predict. SHAPIRO: As you've pointed out, right now, there are important areas where we don't have good guardrails. . . FARID: Yeah. SHAPIRO: . . . Whether that's about companies tracking us through our devices or countries developing autonomous weapons, killer robots. Do you see that changing? Do you think regulation of technology in the next 10 years will be more effective? FARID: I don't know if it'll be more effective, but I'm sure that there will be regulation coming. So we saw that out of Europe with protections of privacy. I will tell you, having spent time in Brussels and in the U. K. and on Capitol Hill, there was a lot of discussion about regulating tech - everything from breaking up what is a monopoly in some cases to forcing these companies to be better at consumer protection and everything from privacy to security. And I think that that regulation is coming. The hope, of course, is that we regulate in a sensible and smart way that is adaptive to a very fast-moving technology space. But I think that is coming in the next decade. We have been absolutely absent around the world in regulating the technology sector, and it has led to the mess that we are in now. And now we have to start putting some guardrails in place. And, of course, it's harder after the fact. But nevertheless, we do have to do that. SHAPIRO: Hany Farid of the University of California, Berkeley. We are going to hang onto this interview, and 10 years from now, we'll ask you back and see how you did. FARID: Thanks so much. ARI SHAPIRO, HOST:  Well, our next guest is someone who's done a lot of work on how to detect that last problem of deepfake videos. We have asked Hany Farid of the University of California, Berkeley to join us now to discuss how technology may develop over the next decade. Welcome back to All Tech Considered. HANY FARID: Good to be here, Ari. SHAPIRO: We're ending this decade on a note of uncertainty and skepticism about technology that might not have been there 10 years ago for whatever reason. Can I just begin by getting a quick sense of how you are feeling about technology heading into 2020? FARID: I think you're right that the last decade has been disappointing, and I think we are frustrated and worried about the direction we are moving in in the industry and how technology is affecting our lives day to day. I think we're reaching the low point. I don't think we're quite at it. But I hope that after that low point, we start to climb back uphill and start to take advantage of all the wonderful things about technology while mitigating many of the awful things that we've seen over the last. . . SHAPIRO: So you're an optimist. FARID: I am an optimist, and I'm a technologist. And I want technology to be great aspects of our lives. I think now the tables have turned a little bit, and I think that we are seeing more downside than upside. But I think that that's fixable with some thought and some regulation and some corporate changes and some new technologies. SHAPIRO: I realize that when I ask you to imagine the next decade in technology, we could be talking about anything from social media. . . FARID: Yeah. SHAPIRO: . . . To health, manufacturing, vehicles, clean energy. Just to get specific, I'd like to hear about one thing that makes you really optimistic and excited and one thing that makes you feel a little bit of dread. Start by telling us something that makes you hopeful. SHAPIRO: Yeah. Where I'm hopeful is the use of technology in medicine. I think one of the most exciting things we are seeing with the advances in machine learning and deep learning and AI is in rapid advances in everything from early detection of cancer to treatments because of the big data. And I think that's really exciting, and I think it has real potential in the next decade. For the dread, I mean, I can go on and on and on. FARID: (Laughter). FARID: But let me. . . SHAPIRO: Give us one thing that. . . FARID: Let me tell you why. SHAPIRO: Yeah. FARID: The thing that worries me most is the dis- and misinformation campaigns - is the sort of the polluting of the online ecosystem of information with absolute garbage. And I think these are real existential threats to our democracy. And I think we are far, far from being ready to deal with it from a technological perspective and from a legislative and regulatory perspective. And I'm very worried about the upcoming elections and the next round and the next round. SHAPIRO: What about the physical, human relationship to technology? Do you think that in the next 10 years, the line between ourselves and the technology that we use will begin to blur? FARID: I would argue it's already blurring. I mean, the cell phone, the mobile device is now ubiquitous. If you go out and - you go to an airport, for example, 99% of the people are glued to a device. They are now part of our lives for better or worse. And what's next? Probably the next step in the next decade is going to be augmented reality, where we all have glasses where all the information is coming in simultaneously and will start to intermingle with the real world in an interesting and maybe dystopian way. I think after that, it's hard to say because, you know, that next step, that next 10 years, is almost impossible to predict. SHAPIRO: As you've pointed out, right now, there are important areas where we don't have good guardrails. . . FARID: Yeah. SHAPIRO: . . . Whether that's about companies tracking us through our devices or countries developing autonomous weapons, killer robots. Do you see that changing? Do you think regulation of technology in the next 10 years will be more effective? FARID: I don't know if it'll be more effective, but I'm sure that there will be regulation coming. So we saw that out of Europe with protections of privacy. I will tell you, having spent time in Brussels and in the U. K. and on Capitol Hill, there was a lot of discussion about regulating tech - everything from breaking up what is a monopoly in some cases to forcing these companies to be better at consumer protection and everything from privacy to security. And I think that that regulation is coming. The hope, of course, is that we regulate in a sensible and smart way that is adaptive to a very fast-moving technology space. But I think that is coming in the next decade. We have been absolutely absent around the world in regulating the technology sector, and it has led to the mess that we are in now. And now we have to start putting some guardrails in place. And, of course, it's harder after the fact. But nevertheless, we do have to do that. SHAPIRO: Hany Farid of the University of California, Berkeley. We are going to hang onto this interview, and 10 years from now, we'll ask you back and see how you did. FARID: Thanks so much.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-25-791351666": {"title": "How Tech Has Changed Our Lives In The Last 10 Years : NPR", "url": "https://www.npr.org/2019/12/25/791351666/how-tech-has-changed-our-lives-in-the-last-10-years", "author": "No author found", "published_date": "2019-12-25", "content": "ARI SHAPIRO, HOST: From NPR News, this is ALL THINGS CONSIDERED. I'm Ari Shapiro. And. . . (SOUNDBITE OF ARCHIVED NPR BROADCAST)MICHELE NORRIS: I'm Michele Norris. MELISSA BLOCK: I'm Melissa Block, and it's time now for All Tech Considered. (SOUNDBITE OF MUSIC)SHAPIRO: That is how the first All Tech Considered of the decade began on January 4, 2010. Sad to say, Michele and Melissa are not here in the studio with me this Christmas Day. A lot has changed since then - hosts of this program and technology. In a minute, we'll look ahead to the next decade in tech. Before we do, let's revisit this one. We asked three experts to pick what they see as the most significant ways tech has changed our lives since 2009. The most obvious advancement was the smartphone. They were around in 2009, but now 81% of Americans own one. And technology reporter Omar Gallaga says they've almost become an appendage. OMAR GALLAGA: I was in a restaurant over the weekend, and, you know, just across from me, a woman's phone was going off every five seconds. You know, I heard a chime or an alert - you know, different alerts out of her phone every - and I'm, like, how is she functioning as a human in this world? You know, she didn't even notice that those alerts were going off. I mean, that's how much a part of her life they must be, so. . . SHAPIRO: And it's not just the phones. Gallaga's daughters, who are 10 and 12, have tablets. He says kids these days just go through life differently from a decade ago. GALLAGA: When you see kids at the doctor's office looking at magazines, and they're trying to, like, you know, scroll the page (laughter), the print page - like, yeah. Yep, that's an iPad kid right there. SHAPIRO: Next, we turn to Erin Hatton. She is an associate professor of sociology at the University at Buffalo. And for her, the most significant change of the decade is something that would not have been possible without the smartphone - it's the gig economy, enabled by apps like Uber, TaskRabbit and Airbnb. Hatton says they've redefined what it is to be a worker. ERIN HATTON: I think that this work has started new conversations between workers and across sectors in rethinking what it means to be a worker and potentially rethinking what kinds of benefits and protections we attach to work. SHAPIRO: Beyond our daily lives or our work, tech in the last decade has also shaped what we believe to be true. Sometimes we know we're being faked, like in Martin Scorsese's movie \"The Irishman. \" Robert De Niro and Joe Pesci, who are both in their 70s, look much younger thanks to technology. In some scenes, they're in their 40s. (SOUNDBITE OF FILM, \"THE IRISHMAN\")JOE PESCI: (As Russell Bufalino) What's the problem, kid? ROBERT DE NIRO: (As Frank Sheeran) I don't know. It sounds funny - stops and starts and loses power. PESCI: (As Russell Bufalino) I can give you a hand. SHAPIRO: It's believable. Of course, there is a darker side of this technology, too. Michael Fink is a professor of Cinematic Arts at the University of Southern California, and he says it's easier than ever before to manipulate videos and make it seem like something happened when it never did. MICHAEL FINK: The software has become so powerful that things can be altered, changed, modified so quickly that people would think, oh, my god. That has to be real. It just happened. And that's not true. The reality is fungible. It can be used by dark forces - let's put it that way - people with absolutely nothing but malevolence at the core of their being. And it's scary. ARI SHAPIRO, HOST:  From NPR News, this is ALL THINGS CONSIDERED. I'm Ari Shapiro. And. . . (SOUNDBITE OF ARCHIVED NPR BROADCAST) MICHELE NORRIS: I'm Michele Norris. MELISSA BLOCK: I'm Melissa Block, and it's time now for All Tech Considered. (SOUNDBITE OF MUSIC) SHAPIRO: That is how the first All Tech Considered of the decade began on January 4, 2010. Sad to say, Michele and Melissa are not here in the studio with me this Christmas Day. A lot has changed since then - hosts of this program and technology. In a minute, we'll look ahead to the next decade in tech. Before we do, let's revisit this one. We asked three experts to pick what they see as the most significant ways tech has changed our lives since 2009. The most obvious advancement was the smartphone. They were around in 2009, but now 81% of Americans own one. And technology reporter Omar Gallaga says they've almost become an appendage. OMAR GALLAGA: I was in a restaurant over the weekend, and, you know, just across from me, a woman's phone was going off every five seconds. You know, I heard a chime or an alert - you know, different alerts out of her phone every - and I'm, like, how is she functioning as a human in this world? You know, she didn't even notice that those alerts were going off. I mean, that's how much a part of her life they must be, so. . . SHAPIRO: And it's not just the phones. Gallaga's daughters, who are 10 and 12, have tablets. He says kids these days just go through life differently from a decade ago. GALLAGA: When you see kids at the doctor's office looking at magazines, and they're trying to, like, you know, scroll the page (laughter), the print page - like, yeah. Yep, that's an iPad kid right there. SHAPIRO: Next, we turn to Erin Hatton. She is an associate professor of sociology at the University at Buffalo. And for her, the most significant change of the decade is something that would not have been possible without the smartphone - it's the gig economy, enabled by apps like Uber, TaskRabbit and Airbnb. Hatton says they've redefined what it is to be a worker. ERIN HATTON: I think that this work has started new conversations between workers and across sectors in rethinking what it means to be a worker and potentially rethinking what kinds of benefits and protections we attach to work. SHAPIRO: Beyond our daily lives or our work, tech in the last decade has also shaped what we believe to be true. Sometimes we know we're being faked, like in Martin Scorsese's movie \"The Irishman. \" Robert De Niro and Joe Pesci, who are both in their 70s, look much younger thanks to technology. In some scenes, they're in their 40s. (SOUNDBITE OF FILM, \"THE IRISHMAN\") JOE PESCI: (As Russell Bufalino) What's the problem, kid? ROBERT DE NIRO: (As Frank Sheeran) I don't know. It sounds funny - stops and starts and loses power. PESCI: (As Russell Bufalino) I can give you a hand. SHAPIRO: It's believable. Of course, there is a darker side of this technology, too. Michael Fink is a professor of Cinematic Arts at the University of Southern California, and he says it's easier than ever before to manipulate videos and make it seem like something happened when it never did. MICHAEL FINK: The software has become so powerful that things can be altered, changed, modified so quickly that people would think, oh, my god. That has to be real. It just happened. And that's not true. The reality is fungible. It can be used by dark forces - let's put it that way - people with absolutely nothing but malevolence at the core of their being. And it's scary.", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-25-784893309": {"title": "Smart Speakers Listen To Us, But Do They Hear Us All The Same? : NPR", "url": "https://www.npr.org/2019/12/25/784893309/would-you-lend-your-voice-to-our-experiment", "author": "No author found", "published_date": "2019-12-25", "content": "", "section": "Technology", "disclaimer": ""}, "2019-12-26-791414982": {"title": "How Pilots Interact With Automation : NPR", "url": "https://www.npr.org/2019/12/26/791414982/how-pilots-interact-with-automation", "author": "No author found", "published_date": "2019-12-26", "content": "DAVID GREENE, HOST: Here's another question we're asking this morning - are commercial jetliners becoming too automated for today's pilots? This is an increasingly common concern following the crashes of two 737 Max airplanes caused in large part by Boeing's flawed design of a new automated flight control system. But investigators also found other contributing factors here, including faulty assumptions about how pilots would react and poor pilot training. NPR's David Schaper looks at efforts to improve how pilots interact with automation. DAVID SCHAPER, BYLINE: In the crashes of Lion Air Flight 610 and Ethiopian Airlines Flight 302, an automated flight control system called MCAS did exactly what it was supposed to do. An angle-of-attack sensor indicated the planes were pitched too high, so it repeatedly pointed the noses down. But the sensors were wrong, and automation forced the planes into nosedives. Investigators say the pilots fought the systems but didn't fully understand what was happening amid a confusing cacophony of warning alarms and alerts. They could not regain control, and the planes crashed, killing a total of 346 people. CLINT BALOG: Automation is very much a double-edged sword. SCHAPER: Clint Balog is a former test pilot an engineer who now teaches at Embry-Riddle Aeronautical University. He says automation has undeniably made commercial aviation safer. . . BALOG: But it also brings with it a lot of new issues that we've never faced in the cockpit before. And those issues are related not so much to the automation itself but to the human's interaction with automation. SCHAPER: In the case of the 737 Max crashes, former National Transportation Safety Board chairman Christopher Hart puts it this way. CHRISTOPHER HART: That means this is not just an airplane problem. This is an airplane/pilot problem. SCHAPER: Hart recently headed a panel of international aviation safety experts to examine the design and certification processes of the 737 Max. And he warns that these kinds of automation problems will likely happen again. HART: Because as automation becomes more and more complex, pilots are less likely to fully understand it and more likely to have problems. And not only that, they're more likely to encounter scenarios in real operations that they haven't seen even in the simulator, so they don't know how to respond. THOMAS SHERRINGHAM: Parking brakes off. Got the lights up. Cabin's ready, and we're good for takeoff. SCHAPER: Those complexities are on full display here, in this simulator of an Airbus A320 cockpit at Purdue University in West Lafayette, Ind. Senior student Naveen Breen and instructor Thomas Sherringham go through the procedures to take off. SHERRINGHAM: All right, V1, rotate. V2, positive climb. Gears up. SCHAPER: Sherringham explains what's going on. SHERRINGHAM: So right now, we just did a departure out of Denver. Fully automated, we engage the autopilot at 500 feet. SCHAPER: He and Breen explain how smoothly the automation takes over and how the many screens and lights in front of them monitor speed, altitude, as well as all of the automated systems. Naveen Breen says you have to know exactly what the automation is doing and stay one step ahead of it. NAVEEN BREEN: The Airbus is always trying to figure out what you want to do next, all right? And it's going to try and present you with that information constantly. And it can be incredibly helpful. It really helps reduce the workload on the pilot. SCHAPER: Purdue aviation professor and pilot Mike Suckow says the pilot isn't really flying the plane so much as managing the systems that fly the plane. MIKE SUCKOW: The pilot is now scaffolding. He needs airmanship, but he also needs a higher level of understanding the system. You know, it's a digital airplane. How you interface with it, how you communicate with it, how you change controls and all that stuff is a little bit different. SCHAPER: Suckow says in that regard, the younger generation has an advantage. SUCKOW: The students today that have been raised with the iPads and touch screens and automation are very fluent in transitioning into this airplane. SCHAPER: But Suckow says they don't always understand the why of aeronautics and avionics, so training has to better incorporate the wisdom of previous generations with the technology of today. SUCKOW: We have to move from the training environment of traditional pulleys and cables and stick and rudder to an automated airplane where it's fly by wire. And so that's a computer input or a code that's moving a controlled surface. SCHAPER: A recent industry study found that three of the last four years have been the safest ever for air travel, but it also labels overreliance on aircraft automation systems as an emerging risk for pilots. That could lead to a loss of situational awareness and confusion. (SOUNDBITE OF ARCHIVED RECORDING)MICA ENDSLEY: Automation confusion is a frequent challenge in aviation accidents, and it was a central problem on the Max 8. SCHAPER: Mica Endsley is a former chief scientist of the U. S. Air Force. In recent congressional testimony, she told lawmakers that pilots must be better trained to be able to step in to fly the plane if automation fails. But even before that, she says, safety starts at the drawing board. (SOUNDBITE OF ARCHIVED RECORDING)ENDSLEY: The first thing you want to do is design the system appropriately because you - it's very hard to train for bad designs. SCHAPER: That means airplane designers need to better take into account human capabilities and limitations in how future pilots perceive, think, move and react because while pilots still play an essential role in commercial airline flight, complex airplane automation is here to stay. David Schaper, NPR News. (SOUNDBITE OF HOLLOW CLOUDS' \"CHERRY PIE JAM\") DAVID GREENE, HOST:  Here's another question we're asking this morning - are commercial jetliners becoming too automated for today's pilots? This is an increasingly common concern following the crashes of two 737 Max airplanes caused in large part by Boeing's flawed design of a new automated flight control system. But investigators also found other contributing factors here, including faulty assumptions about how pilots would react and poor pilot training. NPR's David Schaper looks at efforts to improve how pilots interact with automation. DAVID SCHAPER, BYLINE: In the crashes of Lion Air Flight 610 and Ethiopian Airlines Flight 302, an automated flight control system called MCAS did exactly what it was supposed to do. An angle-of-attack sensor indicated the planes were pitched too high, so it repeatedly pointed the noses down. But the sensors were wrong, and automation forced the planes into nosedives. Investigators say the pilots fought the systems but didn't fully understand what was happening amid a confusing cacophony of warning alarms and alerts. They could not regain control, and the planes crashed, killing a total of 346 people. CLINT BALOG: Automation is very much a double-edged sword. SCHAPER: Clint Balog is a former test pilot an engineer who now teaches at Embry-Riddle Aeronautical University. He says automation has undeniably made commercial aviation safer. . . BALOG: But it also brings with it a lot of new issues that we've never faced in the cockpit before. And those issues are related not so much to the automation itself but to the human's interaction with automation. SCHAPER: In the case of the 737 Max crashes, former National Transportation Safety Board chairman Christopher Hart puts it this way. CHRISTOPHER HART: That means this is not just an airplane problem. This is an airplane/pilot problem. SCHAPER: Hart recently headed a panel of international aviation safety experts to examine the design and certification processes of the 737 Max. And he warns that these kinds of automation problems will likely happen again. HART: Because as automation becomes more and more complex, pilots are less likely to fully understand it and more likely to have problems. And not only that, they're more likely to encounter scenarios in real operations that they haven't seen even in the simulator, so they don't know how to respond. THOMAS SHERRINGHAM: Parking brakes off. Got the lights up. Cabin's ready, and we're good for takeoff. SCHAPER: Those complexities are on full display here, in this simulator of an Airbus A320 cockpit at Purdue University in West Lafayette, Ind. Senior student Naveen Breen and instructor Thomas Sherringham go through the procedures to take off. SHERRINGHAM: All right, V1, rotate. V2, positive climb. Gears up. SCHAPER: Sherringham explains what's going on. SHERRINGHAM: So right now, we just did a departure out of Denver. Fully automated, we engage the autopilot at 500 feet. SCHAPER: He and Breen explain how smoothly the automation takes over and how the many screens and lights in front of them monitor speed, altitude, as well as all of the automated systems. Naveen Breen says you have to know exactly what the automation is doing and stay one step ahead of it. NAVEEN BREEN: The Airbus is always trying to figure out what you want to do next, all right? And it's going to try and present you with that information constantly. And it can be incredibly helpful. It really helps reduce the workload on the pilot. SCHAPER: Purdue aviation professor and pilot Mike Suckow says the pilot isn't really flying the plane so much as managing the systems that fly the plane. MIKE SUCKOW: The pilot is now scaffolding. He needs airmanship, but he also needs a higher level of understanding the system. You know, it's a digital airplane. How you interface with it, how you communicate with it, how you change controls and all that stuff is a little bit different. SCHAPER: Suckow says in that regard, the younger generation has an advantage. SUCKOW: The students today that have been raised with the iPads and touch screens and automation are very fluent in transitioning into this airplane. SCHAPER: But Suckow says they don't always understand the why of aeronautics and avionics, so training has to better incorporate the wisdom of previous generations with the technology of today. SUCKOW: We have to move from the training environment of traditional pulleys and cables and stick and rudder to an automated airplane where it's fly by wire. And so that's a computer input or a code that's moving a controlled surface. SCHAPER: A recent industry study found that three of the last four years have been the safest ever for air travel, but it also labels overreliance on aircraft automation systems as an emerging risk for pilots. That could lead to a loss of situational awareness and confusion. (SOUNDBITE OF ARCHIVED RECORDING) MICA ENDSLEY: Automation confusion is a frequent challenge in aviation accidents, and it was a central problem on the Max 8. SCHAPER: Mica Endsley is a former chief scientist of the U. S. Air Force. In recent congressional testimony, she told lawmakers that pilots must be better trained to be able to step in to fly the plane if automation fails. But even before that, she says, safety starts at the drawing board. (SOUNDBITE OF ARCHIVED RECORDING) ENDSLEY: The first thing you want to do is design the system appropriately because you - it's very hard to train for bad designs. SCHAPER: That means airplane designers need to better take into account human capabilities and limitations in how future pilots perceive, think, move and react because while pilots still play an essential role in commercial airline flight, complex airplane automation is here to stay. David Schaper, NPR News. (SOUNDBITE OF HOLLOW CLOUDS' \"CHERRY PIE JAM\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-27-791918140": {"title": "How Colleges Are Using Tech To Keep Track Of Students : NPR", "url": "https://www.npr.org/2019/12/27/791918140/how-colleges-are-using-tech-to-keep-track-of-students", "author": "No author found", "published_date": "2019-12-27", "content": "AILSA CHANG, HOST: If you've got a smartphone or a Facebook account, if you shop online or use a ride-sharing app, companies are collecting a lot of data about you all the time. If you are a college student, your school may be doing the exact same thing. Drew Harwell at The Washington Post took a deep look at the technology that universities are using to track students. Welcome. DREW HARWELL: Thank you. CHANG: So let's just start with a snapshot of what surveillance on a college campus actually looks like. Can you just paint a quick picture for us? HARWELL: So it all revolves around that phone that students have in their pocket. When they go into certain classrooms, there will be these Bluetooth beacons installed around the room that can log whether they have attended the class, whether they've got in late, whether they left in the middle of the class. CHANG: Wow. HARWELL: All that data ends up going back to the college. And then sometimes when they leave those rooms, there will be Wi-Fi access points that can track wherever they go on campus 24 hours a day. And, you know, for some students, it's up to 6,000 location data points every day. CHANG: You write that most of these schools use this technology simply as a way to monitor class attendance. But what else are schools using this data for? HARWELL: Some of these schools are taking it all the way to the point where they feel like, hey, if we have enough precise location data on people, we can start telling maybe whether they're going through a mental health crisis. Maybe if a student is not leaving the dorms enough or was going to the library all the time and then stopped, maybe we can use that data and send out an RA or an adviser to their room and say, hey, what's going on? And so some of these colleges are using these systems that divine out these risk scores for certain students to say the system is watching, they see something is amiss with your location. What's going on, and what can we do to respond to that? CHANG: And do we have a sense yet of how accurate those so-called risk scores are at pinpointing students who are going through hard times? HARWELL: That's the big question. You know, colleges seem to think if we only knew more, right? If we only had more data on where these students were going, we could have this perfect pinpoint, you know, accuracy on just what's going on in their mind. But from a lot of the AI experts and privacy people I talked to, they feel like this system is just going to be full of errors. There's going to be so much context that these systems don't pick up, so many other explanations that could be behind why you. . . CHANG: Yeah. HARWELL: . . . Didn't go to the library on a certain day. And so they just feel like the result is going to end up putting students in a tough position that they never really deserved. CHANG: Now, how widespread is this? How widespread is it that universities are collecting this level of data on students? HARWELL: So I looked at two specific companies that have been working with a number of colleges. Between them, it's about 60 schools. And these run from huge, you know, state universities in the South that have 50,000, 60,000 students to small, private colleges, even one high school. So it's definitely growing, especially considering that this technology is fairly recent. . . CHANG: Yeah. HARWELL: . . . In sort of a college, and it's only been happening for the last couple years. CHANG: And how aware are students that this level of surveillance is happening on college campuses? HARWELL: That was, I think, one of the most concerning things I found, was that a lot of the students just had no idea what was really happening, right? There were the students who had learned of it through social media or a friend and were really kind of upset over the privacy aspect. But then there are a whole slew of students who just had no idea, right? They got - they put the app on their phone. They click the checkbox to accept a privacy policy, like we all do, and had no real idea of how closely these systems were monitoring them. And so that's a worry. Like, these students are just shedding so much intimate information about where they go every day, and they're not even really realizing the scope of what that entails. CHANG: Drew Harwell is a technology reporter for The Washington Post. Thank you very much for joining us today. HARWELL: Thank you. (SOUNDBITE OF JESPER RYOM'S \"PACER\") AILSA CHANG, HOST:  If you've got a smartphone or a Facebook account, if you shop online or use a ride-sharing app, companies are collecting a lot of data about you all the time. If you are a college student, your school may be doing the exact same thing. Drew Harwell at The Washington Post took a deep look at the technology that universities are using to track students. Welcome. DREW HARWELL: Thank you. CHANG: So let's just start with a snapshot of what surveillance on a college campus actually looks like. Can you just paint a quick picture for us? HARWELL: So it all revolves around that phone that students have in their pocket. When they go into certain classrooms, there will be these Bluetooth beacons installed around the room that can log whether they have attended the class, whether they've got in late, whether they left in the middle of the class. CHANG: Wow. HARWELL: All that data ends up going back to the college. And then sometimes when they leave those rooms, there will be Wi-Fi access points that can track wherever they go on campus 24 hours a day. And, you know, for some students, it's up to 6,000 location data points every day. CHANG: You write that most of these schools use this technology simply as a way to monitor class attendance. But what else are schools using this data for? HARWELL: Some of these schools are taking it all the way to the point where they feel like, hey, if we have enough precise location data on people, we can start telling maybe whether they're going through a mental health crisis. Maybe if a student is not leaving the dorms enough or was going to the library all the time and then stopped, maybe we can use that data and send out an RA or an adviser to their room and say, hey, what's going on? And so some of these colleges are using these systems that divine out these risk scores for certain students to say the system is watching, they see something is amiss with your location. What's going on, and what can we do to respond to that? CHANG: And do we have a sense yet of how accurate those so-called risk scores are at pinpointing students who are going through hard times? HARWELL: That's the big question. You know, colleges seem to think if we only knew more, right? If we only had more data on where these students were going, we could have this perfect pinpoint, you know, accuracy on just what's going on in their mind. But from a lot of the AI experts and privacy people I talked to, they feel like this system is just going to be full of errors. There's going to be so much context that these systems don't pick up, so many other explanations that could be behind why you. . . CHANG: Yeah. HARWELL: . . . Didn't go to the library on a certain day. And so they just feel like the result is going to end up putting students in a tough position that they never really deserved. CHANG: Now, how widespread is this? How widespread is it that universities are collecting this level of data on students? HARWELL: So I looked at two specific companies that have been working with a number of colleges. Between them, it's about 60 schools. And these run from huge, you know, state universities in the South that have 50,000, 60,000 students to small, private colleges, even one high school. So it's definitely growing, especially considering that this technology is fairly recent. . . CHANG: Yeah. HARWELL: . . . In sort of a college, and it's only been happening for the last couple years. CHANG: And how aware are students that this level of surveillance is happening on college campuses? HARWELL: That was, I think, one of the most concerning things I found, was that a lot of the students just had no idea what was really happening, right? There were the students who had learned of it through social media or a friend and were really kind of upset over the privacy aspect. But then there are a whole slew of students who just had no idea, right? They got - they put the app on their phone. They click the checkbox to accept a privacy policy, like we all do, and had no real idea of how closely these systems were monitoring them. And so that's a worry. Like, these students are just shedding so much intimate information about where they go every day, and they're not even really realizing the scope of what that entails. CHANG: Drew Harwell is a technology reporter for The Washington Post. Thank you very much for joining us today. HARWELL: Thank you. (SOUNDBITE OF JESPER RYOM'S \"PACER\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-28-792078881": {"title": "Spotify Becomes Latest Tech Company To Hit Pause On Political Ads : NPR", "url": "https://www.npr.org/2019/12/28/792078881/spotify-becomes-latest-tech-company-to-hit-on-pause-political-ads", "author": "No author found", "published_date": "2019-12-28", "content": "", "section": "Technology", "disclaimer": ""}, "2019-12-30-791190150": {"title": "California Rings In The New Year With A New Data Privacy Law : NPR", "url": "https://www.npr.org/2019/12/30/791190150/california-rings-in-the-new-year-with-a-new-data-privacy-law", "author": "No author found", "published_date": "2019-12-30", "content": "MARY LOUISE KELLY, HOST: Time now for All Tech Considered. (SOUNDBITE OF MUSIC)KELLY: When the new year begins tomorrow, the country's toughest data privacy law will go into effect. It's a state law in California. But with no federal law on the horizon, it is expected to set the national standard. Rachael Myrow of member station KQED in San Francisco has details. RACHAEL MYROW, BYLINE: It's hard to find a better authority on the California consumer privacy act than one of its co-authors, Mary Stone Ross. She doesn't have to reach to give you the highlights. MARY STONE ROSS: On January 1, 2020, all Californians will be able to find out what personal information a business is collecting about them, their devices and their children. They'll be able to opt out of the sale of their personal information, and if a company fails to implement reasonable security practices and their personal information is breached, they'll be allowed to sue those companies. MYROW: Companies can still collect the data - what you buy, where you go and when, all the photos you've ever taken, your emails, even the ones you deleted, etc. What companies must do now is tell you what they're collecting when you ask. And what companies can't do anymore legally is sell that data if you tell them not to. And if they do anyway, can you sue? No, not for that. STONE ROSS: It's only for data breaches. So if certain categories of personal information, for example, your social security number, are breached and a business fails to implement reasonable security practices, then you have cause. MYROW: Also California's attorney general can prosecute after the law goes into effect January 1. MYROW: Right after that first kiss and the hugs and the champagne, the law's in effect. MYROW: That's Attorney General Xavier Becerra. But his budget is limited. He's already said his office is likely to conduct only three enforcement actions a year. Against who, he won't say yet. XAVIER BECERRA: The bigger the company, probably the bigger the problem. The bigger the universe that has data that is used in certain ways that could lead to that violation, the bigger the case will be. MYROW: Industry groups spent the last year trying to rewrite and soften the law. It's expected they'll sue to stop its rollout in the new year. In the meantime, some companies, like Microsoft, are adopting the new rules right away and across the nation. Other companies - not so much. In a post outlining its position, for instance, Facebook argues it doesn't sell your data. It sells advertisers access to you. It's up to the advertisers to let you opt out or not. You can almost see Chris Hoofnagle, who teaches tech regulation at UC Berkeley, rolling his eyes. CHRIS HOOFNAGLE: Facebook, in particular, appears to be interpreting the law in a very opportunistic way so that they don't need to actually do anything to comply with it. MYROW: Hoofnagle thinks the biggest tech companies in Silicon Valley are in a financial position to bet it'll be a while before the attorney general's office comes for them. And in the meantime, the money is no joke. Facebook alone makes billions annually providing advertisers access to users. HOOFNAGLE: Enforcement is the big unknown here. But Facebook will be in trouble if the attorney general picks up the law and uses it. MYROW: Other data privacy laws like this one are expected to crop up in other states, too, because there is no federal law. In California, the attorney general's office is expected to finalize its regulations and begin enforcement July 1. For NPR News, I'm Rachael Myrow in San Francisco. (SOUNDBITE OF THE HELIO SEQUENCE'S \"BATTLE LINES\") MARY LOUISE KELLY, HOST:  Time now for All Tech Considered. (SOUNDBITE OF MUSIC) KELLY: When the new year begins tomorrow, the country's toughest data privacy law will go into effect. It's a state law in California. But with no federal law on the horizon, it is expected to set the national standard. Rachael Myrow of member station KQED in San Francisco has details. RACHAEL MYROW, BYLINE: It's hard to find a better authority on the California consumer privacy act than one of its co-authors, Mary Stone Ross. She doesn't have to reach to give you the highlights. MARY STONE ROSS: On January 1, 2020, all Californians will be able to find out what personal information a business is collecting about them, their devices and their children. They'll be able to opt out of the sale of their personal information, and if a company fails to implement reasonable security practices and their personal information is breached, they'll be allowed to sue those companies. MYROW: Companies can still collect the data - what you buy, where you go and when, all the photos you've ever taken, your emails, even the ones you deleted, etc. What companies must do now is tell you what they're collecting when you ask. And what companies can't do anymore legally is sell that data if you tell them not to. And if they do anyway, can you sue? No, not for that. STONE ROSS: It's only for data breaches. So if certain categories of personal information, for example, your social security number, are breached and a business fails to implement reasonable security practices, then you have cause. MYROW: Also California's attorney general can prosecute after the law goes into effect January 1. MYROW: Right after that first kiss and the hugs and the champagne, the law's in effect. MYROW: That's Attorney General Xavier Becerra. But his budget is limited. He's already said his office is likely to conduct only three enforcement actions a year. Against who, he won't say yet. XAVIER BECERRA: The bigger the company, probably the bigger the problem. The bigger the universe that has data that is used in certain ways that could lead to that violation, the bigger the case will be. MYROW: Industry groups spent the last year trying to rewrite and soften the law. It's expected they'll sue to stop its rollout in the new year. In the meantime, some companies, like Microsoft, are adopting the new rules right away and across the nation. Other companies - not so much. In a post outlining its position, for instance, Facebook argues it doesn't sell your data. It sells advertisers access to you. It's up to the advertisers to let you opt out or not. You can almost see Chris Hoofnagle, who teaches tech regulation at UC Berkeley, rolling his eyes. CHRIS HOOFNAGLE: Facebook, in particular, appears to be interpreting the law in a very opportunistic way so that they don't need to actually do anything to comply with it. MYROW: Hoofnagle thinks the biggest tech companies in Silicon Valley are in a financial position to bet it'll be a while before the attorney general's office comes for them. And in the meantime, the money is no joke. Facebook alone makes billions annually providing advertisers access to users. HOOFNAGLE: Enforcement is the big unknown here. But Facebook will be in trouble if the attorney general picks up the law and uses it. MYROW: Other data privacy laws like this one are expected to crop up in other states, too, because there is no federal law. In California, the attorney general's office is expected to finalize its regulations and begin enforcement July 1. For NPR News, I'm Rachael Myrow in San Francisco. (SOUNDBITE OF THE HELIO SEQUENCE'S \"BATTLE LINES\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2019-12-31-792736845": {"title": "Will A New Law To Curb Robocalls Make A Difference?  : NPR", "url": "https://www.npr.org/2019/12/31/792736845/will-a-new-law-to-curb-robocalls-make-a-difference", "author": "No author found", "published_date": "2019-12-31", "content": "AILSA CHANG, HOST: If you have a phone, you have probably gotten your share of robocalls. As universal and universally heated as they are, they are very hard to stop. Lawmakers and phone companies have tried. And the latest effort is a bill signed into law just yesterday by President Trump. For more on what this law would do, I'm joined now by Wall Street Journal reporter Sarah Krouse. Welcome. SARAH KROUSE: Thank you. Thanks for having me. CHANG: All right. So I guess the million-dollar question is, will this new law make a real difference? KROUSE: This new law will not stop your phone from ringing immediately. CHANG: Great. KROUSE: This is something that gives the FCC greater power to pursue wrongdoing. It gives them more time to investigate nefarious scamming robocalls. It gives carriers, you know, sort of a nudge forward to continue to do more to block services free of charge to consumers. So on that front, there's a lot of sort of incremental progress in getting at the problem, but there - because there is no one solution, this isn't going to stop them across the board. CHANG: Can we at least expect robocalls to slow down as a result of this new law? Or you're just pretty much saying, immediately, there will be no visible effect. KROUSE: So I think what the rate of robocalls will depend on is how aggressive carriers actually are. So they traditionally could have gotten in trouble for blocking calls, and that made them hesitant to really aggressively block robocalls until this year. The FCC gave them more latitude to do so, and this legislation really sort of pushes carriers to offer call-blocking services free of charge, on either an opt-in or an opt-out basis, to all their customers. CHANG: Right, because the concern for a lot of these phone companies was that they might get sued if they blocked too many calls, and this law would protect them from being sued. But do you think that is actually going to work? KROUSE: I think it really depends on what type of traffic each carrier decides to label as suspect, and the reason why this gets tricky is there's this broad spectrum of robocalls in the world. There's the sort of benign ones - which is a school blasting out calls to parents to say there's a snow day. There's the prescription notification that you can go to the pharmacy to get it. And then there's this sort of middle ground with businesses - you know, maybe it's a bank, maybe it's a debt collector - you know, that has a right to contact their customer, but the customer may not want to get those calls. And then there's the really shady stuff, which is the scam and the fraudulent activity. CHANG: Right. KROUSE: But a lot of those calls rely on the same sort of technology and process to be executed. And so you're looking - you're asking the carriers to look at their call traffic, spot the bad stuff, and block it and make a judgment call on the sort of in-between. So, you know, for example, emergency calls - what constitutes an emergency call? How do you identify that and make sure you don't block it? These are some of the sort of thorny questions. CHANG: Now, the FCC has said that stopping robocalls would be one of its main priorities. What have been its biggest challenges in fixing this problem? KROUSE: So one of them is the sort of whack-a-mole problem, let's call it, because it's really cheap and easy to buy a large block of numbers and blast out millions of phone calls. And it's really easy for a bad actor, once they think they're under scrutiny, to sort of close down and pop up under a new name. And so even finding them is difficult. And until recently, the FCC had just a year to bring a robocall-related case. This new legislation expands that to up to four. Another problem that the FCC has had in the past is the ability to collect the fines that they levy. So they've levied, you know, a couple hundred million dollars in fines against bad actors for robocall-related activity, but they've only been able to collect a small portion of that - less than $7,000, and that's partly because they can only seize the assets that a company or an individual has that they can find. So the net effect of that isn't quite as much of a deterrent as it might look. . . CHANG: Right. KROUSE: . . . If you look just at the headline fine. CHANG: That is Sarah Krouse who covers telecommunications for The Wall Street Journal. Thank you very much. KROUSE: Thanks for having me. (SOUNDBITE OF MUSIC) AILSA CHANG, HOST:  If you have a phone, you have probably gotten your share of robocalls. As universal and universally heated as they are, they are very hard to stop. Lawmakers and phone companies have tried. And the latest effort is a bill signed into law just yesterday by President Trump. For more on what this law would do, I'm joined now by Wall Street Journal reporter Sarah Krouse. Welcome. SARAH KROUSE: Thank you. Thanks for having me. CHANG: All right. So I guess the million-dollar question is, will this new law make a real difference? KROUSE: This new law will not stop your phone from ringing immediately. CHANG: Great. KROUSE: This is something that gives the FCC greater power to pursue wrongdoing. It gives them more time to investigate nefarious scamming robocalls. It gives carriers, you know, sort of a nudge forward to continue to do more to block services free of charge to consumers. So on that front, there's a lot of sort of incremental progress in getting at the problem, but there - because there is no one solution, this isn't going to stop them across the board. CHANG: Can we at least expect robocalls to slow down as a result of this new law? Or you're just pretty much saying, immediately, there will be no visible effect. KROUSE: So I think what the rate of robocalls will depend on is how aggressive carriers actually are. So they traditionally could have gotten in trouble for blocking calls, and that made them hesitant to really aggressively block robocalls until this year. The FCC gave them more latitude to do so, and this legislation really sort of pushes carriers to offer call-blocking services free of charge, on either an opt-in or an opt-out basis, to all their customers. CHANG: Right, because the concern for a lot of these phone companies was that they might get sued if they blocked too many calls, and this law would protect them from being sued. But do you think that is actually going to work? KROUSE: I think it really depends on what type of traffic each carrier decides to label as suspect, and the reason why this gets tricky is there's this broad spectrum of robocalls in the world. There's the sort of benign ones - which is a school blasting out calls to parents to say there's a snow day. There's the prescription notification that you can go to the pharmacy to get it. And then there's this sort of middle ground with businesses - you know, maybe it's a bank, maybe it's a debt collector - you know, that has a right to contact their customer, but the customer may not want to get those calls. And then there's the really shady stuff, which is the scam and the fraudulent activity. CHANG: Right. KROUSE: But a lot of those calls rely on the same sort of technology and process to be executed. And so you're looking - you're asking the carriers to look at their call traffic, spot the bad stuff, and block it and make a judgment call on the sort of in-between. So, you know, for example, emergency calls - what constitutes an emergency call? How do you identify that and make sure you don't block it? These are some of the sort of thorny questions. CHANG: Now, the FCC has said that stopping robocalls would be one of its main priorities. What have been its biggest challenges in fixing this problem? KROUSE: So one of them is the sort of whack-a-mole problem, let's call it, because it's really cheap and easy to buy a large block of numbers and blast out millions of phone calls. And it's really easy for a bad actor, once they think they're under scrutiny, to sort of close down and pop up under a new name. And so even finding them is difficult. And until recently, the FCC had just a year to bring a robocall-related case. This new legislation expands that to up to four. Another problem that the FCC has had in the past is the ability to collect the fines that they levy. So they've levied, you know, a couple hundred million dollars in fines against bad actors for robocall-related activity, but they've only been able to collect a small portion of that - less than $7,000, and that's partly because they can only seize the assets that a company or an individual has that they can find. So the net effect of that isn't quite as much of a deterrent as it might look. . . CHANG: Right. KROUSE: . . . If you look just at the headline fine. CHANG: That is Sarah Krouse who covers telecommunications for The Wall Street Journal. Thank you very much. KROUSE: Thanks for having me. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2019 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}}