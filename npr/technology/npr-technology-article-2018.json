{"2018-01-01-574073721": {"title": "Buying Attention | Hidden Brain  : NPR", "url": "https://www.npr.org/2018/01/01/574073721/our-mental-space-under-attack", "author": "No author found", "published_date": "2018-01-01", "content": "SHANKAR VEDANTAM, HOST: This is HIDDEN BRAIN. I'm Shankar Vedantam. As you listen right now to this podcast, you're probably surrounded by stuff - tangled ear buds, favorite sweater, half-empty water bottle, Legos scattered on the floor, a bike, plate of half-finished eggs. Try to remember what made you buy all these things. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTRESS #1: (As narrator) Hello, Moto. VEDANTAM: Your phone. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTRESS #2: (As narrator) It's time to reimagine the smartphone. VEDANTAM: Those sneakers with the frayed shoelaces. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTRESS #3: (As narrator) Skechers Shape Ups. Step into your new body. VEDANTAM: Your car. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTOR #1: (As narrator) Get on the holiday road in a Honda Civic. VEDANTAM: Newspapers, radio and TV have helped us learn about these products. But in order to serve up billions of ads, these forms of mass media have had to first create a very special product of their own. The secret product? You can't buy it in a store. You can't see it. But you are in the process of supplying it at this very second. UNIDENTIFIED MAN #1: Attention. VEDANTAM: This new product. . . UNIDENTIFIED MAN #1: Attention. Tension. Ten-ten-tension. VEDANTAM: . . . Is your attention. TIM WU: We can lose our freedom and become entrapped really by doing what we think are voluntary choices. VEDANTAM: Corporations ranging from Google to Fox News have found ways to grab your attention, package it and then make money from it. Their strategies are part of a long legacy of companies trying to capture and monetize our attention. Columbia University law professor Tim Wu calls these businesses attention merchants. Today we explore the rise of attention merchants and why Tim says the techniques they've invented pose real risks to our autonomy. (SOUNDBITE OF MUSIC)VEDANTAM: We begin today's show with an account of what may be the earliest attention merchant in history. In the early 1800s, the newspaper business in New York City was bleak. The New York Times wasn't around yet, but there were a handful of other papers, The Journal of Commerce, the Morning Courier and New York Enquirer. They typically charged six cents a copy, which was a lot of money in those days. Benjamin Day was working in newspaper printing, and he thought the business model needed a reboot. Six cents was way too much. He decided to start his own paper, The New York Sun, and sell it for one cent. Everyone thought he was crazy, but he knew something that they didn't. His strategy was one that Jeff Bezos from Amazon could appreciate. On August 25, 1835. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED NEWSBOY: Extra, extra. Read all about it. VEDANTAM: . . . The New York Sun ran a front-page story titled. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED NEWSBOY: \"Great Astronomical Discovery Is Lately Made. \"VEDANTAM: Readers learned that an astronomer in South Africa had built a telescope that could see minute details on the surface of the moon. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED NEWSBOY: Read all about it. VEDANTAM: Over the next few weeks, The Sun released a stream of new findings. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED WOMAN #1: Did you read the news? VEDANTAM: The moon contained canyons, oceans, forests. The telescope also identified a new form of life. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED GROUP: (Gasping in unison). VEDANTAM: . . . A creature with the scientific name Vespertilio-homo. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED MAN #2: I can't believe it. VEDANTAM: It looked like a human with bat wings. (SOUNDBITE OF MUSIC)VEDANTAM: Here's how the newspaper described the creature. (Reading) They averaged four feet in height, were covered except on the face with short and glossy copper-colored hair and had wings composed of a thin membrane without hair, lying snugly upon their backs. WU: And apparently a ferocious sexual appetite. VEDANTAM: Obviously the paper was peddling fake news, but that's only obvious to us in the 21st century. (SOUNDBITE OF ARCHIVED RECORDING)NEIL ARMSTRONG: That's one small step for man. . . VEDANTAM: To the average person in 1835, the discovery of moon bats was incredible. And for The New York Sun. . . WU: It carried the paper to unrivalled levels of circulation. VEDANTAM: Columbia University law professor Tim Wu has written a book titled \"The Attention Merchants\" where he recounts the history of the many ways our attention has been hijacked. By selling the newspaper for a penny, Benjamin Day captured market share, and this turned out to produce something much more valuable than newsprint. WU: The New York Sun, which published these stories, was the first paper to run entirely on the harvesting of human attention, what we also call an advertising business model. And so its profit's entirely dependent not on its credibility or anything else, but how many readers it had that it could resell. So that was a crucial historic moment that began the commodification of attention as something very valuable that you could resell and make a lot of money out of. And that's why I think the paper was driven to stories such as discovering life on the moon so it could build a circulation. VEDANTAM: Benjamin Day's business model was a profound discovery. (SOUNDBITE OF ARCHIVED RECORDING)ARMSTRONG: One giant leap for mankind. VEDANTAM: That model is alive and well today. Attention is the fuel that allows everyone from candy makers to car dealers to sell their wares. In fact, attention is so powerful that once you have it you can get people to buy things they didn't even know they needed. Like, for example, mouthwash. In the 1920s, Listerine came up with one of the first examples of something Tim calls demand engineering. It was an advertising campaign built around an unfamiliar word, halitosis. This dreaded condition, the ad claimed, makes you unpopular. WU: Yeah. Well, (laughter), you know, I don't think people thought much about whether they had bad breath or not before the 1910s or 1920s. In that era, a new form of advertising was essentially invented, which the goal of which was to engineer a demand that did not already necessarily exist. It was seen as a scientific process done by professionals and necessary to support new products that might otherwise not sell, mouthwash being one of them, toothbrush, toothpaste being another. People didn't necessarily want them. The key there is that you could take human attention, you know, which you've harvested to some extent, and then transform it or spin it into gold by engineering new demands. And that was the magic or the science of advertising the 1920s, to make people want things they didn't otherwise want. VEDANTAM: So Tim, I understand the Listerine sales grew from $115,000 to over $8 million as a result of this advertising campaign. WU: Yeah. That's right. And there are abundant examples from the 1910s, particularly in the 1920s, of demand engineering working. Now, that's what powered, frankly, the growth of something called an advertising industry, which before had really been a marginal industry. In Listerine, to take a specific example, had previously been used for unclear purposes. It had been a disinfectant that had been sold as something to clean floors with, but the invention of it as a mouthwash to clear, to cure bad breath was the key to its success. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED SINGERS: (Singing) Light up a Lucky. It's light-up time. VEDANTAM: The attention merchants of the 1920s discovered that they could not only create new norms. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED SINGER: (Singing) For the taste that you like, light up a Lucky Strike. VEDANTAM: . . . But they could undo old ones. One of the most effective campaigns was to undermine the taboo against women smoking. WU: It was considered unseemly or taboo for a woman to smoke in public or even to smoke at all, and the tobacco industry, particularly Lucky Strike, took aim at that in two directions. One was to try to brand cigarettes as a symbol of women's independence and co-brand it with the suffragette movement. They invented this phrase, torches of freedom, to refer to the cigarette to show that women were in charge of their own destiny. And the second, which is a well-tried advertising technique, was to link cigarettes to weight loss. There's Lucky Strike advertisements from the era that picture an enormous fat woman and say, is this you in five years? Smoke Lucky Strikes, or, reach for a Lucky, not for a sweet. So they certainly went right at it, and the statistics are dramatic. They went from very little sales to many millions of cartons being sold to women specifically. And so I think it's one of the most successful examples of demand engineering. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED SINGER: (Singing) For the taste that you like, light up a Lucky Strike. UNIDENTIFIED SINGERS: (Singing) Relax. It's light-up time. VEDANTAM: Ninety years ago, you might have heard that Lucky Strike jingle through a new medium that was taking America by storm. (SOUNDBITE OF RADIO STATIC CRACKLING)VEDANTAM: Radio didn't just capture people's attention. It brought them together. Families gathered around the fireplace to listen to FDR. (SOUNDBITE OF ARCHIVED RECORDING)FRANKLIN DELANO ROOSEVELT: My most immediate concern is in carrying out the purposes of the great work program. . . VEDANTAM: And what The New York Sun did in print, Orson Welles did on radio. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED RADIO BROADCASTER #1: The Mercury Theatre and star of these broadcasts, Orson Welles. ORSON WELLES: We know now that in the early years of the 20th century. . . VEDANTAM: This opened up new avenues for attention merchants. Advertisers began sponsoring programs and often slipped the names of companies and products into shows. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED RADIO BROADCASTER #2: Try Rinso. I know you'll join the vast army of women who whistle while they wash. And now the soapy-rich Rinso presents the new \"Amos 'N' Andy\" show. (SOUNDBITE OF MUSIC)VEDANTAM: The community aspect of radio harnessed attention in a way that newspaper publishers could only dream of. WU: You know, there were some 19th-century, early 20th-century writing on the psychology of the crowd. There was the idea - not exactly contemporary psychology - that people listening to things in mass sort of shed their individual identity, became part of a group which behaved more like an animal, and, you know, in some ways was entirely wild. And that was the speculation, that we sort of lost it. You know, I think there's some support for that view. I mean, if you've ever been at a sports event or a political rally and you feel you sort of have submerged yourself into a group. But, you know, it was at that level of theorizing, nothing more scientific than that. VEDANTAM: If radio came along and essentially showed that, you know, it could put newspapers to shame, a new product emerged in the 1950s and it quickly proved that it became the dominant way to capture people's attention. You say that something extraordinary in the history of the attention merchants happened on Sunday, September 9, 1956. WU: Yes, and that is what I label peak attention, otherwise known as Elvis Presley appearing on \"The Ed Sullivan Show. . . \"(CHEERING)WU: . . . Which registered an audience share which has never been rivaled. You know, there've been larger audiences, but the share of the audience has never been quite as large as on that day. (SOUNDBITE OF TV SHOW, \"THE ED SULLIVAN SHOW\")ELVIS PRESLEY: This is probably the greatest honor that I've ever had in my life. WU: And television, even beyond radio, had shown this incredible capacity to capture the entire nation at one time, watching the same information. You know, in retrospect, it's remarkable to think about today how divided people are, how they all listen to their own streams. The whole nation watching one thing at once is really a product of the mid-century and something that was never equaled before, and maybe in some ways never equaled since. (SOUNDBITE OF TV SHOW, \"THE ED SULLIVAN SHOW\")PRESLEY: (Singing) To a heart that's true. I don't want no other love. Baby, it's just you I'm thinking of. Mmm. (CHEERING)VEDANTAM: You know, it used to be that for a long time before radio and television that if you wanted people's attention, you actually had to capture it in something that looked like the public square. And of course with the advent of radio and television, what you have as far as the attention merchants are concerned is an ability to sell things to people even when they're inside their own home. So the home becomes an opportunity to capture this enormous mind space, if you will, this attention of the nation. WU: Yes. I think that's a very significant development, one that people in the '20s thought, you know, radio advertisements in the home? No one's going to stand for that. The home is a sacred place, a place for family. You know, it's impossible to imagine that you'll have acceptance of commercial banter in the home. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED ACTOR #2: (As paper towel salesman) Uh-oh. Ketchup. (SOUNDBITE OF RAPPING ON GLASS KETCHUP BOTTLE)UNIDENTIFIED ACTRESS #4: (As Mrs. Porter) I wish somebody would invent a ketchup bottle that squirts where you aim it. UNIDENTIFIED ACTOR #2: (As paper towel salesman) Mrs. Porter? I've got the next best thing, a new invention from Procter and Gamble. It absorbs like magic. It's called Bounty, the new paper towel that actually attracts moisture. WU: You know, but it came with a lot of sweeteners - Elvis Presley, other radio shows, \"I Love Lucy. \" And so we reached a situation where everyone in the United States would, you know, faithfully sit down after dinner, watch television, and in the course of that, absorb a massive amount of commercial advertising in its most compelling form, namely full sound and full video. And its remarkable transformations almost remarkably allowed commerce to intrude in that way, but it fell, as I said, not with a stick but with a carrot. VEDANTAM: By the late 1950s of course, people are recoiling from the amount of advertising they're seeing on television, and a new product emerges to cater to this concern, and this product is the remote control. The idea is this device is going to allow you freedom to avoid the advertisements, to basically be in charge of your own television-watching experience. Did it do that? WU: Well, what many people may not know is that the remote control, as you suggested, was born as an ad killer. It was invented by Zenith as a solution to the problem of advertising. The early versions of the remote control looked like a revolver, a gun, that you would shoot out the ad, I guess basically turning down the volume or switching channels. And it was marketed as serving the individual. In the long-term, however - and I think most of us have experienced this - it didn't quite have those purposes. It instead began enabling a different kind of behavior, channel surfing, where you, you know, sort of sit there and push the button, push the button, push the button sometimes for hours on end. So there is this paradox that sometimes devices designed to liberate us or empower us end enslaving us in completely different ways mainly because of our weak powers of self-control. (SOUNDBITE OF MUSIC)VEDANTAM: This lack of self-control lies at the very heart of nearly every new invention of the attention merchants. Even as people try to liberate themselves from one form of mind control, skilled merchants find new ways to undermine people's ability to look away. One of their biggest victories in this arms race was the discovery of televised sports. WU: And the turning point for sports was the 1958 National Football League Championships. The game of the - greatest game ever played between the Colts and the Giants. And, you know, it was an incredibly exciting football game. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED SPORTSCASTER: There's the kick. WU: But, more to the point, you know, football had not been watched on TV by large audiences and no one quite understood to that point just how captivating it was. And it has proven to this day. There's been some weakening, but sports audiences are very loyal. They're an exceptionally valuable, maybe the most valuable attention-harvesting opportunity. And this is another of TV's inventions in the 1950s. VEDANTAM: And I have to say as a sports fan myself, I find myself sitting through two and a half minutes of ads at the two-minute warning of a game, asking myself, you know, what in God's name am I doing? But of course I keep doing that every Sunday. WU: It's one of the few times I think that the old model of the '50s still has its sway in an era of streaming and other competitors. Sports is the Gibraltar of the traditional broadcast model. And, as you said, you know, I like sports, too, and I will sit through ads (laughter) when I would never do it for anything else. So I think you're right. VEDANTAM: As the television networks captured an ever larger share of people's mind space, new entrants found it difficult to compete. Producing compelling television was expensive. In 1992, MTV was looking for a way to grab and hold people's attention without spending too much money. The solution they came up with? Pure genius. (SOUNDBITE OF TV SHOW, \"THE REAL WORLD\")UNIDENTIFIED MAN #3: This is the true story. . . UNIDENTIFIED MAN #4: True story. . . UNIDENTIFIED WOMAN #2: Of seven strangers. . . UNIDENTIFIED WOMAN #3: Picked to live in a loft. UNIDENTIFIED MAN #5: And have their lives taped. . . UNIDENTIFIED MAN #4: To find out what happens. . . UNIDENTIFIED WOMAN #3: What? UNIDENTIFIED WOMAN #4: When people stop being polite. Could you get the phone? UNIDENTIFIED MAN #6: . . . And start getting real. UNIDENTIFIED MAN #4: \"The Real World. \"(SOUNDBITE OF DOG BARKING)VEDANTAM: Talk about this idea that this was in some ways the discovery of what today we would call reality television. WU: Yes. No, absolutely. You know, MTV '90s started to think, well, you know, it could be that the era of Michael Jackson's videos are coming to an end, or, Duran Duran. You know, people aren't going to watch videos anymore. We need something else. They actually thought about broadcasting football. They did a game show for a little while, but then someone had the idea that what they really needed was a soap opera. And, as we already suggested, they looked at soap opera and realized that they were far too expensive. MTV was run on the cheap. You know, they had basically no costs other than the veejays who they paid, and parties and, you know, some minimal salary. So they had the idea of getting a bunch of amateurs or regular people together, putting them in a house and then just seeing what happened. The house was in SoHo. The result was a show called \"The Real World. \" And as you already suggested, it was the founding series of reality television and driven really at bottom by cast-cutting, (laughter) you know, the idea that we needed a show on the cheap. The participants in the original \"Real World\" were paid $1,400 for the entire set. So, you know, not very expensive. VEDANTAM: And the argument made to the participants was we are going to pay you, not in dollars and cents, but we're going to pay in attention and fame. WU: Yes. This was the genius discovery in a way - that's one way of putting it - is that, you know, as opposed to shelling out for a big salary, especially for a famous actor, you could instead get, you know, so-called normal, somewhat normal people to do it for the idea that they would themselves become celebrities, at least for a little while. (SOUNDBITE OF MUSIC)VEDANTAM: Thousands of people have taken this idea and run with it. You don't need to be a large corporation anymore to be an attention merchant. The screens on our desks and in our hands have enabled a new breed of merchants who have found ever more powerful ways to keep us coming back. That's coming up after the break. But first we need a moment to monetize your mind space with some messages from our sponsors. Yes, we're attention merchants, too. (SOUNDBITE OF MUSIC)VEDANTAM: Today, we're talking with author Tim Wu about his book \"The Attention Merchants: The Epic Scramble To Get Inside Our Heads. \" Attention merchants are television shows, newspaper articles and podcasts that draw you in and then sell your attention to advertisers. (SOUNDBITE OF MUSIC)UNIDENTIFIED MAN #7: Support for this podcast is the following. . . VEDANTAM: The Internet has redefined the notion of what and who an attention merchant can be. You don't need to be a Fortune 500 company or an advertising behemoth. You can be someone like Jonah Peretti. In 2001, the MIT grad student had an idea. He decided to order some personalized Nike sneakers with the word sweatshop printed on them. WU: Nike didn't really take to that suggestion. They rejected it - or some employee did - as inappropriate slang. He wrote back and pointed out that sweatshop wasn't slang. That it was in the dictionary. And they just canceled the order. And he wrote a final email saying, well, you know, could you please send me a picture of the 12-year-old who's making my shoes. VEDANTAM: He also went on to write a blog post about his experience - or shared this material. Describe to me what happened and sort of the turn of events that turned this, you know, relatively innocuous private interaction into something that was close to a global phenomenon. WU: Well, Jonah Peretti was - here he was in the early 2000s. And he touched a live wire that no one really understood well, which was the tendency of certain stories - I don't know if it was quite a blog post. I think he just sent an email out. And the email got forwarded, got forwarded, got forwarded, got forwarded until millions of people had seen it or read it. We now call that going viral. But that phrase didn't exist back then. You know, Jonah told me he then ended up on, you know, the \"Today\" show talking about sweatshops. The thing blew up. And, you know, that's something we're kind of more familiar with now. But at the time, it was a new phenomenon - especially, you know, an unknown person having their email just go viral. And it showed that there was something new and unusual about this medium the web and the Internet. VEDANTAM: Now Jonah, of course, was not a one-shot wonder. He went on to do several other things. In fact, he had - he demonstrated that he had something of a knack for finding things that went viral. Describe to us some of the websites that most of us have visited that are the brainchild of Jonah Peretti. WU: Yeah, so Jonah, in some ways, did a lot to invent our present. Something about virality fascinated him. I think he just thought that experience with the shoes was so strange and weird and unexpected that, you know, he went back almost like a scientist to see if he could bottle that lightning. He founded two websites. One was the Huffington Post, which he co-founded with other people including Arianna Huffington, which was designed to use these sort of web techniques to push a more left-leaning form of journalism. And, you know, it was a tremendous success, transformed journalism - not all in good ways, but it did. But he even went further and went to the pure distillation of attention with a site name BuzzFeed Laboratories - now known as BuzzFeed. The only goal of which was the pure harvesting of attention by creating viral stories. And that - BuzzFeed has obviously transformed web content today as we know it. VEDANTAM: I remember some time ago, Tim, I was watching something that was forwarded to me by a friend. And it showed a video that BuzzFeed had posted where they had a watermelon sitting on a table, and these two people working at BuzzFeed essentially wrapped rubber bands around the watermelon. And they kept doing so until there were probably hundreds of rubber bands. (SOUNDBITE OF YOUTUBE VIDEO \"HOW MANY RUBBER BANDS DOES IT TAKE TO EXPLODE A WATERMELON? (UNEDITED)\")CHELSEA MARSHALL: Six-seventy-eight. JAMES HARNESS: Six-seventy-nine. MARSHALL: Oh, I see her bursting. (LAUGHTER)VEDANTAM: And the idea was, of course, that at some point the rubber bands would exert enough power on the watermelon to make the watermelon explode. And you sort of knew this was going to happen, but you didn't quite know when it was going to happen. And people like me sat and watched this video unfold for - I don't know how long it was. It might have been even ten or twelve minutes. And all this was of people - was people putting rubber bands on a watermelon. And throughout that process, I found myself asking, why is it that I just simply am not able to look away? And in some ways, it is an act of genius to create content like that. WU: Yeah, BuzzFeed Laboratories - I think the laboratories is an important part of the original name - is they just kept experimenting until they found stuff that, for whatever reason, just grabbed people and wouldn't let it go. And watermelons with rubber bands - maybe more obvious ones like cat photos. They just - people kept coming back. And, you know, I guess we know more about the human mind as a result of BuzzFeed's experiment on us although I'm not really sure that we like what we found. Or at least we found that the things we're interested in, you know, aren't necessarily, you know, reading Tolstoy or something but are these strange things like the one you mentioned. (SOUNDBITE OF YOUTUBE VIDEO \"HOW MANY RUBBER BANDS DOES IT TAKE TO EXPLODE A WATERMELON? (UNEDITED)\")MARSHALL: Six-ninety. (APPLAUSE)VEDANTAM: Let's talk for a moment about Silicon Valley and the work of companies like Google and Twitter and Facebook. They have, in some ways, become masters not just of capturing our attention but monitoring where our attention goes and building products that cater to the drift of our attention. Talk about these new attention merchants and in some ways their enormous power over our lives. WU: Yeah, sure. A big turning point in the history of humanity came at the end of the last century - the last millennia when Silicon Valley, headed by Google, first really started to get into advertising and turned all the resources, all the know-how, all the expertise of engineering and computer science to the art and science of capturing as much attention as possible, getting as much data as possible out of people and reselling it to advertisers. That has been a change with profound consequences. I think many or most of us are hooked on one or more online products, which know more about us than anyone else and frankly are like this incredible supercomputer designed to get as much resellable attention out of us as possible. I think this is something that goes beyond even what television or radio was capable of doing because they know so much more about us. They know so much more about you - your vulnerabilities, your desires. And, you know, customized marketing can really work. And it's something we really need to watch in this next decade. VEDANTAM: Many celebrities have come to understand that attention online translates to money. I was reading a website the other day that was describing the Indian cricket star Virat Kohli, who has nearly 17 million Instagram followers. And the article said that Virat Kohli makes half a million dollars per Instagram post where he promotes a product. That is just - that's just mind-boggling. WU: It does show the commercial value of attention, which is really what my book is all about. And it also speaks to the transformation of celebrity. You know, there was once a point where famous people - you know, say the queen of England or a famous scientist - they sort of tried to stay out of public view. They usually had jobs other than being celebrities. Say - I don't know - Einstein was trying to discover things. And their mystery seemed to add to the sense of wonder or fame. That's not our model at all. Celebrities or aspiring celebrities seek to eke out any minute or second they can get of our attention and stay there - never go away. And as you've suggested, there's commercial reasons to do so - that you can frankly make a lot of money not only doing your job but just by being famous. You know, I think maybe Paris Hilton gets some credit for the theory of just being famous for being famous sake. Famous for being famous is the phrase. But certainly a celebrity has transformed in our times. VEDANTAM: It isn't just megastars who can monetize their celebrity. Increasingly micro-celebrities, often called influencers, are finding there's real money to be made in harvesting the attention of their friends and followers. (SOUNDBITE OF MUSIC)SUE TRAN: Hi, I'm Sue Tran. I'm currently an associate creative director at Refinery29 working in the brand and content space. I also have a micro-large following on Instagram with my Instagram handle Sue Tran with three Ns. VEDANTAM: Sue Tran has about 23,000 followers on Instagram. She joined the site five years ago. Since then, she has built up a following of people interested in food and art around New York City. Scattered among some 1,500 photos are pictures of Yankee Candles, portable printers and most recently pictures of Sue posing with a Google Pixel smartphone. TRAN: Google was actually through an influencer agency. Influencer marketing agencies has been growing in the last like one or two years just because people want to monetize influential Instagram and bloggers and all that stuff. So they kind of create a platform to make it easier for influencers to seek out sponsors or sponsors to seek out them. VEDANTAM: Sue says companies pay influencers based on the number of followers they have. TRAN: I have a rate of 150. VEDANTAM: There's a homemade quality to Sue's sponsored posts. TRAN: Some of them are obviously a little bit more staged, but I don't think I would ever post anything that I didn't feel like was 100 percent me. VEDANTAM: Companies want these messages to feel like authentic recommendations from one friend to another rather than advertising messages directed by a multibillion-dollar company. In one picture, Sue poses with her Google phone in front of a building in Brooklyn. In another, she's holding the phone while sitting in a Chinese restaurant. To a friend, it might look like she loves her Google phone, but. . . TRAN: Don't tell anyone. I'm still on my iPhone (laughter). WU: It just is - indicates sort of a new type of media environment where, as you suggested, many more people can be famous - not in the older traditional sense of, you know, everyone in America knows your face or everyone in the world knows your face, which was the old criteria for People magazine putting your face on the cover, but that, you know, millions of people or hundreds of thousands of people know who you are. And therefore, in some smaller way, you are micro or nano-famous. VEDANTAM: When we think of celebrities, we think of people most often in movies and on television. People like. . . (SOUNDBITE OF TV SHOW, \"THE APPRENTICE\")PRESIDENT DONALD TRUMP: My name's Donald Trump. And I'm the largest real estate developer in New York. VEDANTAM: You have a particular interpretation, Tim, of how \"The Apprentice\" led to Donald Trump's election as president. WU: Yes, I think that Donald Trump through \"The Apprentice,\" and to some degree other parts of his life, understood deeply the power of capturing and using human attention. Now on \"The Apprentice,\" I think he studied what it takes to capture an audience - some of these things we talked about - BuzzFeed, the sort of plot twist, the unusual, surprising behavior. And I think he has, in his presidency and during his campaign, saw it as his primary directive to always win the battle for attention. Sometimes even losing or appearing to lose, it doesn't matter as long as there's a good show, a big fight, and everybody's paying attention to me. In his mind, he thinks he's won. And to some degree, it is truer than any of us would like to admit. At some deep level, there's some genius to it - understanding that the battle for attention is primary to a lot of other battles. You know the whole country, and to some degree the world, is reacting to his agenda, his presence, his tweets, everything he does. That's also known as power. You know, even if people are resisting you, they're still paying attention to you. And so, you know, the mental resources of the entire nation - much of the world have been devoted to this one figure, Mr. Donald Trump. VEDANTAM: You say that because Trump is an attention merchant, his biggest vulnerability, you know, might not be the risk of impeachment but the risk that people will eventually get bored of him. Talk about that idea - that one of the risks of being an attention merchant is that people will eventually start to tune you out. WU: Yes, you know, I think this happens with all advertising, almost all content and many celebrities, with a few exceptions. We have some innate tendency to get bored, to get used to things, develop some immunity. You know, even a hit show like \"I love Lucy\" eventually lost its audience. And so much as Donald Trump rose to power on an intentional move - you know, almost running his campaign and presidency as a reality show - I think when people begin getting bored, begin tuning out, you can expect a loss of power. He may fade less in the way of Richard Nixon and more in the way of Paris Hilton. VEDANTAM: When you step back and look at this long arc of how attention merchants have captured our attention and monetized it and sold it and found ways to figure out what works and what doesn't work, are there broad patterns that emerge about human nature and human psychology? Are there lessons to be drawn about how the mind works from the story of the attention merchants? WU: Yeah, I think there are. So first of all, there's lessons as to how we decide what to pay attention to. It's a mixture of voluntary and involuntary mechanisms. The science suggests - and I think the history suggests it's true. So we like to think we control what we pay attention to. But in fact, we can sort of be conditioned or involuntarily attracted to things. Have you ever found yourself, you know, clicking on Facebook and wondering, why did I do that? Or if you ever find yourself, you know, startled by an ad and watching it, not sure what got you there, you'll know that it's not fully within our voluntary control. VEDANTAM: There's an even deeper message in the history of the attention merchants. WU: Part of this book is motivated by a deep interest in human freedom and, you know, a sense that we can lose our freedom and become entrapped really by doing what we think are voluntary choices. I mean, I don't have to read email. I don't have to be writing tweets or something. Nonetheless, these voluntary choices, in a certain environment, can leave one trapped. Another motivation for this book is the experience, which I'm sure many listeners will have had, where you, you know, go to your computer, and you have the idea you're going to write just one email. And you sit down, and suddenly an hour goes by - maybe two hours. And I don't know what happened. You know, this sort of surrender of control over our lives - the loss of control, to me, speaks deeply to this challenge of freedom and what it means to be autonomous in our time and have a life where you've sort of, to some degree, chosen what you want to do. These are values that seem to me under threat in our times. VEDANTAM: So there's been a war for our attention for a very long time - at least a century, probably much longer than that. Are we just helpless victims in this war, where, you know, people are waging, you know, this battle for our attention? Is there a way that we can in some ways take back this battlefield and own our own minds again? WU: Yeah, this is, as you said, something only a century old. You know, advertising 100 years ago was just getting started. So we're in a relatively new - over the course of human civilization - environment. And I think we can adapt. We still have our individuality and ultimately some choice. Now, the challenge is that we face an industry which has spent a century inventing and developing techniques to get us hooked, to harvest as much attention as possible. And they're good at it. But we do have choices. And I think it begins with the idea that attention is a resource, that you own it and that one should be very conscious about how it's being spent. I was motivated writing this book by the work of William James, the philosopher. And he pointed out something very straightforward, which is, you know, at the end of your days, your life will have been what you paid attention to. And so deciding how that vital resource is spent, in my view, is the key to life, frankly, the key to it meaning, the key to doing and having a life which you think is meaningful. (SOUNDBITE OF MUSIC)VEDANTAM: Tim Wu is a professor at Columbia Law School. He's the author of \"The Attention Merchants: The Epic Scramble To Get Inside Our Heads. \" Tim, thank you for joining me today on HIDDEN BRAIN. WU: Yeah, thank you so much. (SOUNDBITE OF MUSIC)VEDANTAM: This week's show was produced by Parth Shah and edited by Tara Boyle. Our team includes Maggie Penman, Jenny (ph) Schmidt, Rhaina Cohen and Renee Klahr. Our unsung heroes this week our Enzo Doran (ph) and Trey Warman (ph). You heard these two young gentlemen at the beginning of the episode. ENZO DORAN: Extra, extra, read all about it. TREY WARMAN: Great, astronomical discoveries lately made. VEDANTAM: And we greatly appreciate their voice acting work. TREY: Great, astronomical discoveries lately made. UNIDENTIFIED WOMAN #5: Astronomical. TREY: Astronomical discoveries lately made. UNIDENTIFIED WOMAN #5: Yell it. TREY: Great, astronomical lately made. UNIDENTIFIED WOMAN #5: Do it again. But don't forget discoveries. VEDANTAM: For more HIDDEN BRAIN, you can follow us on Facebook, Twitter, and Instagram. From all of us here at the show, we wish you a happy new year. If you're looking for a New Year's resolution, I have a suggestion for you. Recommend HIDDEN BRAIN to as many friends and family members as you can in 2018. I'm Shankar Vedantam, and this is NPR. (SOUNDBITE OF MUSIC) SHANKAR VEDANTAM, HOST:  This is HIDDEN BRAIN. I'm Shankar Vedantam. As you listen right now to this podcast, you're probably surrounded by stuff - tangled ear buds, favorite sweater, half-empty water bottle, Legos scattered on the floor, a bike, plate of half-finished eggs. Try to remember what made you buy all these things. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED ACTRESS #1: (As narrator) Hello, Moto. VEDANTAM: Your phone. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED ACTRESS #2: (As narrator) It's time to reimagine the smartphone. VEDANTAM: Those sneakers with the frayed shoelaces. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED ACTRESS #3: (As narrator) Skechers Shape Ups. Step into your new body. VEDANTAM: Your car. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED ACTOR #1: (As narrator) Get on the holiday road in a Honda Civic. VEDANTAM: Newspapers, radio and TV have helped us learn about these products. But in order to serve up billions of ads, these forms of mass media have had to first create a very special product of their own. The secret product? You can't buy it in a store. You can't see it. But you are in the process of supplying it at this very second. UNIDENTIFIED MAN #1: Attention. VEDANTAM: This new product. . . UNIDENTIFIED MAN #1: Attention. Tension. Ten-ten-tension. VEDANTAM: . . . Is your attention. TIM WU: We can lose our freedom and become entrapped really by doing what we think are voluntary choices. VEDANTAM: Corporations ranging from Google to Fox News have found ways to grab your attention, package it and then make money from it. Their strategies are part of a long legacy of companies trying to capture and monetize our attention. Columbia University law professor Tim Wu calls these businesses attention merchants. Today we explore the rise of attention merchants and why Tim says the techniques they've invented pose real risks to our autonomy. (SOUNDBITE OF MUSIC) VEDANTAM: We begin today's show with an account of what may be the earliest attention merchant in history. In the early 1800s, the newspaper business in New York City was bleak. The New York Times wasn't around yet, but there were a handful of other papers, The Journal of Commerce, the Morning Courier and New York Enquirer. They typically charged six cents a copy, which was a lot of money in those days. Benjamin Day was working in newspaper printing, and he thought the business model needed a reboot. Six cents was way too much. He decided to start his own paper, The New York Sun, and sell it for one cent. Everyone thought he was crazy, but he knew something that they didn't. His strategy was one that Jeff Bezos from Amazon could appreciate. On August 25, 1835. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED NEWSBOY: Extra, extra. Read all about it. VEDANTAM: . . . The New York Sun ran a front-page story titled. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED NEWSBOY: \"Great Astronomical Discovery Is Lately Made. \" VEDANTAM: Readers learned that an astronomer in South Africa had built a telescope that could see minute details on the surface of the moon. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED NEWSBOY: Read all about it. VEDANTAM: Over the next few weeks, The Sun released a stream of new findings. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED WOMAN #1: Did you read the news? VEDANTAM: The moon contained canyons, oceans, forests. The telescope also identified a new form of life. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED GROUP: (Gasping in unison). VEDANTAM: . . . A creature with the scientific name Vespertilio-homo. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED MAN #2: I can't believe it. VEDANTAM: It looked like a human with bat wings. (SOUNDBITE OF MUSIC) VEDANTAM: Here's how the newspaper described the creature. (Reading) They averaged four feet in height, were covered except on the face with short and glossy copper-colored hair and had wings composed of a thin membrane without hair, lying snugly upon their backs. WU: And apparently a ferocious sexual appetite. VEDANTAM: Obviously the paper was peddling fake news, but that's only obvious to us in the 21st century. (SOUNDBITE OF ARCHIVED RECORDING) NEIL ARMSTRONG: That's one small step for man. . . VEDANTAM: To the average person in 1835, the discovery of moon bats was incredible. And for The New York Sun. . . WU: It carried the paper to unrivalled levels of circulation. VEDANTAM: Columbia University law professor Tim Wu has written a book titled \"The Attention Merchants\" where he recounts the history of the many ways our attention has been hijacked. By selling the newspaper for a penny, Benjamin Day captured market share, and this turned out to produce something much more valuable than newsprint. WU: The New York Sun, which published these stories, was the first paper to run entirely on the harvesting of human attention, what we also call an advertising business model. And so its profit's entirely dependent not on its credibility or anything else, but how many readers it had that it could resell. So that was a crucial historic moment that began the commodification of attention as something very valuable that you could resell and make a lot of money out of. And that's why I think the paper was driven to stories such as discovering life on the moon so it could build a circulation. VEDANTAM: Benjamin Day's business model was a profound discovery. (SOUNDBITE OF ARCHIVED RECORDING) ARMSTRONG: One giant leap for mankind. VEDANTAM: That model is alive and well today. Attention is the fuel that allows everyone from candy makers to car dealers to sell their wares. In fact, attention is so powerful that once you have it you can get people to buy things they didn't even know they needed. Like, for example, mouthwash. In the 1920s, Listerine came up with one of the first examples of something Tim calls demand engineering. It was an advertising campaign built around an unfamiliar word, halitosis. This dreaded condition, the ad claimed, makes you unpopular. WU: Yeah. Well, (laughter), you know, I don't think people thought much about whether they had bad breath or not before the 1910s or 1920s. In that era, a new form of advertising was essentially invented, which the goal of which was to engineer a demand that did not already necessarily exist. It was seen as a scientific process done by professionals and necessary to support new products that might otherwise not sell, mouthwash being one of them, toothbrush, toothpaste being another. People didn't necessarily want them. The key there is that you could take human attention, you know, which you've harvested to some extent, and then transform it or spin it into gold by engineering new demands. And that was the magic or the science of advertising the 1920s, to make people want things they didn't otherwise want. VEDANTAM: So Tim, I understand the Listerine sales grew from $115,000 to over $8 million as a result of this advertising campaign. WU: Yeah. That's right. And there are abundant examples from the 1910s, particularly in the 1920s, of demand engineering working. Now, that's what powered, frankly, the growth of something called an advertising industry, which before had really been a marginal industry. In Listerine, to take a specific example, had previously been used for unclear purposes. It had been a disinfectant that had been sold as something to clean floors with, but the invention of it as a mouthwash to clear, to cure bad breath was the key to its success. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED SINGERS: (Singing) Light up a Lucky. It's light-up time. VEDANTAM: The attention merchants of the 1920s discovered that they could not only create new norms. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED SINGER: (Singing) For the taste that you like, light up a Lucky Strike. VEDANTAM: . . . But they could undo old ones. One of the most effective campaigns was to undermine the taboo against women smoking. WU: It was considered unseemly or taboo for a woman to smoke in public or even to smoke at all, and the tobacco industry, particularly Lucky Strike, took aim at that in two directions. One was to try to brand cigarettes as a symbol of women's independence and co-brand it with the suffragette movement. They invented this phrase, torches of freedom, to refer to the cigarette to show that women were in charge of their own destiny. And the second, which is a well-tried advertising technique, was to link cigarettes to weight loss. There's Lucky Strike advertisements from the era that picture an enormous fat woman and say, is this you in five years? Smoke Lucky Strikes, or, reach for a Lucky, not for a sweet. So they certainly went right at it, and the statistics are dramatic. They went from very little sales to many millions of cartons being sold to women specifically. And so I think it's one of the most successful examples of demand engineering. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED SINGER: (Singing) For the taste that you like, light up a Lucky Strike. UNIDENTIFIED SINGERS: (Singing) Relax. It's light-up time. VEDANTAM: Ninety years ago, you might have heard that Lucky Strike jingle through a new medium that was taking America by storm. (SOUNDBITE OF RADIO STATIC CRACKLING) VEDANTAM: Radio didn't just capture people's attention. It brought them together. Families gathered around the fireplace to listen to FDR. (SOUNDBITE OF ARCHIVED RECORDING) FRANKLIN DELANO ROOSEVELT: My most immediate concern is in carrying out the purposes of the great work program. . . VEDANTAM: And what The New York Sun did in print, Orson Welles did on radio. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED RADIO BROADCASTER #1: The Mercury Theatre and star of these broadcasts, Orson Welles. ORSON WELLES: We know now that in the early years of the 20th century. . . VEDANTAM: This opened up new avenues for attention merchants. Advertisers began sponsoring programs and often slipped the names of companies and products into shows. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED RADIO BROADCASTER #2: Try Rinso. I know you'll join the vast army of women who whistle while they wash. And now the soapy-rich Rinso presents the new \"Amos 'N' Andy\" show. (SOUNDBITE OF MUSIC) VEDANTAM: The community aspect of radio harnessed attention in a way that newspaper publishers could only dream of. WU: You know, there were some 19th-century, early 20th-century writing on the psychology of the crowd. There was the idea - not exactly contemporary psychology - that people listening to things in mass sort of shed their individual identity, became part of a group which behaved more like an animal, and, you know, in some ways was entirely wild. And that was the speculation, that we sort of lost it. You know, I think there's some support for that view. I mean, if you've ever been at a sports event or a political rally and you feel you sort of have submerged yourself into a group. But, you know, it was at that level of theorizing, nothing more scientific than that. VEDANTAM: If radio came along and essentially showed that, you know, it could put newspapers to shame, a new product emerged in the 1950s and it quickly proved that it became the dominant way to capture people's attention. You say that something extraordinary in the history of the attention merchants happened on Sunday, September 9, 1956. WU: Yes, and that is what I label peak attention, otherwise known as Elvis Presley appearing on \"The Ed Sullivan Show. . . \" (CHEERING) WU: . . . Which registered an audience share which has never been rivaled. You know, there've been larger audiences, but the share of the audience has never been quite as large as on that day. (SOUNDBITE OF TV SHOW, \"THE ED SULLIVAN SHOW\") ELVIS PRESLEY: This is probably the greatest honor that I've ever had in my life. WU: And television, even beyond radio, had shown this incredible capacity to capture the entire nation at one time, watching the same information. You know, in retrospect, it's remarkable to think about today how divided people are, how they all listen to their own streams. The whole nation watching one thing at once is really a product of the mid-century and something that was never equaled before, and maybe in some ways never equaled since. (SOUNDBITE OF TV SHOW, \"THE ED SULLIVAN SHOW\") PRESLEY: (Singing) To a heart that's true. I don't want no other love. Baby, it's just you I'm thinking of. Mmm. (CHEERING) VEDANTAM: You know, it used to be that for a long time before radio and television that if you wanted people's attention, you actually had to capture it in something that looked like the public square. And of course with the advent of radio and television, what you have as far as the attention merchants are concerned is an ability to sell things to people even when they're inside their own home. So the home becomes an opportunity to capture this enormous mind space, if you will, this attention of the nation. WU: Yes. I think that's a very significant development, one that people in the '20s thought, you know, radio advertisements in the home? No one's going to stand for that. The home is a sacred place, a place for family. You know, it's impossible to imagine that you'll have acceptance of commercial banter in the home. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED ACTOR #2: (As paper towel salesman) Uh-oh. Ketchup. (SOUNDBITE OF RAPPING ON GLASS KETCHUP BOTTLE) UNIDENTIFIED ACTRESS #4: (As Mrs. Porter) I wish somebody would invent a ketchup bottle that squirts where you aim it. UNIDENTIFIED ACTOR #2: (As paper towel salesman) Mrs. Porter? I've got the next best thing, a new invention from Procter and Gamble. It absorbs like magic. It's called Bounty, the new paper towel that actually attracts moisture. WU: You know, but it came with a lot of sweeteners - Elvis Presley, other radio shows, \"I Love Lucy. \" And so we reached a situation where everyone in the United States would, you know, faithfully sit down after dinner, watch television, and in the course of that, absorb a massive amount of commercial advertising in its most compelling form, namely full sound and full video. And its remarkable transformations almost remarkably allowed commerce to intrude in that way, but it fell, as I said, not with a stick but with a carrot. VEDANTAM: By the late 1950s of course, people are recoiling from the amount of advertising they're seeing on television, and a new product emerges to cater to this concern, and this product is the remote control. The idea is this device is going to allow you freedom to avoid the advertisements, to basically be in charge of your own television-watching experience. Did it do that? WU: Well, what many people may not know is that the remote control, as you suggested, was born as an ad killer. It was invented by Zenith as a solution to the problem of advertising. The early versions of the remote control looked like a revolver, a gun, that you would shoot out the ad, I guess basically turning down the volume or switching channels. And it was marketed as serving the individual. In the long-term, however - and I think most of us have experienced this - it didn't quite have those purposes. It instead began enabling a different kind of behavior, channel surfing, where you, you know, sort of sit there and push the button, push the button, push the button sometimes for hours on end. So there is this paradox that sometimes devices designed to liberate us or empower us end enslaving us in completely different ways mainly because of our weak powers of self-control. (SOUNDBITE OF MUSIC) VEDANTAM: This lack of self-control lies at the very heart of nearly every new invention of the attention merchants. Even as people try to liberate themselves from one form of mind control, skilled merchants find new ways to undermine people's ability to look away. One of their biggest victories in this arms race was the discovery of televised sports. WU: And the turning point for sports was the 1958 National Football League Championships. The game of the - greatest game ever played between the Colts and the Giants. And, you know, it was an incredibly exciting football game. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED SPORTSCASTER: There's the kick. WU: But, more to the point, you know, football had not been watched on TV by large audiences and no one quite understood to that point just how captivating it was. And it has proven to this day. There's been some weakening, but sports audiences are very loyal. They're an exceptionally valuable, maybe the most valuable attention-harvesting opportunity. And this is another of TV's inventions in the 1950s. VEDANTAM: And I have to say as a sports fan myself, I find myself sitting through two and a half minutes of ads at the two-minute warning of a game, asking myself, you know, what in God's name am I doing? But of course I keep doing that every Sunday. WU: It's one of the few times I think that the old model of the '50s still has its sway in an era of streaming and other competitors. Sports is the Gibraltar of the traditional broadcast model. And, as you said, you know, I like sports, too, and I will sit through ads (laughter) when I would never do it for anything else. So I think you're right. VEDANTAM: As the television networks captured an ever larger share of people's mind space, new entrants found it difficult to compete. Producing compelling television was expensive. In 1992, MTV was looking for a way to grab and hold people's attention without spending too much money. The solution they came up with? Pure genius. (SOUNDBITE OF TV SHOW, \"THE REAL WORLD\") UNIDENTIFIED MAN #3: This is the true story. . . UNIDENTIFIED MAN #4: True story. . . UNIDENTIFIED WOMAN #2: Of seven strangers. . . UNIDENTIFIED WOMAN #3: Picked to live in a loft. UNIDENTIFIED MAN #5: And have their lives taped. . . UNIDENTIFIED MAN #4: To find out what happens. . . UNIDENTIFIED WOMAN #3: What? UNIDENTIFIED WOMAN #4: When people stop being polite. Could you get the phone? UNIDENTIFIED MAN #6: . . . And start getting real. UNIDENTIFIED MAN #4: \"The Real World. \" (SOUNDBITE OF DOG BARKING) VEDANTAM: Talk about this idea that this was in some ways the discovery of what today we would call reality television. WU: Yes. No, absolutely. You know, MTV '90s started to think, well, you know, it could be that the era of Michael Jackson's videos are coming to an end, or, Duran Duran. You know, people aren't going to watch videos anymore. We need something else. They actually thought about broadcasting football. They did a game show for a little while, but then someone had the idea that what they really needed was a soap opera. And, as we already suggested, they looked at soap opera and realized that they were far too expensive. MTV was run on the cheap. You know, they had basically no costs other than the veejays who they paid, and parties and, you know, some minimal salary. So they had the idea of getting a bunch of amateurs or regular people together, putting them in a house and then just seeing what happened. The house was in SoHo. The result was a show called \"The Real World. \" And as you already suggested, it was the founding series of reality television and driven really at bottom by cast-cutting, (laughter) you know, the idea that we needed a show on the cheap. The participants in the original \"Real World\" were paid $1,400 for the entire set. So, you know, not very expensive. VEDANTAM: And the argument made to the participants was we are going to pay you, not in dollars and cents, but we're going to pay in attention and fame. WU: Yes. This was the genius discovery in a way - that's one way of putting it - is that, you know, as opposed to shelling out for a big salary, especially for a famous actor, you could instead get, you know, so-called normal, somewhat normal people to do it for the idea that they would themselves become celebrities, at least for a little while. (SOUNDBITE OF MUSIC) VEDANTAM: Thousands of people have taken this idea and run with it. You don't need to be a large corporation anymore to be an attention merchant. The screens on our desks and in our hands have enabled a new breed of merchants who have found ever more powerful ways to keep us coming back. That's coming up after the break. But first we need a moment to monetize your mind space with some messages from our sponsors. Yes, we're attention merchants, too. (SOUNDBITE OF MUSIC) VEDANTAM: Today, we're talking with author Tim Wu about his book \"The Attention Merchants: The Epic Scramble To Get Inside Our Heads. \" Attention merchants are television shows, newspaper articles and podcasts that draw you in and then sell your attention to advertisers. (SOUNDBITE OF MUSIC) UNIDENTIFIED MAN #7: Support for this podcast is the following. . . VEDANTAM: The Internet has redefined the notion of what and who an attention merchant can be. You don't need to be a Fortune 500 company or an advertising behemoth. You can be someone like Jonah Peretti. In 2001, the MIT grad student had an idea. He decided to order some personalized Nike sneakers with the word sweatshop printed on them. WU: Nike didn't really take to that suggestion. They rejected it - or some employee did - as inappropriate slang. He wrote back and pointed out that sweatshop wasn't slang. That it was in the dictionary. And they just canceled the order. And he wrote a final email saying, well, you know, could you please send me a picture of the 12-year-old who's making my shoes. VEDANTAM: He also went on to write a blog post about his experience - or shared this material. Describe to me what happened and sort of the turn of events that turned this, you know, relatively innocuous private interaction into something that was close to a global phenomenon. WU: Well, Jonah Peretti was - here he was in the early 2000s. And he touched a live wire that no one really understood well, which was the tendency of certain stories - I don't know if it was quite a blog post. I think he just sent an email out. And the email got forwarded, got forwarded, got forwarded, got forwarded until millions of people had seen it or read it. We now call that going viral. But that phrase didn't exist back then. You know, Jonah told me he then ended up on, you know, the \"Today\" show talking about sweatshops. The thing blew up. And, you know, that's something we're kind of more familiar with now. But at the time, it was a new phenomenon - especially, you know, an unknown person having their email just go viral. And it showed that there was something new and unusual about this medium the web and the Internet. VEDANTAM: Now Jonah, of course, was not a one-shot wonder. He went on to do several other things. In fact, he had - he demonstrated that he had something of a knack for finding things that went viral. Describe to us some of the websites that most of us have visited that are the brainchild of Jonah Peretti. WU: Yeah, so Jonah, in some ways, did a lot to invent our present. Something about virality fascinated him. I think he just thought that experience with the shoes was so strange and weird and unexpected that, you know, he went back almost like a scientist to see if he could bottle that lightning. He founded two websites. One was the Huffington Post, which he co-founded with other people including Arianna Huffington, which was designed to use these sort of web techniques to push a more left-leaning form of journalism. And, you know, it was a tremendous success, transformed journalism - not all in good ways, but it did. But he even went further and went to the pure distillation of attention with a site name BuzzFeed Laboratories - now known as BuzzFeed. The only goal of which was the pure harvesting of attention by creating viral stories. And that - BuzzFeed has obviously transformed web content today as we know it. VEDANTAM: I remember some time ago, Tim, I was watching something that was forwarded to me by a friend. And it showed a video that BuzzFeed had posted where they had a watermelon sitting on a table, and these two people working at BuzzFeed essentially wrapped rubber bands around the watermelon. And they kept doing so until there were probably hundreds of rubber bands. (SOUNDBITE OF YOUTUBE VIDEO \"HOW MANY RUBBER BANDS DOES IT TAKE TO EXPLODE A WATERMELON? (UNEDITED)\") CHELSEA MARSHALL: Six-seventy-eight. JAMES HARNESS: Six-seventy-nine. MARSHALL: Oh, I see her bursting. (LAUGHTER) VEDANTAM: And the idea was, of course, that at some point the rubber bands would exert enough power on the watermelon to make the watermelon explode. And you sort of knew this was going to happen, but you didn't quite know when it was going to happen. And people like me sat and watched this video unfold for - I don't know how long it was. It might have been even ten or twelve minutes. And all this was of people - was people putting rubber bands on a watermelon. And throughout that process, I found myself asking, why is it that I just simply am not able to look away? And in some ways, it is an act of genius to create content like that. WU: Yeah, BuzzFeed Laboratories - I think the laboratories is an important part of the original name - is they just kept experimenting until they found stuff that, for whatever reason, just grabbed people and wouldn't let it go. And watermelons with rubber bands - maybe more obvious ones like cat photos. They just - people kept coming back. And, you know, I guess we know more about the human mind as a result of BuzzFeed's experiment on us although I'm not really sure that we like what we found. Or at least we found that the things we're interested in, you know, aren't necessarily, you know, reading Tolstoy or something but are these strange things like the one you mentioned. (SOUNDBITE OF YOUTUBE VIDEO \"HOW MANY RUBBER BANDS DOES IT TAKE TO EXPLODE A WATERMELON? (UNEDITED)\") MARSHALL: Six-ninety. (APPLAUSE) VEDANTAM: Let's talk for a moment about Silicon Valley and the work of companies like Google and Twitter and Facebook. They have, in some ways, become masters not just of capturing our attention but monitoring where our attention goes and building products that cater to the drift of our attention. Talk about these new attention merchants and in some ways their enormous power over our lives. WU: Yeah, sure. A big turning point in the history of humanity came at the end of the last century - the last millennia when Silicon Valley, headed by Google, first really started to get into advertising and turned all the resources, all the know-how, all the expertise of engineering and computer science to the art and science of capturing as much attention as possible, getting as much data as possible out of people and reselling it to advertisers. That has been a change with profound consequences. I think many or most of us are hooked on one or more online products, which know more about us than anyone else and frankly are like this incredible supercomputer designed to get as much resellable attention out of us as possible. I think this is something that goes beyond even what television or radio was capable of doing because they know so much more about us. They know so much more about you - your vulnerabilities, your desires. And, you know, customized marketing can really work. And it's something we really need to watch in this next decade. VEDANTAM: Many celebrities have come to understand that attention online translates to money. I was reading a website the other day that was describing the Indian cricket star Virat Kohli, who has nearly 17 million Instagram followers. And the article said that Virat Kohli makes half a million dollars per Instagram post where he promotes a product. That is just - that's just mind-boggling. WU: It does show the commercial value of attention, which is really what my book is all about. And it also speaks to the transformation of celebrity. You know, there was once a point where famous people - you know, say the queen of England or a famous scientist - they sort of tried to stay out of public view. They usually had jobs other than being celebrities. Say - I don't know - Einstein was trying to discover things. And their mystery seemed to add to the sense of wonder or fame. That's not our model at all. Celebrities or aspiring celebrities seek to eke out any minute or second they can get of our attention and stay there - never go away. And as you've suggested, there's commercial reasons to do so - that you can frankly make a lot of money not only doing your job but just by being famous. You know, I think maybe Paris Hilton gets some credit for the theory of just being famous for being famous sake. Famous for being famous is the phrase. But certainly a celebrity has transformed in our times. VEDANTAM: It isn't just megastars who can monetize their celebrity. Increasingly micro-celebrities, often called influencers, are finding there's real money to be made in harvesting the attention of their friends and followers. (SOUNDBITE OF MUSIC) SUE TRAN: Hi, I'm Sue Tran. I'm currently an associate creative director at Refinery29 working in the brand and content space. I also have a micro-large following on Instagram with my Instagram handle Sue Tran with three Ns. VEDANTAM: Sue Tran has about 23,000 followers on Instagram. She joined the site five years ago. Since then, she has built up a following of people interested in food and art around New York City. Scattered among some 1,500 photos are pictures of Yankee Candles, portable printers and most recently pictures of Sue posing with a Google Pixel smartphone. TRAN: Google was actually through an influencer agency. Influencer marketing agencies has been growing in the last like one or two years just because people want to monetize influential Instagram and bloggers and all that stuff. So they kind of create a platform to make it easier for influencers to seek out sponsors or sponsors to seek out them. VEDANTAM: Sue says companies pay influencers based on the number of followers they have. TRAN: I have a rate of 150. VEDANTAM: There's a homemade quality to Sue's sponsored posts. TRAN: Some of them are obviously a little bit more staged, but I don't think I would ever post anything that I didn't feel like was 100 percent me. VEDANTAM: Companies want these messages to feel like authentic recommendations from one friend to another rather than advertising messages directed by a multibillion-dollar company. In one picture, Sue poses with her Google phone in front of a building in Brooklyn. In another, she's holding the phone while sitting in a Chinese restaurant. To a friend, it might look like she loves her Google phone, but. . . TRAN: Don't tell anyone. I'm still on my iPhone (laughter). WU: It just is - indicates sort of a new type of media environment where, as you suggested, many more people can be famous - not in the older traditional sense of, you know, everyone in America knows your face or everyone in the world knows your face, which was the old criteria for People magazine putting your face on the cover, but that, you know, millions of people or hundreds of thousands of people know who you are. And therefore, in some smaller way, you are micro or nano-famous. VEDANTAM: When we think of celebrities, we think of people most often in movies and on television. People like. . . (SOUNDBITE OF TV SHOW, \"THE APPRENTICE\") PRESIDENT DONALD TRUMP: My name's Donald Trump. And I'm the largest real estate developer in New York. VEDANTAM: You have a particular interpretation, Tim, of how \"The Apprentice\" led to Donald Trump's election as president. WU: Yes, I think that Donald Trump through \"The Apprentice,\" and to some degree other parts of his life, understood deeply the power of capturing and using human attention. Now on \"The Apprentice,\" I think he studied what it takes to capture an audience - some of these things we talked about - BuzzFeed, the sort of plot twist, the unusual, surprising behavior. And I think he has, in his presidency and during his campaign, saw it as his primary directive to always win the battle for attention. Sometimes even losing or appearing to lose, it doesn't matter as long as there's a good show, a big fight, and everybody's paying attention to me. In his mind, he thinks he's won. And to some degree, it is truer than any of us would like to admit. At some deep level, there's some genius to it - understanding that the battle for attention is primary to a lot of other battles. You know the whole country, and to some degree the world, is reacting to his agenda, his presence, his tweets, everything he does. That's also known as power. You know, even if people are resisting you, they're still paying attention to you. And so, you know, the mental resources of the entire nation - much of the world have been devoted to this one figure, Mr. Donald Trump. VEDANTAM: You say that because Trump is an attention merchant, his biggest vulnerability, you know, might not be the risk of impeachment but the risk that people will eventually get bored of him. Talk about that idea - that one of the risks of being an attention merchant is that people will eventually start to tune you out. WU: Yes, you know, I think this happens with all advertising, almost all content and many celebrities, with a few exceptions. We have some innate tendency to get bored, to get used to things, develop some immunity. You know, even a hit show like \"I love Lucy\" eventually lost its audience. And so much as Donald Trump rose to power on an intentional move - you know, almost running his campaign and presidency as a reality show - I think when people begin getting bored, begin tuning out, you can expect a loss of power. He may fade less in the way of Richard Nixon and more in the way of Paris Hilton. VEDANTAM: When you step back and look at this long arc of how attention merchants have captured our attention and monetized it and sold it and found ways to figure out what works and what doesn't work, are there broad patterns that emerge about human nature and human psychology? Are there lessons to be drawn about how the mind works from the story of the attention merchants? WU: Yeah, I think there are. So first of all, there's lessons as to how we decide what to pay attention to. It's a mixture of voluntary and involuntary mechanisms. The science suggests - and I think the history suggests it's true. So we like to think we control what we pay attention to. But in fact, we can sort of be conditioned or involuntarily attracted to things. Have you ever found yourself, you know, clicking on Facebook and wondering, why did I do that? Or if you ever find yourself, you know, startled by an ad and watching it, not sure what got you there, you'll know that it's not fully within our voluntary control. VEDANTAM: There's an even deeper message in the history of the attention merchants. WU: Part of this book is motivated by a deep interest in human freedom and, you know, a sense that we can lose our freedom and become entrapped really by doing what we think are voluntary choices. I mean, I don't have to read email. I don't have to be writing tweets or something. Nonetheless, these voluntary choices, in a certain environment, can leave one trapped. Another motivation for this book is the experience, which I'm sure many listeners will have had, where you, you know, go to your computer, and you have the idea you're going to write just one email. And you sit down, and suddenly an hour goes by - maybe two hours. And I don't know what happened. You know, this sort of surrender of control over our lives - the loss of control, to me, speaks deeply to this challenge of freedom and what it means to be autonomous in our time and have a life where you've sort of, to some degree, chosen what you want to do. These are values that seem to me under threat in our times. VEDANTAM: So there's been a war for our attention for a very long time - at least a century, probably much longer than that. Are we just helpless victims in this war, where, you know, people are waging, you know, this battle for our attention? Is there a way that we can in some ways take back this battlefield and own our own minds again? WU: Yeah, this is, as you said, something only a century old. You know, advertising 100 years ago was just getting started. So we're in a relatively new - over the course of human civilization - environment. And I think we can adapt. We still have our individuality and ultimately some choice. Now, the challenge is that we face an industry which has spent a century inventing and developing techniques to get us hooked, to harvest as much attention as possible. And they're good at it. But we do have choices. And I think it begins with the idea that attention is a resource, that you own it and that one should be very conscious about how it's being spent. I was motivated writing this book by the work of William James, the philosopher. And he pointed out something very straightforward, which is, you know, at the end of your days, your life will have been what you paid attention to. And so deciding how that vital resource is spent, in my view, is the key to life, frankly, the key to it meaning, the key to doing and having a life which you think is meaningful. (SOUNDBITE OF MUSIC) VEDANTAM: Tim Wu is a professor at Columbia Law School. He's the author of \"The Attention Merchants: The Epic Scramble To Get Inside Our Heads. \" Tim, thank you for joining me today on HIDDEN BRAIN. WU: Yeah, thank you so much. (SOUNDBITE OF MUSIC) VEDANTAM: This week's show was produced by Parth Shah and edited by Tara Boyle. Our team includes Maggie Penman, Jenny (ph) Schmidt, Rhaina Cohen and Renee Klahr. Our unsung heroes this week our Enzo Doran (ph) and Trey Warman (ph). You heard these two young gentlemen at the beginning of the episode. ENZO DORAN: Extra, extra, read all about it. TREY WARMAN: Great, astronomical discoveries lately made. VEDANTAM: And we greatly appreciate their voice acting work. TREY: Great, astronomical discoveries lately made. UNIDENTIFIED WOMAN #5: Astronomical. TREY: Astronomical discoveries lately made. UNIDENTIFIED WOMAN #5: Yell it. TREY: Great, astronomical lately made. UNIDENTIFIED WOMAN #5: Do it again. But don't forget discoveries. VEDANTAM: For more HIDDEN BRAIN, you can follow us on Facebook, Twitter, and Instagram. From all of us here at the show, we wish you a happy new year. If you're looking for a New Year's resolution, I have a suggestion for you. Recommend HIDDEN BRAIN to as many friends and family members as you can in 2018. I'm Shankar Vedantam, and this is NPR. (SOUNDBITE OF MUSIC)", "section": "Hidden Brain", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-01-574985930": {"title": "Chinese Advances In Artificial Intelligence : NPR", "url": "https://www.npr.org/2018/01/01/574985930/chinese-advances-in-artificial-intelligence", "author": "No author found", "published_date": "2018-01-01", "content": "KELLY MCEVERS, HOST: Artificial intelligence, or AI, is everywhere these days, from self-driving cars and voice-activated software like Siri and Alexa. It's being used in fields from criminal justice to finance. So this year in All Tech Considered, we're going to spend some time exploring AI. (SOUNDBITE OF MUSIC)MCEVERS: And today - China. Its leadership wants to dominate the tech world. It's one way China can beat possible competitors and adversaries. NPR's Anthony Kuhn introduces us to the Chinese firm that is leading the way to AI. ANTHONY KUHN, BYLINE: Robin Li, CEO of China's largest search engine, Baidu, took the stage recently at a conference for developers and media. He talked about Baidu's big investments in artificial intelligence. (SOUNDBITE OF ARCHIVED RECORDING)ROBIN LI: (Speaking Chinese). (APPLAUSE)KUHN: \"Today, Baidu understands you better,\" he declared. One of the things Baidu is doing is recognizing voice commands of its search engine users. To understand more, I visited Gao Liang, the lead engineer for voice recognition at Baidu. He showed me some gadgets, including a voice-controlled speaker. GAO LIANG: (Speaking Chinese). COMPUTER-GENERATED VOICE: Hi. GAO: (Speaking Chinese) \"Hotel California. \"KUHN: OK, I guess \"Hotel California's\" not available, so we also try out an interpretation device. Man, that radio program was awesome. COMPUTER-GENERATED VOICE: (Speaking Chinese). KUHN: She got that one right. Gao Liang says that Baidu is working on ways for people to use voice commands to drive cars, do their banking and navigate airports. He explains that the task is complicated by the myriad local accents and dialects in China. Baidu uses vast amounts of data and computing power to learn to understand them. GAO: We extract 100 voice queries that we collect from the user, and we ask a average native Chinese person to listen to it, see if he can understand. The chance are, our search engine will beat that average person. KUHN: In July, China's cabinet released a national plan to become the world's leading power in artificial intelligence by 2030 and create an industry worth nearly $150 billion. China's leaders have long taken pride in their ability to mobilize people and resources for megaprojects. From the Great Wall to the atom bomb, many have had military uses, and AI is no exception. ELSA KANIA: The Chinese military is focused on the ways in which the disruptive impact of AI in warfare could enable it to achieve an advantage and will actively seek to leverage the dynamism of Chinese private sector advances in AI. KUHN: That's Elsa Kania. She studies the Chinese military's approach to emerging technologies at the Center for a New American Security in Washington, D. C. She says China has strengths which could help its bid to dominate emerging technologies, especially at a time when the U. S. seems less focused on them. KANIA: The devotion of resources to AI in the form of investment as well as certain structural advantages that China possesses, including massive amounts of data and a robust potential talent base, could enable China to take the lead in the longer term. KUHN: Kania notes that, as in the U. S. , technological advances by private sector firms like Baidu are quickly adapted to use by the military and the police. So I asked Baidu engineer Gao Liang, what if the government asks you to do something that, for example, threatens users' privacy? GAO: Can we say no? I don't know. Cooperate with the government is one thing - right? because we're building business in China, and we must obey all the regulations. KUHN: Besides, he says, Baidu has its own code of ethics, which he describes as follows. GAO: To make our end user happy and very easy to acquire the information and very easy to get the things they want is our No. 1 goal. KUHN: I also asked Mr. Gao about whether he's concerned about all the people who could be thrown out of work by robots with artificial intelligence. He answered that AI will create many new job opportunities, including teaching robots how to be more human. Anthony Kuhn, NPR News, Beijing. KELLY MCEVERS, HOST:  Artificial intelligence, or AI, is everywhere these days, from self-driving cars and voice-activated software like Siri and Alexa. It's being used in fields from criminal justice to finance. So this year in All Tech Considered, we're going to spend some time exploring AI. (SOUNDBITE OF MUSIC) MCEVERS: And today - China. Its leadership wants to dominate the tech world. It's one way China can beat possible competitors and adversaries. NPR's Anthony Kuhn introduces us to the Chinese firm that is leading the way to AI. ANTHONY KUHN, BYLINE: Robin Li, CEO of China's largest search engine, Baidu, took the stage recently at a conference for developers and media. He talked about Baidu's big investments in artificial intelligence. (SOUNDBITE OF ARCHIVED RECORDING) ROBIN LI: (Speaking Chinese). (APPLAUSE) KUHN: \"Today, Baidu understands you better,\" he declared. One of the things Baidu is doing is recognizing voice commands of its search engine users. To understand more, I visited Gao Liang, the lead engineer for voice recognition at Baidu. He showed me some gadgets, including a voice-controlled speaker. GAO LIANG: (Speaking Chinese). COMPUTER-GENERATED VOICE: Hi. GAO: (Speaking Chinese) \"Hotel California. \" KUHN: OK, I guess \"Hotel California's\" not available, so we also try out an interpretation device. Man, that radio program was awesome. COMPUTER-GENERATED VOICE: (Speaking Chinese). KUHN: She got that one right. Gao Liang says that Baidu is working on ways for people to use voice commands to drive cars, do their banking and navigate airports. He explains that the task is complicated by the myriad local accents and dialects in China. Baidu uses vast amounts of data and computing power to learn to understand them. GAO: We extract 100 voice queries that we collect from the user, and we ask a average native Chinese person to listen to it, see if he can understand. The chance are, our search engine will beat that average person. KUHN: In July, China's cabinet released a national plan to become the world's leading power in artificial intelligence by 2030 and create an industry worth nearly $150 billion. China's leaders have long taken pride in their ability to mobilize people and resources for megaprojects. From the Great Wall to the atom bomb, many have had military uses, and AI is no exception. ELSA KANIA: The Chinese military is focused on the ways in which the disruptive impact of AI in warfare could enable it to achieve an advantage and will actively seek to leverage the dynamism of Chinese private sector advances in AI. KUHN: That's Elsa Kania. She studies the Chinese military's approach to emerging technologies at the Center for a New American Security in Washington, D. C. She says China has strengths which could help its bid to dominate emerging technologies, especially at a time when the U. S. seems less focused on them. KANIA: The devotion of resources to AI in the form of investment as well as certain structural advantages that China possesses, including massive amounts of data and a robust potential talent base, could enable China to take the lead in the longer term. KUHN: Kania notes that, as in the U. S. , technological advances by private sector firms like Baidu are quickly adapted to use by the military and the police. So I asked Baidu engineer Gao Liang, what if the government asks you to do something that, for example, threatens users' privacy? GAO: Can we say no? I don't know. Cooperate with the government is one thing - right? because we're building business in China, and we must obey all the regulations. KUHN: Besides, he says, Baidu has its own code of ethics, which he describes as follows. GAO: To make our end user happy and very easy to acquire the information and very easy to get the things they want is our No. 1 goal. KUHN: I also asked Mr. Gao about whether he's concerned about all the people who could be thrown out of work by robots with artificial intelligence. He answered that AI will create many new job opportunities, including teaching robots how to be more human. Anthony Kuhn, NPR News, Beijing.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-02-575168206": {"title": "Why Europe Is Willing To Regulate Tech More Than The U.S. : NPR", "url": "https://www.npr.org/2018/01/02/575168206/why-europe-is-willing-to-regulate-tech-more-than-the-u-s", "author": "No author found", "published_date": "2018-01-02", "content": "ARI SHAPIRO, HOST: Big tech companies like Google and Facebook influence more of our lives every year. Congress has talked about regulating the tech giants without taking action. In Europe, it's a different story. Just before Christmas, the European Union's highest court issued a ruling against Uber. European courts have also said that Google has to remove some search results at a person's request. It's known as the right to be forgotten. To talk with us about why Europe is regulating these tech companies more aggressively than the U. S. , Jonathan Zittrain joins us now. He's a professor of law and computer science at Harvard. Welcome. JONATHAN ZITTRAIN: Hello. SHAPIRO: Do you see these as isolated examples, or do you think it is generally true that Europe is more willing to regulate these big tech companies than the U. S. is? ZITTRAIN: I think as a general rule, the Europeans are more willing to regulate full stop. There's just a general appetite for regulation and a trust in government perhaps to do it right that may not culturally exist as much in the United States. SHAPIRO: So I gave a couple of examples related to Uber and Google. The big tech giants also include Facebook, Amazon, Apple. Do you also see steps to regulate those companies? ZITTRAIN: Yes. And I think it's - it maybe stems from the earliest days when the Internet went mainstream. I think late-'90s, early 2000s with the dot-com boom, there was some sense possibly, globally, about not wanting to kill the goose laying golden eggs. But we've had a number of eggs now. Not all of them appear to be golden. And there is some willingness now to do something about it. And of course for the Europeans, we see some sense that these aren't their own companies. SHAPIRO: Yeah. ZITTRAIN: They are companies from another country. And the fact that, well, maybe it will end up helping European competition might not be entirely absent from their thinking, too. SHAPIRO: Give us an example of how one of these steps that Europe is taking might affect an Internet user in their daily lives. ZITTRAIN: Well, the right to be forgotten that you mentioned is not a bad example. That's an instance where under an old regime from 1995, a Data Protection Directive, the European courts held that European citizens have a right if there is information that is, quote, \"no longer relevant\" about them - say, inside a search engine - they have an ability to make the case to that search engine - say, Google - that their name should no longer be linked with a particular set of results that they find embarrassing or unwanted. SHAPIRO: I've actually Googled things when I'm in Europe. And with the results, it says at the bottom of the page, some results may be missing as a result of this right to be forgotten ruling. ZITTRAIN: Well, it turns out that that kind of remedy might have been too clever by half because of course if you tell people there's something they don't know about a specific search, that's giving them a big hint that they should roll up their sleeves and start searching more. So those exact notices won't appear when there is something specifically missing. But there are general notifications that say you're in a regime where there may be less stuff than there might otherwise be. SHAPIRO: This is obviously a subjective question. But in your opinion, are European regulators being overzealous, or are American regulators falling down on the job? ZITTRAIN: Well, I think it's possible to say yes to both of those. In the case of Europe, there is a kind of aggressiveness at times that may not map to what we think the ideal regime would be. Now, the other half of the equation is it's been a little bit lax in the United States. And I think that may be changing. But there's certainly been some sense of quite modest restrictions that have been hard-fought. When I think of, for example, the privacy restrictions placed on Internet service providers in the United States by the Federal Communications Commission saying you can't sort of snoop on what your own users are doing and try to sell that data anonymized or not, that was just reversed under the new American administration. So I think there is less of a regulatory hunger being demonstrated by the American executive branch. SHAPIRO: Jonathan Zittrain of Harvard Law is the author of \"The Future Of The Internet And How To Stop It. \" He's also on the board of directors of the Electronic Frontier Foundation. Thanks so much. ZITTRAIN: Thanks so much, Ari. ARI SHAPIRO, HOST:  Big tech companies like Google and Facebook influence more of our lives every year. Congress has talked about regulating the tech giants without taking action. In Europe, it's a different story. Just before Christmas, the European Union's highest court issued a ruling against Uber. European courts have also said that Google has to remove some search results at a person's request. It's known as the right to be forgotten. To talk with us about why Europe is regulating these tech companies more aggressively than the U. S. , Jonathan Zittrain joins us now. He's a professor of law and computer science at Harvard. Welcome. JONATHAN ZITTRAIN: Hello. SHAPIRO: Do you see these as isolated examples, or do you think it is generally true that Europe is more willing to regulate these big tech companies than the U. S. is? ZITTRAIN: I think as a general rule, the Europeans are more willing to regulate full stop. There's just a general appetite for regulation and a trust in government perhaps to do it right that may not culturally exist as much in the United States. SHAPIRO: So I gave a couple of examples related to Uber and Google. The big tech giants also include Facebook, Amazon, Apple. Do you also see steps to regulate those companies? ZITTRAIN: Yes. And I think it's - it maybe stems from the earliest days when the Internet went mainstream. I think late-'90s, early 2000s with the dot-com boom, there was some sense possibly, globally, about not wanting to kill the goose laying golden eggs. But we've had a number of eggs now. Not all of them appear to be golden. And there is some willingness now to do something about it. And of course for the Europeans, we see some sense that these aren't their own companies. SHAPIRO: Yeah. ZITTRAIN: They are companies from another country. And the fact that, well, maybe it will end up helping European competition might not be entirely absent from their thinking, too. SHAPIRO: Give us an example of how one of these steps that Europe is taking might affect an Internet user in their daily lives. ZITTRAIN: Well, the right to be forgotten that you mentioned is not a bad example. That's an instance where under an old regime from 1995, a Data Protection Directive, the European courts held that European citizens have a right if there is information that is, quote, \"no longer relevant\" about them - say, inside a search engine - they have an ability to make the case to that search engine - say, Google - that their name should no longer be linked with a particular set of results that they find embarrassing or unwanted. SHAPIRO: I've actually Googled things when I'm in Europe. And with the results, it says at the bottom of the page, some results may be missing as a result of this right to be forgotten ruling. ZITTRAIN: Well, it turns out that that kind of remedy might have been too clever by half because of course if you tell people there's something they don't know about a specific search, that's giving them a big hint that they should roll up their sleeves and start searching more. So those exact notices won't appear when there is something specifically missing. But there are general notifications that say you're in a regime where there may be less stuff than there might otherwise be. SHAPIRO: This is obviously a subjective question. But in your opinion, are European regulators being overzealous, or are American regulators falling down on the job? ZITTRAIN: Well, I think it's possible to say yes to both of those. In the case of Europe, there is a kind of aggressiveness at times that may not map to what we think the ideal regime would be. Now, the other half of the equation is it's been a little bit lax in the United States. And I think that may be changing. But there's certainly been some sense of quite modest restrictions that have been hard-fought. When I think of, for example, the privacy restrictions placed on Internet service providers in the United States by the Federal Communications Commission saying you can't sort of snoop on what your own users are doing and try to sell that data anonymized or not, that was just reversed under the new American administration. So I think there is less of a regulatory hunger being demonstrated by the American executive branch. SHAPIRO: Jonathan Zittrain of Harvard Law is the author of \"The Future Of The Internet And How To Stop It. \" He's also on the board of directors of the Electronic Frontier Foundation. Thanks so much. ZITTRAIN: Thanks so much, Ari.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-04-575774099": {"title": "Cybersecurity Researchers Find Major Flaws In Widely Used Computer Chips : NPR", "url": "https://www.npr.org/2018/01/04/575774099/cybersecurity-researchers-find-major-flaws-in-widely-used-computer-chips", "author": "No author found", "published_date": "2018-01-04", "content": "ARI SHAPIRO, HOST: Tech companies are rushing to patch up a security flaw found in millions of computer chips. This flaw could make it easier for hackers to steal passwords and other sensitive personal data. Researchers discovered the problem months ago and kept quiet until the companies had a fix. NPR's Laura Sydell reports. LAURA SYDELL, BYLINE: When you install a program on your computer, there's generally a wall between it and other programs. But the security flaws which were built into the chips from Intel Advanced Micro Devices and ARM allows one program to spy on another. MORITZ LIPP: Meaning that if another application has stored your passwords or your holiday pictures, another application can read it. SYDELL: Moritz Lipp is a Ph. D. candidate at Austria's Graz Technical University, and he's one of the researchers who found the flaw. The problem can be found on smartphones, personal computers, browsers and the computers used for cloud storage by companies like Google, Amazon Web Services, Apple and Microsoft. That means millions of computers. If it were a software flaw the problem would be easier to fix, says Lipp, but hardware like chips is another matter. LIPP: If you have an issue in hardware it's not very easy to just change the hardware because you already sold millions of CPUs. And you just can't call them back and change them. SYDELL: Lipp and his colleagues found a flaw many months ago, and they alerted the chip maker Intel. Meanwhile, another team at Google found the problem and a similar one. Google planned to wait to release the information about it, but speculation began to surface online and in media. Google, Microsoft, Intel and others have begun to issue patches that will fix the flaws. But it could have a downside. LIPP: Patches that will come out for the operating systems will decrease the performance. SYDELL: That means your system might move more slowly. All of the companies affected say they see no signs of any breaches, but it's still a good idea to install any patches and updates. Laura Sydell, NPR News, San Francisco. (SOUNDBITE OF STRFKR SONG, \"SATELLITE\") ARI SHAPIRO, HOST:  Tech companies are rushing to patch up a security flaw found in millions of computer chips. This flaw could make it easier for hackers to steal passwords and other sensitive personal data. Researchers discovered the problem months ago and kept quiet until the companies had a fix. NPR's Laura Sydell reports. LAURA SYDELL, BYLINE: When you install a program on your computer, there's generally a wall between it and other programs. But the security flaws which were built into the chips from Intel Advanced Micro Devices and ARM allows one program to spy on another. MORITZ LIPP: Meaning that if another application has stored your passwords or your holiday pictures, another application can read it. SYDELL: Moritz Lipp is a Ph. D. candidate at Austria's Graz Technical University, and he's one of the researchers who found the flaw. The problem can be found on smartphones, personal computers, browsers and the computers used for cloud storage by companies like Google, Amazon Web Services, Apple and Microsoft. That means millions of computers. If it were a software flaw the problem would be easier to fix, says Lipp, but hardware like chips is another matter. LIPP: If you have an issue in hardware it's not very easy to just change the hardware because you already sold millions of CPUs. And you just can't call them back and change them. SYDELL: Lipp and his colleagues found a flaw many months ago, and they alerted the chip maker Intel. Meanwhile, another team at Google found the problem and a similar one. Google planned to wait to release the information about it, but speculation began to surface online and in media. Google, Microsoft, Intel and others have begun to issue patches that will fix the flaws. But it could have a downside. LIPP: Patches that will come out for the operating systems will decrease the performance. SYDELL: That means your system might move more slowly. All of the companies affected say they see no signs of any breaches, but it's still a good idea to install any patches and updates. Laura Sydell, NPR News, San Francisco. (SOUNDBITE OF STRFKR SONG, \"SATELLITE\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-04-575579656": {"title": "Digital Spying And Divorce : NPR", "url": "https://www.npr.org/2018/01/04/575579656/digital-spying-and-divorce", "author": "No author found", "published_date": "2018-01-04", "content": "RACHEL MARTIN, HOST: Digital spy tools are changing divorce as we know it. NPR spoke with dozens of lawyers and investigators who say more and more couples are turning to surveillance when their marriage falls apart. From something as simple as the Find My iPhone feature to insidious spyware that can be installed in a spouse's computer or phone, these tools are cheap and easy to use. NPR's Aarti Shahani has the story. AARTI SHAHANI, BYLINE: A woman discovers she wasn't going crazy. It wasn't in her head. Her ex-husband was in fact following her every move. (SOUNDBITE OF BEEPS)SHAHANI: This is the recording from a police precinct where a sergeant is taking her statement. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED POLICE OFFICER: Just for the record, state your first, middle and last name. SHAHANI: We're going to call her M. NPR interviewed her, her lawyers, the sergeant, and we reviewed her court filings and those of her ex-husband. We won't disclose their names or where she is because she says she fears for her safety. All summer, M. worried her ex-husband was stalking her. As she tells the sergeant, her ex would know where she was, whom she visited, down to the time of day and street. (SOUNDBITE OF ARCHIVED RECORDING)M: There's not really much I can do, so I've understood that what I have to do is I have to make sure that I'm as careful as possible, I have as many safety plans as possible. . . SHAHANI: M. started to change the way she drove - slowing down, driving in circles - in case a private eye was on her. But she didn't see one. Then she went online, and she learned about GPS trackers, small devices you can slip into a car to monitor where it goes 24/7. She looked for one and couldn't find any, but then at her local auto shop, the mechanic found a GPS tracker hidden near the front left tire. That's why she came to this police station. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED POLICE OFFICER: How did you know they found this? M: Chris, the service representative, showed me. She pushed the button in order to show that there was still 25-50 percent energy on it. SHAHANI: Meaning the batteries were fresh, not old. It had been nearly a year since M. left her ex-husband, moved out. But he couldn't let go, and through technology, he didn't have to. This might sound highly invasive, though it turns out, at least in this case, it's completely legal. M. 's husband acknowledged through a lawyer's letter and in family court that he had the GPS tracker installed. The sergeant from the police station - he did a criminal investigation, but he tells NPR prosecutors would not prosecute because the car was jointly owned. If it belongs to both of them, the ex has a right to track it. M: I'm now fully aware that all of those times that I thought I was keeping myself safe, all of those times that I was leaving town, all of those times that I was staying in different places or staying at friends' houses, I never was safe. SHAHANI: Welcome to divorce in the 21st century, where what it means to be safe, how much privacy you're entitled to, is an open question. NPR talked with dozens of marital experts, lawyers, investigators and a leading family judge, and they tell NPR, digital spying is changing divorce as we know it. The tools are abundant, clients use it, and the laws are murky. M. doesn't think her ex stopped at the GPS tracker. She suspects he used another more invasive tool too - spyware on her phone. Sitting in her lawyer's office, M. tells the story of how he went from love of her life to control freak. She says it started with verbal fights over the baby crying or shopping bills. Then one day - she explained this in court, too - he choked her. M: And I wasn't scared. I was shocked. I wasn't scared at all. There was no moment in that interaction where I was scared. I was too shocked. I just couldn't believe it. SHAHANI: And was now worried about physical safety - hers and her child's. She didn't leave right away. She stuck it out until this one night, she says, when he grabbed her and told her, you belong to me. It just clicked. M: I can't explain it, but I knew we needed to leave, and we need to leave fast. SHAHANI: M. took their child and fled. Then he filed for divorce. In family court, in her affidavits and oral testimony, M. laid out her fear of physical abuse and electronic surveillance. M. claimed her ex seemed to know the contents of her text messages, what friends she talked to, even after she left the house. Her ex denied all the allegations, and the judge - the judge focused on the physical stuff. That makes sense. Choking is a familiar offense where the harm is tangible. This newer, quieter intrusion - spying - is harder to grasp. M. gives NPR this one example of a creepy message her ex-husband sent. M: I know all of the ways you've described to me to your friend. And snippets of how I described him were then forwarded to me as a text message. SHAHANI: To this day, M. wonders if he knew this through spyware. These are apps. Like Netflix, you pay a subscription - say, 16. 99 a month - and in return, you get to see everything on your target's phone, every incoming and outgoing message. M. didn't turn to the police because she figured in domestic cases, they investigate assault, not iPhones. Instead, M. went to the Apple Store, where she had to explain her situation in front of a lot of people at the Genius Bar. It was humiliating. M: I can't be the only person who has no clue, goes into the Apple Store and starts jabbering about, I need help because I have stalking issues, which, in itself, sometimes makes people think that you are the stalker. SHAHANI: M. said the Apple geniuses did not look for spyware on her phone. When they saw she was scared, she said, they helped her by swapping the device for a brand-new one. Unfortunately, that also meant the evidence was thrown out. This happens all the time in spyware cases, the experts tell NPR. Victims solve the immediate problem, but that hampers any future investigation. Spyware itself is not the crime. Dozens of companies sell it legally as a tool to monitor kids and employees. But using it secretly on your spouse - that typically is not legal. Back at the police station, M. spelled out for the sergeant how it feels to be watched anytime, any place. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED POLICE OFFICER: Is there anything else that I need to know? M: Yes, I'm terrified. I am absolutely terrified. I might still be functioning, but that doesn't mean that I'm not terrified. SHAHANI: In court, M. 's ex agreed to stay away from her, and the judge ordered him to stay away from their child. He found M. 's fears for her child's safety were credible. While she has full custody now and no contact with her ex, she continues to wonder what he can and cannot see. Aarti Shahani, NPR News. (SOUNDBITE OF AETHER SONG, \"CATHARSIS\") RACHEL MARTIN, HOST:  Digital spy tools are changing divorce as we know it. NPR spoke with dozens of lawyers and investigators who say more and more couples are turning to surveillance when their marriage falls apart. From something as simple as the Find My iPhone feature to insidious spyware that can be installed in a spouse's computer or phone, these tools are cheap and easy to use. NPR's Aarti Shahani has the story. AARTI SHAHANI, BYLINE: A woman discovers she wasn't going crazy. It wasn't in her head. Her ex-husband was in fact following her every move. (SOUNDBITE OF BEEPS) SHAHANI: This is the recording from a police precinct where a sergeant is taking her statement. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED POLICE OFFICER: Just for the record, state your first, middle and last name. SHAHANI: We're going to call her M. NPR interviewed her, her lawyers, the sergeant, and we reviewed her court filings and those of her ex-husband. We won't disclose their names or where she is because she says she fears for her safety. All summer, M. worried her ex-husband was stalking her. As she tells the sergeant, her ex would know where she was, whom she visited, down to the time of day and street. (SOUNDBITE OF ARCHIVED RECORDING) M: There's not really much I can do, so I've understood that what I have to do is I have to make sure that I'm as careful as possible, I have as many safety plans as possible. . . SHAHANI: M. started to change the way she drove - slowing down, driving in circles - in case a private eye was on her. But she didn't see one. Then she went online, and she learned about GPS trackers, small devices you can slip into a car to monitor where it goes 24/7. She looked for one and couldn't find any, but then at her local auto shop, the mechanic found a GPS tracker hidden near the front left tire. That's why she came to this police station. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED POLICE OFFICER: How did you know they found this? M: Chris, the service representative, showed me. She pushed the button in order to show that there was still 25-50 percent energy on it. SHAHANI: Meaning the batteries were fresh, not old. It had been nearly a year since M. left her ex-husband, moved out. But he couldn't let go, and through technology, he didn't have to. This might sound highly invasive, though it turns out, at least in this case, it's completely legal. M. 's husband acknowledged through a lawyer's letter and in family court that he had the GPS tracker installed. The sergeant from the police station - he did a criminal investigation, but he tells NPR prosecutors would not prosecute because the car was jointly owned. If it belongs to both of them, the ex has a right to track it. M: I'm now fully aware that all of those times that I thought I was keeping myself safe, all of those times that I was leaving town, all of those times that I was staying in different places or staying at friends' houses, I never was safe. SHAHANI: Welcome to divorce in the 21st century, where what it means to be safe, how much privacy you're entitled to, is an open question. NPR talked with dozens of marital experts, lawyers, investigators and a leading family judge, and they tell NPR, digital spying is changing divorce as we know it. The tools are abundant, clients use it, and the laws are murky. M. doesn't think her ex stopped at the GPS tracker. She suspects he used another more invasive tool too - spyware on her phone. Sitting in her lawyer's office, M. tells the story of how he went from love of her life to control freak. She says it started with verbal fights over the baby crying or shopping bills. Then one day - she explained this in court, too - he choked her. M: And I wasn't scared. I was shocked. I wasn't scared at all. There was no moment in that interaction where I was scared. I was too shocked. I just couldn't believe it. SHAHANI: And was now worried about physical safety - hers and her child's. She didn't leave right away. She stuck it out until this one night, she says, when he grabbed her and told her, you belong to me. It just clicked. M: I can't explain it, but I knew we needed to leave, and we need to leave fast. SHAHANI: M. took their child and fled. Then he filed for divorce. In family court, in her affidavits and oral testimony, M. laid out her fear of physical abuse and electronic surveillance. M. claimed her ex seemed to know the contents of her text messages, what friends she talked to, even after she left the house. Her ex denied all the allegations, and the judge - the judge focused on the physical stuff. That makes sense. Choking is a familiar offense where the harm is tangible. This newer, quieter intrusion - spying - is harder to grasp. M. gives NPR this one example of a creepy message her ex-husband sent. M: I know all of the ways you've described to me to your friend. And snippets of how I described him were then forwarded to me as a text message. SHAHANI: To this day, M. wonders if he knew this through spyware. These are apps. Like Netflix, you pay a subscription - say, 16. 99 a month - and in return, you get to see everything on your target's phone, every incoming and outgoing message. M. didn't turn to the police because she figured in domestic cases, they investigate assault, not iPhones. Instead, M. went to the Apple Store, where she had to explain her situation in front of a lot of people at the Genius Bar. It was humiliating. M: I can't be the only person who has no clue, goes into the Apple Store and starts jabbering about, I need help because I have stalking issues, which, in itself, sometimes makes people think that you are the stalker. SHAHANI: M. said the Apple geniuses did not look for spyware on her phone. When they saw she was scared, she said, they helped her by swapping the device for a brand-new one. Unfortunately, that also meant the evidence was thrown out. This happens all the time in spyware cases, the experts tell NPR. Victims solve the immediate problem, but that hampers any future investigation. Spyware itself is not the crime. Dozens of companies sell it legally as a tool to monitor kids and employees. But using it secretly on your spouse - that typically is not legal. Back at the police station, M. spelled out for the sergeant how it feels to be watched anytime, any place. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED POLICE OFFICER: Is there anything else that I need to know? M: Yes, I'm terrified. I am absolutely terrified. I might still be functioning, but that doesn't mean that I'm not terrified. SHAHANI: In court, M. 's ex agreed to stay away from her, and the judge ordered him to stay away from their child. He found M. 's fears for her child's safety were credible. While she has full custody now and no contact with her ex, she continues to wonder what he can and cannot see. Aarti Shahani, NPR News. (SOUNDBITE OF AETHER SONG, \"CATHARSIS\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-05-575876633": {"title": "Nearly Every Computer Could Be Impacted By New Security Flaws : NPR", "url": "https://www.npr.org/2018/01/05/575876633/nearly-every-computer-could-be-impacted-by-new-security-flaws", "author": "No author found", "published_date": "2018-01-05", "content": "STEVE INSKEEP, HOST: If you own an iPhone or a Mac or pretty much any smartphone or computer, chances are you have a security problem, a problem with the chips that power those devices. Researchers discovered they are vulnerable to hackers and have been for years. The names given to these defects are not exactly comforting - Meltdown, Spectre. Matt Tait's going to discuss this with us. He's a cybersecurity fellow at the University of Texas at Austin. Good morning, sir. MATT TAIT: Good morning. Thanks for having me. INSKEEP: OK. So I'm holding a smartphone. Is this defect essentially a door that's left open to my supposedly secure device? TAIT: Basically, yes. What's happened is there's two of these vulnerabilities, which affect basically all computer processes which exist in your phone, in your laptop, in cloud computing environments, as well. And the problem is that this defect affects the hardware itself, which means that, unlike software, where we can just ship an ordinary software update to fix these defects, we can't just ship it. INSKEEP: And the defect means that a hacker could get into that phone, get into that computer, get the information out? TAIT: So what the defect allows is for malware to steal the computer memory of a different process that's running. And that's particularly dangerous in the context of cloud computing, where, of course, lots of different people are using the same computers. And you don't want some people to steal the memory from other customers' devices. INSKEEP: Do you have any sense of how much this defect has been exploited? TAIT: So we can't tell whether or not it's been exploited. It's completely invisible. But what we have been able to see is that, although we can't fix the hardware itself, we've been able to invent new bits of computer science in order to make operating systems safe in order to protect against this particular defect from being exploited. INSKEEP: How so? TAIT: So in the event that you install your software updates - and, you know, Microsoft and Linux and Apple have all issued software updates that will protect the operating system against this defect being exploited - then hackers can't use this vulnerability in order to attack other processes and steal their computer. INSKEEP: So your argument is that it's - well, their argument is that it's OK now, so long as you've taken whatever updates you've been offered. TAIT: So for people at home, yes. The takeaway is that you need to install your security updates. And then you'll be able to protect yourself against a lot of these defects. And, really, the interesting thing about this vulnerability is the sheer amount of work that's had to be put in by operating system developers, people that make web browsers, by people that work in cloud computing companies in order to find completely novel ways of protecting against this vulnerability from being exploited. INSKEEP: You know, I got to ask - sometimes, when there's a disaster in the physical world - you know, the dam breaks, the bridge collapses - there's somebody who warned that there was a problem that was overlooked. Is there any evidence that chip makers and computer companies had some kind of warning that there was a problem here and went ahead and sold millions and millions of devices? TAIT: So we don't know whether or not they knew that this was going to be a defect in advance. But what we do know is that this vulnerability has been worked on for months and months and months by a very large amount of people in the U. S. technology community because it's such a weird vulnerability that required completely new parts of computer science to be invented in order to find ways to protect against it. INSKEEP: And they were doing this in secret, in effect, to avoid word of the vulnerability spreading too far? TAIT: Yes. So this was completely secret. We started to get word that this vulnerability might exist. And people were able to reverse engineer what that vulnerability was in the final days of the embargo. But, yes, this was all being done in secret by lots of very big computer technology companies. INSKEEP: OK. Matt Tait, thanks very much. Really appreciate it. TAIT: Thanks so much for having me. INSKEEP: He's senior cybersecurity fellow at the University of Texas at Austin. STEVE INSKEEP, HOST:  If you own an iPhone or a Mac or pretty much any smartphone or computer, chances are you have a security problem, a problem with the chips that power those devices. Researchers discovered they are vulnerable to hackers and have been for years. The names given to these defects are not exactly comforting - Meltdown, Spectre. Matt Tait's going to discuss this with us. He's a cybersecurity fellow at the University of Texas at Austin. Good morning, sir. MATT TAIT: Good morning. Thanks for having me. INSKEEP: OK. So I'm holding a smartphone. Is this defect essentially a door that's left open to my supposedly secure device? TAIT: Basically, yes. What's happened is there's two of these vulnerabilities, which affect basically all computer processes which exist in your phone, in your laptop, in cloud computing environments, as well. And the problem is that this defect affects the hardware itself, which means that, unlike software, where we can just ship an ordinary software update to fix these defects, we can't just ship it. INSKEEP: And the defect means that a hacker could get into that phone, get into that computer, get the information out? TAIT: So what the defect allows is for malware to steal the computer memory of a different process that's running. And that's particularly dangerous in the context of cloud computing, where, of course, lots of different people are using the same computers. And you don't want some people to steal the memory from other customers' devices. INSKEEP: Do you have any sense of how much this defect has been exploited? TAIT: So we can't tell whether or not it's been exploited. It's completely invisible. But what we have been able to see is that, although we can't fix the hardware itself, we've been able to invent new bits of computer science in order to make operating systems safe in order to protect against this particular defect from being exploited. INSKEEP: How so? TAIT: So in the event that you install your software updates - and, you know, Microsoft and Linux and Apple have all issued software updates that will protect the operating system against this defect being exploited - then hackers can't use this vulnerability in order to attack other processes and steal their computer. INSKEEP: So your argument is that it's - well, their argument is that it's OK now, so long as you've taken whatever updates you've been offered. TAIT: So for people at home, yes. The takeaway is that you need to install your security updates. And then you'll be able to protect yourself against a lot of these defects. And, really, the interesting thing about this vulnerability is the sheer amount of work that's had to be put in by operating system developers, people that make web browsers, by people that work in cloud computing companies in order to find completely novel ways of protecting against this vulnerability from being exploited. INSKEEP: You know, I got to ask - sometimes, when there's a disaster in the physical world - you know, the dam breaks, the bridge collapses - there's somebody who warned that there was a problem that was overlooked. Is there any evidence that chip makers and computer companies had some kind of warning that there was a problem here and went ahead and sold millions and millions of devices? TAIT: So we don't know whether or not they knew that this was going to be a defect in advance. But what we do know is that this vulnerability has been worked on for months and months and months by a very large amount of people in the U. S. technology community because it's such a weird vulnerability that required completely new parts of computer science to be invented in order to find ways to protect against it. INSKEEP: And they were doing this in secret, in effect, to avoid word of the vulnerability spreading too far? TAIT: Yes. So this was completely secret. We started to get word that this vulnerability might exist. And people were able to reverse engineer what that vulnerability was in the final days of the embargo. But, yes, this was all being done in secret by lots of very big computer technology companies. INSKEEP: OK. Matt Tait, thanks very much. Really appreciate it. TAIT: Thanks so much for having me. INSKEEP: He's senior cybersecurity fellow at the University of Texas at Austin.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-05-575876563": {"title": "Suspicious Spouses Monitor Partners Digitally, Divorce Lawyers Say : NPR", "url": "https://www.npr.org/2018/01/05/575876563/suspicious-spouses-monitor-partners-digitally-divorce-lawyers-say", "author": "No author found", "published_date": "2018-01-05", "content": "STEVE INSKEEP, HOST: Let's dig more deeply into a trend in love - or rather, its end. We reported yesterday on a woman whose ex-husband used a digital spying tool to track her moves. Divorce lawyers say electronic spying factors in many of their cases. So let's ask how this spying works and whether it's legal. NPR's Aarti Shahani and Lauren Silverman of our member station KERA have been covering this story, and they're both with us. Hi, guys. AARTI SHAHANI, BYLINE: Hi. LAUREN SILVERMAN, BYLINE: Hey, Steve. INSKEEP: Aarti, you first, what got you thinking that this might be a trend? SHAHANI: Well, you know, as a matter of fact, I have friends who are going through divorces - ugly divorces. And they've absolutely thought about spying tools - I mean, whether they're concerned their ex is using it or, frankly, they've thought about using it. This stuff hits very close to home. People talk on Facebook groups about it. You can find them pretty easily on Google. And that basically got us thinking, could this be a real trend? So we started calling family lawyers - people who presumably would have their finger on the pulse. And they said, yep (ph), this is happening enough so that it's actually changing their practice. INSKEEP: Changing their practice, Lauren Silverman? SILVERMAN: Well, what's different is the evidence that they have to work with. We talked with more than two dozen lawyers and digital forensics investigators. And they said clients are often talking about wanting to spy or worry that they've been spied on. But the bulk of the spying is pretty subtle - maybe a husband who uses that Find My iPhone feature to see if his wife is actually at the gym, where she says she is. We talked to a lawyer in Dallas, Rick Robertson, and he says clients are using digital tools they already have. RICK ROBERTSON: I've had clients who came in and say, I left my iPad in my husband's car and I found out all this stuff. So technically, they're using tracking technology. It's just not necessarily something illegal. SILVERMAN: And when you think about it, Steve, a lot of parents are using apps to monitor their kids to try and keep them safe online. If you stop trusting your romantic partner, you can use those same tools on him or her. INSKEEP: Well, Aarti, at what point does this become illegal? SHAHANI: OK. So we're not going to give legal advice. But, in general, there are two things you have to ask. One, are you intercepting communication illegally? That means eavesdropping or wiretapping. And then, separately, whose property are you monitoring? Is it yours or someone else's? What courts have found is when a couple jointly owns a car, either person can have a GPS tracker in it and arguably that's legal. But with smartphones, it's different. Even if you paid for the phone, it's considered such an intimate device it belongs to the person who's using it. INSKEEP: OK. So putting spyware on somebody else's phone - anybody else's phone, even your spouse, that's going across the line? SILVERMAN: Probably. But you can use digital tracking technology legitimately to monitor kids or employees. And that's exactly how spyware makers market the technology. What becomes dicey territory is, as you said, when you use it on an intimate partner. For example, in the case of one person I interviewed, he found spyware on his home computer which his ex-wife admitted to putting there. And he's angry at the people who make spyware. He says, hey, you say it's for monitoring kids and employees, but, in reality, exes are using it in breakups. SHAHANI: And I do want to note, NPR reached out to spyware makers who say exactly as Lauren mentioned, it's for kids and employees and using it on partners is illegal. But, you know, they're still creating a product that's completely hidden by design. There's not some little red icon letting the person know, hey, this thing is on, and you're being watched. INSKEEP: Well, if it's illegal to use this product in a certain way, could you actually get prosecuted for doing it? SILVERMAN: What we learned is that it's really unlikely that local police are going to go so far as to actually run a spyware detection program for that. You generally have to pay a digital investigator, and that can cost thousands of dollars. So it's just cheaper to throw out your device or to wipe it, even though by doing that you get rid of the evidence that could be used in court, which is exactly what the people Aarti and I interviewed did. INSKEEP: Wait a minute. In order to protect themselves going forward, they destroyed anything they could use to get justice for the past? SILVERMAN: Yes. SHAHANI: Exactly. And, you know, just the other thing to note is that even when there is evidence, judges and prosecutors have a hard time figuring out how to deal with it, OK? So they know what burglary looks like but hacking - not so much. So, Steve, this trend of surveillance and divorce - it becomes yet another visceral example of the law can't keep up with technology. INSKEEP: NPR's Aarti Shahani and KERA'S Lauren Silverman. Thanks, guys. SILVERMAN: Thank you. SHAHANI: Thanks. INSKEEP: And you can visit npr. org/alltech to check out their series and read up on how to get help if you think you may be a victim. STEVE INSKEEP, HOST:  Let's dig more deeply into a trend in love - or rather, its end. We reported yesterday on a woman whose ex-husband used a digital spying tool to track her moves. Divorce lawyers say electronic spying factors in many of their cases. So let's ask how this spying works and whether it's legal. NPR's Aarti Shahani and Lauren Silverman of our member station KERA have been covering this story, and they're both with us. Hi, guys. AARTI SHAHANI, BYLINE: Hi. LAUREN SILVERMAN, BYLINE: Hey, Steve. INSKEEP: Aarti, you first, what got you thinking that this might be a trend? SHAHANI: Well, you know, as a matter of fact, I have friends who are going through divorces - ugly divorces. And they've absolutely thought about spying tools - I mean, whether they're concerned their ex is using it or, frankly, they've thought about using it. This stuff hits very close to home. People talk on Facebook groups about it. You can find them pretty easily on Google. And that basically got us thinking, could this be a real trend? So we started calling family lawyers - people who presumably would have their finger on the pulse. And they said, yep (ph), this is happening enough so that it's actually changing their practice. INSKEEP: Changing their practice, Lauren Silverman? SILVERMAN: Well, what's different is the evidence that they have to work with. We talked with more than two dozen lawyers and digital forensics investigators. And they said clients are often talking about wanting to spy or worry that they've been spied on. But the bulk of the spying is pretty subtle - maybe a husband who uses that Find My iPhone feature to see if his wife is actually at the gym, where she says she is. We talked to a lawyer in Dallas, Rick Robertson, and he says clients are using digital tools they already have. RICK ROBERTSON: I've had clients who came in and say, I left my iPad in my husband's car and I found out all this stuff. So technically, they're using tracking technology. It's just not necessarily something illegal. SILVERMAN: And when you think about it, Steve, a lot of parents are using apps to monitor their kids to try and keep them safe online. If you stop trusting your romantic partner, you can use those same tools on him or her. INSKEEP: Well, Aarti, at what point does this become illegal? SHAHANI: OK. So we're not going to give legal advice. But, in general, there are two things you have to ask. One, are you intercepting communication illegally? That means eavesdropping or wiretapping. And then, separately, whose property are you monitoring? Is it yours or someone else's? What courts have found is when a couple jointly owns a car, either person can have a GPS tracker in it and arguably that's legal. But with smartphones, it's different. Even if you paid for the phone, it's considered such an intimate device it belongs to the person who's using it. INSKEEP: OK. So putting spyware on somebody else's phone - anybody else's phone, even your spouse, that's going across the line? SILVERMAN: Probably. But you can use digital tracking technology legitimately to monitor kids or employees. And that's exactly how spyware makers market the technology. What becomes dicey territory is, as you said, when you use it on an intimate partner. For example, in the case of one person I interviewed, he found spyware on his home computer which his ex-wife admitted to putting there. And he's angry at the people who make spyware. He says, hey, you say it's for monitoring kids and employees, but, in reality, exes are using it in breakups. SHAHANI: And I do want to note, NPR reached out to spyware makers who say exactly as Lauren mentioned, it's for kids and employees and using it on partners is illegal. But, you know, they're still creating a product that's completely hidden by design. There's not some little red icon letting the person know, hey, this thing is on, and you're being watched. INSKEEP: Well, if it's illegal to use this product in a certain way, could you actually get prosecuted for doing it? SILVERMAN: What we learned is that it's really unlikely that local police are going to go so far as to actually run a spyware detection program for that. You generally have to pay a digital investigator, and that can cost thousands of dollars. So it's just cheaper to throw out your device or to wipe it, even though by doing that you get rid of the evidence that could be used in court, which is exactly what the people Aarti and I interviewed did. INSKEEP: Wait a minute. In order to protect themselves going forward, they destroyed anything they could use to get justice for the past? SILVERMAN: Yes. SHAHANI: Exactly. And, you know, just the other thing to note is that even when there is evidence, judges and prosecutors have a hard time figuring out how to deal with it, OK? So they know what burglary looks like but hacking - not so much. So, Steve, this trend of surveillance and divorce - it becomes yet another visceral example of the law can't keep up with technology. INSKEEP: NPR's Aarti Shahani and KERA'S Lauren Silverman. Thanks, guys. SILVERMAN: Thank you. SHAHANI: Thanks. INSKEEP: And you can visit npr. org/alltech to check out their series and read up on how to get help if you think you may be a victim.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-07-576301218": {"title": "The Call-In: Sharing Your Passwords With Your Partner : NPR", "url": "https://www.npr.org/2018/01/07/576301218/the-call-in-sharing-your-passwords-with-your-partner", "author": "No author found", "published_date": "2018-01-07", "content": "LULU GARCIA-NAVARRO, HOST:  Time now for The Call-In. (SOUNDBITE OF CORDUROI'S \"MY DEAR\")GARCIA-NAVARRO: Digital privacy in relationships - sharing passwords or devices with your significant other can be a convenience but at what cost? Last week, we asked listeners to tell us how much of their digital lives they share with their spouse or partner. We heard from many of you. This email from Abby Power in Minneapolis, Minn. , sums up many of your experiences. No secret passwords in our house, she said. Myself, my husband and my two teenage boys all share passwords with each other. And we don't violate each other's privacy - fingers crossed that everyone is doing the right thing. But sometimes, having the keys to your boyfriend's digital kingdom can be a temptation. Becky McDougal of Malden, Mass. , had complete access to her fiance of four and a half years' phone until one day when she got a little too curious about something she was really hoping would happen. BECKY MCDOUGAL: We had just moved into a condo that he bought. And we've had engagement conversations. And I felt like I just want to know. I'm a little bit curious. Are there clues? When is this going to happen? GARCIA-NAVARRO: When was he going to pop the question? You were trying to figure out when that was going to happen. MCDOUGAL: Right, right. Has anything been been done as far as buying a ring or anything like that? So I, one night, abused my thumbprint access and got into his phone. And I didn't really know what I was looking for. But I found emails that looked like plans for a weekend getaway. And I felt like that was going to be engagement weekend. And so I was getting really excited about it. And it came to the time where I thought we would be leaving and realized it's not happening when I'm asking about - not so subtly - what are our plans? What do we want to do tomorrow? And he - it was very clear nothing was really happening. And so I was a little bit disappointed. But in a turn of events, it did end up being engagement weekend. It was just in our house. GARCIA-NAVARRO: So he didn't take you anywhere, but he did pop the question. MCDOUGAL: He did. So I at least figured out the date. But I did fess up afterwards and. . . GARCIA-NAVARRO: So you confessed? MCDOUGAL: I did. I confessed. I felt like it was the right thing to do because having that digital access, that thumbprint, is a big trust. And I did know that I broke that. And I could've really ruined something big for him. GARCIA-NAVARRO: So how did he react when you told him that you had been snooping through the phone? MCDOUGAL: When he - his reaction - I don't think he was mad. But certainly, we both talked about it and agreed. I did not want that access again. I'm a really nosy person, clearly. And I don't want to spoil another big surprise like that ever again. So - and he felt like, I don't really want to give you that access again. There's nothing that you really need in there. So it did change things in our relationship there. But we certainly both trust each other. And there's nothing I would hide from him anyways. GARCIA-NAVARRO: So let me understand. So now you don't have access to his phone anymore. MCDOUGAL: Correct, yes. GARCIA-NAVARRO: How do you feel about this now when you look back on what happened? MCDOUGAL: I feel like it was all fun and games until it wasn't, you know, and I feel like I'm not proud of what I did. And I think that's important to be honest about that. And it still is a breach of trust to go and snoop around and dig for that when I should've just trusted it's coming, and it's going to happen in a beautiful way no matter what. GARCIA-NAVARRO: Becky and her fiance are getting married, despite the digital transgressions, this June. NANCY BAYM: You know, I taught interpersonal communication for about 25 years. And I heard that line - we share everything. We have no secrets - so often. And you know what? It's not true. It's never true. There's all kinds of things that we don't tell our partners just because we love them, and we want to be kind to them. GARCIA-NAVARRO: That's Nancy Baym of Microsoft Research. She's author of the book \"Personal Connections In The Digital Age. \" She says how digital access is negotiated between partners depends on their expectations. And those can change overtime. BAYM: I don't think that it's necessarily something where on one day you can say, OK, here's our rules. And you never ever have to revisit it again because you might find that down the road, something you thought would bother you didn't, or something that you thought would not bother you actually bothered you a lot. GARCIA-NAVARRO: And if you screw up and maybe get a little too snoopy. . . BAYM: Probably, the best strategy is not to be blaming the technology and not necessarily to be blaming the other person but to be looking for - what is the underlying reason that this person wanted to do that in the first place? And what kinds of relational concerns or issues is that pointing out? GARCIA-NAVARRO: There are broader issues, though, when you are navigating digital privacy and relationships. WOODROW HARTZOG: Often, the only threat that we think about is, would this person want to log in and snoop through all my things? But that's not the only thing that can go wrong when you share your username or password with someone. GARCIA-NAVARRO: Woodrow Hartzog is a professor of law and computer science at Northeastern University. HARTZOG: The person that you trust might be totally trustworthy but just make a mistake by clicking on the wrong link that downloads a virus or getting hacked somehow and compromising your username and password without even meaning to. GARCIA-NAVARRO: And he says, don't feel bad if your spouse or partner doesn't give you total access. HARTZOG: Just because you don't share anything doesn't mean you're trying to hide anything. And I think the notion that privacy is about hiding things is really outdated in the modern world. And we've got a lot of other things to worry about. GARCIA-NAVARRO: That was Woodrow Hartzog, professor of law and computer science at Northeastern University. (SOUNDBITE OF CORDUROI'S \"MY DEAR\")GARCIA-NAVARRO: Next week on The Call-In, there's a massive nursing shortage across the country. Depending on where you live, nurses can be in short supply. We want to hear from people in the field. Are you a nurse who's seeing your industry change? Are you at the end of your career? We're just getting started. We want to hear the challenges you're facing. Call in at 202-216-9217. Be sure to include your full name, where you're from and your phone number, and we may use it on the air. That's 202-216-9217. (SOUNDBITE OF CORDUROI'S \"MY DEAR\") LULU GARCIA-NAVARRO, HOST:   Time now for The Call-In. (SOUNDBITE OF CORDUROI'S \"MY DEAR\") GARCIA-NAVARRO: Digital privacy in relationships - sharing passwords or devices with your significant other can be a convenience but at what cost? Last week, we asked listeners to tell us how much of their digital lives they share with their spouse or partner. We heard from many of you. This email from Abby Power in Minneapolis, Minn. , sums up many of your experiences. No secret passwords in our house, she said. Myself, my husband and my two teenage boys all share passwords with each other. And we don't violate each other's privacy - fingers crossed that everyone is doing the right thing. But sometimes, having the keys to your boyfriend's digital kingdom can be a temptation. Becky McDougal of Malden, Mass. , had complete access to her fiance of four and a half years' phone until one day when she got a little too curious about something she was really hoping would happen. BECKY MCDOUGAL: We had just moved into a condo that he bought. And we've had engagement conversations. And I felt like I just want to know. I'm a little bit curious. Are there clues? When is this going to happen? GARCIA-NAVARRO: When was he going to pop the question? You were trying to figure out when that was going to happen. MCDOUGAL: Right, right. Has anything been been done as far as buying a ring or anything like that? So I, one night, abused my thumbprint access and got into his phone. And I didn't really know what I was looking for. But I found emails that looked like plans for a weekend getaway. And I felt like that was going to be engagement weekend. And so I was getting really excited about it. And it came to the time where I thought we would be leaving and realized it's not happening when I'm asking about - not so subtly - what are our plans? What do we want to do tomorrow? And he - it was very clear nothing was really happening. And so I was a little bit disappointed. But in a turn of events, it did end up being engagement weekend. It was just in our house. GARCIA-NAVARRO: So he didn't take you anywhere, but he did pop the question. MCDOUGAL: He did. So I at least figured out the date. But I did fess up afterwards and. . . GARCIA-NAVARRO: So you confessed? MCDOUGAL: I did. I confessed. I felt like it was the right thing to do because having that digital access, that thumbprint, is a big trust. And I did know that I broke that. And I could've really ruined something big for him. GARCIA-NAVARRO: So how did he react when you told him that you had been snooping through the phone? MCDOUGAL: When he - his reaction - I don't think he was mad. But certainly, we both talked about it and agreed. I did not want that access again. I'm a really nosy person, clearly. And I don't want to spoil another big surprise like that ever again. So - and he felt like, I don't really want to give you that access again. There's nothing that you really need in there. So it did change things in our relationship there. But we certainly both trust each other. And there's nothing I would hide from him anyways. GARCIA-NAVARRO: So let me understand. So now you don't have access to his phone anymore. MCDOUGAL: Correct, yes. GARCIA-NAVARRO: How do you feel about this now when you look back on what happened? MCDOUGAL: I feel like it was all fun and games until it wasn't, you know, and I feel like I'm not proud of what I did. And I think that's important to be honest about that. And it still is a breach of trust to go and snoop around and dig for that when I should've just trusted it's coming, and it's going to happen in a beautiful way no matter what. GARCIA-NAVARRO: Becky and her fiance are getting married, despite the digital transgressions, this June. NANCY BAYM: You know, I taught interpersonal communication for about 25 years. And I heard that line - we share everything. We have no secrets - so often. And you know what? It's not true. It's never true. There's all kinds of things that we don't tell our partners just because we love them, and we want to be kind to them. GARCIA-NAVARRO: That's Nancy Baym of Microsoft Research. She's author of the book \"Personal Connections In The Digital Age. \" She says how digital access is negotiated between partners depends on their expectations. And those can change overtime. BAYM: I don't think that it's necessarily something where on one day you can say, OK, here's our rules. And you never ever have to revisit it again because you might find that down the road, something you thought would bother you didn't, or something that you thought would not bother you actually bothered you a lot. GARCIA-NAVARRO: And if you screw up and maybe get a little too snoopy. . . BAYM: Probably, the best strategy is not to be blaming the technology and not necessarily to be blaming the other person but to be looking for - what is the underlying reason that this person wanted to do that in the first place? And what kinds of relational concerns or issues is that pointing out? GARCIA-NAVARRO: There are broader issues, though, when you are navigating digital privacy and relationships. WOODROW HARTZOG: Often, the only threat that we think about is, would this person want to log in and snoop through all my things? But that's not the only thing that can go wrong when you share your username or password with someone. GARCIA-NAVARRO: Woodrow Hartzog is a professor of law and computer science at Northeastern University. HARTZOG: The person that you trust might be totally trustworthy but just make a mistake by clicking on the wrong link that downloads a virus or getting hacked somehow and compromising your username and password without even meaning to. GARCIA-NAVARRO: And he says, don't feel bad if your spouse or partner doesn't give you total access. HARTZOG: Just because you don't share anything doesn't mean you're trying to hide anything. And I think the notion that privacy is about hiding things is really outdated in the modern world. And we've got a lot of other things to worry about. GARCIA-NAVARRO: That was Woodrow Hartzog, professor of law and computer science at Northeastern University. (SOUNDBITE OF CORDUROI'S \"MY DEAR\") GARCIA-NAVARRO: Next week on The Call-In, there's a massive nursing shortage across the country. Depending on where you live, nurses can be in short supply. We want to hear from people in the field. Are you a nurse who's seeing your industry change? Are you at the end of your career? We're just getting started. We want to hear the challenges you're facing. Call in at 202-216-9217. Be sure to include your full name, where you're from and your phone number, and we may use it on the air. That's 202-216-9217. (SOUNDBITE OF CORDUROI'S \"MY DEAR\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-08-576566436": {"title": "The Most-Hyped Items For This Year's Consumer Electronics Show : NPR", "url": "https://www.npr.org/2018/01/08/576566436/the-most-hyped-items-for-this-years-consumer-electronics-show", "author": "No author found", "published_date": "2018-01-08", "content": "ARI SHAPIRO, HOST:  Can robots help U. S. furniture makers survive? That's a question we'll try to answer on this week's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")SHAPIRO: In a few minutes, we'll hear the latest piece in our series, Is My Job Safe? First let's turn to this year's Consumer Electronics Show in Las Vegas. This is the place where the tech industry gives us a glimpse of the future that they predict. Joanna Stern is there. She's the personal technology columnist for The Wall Street Journal. Welcome. JOANNA STERN: Great to speak to you. SHAPIRO: What's getting the most buzz this year? STERN: Lots of things are getting some buzz here, but I think the biggest thing is the fight between Amazon and Google in the personal assistant space. Amazon has its Alexa, and Alexa was huge at CES last year. All these manufacturers integrating Alexa - the voice assistant - in everything from smart home products to laptops and different types of computing devices, cars, et cetera. This year, Google is here in a big way. Google hasn't been at the show in a number of years in a booth presence. But they've put up a huge booth, and they're talking to all the different manufacturers here and saying, we're going to have our Google assistant in those products. And so they are advertising their, hey, Google or the Google assistant pretty much everywhere you are in Vegas. SHAPIRO: Often at the Consumer Electronics Show, we hear about things that sound really futuristic, and so I'm wondering how many of those predictions have come to pass. When you think about the things that you saw at last year's show, are many of those integrated into our daily life now or at least something that people are familiar with? STERN: I think there's these wacky ideas that come out of CES. For instance, last night I saw a robot that folds laundry. But is something like that coming along - I don't know - in the next year? And is it going to come along from a major manufacturer you want to buy it from - probably not. I mean, this company definitely seems to be putting together some neat stuff, but that's sort of what CES is all about - seeing these prototypes and how they may come to fruition. I sometimes think 5 to 10 years out. This year, the focus on the voice assistants - obviously that's here right now. Another major thing people talking about here are the autonomous vehicles - the self-driving vehicles, self-driving cars - also a number of years out just mostly because of regulatory issues. But you know, companies like Lyft here are picking people up in self-driving cars - lots of other auto manufacturers showing off their improvements there, too. SHAPIRO: What about the technologies that so many of us interact with every day - our laptops, our mobile phones? Are you seeing dramatic upgrades or changes in what these tech companies predict for the future of those? STERN: Not so much on the phones here but definitely on the laptops. Dell has a new laptop but a lot more of the same. It's looks very nice but a lot more of - you know, thinner, lighter, longer battery life. The interesting thing to me is - is that as you look around at CES, all of these innovations depend on a type of technology that's actually pretty old, and that's lithium-ion batteries. And what are these companies doing to extend the life of these to make them safer and to, you know, really deal with the fact that we're holding onto these gadgets for longer; these batteries need to last for longer - so some interesting developments there but again kind of showing how our future is being held back by some of these technologies we need to work. SHAPIRO: OK, so battery life is very functional, utilitarian, important but, some might say, boring. Is there anything you've seen that is just really fantastical that you're telling everybody about? STERN: Yesterday I was followed around by a piece of luggage that is a self-driving piece of luggage. SHAPIRO: (Laughter). STERN: And I kid you not. SHAPIRO: And - what? - it just, like, connects to your phone and knows not to stay far from the thing that's in your pocket. Is that how that works? STERN: Exactly right. And so because pulling your luggage is hard at the airport, these bags can follow you around. And actually it worked pretty well, which was, you know, the really surprising part. And on top of that, not one, not two, but three or four companies are showing this off here. So yeah, that's the main trend. SHAPIRO: That's Wall Street Journal personal technology columnist Joanna Stern speaking with us from the Consumer Electronics Show in Las Vegas. Thanks so much. STERN: Thank you. ARI SHAPIRO, HOST:   Can robots help U. S. furniture makers survive? That's a question we'll try to answer on this week's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") SHAPIRO: In a few minutes, we'll hear the latest piece in our series, Is My Job Safe? First let's turn to this year's Consumer Electronics Show in Las Vegas. This is the place where the tech industry gives us a glimpse of the future that they predict. Joanna Stern is there. She's the personal technology columnist for The Wall Street Journal. Welcome. JOANNA STERN: Great to speak to you. SHAPIRO: What's getting the most buzz this year? STERN: Lots of things are getting some buzz here, but I think the biggest thing is the fight between Amazon and Google in the personal assistant space. Amazon has its Alexa, and Alexa was huge at CES last year. All these manufacturers integrating Alexa - the voice assistant - in everything from smart home products to laptops and different types of computing devices, cars, et cetera. This year, Google is here in a big way. Google hasn't been at the show in a number of years in a booth presence. But they've put up a huge booth, and they're talking to all the different manufacturers here and saying, we're going to have our Google assistant in those products. And so they are advertising their, hey, Google or the Google assistant pretty much everywhere you are in Vegas. SHAPIRO: Often at the Consumer Electronics Show, we hear about things that sound really futuristic, and so I'm wondering how many of those predictions have come to pass. When you think about the things that you saw at last year's show, are many of those integrated into our daily life now or at least something that people are familiar with? STERN: I think there's these wacky ideas that come out of CES. For instance, last night I saw a robot that folds laundry. But is something like that coming along - I don't know - in the next year? And is it going to come along from a major manufacturer you want to buy it from - probably not. I mean, this company definitely seems to be putting together some neat stuff, but that's sort of what CES is all about - seeing these prototypes and how they may come to fruition. I sometimes think 5 to 10 years out. This year, the focus on the voice assistants - obviously that's here right now. Another major thing people talking about here are the autonomous vehicles - the self-driving vehicles, self-driving cars - also a number of years out just mostly because of regulatory issues. But you know, companies like Lyft here are picking people up in self-driving cars - lots of other auto manufacturers showing off their improvements there, too. SHAPIRO: What about the technologies that so many of us interact with every day - our laptops, our mobile phones? Are you seeing dramatic upgrades or changes in what these tech companies predict for the future of those? STERN: Not so much on the phones here but definitely on the laptops. Dell has a new laptop but a lot more of the same. It's looks very nice but a lot more of - you know, thinner, lighter, longer battery life. The interesting thing to me is - is that as you look around at CES, all of these innovations depend on a type of technology that's actually pretty old, and that's lithium-ion batteries. And what are these companies doing to extend the life of these to make them safer and to, you know, really deal with the fact that we're holding onto these gadgets for longer; these batteries need to last for longer - so some interesting developments there but again kind of showing how our future is being held back by some of these technologies we need to work. SHAPIRO: OK, so battery life is very functional, utilitarian, important but, some might say, boring. Is there anything you've seen that is just really fantastical that you're telling everybody about? STERN: Yesterday I was followed around by a piece of luggage that is a self-driving piece of luggage. SHAPIRO: (Laughter). STERN: And I kid you not. SHAPIRO: And - what? - it just, like, connects to your phone and knows not to stay far from the thing that's in your pocket. Is that how that works? STERN: Exactly right. And so because pulling your luggage is hard at the airport, these bags can follow you around. And actually it worked pretty well, which was, you know, the really surprising part. And on top of that, not one, not two, but three or four companies are showing this off here. So yeah, that's the main trend. SHAPIRO: That's Wall Street Journal personal technology columnist Joanna Stern speaking with us from the Consumer Electronics Show in Las Vegas. Thanks so much. STERN: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-08-576566449": {"title": "How Robots Are Helping A Furniture Shop Without Putting Workers Out Of Jobs : NPR", "url": "https://www.npr.org/2018/01/08/576566449/how-robots-are-helping-a-furniture-shop-without-putting-workers-out-of-jobs", "author": "No author found", "published_date": "2018-01-08", "content": "KELLY MCEVERS, HOST:  As technology advances and robots learn new tricks like folding laundry, it makes people wonder, is my job safe? That is the name of an NPR series that looks at how tech is changing our work, and today, furniture making. It's based on old-fashioned handiwork, but a furniture maker in Massachusetts is using robots not to replace workers, but to help them. From member station WBUR in Boston, Asma Khalid reports. ASMA KHALID, BYLINE: Josh Weissman is a furniture man. His dad started the company Moduform in 1976. It makes nightstands, beds, dressers, the whole deal for university dorms and hospitals. JOSH WEISSMAN: Back in the '80s and '90s there was a lot of furniture manufacturing done in north central Massachusetts. So we had craftspeople because in this neck of the woods, in north central Massachusetts, it was a haven for furniture. KHALID: But Weissman says times have changed. People don't want to stand on a production floor for eight to 10 hours a day picking up a piece of wood and putting it through a sanding machine. WEISSMAN: When we put a job ad out there, we're lucky if we put an ad out there if we get five or six responses. KHALID: One day in the summer of 2016, Weissman had this backlog of customer orders to fill, and he was getting really worried. He turned on his computer and noticed a news blurb about a company called Rethink Robotics. It's the brainchild of Rodney Brooks, the man who for years had led the Computer Science and Artificial Intelligence Lab at MIT. RODNEY BROOKS: Our robots do simple, repetitive tasks. KHALID: I meet Brooks at his company's swanky warehouse office in Boston, where dozens of engineers are testing out a robotic arm that you can program and wheel around. (SOUNDBITE OF WHIRRING)KHALID: That sound is the robot picking up a circuit board and putting it down again and again. That's what it's good at. (SOUNDBITE OF WHIRRING)WEISSMAN: This is the original factory, right? So. . . KHALID: Back at the furniture company, Weissman takes me to see the Rethink robot he bought. It's got four grippers on a red arm that swivels to put together a dresser drawer. WEISSMAN: It's picking the drawer front up and it's feeding it into the machine that's actually cutting and routing those dovetails. (SOUNDBITE OF SAW BUZZING)KHALID: That noise is the wood getting cut. In the past, you would have had somebody feeding the machine by hand, someone like Brandon Correia. BRANDON CORREIA: I started working here over the summer - just a plain factory worker. Like, you know, sometimes I would work this. Sometimes I would be assembling. KHALID: So you've done this job by hand. CORREIA: Yeah. KHALID: How was it? CORREIA: It's boring. It gets very old very quickly. KHALID: When Moduform brought this robot in, Correia was asked to set it up. CORREIA: This was really the first time I've ever tried to program anything like this. KHALID: He says it was hard. But now he's figured out how to set different programs for different drawer sizes. He's essentially become the robot's supervisor. CORREIA: So I'll come back after a half hour, see if it's working, make sure everything's going OK. I'll come back when it's done and make sure there were no errors. KHALID: Correia says this one robot has already changed his job. It frees him up to do other work like managing customer orders. He says all this talk about robots taking jobs feels overblown. CORREIA: There is still some sort of human that sets up the robot. You know, could we have three of these and have one person program all three and then we don't need as many workers? I could see that. But the way they are now, they're not foolproof. KHALID: Correia is kind of a rarity at Moduform. He's just 24. The average worker here is over 50. And that worries the company's owner, Josh Weissman. He's hoping that maybe the chance to use computer skills and robots will make this old-school job more attractive to young workers. For NPR News, I'm Asma Khalid. KELLY MCEVERS, HOST:   As technology advances and robots learn new tricks like folding laundry, it makes people wonder, is my job safe? That is the name of an NPR series that looks at how tech is changing our work, and today, furniture making. It's based on old-fashioned handiwork, but a furniture maker in Massachusetts is using robots not to replace workers, but to help them. From member station WBUR in Boston, Asma Khalid reports. ASMA KHALID, BYLINE: Josh Weissman is a furniture man. His dad started the company Moduform in 1976. It makes nightstands, beds, dressers, the whole deal for university dorms and hospitals. JOSH WEISSMAN: Back in the '80s and '90s there was a lot of furniture manufacturing done in north central Massachusetts. So we had craftspeople because in this neck of the woods, in north central Massachusetts, it was a haven for furniture. KHALID: But Weissman says times have changed. People don't want to stand on a production floor for eight to 10 hours a day picking up a piece of wood and putting it through a sanding machine. WEISSMAN: When we put a job ad out there, we're lucky if we put an ad out there if we get five or six responses. KHALID: One day in the summer of 2016, Weissman had this backlog of customer orders to fill, and he was getting really worried. He turned on his computer and noticed a news blurb about a company called Rethink Robotics. It's the brainchild of Rodney Brooks, the man who for years had led the Computer Science and Artificial Intelligence Lab at MIT. RODNEY BROOKS: Our robots do simple, repetitive tasks. KHALID: I meet Brooks at his company's swanky warehouse office in Boston, where dozens of engineers are testing out a robotic arm that you can program and wheel around. (SOUNDBITE OF WHIRRING) KHALID: That sound is the robot picking up a circuit board and putting it down again and again. That's what it's good at. (SOUNDBITE OF WHIRRING) WEISSMAN: This is the original factory, right? So. . . KHALID: Back at the furniture company, Weissman takes me to see the Rethink robot he bought. It's got four grippers on a red arm that swivels to put together a dresser drawer. WEISSMAN: It's picking the drawer front up and it's feeding it into the machine that's actually cutting and routing those dovetails. (SOUNDBITE OF SAW BUZZING) KHALID: That noise is the wood getting cut. In the past, you would have had somebody feeding the machine by hand, someone like Brandon Correia. BRANDON CORREIA: I started working here over the summer - just a plain factory worker. Like, you know, sometimes I would work this. Sometimes I would be assembling. KHALID: So you've done this job by hand. CORREIA: Yeah. KHALID: How was it? CORREIA: It's boring. It gets very old very quickly. KHALID: When Moduform brought this robot in, Correia was asked to set it up. CORREIA: This was really the first time I've ever tried to program anything like this. KHALID: He says it was hard. But now he's figured out how to set different programs for different drawer sizes. He's essentially become the robot's supervisor. CORREIA: So I'll come back after a half hour, see if it's working, make sure everything's going OK. I'll come back when it's done and make sure there were no errors. KHALID: Correia says this one robot has already changed his job. It frees him up to do other work like managing customer orders. He says all this talk about robots taking jobs feels overblown. CORREIA: There is still some sort of human that sets up the robot. You know, could we have three of these and have one person program all three and then we don't need as many workers? I could see that. But the way they are now, they're not foolproof. KHALID: Correia is kind of a rarity at Moduform. He's just 24. The average worker here is over 50. And that worries the company's owner, Josh Weissman. He's hoping that maybe the chance to use computer skills and robots will make this old-school job more attractive to young workers. For NPR News, I'm Asma Khalid.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-09-576669360": {"title": "Apple Asked To Help Wean Digital-Addicted Youths : NPR", "url": "https://www.npr.org/2018/01/09/576669360/apple-asked-to-help-wean-digital-addicted-youths", "author": "No author found", "published_date": "2018-01-09", "content": "STEVE INSKEEP, HOST: Two of Apple's biggest investors don't like what Apple products do to kids. One investor is called Jana Partners. The other is the California State Teachers Retirement System. And they wrote an open letter to the company. It says smartphones are especially addictive for kids, and they want Apple to give parents more tools to protect children. The investors were advised by our next guest, Dr. Michael Rich who runs the Center on Media and Child Health at Boston Children's Hospital. Good morning, sir. MICHAEL RICH: Good morning. INSKEEP: Is addictive the right word for an iPhone? RICH: I do not believe addictive is simply because not only are we not seeing physiologic changes either when using or withdrawing, as you do with alcohol or heroin, but we're calling it problematic interactive media use for the reasons that they do get functionally impaired. They lose sleep. They place it as a priority over other activities and often will withdraw themselves from society, from families, from friends in order to stay online. So while these behaviors look much like addiction, it's actually not an accurate term, nor is it one that's helpful because of it's stigma. INSKEEP: OK, so let's not say addiction. But let's say it's a problem and something that you could find in my family if you looked around the kids in my family from time to time. But let's talk about who's responsible for fixing that. I mean, you have this device that's designed to be really easy and pleasant to use, and there are all kinds of apps, which have been deliberately designed to get people hooked on them. What is Apple's responsibility when that phone gets into the hands of a child? RICH: Well, I think it's all of our responsibility as individuals and as a society to address this. Apple has a part in that because they are brilliant designers not only of the technology but of the interface between the human psychology, the human mind, eye and finger and this thinking machine. So the goal of this letter, as I understand it, is to bring together the stakeholders - bringing their different expertise together to really think this problem through, working with the science that I and other researchers have put together and working with the technology to come up with a better mousetrap here. INSKEEP: Is the answer that I as a parent might have better tools to monitor the phone than perhaps my kid has in his or her hands? RICH: Absolutely, given the tools - but also the education and empowerment to use them. I think that what's happened is because many parents feel less adequate or less facile in that digital environment than their children, is that they check out of parenting in that space. INSKEEP: You know, that raises one other thing I want to ask you about. Anya Kamenetz, our education reporter, has talked a lot about screen time for kids on this program and has noted that you can argue that screen time is good for kids if it's interactive, if it makes them think and if the parents are participating with the kids in some way. RICH: Exactly. You know, this is really a tool just as, you know, a walk in the woods is a tool or a blackboard and a piece of chalk is a tool. We have to realize that the human element is what matters here. And whether that's a parent, whether that's a teacher, whether that's a clinician, we need to use these tools and keep them in their role in the children's lives. And ultimately what we owe our children is a rich and diverse menu of experience of which media can be one. INSKEEP: Would you keep a phone away from a kid? RICH: It depends on the age of the kid, and it depends on the kid. INSKEEP: What age? RICH: I think that, first of all, we have to give different tools to different kids at different developmental stages. And very early use, preschoolers, et cetera, they will get attracted and sucked into it, but they will not be able to regulate themselves. INSKEEP: Dr. Michael Rich, thanks very much - appreciate it. RICH: Thank you. INSKEEP: And Apple has responded saying it takes responsibility for its products and is committed to exceeding customers' expectations, especially when it comes to protecting kids. STEVE INSKEEP, HOST:  Two of Apple's biggest investors don't like what Apple products do to kids. One investor is called Jana Partners. The other is the California State Teachers Retirement System. And they wrote an open letter to the company. It says smartphones are especially addictive for kids, and they want Apple to give parents more tools to protect children. The investors were advised by our next guest, Dr. Michael Rich who runs the Center on Media and Child Health at Boston Children's Hospital. Good morning, sir. MICHAEL RICH: Good morning. INSKEEP: Is addictive the right word for an iPhone? RICH: I do not believe addictive is simply because not only are we not seeing physiologic changes either when using or withdrawing, as you do with alcohol or heroin, but we're calling it problematic interactive media use for the reasons that they do get functionally impaired. They lose sleep. They place it as a priority over other activities and often will withdraw themselves from society, from families, from friends in order to stay online. So while these behaviors look much like addiction, it's actually not an accurate term, nor is it one that's helpful because of it's stigma. INSKEEP: OK, so let's not say addiction. But let's say it's a problem and something that you could find in my family if you looked around the kids in my family from time to time. But let's talk about who's responsible for fixing that. I mean, you have this device that's designed to be really easy and pleasant to use, and there are all kinds of apps, which have been deliberately designed to get people hooked on them. What is Apple's responsibility when that phone gets into the hands of a child? RICH: Well, I think it's all of our responsibility as individuals and as a society to address this. Apple has a part in that because they are brilliant designers not only of the technology but of the interface between the human psychology, the human mind, eye and finger and this thinking machine. So the goal of this letter, as I understand it, is to bring together the stakeholders - bringing their different expertise together to really think this problem through, working with the science that I and other researchers have put together and working with the technology to come up with a better mousetrap here. INSKEEP: Is the answer that I as a parent might have better tools to monitor the phone than perhaps my kid has in his or her hands? RICH: Absolutely, given the tools - but also the education and empowerment to use them. I think that what's happened is because many parents feel less adequate or less facile in that digital environment than their children, is that they check out of parenting in that space. INSKEEP: You know, that raises one other thing I want to ask you about. Anya Kamenetz, our education reporter, has talked a lot about screen time for kids on this program and has noted that you can argue that screen time is good for kids if it's interactive, if it makes them think and if the parents are participating with the kids in some way. RICH: Exactly. You know, this is really a tool just as, you know, a walk in the woods is a tool or a blackboard and a piece of chalk is a tool. We have to realize that the human element is what matters here. And whether that's a parent, whether that's a teacher, whether that's a clinician, we need to use these tools and keep them in their role in the children's lives. And ultimately what we owe our children is a rich and diverse menu of experience of which media can be one. INSKEEP: Would you keep a phone away from a kid? RICH: It depends on the age of the kid, and it depends on the kid. INSKEEP: What age? RICH: I think that, first of all, we have to give different tools to different kids at different developmental stages. And very early use, preschoolers, et cetera, they will get attracted and sucked into it, but they will not be able to regulate themselves. INSKEEP: Dr. Michael Rich, thanks very much - appreciate it. RICH: Thank you. INSKEEP: And Apple has responded saying it takes responsibility for its products and is committed to exceeding customers' expectations, especially when it comes to protecting kids.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-09-576669374": {"title": "Time For Harassers To Be Held Accountable, Female Gamer Says : NPR", "url": "https://www.npr.org/2018/01/09/576669374/time-for-harassers-to-be-held-accountable-female-gamer-says", "author": "No author found", "published_date": "2018-01-09", "content": "STEVE INSKEEP, HOST: The #MeToo campaign forced many industries into a moral reckoning over sexual harassment and abuse. DAVID GREENE, HOST: But video game developer Brianna Wu says she's still waiting for the gaming industry to take a hard look at itself. Back in 2014, Wu and several other female gamers endured a campaign of harassment and abuse that was dubbed Gamergate. The FBI investigated but no one was prosecuted. BRIANNA WU: I can tell you in 2018 looking back at it, nothing of note really happened. We expected help to come, and it didn't. INSKEEP: Here's something that is happening in 2018 - Brianna Wu is running for Congress in the state of Massachusetts. She spoke with our co-host, Rachel Martin. WU: The video game industry traditionally has been a very male-dominated field. You know, with the advent of the iPhone, the number of women gamers exploded. We're actually more than half of gamers in 2018. And Gamergate was kind of a very aggressive backlash to that growing diversity. And what ended up happening is women like myself that have been advocating for greater inclusion in our field, we received just an extreme avalanche of death threats and rape threats and really the destruction of our personal lives in a way that was just horrifying for many people to watch. RACHEL MARTIN, BYLINE: So now we are in this #MeToo moment and the associated movement around this larger social reckoning about sexual abuse and sexual harassment in the broader culture. Is that changing things when it comes to online harassment and abuse? WU: You know, it's hard for me because I want to have a hopeful message, especially for young women that are out there listening to this, but when it comes to the game industry itself, we are not having a #MeToo moment at all. I think what a lot of women in the game industry saw with Gamergate is they saw if they came forward, help was not going to come. They saw that they will be out there on the front lines and, you know, you'll certainly have journalists that are happy to capture the spectacle. But as far as change, as far as getting backup from the industry, I think that they are scared to come forward because so many women have had their careers destroyed trying to come forward. So what I need is I need our investigative institutions to start looking at game studios. Look at the hiring, look at the culture, look at who makes it up the food chain. Because we've really created a culture of silence in my field that is very similar to why Harvey Weinstein was able to go for so long unchecked. MARTIN: So what are the broader implications if sexual harassment, gender-based abuse online isn't curtailed? WU: So if you look historically at what engineering is, you know, engineers build societies. That is what we do. So when you have an entire field of engineering and STEM where women are kind of just shown the door very subtly, what that means is that women don't get a voice in how society is running. So I think what you're seeing with the Internet right now where it's an institution that was created by men for men, and there are a lot of voices that weren't heard in the creation of that. So when we're talking about women being able to pursue STEM careers, we're really talking about women having a voice in how online systems function. MARTIN: Brianna, thanks so much. WU: Always a pleasure. (SOUNDBITE OF SABREPULSE'S \"TAKEN\")INSKEEP: Game developer and congressional candidate Brianna Wu talking with our colleague, Rachel Martin. STEVE INSKEEP, HOST:  The #MeToo campaign forced many industries into a moral reckoning over sexual harassment and abuse. DAVID GREENE, HOST:  But video game developer Brianna Wu says she's still waiting for the gaming industry to take a hard look at itself. Back in 2014, Wu and several other female gamers endured a campaign of harassment and abuse that was dubbed Gamergate. The FBI investigated but no one was prosecuted. BRIANNA WU: I can tell you in 2018 looking back at it, nothing of note really happened. We expected help to come, and it didn't. INSKEEP: Here's something that is happening in 2018 - Brianna Wu is running for Congress in the state of Massachusetts. She spoke with our co-host, Rachel Martin. WU: The video game industry traditionally has been a very male-dominated field. You know, with the advent of the iPhone, the number of women gamers exploded. We're actually more than half of gamers in 2018. And Gamergate was kind of a very aggressive backlash to that growing diversity. And what ended up happening is women like myself that have been advocating for greater inclusion in our field, we received just an extreme avalanche of death threats and rape threats and really the destruction of our personal lives in a way that was just horrifying for many people to watch. RACHEL MARTIN, BYLINE: So now we are in this #MeToo moment and the associated movement around this larger social reckoning about sexual abuse and sexual harassment in the broader culture. Is that changing things when it comes to online harassment and abuse? WU: You know, it's hard for me because I want to have a hopeful message, especially for young women that are out there listening to this, but when it comes to the game industry itself, we are not having a #MeToo moment at all. I think what a lot of women in the game industry saw with Gamergate is they saw if they came forward, help was not going to come. They saw that they will be out there on the front lines and, you know, you'll certainly have journalists that are happy to capture the spectacle. But as far as change, as far as getting backup from the industry, I think that they are scared to come forward because so many women have had their careers destroyed trying to come forward. So what I need is I need our investigative institutions to start looking at game studios. Look at the hiring, look at the culture, look at who makes it up the food chain. Because we've really created a culture of silence in my field that is very similar to why Harvey Weinstein was able to go for so long unchecked. MARTIN: So what are the broader implications if sexual harassment, gender-based abuse online isn't curtailed? WU: So if you look historically at what engineering is, you know, engineers build societies. That is what we do. So when you have an entire field of engineering and STEM where women are kind of just shown the door very subtly, what that means is that women don't get a voice in how society is running. So I think what you're seeing with the Internet right now where it's an institution that was created by men for men, and there are a lot of voices that weren't heard in the creation of that. So when we're talking about women being able to pursue STEM careers, we're really talking about women having a voice in how online systems function. MARTIN: Brianna, thanks so much. WU: Always a pleasure. (SOUNDBITE OF SABREPULSE'S \"TAKEN\") INSKEEP: Game developer and congressional candidate Brianna Wu talking with our colleague, Rachel Martin.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-09-576669332": {"title": "Ex-Google Engineer Files Suit, Saying He Was Retaliated Against : NPR", "url": "https://www.npr.org/2018/01/09/576669332/ex-google-engineer-files-suit-saying-he-was-retaliated-against", "author": "No author found", "published_date": "2018-01-09", "content": "STEVE INSKEEP, HOST: Here's a court case illustrating some of the tensions of our time. A former Google engineer filed a lawsuit against the company saying he's a victim of workplace discrimination and retaliation. He says Google discriminates against conservative white men. Here's NPR's Laura Sydell. LAURA SYDELL, BYLINE: Last year, an engineer at Google named James Damore wrote a long criticism to Google's HR team about the company's diversity program. In it, he cited studies that he said suggests that biology rather than discrimination is the reason there are fewer women engineers at Google. When his essay found its way to the public, there was a storm on social media. Ultimately, Google fired Damore. Harmeet Dhillon, Damore's attorney, says Damore was encouraged at Google to share his opinion, and he shouldn't have been fired for it. HARMEET DHILLON: They had this vague, gestalt response, saying he perpetuated gender stereotypes. Well, Google perpetuates negative gender stereotypes about men every day. SYDELL: The case was filed in California's Superior Court. Dhillon says in California it's illegal to discriminate against someone because of their political opinion. Damore is joined in the suit by other employees who Dhillon says were fired because of their political opinions. DHILLON: It's equally offensive to me to hear that white people should get rid of their toxic whiteness at Google. That's offensive. But nobody gets fired for saying that. SYDELL: Google did not respond to requests for comment. However, when Damore was fired, Google's CEO Sundar Pichai said that Google employees can express themselves, but that doesn't mean that anything goes. Deborah Rhode, a law professor at Stanford, says what the case is likely to pivot on is whether the comments of Damore, and the others bringing the lawsuit, made it a hostile work environment. She says women and minorities have special protections under the law. DEBORAH RHODE: There's also a whole line of cases on what creates a hostile environment, and verbal conduct can do that. SYDELL: Ironically, Google is facing this suit while at the same time it's fighting claims that it has systematically underpaid women. Laura Sydell, NPR News. STEVE INSKEEP, HOST:  Here's a court case illustrating some of the tensions of our time. A former Google engineer filed a lawsuit against the company saying he's a victim of workplace discrimination and retaliation. He says Google discriminates against conservative white men. Here's NPR's Laura Sydell. LAURA SYDELL, BYLINE: Last year, an engineer at Google named James Damore wrote a long criticism to Google's HR team about the company's diversity program. In it, he cited studies that he said suggests that biology rather than discrimination is the reason there are fewer women engineers at Google. When his essay found its way to the public, there was a storm on social media. Ultimately, Google fired Damore. Harmeet Dhillon, Damore's attorney, says Damore was encouraged at Google to share his opinion, and he shouldn't have been fired for it. HARMEET DHILLON: They had this vague, gestalt response, saying he perpetuated gender stereotypes. Well, Google perpetuates negative gender stereotypes about men every day. SYDELL: The case was filed in California's Superior Court. Dhillon says in California it's illegal to discriminate against someone because of their political opinion. Damore is joined in the suit by other employees who Dhillon says were fired because of their political opinions. DHILLON: It's equally offensive to me to hear that white people should get rid of their toxic whiteness at Google. That's offensive. But nobody gets fired for saying that. SYDELL: Google did not respond to requests for comment. However, when Damore was fired, Google's CEO Sundar Pichai said that Google employees can express themselves, but that doesn't mean that anything goes. Deborah Rhode, a law professor at Stanford, says what the case is likely to pivot on is whether the comments of Damore, and the others bringing the lawsuit, made it a hostile work environment. She says women and minorities have special protections under the law. DEBORAH RHODE: There's also a whole line of cases on what creates a hostile environment, and verbal conduct can do that. SYDELL: Ironically, Google is facing this suit while at the same time it's fighting claims that it has systematically underpaid women. Laura Sydell, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-09-575352051": {"title": "How Racial Discrimination Plays Out in Online Dating : NPR", "url": "https://www.npr.org/2018/01/09/575352051/least-desirable-how-racial-discrimination-plays-out-in-online-dating", "author": "No author found", "published_date": "2018-01-09", "content": "STEVE INSKEEP, HOST: Millions of people have tried online dating or at least thought about it. Almost one third of Americans who have never been married have gone on dating apps or dating sites, according to the Pew Research Center. And a recent study based on data from the National Academy of Sciences found the rise in digital dating coincides with a rise in interracial marriages. One possibility here is that online users are exposed to people they normally might not meet in person in our segregated lives, including people of different races and ethnicities. But there's also some data here suggesting that race and ethnicity play into online dating in a more complicated way. CHRISTIAN RUDDER: On these sites, black users especially, there's a bias against them. Every kind of way you can measure their success on a site - how people rate them, how often they reply to their messages, how many messages they get - that's all reduced. INSKEEP: Christian Rudder is the co-founder of OKCupid, a major dating site, who looked at data from his site and other sites back in 2014. It turns out that online dating reflects something that many people have perceived in the offline world for generations, black women and Asian men are rejected more often than other people. NPR's Ashley Brown talked with some dating app users about whether the numbers reflect their reality. ASHLEY BROWN, BYLINE: I met Jason on a balmy winter afternoon in Los Angeles. He was about to take his puppy on a walk. He'd just gotten home from his internship. He works with men and women with mental health needs. And we're not using his last name, to protect his privacy and his clients' privacy. JASON: You got to take care of yourself. Our professors say that all the time. Self-care this, self-care that. But it's true. BROWN: Self-care ended up playing a big role in Jason's personal life, too. He started using dating apps and websites about seven years ago, and he told me things got ugly. JASON: The messages were saying, I don't date Asians, sorry not sorry, you're cute for an Asian. Like, it was really disheartening. BROWN: Jason is gay and Filipino. He says some of the rejections he got were overtly racist. JASON: It was like, like, I usually like bears, but no panda bears. I'm like, yikes. It really hurt my self-esteem. BROWN: So Jason says he wasn't surprised to see some of the numbers from OKCupid making headlines back in 2014. The dating site's blog said Asian men and black women were rated the least attractive compared to other races and genders. Even though the numbers focused on straight users, Jason says he could definitely relate. JASON: It was like an unfulfilled validation. Like, yeah, I was right, but it feels like [expletive] that I was right. BROWN: I also talked to Ari Curtis. She says she feels the same way. She even started a blog about her experience dating as a black woman. Here's a little bit of one of her blog entries. (SOUNDBITE OF ARCHIVED RECORDING)ARI CURTIS: For black women like me, this is life. The data are mere tiny representations of a messy existence. And while I'm a big fan of big data, the good stuff begins where the data ends. BROWN: Her blog is called Least Desirable. One of her posts talks about an OKCupid date she had at a bar in Crown Heights, Brooklyn. CURTIS: This is it. Yup. BROWN: Ari took me to that bar, and she told me what the date told her over drinks. CURTIS: He had recently gone home, and he was like, yeah, my family would never approve of you. (Laughter) And I was just like, OK (laughter). BROWN: Because she's black. CURTIS: Yeah. Because I'm black. BROWN: That interaction left Ari feeling confused, really uncomfortable. And, it wasn't a new feeling. She also shared this account of a date with another white man she met on Tinder. CURTIS: He was like, so we have to bring the hood out of you, bring the ghetto out of you. And I was like, I'm sorry, what? (Laughter). It made me feel like I wasn't enough, that who I am wasn't what he expected and that he wanted me to be somebody else based on my race (laughter). BROWN: OKCupid told me the site is definitely paying attention to all of this. I talked to their chief marketing officer Melissa Hobley, and she said they've changed a lot about the app over the years. They want to encourage users to focus less on looks alone and more on what she called psychographics. MELISSA HOBLEY: Things like what you're interested in, what moves you, what your passions are. BROWN: Hobley said OKCupid's also talked to social scientists about why people's dating preferences come off as racist. HOBLEY: I think that what OKCupid was seeing in the data was reflective of what happens IRL, in real life. And people tend to be often attracted to the people that they are familiar with. And in a segregated society, that can be harder in certain areas than in others. BROWN: Ari told me she understands that. She's had to come to term with her own biases. She grew up in the mostly white town of Fort Collins, Colo. , and says she was only dating white guys until she moved to New York. Do you think that people expressing a racial preference on a dating app is just like expressing a preference for any other physical attribute, or is there something different about race? CURTIS: I feel like there is room, honestly, to say I have a preference for somebody who looks like this, and if that person happens to be of a certain race - it's hard. It's hard to blame somebody for that. It really is. But on the other hand, you have to wonder if racism weren't so ingrained in our culture if they would have those preferences. BROWN: She says she's still conflicted about her own preferences and conflicted about whether she'll even keep using dating apps, but for now she says her strategy is just to keep a casual attitude about all of it. CURTIS: If I don't take it seriously then I don't have to be disappointed when it doesn't go well because I wasn't taking it seriously anyway. BROWN: Jason's out of the dating game entirely now. That's because he ended up finding his current partner, a white man, on an app a couple of years ago. He credits part of his success to making bold statements about his values and his profile. JASON: Yeah. I said something, like, really obnoxious looking back on it now. I think one of the first lines I said was, like, social justice warriors to the front of the line, please. BROWN: He says weeding through those racist messages was really hard, but for him, worth it in the end. JASON: Everyone deserves love and kindness and support. And pushing through and holding that close to yourself is, I think, actually also what kept me in this online dating realm, just knowing that I deserve this. And if I'm lucky enough, it'll happen. And it did. BROWN: Ashley Brown, NPR News. (SOUNDBITE OF MUSIC)INSKEEP: Hey. That story is part of What Makes Us Click, MORNING EDITION's series on online dating. STEVE INSKEEP, HOST:  Millions of people have tried online dating or at least thought about it. Almost one third of Americans who have never been married have gone on dating apps or dating sites, according to the Pew Research Center. And a recent study based on data from the National Academy of Sciences found the rise in digital dating coincides with a rise in interracial marriages. One possibility here is that online users are exposed to people they normally might not meet in person in our segregated lives, including people of different races and ethnicities. But there's also some data here suggesting that race and ethnicity play into online dating in a more complicated way. CHRISTIAN RUDDER: On these sites, black users especially, there's a bias against them. Every kind of way you can measure their success on a site - how people rate them, how often they reply to their messages, how many messages they get - that's all reduced. INSKEEP: Christian Rudder is the co-founder of OKCupid, a major dating site, who looked at data from his site and other sites back in 2014. It turns out that online dating reflects something that many people have perceived in the offline world for generations, black women and Asian men are rejected more often than other people. NPR's Ashley Brown talked with some dating app users about whether the numbers reflect their reality. ASHLEY BROWN, BYLINE: I met Jason on a balmy winter afternoon in Los Angeles. He was about to take his puppy on a walk. He'd just gotten home from his internship. He works with men and women with mental health needs. And we're not using his last name, to protect his privacy and his clients' privacy. JASON: You got to take care of yourself. Our professors say that all the time. Self-care this, self-care that. But it's true. BROWN: Self-care ended up playing a big role in Jason's personal life, too. He started using dating apps and websites about seven years ago, and he told me things got ugly. JASON: The messages were saying, I don't date Asians, sorry not sorry, you're cute for an Asian. Like, it was really disheartening. BROWN: Jason is gay and Filipino. He says some of the rejections he got were overtly racist. JASON: It was like, like, I usually like bears, but no panda bears. I'm like, yikes. It really hurt my self-esteem. BROWN: So Jason says he wasn't surprised to see some of the numbers from OKCupid making headlines back in 2014. The dating site's blog said Asian men and black women were rated the least attractive compared to other races and genders. Even though the numbers focused on straight users, Jason says he could definitely relate. JASON: It was like an unfulfilled validation. Like, yeah, I was right, but it feels like [expletive] that I was right. BROWN: I also talked to Ari Curtis. She says she feels the same way. She even started a blog about her experience dating as a black woman. Here's a little bit of one of her blog entries. (SOUNDBITE OF ARCHIVED RECORDING) ARI CURTIS: For black women like me, this is life. The data are mere tiny representations of a messy existence. And while I'm a big fan of big data, the good stuff begins where the data ends. BROWN: Her blog is called Least Desirable. One of her posts talks about an OKCupid date she had at a bar in Crown Heights, Brooklyn. CURTIS: This is it. Yup. BROWN: Ari took me to that bar, and she told me what the date told her over drinks. CURTIS: He had recently gone home, and he was like, yeah, my family would never approve of you. (Laughter) And I was just like, OK (laughter). BROWN: Because she's black. CURTIS: Yeah. Because I'm black. BROWN: That interaction left Ari feeling confused, really uncomfortable. And, it wasn't a new feeling. She also shared this account of a date with another white man she met on Tinder. CURTIS: He was like, so we have to bring the hood out of you, bring the ghetto out of you. And I was like, I'm sorry, what? (Laughter). It made me feel like I wasn't enough, that who I am wasn't what he expected and that he wanted me to be somebody else based on my race (laughter). BROWN: OKCupid told me the site is definitely paying attention to all of this. I talked to their chief marketing officer Melissa Hobley, and she said they've changed a lot about the app over the years. They want to encourage users to focus less on looks alone and more on what she called psychographics. MELISSA HOBLEY: Things like what you're interested in, what moves you, what your passions are. BROWN: Hobley said OKCupid's also talked to social scientists about why people's dating preferences come off as racist. HOBLEY: I think that what OKCupid was seeing in the data was reflective of what happens IRL, in real life. And people tend to be often attracted to the people that they are familiar with. And in a segregated society, that can be harder in certain areas than in others. BROWN: Ari told me she understands that. She's had to come to term with her own biases. She grew up in the mostly white town of Fort Collins, Colo. , and says she was only dating white guys until she moved to New York. Do you think that people expressing a racial preference on a dating app is just like expressing a preference for any other physical attribute, or is there something different about race? CURTIS: I feel like there is room, honestly, to say I have a preference for somebody who looks like this, and if that person happens to be of a certain race - it's hard. It's hard to blame somebody for that. It really is. But on the other hand, you have to wonder if racism weren't so ingrained in our culture if they would have those preferences. BROWN: She says she's still conflicted about her own preferences and conflicted about whether she'll even keep using dating apps, but for now she says her strategy is just to keep a casual attitude about all of it. CURTIS: If I don't take it seriously then I don't have to be disappointed when it doesn't go well because I wasn't taking it seriously anyway. BROWN: Jason's out of the dating game entirely now. That's because he ended up finding his current partner, a white man, on an app a couple of years ago. He credits part of his success to making bold statements about his values and his profile. JASON: Yeah. I said something, like, really obnoxious looking back on it now. I think one of the first lines I said was, like, social justice warriors to the front of the line, please. BROWN: He says weeding through those racist messages was really hard, but for him, worth it in the end. JASON: Everyone deserves love and kindness and support. And pushing through and holding that close to yourself is, I think, actually also what kept me in this online dating realm, just knowing that I deserve this. And if I'm lucky enough, it'll happen. And it did. BROWN: Ashley Brown, NPR News. (SOUNDBITE OF MUSIC) INSKEEP: Hey. That story is part of What Makes Us Click, MORNING EDITION's series on online dating.", "section": "What Makes Us Click", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-10-576976740": {"title": "Teacher's Twitter Post About Twitter Goes Viral : NPR", "url": "https://www.npr.org/2018/01/10/576976740/teachers-twitter-post-about-twitter-goes-viral", "author": "No author found", "published_date": "2018-01-10", "content": "RACHEL MARTIN, HOST: Good morning. I'm Rachel Martin. A teacher in Kansas wanted to teach his third-graders how quickly information can spread, so he sent a tweet explaining his lesson and asking people to retweet so his class could track it. It was tweeted in Oklahoma in minutes, and the kids were amazed. By recess, New York; overnight, Malaysia, Japan, Kenya. The actor Eric Stonestreet retweeted. So did the political analyst Dana Perino. The lesson - yes, information travels fast. The real lesson - be careful what you tweet. It's MORNING EDITION. RACHEL MARTIN, HOST:  Good morning. I'm Rachel Martin. A teacher in Kansas wanted to teach his third-graders how quickly information can spread, so he sent a tweet explaining his lesson and asking people to retweet so his class could track it. It was tweeted in Oklahoma in minutes, and the kids were amazed. By recess, New York; overnight, Malaysia, Japan, Kenya. The actor Eric Stonestreet retweeted. So did the political analyst Dana Perino. The lesson - yes, information travels fast. The real lesson - be careful what you tweet. It's MORNING EDITION.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-11-577101803": {"title": "A School's Way To Fight Phones In Class: Lock 'Em Up : NPR", "url": "https://www.npr.org/2018/01/11/577101803/a-schools-way-to-fight-phones-in-class-lock-em-up", "author": "No author found", "published_date": "2018-01-11", "content": "KELLY MCEVERS, HOST: As most teachers will tell you, class hasn't been the same since kids started bringing cellphones to school. Ancient Roman history will pretty much never win out when you're competing with Snapchat and Instagram. Now, as NPR's Tovia Smith reports, a growing number of teachers are trying an innovative strategy to turn their classes into phone-free zones. TOVIA SMITH, BYLINE: The teachers all know what's going on with students' zombie looks and, can you say that again? TONY PATELIS: You see that they're not listening to you. They're looking down. And they tell me they're checking the time even though the clock's on the wall. SMITIH: Newton North High School history teacher Tony Patelis says he's a psycho about cell phones, constantly confiscating them. PATELIS: It's a daily battle every class. SMITIH: Who's winning? PATELIS: The kids. SMITIH: But now exasperated teachers have another weapon in their arsenal. UNIDENTIFIED MAN #1: Phones this way - thank you. Good morning. SMITIH: At the City on a Hill Charter School in Boston, a phalanx of administrators just inside the front door take phones from every entering student. UNIDENTIFIED MAN #1: J-22. UNIDENTIFIED MAN #2: Got it. SMITIH: Each phone is locked into a soft pouch with a snap on it like security tags in clothing stores and given back. Students can only unlock them with a machine at dismissal time nearly eight hours from now. TONINHO EMANUEL: It sucks. SMITIH: Needless to say, kids like Toninho Emanuel and Tyler Martin are not thrilled. TYLER MARTIN: 'Cause it's, like, your toy. And they take it from you, and then you can't use it. Like, it's something - it just stays with you all the time. It's like glasses. SMITIH: It's like - well, glasses you need. TYLER: Yeah. It's my phone. I need it sometimes. SMITIH: But their protests are not persuading school principal DeOtis Williams Jr. DEOTIS WILLIAMS JR: It's like my mom used to always tell me. Kids know what they want but don't know what they need. We know what the students need. SMITIH: Williams says kids have used tools and magnets to try to pry the bags open. Some bring decoy phones to pouch or cut the bags open. But overall, teachers say the pouch policy is paying off. JOANIE DECOPAIN: All right, the last question - X to the third power minus 13-X. . . SMITIH: In Joanie DeCopain's algebra class, all eyes are on the board. DECOPAIN: . . . Divided by X minus seven. How do I set it up? UNIDENTIFIED STUDENT #1: It's going to be X squared. SMITIH: DeCopain says students are more engaged, and some are starting to see the virtue in the pouches - sort of. DECOPAIN: They think, like, it's a good thing but for other people, not for them. So it's always about, like - someone else needs the medication, not me. SMITIH: Some brave souls like senior Yalena Terrero Martinez do admit they're less distracted in school and even after. YALENA TERRERO MARTINEZ: I don't reach for my phone as much because it's, like, if you don't feed into the habit, the habit eventually slows down. SMITIH: Martinez says the pouches are also making a difference socially. MARTINEZ: Yeah because, oh, my gosh, all my friends would be, like, on their phone during lunch. But now, like, we talk a lot more. SMITIH: That's exactly what the folks who make the pouches were hoping for. Graham Dugoni founded the company called Yondr four years ago after he was annoyed by people using their phones at concerts. Turns out performers were, too, and now hundreds of them like Chris Rock and Ariana Grande have been forcing fans to lock up their phones. GRAHAM DUGONI: I see it all as part of a social movement. You know, people are aware that something's out of whack, and they're looking for answers. There's a deep sense that the more efficient and faster everything gets in life and easier, the more meaning is being hollowed out. SMITIH: Big performers pay about $2 a pouch. Schools can rent by the year for $30 each, and some 600 now are. ALBERT CHO: Great, OK, have a good day. UNIDENTIFIED STUDENT #2: Thank you, Mr. Cho. CHO: Thank you. SMITIH: The minute Albert Cho dismisses class at Newton North High School, students race for the unlocking device. Cho is the only teacher here using the pouches. But even a phone-free hour is a hard sell to students like Sheil Mehta. SHEIL MEHTA: I think our generation gets pushed into this one image where we're all, like, crazed. And like, if you take away our phones, we'll start eating each other. Like, I don't think we're really all like that. SMITIH: But their protests belie just how attached they are. Even with their phones in pouches, students like Carmen McCauliffe still clutch them in their hands. CARMEN MCCAULIFFE: I guess it's, like, sickly therapeutic in a way (laughter), just being able to feel it. I, like, always am, like, pressing the button, like, even through the case. SMITIH: Because their attachment is so intense, California State University professor Larry Rosen says cutting the cord cold turkey during class may actually backfire. LARRY ROSEN: You're inducing massive anxiety. And you're going to get a group of people who really can't pay attention because that anxiety is an overriding feeling. SMITIH: Rosen recommends what he calls a quick tech break every 15 or 30 minutes. But teachers who've tried the cold turkey approach insist most teens do adapt so much so, some actually forget their phones in class, which pretty much never happened before. Tovia Smith, NPR News, Boston. KELLY MCEVERS, HOST:  As most teachers will tell you, class hasn't been the same since kids started bringing cellphones to school. Ancient Roman history will pretty much never win out when you're competing with Snapchat and Instagram. Now, as NPR's Tovia Smith reports, a growing number of teachers are trying an innovative strategy to turn their classes into phone-free zones. TOVIA SMITH, BYLINE: The teachers all know what's going on with students' zombie looks and, can you say that again? TONY PATELIS: You see that they're not listening to you. They're looking down. And they tell me they're checking the time even though the clock's on the wall. SMITIH: Newton North High School history teacher Tony Patelis says he's a psycho about cell phones, constantly confiscating them. PATELIS: It's a daily battle every class. SMITIH: Who's winning? PATELIS: The kids. SMITIH: But now exasperated teachers have another weapon in their arsenal. UNIDENTIFIED MAN #1: Phones this way - thank you. Good morning. SMITIH: At the City on a Hill Charter School in Boston, a phalanx of administrators just inside the front door take phones from every entering student. UNIDENTIFIED MAN #1: J-22. UNIDENTIFIED MAN #2: Got it. SMITIH: Each phone is locked into a soft pouch with a snap on it like security tags in clothing stores and given back. Students can only unlock them with a machine at dismissal time nearly eight hours from now. TONINHO EMANUEL: It sucks. SMITIH: Needless to say, kids like Toninho Emanuel and Tyler Martin are not thrilled. TYLER MARTIN: 'Cause it's, like, your toy. And they take it from you, and then you can't use it. Like, it's something - it just stays with you all the time. It's like glasses. SMITIH: It's like - well, glasses you need. TYLER: Yeah. It's my phone. I need it sometimes. SMITIH: But their protests are not persuading school principal DeOtis Williams Jr. DEOTIS WILLIAMS JR: It's like my mom used to always tell me. Kids know what they want but don't know what they need. We know what the students need. SMITIH: Williams says kids have used tools and magnets to try to pry the bags open. Some bring decoy phones to pouch or cut the bags open. But overall, teachers say the pouch policy is paying off. JOANIE DECOPAIN: All right, the last question - X to the third power minus 13-X. . . SMITIH: In Joanie DeCopain's algebra class, all eyes are on the board. DECOPAIN: . . . Divided by X minus seven. How do I set it up? UNIDENTIFIED STUDENT #1: It's going to be X squared. SMITIH: DeCopain says students are more engaged, and some are starting to see the virtue in the pouches - sort of. DECOPAIN: They think, like, it's a good thing but for other people, not for them. So it's always about, like - someone else needs the medication, not me. SMITIH: Some brave souls like senior Yalena Terrero Martinez do admit they're less distracted in school and even after. YALENA TERRERO MARTINEZ: I don't reach for my phone as much because it's, like, if you don't feed into the habit, the habit eventually slows down. SMITIH: Martinez says the pouches are also making a difference socially. MARTINEZ: Yeah because, oh, my gosh, all my friends would be, like, on their phone during lunch. But now, like, we talk a lot more. SMITIH: That's exactly what the folks who make the pouches were hoping for. Graham Dugoni founded the company called Yondr four years ago after he was annoyed by people using their phones at concerts. Turns out performers were, too, and now hundreds of them like Chris Rock and Ariana Grande have been forcing fans to lock up their phones. GRAHAM DUGONI: I see it all as part of a social movement. You know, people are aware that something's out of whack, and they're looking for answers. There's a deep sense that the more efficient and faster everything gets in life and easier, the more meaning is being hollowed out. SMITIH: Big performers pay about $2 a pouch. Schools can rent by the year for $30 each, and some 600 now are. ALBERT CHO: Great, OK, have a good day. UNIDENTIFIED STUDENT #2: Thank you, Mr. Cho. CHO: Thank you. SMITIH: The minute Albert Cho dismisses class at Newton North High School, students race for the unlocking device. Cho is the only teacher here using the pouches. But even a phone-free hour is a hard sell to students like Sheil Mehta. SHEIL MEHTA: I think our generation gets pushed into this one image where we're all, like, crazed. And like, if you take away our phones, we'll start eating each other. Like, I don't think we're really all like that. SMITIH: But their protests belie just how attached they are. Even with their phones in pouches, students like Carmen McCauliffe still clutch them in their hands. CARMEN MCCAULIFFE: I guess it's, like, sickly therapeutic in a way (laughter), just being able to feel it. I, like, always am, like, pressing the button, like, even through the case. SMITIH: Because their attachment is so intense, California State University professor Larry Rosen says cutting the cord cold turkey during class may actually backfire. LARRY ROSEN: You're inducing massive anxiety. And you're going to get a group of people who really can't pay attention because that anxiety is an overriding feeling. SMITIH: Rosen recommends what he calls a quick tech break every 15 or 30 minutes. But teachers who've tried the cold turkey approach insist most teens do adapt so much so, some actually forget their phones in class, which pretty much never happened before. Tovia Smith, NPR News, Boston.", "section": "National", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-12-577435746": {"title": "Tim Kruger: How Do We Slow Climate Change Before It's Too Late? : NPR", "url": "https://www.npr.org/2018/01/12/577435746/tim-kruger-how-do-we-slow-climate-change-before-its-too-late", "author": "No author found", "published_date": "2018-01-12", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. I'm Guy Raz. So as we start the new year, there's some good news and some bad news about the future of the planet. So first let's do the bad news. We'll get to the good news in a minute, but let's just get the bad stuff out of the way - because the last time this amount of carbon dioxide was in our atmosphere was probably back in the Pliocene Epoch millions of years ago when giant creatures like mastodon still roamed the earth, when there wasn't much of an ice sheet and what we now think of as Florida and New Jersey were totally underwater. And today, well, we're on track to exceed those carbon dioxide levels by a lot. In fact, by an unprecedented amount. TIM KRUGER: And this is changing our climate enormously. RAZ: This is Tim Kruger. He studies climate change at the University of Oxford. KRUGER: So we will see temperatures rise very rapidly, and that will cause massive changes in terms of agriculture, natural systems, biodiversity. But it's more than that. We will also see the oceans becoming increasingly acidified so more CO2 will move from the atmosphere into the oceans and the pH of the oceans will come down, and that will affect organisms at the base of the food chain in the ocean with very severe consequences. RAZ: What is the maximum temperature rise that scientists think humans can deal with? KRUGER: So most of the world - unfortunately, not the country where you're based - but most of the world is committed to the Paris Agreement. And what that says is that we will aim to stop the temperature rise between 1. 5 and 2 degrees above pre-industrial levels. If it goes beyond that, you will see species going extinct. You will see coral reefs being irreparably damaged. Can we as humans survive as the temperature goes up on the planet? Yes, probably we can. But it will be an impoverished planet that we hand on to future generations. So I think that we have problems around food security, we have problems about global leadership, about economics and the inequality there and migration, but climate change is something that is going to affect all of those, and it's going to be made considerably worse by its actions. (SOUNDBITE OF MUSIC)RAZ: Today on the show, we're going to explore all of those problems - climate change, our political systems, whether we'll have enough food to feed everyone, refugees and migration, and the growing inequality between the richest people and the poorest. And we're calling these The Big Five, ideas about the five biggest issues that will affect all humans and whether, with help from some radical solutions, we can stop them. And, as we heard from Tim Kruger, climate change is the biggest one of all and the one that's kind of hard for people to really feel and grasp. KRUGER: It's like a tidal wave made of treacle. It's moving towards us incredibly slowly and we need to make sure that we are aware of it, but it's very difficult to alert people to the danger as this danger comes closer and closer because it seems so slow. RAZ: But here's a little bit of good news. Finally, right? Now, we could potentially stop this. If the world could get its act together, we might eventually reverse the effects of climate change. But it's going to take a lot of dedication, a lot of ideas, a lot of money and a lot of work because not only do we have to reduce CO2 from the atmosphere, we also need to remove it. KRUGER: And what most people don't realize is that the climate models that are used to understand how we can avoid crossing the 2-degree threshold, they assume that we have technologies that can take carbon dioxide out of the atmosphere. RAZ: Wow. KRUGER: They assume it, at a massive scale. They assume that we can remove between 600 and 800 billion tons of carbon dioxide out of the atmosphere in the decades ahead. And these models are ones that also assume heroically optimistic levels of mitigation. So in reality we are going to need to remove trillions of tons, trillions of tons of carbon dioxide from the atmosphere in the decades ahead. RAZ: So how? What do you - I mean, it sounds like you're talking about something bigger than, you know, World War II and the Marshall Plan and the Manhattan Project all combined - like, the biggest thing humans have ever done. KRUGER: I think that we are going to face a multi-generational challenge to restore the atmosphere. RAZ: And not only does Tim think about removing all that carbon dioxide from the atmosphere, he's actually trying to do it. Here's Tim Kruger on the TED stage. (SOUNDBITE OF TED TALK)KRUGER: I work assessing a whole range of these proposed techniques to see if they can work. We could use plants to take CO2 out and then store it in trees, in the soil deep underground, or in the oceans. We could build large machines, so-called artificial trees that will scrub CO2 from the air. For these ideas to be feasible, we need to understand whether they can be applied at a vast scale in a way that is safe, economic and socially acceptable. All of these ideas come with tradeoffs. None of them are perfect, but many have potential. It's unlikely that any one of them will solve it on its own. There is no silver bullet, but potentially together they may form the silver buckshot that we need to stop climate change in its tracks. I'm working independently on one particular idea which uses natural gas to generate electricity in a way that takes carbon dioxide out of the air. How does that work? So the origin power process feeds natural gas into a fuel cell. About half the chemical energy is converted into electricity, and the remainder into heat which is used to break down limestone into lime and carbon dioxide. It's actually generating carbon dioxide. But the key point is all of the carbon dioxide generated, both from the fuel cell and from the lime kiln is pure, and that's really important because it means you can either use that carbon dioxide or you can store it away deep underground at low cost. And then the line that you produce can be used in industrial processes, and in being used, it scrubs CO2 out of the air. Overall, the process is carbon negative. It removes carbon dioxide from the air. If you normally generate electricity from natural gas, you emit about 400 grams of CO2 into the air for every kilowatt hour. With this process, that figure is minus 600. At the moment, power generation is responsible for about a quarter of all carbon dioxide emissions. Hypothetically, if you replaced all power generation with this process then you would not only eliminate all of the emissions from power generation but you would start removing emissions from other sectors as well, potentially cutting 60 percent of overall carbon emissions. (SOUNDBITE OF MUSIC)RAZ: So OK. Just so I understand this, once this process is done, once you've removed the carbon dioxide from the air, the like, byproduct is lime, is limestone? KRUGER: Yeah, that's right. So the whole process is kind of using the limestone as a kind of intermediate product. So you start off with limestone, you end up with limestone. But the net result of the processes that you have taken dilute carbon dioxide out of the air and generated pure carbon dioxide, and that's important because once you have pure carbon dioxide you can bury that deep underground. If you've just got dilute carbon dioxide, you can't bury it underground as it is. RAZ: So when you say you can store away that carbon dioxide underground, what does that look like? Is it, like, a liquid? Is it a gas? I mean, describe what it means to store it underground. KRUGER: So what you do is you compress the CO2 into a state called a supercritical fluid, and that is something which is kind of partway between a gas and a liquid and it's got characteristics of both. And you inject it deep underground, and there it will react with the rocks and remain stored deep underground. So part of the storage is what's known as physical. It gets trapped in gaps underground. And part of it is chemical. It will react with the rocks underground. But basically once you inject it, it stays there. It's down there for good. RAZ: So I mean, as promising as this technology sounds -'cause it sounds incredible that we could actually start to, you know, clean up the mess that we've created over the past 250, 300 years - it sounds a little bit like, you know, a mission to Mars. Like, we know, humans know that we can get something to Mars. We've done it before. So we should be able to get humans to Mars. But there are some challenges, like, how do you bring up enough water for a six-month journey in a spacecraft, right? So you know, that'll be resolved at some point, but we can't do it right now. It sounds like it's similar to this, that this is not something we can implement today. KRUGER: No, and it isn't something that we can implement today. So we should not in any way cut back on any of our efforts to reduce emissions. So that's got to be the top priority. But even if we reduce emissions incredibly rapidly, it's not enough on its own. We need these technologies to take CO2 out of the atmosphere. It's not available today. It won't be available for many years to come, but we need to try. There's a story about a man who fell on hard times. And he prayed and he said, please, God, let me win the lottery, let me win the lottery. And he didn't win the lottery. And then next week he said, I'm in a real bind. I really need to get some money. You know, let me win the lottery. And still he didn't win it. And the third time, he climbed up to the top of a mountain. He said, I'm going to throw myself off, I'm going to kill myself. God, you've got to help me on this. And just as he was about to throw himself off, there was this roll of thunder and a booming voice rang out. He said, for goodness sake, man, meet me halfway, buy a ticket. And that's what we need to do. We actually need to try. It's no good saying, well, we need to do this. We actually need to put the resources behind it. The question comes, who would do this? And at the moment, really no one. So we could have the ideas, but without there being some sort of motivation, some sort of incentive to do so it's just not going to happen. We need to raise our game significantly if we are going to be able to address this problem. (SOUNDBITE OF MUSIC)RAZ: Tim Kruger. He researches geoengineering at the University of Oxford. He's currently working on a smaller prototype of the technology described on the TED stage. You can see his full talk at ted. com. On the show today, the five biggest problems that face the world and what we might do to stop them. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. So as we start the new year, there's some good news and some bad news about the future of the planet. So first let's do the bad news. We'll get to the good news in a minute, but let's just get the bad stuff out of the way - because the last time this amount of carbon dioxide was in our atmosphere was probably back in the Pliocene Epoch millions of years ago when giant creatures like mastodon still roamed the earth, when there wasn't much of an ice sheet and what we now think of as Florida and New Jersey were totally underwater. And today, well, we're on track to exceed those carbon dioxide levels by a lot. In fact, by an unprecedented amount. TIM KRUGER: And this is changing our climate enormously. RAZ: This is Tim Kruger. He studies climate change at the University of Oxford. KRUGER: So we will see temperatures rise very rapidly, and that will cause massive changes in terms of agriculture, natural systems, biodiversity. But it's more than that. We will also see the oceans becoming increasingly acidified so more CO2 will move from the atmosphere into the oceans and the pH of the oceans will come down, and that will affect organisms at the base of the food chain in the ocean with very severe consequences. RAZ: What is the maximum temperature rise that scientists think humans can deal with? KRUGER: So most of the world - unfortunately, not the country where you're based - but most of the world is committed to the Paris Agreement. And what that says is that we will aim to stop the temperature rise between 1. 5 and 2 degrees above pre-industrial levels. If it goes beyond that, you will see species going extinct. You will see coral reefs being irreparably damaged. Can we as humans survive as the temperature goes up on the planet? Yes, probably we can. But it will be an impoverished planet that we hand on to future generations. So I think that we have problems around food security, we have problems about global leadership, about economics and the inequality there and migration, but climate change is something that is going to affect all of those, and it's going to be made considerably worse by its actions. (SOUNDBITE OF MUSIC) RAZ: Today on the show, we're going to explore all of those problems - climate change, our political systems, whether we'll have enough food to feed everyone, refugees and migration, and the growing inequality between the richest people and the poorest. And we're calling these The Big Five, ideas about the five biggest issues that will affect all humans and whether, with help from some radical solutions, we can stop them. And, as we heard from Tim Kruger, climate change is the biggest one of all and the one that's kind of hard for people to really feel and grasp. KRUGER: It's like a tidal wave made of treacle. It's moving towards us incredibly slowly and we need to make sure that we are aware of it, but it's very difficult to alert people to the danger as this danger comes closer and closer because it seems so slow. RAZ: But here's a little bit of good news. Finally, right? Now, we could potentially stop this. If the world could get its act together, we might eventually reverse the effects of climate change. But it's going to take a lot of dedication, a lot of ideas, a lot of money and a lot of work because not only do we have to reduce CO2 from the atmosphere, we also need to remove it. KRUGER: And what most people don't realize is that the climate models that are used to understand how we can avoid crossing the 2-degree threshold, they assume that we have technologies that can take carbon dioxide out of the atmosphere. RAZ: Wow. KRUGER: They assume it, at a massive scale. They assume that we can remove between 600 and 800 billion tons of carbon dioxide out of the atmosphere in the decades ahead. And these models are ones that also assume heroically optimistic levels of mitigation. So in reality we are going to need to remove trillions of tons, trillions of tons of carbon dioxide from the atmosphere in the decades ahead. RAZ: So how? What do you - I mean, it sounds like you're talking about something bigger than, you know, World War II and the Marshall Plan and the Manhattan Project all combined - like, the biggest thing humans have ever done. KRUGER: I think that we are going to face a multi-generational challenge to restore the atmosphere. RAZ: And not only does Tim think about removing all that carbon dioxide from the atmosphere, he's actually trying to do it. Here's Tim Kruger on the TED stage. (SOUNDBITE OF TED TALK) KRUGER: I work assessing a whole range of these proposed techniques to see if they can work. We could use plants to take CO2 out and then store it in trees, in the soil deep underground, or in the oceans. We could build large machines, so-called artificial trees that will scrub CO2 from the air. For these ideas to be feasible, we need to understand whether they can be applied at a vast scale in a way that is safe, economic and socially acceptable. All of these ideas come with tradeoffs. None of them are perfect, but many have potential. It's unlikely that any one of them will solve it on its own. There is no silver bullet, but potentially together they may form the silver buckshot that we need to stop climate change in its tracks. I'm working independently on one particular idea which uses natural gas to generate electricity in a way that takes carbon dioxide out of the air. How does that work? So the origin power process feeds natural gas into a fuel cell. About half the chemical energy is converted into electricity, and the remainder into heat which is used to break down limestone into lime and carbon dioxide. It's actually generating carbon dioxide. But the key point is all of the carbon dioxide generated, both from the fuel cell and from the lime kiln is pure, and that's really important because it means you can either use that carbon dioxide or you can store it away deep underground at low cost. And then the line that you produce can be used in industrial processes, and in being used, it scrubs CO2 out of the air. Overall, the process is carbon negative. It removes carbon dioxide from the air. If you normally generate electricity from natural gas, you emit about 400 grams of CO2 into the air for every kilowatt hour. With this process, that figure is minus 600. At the moment, power generation is responsible for about a quarter of all carbon dioxide emissions. Hypothetically, if you replaced all power generation with this process then you would not only eliminate all of the emissions from power generation but you would start removing emissions from other sectors as well, potentially cutting 60 percent of overall carbon emissions. (SOUNDBITE OF MUSIC) RAZ: So OK. Just so I understand this, once this process is done, once you've removed the carbon dioxide from the air, the like, byproduct is lime, is limestone? KRUGER: Yeah, that's right. So the whole process is kind of using the limestone as a kind of intermediate product. So you start off with limestone, you end up with limestone. But the net result of the processes that you have taken dilute carbon dioxide out of the air and generated pure carbon dioxide, and that's important because once you have pure carbon dioxide you can bury that deep underground. If you've just got dilute carbon dioxide, you can't bury it underground as it is. RAZ: So when you say you can store away that carbon dioxide underground, what does that look like? Is it, like, a liquid? Is it a gas? I mean, describe what it means to store it underground. KRUGER: So what you do is you compress the CO2 into a state called a supercritical fluid, and that is something which is kind of partway between a gas and a liquid and it's got characteristics of both. And you inject it deep underground, and there it will react with the rocks and remain stored deep underground. So part of the storage is what's known as physical. It gets trapped in gaps underground. And part of it is chemical. It will react with the rocks underground. But basically once you inject it, it stays there. It's down there for good. RAZ: So I mean, as promising as this technology sounds -'cause it sounds incredible that we could actually start to, you know, clean up the mess that we've created over the past 250, 300 years - it sounds a little bit like, you know, a mission to Mars. Like, we know, humans know that we can get something to Mars. We've done it before. So we should be able to get humans to Mars. But there are some challenges, like, how do you bring up enough water for a six-month journey in a spacecraft, right? So you know, that'll be resolved at some point, but we can't do it right now. It sounds like it's similar to this, that this is not something we can implement today. KRUGER: No, and it isn't something that we can implement today. So we should not in any way cut back on any of our efforts to reduce emissions. So that's got to be the top priority. But even if we reduce emissions incredibly rapidly, it's not enough on its own. We need these technologies to take CO2 out of the atmosphere. It's not available today. It won't be available for many years to come, but we need to try. There's a story about a man who fell on hard times. And he prayed and he said, please, God, let me win the lottery, let me win the lottery. And he didn't win the lottery. And then next week he said, I'm in a real bind. I really need to get some money. You know, let me win the lottery. And still he didn't win it. And the third time, he climbed up to the top of a mountain. He said, I'm going to throw myself off, I'm going to kill myself. God, you've got to help me on this. And just as he was about to throw himself off, there was this roll of thunder and a booming voice rang out. He said, for goodness sake, man, meet me halfway, buy a ticket. And that's what we need to do. We actually need to try. It's no good saying, well, we need to do this. We actually need to put the resources behind it. The question comes, who would do this? And at the moment, really no one. So we could have the ideas, but without there being some sort of motivation, some sort of incentive to do so it's just not going to happen. We need to raise our game significantly if we are going to be able to address this problem. (SOUNDBITE OF MUSIC) RAZ: Tim Kruger. He researches geoengineering at the University of Oxford. He's currently working on a smaller prototype of the technology described on the TED stage. You can see his full talk at ted. com. On the show today, the five biggest problems that face the world and what we might do to stop them. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-12-577565237": {"title": "Innovation At CES Trade Show Couldn't Keep Lights From Going Out : NPR", "url": "https://www.npr.org/2018/01/12/577565237/innovation-at-ces-trade-show-couldnt-keep-lights-from-going-out", "author": "No author found", "published_date": "2018-01-12", "content": "RACHEL MARTIN, HOST:  Good morning. I'm Rachel Martin. This week, some of the world's top tech entrepreneurs gathered for the annual CES trade show in Las Vegas. These people tackle really complicated problems with the most cutting-edge solutions. But all this innovation couldn't keep the lights from going out. On Wednesday, the whole convention center temporarily lost power. The entire place went dark, and all the fancy displays were shut down. One news site tweeted - lights out at CES 2018. Does anybody have a battery pack? It's MORNING EDITION. RACHEL MARTIN, HOST:   Good morning. I'm Rachel Martin. This week, some of the world's top tech entrepreneurs gathered for the annual CES trade show in Las Vegas. These people tackle really complicated problems with the most cutting-edge solutions. But all this innovation couldn't keep the lights from going out. On Wednesday, the whole convention center temporarily lost power. The entire place went dark, and all the fancy displays were shut down. One news site tweeted - lights out at CES 2018. Does anybody have a battery pack? It's MORNING EDITION.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-13-577833664": {"title": "Sponsored Posts On Instagram, Facebook Shake Up Fashion E-Commerce : NPR", "url": "https://www.npr.org/2018/01/13/577833664/shopify-and-your-instagram-feed", "author": "No author found", "published_date": "2018-01-13", "content": "SCOTT SIMON, HOST: All Alexis Madrigal wanted was a coat. He bought that coat through an ad he saw on Instagram. A sponsored post popped up in his photo feed. He hadn't heard of the retailer, but it billed itself as luxury for modern gentlemen. And what a deal - the price was less than a hundred dollars. The jacket Alexis Madrigal got delivered, however, turned out to be less an expression of luxury and more a lesson into how e-commerce has changed in the age of Instagram and Facebook. Alexis Madrigal wrote about this for The Atlantic, and he joins us now from Oakland, Calif. Thanks so much for being with us. ALEXIS MADRIGAL: Thank you. SIMON: What'd the coat look like? MADRIGAL: Well, it kind of looked like a carpet, I would say, formed into a roughly coat-like shape. (Laughter) It kind of had a velour sheen and kind of the texture of indoor-outdoor carpeting. SIMON: (Laughter). MADRIGAL: It was not the nicest coat that I'd ever seen. SIMON: So what's your complaint? MADRIGAL: Well, the complaint was I wanted to get something for nothing. You know what I mean? I wanted a nice, expensive-looking coat for less than a hundred dollars, which, as it turns out, even with all the magic of the Internet, is still impossible. SIMON: Yeah. Well, explain some of that magic of the Internet because this is territory you know well. How are you - I'm not sure taking it is the word because it seems to me if you were, forgive me, naive enough to think you could get a good coat for under a hundred dollars - well, you can finish that sentence. What was the magic that took you in? MADRIGAL: Well, so what I got really interested in was that when the coat showed up, it showed up in a black plastic package from China Post. So I thought I was buying a coat from some brand that represented itself as kind of this modern gentleman. You know, I was expecting it to have its headquarters, you know, in New York or San Francisco or Los Angeles or something. And this thing is shipped direct from a technology park in China. So I kind of started thinking like, well, what is this brand? And I started diving into this new class of online retailer that use a tool called Shopify, which allows anyone to kind of spin up a retail store in five minutes; sucks products in from a service called AliExpress, which is sort of like Amazon but in China, and it's dedicated to kind of the export market for Chinese and other Asian manufacturers; and allows - basically - consumers in the U. S. to take a different route into this manufacturing ecosystem, which makes so many clothes, which makes so many consumer goods, in Asia. SIMON: When you ordered this coat, were you genuinely surprised to be able to trace this stuff back, or was that the whole idea? MADRIGAL: Well, no. I was genuinely surprised. I was literally - I had been tagged in the Facebook advertising system, which Instagram also uses, as someone who likes to buy clothes. And what I came to find was that this is a pretty widespread phenomenon. Shopify says that they have 500,000 merchants. The overwhelming majority of them are small- and medium-sized businesses. And some subset - although no one's totally sure exactly how large it is - are these people who do what's called drop shipping, which means that they never handle the goods that they sell. They essentially are a front end, just a way of accessing these items that are then directly shipped from their Chinese manufacturers. So in some weird way, it sort of cuts out the middleman of sort of an H&M or a Zara or some other fast-fashion place. But it inserts this new middle person, which is a Shopify e-commerce site that took somebody five minutes to set up. And one of the people that I found doing that was this, you know, 17-year-old, or at least presumed, he calls himself a 17-year-old, living in, like, suburban Dublin. SIMON: Well, he sounds like a very ambitious entrepreneur. MADRIGAL: I think they are, you know. I think the appeal of it, obviously, for entrepreneurs - this drop shipping model - is that it takes almost nothing upfront. You don't need money to buy the stuff. The tools online are very inexpensive, and it's this possibility that these entrepreneurs can get something for nothing - right? - that they can generate money basically out of thin air. SIMON: And not to make any comparisons, but I seem to recall that there was a time when J. Crew was a catalog retailer - didn't have any real stores. And now, of course, they've got plenty of them in addition to a very ambitious online retail site. MADRIGAL: Absolutely. The way that I've thought about these Shopify stores is they're undoubtedly strange because they're new, but their model is fundamentally not that different from, like, a big corporate enterprise. It's just that the tools have democratized the ability for people to tap into the globalized economy. And so before, it took the idea of having a supply chain and having all these people who would know factory owners in China and all these other kinds of things - that took a lot of infrastructure. What these tools have done is eliminate the need for all that infrastructure. And so now it's this alternative way into what is a real thing about our economy, which is that many, many of the goods that we all purchase are made in Asian factories and are sold to us at a very high markup from their production costs. SIMON: So where's the coat now, may I ask? MADRIGAL: It's hanging up in my closet, kind of towards the back - in there with the things that are too small for me to fit into now (laughter). But I'll bring it out at some point, I'm sure. Everyone needs a coat like that, you know, maybe for gardening or something. SIMON: (Laughter) Alexis Madrigal, staff writer of The Atlantic speaking with us by Skype. Thanks so much. And you know, I can tell - even over Skype - you're looking great. MADRIGAL: (Laughter) Thank you, Scott. SCOTT SIMON, HOST:  All Alexis Madrigal wanted was a coat. He bought that coat through an ad he saw on Instagram. A sponsored post popped up in his photo feed. He hadn't heard of the retailer, but it billed itself as luxury for modern gentlemen. And what a deal - the price was less than a hundred dollars. The jacket Alexis Madrigal got delivered, however, turned out to be less an expression of luxury and more a lesson into how e-commerce has changed in the age of Instagram and Facebook. Alexis Madrigal wrote about this for The Atlantic, and he joins us now from Oakland, Calif. Thanks so much for being with us. ALEXIS MADRIGAL: Thank you. SIMON: What'd the coat look like? MADRIGAL: Well, it kind of looked like a carpet, I would say, formed into a roughly coat-like shape. (Laughter) It kind of had a velour sheen and kind of the texture of indoor-outdoor carpeting. SIMON: (Laughter). MADRIGAL: It was not the nicest coat that I'd ever seen. SIMON: So what's your complaint? MADRIGAL: Well, the complaint was I wanted to get something for nothing. You know what I mean? I wanted a nice, expensive-looking coat for less than a hundred dollars, which, as it turns out, even with all the magic of the Internet, is still impossible. SIMON: Yeah. Well, explain some of that magic of the Internet because this is territory you know well. How are you - I'm not sure taking it is the word because it seems to me if you were, forgive me, naive enough to think you could get a good coat for under a hundred dollars - well, you can finish that sentence. What was the magic that took you in? MADRIGAL: Well, so what I got really interested in was that when the coat showed up, it showed up in a black plastic package from China Post. So I thought I was buying a coat from some brand that represented itself as kind of this modern gentleman. You know, I was expecting it to have its headquarters, you know, in New York or San Francisco or Los Angeles or something. And this thing is shipped direct from a technology park in China. So I kind of started thinking like, well, what is this brand? And I started diving into this new class of online retailer that use a tool called Shopify, which allows anyone to kind of spin up a retail store in five minutes; sucks products in from a service called AliExpress, which is sort of like Amazon but in China, and it's dedicated to kind of the export market for Chinese and other Asian manufacturers; and allows - basically - consumers in the U. S. to take a different route into this manufacturing ecosystem, which makes so many clothes, which makes so many consumer goods, in Asia. SIMON: When you ordered this coat, were you genuinely surprised to be able to trace this stuff back, or was that the whole idea? MADRIGAL: Well, no. I was genuinely surprised. I was literally - I had been tagged in the Facebook advertising system, which Instagram also uses, as someone who likes to buy clothes. And what I came to find was that this is a pretty widespread phenomenon. Shopify says that they have 500,000 merchants. The overwhelming majority of them are small- and medium-sized businesses. And some subset - although no one's totally sure exactly how large it is - are these people who do what's called drop shipping, which means that they never handle the goods that they sell. They essentially are a front end, just a way of accessing these items that are then directly shipped from their Chinese manufacturers. So in some weird way, it sort of cuts out the middleman of sort of an H&M or a Zara or some other fast-fashion place. But it inserts this new middle person, which is a Shopify e-commerce site that took somebody five minutes to set up. And one of the people that I found doing that was this, you know, 17-year-old, or at least presumed, he calls himself a 17-year-old, living in, like, suburban Dublin. SIMON: Well, he sounds like a very ambitious entrepreneur. MADRIGAL: I think they are, you know. I think the appeal of it, obviously, for entrepreneurs - this drop shipping model - is that it takes almost nothing upfront. You don't need money to buy the stuff. The tools online are very inexpensive, and it's this possibility that these entrepreneurs can get something for nothing - right? - that they can generate money basically out of thin air. SIMON: And not to make any comparisons, but I seem to recall that there was a time when J. Crew was a catalog retailer - didn't have any real stores. And now, of course, they've got plenty of them in addition to a very ambitious online retail site. MADRIGAL: Absolutely. The way that I've thought about these Shopify stores is they're undoubtedly strange because they're new, but their model is fundamentally not that different from, like, a big corporate enterprise. It's just that the tools have democratized the ability for people to tap into the globalized economy. And so before, it took the idea of having a supply chain and having all these people who would know factory owners in China and all these other kinds of things - that took a lot of infrastructure. What these tools have done is eliminate the need for all that infrastructure. And so now it's this alternative way into what is a real thing about our economy, which is that many, many of the goods that we all purchase are made in Asian factories and are sold to us at a very high markup from their production costs. SIMON: So where's the coat now, may I ask? MADRIGAL: It's hanging up in my closet, kind of towards the back - in there with the things that are too small for me to fit into now (laughter). But I'll bring it out at some point, I'm sure. Everyone needs a coat like that, you know, maybe for gardening or something. SIMON: (Laughter) Alexis Madrigal, staff writer of The Atlantic speaking with us by Skype. Thanks so much. And you know, I can tell - even over Skype - you're looking great. MADRIGAL: (Laughter) Thank you, Scott.", "section": "Business", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-14-577969778": {"title": "What Your Smart Devices Know About You : NPR", "url": "https://www.npr.org/2018/01/14/577969778/what-your-smart-devices-know-about-you", "author": "No author found", "published_date": "2018-01-14", "content": "LULU GARCIA-NAVARRO, HOST: With 1 in 6 Americans now owning voice-activated assistance, many of you tuned into the show this morning by saying, Alexa or Google, play NPR. These smart devices can order dinner delivered to your door. They are chatty and can entertain your kids with lame jokes like this. . . COMPUTER-GENERATED VOICE: What was Bruce Wayne's favorite baby toy? The Batmobile. GARCIA-NAVARRO: Or help you live your Star Trek fantasy by firing photon torpedoes. (SOUNDBITE OF EXPLOSION)GARCIA-NAVARRO: As the popularity of voice-activated assistance grow, so do concerns that Alexa and Google Home are doing more than just ordering takeout. Joining us from member station WBHM is Brian Barrett who writes for WIRED magazine. Good morning. BRIAN BARRETT: Good morning. GARCIA-NAVARRO: And also joining us is Echo. Echo, good morning. COMPUTER-GENERATED VOICE: Good morning. GARCIA-NAVARRO: All right, let's start with how these devices work. And this is for you, Brian. Are they always listening? BARRETT: The device itself is always listening. There are microphones on there that are picking up everything that you say. But it's not until you say a wake word, which you can set on the device - it can either be Alexa. It could be Echo, as we just showed. For Google Home, it's usually, OK, Google or, hey, Google. And only when you say that wake word, does the device actually send what you're saying back to those companies servers. So only then does it connect to the Internet, and that's an important distinction. I think a lot of the discomfort that people have is the idea that Google or Amazon themselves are collecting every single thing you say. That's not really true. It's sort of a passive listening until you say that word, and that's when they start actually collecting that data. GARCIA-NAVARRO: So let's give an example. Echo, are you listening? COMPUTER-GENERATED VOICE: Hi, I'm here. I start listening when I hear the wake word. GARCIA-NAVARRO: There you go. But these machines are also learning about us - aren't they? - our preferences, what we ask for. They're in our homes. That's how artificial intelligence works. And these devices are AI-based essentially. BARRETT: So that's true. And the more data that you give it after the wake word, the more these companies will know about you. I think, though, I would say in the same way that the more often you search on Google for things, the more Google knows what ads to serve you. I think that it's partly discomforting because it's a new kind of that data collection, not that it's a totally different thing than we've experienced before. And people could be right to be uncomfortable with that if they don't want the same sort of tracking that happens and the same sort of data mining that happens online to sort of bleed into their real life, as well. GARCIA-NAVARRO: But we've heard reports of people all of a sudden hearing Alexa suddenly start talking when the room was completely silent. It might flash a blue light, which means it's listening, but I may have not said anything. What's that about. BARRETT: When you ask these companies what is going on with that, they don't really have a great answer. The best that you can get out of them is, well. . . GARCIA-NAVARRO: It's alive. BARRETT: (Laughter) Exactly. GARCIA-NAVARRO: (Laughter). BARRETT: The best you get out of them is that they are continually working to improve wake word technology, which is another way of saying that these machines aren't that smart yet, and they sometimes think they're hearing things even though they're not. The good news is, though, when that happens, you can go into your app, and you'll see a history of everything that it has heard. But they do give you an out if you really are uncomfortable with it, which a lot of people understandably might be because they do just sort of keep that information on their servers indefinitely. GARCIA-NAVARRO: Yeah. And prosecutors have subpoenaed Amazon and Google, looking for evidence that could help them prosecute crimes. BARRETT: It's true. And I would say that the kind of data that these devices collect is fairly limited. You would have to have a very specific scenario in which case that information would be useful in a criminal case. You would have to have said the wake word and then something incriminating. GARCIA-NAVARRO: I'm curious - let's see what Echo says. Echo, I'm going to rob a bank. COMPUTER-GENERATED VOICE: I'm not answering that. (LAUGHTER)BARRETT: So plausible deniability. GARCIA-NAVARRO: Yeah, plausible deniability. BARRETT: One good way to think about these devices - it's all just data. So in the same way that police often go to - or law enforcement often goes to Google or Amazon or Apple or anybody and says, I need to access what's on your cloud - I need to access this person's search history - those requests are often complied with. So it feels different because it's voice. But the fundamental principles are the same that you have in any other of these digital spheres. GARCIA-NAVARRO: But couldn't hackers or law enforcement use these devices to eavesdrop on us? BARRETT: Yes someone could hack eventually a Google Home or an Alexa device. But it seems like there are easier ways to get that information. And even if you want that information, I think we often sometimes think that what we're saying is of keen interest to a lot of people, where that's not often the case, at least in terms of the kinds of people who would be capable of accessing these devices. I think if the NSA is after you, you probably shouldn't keep a Google Home in your apartment. And you've also probably got a lot of other things to worry about too. GARCIA-NAVARRO: I'm going to end with Echo. Echo, can we trust you? COMPUTER-GENERATED VOICE: Sorry, I'm not sure. GARCIA-NAVARRO: (Laughter) And there you go. Brian Barrett - he writes for Wired. Thank you very much. BARRETT: Thanks so much. GARCIA-NAVARRO: Echo, thank you very much. COMPUTER-GENERATED VOICE: My pleasure. LULU GARCIA-NAVARRO, HOST:  With 1 in 6 Americans now owning voice-activated assistance, many of you tuned into the show this morning by saying, Alexa or Google, play NPR. These smart devices can order dinner delivered to your door. They are chatty and can entertain your kids with lame jokes like this. . . COMPUTER-GENERATED VOICE: What was Bruce Wayne's favorite baby toy? The Batmobile. GARCIA-NAVARRO: Or help you live your Star Trek fantasy by firing photon torpedoes. (SOUNDBITE OF EXPLOSION) GARCIA-NAVARRO: As the popularity of voice-activated assistance grow, so do concerns that Alexa and Google Home are doing more than just ordering takeout. Joining us from member station WBHM is Brian Barrett who writes for WIRED magazine. Good morning. BRIAN BARRETT: Good morning. GARCIA-NAVARRO: And also joining us is Echo. Echo, good morning. COMPUTER-GENERATED VOICE: Good morning. GARCIA-NAVARRO: All right, let's start with how these devices work. And this is for you, Brian. Are they always listening? BARRETT: The device itself is always listening. There are microphones on there that are picking up everything that you say. But it's not until you say a wake word, which you can set on the device - it can either be Alexa. It could be Echo, as we just showed. For Google Home, it's usually, OK, Google or, hey, Google. And only when you say that wake word, does the device actually send what you're saying back to those companies servers. So only then does it connect to the Internet, and that's an important distinction. I think a lot of the discomfort that people have is the idea that Google or Amazon themselves are collecting every single thing you say. That's not really true. It's sort of a passive listening until you say that word, and that's when they start actually collecting that data. GARCIA-NAVARRO: So let's give an example. Echo, are you listening? COMPUTER-GENERATED VOICE: Hi, I'm here. I start listening when I hear the wake word. GARCIA-NAVARRO: There you go. But these machines are also learning about us - aren't they? - our preferences, what we ask for. They're in our homes. That's how artificial intelligence works. And these devices are AI-based essentially. BARRETT: So that's true. And the more data that you give it after the wake word, the more these companies will know about you. I think, though, I would say in the same way that the more often you search on Google for things, the more Google knows what ads to serve you. I think that it's partly discomforting because it's a new kind of that data collection, not that it's a totally different thing than we've experienced before. And people could be right to be uncomfortable with that if they don't want the same sort of tracking that happens and the same sort of data mining that happens online to sort of bleed into their real life, as well. GARCIA-NAVARRO: But we've heard reports of people all of a sudden hearing Alexa suddenly start talking when the room was completely silent. It might flash a blue light, which means it's listening, but I may have not said anything. What's that about. BARRETT: When you ask these companies what is going on with that, they don't really have a great answer. The best that you can get out of them is, well. . . GARCIA-NAVARRO: It's alive. BARRETT: (Laughter) Exactly. GARCIA-NAVARRO: (Laughter). BARRETT: The best you get out of them is that they are continually working to improve wake word technology, which is another way of saying that these machines aren't that smart yet, and they sometimes think they're hearing things even though they're not. The good news is, though, when that happens, you can go into your app, and you'll see a history of everything that it has heard. But they do give you an out if you really are uncomfortable with it, which a lot of people understandably might be because they do just sort of keep that information on their servers indefinitely. GARCIA-NAVARRO: Yeah. And prosecutors have subpoenaed Amazon and Google, looking for evidence that could help them prosecute crimes. BARRETT: It's true. And I would say that the kind of data that these devices collect is fairly limited. You would have to have a very specific scenario in which case that information would be useful in a criminal case. You would have to have said the wake word and then something incriminating. GARCIA-NAVARRO: I'm curious - let's see what Echo says. Echo, I'm going to rob a bank. COMPUTER-GENERATED VOICE: I'm not answering that. (LAUGHTER) BARRETT: So plausible deniability. GARCIA-NAVARRO: Yeah, plausible deniability. BARRETT: One good way to think about these devices - it's all just data. So in the same way that police often go to - or law enforcement often goes to Google or Amazon or Apple or anybody and says, I need to access what's on your cloud - I need to access this person's search history - those requests are often complied with. So it feels different because it's voice. But the fundamental principles are the same that you have in any other of these digital spheres. GARCIA-NAVARRO: But couldn't hackers or law enforcement use these devices to eavesdrop on us? BARRETT: Yes someone could hack eventually a Google Home or an Alexa device. But it seems like there are easier ways to get that information. And even if you want that information, I think we often sometimes think that what we're saying is of keen interest to a lot of people, where that's not often the case, at least in terms of the kinds of people who would be capable of accessing these devices. I think if the NSA is after you, you probably shouldn't keep a Google Home in your apartment. And you've also probably got a lot of other things to worry about too. GARCIA-NAVARRO: I'm going to end with Echo. Echo, can we trust you? COMPUTER-GENERATED VOICE: Sorry, I'm not sure. GARCIA-NAVARRO: (Laughter) And there you go. Brian Barrett - he writes for Wired. Thank you very much. BARRETT: Thanks so much. GARCIA-NAVARRO: Echo, thank you very much. COMPUTER-GENERATED VOICE: My pleasure.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-15-578172758": {"title": "LinkedIn Co-Founder On What Resolutions Silicon Valley Should Make For 2018 : NPR", "url": "https://www.npr.org/2018/01/15/578172758/linkedin-co-founder-on-what-resolutions-silicon-valley-should-make-for-2018", "author": "No author found", "published_date": "2018-01-15", "content": "KELLY MCEVERS, HOST: And in All Tech Considered this week, we are talking about what is ahead for the tech industry in 2018. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")MCEVERS: In some ways, 2017 was a great year for tech companies - big growth, big profits. In other ways, it was a pretty bad year. Facebook, Twitter and Google admitted that Russian operatives used their platforms to promote fake stories during the election. There were sexual harassment scandals and criticism of the fact that white men still basically run the place. My guest is someone who knows many of the leaders of these companies, Reid Hoffman. He is a venture capitalist. He's the co-founder of LinkedIn. And he's got some thoughts about how Silicon Valley can do better in 2018. Welcome to the show. REID HOFFMAN: It's great to be here. MCEVERS: Up until now, tech companies have not been super great at taking responsibility for their problems. Do you think that this is the year of reckoning for tech companies? HOFFMAN: Well, I hope that it's actually a year of growth. I think actually part of what the tech companies are learning is they started as challengers, these kind of, you know - think of it as young teenagers with good ideas. . . MCEVERS: Yeah. HOFFMAN: . . . Trying to prove themselves. And now they realize I think, no, actually, in fact, we're the incumbents. We're the providers of the infrastructure. We have influence in the national dialogue, and we need to upgrade our play. And I think you can see that as the changing messages from them throughout the year. The way I like to look at this is - I have a funny phrase. It's Spider-Man ethics. With power comes responsibility. With great power comes great responsibility. And I think there's beginning to have that recognition of, we have this responsibility, and we know that we need to act now both in conversation with society and societies and also to make sure that, like, there's a higher level of trust and reliability and information and in a kind of - a sense of safety and security in your participation in these online networks and communities. MCEVERS: Last year you came out with a decency pledge, and it was aimed at stopping sexual harassment in the industry. And one of the things you talked about was how much power venture capitalists have over entrepreneurs, entrepreneurs who frankly need the money (laughter), right? HOFFMAN: Yeah. MCEVERS: What's being done about that? Like, how - you're not going to undo that system, the way things work in that industry. So how do you address the problem? HOFFMAN: Well, so the decency pledge is meant to be a - kind of a first step to just have a whole bunch of people say, look; I will not do business with people who are, you know, sexual predators, harassers, abusing their position of power in any capacity. And then everyone can make the public statement to that and that part of that public statement is then not only am I articulating a voice, but I can also be held accountable by the people who know me and see what's going on. I think the thing that we need to move from is - you know, last year's decency pledge was very much of the, look; here's a baseline that we can all do to step forward as individuals. To react to this, I think we now need to move from that reactive game to a proactive game. MCEVERS: I just want to be really specific. Like, is that a step that's led to any change, you know, that you can point to, any examples? HOFFMAN: Well, I think one of the things that was really awesome is a large number of the powerful VCs in the Valley all publicly signed up to it. MCEVERS: Yeah. HOFFMAN: They all said, hey, I'm taking this, too. So I do think - I've heard from women entrepreneur friends of mine, women investor friends of mine that the atmosphere has become much more conducive to being able to speak up. And I do think that people are paying attention to - is, like, not only do we protect the victims, but we also try to fix the system. And we try to say, we have a zero tolerance around, you know, sexual harassment, sexual predators trying to abuse these power relationships. And so I think at least it's moved the culture in the right direction. And I've heard good signs and conversation. I don't have a dashboard that I could share. MCEVERS: Recently some Apple investors urged the company to address concerns that its technology was hurting children. You know, there's been a lot of fear out there - right? - that tech is addictive and it's harmful or, at the very least, it's replacing, you know, human interactions. Do you think one day we will think of tech companies the way we think of big tobacco - you know, this idea of, like, selling a dangerous product without consequences, without remorse at least? HOFFMAN: Look; so technology always has some rough edges and downsides as well as upsides. But overall, you know, I'm glad we have it. I'm glad we're more globally connected. I'm glad we have - even though it's an information overload, (laughter) I'm glad we have a lot of information and can do searches and find information on things. And you know, I tend to think that a lot of this tends to be older generations. Like, we feel, like, addicted and overwhelmed, but then the younger generations learn and adapt. So I tend to think that people are adaptive. And we can improve the technology to be net massively positive. So I think the chance that future technology is looked at as Big Tobacco is almost zero. MCEVERS: But it puts the onus on the people who are using it - right? - to sort of adapt. And it puts a lot of, like, faith in you guys to do good. And I think that's the hard sell right now, right? HOFFMAN: Well, so that's a little bit of the reason why was saying I think the broad move is towards more transparency. And either the industry will adopt ways of being transparent, which I hope and I'm pushing for. Or the government will say, well, OK, since you're not actually being sufficiently disclosive to make sure that we don't feel like we're being manipulated and so forth, then we're going to establish some rules. And the rules may limit your ability to innovate and create great new things for the world and for us, but c'est la vie. MCEVERS: Yeah. HOFFMAN: But I have faith that part of what I'm seeing happening, as I mentioned in the beginning, in 2017 is that the technologists and the technology companies are going, hey, no, we have responsibilities here. Let's try to figure them out. MCEVERS: Do you think it's time in 2018 for, you know, more regulation? HOFFMAN: Well, my big worry is that the most common pattern in regulation is to lock the past in slow motion against the future. MCEVERS: Right. HOFFMAN: And, A, I think the future's very good for us - you know, what we can invent in precision medicine and what we can invent in anything from new communications technologies to autonomous vehicles and so forth. The second point of it is - is actually, you know, nations and groups are in competition. So if we say, well, we're going to slow down our tech development, (laughter) you know, I don't think other countries - China, et cetera - I don't think they will be. Now, that being said, if you said, OK, you know, we got to do some regulation - must do - what would it be? It would be like, well, try to demand some more transparency on the variables that most matter to you, like how much, you know, for example, election hacking is actually going on on your platform, (laughter) right? MCEVERS: Right, yeah. HOFFMAN: And what are you doing about it? MCEVERS: Yeah. HOFFMAN: And you need to be transparent about that. It's not that we say, no election hacking. It's - we say, you've got to give us good reports about what's going on and how you're making progress and what you're doing about it. And I think that would be the kind of thing that I think - I hope tech companies will do more voluntarily. And I also think that if you - if I were to start doing anything because I'm so concerned about technology being part of the solution as opposed to part of the problem - that we don't slow down our path to the solution. MCEVERS: Reid Hoffman is a partner at the Silicon Valley venture capital firm Greylock. He also hosts the Masters of Scale podcast. Thanks for being with us. HOFFMAN: Awesome to be here. (SOUNDBITE OF CUT CHEMIST SONG, \"THE GARDEN\") KELLY MCEVERS, HOST:  And in All Tech Considered this week, we are talking about what is ahead for the tech industry in 2018. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") MCEVERS: In some ways, 2017 was a great year for tech companies - big growth, big profits. In other ways, it was a pretty bad year. Facebook, Twitter and Google admitted that Russian operatives used their platforms to promote fake stories during the election. There were sexual harassment scandals and criticism of the fact that white men still basically run the place. My guest is someone who knows many of the leaders of these companies, Reid Hoffman. He is a venture capitalist. He's the co-founder of LinkedIn. And he's got some thoughts about how Silicon Valley can do better in 2018. Welcome to the show. REID HOFFMAN: It's great to be here. MCEVERS: Up until now, tech companies have not been super great at taking responsibility for their problems. Do you think that this is the year of reckoning for tech companies? HOFFMAN: Well, I hope that it's actually a year of growth. I think actually part of what the tech companies are learning is they started as challengers, these kind of, you know - think of it as young teenagers with good ideas. . . MCEVERS: Yeah. HOFFMAN: . . . Trying to prove themselves. And now they realize I think, no, actually, in fact, we're the incumbents. We're the providers of the infrastructure. We have influence in the national dialogue, and we need to upgrade our play. And I think you can see that as the changing messages from them throughout the year. The way I like to look at this is - I have a funny phrase. It's Spider-Man ethics. With power comes responsibility. With great power comes great responsibility. And I think there's beginning to have that recognition of, we have this responsibility, and we know that we need to act now both in conversation with society and societies and also to make sure that, like, there's a higher level of trust and reliability and information and in a kind of - a sense of safety and security in your participation in these online networks and communities. MCEVERS: Last year you came out with a decency pledge, and it was aimed at stopping sexual harassment in the industry. And one of the things you talked about was how much power venture capitalists have over entrepreneurs, entrepreneurs who frankly need the money (laughter), right? HOFFMAN: Yeah. MCEVERS: What's being done about that? Like, how - you're not going to undo that system, the way things work in that industry. So how do you address the problem? HOFFMAN: Well, so the decency pledge is meant to be a - kind of a first step to just have a whole bunch of people say, look; I will not do business with people who are, you know, sexual predators, harassers, abusing their position of power in any capacity. And then everyone can make the public statement to that and that part of that public statement is then not only am I articulating a voice, but I can also be held accountable by the people who know me and see what's going on. I think the thing that we need to move from is - you know, last year's decency pledge was very much of the, look; here's a baseline that we can all do to step forward as individuals. To react to this, I think we now need to move from that reactive game to a proactive game. MCEVERS: I just want to be really specific. Like, is that a step that's led to any change, you know, that you can point to, any examples? HOFFMAN: Well, I think one of the things that was really awesome is a large number of the powerful VCs in the Valley all publicly signed up to it. MCEVERS: Yeah. HOFFMAN: They all said, hey, I'm taking this, too. So I do think - I've heard from women entrepreneur friends of mine, women investor friends of mine that the atmosphere has become much more conducive to being able to speak up. And I do think that people are paying attention to - is, like, not only do we protect the victims, but we also try to fix the system. And we try to say, we have a zero tolerance around, you know, sexual harassment, sexual predators trying to abuse these power relationships. And so I think at least it's moved the culture in the right direction. And I've heard good signs and conversation. I don't have a dashboard that I could share. MCEVERS: Recently some Apple investors urged the company to address concerns that its technology was hurting children. You know, there's been a lot of fear out there - right? - that tech is addictive and it's harmful or, at the very least, it's replacing, you know, human interactions. Do you think one day we will think of tech companies the way we think of big tobacco - you know, this idea of, like, selling a dangerous product without consequences, without remorse at least? HOFFMAN: Look; so technology always has some rough edges and downsides as well as upsides. But overall, you know, I'm glad we have it. I'm glad we're more globally connected. I'm glad we have - even though it's an information overload, (laughter) I'm glad we have a lot of information and can do searches and find information on things. And you know, I tend to think that a lot of this tends to be older generations. Like, we feel, like, addicted and overwhelmed, but then the younger generations learn and adapt. So I tend to think that people are adaptive. And we can improve the technology to be net massively positive. So I think the chance that future technology is looked at as Big Tobacco is almost zero. MCEVERS: But it puts the onus on the people who are using it - right? - to sort of adapt. And it puts a lot of, like, faith in you guys to do good. And I think that's the hard sell right now, right? HOFFMAN: Well, so that's a little bit of the reason why was saying I think the broad move is towards more transparency. And either the industry will adopt ways of being transparent, which I hope and I'm pushing for. Or the government will say, well, OK, since you're not actually being sufficiently disclosive to make sure that we don't feel like we're being manipulated and so forth, then we're going to establish some rules. And the rules may limit your ability to innovate and create great new things for the world and for us, but c'est la vie. MCEVERS: Yeah. HOFFMAN: But I have faith that part of what I'm seeing happening, as I mentioned in the beginning, in 2017 is that the technologists and the technology companies are going, hey, no, we have responsibilities here. Let's try to figure them out. MCEVERS: Do you think it's time in 2018 for, you know, more regulation? HOFFMAN: Well, my big worry is that the most common pattern in regulation is to lock the past in slow motion against the future. MCEVERS: Right. HOFFMAN: And, A, I think the future's very good for us - you know, what we can invent in precision medicine and what we can invent in anything from new communications technologies to autonomous vehicles and so forth. The second point of it is - is actually, you know, nations and groups are in competition. So if we say, well, we're going to slow down our tech development, (laughter) you know, I don't think other countries - China, et cetera - I don't think they will be. Now, that being said, if you said, OK, you know, we got to do some regulation - must do - what would it be? It would be like, well, try to demand some more transparency on the variables that most matter to you, like how much, you know, for example, election hacking is actually going on on your platform, (laughter) right? MCEVERS: Right, yeah. HOFFMAN: And what are you doing about it? MCEVERS: Yeah. HOFFMAN: And you need to be transparent about that. It's not that we say, no election hacking. It's - we say, you've got to give us good reports about what's going on and how you're making progress and what you're doing about it. And I think that would be the kind of thing that I think - I hope tech companies will do more voluntarily. And I also think that if you - if I were to start doing anything because I'm so concerned about technology being part of the solution as opposed to part of the problem - that we don't slow down our path to the solution. MCEVERS: Reid Hoffman is a partner at the Silicon Valley venture capital firm Greylock. He also hosts the Masters of Scale podcast. Thanks for being with us. HOFFMAN: Awesome to be here. (SOUNDBITE OF CUT CHEMIST SONG, \"THE GARDEN\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-17-578666058": {"title": "Apple Says It Will Create 20,000 Jobs With New Campus : NPR", "url": "https://www.npr.org/2018/01/17/578666058/apple-says-it-will-create-20-000-jobs-with-new-campus", "author": "No author found", "published_date": "2018-01-17", "content": "KELLY MCEVERS, HOST: Apple made a big announcement today. It is going to invest a lot more money in the United States and create 20,000 jobs here over the next five years. NPR's Laura Sydell is with us to talk about what Apple is doing and why it is doing it now. Hey, Laura. LAURA SYDELL, BYLINE: Hello. MCEVERS: So what else is in this big announcement? SYDELL: Well, I'd say one of the most interesting things in the announcement is that Apple's saying it's going to build a new campus, and that comes after they just spent billions of dollars on a fancy new headquarters in Cupertino. They're not saying yet where this new campus will be built. They did say it will initially be focused on technical support for customers. And it's possible they're doing this because the cost of living out in Silicon Valley is frankly very high. So they're looking for, you know, cheaper land, lower property values. And it's interesting. They made this announcement after Amazon made this huge splash with plans to build a second headquarters somewhere in the U. S. The big emphasis in this announcement is just that Apple wants everyone to know that it's spending money and creating jobs in the U. S. And I'm sure that's because it has faced a lot of criticism over outsourcing to China in particular and for keeping a large portion of their profits overseas. MCEVERS: So is that going to change now, too? SYDELL: Well, it's hard to say. We don't know all the details yet, but they say they're going to be paying some $38 billion in taxes, money that they're bringing back into the country. And that number suggests they'll be bringing back a lot of overseas cash. And they've got a lot to bring back. Apple's most recent report said they had $250 billion overseas. For years, CEO Tim Cook has been very critical of U. S. tax laws, and he's been sensitive to criticism that Apple's been dodging U. S. taxes. With today's announcement, he said, quote, \"we have a deep sense of responsibility to give back to our country and the people who make our success possible. \"MCEVERS: And Apple says all these moves will add up to a $350 billion contribution to the U. S. economy over five years. Does that sound plausible to you? SYDELL: Well, let's be clear what we're talking about here. MCEVERS: OK. SYDELL: Apple isn't saying, you know, that it's going to spend $350 billion in the U. S. MCEVERS: Right. SYDELL: What it is saying is that the impact of what it's doing is going to contribute that much money to the economy, but that's a very hard thing to measure or predict. More specifically, they are doing things like pledging more investment in what they call their manufacturing fund. So $5 billion will go to that. And this fund helps other manufacturers that supply Apple be innovative and grow. MCEVERS: How much of what Apple is doing can we attribute to the new tax law which lowered the corporate tax rate? SYDELL: Well, it is definitely part of it - the part about the taxes Apple will pay on the money it's going to bring back from overseas. They're bringing back, as we said, $38 billion in taxes. That's a direct impact from the tax bill. It brought down the amount of tax Apple would have to pay for bringing profits back into the U. S. The rest - it's hard to say. Keep in mind Apple is a very successful company, and they could have done any of these other things at any time. However, it could be that Apple's just feeling more political pressure to show that it's a patriotic company and it's supporting American workers. MCEVERS: NPR tech correspondent Laura Sydell, thank you. SYDELL: You're welcome. KELLY MCEVERS, HOST:  Apple made a big announcement today. It is going to invest a lot more money in the United States and create 20,000 jobs here over the next five years. NPR's Laura Sydell is with us to talk about what Apple is doing and why it is doing it now. Hey, Laura. LAURA SYDELL, BYLINE: Hello. MCEVERS: So what else is in this big announcement? SYDELL: Well, I'd say one of the most interesting things in the announcement is that Apple's saying it's going to build a new campus, and that comes after they just spent billions of dollars on a fancy new headquarters in Cupertino. They're not saying yet where this new campus will be built. They did say it will initially be focused on technical support for customers. And it's possible they're doing this because the cost of living out in Silicon Valley is frankly very high. So they're looking for, you know, cheaper land, lower property values. And it's interesting. They made this announcement after Amazon made this huge splash with plans to build a second headquarters somewhere in the U. S. The big emphasis in this announcement is just that Apple wants everyone to know that it's spending money and creating jobs in the U. S. And I'm sure that's because it has faced a lot of criticism over outsourcing to China in particular and for keeping a large portion of their profits overseas. MCEVERS: So is that going to change now, too? SYDELL: Well, it's hard to say. We don't know all the details yet, but they say they're going to be paying some $38 billion in taxes, money that they're bringing back into the country. And that number suggests they'll be bringing back a lot of overseas cash. And they've got a lot to bring back. Apple's most recent report said they had $250 billion overseas. For years, CEO Tim Cook has been very critical of U. S. tax laws, and he's been sensitive to criticism that Apple's been dodging U. S. taxes. With today's announcement, he said, quote, \"we have a deep sense of responsibility to give back to our country and the people who make our success possible. \" MCEVERS: And Apple says all these moves will add up to a $350 billion contribution to the U. S. economy over five years. Does that sound plausible to you? SYDELL: Well, let's be clear what we're talking about here. MCEVERS: OK. SYDELL: Apple isn't saying, you know, that it's going to spend $350 billion in the U. S. MCEVERS: Right. SYDELL: What it is saying is that the impact of what it's doing is going to contribute that much money to the economy, but that's a very hard thing to measure or predict. More specifically, they are doing things like pledging more investment in what they call their manufacturing fund. So $5 billion will go to that. And this fund helps other manufacturers that supply Apple be innovative and grow. MCEVERS: How much of what Apple is doing can we attribute to the new tax law which lowered the corporate tax rate? SYDELL: Well, it is definitely part of it - the part about the taxes Apple will pay on the money it's going to bring back from overseas. They're bringing back, as we said, $38 billion in taxes. That's a direct impact from the tax bill. It brought down the amount of tax Apple would have to pay for bringing profits back into the U. S. The rest - it's hard to say. Keep in mind Apple is a very successful company, and they could have done any of these other things at any time. However, it could be that Apple's just feeling more political pressure to show that it's a patriotic company and it's supporting American workers. MCEVERS: NPR tech correspondent Laura Sydell, thank you. SYDELL: You're welcome.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-18-578956975": {"title": "Google's Art Selfie App Offers A Lesson In Biometric Privacy Laws In U.S. : NPR", "url": "https://www.npr.org/2018/01/18/578956975/googles-art-selfie-app-offers-a-lesson-in-biometric-privacy-laws-in-u-s", "author": "No author found", "published_date": "2018-01-18", "content": "MARY LOUISE KELLY, HOST:  I spent a few minutes today doing something millions of Americans have done in recent days - trying out the Google Arts and Culture app. Here we go. Take a selfie and search thousands of artworks to see if any look like you. Get started. All right, three, two, one - cheese. Yeah, so the app is just what it sounds like. You take a selfie. You get an instant match with a great work of art that is supposed to resemble you. Sometimes the result is less than flattering. (Laughter) It actually doesn't look anything like me, so I'm going to try this one more time. Oh, I'm liking this one. This is a portrait of Thirza Whysall from the collection of the Royal College of Music painted by Elizabeth Robinson McCallum. She does have kind of wavy blonde hair. She's playing a violin. So there you go, my inner artiste is coming out. (Laughter) OK, so that's how the app is supposed to work. Here's the thing - if I were sitting in Texas or in Illinois, I couldn't do this. The app doesn't work. Law professor Matthew Kugler of Northwestern University has studied Illinois' biometric privacy laws, and he joins us now. Welcome. MATTHEW KUGLER: Thank you. Thank you. KELLY: You are in Chicago right now. Explain for us what happens if you try to open the app and use the art selfie feature. KUGLER: So what people have been discovering - and this was not publicized, they've been discovering it as they attempt to do it - is that the option is simply not available. And what has become apparent over the last two days in particular is that Google has intentionally disabled this functionality in Illinois and in Texas in response to the biometric privacy laws. KELLY: Google has not actually put out a statement one way or the other. We're reaching out to them to see what their take on this is. But what you're describing there is biometric law in Illinois that restricts the amount of information that tech companies can collect from users like you or me. KUGLER: Yes. So in these two states, if you want to collect those biometrics you have to give certain disclosures to people telling them what you're going to do with the information. You have to get their permission for a variety of things. And even with that there are restrictions on how you can share the information and whether you can sell it. KELLY: Now, in this case, we looked. Google does have a disclaimer on the app. It says - and I'll quote it - \"Google won't use data from your photo for any other purpose and will only store your photo for the time it takes to search for matches. \" And it makes you choose either I accept or, you know, don't accept and then you don't proceed. So with that disclaimer, how does the app not satisfy Illinois legal requirements? KUGLER: My impression is that Google is showing an abundance of caution here. The law is still being litigated in a number of courts. Google is involved in litigation involving the law. And they may be concerned that someone might upload someone else's picture and therefore they don't have the consent of the person whose image is being captured. That's a problem with home security cameras. And there's a Nest product not available in Illinois as far as we can tell because of this concern. KELLY: Nest, we should mention, is another technology company, uses biometric data for different applications. And it's owned by the same parent company as Google. KUGLER: Yes. So I'm wondering if there's perhaps some Google-wide understanding of caution, though obviously Google has not been forthcoming explaining exactly why it's doing some of these things. KELLY: Why was this law passed in Illinois in the first place? This was to protect privacy rights of residents there. KUGLER: Yes. There was a company going through bankruptcy in Illinois that had a fair bit of biometric data. And as it was going through bankruptcy, there was concern that the data had been collected when consumers had certain impressions of how it was going to be used. And in bankruptcy, that data might be sold to someone else who has completely different intentions. So maybe you trust Google with certain data, but for Google to hand that data off to someone else you wouldn't trust them. So this law was passed in part to avoid that kind of unintended consequence. KELLY: And meanwhile, to loop us back to where we began, are people in Illinois finding ways to circumvent this law? KUGLER: Several of my friends have commented that when they travel out of state they're going to make a point to stop by the side of the road and make use of this feature (laughter). How much they care is unclear. This is perhaps the flavor of the moment, and in a week we'll all wonder why we cared. KELLY: We'll all be moving on to being agitated about something else. Matthew Kugler, thanks very much. KUGLER: Thank you. KELLY: That's Matthew Kugler of the Pritzker School of Law at Northwestern University. MARY LOUISE KELLY, HOST:   I spent a few minutes today doing something millions of Americans have done in recent days - trying out the Google Arts and Culture app. Here we go. Take a selfie and search thousands of artworks to see if any look like you. Get started. All right, three, two, one - cheese. Yeah, so the app is just what it sounds like. You take a selfie. You get an instant match with a great work of art that is supposed to resemble you. Sometimes the result is less than flattering. (Laughter) It actually doesn't look anything like me, so I'm going to try this one more time. Oh, I'm liking this one. This is a portrait of Thirza Whysall from the collection of the Royal College of Music painted by Elizabeth Robinson McCallum. She does have kind of wavy blonde hair. She's playing a violin. So there you go, my inner artiste is coming out. (Laughter) OK, so that's how the app is supposed to work. Here's the thing - if I were sitting in Texas or in Illinois, I couldn't do this. The app doesn't work. Law professor Matthew Kugler of Northwestern University has studied Illinois' biometric privacy laws, and he joins us now. Welcome. MATTHEW KUGLER: Thank you. Thank you. KELLY: You are in Chicago right now. Explain for us what happens if you try to open the app and use the art selfie feature. KUGLER: So what people have been discovering - and this was not publicized, they've been discovering it as they attempt to do it - is that the option is simply not available. And what has become apparent over the last two days in particular is that Google has intentionally disabled this functionality in Illinois and in Texas in response to the biometric privacy laws. KELLY: Google has not actually put out a statement one way or the other. We're reaching out to them to see what their take on this is. But what you're describing there is biometric law in Illinois that restricts the amount of information that tech companies can collect from users like you or me. KUGLER: Yes. So in these two states, if you want to collect those biometrics you have to give certain disclosures to people telling them what you're going to do with the information. You have to get their permission for a variety of things. And even with that there are restrictions on how you can share the information and whether you can sell it. KELLY: Now, in this case, we looked. Google does have a disclaimer on the app. It says - and I'll quote it - \"Google won't use data from your photo for any other purpose and will only store your photo for the time it takes to search for matches. \" And it makes you choose either I accept or, you know, don't accept and then you don't proceed. So with that disclaimer, how does the app not satisfy Illinois legal requirements? KUGLER: My impression is that Google is showing an abundance of caution here. The law is still being litigated in a number of courts. Google is involved in litigation involving the law. And they may be concerned that someone might upload someone else's picture and therefore they don't have the consent of the person whose image is being captured. That's a problem with home security cameras. And there's a Nest product not available in Illinois as far as we can tell because of this concern. KELLY: Nest, we should mention, is another technology company, uses biometric data for different applications. And it's owned by the same parent company as Google. KUGLER: Yes. So I'm wondering if there's perhaps some Google-wide understanding of caution, though obviously Google has not been forthcoming explaining exactly why it's doing some of these things. KELLY: Why was this law passed in Illinois in the first place? This was to protect privacy rights of residents there. KUGLER: Yes. There was a company going through bankruptcy in Illinois that had a fair bit of biometric data. And as it was going through bankruptcy, there was concern that the data had been collected when consumers had certain impressions of how it was going to be used. And in bankruptcy, that data might be sold to someone else who has completely different intentions. So maybe you trust Google with certain data, but for Google to hand that data off to someone else you wouldn't trust them. So this law was passed in part to avoid that kind of unintended consequence. KELLY: And meanwhile, to loop us back to where we began, are people in Illinois finding ways to circumvent this law? KUGLER: Several of my friends have commented that when they travel out of state they're going to make a point to stop by the side of the road and make use of this feature (laughter). How much they care is unclear. This is perhaps the flavor of the moment, and in a week we'll all wonder why we cared. KELLY: We'll all be moving on to being agitated about something else. Matthew Kugler, thanks very much. KUGLER: Thank you. KELLY: That's Matthew Kugler of the Pritzker School of Law at Northwestern University.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-18-578800271": {"title": "Finding Your Lost Bitcoins : NPR", "url": "https://www.npr.org/2018/01/18/578800271/finding-your-lost-bitcoins", "author": "No author found", "published_date": "2018-01-18", "content": "RACHEL MARTIN, HOST: As the digital currency bitcoin has skyrocketed in value, many of the early adopters have become millionaires - only if, that is, they can find their bitcoins. Kenny Malone from our Planet Money team went on a virtual treasure hunt. KENNY MALONE, BYLINE: Syl Turner heard about bitcoin about 10 years ago, and he figured, what the heck, got a couple of coins and then saved them to what's known as a digital wallet on his hard drive. SYL TURNER: You know, at the time I didn't think bitcoin was worth anything so I didn't (laughter), back anything up. MALONE: To be fair, it wasn't worth anything at the time. Turner had digital pennies and then let them fall between the digital couch cushions. And a decade later, those pennies were worth $25,000. And so Turner is standing in his attic, staring at a waist-high layer of junk, looking for a 10-year-old hard drive. So let's dig in. Let's do this. TURNER: Yeah. Yeah, we can start digging in. MALONE: I think. . . TURNER: All right. There's, like, a broken fog machine. MALONE: Some kind of table saw thing, paper shredder, Game Boy Advance. TURNER: Baby seat. MALONE: Two TVs that are identical, Goldfish crackers box. While Turner rummages, we found an expert specifically in lost bitcoin. What do people actually say? Bitcoin? Bitcoins? JONATHAN LEVIN: You can have bitcoins, and bitcoin is, like, the currency unit. MALONE: OK. Jonathan Levin is co-founder of Chainalysis, a company that makes bitcoin analysis software, and the company did a study that found around 20 percent of all bitcoin is lost, out of circulation. By today's valuation, that's more than $25 billion. And Levin says the confusing thing is that, technically speaking, bitcoin can't be lost. LEVIN: Yeah. So if I send you bitcoin, I send bitcoin to your account on the bitcoin blockchain. MALONE: Blockchain - big word, sure, but here's a very non-technical way of understanding what it means. Imagine with me, if you will, a massive auditorium filled with bitcoin bookkeepers. Now, Jonathan wants to send me one bitcoin. He walks on stage in front of all these bookkeepers, steps up to a microphone, and he's like. . . LEVIN: (Clearing throat). Hello, entire bitcoin universe. As all of your books show, I, Jonathan, have three bitcoins to my name. MALONE: Of course, it would not be a name. It would be an anonymized account number. LEVIN: I would like everyone here to know that I am giving one bitcoin to Kenny. MALONE: After Jonathan says this, all of these virtual ledger keepers sort of scribble this transaction down, deduct one bitcoin from Jonathan's account, increase Kenny's account by one, and that is a bitcoin transaction. Nothing is really transferred. It's more like an instantaneous adjustment across a whole bunch of ledgers. When people talk about the blockchain, they're talking about this system where there is no central bookkeeper. TURNER: Let's see. Stepped on something. MALONE: Syl Turner, still digging through his attic. TURNER: Here's a frisbee. MALONE: Whatever bitcoins Syl Turner has, they're still there sitting in a kind of virtual vault, but he has lost the key. He's looking for an old hard drive with a super complicated password, and it's not up here. TURNER: I don't think I put it in this one. MALONE: Bitcoin was created to be this decentralized currency. There's no federal reserve tweaking interest rates, there's no treasury department printing new money and there's no customer service line to call if you are Syl Turner. But that lost bitcoin expert Jonathan Levin, he does have a phone number. (SOUNDBITE OF PHONE LINE RINGING)MALONE: Can you guys all hear each other OK? TURNER: Yup. LEVIN: Yeah. MALONE: So we have gathered here today to try and resurrect Syl's lost bitcoin. And I put Jonathan Levin on the phone with Syl Turner to see if there was any way to salvage Turner's bitcoin. TURNER: All right. So probably 2009 or 2010, somewhere around there. . . MALONE: Turner explained how he knows he's got tens of thousands of dollars of bitcoins locked away, he just cannot find that hard drive with the digital wallet with the private key. TURNER: So that's where I'm at. LEVIN: OK. Well, what we need to find the needle in the haystack is even, like, a little bit of a key. MALONE: What Levin is saying is that bitcoin private keys are designed to be un-guessable even by the most powerful computers we have right now. But if Turner happened to write down part of this key at some point, there are companies that will use that information to help him break into his account. TURNER: So there are bitcoin bounty hunters out there. LEVIN: There are bitcoin bounty hunters out there. MALONE: But, like lots and lots of other early bitcoin enthusiasts, Syl Turner did not write down any part of the key and has nothing to go on. LEVIN: Yeah. Unfortunately - unfortunately we can't actually help you out. MALONE: I don't know. What kind of things do you say to somebody in Syl's position? LEVIN: For the people that have lost their bitcoins, I say tough luck. (Laughter). TURNER: I know this is going to be, like, my deathbed regret. I'm like, I should have backed up that wallet. MALONE: Kenny Malone, NPR News. RACHEL MARTIN, HOST:  As the digital currency bitcoin has skyrocketed in value, many of the early adopters have become millionaires - only if, that is, they can find their bitcoins. Kenny Malone from our Planet Money team went on a virtual treasure hunt. KENNY MALONE, BYLINE: Syl Turner heard about bitcoin about 10 years ago, and he figured, what the heck, got a couple of coins and then saved them to what's known as a digital wallet on his hard drive. SYL TURNER: You know, at the time I didn't think bitcoin was worth anything so I didn't (laughter), back anything up. MALONE: To be fair, it wasn't worth anything at the time. Turner had digital pennies and then let them fall between the digital couch cushions. And a decade later, those pennies were worth $25,000. And so Turner is standing in his attic, staring at a waist-high layer of junk, looking for a 10-year-old hard drive. So let's dig in. Let's do this. TURNER: Yeah. Yeah, we can start digging in. MALONE: I think. . . TURNER: All right. There's, like, a broken fog machine. MALONE: Some kind of table saw thing, paper shredder, Game Boy Advance. TURNER: Baby seat. MALONE: Two TVs that are identical, Goldfish crackers box. While Turner rummages, we found an expert specifically in lost bitcoin. What do people actually say? Bitcoin? Bitcoins? JONATHAN LEVIN: You can have bitcoins, and bitcoin is, like, the currency unit. MALONE: OK. Jonathan Levin is co-founder of Chainalysis, a company that makes bitcoin analysis software, and the company did a study that found around 20 percent of all bitcoin is lost, out of circulation. By today's valuation, that's more than $25 billion. And Levin says the confusing thing is that, technically speaking, bitcoin can't be lost. LEVIN: Yeah. So if I send you bitcoin, I send bitcoin to your account on the bitcoin blockchain. MALONE: Blockchain - big word, sure, but here's a very non-technical way of understanding what it means. Imagine with me, if you will, a massive auditorium filled with bitcoin bookkeepers. Now, Jonathan wants to send me one bitcoin. He walks on stage in front of all these bookkeepers, steps up to a microphone, and he's like. . . LEVIN: (Clearing throat). Hello, entire bitcoin universe. As all of your books show, I, Jonathan, have three bitcoins to my name. MALONE: Of course, it would not be a name. It would be an anonymized account number. LEVIN: I would like everyone here to know that I am giving one bitcoin to Kenny. MALONE: After Jonathan says this, all of these virtual ledger keepers sort of scribble this transaction down, deduct one bitcoin from Jonathan's account, increase Kenny's account by one, and that is a bitcoin transaction. Nothing is really transferred. It's more like an instantaneous adjustment across a whole bunch of ledgers. When people talk about the blockchain, they're talking about this system where there is no central bookkeeper. TURNER: Let's see. Stepped on something. MALONE: Syl Turner, still digging through his attic. TURNER: Here's a frisbee. MALONE: Whatever bitcoins Syl Turner has, they're still there sitting in a kind of virtual vault, but he has lost the key. He's looking for an old hard drive with a super complicated password, and it's not up here. TURNER: I don't think I put it in this one. MALONE: Bitcoin was created to be this decentralized currency. There's no federal reserve tweaking interest rates, there's no treasury department printing new money and there's no customer service line to call if you are Syl Turner. But that lost bitcoin expert Jonathan Levin, he does have a phone number. (SOUNDBITE OF PHONE LINE RINGING) MALONE: Can you guys all hear each other OK? TURNER: Yup. LEVIN: Yeah. MALONE: So we have gathered here today to try and resurrect Syl's lost bitcoin. And I put Jonathan Levin on the phone with Syl Turner to see if there was any way to salvage Turner's bitcoin. TURNER: All right. So probably 2009 or 2010, somewhere around there. . . MALONE: Turner explained how he knows he's got tens of thousands of dollars of bitcoins locked away, he just cannot find that hard drive with the digital wallet with the private key. TURNER: So that's where I'm at. LEVIN: OK. Well, what we need to find the needle in the haystack is even, like, a little bit of a key. MALONE: What Levin is saying is that bitcoin private keys are designed to be un-guessable even by the most powerful computers we have right now. But if Turner happened to write down part of this key at some point, there are companies that will use that information to help him break into his account. TURNER: So there are bitcoin bounty hunters out there. LEVIN: There are bitcoin bounty hunters out there. MALONE: But, like lots and lots of other early bitcoin enthusiasts, Syl Turner did not write down any part of the key and has nothing to go on. LEVIN: Yeah. Unfortunately - unfortunately we can't actually help you out. MALONE: I don't know. What kind of things do you say to somebody in Syl's position? LEVIN: For the people that have lost their bitcoins, I say tough luck. (Laughter). TURNER: I know this is going to be, like, my deathbed regret. I'm like, I should have backed up that wallet. MALONE: Kenny Malone, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-22-579787718": {"title": "Tech Companies Working On Fixes For Fake News As Midterms Approach : NPR", "url": "https://www.npr.org/2018/01/22/579787718/tech-companies-working-on-fixes-for-fake-news-as-midterms-approach", "author": "No author found", "published_date": "2018-01-22", "content": "MARY LOUISE KELLY, HOST: Now it's time for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")KELLY: Hate speech, terrorism, foreign interference in American democracy, fake news - we have spent many hours talking about what went wrong online during the 2016 presidential election. Well, tech companies are working hard to avoid a repeat as the 2018 midterm elections approach. They're fast upon us. And here to tell us what Facebook, Google and Twitter are doing is NPR tech correspondent Aarti Shahani. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. KELLY: Let's talk first about Facebook. It took a long time for CEO Mark Zuckerberg to acknowledge that Facebook was a platform for fake news, fake ads during the 2016 election. What are they doing to get ready for this coming November? SHAHANI: Well, none of the tech giants claims to be ready, OK? They each have a version of, we're getting there but only for ads explicitly about candidates like Donald Trump or Hillary Clinton, not ads about issues like DACA or global warming. Facebook says it's creating a new documentation process where advertisers may be required to verify who they are and where they are. KELLY: And then I suppose there's the process of verifying that they're telling the truth, which is another layer of this. SHAHANI: That's right. You're getting it, exactly. KELLY: Yeah, well, let me move you to Google and YouTube, we should mention, as part of Google. What changes are they making? SHAHANI: So Google does not yet have a working definition of what counts as an election ad. They're working on that. And what each company is desperately trying to stave off is any kind of regulation. They don't want politicians putting rules on them like legacy media has. KELLY: What about, Aarti, the favored platform of our tweeter in chief? I'm talking of course about Twitter. Their latest, I was interested to read, is Twitter is reaching out to something like close to 700,000 people here in the U. S. , letting them know if they maybe were shown political ads on their Twitter feed that may have had some kind of link to Russia. SHAHANI: Yeah. And you know, Twitter is much smaller than Google and Facebook. And a spokesperson at Twitter tells me their investigation into the 2016 election mishap is setting back their efforts to fix the broken advertising tools in time for the midterms. They're concerned about that. And now, you know, each company has a fundamental tension. By way of analogy, let's take a nightclub. The official policy would be no one under 21 allowed. But you don't want to check ID because you're afraid that'll slow down the line and you'll lose customers, so you try to figure some workaround, some other way to do it. Now, when I shared that (laughter) specific analogy with Twitter, got to say, the spokesperson laughed very hard and told me, these are exactly the questions we're asking. KELLY: One related thing to this - I wanted to follow up on my question about Facebook, which is, Facebook last week said it's going to start picking and choosing what news to show us, what news we're going to see. This is related to the quest to show us less fake news. SHAHANI: Yeah. And you know, CEO Mark Zuckerberg - he posted on his Facebook page that the company is going to start showing more news from sources that are high-quality, in his words, and push down stuff that purports to be news but may be lower-quality. This is the single biggest announcement the company has made directly in response to the fake news controversy. And you know, it is important to point out that many experts say that this is how a powerful tech giant censors speech on the Internet. Now, granted, Facebook is not zapping away articles. What they're doing in an attention economy - right? - it's about a competition for people's eyeballs here - is that the company censors by pushing things so far down a bottomless feed that eyeballs never get to see it. KELLY: Which of course prompts the question of how Facebook is going to decide what counts as high-quality news and what's low-quality news. SHAHANI: They're going to let the crowd decide. They're asking Facebook users to vote on it. KELLY: What's been the reaction from mainstream news media to that? SHAHANI: Well, veteran news leaders are, to put it mildly, horrified that Facebook would take an important decision like that and throw it to a popular vote. There's another kind of criticism among conservatives who are concerned that liberal Facebook will push down right-wing content. And meanwhile, Facebook has decided they're not offering interviews to talk about the rationale for their move. KELLY: NPR's Aarti Shahani - thank you, Aarti. SHAHANI: Thank you. (SOUNDBITE OF MINOTAUR SHOCK'S \"MY BURR\") MARY LOUISE KELLY, HOST:  Now it's time for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") KELLY: Hate speech, terrorism, foreign interference in American democracy, fake news - we have spent many hours talking about what went wrong online during the 2016 presidential election. Well, tech companies are working hard to avoid a repeat as the 2018 midterm elections approach. They're fast upon us. And here to tell us what Facebook, Google and Twitter are doing is NPR tech correspondent Aarti Shahani. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. KELLY: Let's talk first about Facebook. It took a long time for CEO Mark Zuckerberg to acknowledge that Facebook was a platform for fake news, fake ads during the 2016 election. What are they doing to get ready for this coming November? SHAHANI: Well, none of the tech giants claims to be ready, OK? They each have a version of, we're getting there but only for ads explicitly about candidates like Donald Trump or Hillary Clinton, not ads about issues like DACA or global warming. Facebook says it's creating a new documentation process where advertisers may be required to verify who they are and where they are. KELLY: And then I suppose there's the process of verifying that they're telling the truth, which is another layer of this. SHAHANI: That's right. You're getting it, exactly. KELLY: Yeah, well, let me move you to Google and YouTube, we should mention, as part of Google. What changes are they making? SHAHANI: So Google does not yet have a working definition of what counts as an election ad. They're working on that. And what each company is desperately trying to stave off is any kind of regulation. They don't want politicians putting rules on them like legacy media has. KELLY: What about, Aarti, the favored platform of our tweeter in chief? I'm talking of course about Twitter. Their latest, I was interested to read, is Twitter is reaching out to something like close to 700,000 people here in the U. S. , letting them know if they maybe were shown political ads on their Twitter feed that may have had some kind of link to Russia. SHAHANI: Yeah. And you know, Twitter is much smaller than Google and Facebook. And a spokesperson at Twitter tells me their investigation into the 2016 election mishap is setting back their efforts to fix the broken advertising tools in time for the midterms. They're concerned about that. And now, you know, each company has a fundamental tension. By way of analogy, let's take a nightclub. The official policy would be no one under 21 allowed. But you don't want to check ID because you're afraid that'll slow down the line and you'll lose customers, so you try to figure some workaround, some other way to do it. Now, when I shared that (laughter) specific analogy with Twitter, got to say, the spokesperson laughed very hard and told me, these are exactly the questions we're asking. KELLY: One related thing to this - I wanted to follow up on my question about Facebook, which is, Facebook last week said it's going to start picking and choosing what news to show us, what news we're going to see. This is related to the quest to show us less fake news. SHAHANI: Yeah. And you know, CEO Mark Zuckerberg - he posted on his Facebook page that the company is going to start showing more news from sources that are high-quality, in his words, and push down stuff that purports to be news but may be lower-quality. This is the single biggest announcement the company has made directly in response to the fake news controversy. And you know, it is important to point out that many experts say that this is how a powerful tech giant censors speech on the Internet. Now, granted, Facebook is not zapping away articles. What they're doing in an attention economy - right? - it's about a competition for people's eyeballs here - is that the company censors by pushing things so far down a bottomless feed that eyeballs never get to see it. KELLY: Which of course prompts the question of how Facebook is going to decide what counts as high-quality news and what's low-quality news. SHAHANI: They're going to let the crowd decide. They're asking Facebook users to vote on it. KELLY: What's been the reaction from mainstream news media to that? SHAHANI: Well, veteran news leaders are, to put it mildly, horrified that Facebook would take an important decision like that and throw it to a popular vote. There's another kind of criticism among conservatives who are concerned that liberal Facebook will push down right-wing content. And meanwhile, Facebook has decided they're not offering interviews to talk about the rationale for their move. KELLY: NPR's Aarti Shahani - thank you, Aarti. SHAHANI: Thank you. (SOUNDBITE OF MINOTAUR SHOCK'S \"MY BURR\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-25-579160550": {"title": "Warehouse Robots: For Many Workers, Automation Seems A Distant Threat : NPR", "url": "https://www.npr.org/2018/01/25/579160550/don-t-think-a-robot-could-do-this-warehouse-workers-aren-t-worried-for-their-job", "author": "No author found", "published_date": "2018-01-25", "content": "DAVID GREENE, HOST: This week, we have been exploring the realities of modern work. A new NPR/Marist poll found a lot of confidence in the future among American workers. A vast majority said they actually don't feel that threatened by the economic forces we hear so much about, like automation. NPR's Alina Selyukh talked to some people who could be affected by it. (SOUNDBITE OF WHEELS SQUEAKING)CHRIS BEATTY: Next aisle. ALINA SELYUKH, BYLINE: When you go shopping online, chances are whatever you bought has been in a warehouse cart sort of like this one - tall metal shelves on wheels. BEATTY: Seventeen D (ph), one piece, slot one. SELYUKH: Chris Beatty is filling this cart in a warehouse in southern New Jersey that handles some of the top cosmetic brands. Today, Beatty is picking. He rolls the cart through aisles, finds the creams or lipsticks people bought and drops them into slots on his cart to fill online orders. He is definitely humming while he works. BEATTY: (Humming). SELYUKH: Beatty's 26, a lean guy in jeans, a sweatshirt and a knit cap. He's been working in warehouses for a few years now. He says his father got him into it. He used to operate a forklift. BEATTY: He just told me, hey, look, just go with the flow. If they need help, go help them, you know? And that's what I did. I - anytime they needed help, I helped them. Anytime - overtime - I stayed. So that's how I got into warehouses. Warehouse work is pretty fun. SELYUKH: Right now, Beatty works for a company called Radial. They're pretty new, and they're not a giant operation like Amazon or Walmart. But if you ask most labor economists, they'll tell you automation is coming fast to the whole industry, just like in manufacturing years back. Do you think your job could be done by a robot? BEATTY: That's a tough one, but I don't think a robot could do this. SELYUKH: Why not? BEATTY: It. . . SELYUKH: Or you just don't want it to do the job. BEATTY: Nah, nah. SELYUKH: (Laughter). BEATTY: I love my job too much. SELYUKH: I heard this kind of optimism in conversations with other Radial workers and with some who worked for Amazon, though Amazon workers spoke off the record to comply with corporate nondisclosure policies. Neither group was particularly worried about robots, and their confidence aligns with a new NPR/Marist poll, which found 94 percent of U. S. workers - almost all of them - say it's unlikely they will lose their jobs to automation. MARC MUNN: There's a lot of jobs in here that could be taken over by machines. But who's going to run the building if the machines are in here? SELYUKH: Marc Munn is a manager at Radial. Beatty works in his department. He told me he felt safe about his job because he is a senior manager who helps keep this place running. MUNN: And you still need someone to come in here, open it up. You still need someone to oversee it. If something breaks and there's a machine running in here, I don't think we're going to have other machines in here to fix that, so that's where my job comes into play. BIBIANA RAMOS: I know there is machines that make the boxes but not this kind of boxes. SELYUKH: Bibiana Ramos is a packer. She carefully folds nice tissue paper inside a special box. RAMOS: I think our customers - they like their products to look nice and presentable. SELYUKH: Basically, you're saying you can make it look good in a way that a machine can't. RAMOS: Right. Yes. SELYUKH: All this illustrates the complexity behind the buzzword automation. For now, warehouses are hiring a lot to keep up with our online shopping boom. But people studying the field point to Amazon's investment in thousands of robots as a sign of things to come. For now, smaller competitors like Radial can't spend that kind of money. Plus, the machines aren't that smart yet. At Radial, one item of automation is a conveyor belt that sorts boxes by shipping type. But it can't process something small like an envelope, so that part of someone's job. In this case, it's Kyle Niver who is scanning the envelopes manually. Do you think your job could be done by a machine? KYLE NIVER: Yes. I worked in a place that does this kind of stuff. They build machines for this. So yes, I do feel like that could be taken away. SELYUKH: So they build sorting machines. NIVER: They build sorting machines and picking machines. SELYUKH: What the companies tend to say about automation is, yes, the robots are coming, but they won't completely replace human dexterity and versatility. The men who run this Radial warehouse told me they definitely didn't see robots taking over in the next five years. After that, it's hard to predict. When I caught up again with Beatty, I told him about the Amazon robots that automate the very job he's doing today. Instead of workers walking the aisles to find products on shelves, Amazon's machines bring the shelves to the workers. BEATTY: That would be pretty cool to see a robot bring some of your work to you, but I'm a hands-on guy. I like to do my own stuff. If it - if they come, they come, you know? There's nothing we could do about it. We just have to keep on doing what we do. SELYUKH: Beatty says he and his father have talked about the future and automation a few times, but he says he just can't worry about that for now. Alina Selyukh, NPR News Burlington, N. J. (SOUNDBITE OF VULFPECK'S \"CENTERING FUGUE\") DAVID GREENE, HOST:  This week, we have been exploring the realities of modern work. A new NPR/Marist poll found a lot of confidence in the future among American workers. A vast majority said they actually don't feel that threatened by the economic forces we hear so much about, like automation. NPR's Alina Selyukh talked to some people who could be affected by it. (SOUNDBITE OF WHEELS SQUEAKING) CHRIS BEATTY: Next aisle. ALINA SELYUKH, BYLINE: When you go shopping online, chances are whatever you bought has been in a warehouse cart sort of like this one - tall metal shelves on wheels. BEATTY: Seventeen D (ph), one piece, slot one. SELYUKH: Chris Beatty is filling this cart in a warehouse in southern New Jersey that handles some of the top cosmetic brands. Today, Beatty is picking. He rolls the cart through aisles, finds the creams or lipsticks people bought and drops them into slots on his cart to fill online orders. He is definitely humming while he works. BEATTY: (Humming). SELYUKH: Beatty's 26, a lean guy in jeans, a sweatshirt and a knit cap. He's been working in warehouses for a few years now. He says his father got him into it. He used to operate a forklift. BEATTY: He just told me, hey, look, just go with the flow. If they need help, go help them, you know? And that's what I did. I - anytime they needed help, I helped them. Anytime - overtime - I stayed. So that's how I got into warehouses. Warehouse work is pretty fun. SELYUKH: Right now, Beatty works for a company called Radial. They're pretty new, and they're not a giant operation like Amazon or Walmart. But if you ask most labor economists, they'll tell you automation is coming fast to the whole industry, just like in manufacturing years back. Do you think your job could be done by a robot? BEATTY: That's a tough one, but I don't think a robot could do this. SELYUKH: Why not? BEATTY: It. . . SELYUKH: Or you just don't want it to do the job. BEATTY: Nah, nah. SELYUKH: (Laughter). BEATTY: I love my job too much. SELYUKH: I heard this kind of optimism in conversations with other Radial workers and with some who worked for Amazon, though Amazon workers spoke off the record to comply with corporate nondisclosure policies. Neither group was particularly worried about robots, and their confidence aligns with a new NPR/Marist poll, which found 94 percent of U. S. workers - almost all of them - say it's unlikely they will lose their jobs to automation. MARC MUNN: There's a lot of jobs in here that could be taken over by machines. But who's going to run the building if the machines are in here? SELYUKH: Marc Munn is a manager at Radial. Beatty works in his department. He told me he felt safe about his job because he is a senior manager who helps keep this place running. MUNN: And you still need someone to come in here, open it up. You still need someone to oversee it. If something breaks and there's a machine running in here, I don't think we're going to have other machines in here to fix that, so that's where my job comes into play. BIBIANA RAMOS: I know there is machines that make the boxes but not this kind of boxes. SELYUKH: Bibiana Ramos is a packer. She carefully folds nice tissue paper inside a special box. RAMOS: I think our customers - they like their products to look nice and presentable. SELYUKH: Basically, you're saying you can make it look good in a way that a machine can't. RAMOS: Right. Yes. SELYUKH: All this illustrates the complexity behind the buzzword automation. For now, warehouses are hiring a lot to keep up with our online shopping boom. But people studying the field point to Amazon's investment in thousands of robots as a sign of things to come. For now, smaller competitors like Radial can't spend that kind of money. Plus, the machines aren't that smart yet. At Radial, one item of automation is a conveyor belt that sorts boxes by shipping type. But it can't process something small like an envelope, so that part of someone's job. In this case, it's Kyle Niver who is scanning the envelopes manually. Do you think your job could be done by a machine? KYLE NIVER: Yes. I worked in a place that does this kind of stuff. They build machines for this. So yes, I do feel like that could be taken away. SELYUKH: So they build sorting machines. NIVER: They build sorting machines and picking machines. SELYUKH: What the companies tend to say about automation is, yes, the robots are coming, but they won't completely replace human dexterity and versatility. The men who run this Radial warehouse told me they definitely didn't see robots taking over in the next five years. After that, it's hard to predict. When I caught up again with Beatty, I told him about the Amazon robots that automate the very job he's doing today. Instead of workers walking the aisles to find products on shelves, Amazon's machines bring the shelves to the workers. BEATTY: That would be pretty cool to see a robot bring some of your work to you, but I'm a hands-on guy. I like to do my own stuff. If it - if they come, they come, you know? There's nothing we could do about it. We just have to keep on doing what we do. SELYUKH: Beatty says he and his father have talked about the future and automation a few times, but he says he just can't worry about that for now. Alina Selyukh, NPR News Burlington, N. J. (SOUNDBITE OF VULFPECK'S \"CENTERING FUGUE\")", "section": "The Rise Of The Contract Workers", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-25-580263402": {"title": "Spotify's Unusual IPO Model Will Test The Company's Strength : NPR", "url": "https://www.npr.org/2018/01/25/580263402/spotifys-unusual-ipo-model-will-test-the-companys-strength", "author": "No author found", "published_date": "2018-01-25", "content": "DAVID GREENE, HOST: Spotify, the music streaming service, is popular and also unprofitable. And now Spotify is planning a very unconventional initial public offering. Embracing its status as an industry disruptor, Spotify's breaking with convention and opting to launch its IPO without the help of an investment bank. As Charles Lane from member station WSHU reports, not everyone is convinced this is such a good idea. CHARLES LANE, BYLINE: Typically how IPOs work is the company going public hires a big investment banker, several of them, to both issue new shares and then go out and sell them. But Spotify plans to simply list its shares on the market and then let them trade. George Parker is a finance professor at Stanford. He says this is almost unheard of. GEORGE PARKER: To me, that is very analogous to a person that puts a sign out on the street and says this property is for sale by owner. LANE: It's like saying, I got the coolest house on the block, everybody will want to buy it - so why give a cut to a broker? PARKER: Spotify, by doing this, is very confident that the public already understands Spotify's value and that it does not need others to tell the story. LANE: Some analysts estimate this could save the company as much as $300 dollars in fees. Also Spotify's current private investors can simply cash out without waiting for the traditional lock-in period to end. From this perspective, a direct listing is just a more efficient way to IPO. KATHLEEN SMITH: We don't think this is at all another way to go public. It's an inferior way, a defensive way to come out into the public market. LANE: Kathleen Smith is founder of Renaissance Capital. She says in 2016, Spotify got an unusual loan from a group of private investment firms, including Goldman Sachs. The investors demanded a number of conditions to the loan. SMITH: Suggesting that the investors thought the company's private valuation was way too high. LANE: Spotify boasts a 140 million users, but most of them don't pay. They listen to the ad-supported stream, and ad revenue is only $300 million a year, a fraction of what the service truly costs. Spotify recently announced plans to move more aggressively into podcasting and multimedia news, a space where ad revenue may be more lucrative. Jake Shapiro, CEO at the podcasting platform RadioPublic, says global podcasting ad revenue is about $250 million a year. JAKE SHAPIRO: But by all measures it's growing by leaps and bounds, and we anticipate doubling and tripling or more of that revenue in the coming months and years. LANE: So if Spotify's plans are successful, it would be positioned to take advantage of that growth. But right now Spotify is still an unprofitable company pushed towards an IPO by private equity firms eager to cash out. Smith says mom and pop investors won't buy it. SMITH: Investors have been more cautious about companies that don't make money. LANE: Spotify's IPO is scheduled for late March or early April, but many details remain to be worked out. For NPR News, I'm Charles Lane. DAVID GREENE, HOST:  Spotify, the music streaming service, is popular and also unprofitable. And now Spotify is planning a very unconventional initial public offering. Embracing its status as an industry disruptor, Spotify's breaking with convention and opting to launch its IPO without the help of an investment bank. As Charles Lane from member station WSHU reports, not everyone is convinced this is such a good idea. CHARLES LANE, BYLINE: Typically how IPOs work is the company going public hires a big investment banker, several of them, to both issue new shares and then go out and sell them. But Spotify plans to simply list its shares on the market and then let them trade. George Parker is a finance professor at Stanford. He says this is almost unheard of. GEORGE PARKER: To me, that is very analogous to a person that puts a sign out on the street and says this property is for sale by owner. LANE: It's like saying, I got the coolest house on the block, everybody will want to buy it - so why give a cut to a broker? PARKER: Spotify, by doing this, is very confident that the public already understands Spotify's value and that it does not need others to tell the story. LANE: Some analysts estimate this could save the company as much as $300 dollars in fees. Also Spotify's current private investors can simply cash out without waiting for the traditional lock-in period to end. From this perspective, a direct listing is just a more efficient way to IPO. KATHLEEN SMITH: We don't think this is at all another way to go public. It's an inferior way, a defensive way to come out into the public market. LANE: Kathleen Smith is founder of Renaissance Capital. She says in 2016, Spotify got an unusual loan from a group of private investment firms, including Goldman Sachs. The investors demanded a number of conditions to the loan. SMITH: Suggesting that the investors thought the company's private valuation was way too high. LANE: Spotify boasts a 140 million users, but most of them don't pay. They listen to the ad-supported stream, and ad revenue is only $300 million a year, a fraction of what the service truly costs. Spotify recently announced plans to move more aggressively into podcasting and multimedia news, a space where ad revenue may be more lucrative. Jake Shapiro, CEO at the podcasting platform RadioPublic, says global podcasting ad revenue is about $250 million a year. JAKE SHAPIRO: But by all measures it's growing by leaps and bounds, and we anticipate doubling and tripling or more of that revenue in the coming months and years. LANE: So if Spotify's plans are successful, it would be positioned to take advantage of that growth. But right now Spotify is still an unprofitable company pushed towards an IPO by private equity firms eager to cash out. Smith says mom and pop investors won't buy it. SMITH: Investors have been more cautious about companies that don't make money. LANE: Spotify's IPO is scheduled for late March or early April, but many details remain to be worked out. For NPR News, I'm Charles Lane.", "section": "Music", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-26-580619306": {"title": "Mona Chalabi: How Can We Tell The Good Statistics From The Bad Ones? : NPR", "url": "https://www.npr.org/2018/01/26/580619306/mona-chalabi-how-can-we-tell-the-good-statistics-from-the-bad-ones", "author": "No author found", "published_date": "2018-01-26", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, Can We Trust The Numbers? (SOUNDBITE OF MUSIC)RAZ: And if you just watch like an hour of cable news. . . (SOUNDBITE OF NEWS MONTAGE)UNIDENTIFIED REPORTER #1: Important - 228,000 net new jobs were created. UNIDENTIFIED REPORTER #2: That middle fifth gets 1. 5 percent of the total. . . RAZ: . . . You get a lot of numbers thrown at you. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #3: Thirty thousand undocumented immigrants with criminal records released last year. UNIDENTIFIED REPORTER #4: The uninsured rate dropped to below 10 percent, the lowest in history. RAZ: . . . Form a lot of different sources. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #5: Thirty-six percent of Americans think global warming is a serious threat. UNIDENTIFIED REPORTER #6: The top 1 percent gets 8. 5 percent of the benefit. RAZ: And while some people might have too much faith in the numbers, this kind of barrage makes other people really skeptical. MONA CHALABI: So I think the reason why people are more distrustful of statistics is because they feel more alienated by them than they do with other sources of information. They feel that it's beyond them. They feel that they can't do it. And very often, it's just tempting to say, oh, whatever, it's probably a lie. RAZ: This is Mona Chalabi. She's a data editor for The Guardian newspaper. CHALABI: Basically, my job is to take a story and to kind of zoom out and provide context for readers. And the thing that excites me about data is just the scale of it, right? Like, data gives you a scale. It gives you a new frame of understanding. But very often, the way that statistics can be misleading is by simply changing what that context is. And I think people don't necessarily feel equipped to say, I don't really understand where these numbers came from. But they also know that that plays a role. And I think that's part of where the skepticism with statistics comes from. (SOUNDBITE OF MUSIC)RAZ: And, Mona says, we should be skeptical because there are a lot of bad numbers out there. But that means it's kind of up to us to blur how to spot the good statistics from the bad ones. CHALABI: So if you get to the bottom of a piece and you think, I don't really get it - ask questions. Ask how they gathered that data. What is the base number? How many people are we talking about here? For any claim. (SOUNDBITE OF TED TALK)CHALABI: Anyone can do this. You don't have to be a geek or a nerd. You can ignore those words. They're used by people who are trying to say they're smart or pretending they're humble. Absolutely anyone can do this. RAZ: Mona Chalabi picks up the idea from the TED stage. (SOUNDBITE OF TED TALK)CHALABI: So I want to give you guys questions that will help you be able to spot some bad statistics. So can you see uncertainty? Now, one of the things that's really changed people's relationship with numbers - and, in fact, even their trust in the media - has been the use of political polls. Based on national elections in the U. K. , Italy and, of course, the most recent U. S. presidential election, using polls to predict electoral outcomes is about as accurate as using the moon to predict hospital admissions. No, seriously, I used actual data from an academic study to draw this. There are a lot of reasons why polling has become so inaccurate. Our societies have become really diverse, which makes it very difficult for pollsters to get a really nice representative sample of the population for their polls. People are really reluctant to answer their phones to pollsters. And also, shockingly enough, people might lie. But you wouldn't necessarily know that to look at the media. For one thing, the probability of a Hillary Clinton win was communicated with decimal places. How on earth can predicting the behavior of 230 million voters in this country be that precise? And then there were those sleek charts. See, a lot of data visualizations will overstate certainty and it works. These charts can numb our brains to criticism. When you hear a statistic, you might feel skeptical. As soon as it's buried in a chart, it feels like some kind of objective science and it's not. The second question that you guys should be asking yourselves to spot bad numbers is, can I see myself in the data? Because part of the reason why people are so frustrated with these national statistics is they don't really tell the story of who's winning and who's losing from national policy. It's easy to understand why people are frustrated with these global averages when they don't match up with their personal experiences. The point of this isn't necessarily that every single dataset has to relate specifically to you. The point of asking where you fit in is to get as much context as possible. OK. So the final question I want you guys to think about when you're looking at statistics is, how was the data collected? And I know this is tough because methodologies can be opaque and actually kind of boring. But there are still some simple steps you can take to check this. One poll found that 41 percent of Muslims in this country support jihad, which is, obviously, pretty scary. And it was reported everywhere in 2015. Now, when I want to check a number like that, I'll start off by finding the original questionnaire. And it turns out that journalists who reported on that statistic ignored a question lower down on the survey that asked respondents how they defined jihad. And most of them defined it as, quote, \"Muslims' personal, peaceful struggle to be more religious. \" Only 16 percent defined it as violent holy war against unbelievers. Now this is the really important point. Based on those numbers, it's totally possible but no one in the survey who defined it as violent holy war also said they support it. Those two groups might overlap at all. It's also worth asking how the survey was carried out. This was something called an opt-in poll, which means that anyone could have found it on the Internet and completed it. There's no way of knowing if those people really even identified as Muslim. And finally, there were 600 respondents in that poll. There are roughly 3 million Muslims in this country according to Pew Research Center. That means the poll spoke to roughly 1 in every 5,000 Muslims in this country. (SOUNDBITE OF MUSIC)CHALABI: That survey, it was conducted by a polling organization called Woman Trend. And Woman Trend was set up by a woman called Kellyanne Conway, who is now part of the Trump administration. RAZ: Yeah. CHALABI: And so understanding the incentives that people might have behind those data-gathering organizations to reach certain conclusions is really important too. And again, for any statistic it's really important to ask, how was it gathered? RAZ: But, I mean, we're so inundated with so many statistics. I'm not sure if, you know, most of us have the tools or the patience or the context to differentiate between the good ones and the bad ones. CHALABI: It's quite funny. I think people think of numbers as being a very intellectual thing, but there are a lot of feelings wrapped up in it. So, you know, people literally have nightmares about being back in math class. And I think that actually people do have the tools. I haven't met anyone who doesn't have the intelligence to be able to kind of ask these questions. A lot of it is about feeling intimidated by the numbers and feeling like you can't question them. RAZ: So on the one hand, it seems like you're saying, hey, be skeptical. Like, you should not really trust statistics. But on the other hand, you're saying, well, don't count them out either. CHALABI: Yeah, absolutely. Skepticism is an inherently healthy and positive thing to have, but don't use that skepticism to just write numbers off. Just channel that skepticism to ask questions and feel empowered about the numbers that are available you. (SOUNDBITE OF MUSIC)RAZ: That's Mona Chalabi. She's a data editor for The Guardian newspaper and host of the new podcast Strange Bird. You can hear her entire talk at ted. com. GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, Can We Trust The Numbers? (SOUNDBITE OF MUSIC) RAZ: And if you just watch like an hour of cable news. . . (SOUNDBITE OF NEWS MONTAGE) UNIDENTIFIED REPORTER #1: Important - 228,000 net new jobs were created. UNIDENTIFIED REPORTER #2: That middle fifth gets 1. 5 percent of the total. . . RAZ: . . . You get a lot of numbers thrown at you. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #3: Thirty thousand undocumented immigrants with criminal records released last year. UNIDENTIFIED REPORTER #4: The uninsured rate dropped to below 10 percent, the lowest in history. RAZ: . . . Form a lot of different sources. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #5: Thirty-six percent of Americans think global warming is a serious threat. UNIDENTIFIED REPORTER #6: The top 1 percent gets 8. 5 percent of the benefit. RAZ: And while some people might have too much faith in the numbers, this kind of barrage makes other people really skeptical. MONA CHALABI: So I think the reason why people are more distrustful of statistics is because they feel more alienated by them than they do with other sources of information. They feel that it's beyond them. They feel that they can't do it. And very often, it's just tempting to say, oh, whatever, it's probably a lie. RAZ: This is Mona Chalabi. She's a data editor for The Guardian newspaper. CHALABI: Basically, my job is to take a story and to kind of zoom out and provide context for readers. And the thing that excites me about data is just the scale of it, right? Like, data gives you a scale. It gives you a new frame of understanding. But very often, the way that statistics can be misleading is by simply changing what that context is. And I think people don't necessarily feel equipped to say, I don't really understand where these numbers came from. But they also know that that plays a role. And I think that's part of where the skepticism with statistics comes from. (SOUNDBITE OF MUSIC) RAZ: And, Mona says, we should be skeptical because there are a lot of bad numbers out there. But that means it's kind of up to us to blur how to spot the good statistics from the bad ones. CHALABI: So if you get to the bottom of a piece and you think, I don't really get it - ask questions. Ask how they gathered that data. What is the base number? How many people are we talking about here? For any claim. (SOUNDBITE OF TED TALK) CHALABI: Anyone can do this. You don't have to be a geek or a nerd. You can ignore those words. They're used by people who are trying to say they're smart or pretending they're humble. Absolutely anyone can do this. RAZ: Mona Chalabi picks up the idea from the TED stage. (SOUNDBITE OF TED TALK) CHALABI: So I want to give you guys questions that will help you be able to spot some bad statistics. So can you see uncertainty? Now, one of the things that's really changed people's relationship with numbers - and, in fact, even their trust in the media - has been the use of political polls. Based on national elections in the U. K. , Italy and, of course, the most recent U. S. presidential election, using polls to predict electoral outcomes is about as accurate as using the moon to predict hospital admissions. No, seriously, I used actual data from an academic study to draw this. There are a lot of reasons why polling has become so inaccurate. Our societies have become really diverse, which makes it very difficult for pollsters to get a really nice representative sample of the population for their polls. People are really reluctant to answer their phones to pollsters. And also, shockingly enough, people might lie. But you wouldn't necessarily know that to look at the media. For one thing, the probability of a Hillary Clinton win was communicated with decimal places. How on earth can predicting the behavior of 230 million voters in this country be that precise? And then there were those sleek charts. See, a lot of data visualizations will overstate certainty and it works. These charts can numb our brains to criticism. When you hear a statistic, you might feel skeptical. As soon as it's buried in a chart, it feels like some kind of objective science and it's not. The second question that you guys should be asking yourselves to spot bad numbers is, can I see myself in the data? Because part of the reason why people are so frustrated with these national statistics is they don't really tell the story of who's winning and who's losing from national policy. It's easy to understand why people are frustrated with these global averages when they don't match up with their personal experiences. The point of this isn't necessarily that every single dataset has to relate specifically to you. The point of asking where you fit in is to get as much context as possible. OK. So the final question I want you guys to think about when you're looking at statistics is, how was the data collected? And I know this is tough because methodologies can be opaque and actually kind of boring. But there are still some simple steps you can take to check this. One poll found that 41 percent of Muslims in this country support jihad, which is, obviously, pretty scary. And it was reported everywhere in 2015. Now, when I want to check a number like that, I'll start off by finding the original questionnaire. And it turns out that journalists who reported on that statistic ignored a question lower down on the survey that asked respondents how they defined jihad. And most of them defined it as, quote, \"Muslims' personal, peaceful struggle to be more religious. \" Only 16 percent defined it as violent holy war against unbelievers. Now this is the really important point. Based on those numbers, it's totally possible but no one in the survey who defined it as violent holy war also said they support it. Those two groups might overlap at all. It's also worth asking how the survey was carried out. This was something called an opt-in poll, which means that anyone could have found it on the Internet and completed it. There's no way of knowing if those people really even identified as Muslim. And finally, there were 600 respondents in that poll. There are roughly 3 million Muslims in this country according to Pew Research Center. That means the poll spoke to roughly 1 in every 5,000 Muslims in this country. (SOUNDBITE OF MUSIC) CHALABI: That survey, it was conducted by a polling organization called Woman Trend. And Woman Trend was set up by a woman called Kellyanne Conway, who is now part of the Trump administration. RAZ: Yeah. CHALABI: And so understanding the incentives that people might have behind those data-gathering organizations to reach certain conclusions is really important too. And again, for any statistic it's really important to ask, how was it gathered? RAZ: But, I mean, we're so inundated with so many statistics. I'm not sure if, you know, most of us have the tools or the patience or the context to differentiate between the good ones and the bad ones. CHALABI: It's quite funny. I think people think of numbers as being a very intellectual thing, but there are a lot of feelings wrapped up in it. So, you know, people literally have nightmares about being back in math class. And I think that actually people do have the tools. I haven't met anyone who doesn't have the intelligence to be able to kind of ask these questions. A lot of it is about feeling intimidated by the numbers and feeling like you can't question them. RAZ: So on the one hand, it seems like you're saying, hey, be skeptical. Like, you should not really trust statistics. But on the other hand, you're saying, well, don't count them out either. CHALABI: Yeah, absolutely. Skepticism is an inherently healthy and positive thing to have, but don't use that skepticism to just write numbers off. Just channel that skepticism to ask questions and feel empowered about the numbers that are available you. (SOUNDBITE OF MUSIC) RAZ: That's Mona Chalabi. She's a data editor for The Guardian newspaper and host of the new podcast Strange Bird. You can hear her entire talk at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-26-580619944": {"title": "Anne Milgram: How Can Smarter Statistics Help Us Fight Crime? : NPR", "url": "https://www.npr.org/2018/01/26/580619944/anne-milgram-how-can-smarter-statistics-help-us-fight-crime", "author": "No author found", "published_date": "2018-01-26", "content": "GUY RAZ, HOST: So on the show today, we've been asking, Can We Trust The Numbers? And, of course, there are many, many examples when numbers are crucial. When Anne Milgram was just starting out as the attorney general of New Jersey back in 2007. . . ANNE MILGRAM: I was, you know, 35 years old at the time and really eager to, you know, sort of work on high-level criminal justice issues. RAZ: She turned her attention to not only one of the deadliest cities in her state but also in the country - Camden, N. J. MILGRAM: It's a very unique power, but the New Jersey AG is the chief law enforcement officer. And so the AG can take over any police department, any case, any prosecutor's office. And, of course, on day one, I was then, as attorney general, in charge of the most dangerous city in America. RAZ: Wow. So you - when you became attorney general you also became the police chief, basically, for Camden. MILGRAM: Yes, exactly. And I went to this COMSTAT meeting where the senior police commanders come in the room and they are grilled by the leadership of the department on trends in crime, on police responses to crime. And so I went to see this in Camden. And, you know, there was a guy there with a pad of yellow stickies and a black Sharpie marker. The first officer said we had a, you know, robbery last week on such and such a street corner, no suspects. And the guy wrote it on the yellow sticky and put it on the wall. And on the wall was a map of Camden. And so we were sort of loosely mapping where crime was. And then the next senior officer said, you know, we had a homicide, no suspects. And this went on for about two hours. And at the end, we had a wall filled with yellow stickies but virtually no idea of where the next crime would happen or how we could reduce crime. RAZ: Wow. MILGRAM: I saw for the first time that the systemic failure is that without data and without information, a system that's run really subjectively based on our gut and our instinct, we don't know what we're doing. We don't know whether we're doing it well. And we don't know whether or not we can do it better. RAZ: So to radically change things, Anne drew inspiration from an unlikely place, the book and later the movie \"Moneyball. \"(SOUNDBITE OF FILM, \"MONEYBALL\")JONAH HILL: (As Peter Brand) There is an epidemic failure within the game to understand what is really happening. RAZ: Anne decided to \"Moneyball\" the criminal justice system in Camden, N. J. , using data and statistics. (SOUNDBITE OF FILM, \"MONEYBALL\")HILL: (As Peter Brand) And this leads people who run Major League Baseball teams to misjudge their players and mismanage their teams. MILGRAM: When you think about baseball scouts, what they did solely back in the day was they would use their instinct and their judgment. And they would go out and basically say, OK, this guy's going to be great. This guy's not going to be great. And then you have \"Moneyball,\" where you've got the Oakland A's coming in and basically saying, look, when we do the statistical analysis, we find out that what really matters is on-base percentage. That's how teams score runs. That's how teams win. And so in criminal justice, the system is remarkably similar. It is a very subjective system where the police officer, the prosecutor, the judges are making decisions based on their experience and instinct. RAZ: So you decide to do this in New Jersey, in criminal justice. How? What did you do? MILGRAM: We did a number of things. We pulled data to understand where the police officers were being deployed to understand where the crimes were happening. We started to look at who the next shooters were, at - what data and information we had. We put GPS on the police car, so at every moment, we could understand which car was closest to the 911 call. We literally pulled every piece of information that we had access to in the department. And we started to gather new forms of data and information as well and to build systems around that to track patterns and changes in crime. So we would know when a neighborhood was heating up, when it was cooling down and where we should start thinking about - we should be spending our time and our effort. RAZ: Here's more from Anne Milgram on the TED stage. (SOUNDBITE OF TED TALK)MILGRAM: It worked for the Oakland A's, and it worked in the state of New Jersey. We took Camden off the top of the list as the most-dangerous city in America. We reduced murders there by 41 percent, which actually means 37 lives were saved. And we reduced all crime in the city by 26 percent. We also changed the way we did criminal prosecutions. So we went from doing low-level drug crimes that were outside our building to doing cases of statewide importance on things like reducing violence with the most-violent offenders, prosecuting street gangs, gun and drug trafficking and political corruption. RAZ: After Anne finished serving her time as attorney general, she went to work for a foundation to tackle bail reform. And she used the same kind of \"Moneyball\" approach that she used in Camden. (SOUNDBITE OF TED TALK)MILGRAM: We have 12 million arrests every single year. Less than 5 percent of all arrests are for violent crime, yet we spend $75 billion - that's B for billion - dollars a year on state and local corrections costs. Right now today we have 2. 3 million people in our jails and prisons. And we face unbelievable public safety challenges because we have a situation in which two thirds of the people in our jails are there waiting for trial. They're just waiting for their day in court. And 67 percent of people come back. Our recidivism rate is amongst the highest in the world. The reason for this is the way we make decisions. Judges have the best intentions when they make these decisions about risk, but they're making them subjectively. What we need in this space are strong data and analytics. So I went out and built a phenomenal team of data scientists and researchers and statisticians to build a universal risk assessment tool so that every single judge in the United States of America can have an objective, scientific measure of risk. And the tool that we've built, what we did was we collected 1. 5 million cases from all around the United States. And we found that there were nine specific things that mattered all across the country and that were the most highly predictive of risk, things like the defendant's prior convictions, whether they'd been sentenced to incarceration, whether they've engaged in violence before, whether they've even failed to come back to court. And with this tool we can predict three things. First, whether or not someone will commit a new crime if they're released. Second, for the first time, and I think this is incredibly important, we can predict whether someone will commit an act of violence if they're released. And that's the single most important thing that judges say when you talk to them. And third, we can predict whether someone will come back to court. And every single judge in the United States of America can use it because it's been created on a universal data set. (SOUNDBITE OF MUSIC)RAZ: So Anne, we've been talking about the \"Moneyball\" side of this, and \"Moneyball's\" great. Everyone loves \"Moneyball. \" But we haven't talked about the \"Minority Report\" report side of this, which is a little bit darker. Because earlier in the show, we were talking about algorithmic bias, and Joy Buolamwini pointed out, you know, the more sinister side of algorithms, especially in predictive policing. Like, you could say, for example, that, you know, in most American cities, low-level crimes might involve a young man between the ages of 17 and 25, probably a young man of color because men of color are more likely to be arrested than white men doing the same crimes. And you could give that kind of statistic in theory to a police department, and they could say, OK, we've got to focus our efforts on policing 17 to 25-year-old men of color. And that opens up a whole other set of problems. MILGRAM: Yeah. So here's how I would think about it, and I'll come back to the data bias in a moment, but I think the first question is, can we accept the current system as it is? And I would argue that the answer is no. You know, we have the highest rate of incarceration in the world. We spend $280 billion a year. We have 70 or 80 million Americans who now have criminal arrest records. We are not as smart about how we use resources in a cost-effective way, and we are not as fair or equitable. The second point is that we really don't know a lot of what we're doing. This lack of information is simply unacceptable to me. It's really hard for me to understand how criminal justice could be a space where we just leave it to gut and instinct. Now, the bias piece, I am someone who believes that virtually all data is biased, that there is bias in data. The question to me is not whether or not we should use data in criminal justice, it's how we should use data. The standard for technology and for data is not perfection, it's, is it better? It's, can we make an improvement upon our current system? RAZ: I mean, can we get to a place quickly where we can gather enough data, enough statistics to actually create a more just criminal justice system, a full 360 system that actually treats everybody equitably? MILGRAM: We have to. We need to start pulling the data. We need to start understanding what's happening, and we need to start thinking about this. But we have to understand what's happening, who's in our system, what are our outcomes and how do we actually make the public safer? How do we reduce crime? And I think, you know, we're not doing a very good job right now unless we start embracing data and thinking about how and when we use it. (SOUNDBITE OF MUSIC)RAZ: Anne Milgram. She's the former attorney general of New Jersey and now a professor at the NYU School of Law. You can see her full talk at ted. com. (SOUNDBITE OF SONG, \"NUMBERS DON'T LIE\")THE MYNABIRDS: (Singing) Baby, if you want to be right, I will let you be right. I will let you be right. You know that the numbers don't lie. Oh, no, the numbers don't lie. Two wrongs will not make it right. RAZ: Hey, thanks for listening to our show, Can We Trust The Numbers, this week. If you want to find out more about who was on it, go to ted. npr. org. To see hundreds more TED Talks, check out ted. com or the TED app. And you can listen to this show anytime by subscribing to our podcast. You can do it right now on Apple Podcasts or however you get your podcasts. Our production staff at NPR includes Jeff Rogers, Sanaz Meshkinpour, Jinae West, Neva Grant, Rund Abdelfatah, Casey Herman and Rachel Faulkner, with help from Daniel Shukin and Benjamin Klempay. Our intern is Diba Mohtasham. Our partners at TED are Chris Anderson, Colin Helms, Anna Phelan and Janet Lee. If you want to let us know what you think about the show, you can write us at tedradiohour@npr. org. You can tweet us. It's @TEDRadioHour. I'm Guy Raz, and you've been listening to ideas worth spreading, right here on the TED Radio Hour from NPR. (SOUNDBITE OF SONG, \"NUMBERS DON'T LIE\")THE MYNABIRDS: (Singing) Oh, no, the numbers don't lie. GUY RAZ, HOST:  So on the show today, we've been asking, Can We Trust The Numbers? And, of course, there are many, many examples when numbers are crucial. When Anne Milgram was just starting out as the attorney general of New Jersey back in 2007. . . ANNE MILGRAM: I was, you know, 35 years old at the time and really eager to, you know, sort of work on high-level criminal justice issues. RAZ: She turned her attention to not only one of the deadliest cities in her state but also in the country - Camden, N. J. MILGRAM: It's a very unique power, but the New Jersey AG is the chief law enforcement officer. And so the AG can take over any police department, any case, any prosecutor's office. And, of course, on day one, I was then, as attorney general, in charge of the most dangerous city in America. RAZ: Wow. So you - when you became attorney general you also became the police chief, basically, for Camden. MILGRAM: Yes, exactly. And I went to this COMSTAT meeting where the senior police commanders come in the room and they are grilled by the leadership of the department on trends in crime, on police responses to crime. And so I went to see this in Camden. And, you know, there was a guy there with a pad of yellow stickies and a black Sharpie marker. The first officer said we had a, you know, robbery last week on such and such a street corner, no suspects. And the guy wrote it on the yellow sticky and put it on the wall. And on the wall was a map of Camden. And so we were sort of loosely mapping where crime was. And then the next senior officer said, you know, we had a homicide, no suspects. And this went on for about two hours. And at the end, we had a wall filled with yellow stickies but virtually no idea of where the next crime would happen or how we could reduce crime. RAZ: Wow. MILGRAM: I saw for the first time that the systemic failure is that without data and without information, a system that's run really subjectively based on our gut and our instinct, we don't know what we're doing. We don't know whether we're doing it well. And we don't know whether or not we can do it better. RAZ: So to radically change things, Anne drew inspiration from an unlikely place, the book and later the movie \"Moneyball. \" (SOUNDBITE OF FILM, \"MONEYBALL\") JONAH HILL: (As Peter Brand) There is an epidemic failure within the game to understand what is really happening. RAZ: Anne decided to \"Moneyball\" the criminal justice system in Camden, N. J. , using data and statistics. (SOUNDBITE OF FILM, \"MONEYBALL\") HILL: (As Peter Brand) And this leads people who run Major League Baseball teams to misjudge their players and mismanage their teams. MILGRAM: When you think about baseball scouts, what they did solely back in the day was they would use their instinct and their judgment. And they would go out and basically say, OK, this guy's going to be great. This guy's not going to be great. And then you have \"Moneyball,\" where you've got the Oakland A's coming in and basically saying, look, when we do the statistical analysis, we find out that what really matters is on-base percentage. That's how teams score runs. That's how teams win. And so in criminal justice, the system is remarkably similar. It is a very subjective system where the police officer, the prosecutor, the judges are making decisions based on their experience and instinct. RAZ: So you decide to do this in New Jersey, in criminal justice. How? What did you do? MILGRAM: We did a number of things. We pulled data to understand where the police officers were being deployed to understand where the crimes were happening. We started to look at who the next shooters were, at - what data and information we had. We put GPS on the police car, so at every moment, we could understand which car was closest to the 911 call. We literally pulled every piece of information that we had access to in the department. And we started to gather new forms of data and information as well and to build systems around that to track patterns and changes in crime. So we would know when a neighborhood was heating up, when it was cooling down and where we should start thinking about - we should be spending our time and our effort. RAZ: Here's more from Anne Milgram on the TED stage. (SOUNDBITE OF TED TALK) MILGRAM: It worked for the Oakland A's, and it worked in the state of New Jersey. We took Camden off the top of the list as the most-dangerous city in America. We reduced murders there by 41 percent, which actually means 37 lives were saved. And we reduced all crime in the city by 26 percent. We also changed the way we did criminal prosecutions. So we went from doing low-level drug crimes that were outside our building to doing cases of statewide importance on things like reducing violence with the most-violent offenders, prosecuting street gangs, gun and drug trafficking and political corruption. RAZ: After Anne finished serving her time as attorney general, she went to work for a foundation to tackle bail reform. And she used the same kind of \"Moneyball\" approach that she used in Camden. (SOUNDBITE OF TED TALK) MILGRAM: We have 12 million arrests every single year. Less than 5 percent of all arrests are for violent crime, yet we spend $75 billion - that's B for billion - dollars a year on state and local corrections costs. Right now today we have 2. 3 million people in our jails and prisons. And we face unbelievable public safety challenges because we have a situation in which two thirds of the people in our jails are there waiting for trial. They're just waiting for their day in court. And 67 percent of people come back. Our recidivism rate is amongst the highest in the world. The reason for this is the way we make decisions. Judges have the best intentions when they make these decisions about risk, but they're making them subjectively. What we need in this space are strong data and analytics. So I went out and built a phenomenal team of data scientists and researchers and statisticians to build a universal risk assessment tool so that every single judge in the United States of America can have an objective, scientific measure of risk. And the tool that we've built, what we did was we collected 1. 5 million cases from all around the United States. And we found that there were nine specific things that mattered all across the country and that were the most highly predictive of risk, things like the defendant's prior convictions, whether they'd been sentenced to incarceration, whether they've engaged in violence before, whether they've even failed to come back to court. And with this tool we can predict three things. First, whether or not someone will commit a new crime if they're released. Second, for the first time, and I think this is incredibly important, we can predict whether someone will commit an act of violence if they're released. And that's the single most important thing that judges say when you talk to them. And third, we can predict whether someone will come back to court. And every single judge in the United States of America can use it because it's been created on a universal data set. (SOUNDBITE OF MUSIC) RAZ: So Anne, we've been talking about the \"Moneyball\" side of this, and \"Moneyball's\" great. Everyone loves \"Moneyball. \" But we haven't talked about the \"Minority Report\" report side of this, which is a little bit darker. Because earlier in the show, we were talking about algorithmic bias, and Joy Buolamwini pointed out, you know, the more sinister side of algorithms, especially in predictive policing. Like, you could say, for example, that, you know, in most American cities, low-level crimes might involve a young man between the ages of 17 and 25, probably a young man of color because men of color are more likely to be arrested than white men doing the same crimes. And you could give that kind of statistic in theory to a police department, and they could say, OK, we've got to focus our efforts on policing 17 to 25-year-old men of color. And that opens up a whole other set of problems. MILGRAM: Yeah. So here's how I would think about it, and I'll come back to the data bias in a moment, but I think the first question is, can we accept the current system as it is? And I would argue that the answer is no. You know, we have the highest rate of incarceration in the world. We spend $280 billion a year. We have 70 or 80 million Americans who now have criminal arrest records. We are not as smart about how we use resources in a cost-effective way, and we are not as fair or equitable. The second point is that we really don't know a lot of what we're doing. This lack of information is simply unacceptable to me. It's really hard for me to understand how criminal justice could be a space where we just leave it to gut and instinct. Now, the bias piece, I am someone who believes that virtually all data is biased, that there is bias in data. The question to me is not whether or not we should use data in criminal justice, it's how we should use data. The standard for technology and for data is not perfection, it's, is it better? It's, can we make an improvement upon our current system? RAZ: I mean, can we get to a place quickly where we can gather enough data, enough statistics to actually create a more just criminal justice system, a full 360 system that actually treats everybody equitably? MILGRAM: We have to. We need to start pulling the data. We need to start understanding what's happening, and we need to start thinking about this. But we have to understand what's happening, who's in our system, what are our outcomes and how do we actually make the public safer? How do we reduce crime? And I think, you know, we're not doing a very good job right now unless we start embracing data and thinking about how and when we use it. (SOUNDBITE OF MUSIC) RAZ: Anne Milgram. She's the former attorney general of New Jersey and now a professor at the NYU School of Law. You can see her full talk at ted. com. (SOUNDBITE OF SONG, \"NUMBERS DON'T LIE\") THE MYNABIRDS: (Singing) Baby, if you want to be right, I will let you be right. I will let you be right. You know that the numbers don't lie. Oh, no, the numbers don't lie. Two wrongs will not make it right. RAZ: Hey, thanks for listening to our show, Can We Trust The Numbers, this week. If you want to find out more about who was on it, go to ted. npr. org. To see hundreds more TED Talks, check out ted. com or the TED app. And you can listen to this show anytime by subscribing to our podcast. You can do it right now on Apple Podcasts or however you get your podcasts. Our production staff at NPR includes Jeff Rogers, Sanaz Meshkinpour, Jinae West, Neva Grant, Rund Abdelfatah, Casey Herman and Rachel Faulkner, with help from Daniel Shukin and Benjamin Klempay. Our intern is Diba Mohtasham. Our partners at TED are Chris Anderson, Colin Helms, Anna Phelan and Janet Lee. If you want to let us know what you think about the show, you can write us at tedradiohour@npr. org. You can tweet us. It's @TEDRadioHour. I'm Guy Raz, and you've been listening to ideas worth spreading, right here on the TED Radio Hour from NPR. (SOUNDBITE OF SONG, \"NUMBERS DON'T LIE\") THE MYNABIRDS: (Singing) Oh, no, the numbers don't lie.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-26-580619086": {"title": "Joy Buolamwini: How Does Facial Recognition Software See Skin Color? : NPR", "url": "https://www.npr.org/2018/01/26/580619086/joy-buolamwini-how-does-facial-recognition-software-see-skin-color", "author": "No author found", "published_date": "2018-01-26", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today - Can We Trust The Numbers? - ideas about our growing faith in data, algorithms and statistics to predict outcomes. Joy, normally, we have a criteria for people who are on the show. So we're just making a special exception for you. Normally, you have to be the following to be the show - a Rhodes Scholar, a Fulbright Fellow, an Anita Borg scholar, an astronaut scholar. Plus, you also have to win a Nobel Prize. JOY BUOLAMWINI: I fell short? I appreciate the exception, though (laughter). RAZ: This is Joy Buolamwini. And, OK, she might not have a Nobel Prize, but she does have all those other awards. She's also a graduate researcher at the MIT Media Lab. BUOLAMWINI: And I am the founder of the Algorithmic Justice League. So my personal mission is to fight algorithmic bias. RAZ: Yes, the Algorithmic Justice League, which is a group of computer scientists and coders who try to raise awareness about the social problems that exist in algorithms. It's something Joy recently demonstrated by using a basic webcam and facial analysis technology. And it's a kind of technology you might find when you upload a picture on social media. BUOLAMWINI: And - what I do is I sit in front of the camera hoping for my face to be detected. And I have pretty dark skin. So I'm sitting there with my face, dark skin. There's no detection. Then I pull on my friend's face, who has much lighter skin than I do. She's Chinese. And you see that her face is immediately detected. So then I switch back to my face, dark skinned and gorgeous, not detected. I put on a white mask. And after I put on the white mask, that's when I'm detected. And I wanted to show this as an example that in the same conditions - right? - a typically lit office, we were having a different experience. RAZ: So facial recognition software, this is like the stuff that, like, how Facebook and Google know who to tag in photos and stuff. You're saying that a lot of the software doesn't detect black faces? BUOLAMWINI: Absolutely. This is the kind of technology that you're starting to see in things like the iPhone 10 with Face ID. . . RAZ: Oh, yeah. BUOLAMWINI: . . . Of course with Facebook with the auto-tagging and so forth. So the - this kind of technology is being built on machine-learning techniques. And machine-learning techniques are based on data. So if you have biased data in the input and it's not addressed, you're going to have biased outcomes. RAZ: Joy explained more about this from the TED stage. SOUNDBITE OF TED TALK)BUOLAMWINI: Unfortunately, I've run into this issue before. When I was an undergraduate at Georgia Tech studying computer science, I used to work on social robots. And one of my tasks was to get a robot to play peek-a-boo. The problem is, peek-a-boo doesn't really work if I can't see you. And my robot couldn't see me. Not too long after, I was in Hong Kong on a tour of local startups. One of the startups had a social robot. And they decided to do a demo. The demo worked on everybody until it got to me. And you can probably guess it. It couldn't detect my face. So what's going on? Why isn't my face being detected? Well, we have to look at how we give machines sight. Computer vision uses machine-learning techniques to do facial recognition. So how this works is you create a training set with examples of faces. This is a face. This is a face. This is not a face. And over time, you can teach a computer how to recognize other faces. However, if the training sets aren't really that diverse, any face that deviates too much from the established norm will be harder to detect, which is what was happening to me. RAZ: Joy, you gave this talk a couple of years ago, but this problem still exists. BUOLAMWINI: Yes. And it's even more urgent now because there is this assumption that we've arrived. RAZ: Yeah. BUOLAMWINI: So, for example, in 2014, Facebook released a paper called \"DeepFace\" that showed a major breakthrough for facial recognition technology. They achieved 97. 35 percent accuracy on the gold standard benchmark for facial recognition at the time. But we always have to ask with these types of technologies, with AI - who's included? Who's excluded? So now I just told you 97. 35 percent accuracy. RAZ: Sounds great. BUOLAMWINI: Guess what the gender ratio was? RAZ: I don't know, 50/50? BUOLAMWINI: It's a gold standard. You would think 50/50. RAZ: Yeah. BUOLAMWINI: It was 77. 5 percent male. RAZ: Wow. BUOLAMWINI: And then demographic breakdown was - I want to say 80. 5 percent white for this gold standard benchmark. RAZ: Wow. BUOLAMWINI: So now when you know that the gold standard has these skews, when you see something like 97. 35 percent accuracy, we've made a major breakthrough, you start to get a better understanding of exactly which faces - right? - this breakthrough applies to and which ones might not be included. And it's a reflection of people who are in positions of power to mold artificial intelligence. And that's a very limited group right now. (SOUNDBITE OF TED TALK)BUOLAMWINI: Across the U. S. , police departments are starting to use facial recognition software in their crime-fighting arsenal. Georgetown Law published a report showing that 1 in 2 adults in the U. S. - that's 117 million people - have their faces in facial recognition networks. Police departments can currently look at these networks unregulated using algorithms that have not been audited for accuracy. Yet, we know facial recognition is not fail-proof. And labeling faces consistently remains a challenge. You might have seen this on Facebook. My friends and I laugh all the time when we see other people mislabeled in our photos. But misidentifying a suspected criminal is no laughing matter, nor is breaching civil liberties. Law enforcement is also starting to use machine-learning for predictive policing. Some judges use machine-generated risk scores to determine how long an individual is going to spend in prison. So we really have to think about these decisions. Are they fair? And we've seen that algorithmic bias doesn't necessarily always lead to fair outcomes. RAZ: So how do you stop this? I mean, how do you fight algorithmic bias? BUOLAMWINI: So I feel like the minimum thing we can do is actually check for the performance of these systems across groups that we already know have historically been disenfranchised - right? - in the first place. I feel like that's a minimal thing. Then we also need to think about what are steps to take to address the bias as well, right? You know, this is why I think it's critically important we have diverse people participating in the creation of the future. And that means having diverse people shaping the priorities as well as developing the technology. RAZ: So here's what I wonder. Can - I mean, is it possible to create a completely unbiased algorithm? BUOLAMWINI: It depends on what the task is. But I think the question I think about more so is knowing that we are deeply biased, even in our language, our classification systems, et cetera. How do we create systems that work well for humanity? But, also, how do we keep ourselves honest as we're making progress, right? So it's like if you're talking about perfecting a democracy. Will there be a perfect democracy? From what I look and see, probably not, you know. But you're trying to create a more perfect union in some way. So in trying to create more perfect AI, you strive for these ideals of inclusion. You want to mitigate bias, et cetera. But we also have the humility to know that being fallible, being human and being humans who embed our fallibility into the machines we create, we're not necessarily going to be perfect all the time. But we have to try to do our best and continue to improve. And if we exclude people and we limit people's humanity - right? - which is what happens when we have the algorithmic bias that's not addressed, we really limit the potential for all of us in the long run. RAZ: Joy Boulamwini. She's a computer scientist and the founder of the Algorithmic Justice League. You can see her full talk at ted. com. GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today - Can We Trust The Numbers? - ideas about our growing faith in data, algorithms and statistics to predict outcomes. Joy, normally, we have a criteria for people who are on the show. So we're just making a special exception for you. Normally, you have to be the following to be the show - a Rhodes Scholar, a Fulbright Fellow, an Anita Borg scholar, an astronaut scholar. Plus, you also have to win a Nobel Prize. JOY BUOLAMWINI: I fell short? I appreciate the exception, though (laughter). RAZ: This is Joy Buolamwini. And, OK, she might not have a Nobel Prize, but she does have all those other awards. She's also a graduate researcher at the MIT Media Lab. BUOLAMWINI: And I am the founder of the Algorithmic Justice League. So my personal mission is to fight algorithmic bias. RAZ: Yes, the Algorithmic Justice League, which is a group of computer scientists and coders who try to raise awareness about the social problems that exist in algorithms. It's something Joy recently demonstrated by using a basic webcam and facial analysis technology. And it's a kind of technology you might find when you upload a picture on social media. BUOLAMWINI: And - what I do is I sit in front of the camera hoping for my face to be detected. And I have pretty dark skin. So I'm sitting there with my face, dark skin. There's no detection. Then I pull on my friend's face, who has much lighter skin than I do. She's Chinese. And you see that her face is immediately detected. So then I switch back to my face, dark skinned and gorgeous, not detected. I put on a white mask. And after I put on the white mask, that's when I'm detected. And I wanted to show this as an example that in the same conditions - right? - a typically lit office, we were having a different experience. RAZ: So facial recognition software, this is like the stuff that, like, how Facebook and Google know who to tag in photos and stuff. You're saying that a lot of the software doesn't detect black faces? BUOLAMWINI: Absolutely. This is the kind of technology that you're starting to see in things like the iPhone 10 with Face ID. . . RAZ: Oh, yeah. BUOLAMWINI: . . . Of course with Facebook with the auto-tagging and so forth. So the - this kind of technology is being built on machine-learning techniques. And machine-learning techniques are based on data. So if you have biased data in the input and it's not addressed, you're going to have biased outcomes. RAZ: Joy explained more about this from the TED stage. SOUNDBITE OF TED TALK) BUOLAMWINI: Unfortunately, I've run into this issue before. When I was an undergraduate at Georgia Tech studying computer science, I used to work on social robots. And one of my tasks was to get a robot to play peek-a-boo. The problem is, peek-a-boo doesn't really work if I can't see you. And my robot couldn't see me. Not too long after, I was in Hong Kong on a tour of local startups. One of the startups had a social robot. And they decided to do a demo. The demo worked on everybody until it got to me. And you can probably guess it. It couldn't detect my face. So what's going on? Why isn't my face being detected? Well, we have to look at how we give machines sight. Computer vision uses machine-learning techniques to do facial recognition. So how this works is you create a training set with examples of faces. This is a face. This is a face. This is not a face. And over time, you can teach a computer how to recognize other faces. However, if the training sets aren't really that diverse, any face that deviates too much from the established norm will be harder to detect, which is what was happening to me. RAZ: Joy, you gave this talk a couple of years ago, but this problem still exists. BUOLAMWINI: Yes. And it's even more urgent now because there is this assumption that we've arrived. RAZ: Yeah. BUOLAMWINI: So, for example, in 2014, Facebook released a paper called \"DeepFace\" that showed a major breakthrough for facial recognition technology. They achieved 97. 35 percent accuracy on the gold standard benchmark for facial recognition at the time. But we always have to ask with these types of technologies, with AI - who's included? Who's excluded? So now I just told you 97. 35 percent accuracy. RAZ: Sounds great. BUOLAMWINI: Guess what the gender ratio was? RAZ: I don't know, 50/50? BUOLAMWINI: It's a gold standard. You would think 50/50. RAZ: Yeah. BUOLAMWINI: It was 77. 5 percent male. RAZ: Wow. BUOLAMWINI: And then demographic breakdown was - I want to say 80. 5 percent white for this gold standard benchmark. RAZ: Wow. BUOLAMWINI: So now when you know that the gold standard has these skews, when you see something like 97. 35 percent accuracy, we've made a major breakthrough, you start to get a better understanding of exactly which faces - right? - this breakthrough applies to and which ones might not be included. And it's a reflection of people who are in positions of power to mold artificial intelligence. And that's a very limited group right now. (SOUNDBITE OF TED TALK) BUOLAMWINI: Across the U. S. , police departments are starting to use facial recognition software in their crime-fighting arsenal. Georgetown Law published a report showing that 1 in 2 adults in the U. S. - that's 117 million people - have their faces in facial recognition networks. Police departments can currently look at these networks unregulated using algorithms that have not been audited for accuracy. Yet, we know facial recognition is not fail-proof. And labeling faces consistently remains a challenge. You might have seen this on Facebook. My friends and I laugh all the time when we see other people mislabeled in our photos. But misidentifying a suspected criminal is no laughing matter, nor is breaching civil liberties. Law enforcement is also starting to use machine-learning for predictive policing. Some judges use machine-generated risk scores to determine how long an individual is going to spend in prison. So we really have to think about these decisions. Are they fair? And we've seen that algorithmic bias doesn't necessarily always lead to fair outcomes. RAZ: So how do you stop this? I mean, how do you fight algorithmic bias? BUOLAMWINI: So I feel like the minimum thing we can do is actually check for the performance of these systems across groups that we already know have historically been disenfranchised - right? - in the first place. I feel like that's a minimal thing. Then we also need to think about what are steps to take to address the bias as well, right? You know, this is why I think it's critically important we have diverse people participating in the creation of the future. And that means having diverse people shaping the priorities as well as developing the technology. RAZ: So here's what I wonder. Can - I mean, is it possible to create a completely unbiased algorithm? BUOLAMWINI: It depends on what the task is. But I think the question I think about more so is knowing that we are deeply biased, even in our language, our classification systems, et cetera. How do we create systems that work well for humanity? But, also, how do we keep ourselves honest as we're making progress, right? So it's like if you're talking about perfecting a democracy. Will there be a perfect democracy? From what I look and see, probably not, you know. But you're trying to create a more perfect union in some way. So in trying to create more perfect AI, you strive for these ideals of inclusion. You want to mitigate bias, et cetera. But we also have the humility to know that being fallible, being human and being humans who embed our fallibility into the machines we create, we're not necessarily going to be perfect all the time. But we have to try to do our best and continue to improve. And if we exclude people and we limit people's humanity - right? - which is what happens when we have the algorithmic bias that's not addressed, we really limit the potential for all of us in the long run. RAZ: Joy Boulamwini. She's a computer scientist and the founder of the Algorithmic Justice League. You can see her full talk at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-26-580619282": {"title": "Alan Smith: Why Do We Trust Intuition Over Even The Most Reliable Numbers? : NPR", "url": "https://www.npr.org/2018/01/26/580619282/alan-smith-why-do-we-trust-intuition-over-even-the-most-reliable-numbers", "author": "No author found", "published_date": "2018-01-26", "content": "GUY RAZ, HOST: Can I ask you a philosophical question? ALAN SMITH: Sure. RAZ: Is the right or the correct statistic an objective truth? SMITH: Whoa. That's a very good question. RAZ: This is Alan Smith. SMITH: I'm the data visualization editor at the Financial Times. RAZ: And Alan says, unlike an algorithm, a statistic can be a more reliable measure of what's true. But in a certain sense, even that doesn't matter. SMITH: People have this idea, this kind of binary notion that it's either right or wrong. But what's really interesting is seeing people's reactions to it, about how they're going to use that information. RAZ: Before Alan worked in journalism, he spent over a decade working for the U. K. 's Office of National Statistics. SMITH: So that's kind of like the U. K. equivalent of the Census Bureau. RAZ: And while he was there, he noticed something that really intrigued him. SMITH: When we do a census, the whole point is to try and kind of capture everybody. . . RAZ: Yeah. SMITH: . . . In the census so we know more about ourselves. And it just struck me how little we knew about ourselves. When I would talk to people about my job and describe the sort of data we were working with, it never really seemed to marry up with other people's views. And that's really interesting. It always fascinated me that the way that people were perceiving the world was different from reality. RAZ: So, for example, Alan says that one annual survey by a U. K. polling firm has captured this difference pretty well, the difference between perception and reality. Alan described the results from the TED stage. (SOUNDBITE OF TED TALK)SMITH: They did a survey of over 1,000 adults in the U. K. and said, OK, for every 100 people in England and Wales, how many of them are Muslim? Now, the average answer from this survey - which was supposed to be representative of the population - was 24. British people think 24 out of every 100 people in the country are Muslim. Now, official figures reveal that figure to be about 5. They asked Saudi Arabians - for every 100 adults in your country, how many of them are overweight or obese? And the average answer was just over a quarter. The official figures show actually it's. . . (LAUGHTER)SMITH: . . . Nearer to three-quarters. And I love this one. They asked - in Japan, they said, for every 100 Japanese people, how many of them live in rural areas? And the average - again, this is the average - 56 out of every 100 Japanese people lived in rural areas. The official figure is seven - so extraordinary variations and surprising to some, but not surprising to people who've read the work of Daniel Kahneman, for example. Him and his colleague, Amos Tversky, spent years researching this disjoint between what people perceive and the reality, the fact that people are actually pretty poor intuitive statisticians. And there are many reasons for this. Individual experiences certainly can influence our perceptions but so, too, can things like the media reporting things by exception. Kahneman had a nice way of referring to that. He said, we can be blind to the obvious - so we've got the numbers wrong - but we can be blind to our blindness about it. And that has enormous repercussions for decision-making. RAZ: I mean, you can imagine that this might have really big consequences. Like, if people perceive, for instance, that the percentage of Muslims in the U. K. is so much higher than it is in reality, that could determine how they vote. SMITH: Exactly. And then, you know, that - for me, that's just astonishing because that's not exactly information that we've been trying to hide. That's information that had been broadcast for a good year or two after the census. And yet, people still had their own impression. RAZ: Why do you think so many people trust their intuitions and their lived experiences more than a statistic that is quantifiable, reliable and, for lack of a better word, true? SMITH: Why do people trust their intuitions? I think because it keeps them alive, right? Like, it does what it needs to do, which is makes you run away from a lion before you've even realized there's a lion running at you, right? Like, so intuitions, without being precise, can be very, very valuable. But I think that people find it difficult to relate to statistics when they're about the aggregate, right? RAZ: Yeah. SMITH: Like, no one actually thinks of themselves as 1 of 65 million people. RAZ: Right. SMITH: You're probably thinking of yourself, your family, your colleagues, the people that you interact with regularly. And that's a very different world from the one that's generally reflected in official statistics. (SOUNDBITE OF TED TALK)SMITH: So at the statistics office, while this was all going on, I had an idea, which was if we reframed the questions and say - how well do you know your local area? - would your answers be any more accurate? So I devised a quiz. How well do you know your area? It's a simple web app. You put in a post code, and then it will ask you questions based on census data for your local area. There are seven questions. Each question - there's a possible answer between zero and a hundred. And at the end of the quiz, you get an overall score between zero and a hundred. And so the first question is, for every 100 people, how many are aged under 16? You drag the slider to highlight your icons and then just click submit to answer. And we animate away the difference between your answer and reality. RAZ: OK. So your theory here was that people would know their own neighborhood better - you know, like, the place where they live and work and see people every day - that their perception of their local area would be more accurate. SMITH: Yeah, that's right. And, I mean, in fact, it turns out, people weren't any better with their local areas. You know, they just weren't. And we found that very many people were overestimating even in their local area things like the proportion of people who were Muslim. They were amazed at how many people did or didn't own a car in their street - kind of really simple stuff like that. The one that I found really interesting was - I actually thought the demographics - like, how old or young your area was - I thought people would nail that, but it turned out lots of people got that wrong, too. And it was really interesting to get that kind of. . . RAZ: Yeah. SMITH: . . . Their view of the neighborhood relative to the official figures. (SOUNDBITE OF TED TALK)SMITH: It turns out, the reaction was more than I could've hoped for. It was a long-held ambition of mine to bring down a statistics website due to public demand. (LAUGHTER)SMITH: This URL contains the words statistics, gov and U. K. , which is 3 of people's least-favorite words in a URL. And the amazing thing about this was that the website came down at quarter to 10 at night because people were actually engaging with this data of their own free will, using their own personal time. I was very interested to see that we got something like quarter of a million people playing the quiz within the space of 48 hours of launching it. And it sparked an enormous discussion online on social media, which was largely dominated by people having fun with their misconceptions, which is something that, you know - I couldn't have hoped for any better in some respects. It's really because statistics are about us. If you look at the etymology of the word statistics, it's the science of dealing with data about the state or the community that we live in. So statistics are about us as a group, not us as individuals. And I think as social animals, we share this fascination about how we as individuals relate to our groups, to our peers. RAZ: Do you think that the better we get at measuring things and coming up with reliable statistics, the more sort of progressive we become as humans, that we just continue to move forward and make better decisions? SMITH: I think better data is something that is a vital ingredient for progression. But I think changing the way that people interact with data, for me, is absolutely critical. And in fact, on that point, the thing I think that was really interesting was that I had quite a few teachers write to me. One said thank you, finally. They said, now we've got a way of trying to introduce younger children to why we even bother with this statistics stuff. And they said what they'd done is they'd got these children in their classes to do the quiz for the areas that they live. They were then encouraged to discuss the differences. And just that reasoning with data - I think what we've really got to think about is how we start to promote things like statistical reasoning as an integral part of our education. And, I mean, I think my interest originally in going to work for the statistics office was this idea that there was some public good to be derived from it. And I think people are now hoping that we're going to find better ways of applying it in future. RAZ: Alan Smith is the data visualization editor at the Financial Times. You can see his entire talk at ted. com. Today on the show, Can We Trust The Numbers? I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC) GUY RAZ, HOST:  Can I ask you a philosophical question? ALAN SMITH: Sure. RAZ: Is the right or the correct statistic an objective truth? SMITH: Whoa. That's a very good question. RAZ: This is Alan Smith. SMITH: I'm the data visualization editor at the Financial Times. RAZ: And Alan says, unlike an algorithm, a statistic can be a more reliable measure of what's true. But in a certain sense, even that doesn't matter. SMITH: People have this idea, this kind of binary notion that it's either right or wrong. But what's really interesting is seeing people's reactions to it, about how they're going to use that information. RAZ: Before Alan worked in journalism, he spent over a decade working for the U. K. 's Office of National Statistics. SMITH: So that's kind of like the U. K. equivalent of the Census Bureau. RAZ: And while he was there, he noticed something that really intrigued him. SMITH: When we do a census, the whole point is to try and kind of capture everybody. . . RAZ: Yeah. SMITH: . . . In the census so we know more about ourselves. And it just struck me how little we knew about ourselves. When I would talk to people about my job and describe the sort of data we were working with, it never really seemed to marry up with other people's views. And that's really interesting. It always fascinated me that the way that people were perceiving the world was different from reality. RAZ: So, for example, Alan says that one annual survey by a U. K. polling firm has captured this difference pretty well, the difference between perception and reality. Alan described the results from the TED stage. (SOUNDBITE OF TED TALK) SMITH: They did a survey of over 1,000 adults in the U. K. and said, OK, for every 100 people in England and Wales, how many of them are Muslim? Now, the average answer from this survey - which was supposed to be representative of the population - was 24. British people think 24 out of every 100 people in the country are Muslim. Now, official figures reveal that figure to be about 5. They asked Saudi Arabians - for every 100 adults in your country, how many of them are overweight or obese? And the average answer was just over a quarter. The official figures show actually it's. . . (LAUGHTER) SMITH: . . . Nearer to three-quarters. And I love this one. They asked - in Japan, they said, for every 100 Japanese people, how many of them live in rural areas? And the average - again, this is the average - 56 out of every 100 Japanese people lived in rural areas. The official figure is seven - so extraordinary variations and surprising to some, but not surprising to people who've read the work of Daniel Kahneman, for example. Him and his colleague, Amos Tversky, spent years researching this disjoint between what people perceive and the reality, the fact that people are actually pretty poor intuitive statisticians. And there are many reasons for this. Individual experiences certainly can influence our perceptions but so, too, can things like the media reporting things by exception. Kahneman had a nice way of referring to that. He said, we can be blind to the obvious - so we've got the numbers wrong - but we can be blind to our blindness about it. And that has enormous repercussions for decision-making. RAZ: I mean, you can imagine that this might have really big consequences. Like, if people perceive, for instance, that the percentage of Muslims in the U. K. is so much higher than it is in reality, that could determine how they vote. SMITH: Exactly. And then, you know, that - for me, that's just astonishing because that's not exactly information that we've been trying to hide. That's information that had been broadcast for a good year or two after the census. And yet, people still had their own impression. RAZ: Why do you think so many people trust their intuitions and their lived experiences more than a statistic that is quantifiable, reliable and, for lack of a better word, true? SMITH: Why do people trust their intuitions? I think because it keeps them alive, right? Like, it does what it needs to do, which is makes you run away from a lion before you've even realized there's a lion running at you, right? Like, so intuitions, without being precise, can be very, very valuable. But I think that people find it difficult to relate to statistics when they're about the aggregate, right? RAZ: Yeah. SMITH: Like, no one actually thinks of themselves as 1 of 65 million people. RAZ: Right. SMITH: You're probably thinking of yourself, your family, your colleagues, the people that you interact with regularly. And that's a very different world from the one that's generally reflected in official statistics. (SOUNDBITE OF TED TALK) SMITH: So at the statistics office, while this was all going on, I had an idea, which was if we reframed the questions and say - how well do you know your local area? - would your answers be any more accurate? So I devised a quiz. How well do you know your area? It's a simple web app. You put in a post code, and then it will ask you questions based on census data for your local area. There are seven questions. Each question - there's a possible answer between zero and a hundred. And at the end of the quiz, you get an overall score between zero and a hundred. And so the first question is, for every 100 people, how many are aged under 16? You drag the slider to highlight your icons and then just click submit to answer. And we animate away the difference between your answer and reality. RAZ: OK. So your theory here was that people would know their own neighborhood better - you know, like, the place where they live and work and see people every day - that their perception of their local area would be more accurate. SMITH: Yeah, that's right. And, I mean, in fact, it turns out, people weren't any better with their local areas. You know, they just weren't. And we found that very many people were overestimating even in their local area things like the proportion of people who were Muslim. They were amazed at how many people did or didn't own a car in their street - kind of really simple stuff like that. The one that I found really interesting was - I actually thought the demographics - like, how old or young your area was - I thought people would nail that, but it turned out lots of people got that wrong, too. And it was really interesting to get that kind of. . . RAZ: Yeah. SMITH: . . . Their view of the neighborhood relative to the official figures. (SOUNDBITE OF TED TALK) SMITH: It turns out, the reaction was more than I could've hoped for. It was a long-held ambition of mine to bring down a statistics website due to public demand. (LAUGHTER) SMITH: This URL contains the words statistics, gov and U. K. , which is 3 of people's least-favorite words in a URL. And the amazing thing about this was that the website came down at quarter to 10 at night because people were actually engaging with this data of their own free will, using their own personal time. I was very interested to see that we got something like quarter of a million people playing the quiz within the space of 48 hours of launching it. And it sparked an enormous discussion online on social media, which was largely dominated by people having fun with their misconceptions, which is something that, you know - I couldn't have hoped for any better in some respects. It's really because statistics are about us. If you look at the etymology of the word statistics, it's the science of dealing with data about the state or the community that we live in. So statistics are about us as a group, not us as individuals. And I think as social animals, we share this fascination about how we as individuals relate to our groups, to our peers. RAZ: Do you think that the better we get at measuring things and coming up with reliable statistics, the more sort of progressive we become as humans, that we just continue to move forward and make better decisions? SMITH: I think better data is something that is a vital ingredient for progression. But I think changing the way that people interact with data, for me, is absolutely critical. And in fact, on that point, the thing I think that was really interesting was that I had quite a few teachers write to me. One said thank you, finally. They said, now we've got a way of trying to introduce younger children to why we even bother with this statistics stuff. And they said what they'd done is they'd got these children in their classes to do the quiz for the areas that they live. They were then encouraged to discuss the differences. And just that reasoning with data - I think what we've really got to think about is how we start to promote things like statistical reasoning as an integral part of our education. And, I mean, I think my interest originally in going to work for the statistics office was this idea that there was some public good to be derived from it. And I think people are now hoping that we're going to find better ways of applying it in future. RAZ: Alan Smith is the data visualization editor at the Financial Times. You can see his entire talk at ted. com. Today on the show, Can We Trust The Numbers? I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC)", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-26-580617998": {"title": "Cathy O'Neil: Do Algorithms Perpetuate Human Bias? : NPR", "url": "https://www.npr.org/2018/01/26/580617998/cathy-oneil-do-algorithms-perpetuate-human-bias", "author": "No author found", "published_date": "2018-01-26", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. I'm Guy Raz. So in the mid-2000s, Cathy O'Neil. . . CATHY O'NEIL: My name's Cathy O'Neil. I'm a mathematician, data scientist and author of \"Weapons Of Math Destruction. \"RAZ: . . . Was working on Wall Street as a - Cathy, what was it called again? O'NEIL: A hedge fund called quant - (laughter) let me say that again - a hedge fudge - oh, my gosh - a hedge fund quant is - well, quant is short for quantitative analysts. RAZ: Oh, OK. I've got you, right. O'NEIL: So it's somebody who builds algorithms to try to predict the market. And in my case, I was trying to predict the futures market. But I entered finance in 2007, right as the crisis was unfolding. (SOUNDBITE OF ARCHIVED RECORDING MONTAGE)UNIDENTIFIED REPORTER #1: Apple's under pressure. Yahoo, down 8. 5 percent; Cisco, 6. 5 percent - researchers. . . UNIDENTIFIED REPORTER #2: Oil is down more than $4. Traders here working the phones say a lot of their customers are freaked out, waiting to see how low the Dow will go. RAZ: So ostensibly, your job was to make decisions that could help your clients get richer. I guess? O'NEIL: I mean, I don't think I actually contributed anything that made them money, which is kind of, like, a feather in my cap at this point. But at the time, I was like, man. . . RAZ: A few years later, Cathy left finance to become a data scientist, crunching numbers that were used by companies to help them target ads to consumers. O'NEIL: Basically, the same stuff. But instead of predicting markets, my new job was to predict people. So I was - you know, there I was as a data scientist. I was kind of like, oh, at least I'm not messing up the world anymore. But, you know, what I realized is that I was separating people with my new algorithms. I was separating people by class and often by race. RAZ: Yeah. O'NEIL: And I was giving some of them opportunities. And others of them, I was denying opportunities. And I was doing, you know, relatively benign things. But what I realized was, like, that's what data science does. We separate people into winners and losers. RAZ: Yeah. O'NEIL: And sometimes, those - what they win is really important to them. Sometimes it's a mortgage or a credit card or a job or prison time. RAZ: Yeah. O'NEIL: And the more I learned, the more I said, wow, this is a real problem. These algorithms are placeholders for these very, very difficult discussions that we don't really want to have as a society. So we're sort of hiding them in these black boxes. RAZ: How ubiquitous is the use of algorithms now in, like, everyday life - in the world, in the U. S. , wherever? How - I mean, are algorithms used all over the place now? O'NEIL: So let me just take an average person. The average person spends, you know, some amount of time on Facebook. . . RAZ: Yeah. O'NEIL: . . . Or Twitter or Google. And the answer is absolutely, algorithms are completely controlling their experience and their atmosphere and their environment. But you know, besides that, most of the time the algorithms that I worry about the most - they happen at certain specific junctures of people's lives, where critical decisions are being made. Where do I go to college? Where do I get a job? Like, how do I get a mortgage? And so you should think of these as, like, bureaucratic decisions that other people make about you. And at those moments, it's almost always algorithmic at this point. (SOUNDBITE OF MUSIC)RAZ: On the show today, Can We Trust The Numbers? We're going explore the ups and downs of relying too much on data and hear ideas about how our faith in data, statistics and algorithms can sometimes lead us astray. And as we heard from Cathy O'Neil, those algorithms she mentioned weren't just predicting outcomes. In some cases, they were actually causing them. O'NEIL: Because one of the things is they all kind of act the same - they're not exactly the same. But when I talk about, like, algorithms that sort through resumes or algorithms that - personality tests or algorithms that decide who is a good insurance risk, they're very, very similar in different companies. So they're sorting people in the same kind of way. And if you think about what that does on a society level, it's sorting winners and losers in the standard, old-fashioned way that we've been trying to get over, that we've been trying to transcend - through class, through gender, through race. And it's against the American dream. You know, it is actually a social mobility problem. And that's what I realized. I was like, I'm working on this. I left finance, and now what I'm doing is I'm sort of codifying inequality. RAZ: So this is the thing. There is some data - a lot of data - a lot of historical data that is right, that is accurate and that we have to use. Right? O'NEIL: I mean, look - it's really important to understand the difference between accuracy and fairness. So it used to be that life insurance companies made black men pay more for life insurance than white men simply because they were going to die sooner. That lasted for a long time before the regulators in question were like - wait a second - that's racist. And it's racist because we have to ask the question why. Why are black men living less than white men? And is that their fault that they should take responsibility for and they should pay for, or is that a problem that society itself should take on and fix? So it wasn't an inaccurate fact that black men lived less time. But the question was, how should we deal with that? And that's a question of fairness, and it's a question that we all have to grapple with together. And many of these questions are of that nature. So yes, it's true that people who live in this ZIP code are more likely to default on their debt. Does that mean we don't loan them any money, or do we make a rule that people of this age who have a job, who finish college or whatever - what do we decide is fair? And that's a really hard question. Data science has done nothing to address that question. RAZ: So why is it that when most of us hear the term data science, we think, yeah, that must be right? O'NEIL: Well, I think most of us are intimidated by what I call the authority of the inscrutable. RAZ: Yeah. O'NEIL: If we don't understand something, we don't feel like we're expert enough to complain about it. I've seen it happen. I mean, I'm a mathematician. I have a Ph. D. in math, you know. And when I was in school for my Ph. D. , I would sit next to somebody on the airplane. And they'd say, oh, wow, you must be really smart. I hated math in junior high. You know, they'd always say that. RAZ: Yeah. O'NEIL: And they would, like, defer to me for all sorts of ridiculous things simply because I'm good at math. So I've seen it happen in real time. But I've also seen people use that authority to make people stop asking questions. RAZ: Which you can do. You can, like, bully people with data. . . O'NEIL: It's a form of bullying. RAZ: . . . And statistics. You can bully them and say, look, this is what it says. O'NEIL: I call it math-washing (ph). RAZ: Math-washing? O'NEIL: Yeah. It's like, you know, don't look behind this curtain. RAZ: It's like math-splaining (ph). O'NEIL: (Laughter) Yeah, exactly. RAZ: I don't want to be math-splained. O'NEIL: No, you don't. You want to see the evidence. Show me evidence that this data science thing works. RAZ: Yeah. O'NEIL: Show me - for whom does this fail? Does this fail more for women than for men? Does this fail more for African-Americans than for white? Does this fail more for people with mental health status than for others? Like, show me the data. And until you've shown me the evidence that this works, why should I trust it? Why should I think it's fair? RAZ: I mean, there's - essentially, the people who have access to the data and that can decide how that data is presented have a tremendous amount of power. O'NEIL: It's about power. RAZ: Yeah, it really is. O'NEIL: It's more than you think. It's about power because they get to decide what experiment. It's about power because they get to math-splain. It's also about power because they get to decide what success looks like. RAZ: Here's Cathy O'Neil on the TED stage. (SOUNDBITE OF TED TALK)O'NEIL: This is Roger Ailes. He founded Fox News in 1996. More than 20 women complained about sexual harassment. They said they weren't allowed to succeed at Fox News. He was ousted last year, but we've seen recently that the problems have persisted. That begs the question, what should Fox News do to turn over another leaf? Well, what if they replaced their hiring process with a machine learning algorithm? That sounds good. Right? Think about it. The data - what would the data be? A reasonable choice would be the last 21 years of applications to Fox News - reasonable. What about the definition of success? Reasonable choice would be - well, who's successful at Fox News? I guess someone who, say, stayed there for four years and was promoted at least once - sounds reasonable. And then the algorithm would be trained. It would be trained to look for people to learn what led to success. What kind of applications historically led to success by that definition? Now think about what would happen if we applied that to a current pool of applicants. It would filter out women because they do not look like people who were successful in the past. Algorithms don't make things fair if you just blithely, blindly apply algorithms. They don't make things fair. They repeat our past practices, our patterns. They automate the status quo. That would be great if we had a perfect world, but we don't. And I'll add that most companies don't have embarrassing lawsuits. But the data scientists in those companies are told to follow the data, to focus on accuracy. Think about what that means. Because we all have bias, it means they could be codifying sexism or any other kind of bigotry. (SOUNDBITE OF MUSIC)O'NEIL: I think about it like this. Like, you know, I'm sure - and I'm not a historian - but like, I'm sure that when cars first came out, people were just, like, so floored by them. They were like, oh, my God, cars are the best. And they didn't notice that cars - you know, sometimes wheels fell off. And sometimes people got killed by spiky things in the car that didn't need to be so spiky - when they got into an accident or fell off the road or something. And it was only after quite a while that we started insisting on airbags and bumpers and safety. It wasn't some magical experience. It was an actual fight by consumer advocates. So I feel like we're pre-consumer advocates in the world of algorithms. We are still driving around without a speedometer, without bumpers and without airbags - assuming that these algorithms work perfectly. The difference between car safety and algorithmic safety is that it's much harder to see when people's lives get ruined via algorithm than it was to see that people died in a car crash. Car crashes are like public tragedies. Everyone can see a car crash. But when somebody gets denied a job, especially in this situation where they often don't even find out why, it's really, really difficult to say - hey, that's a failure of the algorithm; let's fix it. RAZ: But - I mean, given that algorithms are fed via statistics, how do you feel about statistics? Are you skeptical of those? O'NEIL: Not at all. RAZ: No, OK. O'NEIL: I'm not at all skeptical of statistics. In fact - I mean, let me get bigger. I feel like science is our only hope. And I feel like what we've done is we've created a field we call data science, but there's no science in it. We have not demanded evidence. The sort of hallmark of science is that it needs to be tested and testable. And we need to see the evidence, and we need to test every assumption. And we just haven't done any of that. We've just been driving blind on our Model T algorithms. RAZ: Cathy O'Neil - she's a data scientist and author of the book \"Weapons Of Math Destruction: How Big Data Increases Inequality And Threatens Democracy. \" You can see her full talk at ted. com. On the show today, Can We Trust The Numbers? I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. So in the mid-2000s, Cathy O'Neil. . . CATHY O'NEIL: My name's Cathy O'Neil. I'm a mathematician, data scientist and author of \"Weapons Of Math Destruction. \" RAZ: . . . Was working on Wall Street as a - Cathy, what was it called again? O'NEIL: A hedge fund called quant - (laughter) let me say that again - a hedge fudge - oh, my gosh - a hedge fund quant is - well, quant is short for quantitative analysts. RAZ: Oh, OK. I've got you, right. O'NEIL: So it's somebody who builds algorithms to try to predict the market. And in my case, I was trying to predict the futures market. But I entered finance in 2007, right as the crisis was unfolding. (SOUNDBITE OF ARCHIVED RECORDING MONTAGE) UNIDENTIFIED REPORTER #1: Apple's under pressure. Yahoo, down 8. 5 percent; Cisco, 6. 5 percent - researchers. . . UNIDENTIFIED REPORTER #2: Oil is down more than $4. Traders here working the phones say a lot of their customers are freaked out, waiting to see how low the Dow will go. RAZ: So ostensibly, your job was to make decisions that could help your clients get richer. I guess? O'NEIL: I mean, I don't think I actually contributed anything that made them money, which is kind of, like, a feather in my cap at this point. But at the time, I was like, man. . . RAZ: A few years later, Cathy left finance to become a data scientist, crunching numbers that were used by companies to help them target ads to consumers. O'NEIL: Basically, the same stuff. But instead of predicting markets, my new job was to predict people. So I was - you know, there I was as a data scientist. I was kind of like, oh, at least I'm not messing up the world anymore. But, you know, what I realized is that I was separating people with my new algorithms. I was separating people by class and often by race. RAZ: Yeah. O'NEIL: And I was giving some of them opportunities. And others of them, I was denying opportunities. And I was doing, you know, relatively benign things. But what I realized was, like, that's what data science does. We separate people into winners and losers. RAZ: Yeah. O'NEIL: And sometimes, those - what they win is really important to them. Sometimes it's a mortgage or a credit card or a job or prison time. RAZ: Yeah. O'NEIL: And the more I learned, the more I said, wow, this is a real problem. These algorithms are placeholders for these very, very difficult discussions that we don't really want to have as a society. So we're sort of hiding them in these black boxes. RAZ: How ubiquitous is the use of algorithms now in, like, everyday life - in the world, in the U. S. , wherever? How - I mean, are algorithms used all over the place now? O'NEIL: So let me just take an average person. The average person spends, you know, some amount of time on Facebook. . . RAZ: Yeah. O'NEIL: . . . Or Twitter or Google. And the answer is absolutely, algorithms are completely controlling their experience and their atmosphere and their environment. But you know, besides that, most of the time the algorithms that I worry about the most - they happen at certain specific junctures of people's lives, where critical decisions are being made. Where do I go to college? Where do I get a job? Like, how do I get a mortgage? And so you should think of these as, like, bureaucratic decisions that other people make about you. And at those moments, it's almost always algorithmic at this point. (SOUNDBITE OF MUSIC) RAZ: On the show today, Can We Trust The Numbers? We're going explore the ups and downs of relying too much on data and hear ideas about how our faith in data, statistics and algorithms can sometimes lead us astray. And as we heard from Cathy O'Neil, those algorithms she mentioned weren't just predicting outcomes. In some cases, they were actually causing them. O'NEIL: Because one of the things is they all kind of act the same - they're not exactly the same. But when I talk about, like, algorithms that sort through resumes or algorithms that - personality tests or algorithms that decide who is a good insurance risk, they're very, very similar in different companies. So they're sorting people in the same kind of way. And if you think about what that does on a society level, it's sorting winners and losers in the standard, old-fashioned way that we've been trying to get over, that we've been trying to transcend - through class, through gender, through race. And it's against the American dream. You know, it is actually a social mobility problem. And that's what I realized. I was like, I'm working on this. I left finance, and now what I'm doing is I'm sort of codifying inequality. RAZ: So this is the thing. There is some data - a lot of data - a lot of historical data that is right, that is accurate and that we have to use. Right? O'NEIL: I mean, look - it's really important to understand the difference between accuracy and fairness. So it used to be that life insurance companies made black men pay more for life insurance than white men simply because they were going to die sooner. That lasted for a long time before the regulators in question were like - wait a second - that's racist. And it's racist because we have to ask the question why. Why are black men living less than white men? And is that their fault that they should take responsibility for and they should pay for, or is that a problem that society itself should take on and fix? So it wasn't an inaccurate fact that black men lived less time. But the question was, how should we deal with that? And that's a question of fairness, and it's a question that we all have to grapple with together. And many of these questions are of that nature. So yes, it's true that people who live in this ZIP code are more likely to default on their debt. Does that mean we don't loan them any money, or do we make a rule that people of this age who have a job, who finish college or whatever - what do we decide is fair? And that's a really hard question. Data science has done nothing to address that question. RAZ: So why is it that when most of us hear the term data science, we think, yeah, that must be right? O'NEIL: Well, I think most of us are intimidated by what I call the authority of the inscrutable. RAZ: Yeah. O'NEIL: If we don't understand something, we don't feel like we're expert enough to complain about it. I've seen it happen. I mean, I'm a mathematician. I have a Ph. D. in math, you know. And when I was in school for my Ph. D. , I would sit next to somebody on the airplane. And they'd say, oh, wow, you must be really smart. I hated math in junior high. You know, they'd always say that. RAZ: Yeah. O'NEIL: And they would, like, defer to me for all sorts of ridiculous things simply because I'm good at math. So I've seen it happen in real time. But I've also seen people use that authority to make people stop asking questions. RAZ: Which you can do. You can, like, bully people with data. . . O'NEIL: It's a form of bullying. RAZ: . . . And statistics. You can bully them and say, look, this is what it says. O'NEIL: I call it math-washing (ph). RAZ: Math-washing? O'NEIL: Yeah. It's like, you know, don't look behind this curtain. RAZ: It's like math-splaining (ph). O'NEIL: (Laughter) Yeah, exactly. RAZ: I don't want to be math-splained. O'NEIL: No, you don't. You want to see the evidence. Show me evidence that this data science thing works. RAZ: Yeah. O'NEIL: Show me - for whom does this fail? Does this fail more for women than for men? Does this fail more for African-Americans than for white? Does this fail more for people with mental health status than for others? Like, show me the data. And until you've shown me the evidence that this works, why should I trust it? Why should I think it's fair? RAZ: I mean, there's - essentially, the people who have access to the data and that can decide how that data is presented have a tremendous amount of power. O'NEIL: It's about power. RAZ: Yeah, it really is. O'NEIL: It's more than you think. It's about power because they get to decide what experiment. It's about power because they get to math-splain. It's also about power because they get to decide what success looks like. RAZ: Here's Cathy O'Neil on the TED stage. (SOUNDBITE OF TED TALK) O'NEIL: This is Roger Ailes. He founded Fox News in 1996. More than 20 women complained about sexual harassment. They said they weren't allowed to succeed at Fox News. He was ousted last year, but we've seen recently that the problems have persisted. That begs the question, what should Fox News do to turn over another leaf? Well, what if they replaced their hiring process with a machine learning algorithm? That sounds good. Right? Think about it. The data - what would the data be? A reasonable choice would be the last 21 years of applications to Fox News - reasonable. What about the definition of success? Reasonable choice would be - well, who's successful at Fox News? I guess someone who, say, stayed there for four years and was promoted at least once - sounds reasonable. And then the algorithm would be trained. It would be trained to look for people to learn what led to success. What kind of applications historically led to success by that definition? Now think about what would happen if we applied that to a current pool of applicants. It would filter out women because they do not look like people who were successful in the past. Algorithms don't make things fair if you just blithely, blindly apply algorithms. They don't make things fair. They repeat our past practices, our patterns. They automate the status quo. That would be great if we had a perfect world, but we don't. And I'll add that most companies don't have embarrassing lawsuits. But the data scientists in those companies are told to follow the data, to focus on accuracy. Think about what that means. Because we all have bias, it means they could be codifying sexism or any other kind of bigotry. (SOUNDBITE OF MUSIC) O'NEIL: I think about it like this. Like, you know, I'm sure - and I'm not a historian - but like, I'm sure that when cars first came out, people were just, like, so floored by them. They were like, oh, my God, cars are the best. And they didn't notice that cars - you know, sometimes wheels fell off. And sometimes people got killed by spiky things in the car that didn't need to be so spiky - when they got into an accident or fell off the road or something. And it was only after quite a while that we started insisting on airbags and bumpers and safety. It wasn't some magical experience. It was an actual fight by consumer advocates. So I feel like we're pre-consumer advocates in the world of algorithms. We are still driving around without a speedometer, without bumpers and without airbags - assuming that these algorithms work perfectly. The difference between car safety and algorithmic safety is that it's much harder to see when people's lives get ruined via algorithm than it was to see that people died in a car crash. Car crashes are like public tragedies. Everyone can see a car crash. But when somebody gets denied a job, especially in this situation where they often don't even find out why, it's really, really difficult to say - hey, that's a failure of the algorithm; let's fix it. RAZ: But - I mean, given that algorithms are fed via statistics, how do you feel about statistics? Are you skeptical of those? O'NEIL: Not at all. RAZ: No, OK. O'NEIL: I'm not at all skeptical of statistics. In fact - I mean, let me get bigger. I feel like science is our only hope. And I feel like what we've done is we've created a field we call data science, but there's no science in it. We have not demanded evidence. The sort of hallmark of science is that it needs to be tested and testable. And we need to see the evidence, and we need to test every assumption. And we just haven't done any of that. We've just been driving blind on our Model T algorithms. RAZ: Cathy O'Neil - she's a data scientist and author of the book \"Weapons Of Math Destruction: How Big Data Increases Inequality And Threatens Democracy. \" You can see her full talk at ted. com. On the show today, Can We Trust The Numbers? I'm Guy Raz, and you're listening to the TED Radio Hour from NPR.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-26-580933872": {"title": "What's New At The Detroit Auto Show : NPR", "url": "https://www.npr.org/2018/01/26/580933872/whats-new-at-the-detroit-auto-show", "author": "No author found", "published_date": "2018-01-26", "content": "STEVE INSKEEP, HOST: You know what has happened to the phone over time. You know, years ago - old days - 10 years ago, 15 years ago, it was just a phone. RACHEL MARTIN, HOST: Right - a phone. So now it's also a watch a camera a map a way to shame your friends into paying you back for the dinner the other night that they. . . INSKEEP: (Laughter). MARTIN: . . . Did not pay for. INSKEEP: And so much more. NPR's Sonari Glinton is at the big auto show in Detroit and finds that the same thing is now happening to cars. SONARI GLINTON, BYLINE: The options for a new vehicle are kind of like a diner menu of features. Lincoln will connect you with a concierge. And Mercedes - the car knows you're on a windy road and will help you control your steering into the curve. Subaru has a built-in dog ramp. And Honda is focusing on your parenting skills. MICAH MUZIO: Honda has done so much clever stuff by integrating technologies that aren't just cool and techie but really make the job of parenting easier. GLINTON: Micah Muzio is with Kelley Blue Book. He's the father of a toddler, and he showed me around a Honda Odyssey to make his point. MUZIO: So when I say make parenting easier, I mean, as the driver - you know, we have had a representative up front there. Say hi, Davis Adams. DAVIS ADAMS: Hi, Davis Adams. MUZIO: That's good shtick. Should we roleplay? GLINTON: OK. So you'll sit in the back seat. MUZIO: OK. OK, all right. GLINTON: So I'll sit in the middle row? MUZIO: So I've positioned myself in the third row. And I am a fussy, fussy toddler, and Davis would like to keep tabs on what I'm doing. GLINTON: Oh and I'm looking - I just realized there's a camera that's inside the car. And Davis is pinching it, and he's seeing my bald head. (LAUGHTER)GLINTON: And he can zoom back. And you're in the third row, and you can see you making obscene gestures (laughter). MUZIO: So what's great about that is that when you're the driver, you're pointing the exact opposite direction of where your children are. And that is a great way that, rather than taking his gaze completely away from where he's driving, he can look at the screen right in the middle of the dash there and see. How many fingers am I holding up, Davis? ADAMS: You're holding up two, Micah. MUZIO: Look on the lower left side. You see what it says on the lower left side of the screen? GLINTON: Cabin Talk. MUZIO: Yeah. Davis, you want to explain that? ADAMS: Cabin Talk allows you to use the microphone in the front of the car and broadcast your voice to the back of the car. So if you want to parent from the front seat without having to raise your voice or something like that, you can do that right from here without having it too crazy. MUZIO: Don't you want to hear that from back here? Let's really roleplay. Oh, no. I'm being fussy and disagreeable. Davis, discipline me. ADAMS: Micah, don't make me come back there. MUZIO: Yes, sir. GLINTON: Here's something interesting. Many car companies have been fiddling with the layout of the interior of cars, lighter materials that will allow you to change the configuration of your minivan or your SUV really quickly. MUZIO: One of the things that I really like about the second row in the the Odyssey is that it's got something called Buddy Mode. Can I demonstrate Buddy Mode to you? GLINTON: By the way, he's always kind of like this. MUZIO: OK. No, no, no, no. You got to sit in here for this. OK. GLINTON: OK. MUZIO: So watch what I'm going to do right now. OK. Whee. So I've now slid immediately next to Sonari, making him very uncomfortable. But what's great about this is that it - you can keep them in separate positions in case, you know, the kids are fighting. And what's wonderful about this technology is that this is the kind of stuff that's super easy to use. And mothers and fathers are going to be more attentive and safer when they drive. That's fantastic. GLINTON: Don't make me come back there. Sonari Glinton, NPR News, Detroit. (SOUNDBITE OF SONG, \"DRIVE MY CAR\")THE BEATLES: (Singing) Baby, you can drive my car. STEVE INSKEEP, HOST:  You know what has happened to the phone over time. You know, years ago - old days - 10 years ago, 15 years ago, it was just a phone. RACHEL MARTIN, HOST:  Right - a phone. So now it's also a watch a camera a map a way to shame your friends into paying you back for the dinner the other night that they. . . INSKEEP: (Laughter). MARTIN: . . . Did not pay for. INSKEEP: And so much more. NPR's Sonari Glinton is at the big auto show in Detroit and finds that the same thing is now happening to cars. SONARI GLINTON, BYLINE: The options for a new vehicle are kind of like a diner menu of features. Lincoln will connect you with a concierge. And Mercedes - the car knows you're on a windy road and will help you control your steering into the curve. Subaru has a built-in dog ramp. And Honda is focusing on your parenting skills. MICAH MUZIO: Honda has done so much clever stuff by integrating technologies that aren't just cool and techie but really make the job of parenting easier. GLINTON: Micah Muzio is with Kelley Blue Book. He's the father of a toddler, and he showed me around a Honda Odyssey to make his point. MUZIO: So when I say make parenting easier, I mean, as the driver - you know, we have had a representative up front there. Say hi, Davis Adams. DAVIS ADAMS: Hi, Davis Adams. MUZIO: That's good shtick. Should we roleplay? GLINTON: OK. So you'll sit in the back seat. MUZIO: OK. OK, all right. GLINTON: So I'll sit in the middle row? MUZIO: So I've positioned myself in the third row. And I am a fussy, fussy toddler, and Davis would like to keep tabs on what I'm doing. GLINTON: Oh and I'm looking - I just realized there's a camera that's inside the car. And Davis is pinching it, and he's seeing my bald head. (LAUGHTER) GLINTON: And he can zoom back. And you're in the third row, and you can see you making obscene gestures (laughter). MUZIO: So what's great about that is that when you're the driver, you're pointing the exact opposite direction of where your children are. And that is a great way that, rather than taking his gaze completely away from where he's driving, he can look at the screen right in the middle of the dash there and see. How many fingers am I holding up, Davis? ADAMS: You're holding up two, Micah. MUZIO: Look on the lower left side. You see what it says on the lower left side of the screen? GLINTON: Cabin Talk. MUZIO: Yeah. Davis, you want to explain that? ADAMS: Cabin Talk allows you to use the microphone in the front of the car and broadcast your voice to the back of the car. So if you want to parent from the front seat without having to raise your voice or something like that, you can do that right from here without having it too crazy. MUZIO: Don't you want to hear that from back here? Let's really roleplay. Oh, no. I'm being fussy and disagreeable. Davis, discipline me. ADAMS: Micah, don't make me come back there. MUZIO: Yes, sir. GLINTON: Here's something interesting. Many car companies have been fiddling with the layout of the interior of cars, lighter materials that will allow you to change the configuration of your minivan or your SUV really quickly. MUZIO: One of the things that I really like about the second row in the the Odyssey is that it's got something called Buddy Mode. Can I demonstrate Buddy Mode to you? GLINTON: By the way, he's always kind of like this. MUZIO: OK. No, no, no, no. You got to sit in here for this. OK. GLINTON: OK. MUZIO: So watch what I'm going to do right now. OK. Whee. So I've now slid immediately next to Sonari, making him very uncomfortable. But what's great about this is that it - you can keep them in separate positions in case, you know, the kids are fighting. And what's wonderful about this technology is that this is the kind of stuff that's super easy to use. And mothers and fathers are going to be more attentive and safer when they drive. That's fantastic. GLINTON: Don't make me come back there. Sonari Glinton, NPR News, Detroit. (SOUNDBITE OF SONG, \"DRIVE MY CAR\") THE BEATLES: (Singing) Baby, you can drive my car.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-01-27-579683042": {"title": "5 Ways Election Interference Could (And Probably Will) Worsen In 2018 And Beyond : NPR", "url": "https://www.npr.org/2018/01/27/579683042/5-ways-election-interference-could-and-probably-will-worsen-in-2018-and-beyond", "author": "No author found", "published_date": "2018-01-27", "content": "", "section": "National Security", "disclaimer": ""}, "2018-02-02-582105045": {"title": "6 Rules For Roundtable Podcasting  : NPR", "url": "https://www.npr.org/2018/02/02/582105045/its-all-in-your-head-the-one-way-intimacy-of-podcast-listening", "author": "No author found", "published_date": "2018-02-02", "content": "", "section": "Pop Culture Happy Hour", "disclaimer": ""}, "2018-02-03-582767531": {"title": "In An Era Of Fake News, Advancing Face-Swap Apps Blur More Lines : NPR", "url": "https://www.npr.org/2018/02/03/582767531/in-an-era-of-fake-news-advancing-face-swap-apps-blur-more-lines", "author": "No author found", "published_date": "2018-02-03", "content": "SCOTT SIMON, HOST: Most people familiar with face-swapping know it as a harmless, fun feature on social media apps. An algorithm captures a person's face and places it on somebody else's head. The result is rarely seamless, and often it's pretty funny. But face-swapping has recently been used to superimpose the faces of celebrities into pornographic films. This isn't just alarming for actors and actresses who appear to perform in movies they never made. Because the technology is more advanced and accessible, not-so-famous faces are worried where they might show up online. Is face-swapping a dark sign for online identities? Samantha Cole is an editor at Motherboard and has been covering this. Thanks very much for being with us. SAMANTHA COLE: Sure. Thanks for having me. SIMON: You've seen one of these, right? COLE: Yes. I've seen probably dozens, if not a hundred, of them by now. SIMON: Well, you tracked down and interviewed a Reddit user who goes by the name of Deepfakes who, I guess, has created three adult films with celebrity faces, yes? COLE: He's created probably a lot more than that, to be honest. He was the person who first posted one of these on Reddit, and his name has become the name for this form of video - these fake porn videos. SIMON: How does it work? COLE: So basically, it's generated using a machine-learning algorithm. So someone takes a dataset of a lot of people's - or one person's face and a lot of pictures of that person's face, and then a video that they want to put it on. And they run a machine-learning algorithm, train it on these two images. And after a few hours, it gives you the result, which is these very realistic fake porn videos. SIMON: So hypothetically, could you take somebody's photos or videos off their social media feeds and put them into adult films? COLE: So yes. Hypothetically, it's definitely possible if you have enough images of someone. It's not something that we've seen happen yet. But as quickly as this technology is moving, it's definitely possible. SIMON: Is it legal? Or does anyone care? COLE: (Laughter) I think both sides care quite a bit - the people making them and the people who are the targets of them. The legality is honestly in a very gray area. It's all very hazy right now. We're not really sure what to make of it. Celebrities could sue for misappropriation of their images, like when you use a celebrity's face for an ad without their permission - things like that. But the average person has little recourse, honestly. Revenge porn laws don't include the right kind of language to cover this situation because it's a mashup of two things. SIMON: Yeah. Revenge porn is when someone takes a. . . COLE: Right. SIMON: . . . Intimate film of someone, and they don't have their permission. COLE: Exactly. Yeah. So this is not quite that. And that's creating a lot of problems legally and a lot of questions of how we're going to handle this. SIMON: I have to tell you my biggest worry as a citizen is not porn but that somebody might put somebody's face - let's say - at a crime scene or in some other - you know, at a rally that you never attended or something like that. COLE: That's definitely possible, and that's something that we're thinking about. It's splashy right now because it is porn. And celebrities and porn performers are two groups of people that have lots of images of themselves publicly out there, so they're easy targets for this. But so are politicians, you know, anyone who's on TV or on the Internet, showing their face quite a bit. SIMON: And what about regular citizens who just have a lot of pictures and videos on social media sites? Could they be victimized, too? COLE: I mean, it's theoretically definitely possible. You would need hundreds of pictures of someone. It's worth taking a look at your privacy settings and thinking about how you use the Internet and whether or not you're sharing your face in all these private forums. But then again, that puts a lot of pressure on users to - for them to kind of self-regulate over platforms. And those are the ones that really need to be accountable for taking care of the people who are using these platforms and kind of regulating how people are using them and hoping that they're not for harm. SIMON: I mean, if the solution is just don't put pictures or videos on social media platforms, that also kind of destroys the utility of social media platforms, doesn't it? COLE: Sure. And that's definitely not - that's not what I'm saying. I'm not saying don't put pictures of yourself out there. That's an extreme solution to this. The better solution would be to have more stringent laws around revenge porn, ownership of our own images, more responsive platforms who act quickly and serve their users better. Yeah. It's - right now, it's just easier to say think twice about your privacy settings because that's all we can do. That's all we have control of right now. SIMON: Samantha Cole at Motherboard, thanks so much for being with us. COLE: Thank you for having me. SCOTT SIMON, HOST:  Most people familiar with face-swapping know it as a harmless, fun feature on social media apps. An algorithm captures a person's face and places it on somebody else's head. The result is rarely seamless, and often it's pretty funny. But face-swapping has recently been used to superimpose the faces of celebrities into pornographic films. This isn't just alarming for actors and actresses who appear to perform in movies they never made. Because the technology is more advanced and accessible, not-so-famous faces are worried where they might show up online. Is face-swapping a dark sign for online identities? Samantha Cole is an editor at Motherboard and has been covering this. Thanks very much for being with us. SAMANTHA COLE: Sure. Thanks for having me. SIMON: You've seen one of these, right? COLE: Yes. I've seen probably dozens, if not a hundred, of them by now. SIMON: Well, you tracked down and interviewed a Reddit user who goes by the name of Deepfakes who, I guess, has created three adult films with celebrity faces, yes? COLE: He's created probably a lot more than that, to be honest. He was the person who first posted one of these on Reddit, and his name has become the name for this form of video - these fake porn videos. SIMON: How does it work? COLE: So basically, it's generated using a machine-learning algorithm. So someone takes a dataset of a lot of people's - or one person's face and a lot of pictures of that person's face, and then a video that they want to put it on. And they run a machine-learning algorithm, train it on these two images. And after a few hours, it gives you the result, which is these very realistic fake porn videos. SIMON: So hypothetically, could you take somebody's photos or videos off their social media feeds and put them into adult films? COLE: So yes. Hypothetically, it's definitely possible if you have enough images of someone. It's not something that we've seen happen yet. But as quickly as this technology is moving, it's definitely possible. SIMON: Is it legal? Or does anyone care? COLE: (Laughter) I think both sides care quite a bit - the people making them and the people who are the targets of them. The legality is honestly in a very gray area. It's all very hazy right now. We're not really sure what to make of it. Celebrities could sue for misappropriation of their images, like when you use a celebrity's face for an ad without their permission - things like that. But the average person has little recourse, honestly. Revenge porn laws don't include the right kind of language to cover this situation because it's a mashup of two things. SIMON: Yeah. Revenge porn is when someone takes a. . . COLE: Right. SIMON: . . . Intimate film of someone, and they don't have their permission. COLE: Exactly. Yeah. So this is not quite that. And that's creating a lot of problems legally and a lot of questions of how we're going to handle this. SIMON: I have to tell you my biggest worry as a citizen is not porn but that somebody might put somebody's face - let's say - at a crime scene or in some other - you know, at a rally that you never attended or something like that. COLE: That's definitely possible, and that's something that we're thinking about. It's splashy right now because it is porn. And celebrities and porn performers are two groups of people that have lots of images of themselves publicly out there, so they're easy targets for this. But so are politicians, you know, anyone who's on TV or on the Internet, showing their face quite a bit. SIMON: And what about regular citizens who just have a lot of pictures and videos on social media sites? Could they be victimized, too? COLE: I mean, it's theoretically definitely possible. You would need hundreds of pictures of someone. It's worth taking a look at your privacy settings and thinking about how you use the Internet and whether or not you're sharing your face in all these private forums. But then again, that puts a lot of pressure on users to - for them to kind of self-regulate over platforms. And those are the ones that really need to be accountable for taking care of the people who are using these platforms and kind of regulating how people are using them and hoping that they're not for harm. SIMON: I mean, if the solution is just don't put pictures or videos on social media platforms, that also kind of destroys the utility of social media platforms, doesn't it? COLE: Sure. And that's definitely not - that's not what I'm saying. I'm not saying don't put pictures of yourself out there. That's an extreme solution to this. The better solution would be to have more stringent laws around revenge porn, ownership of our own images, more responsive platforms who act quickly and serve their users better. Yeah. It's - right now, it's just easier to say think twice about your privacy settings because that's all we can do. That's all we have control of right now. SIMON: Samantha Cole at Motherboard, thanks so much for being with us. COLE: Thank you for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-02-04-583095400": {"title": "Just How Real Is Virtual Reality? : NPR", "url": "https://www.npr.org/2018/02/04/583095400/just-how-real-is-virtual-reality", "author": "No author found", "published_date": "2018-02-04", "content": "LULU GARCIA-NAVARRO, HOST: Virtual reality hasn't become a reality for most of us yet. So if you want to know how real virtual reality feels, listen as this young girl balances on a virtual plank and then steps off. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED WOMAN #1: Help. Help. No. No. GARCIA-NAVARRO: Don't worry. She survived. But how will I do when I walk the plank? Keep listening. In the last few years, we've seen a new wave of VR technology, from those 360 videos on cardboard viewers to more sophisticated headsets. And now there's the VR arcade. I recently went to check one out in New York City. It's called VR World, advertised as the largest virtual reality center on this side of the planet. Joining me there is Jeremy Bailenson. He runs the Virtual Human Interaction Lab at Stanford University. And he's written a new book on VR called \"Experience On Demand. \" Full disclosure - he's also a friend. Bailenson's a VR pioneer, but he's never seen anything quite like VR World. JEREMY BAILENSON: I've been doing virtual reality since 1999. If you would've told me back then that I could walk into an arcade like this with literally - I think I see 60 stations here - it's taking me off guard, to be honest. It's really special. GARCIA-NAVARRO: I've never done VR before - like, not even one of those cardboard viewers looking at your phone. BAILENSON: It's a privilege to be here for your first time. GARCIA-NAVARRO: (Laughter). BAILENSON: And we're going to make sure you stay safe, OK? GARCIA-NAVARRO: OK. Safe - but first, of course. . . UNIDENTIFIED MAN #1: We'll just have you just fill out a very basic waiver. GARCIA-NAVARRO: That's in case, while I'm doing virtual reality, I accidentally hurt myself for real. Now it's time for my plank experience. I put on a chunky, black VR headset. And suddenly, I'm standing in an elevator complete with elevator music. (SOUNDBITE OF ELEVATOR MUSIC)KEVIN: And up we go. GARCIA-NAVARRO: The elevator doors open. (SOUNDBITE OF DING)GARCIA-NAVARRO: And I'm at the top of a skyscraper so high that I'm above the clouds, balancing on a tiny, wooden plank with nothing on either side but a massive drop. GARCIA-NAVARRO: Oh, no, no, no, no, no, no, no, no, no, no. UNIDENTIFIED MAN #2: Try looking down. GARCIA-NAVARRO: Oh, my God (laughter). KEVIN: So now you take step out on the plank. GARCIA-NAVARRO: OK. KEVIN: I believe in you. You got this. GARCIA-NAVARRO: I - oh, my God. Yeah, your mind is like, no, no, no, no, no, no. OK. We're done now. Thank you. It's terrifying. Oh, my God. That's terrifying. I could barely take a single step. And that, Bailenson says, is the magic of VR. It actually tricks your brain. BAILENSON: What you just did was sane, actually. Assuming that the brain treats that as real, why would you ever step on that plank in the first place? GARCIA-NAVARRO: Right. BAILENSON: And the whole point of everything that I do from research standpoint - we make a mistake where we talk about VR as a medium. The brain tends to treat the experience as if it were real. GARCIA-NAVARRO: That feeling of realness is what Bailenson wants to harness. As we're about to head upstairs to talk more about his research, Bailenson asks us to hold up. BAILENSON: Now, I want to do this for two minutes. GARCIA-NAVARRO: We stop at a popular VR first-person shooter game, \"Raw Data. \" Bailenson's never played it before because he's a scientist. And he doesn't get to have a lot of fun with VR. So \"Raw Data\" - there's a complex storyline, but the important part is you get to shoot robots. Bailenson gets a headset and two handheld controllers. He's supposed to use them as guns. UNIDENTIFIED MAN #3: Grab your guns at any point in time. . . BAILENSON: Do I have to use guns? Can I play this game without using guns? UNIDENTIFIED MAN #3: No, unfortunately. BAILENSON: The only way to interact with anyone in this game is by using guns? UNIDENTIFIED MAN #3: For this character, yes. BAILENSON: OK. GARCIA-NAVARRO: Bailenson tries it out but never fires a virtual shot. Remember this moment because I'm going to ask him about it later. But for now, we go upstairs, where Bailenson shows me how his lab is using VR. (SOUNDBITE OF VIRTUAL REALITY SIMULATION)UNIDENTIFIED WOMAN #2: Climate change has begun. It is already hurting our planet. GARCIA-NAVARRO: This is one of Bailenson's VR simulations. It transports you to the bottom of the ocean to see the effects of CO2 pollution. In his new book, Bailenson writes about that experiment and the myriad other uses he's found for VR - mitigating the effects of PTSD, developing training software he sells to pro sports teams and creating understanding of some of the world's most complex problems like deforestation and racism. BAILENSON: So imagine that you look down in your NVR, and you see your right arm, and it's a different skin color. We do something called body transfer, which is - you feel like you're someone else. We use a set of exercises that - as you move around and see your body move with you, the brain tends to treat that - we call that an avatar - as part of the self. In other words, you truly feel like this avatar you're wearing is someone else. Then you experience some level of trauma. You experience some prejudice about your race, about your gender, about your age. Perhaps you're disabled. And you experience prejudice firsthand while walking a mile in someone else's shoes. GARCIA-NAVARRO: Experiments like that have proven that VR can build empathy and change the way people think. Bailenson says he thinks that's exactly the kind of thing this powerful tool that affects our perception should be used for. BAILENSON: Save it for things that are impossible to do in the real world - like change your skin color - things that are dangerous to do in the real world, like go down to the bottom of the ocean - and then, of course, things that are super expensive or rare. If five years from now, Lulu, you're checking your email in VR, then I've done something wrong as an evangelist. GARCIA-NAVARRO: So back to that robot game. If you'll recall, Bailenson didn't want to shoot the robots, which makes him, by the way, a terrible gamer. In fact, he has never shot anything in VR because he says there are some real dangers to this technology and how we could use it. BAILENSON: I watch zombie movies. I'm not against watching TV shows that have got all sorts of intense stuff. I love it. And video games - you know, currently, there's a lot of research on desensitization. But you're still kind of using your thumbs on buttons. And there's this level of separation. With VR, you're actually getting the muscle memory to kill. And when it gets done really well, you're actually going to be learning the skills to succeed at violence. You're going to know how exactly to hold a gun and to reload and how to, you know, crouch down for lines of sites. And it's - you know, I can't look you in the eye and say that I truly believe - and I do based on our data - that VR can actually change behaviors about racism and about sexism - but, if you do bad stuff, that it won't equally affect you. I can't maintain that position. GARCIA-NAVARRO: So should it be regulated? BAILENSON: From a legal standpoint, you're saying? GARCIA-NAVARRO: I don't know. I'm asking you, should it be regulated? Should this nascent industry self-regulate? BAILENSON: I spend a lot of time with these companies. I talk to them at the top level - the CEOs. I talk to the - they each have their own lobbyists. And maybe this is cliche. I think they want to do the right thing. My interactions with the VR teams on these companies is they'd be open to that - kind of a self-regulation. I don't think anybody wants to train murderers in VR. GARCIA-NAVARRO: Well, once a technology's out there, what's to stop people from taking it in a direction that could be harmful? We've seen that with social media. BAILENSON: I have very, very little ability to control anything in this world. What I can do is I can talk to people like you. I can be really honest about the downsides. What - you know, functionally, what I can do right now, most importantly, is for friends and family - give them the right kind of guidelines. How long is too long? How young is too young? And, you know, we're struggling with these immediate answers right now. GARCIA-NAVARRO: As someone who's been doing this for a long time, do you feel like this is a moment where this is actually going to change the way that we understand and interact with this medium? BAILENSON: Look. We are in an arcade in New York City. And they opened the doors. There was a line of kids that just came in here and are running around and playing. So this is a little different than anything I've experienced. So I know VR - the joke that everyone's had is the next year, it's going to be in your living rooms. And we've been saying that since the '90s. But this does feel like a special time. GARCIA-NAVARRO: Jeremy Bailenson directs Stanford's Virtual Human Interaction Lab. His new book is \"Experience On Demand\" - out now. By the way, if you're wondering how long is too long or how young is too young for VR, Bailenson told me he's let his 6-year-old daughter try out VR only four times and then only for a couple of minutes on each occasion. (SOUNDBITE OF KETTEL'S \"CLEAR\") LULU GARCIA-NAVARRO, HOST:  Virtual reality hasn't become a reality for most of us yet. So if you want to know how real virtual reality feels, listen as this young girl balances on a virtual plank and then steps off. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED WOMAN #1: Help. Help. No. No. GARCIA-NAVARRO: Don't worry. She survived. But how will I do when I walk the plank? Keep listening. In the last few years, we've seen a new wave of VR technology, from those 360 videos on cardboard viewers to more sophisticated headsets. And now there's the VR arcade. I recently went to check one out in New York City. It's called VR World, advertised as the largest virtual reality center on this side of the planet. Joining me there is Jeremy Bailenson. He runs the Virtual Human Interaction Lab at Stanford University. And he's written a new book on VR called \"Experience On Demand. \" Full disclosure - he's also a friend. Bailenson's a VR pioneer, but he's never seen anything quite like VR World. JEREMY BAILENSON: I've been doing virtual reality since 1999. If you would've told me back then that I could walk into an arcade like this with literally - I think I see 60 stations here - it's taking me off guard, to be honest. It's really special. GARCIA-NAVARRO: I've never done VR before - like, not even one of those cardboard viewers looking at your phone. BAILENSON: It's a privilege to be here for your first time. GARCIA-NAVARRO: (Laughter). BAILENSON: And we're going to make sure you stay safe, OK? GARCIA-NAVARRO: OK. Safe - but first, of course. . . UNIDENTIFIED MAN #1: We'll just have you just fill out a very basic waiver. GARCIA-NAVARRO: That's in case, while I'm doing virtual reality, I accidentally hurt myself for real. Now it's time for my plank experience. I put on a chunky, black VR headset. And suddenly, I'm standing in an elevator complete with elevator music. (SOUNDBITE OF ELEVATOR MUSIC) KEVIN: And up we go. GARCIA-NAVARRO: The elevator doors open. (SOUNDBITE OF DING) GARCIA-NAVARRO: And I'm at the top of a skyscraper so high that I'm above the clouds, balancing on a tiny, wooden plank with nothing on either side but a massive drop. GARCIA-NAVARRO: Oh, no, no, no, no, no, no, no, no, no, no. UNIDENTIFIED MAN #2: Try looking down. GARCIA-NAVARRO: Oh, my God (laughter). KEVIN: So now you take step out on the plank. GARCIA-NAVARRO: OK. KEVIN: I believe in you. You got this. GARCIA-NAVARRO: I - oh, my God. Yeah, your mind is like, no, no, no, no, no, no. OK. We're done now. Thank you. It's terrifying. Oh, my God. That's terrifying. I could barely take a single step. And that, Bailenson says, is the magic of VR. It actually tricks your brain. BAILENSON: What you just did was sane, actually. Assuming that the brain treats that as real, why would you ever step on that plank in the first place? GARCIA-NAVARRO: Right. BAILENSON: And the whole point of everything that I do from research standpoint - we make a mistake where we talk about VR as a medium. The brain tends to treat the experience as if it were real. GARCIA-NAVARRO: That feeling of realness is what Bailenson wants to harness. As we're about to head upstairs to talk more about his research, Bailenson asks us to hold up. BAILENSON: Now, I want to do this for two minutes. GARCIA-NAVARRO: We stop at a popular VR first-person shooter game, \"Raw Data. \" Bailenson's never played it before because he's a scientist. And he doesn't get to have a lot of fun with VR. So \"Raw Data\" - there's a complex storyline, but the important part is you get to shoot robots. Bailenson gets a headset and two handheld controllers. He's supposed to use them as guns. UNIDENTIFIED MAN #3: Grab your guns at any point in time. . . BAILENSON: Do I have to use guns? Can I play this game without using guns? UNIDENTIFIED MAN #3: No, unfortunately. BAILENSON: The only way to interact with anyone in this game is by using guns? UNIDENTIFIED MAN #3: For this character, yes. BAILENSON: OK. GARCIA-NAVARRO: Bailenson tries it out but never fires a virtual shot. Remember this moment because I'm going to ask him about it later. But for now, we go upstairs, where Bailenson shows me how his lab is using VR. (SOUNDBITE OF VIRTUAL REALITY SIMULATION) UNIDENTIFIED WOMAN #2: Climate change has begun. It is already hurting our planet. GARCIA-NAVARRO: This is one of Bailenson's VR simulations. It transports you to the bottom of the ocean to see the effects of CO2 pollution. In his new book, Bailenson writes about that experiment and the myriad other uses he's found for VR - mitigating the effects of PTSD, developing training software he sells to pro sports teams and creating understanding of some of the world's most complex problems like deforestation and racism. BAILENSON: So imagine that you look down in your NVR, and you see your right arm, and it's a different skin color. We do something called body transfer, which is - you feel like you're someone else. We use a set of exercises that - as you move around and see your body move with you, the brain tends to treat that - we call that an avatar - as part of the self. In other words, you truly feel like this avatar you're wearing is someone else. Then you experience some level of trauma. You experience some prejudice about your race, about your gender, about your age. Perhaps you're disabled. And you experience prejudice firsthand while walking a mile in someone else's shoes. GARCIA-NAVARRO: Experiments like that have proven that VR can build empathy and change the way people think. Bailenson says he thinks that's exactly the kind of thing this powerful tool that affects our perception should be used for. BAILENSON: Save it for things that are impossible to do in the real world - like change your skin color - things that are dangerous to do in the real world, like go down to the bottom of the ocean - and then, of course, things that are super expensive or rare. If five years from now, Lulu, you're checking your email in VR, then I've done something wrong as an evangelist. GARCIA-NAVARRO: So back to that robot game. If you'll recall, Bailenson didn't want to shoot the robots, which makes him, by the way, a terrible gamer. In fact, he has never shot anything in VR because he says there are some real dangers to this technology and how we could use it. BAILENSON: I watch zombie movies. I'm not against watching TV shows that have got all sorts of intense stuff. I love it. And video games - you know, currently, there's a lot of research on desensitization. But you're still kind of using your thumbs on buttons. And there's this level of separation. With VR, you're actually getting the muscle memory to kill. And when it gets done really well, you're actually going to be learning the skills to succeed at violence. You're going to know how exactly to hold a gun and to reload and how to, you know, crouch down for lines of sites. And it's - you know, I can't look you in the eye and say that I truly believe - and I do based on our data - that VR can actually change behaviors about racism and about sexism - but, if you do bad stuff, that it won't equally affect you. I can't maintain that position. GARCIA-NAVARRO: So should it be regulated? BAILENSON: From a legal standpoint, you're saying? GARCIA-NAVARRO: I don't know. I'm asking you, should it be regulated? Should this nascent industry self-regulate? BAILENSON: I spend a lot of time with these companies. I talk to them at the top level - the CEOs. I talk to the - they each have their own lobbyists. And maybe this is cliche. I think they want to do the right thing. My interactions with the VR teams on these companies is they'd be open to that - kind of a self-regulation. I don't think anybody wants to train murderers in VR. GARCIA-NAVARRO: Well, once a technology's out there, what's to stop people from taking it in a direction that could be harmful? We've seen that with social media. BAILENSON: I have very, very little ability to control anything in this world. What I can do is I can talk to people like you. I can be really honest about the downsides. What - you know, functionally, what I can do right now, most importantly, is for friends and family - give them the right kind of guidelines. How long is too long? How young is too young? And, you know, we're struggling with these immediate answers right now. GARCIA-NAVARRO: As someone who's been doing this for a long time, do you feel like this is a moment where this is actually going to change the way that we understand and interact with this medium? BAILENSON: Look. We are in an arcade in New York City. And they opened the doors. There was a line of kids that just came in here and are running around and playing. So this is a little different than anything I've experienced. So I know VR - the joke that everyone's had is the next year, it's going to be in your living rooms. And we've been saying that since the '90s. But this does feel like a special time. GARCIA-NAVARRO: Jeremy Bailenson directs Stanford's Virtual Human Interaction Lab. His new book is \"Experience On Demand\" - out now. By the way, if you're wondering how long is too long or how young is too young for VR, Bailenson told me he's let his 6-year-old daughter try out VR only four times and then only for a couple of minutes on each occasion. (SOUNDBITE OF KETTEL'S \"CLEAR\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-02-05-583461873": {"title": "Why The Pyeongchang Olympics Are Also A Tech Showcase For South Korea : NPR", "url": "https://www.npr.org/2018/02/05/583461873/why-the-pyeongchang-olympics-are-also-a-tech-showcase-for-south-korea", "author": "No author found", "published_date": "2018-02-05", "content": "ARI SHAPIRO, HOST: The Winter Olympics in South Korea will be a chance for the country to show off its newest robot creations. NPR's Elise Hu reports that the peek into the future starts as soon as you land at the airport. ELISE HU, BYLINE: We're in the arrival hall of Terminal 1 of the Incheon International Airport. It is the main airport of Seoul. And a robot is rolling up upon us in the arrival hall with a theme song. (SOUNDBITE OF MUSIC)HU: It's about 5 feet tall, smooth, white. Cylindrical with a swiveling dome-shaped head and a tablet screen for a face. There's a vertical screen on the front of its body. And here we find out its name is Airstar. It cruises around on a base that resembles a Roomba floor cleaner. And Airstar's main purpose is to guide you where you need to go at the airport. COMPUTER-GENERATED VOICE: Let's start. Speak into the vocal guide or touch the button below. HU: Pyeongchang Olympic Game desk. COMPUTER-GENERATED VOICE: The route to your destination is shown below. HU: Yes, but it also says request escorting, so I'm going to request that Airstar take me. COMPUTER-GENERATED VOICE: Preparing to escort you. Please wait a moment. HU: For South Korea, home of Samsung and LG Electronics, super-fast Internet speeds and heavy government investment in science and tech, this international sporting event is also a tech showcase. LORNA CAMPBELL: The robotic things are amazing. HU: Lorna Campbell is a spokeswoman for the Pyeongchang Olympic organizing committee. In all, some 85 robots are working the games, doing everything from carpet cleaning to serving snacks. . . CAMPBELL: And you can open the front of them, and they've got cookies and biscuits inside. HU: . . . To painting murals of MVPs. CAMPBELL: It's very Korean. So Korea is known across Asia as, like, probably the most technologically advanced country. They've always been one step ahead. They've got all the research and development centers. HU: The games give Korean engineers a chance to really see how their robot creations interact. Airstar, their airport robot, can speak six languages. But things still get lost in translation. How do I get to the train station? COMPUTER-GENERATED VOICE: I did not understand what you said. HU: Just as we hit this glitch, we spot a young man in a blue hoodie tailing Airstar and typing into his tablet. He's Ee Joo-hyung (ph), and he tells us he's a robot caretaker. He follows Airstar around all day, logging its interactions as well as potential bugs. EE JOO-HYUNG: (Through interpreter) The robots were functioning well in the labs, but once they're actually interacting with people, things might be different. HU: He admits the robots still need a bit more work before they're perfect. And that makes sense, says Sam Byford, Asia editor for the tech site The Verge. SAM BYFORD: I tend to be skeptical of things that are sort of tied to major sporting events and that kind of thing because it's a pretty arbitrary time to show these things off, right? Like, it's not going to be tied to the actual research breakthrough. HU: But he says he expected Korea to use this opportunity to show and tell its tech story, especially since the next Olympics will be hosted by Japan, a fierce competitor in robotics. BYFORD: Robotics in general is always about baby steps. HU: Airstar the airport robot glides more than it steps, really. And it did eventually guide me to the Olympic help desk at the airport, where I found humans to talk with. And then now she says, forgive me, but I must return home. (SOUNDBITE OF MUSIC)HU: And her screen says bye. (SOUNDBITE OF MUSIC)HU: Elise Hu, NPR News, Incheon, South Korea. ARI SHAPIRO, HOST:  The Winter Olympics in South Korea will be a chance for the country to show off its newest robot creations. NPR's Elise Hu reports that the peek into the future starts as soon as you land at the airport. ELISE HU, BYLINE: We're in the arrival hall of Terminal 1 of the Incheon International Airport. It is the main airport of Seoul. And a robot is rolling up upon us in the arrival hall with a theme song. (SOUNDBITE OF MUSIC) HU: It's about 5 feet tall, smooth, white. Cylindrical with a swiveling dome-shaped head and a tablet screen for a face. There's a vertical screen on the front of its body. And here we find out its name is Airstar. It cruises around on a base that resembles a Roomba floor cleaner. And Airstar's main purpose is to guide you where you need to go at the airport. COMPUTER-GENERATED VOICE: Let's start. Speak into the vocal guide or touch the button below. HU: Pyeongchang Olympic Game desk. COMPUTER-GENERATED VOICE: The route to your destination is shown below. HU: Yes, but it also says request escorting, so I'm going to request that Airstar take me. COMPUTER-GENERATED VOICE: Preparing to escort you. Please wait a moment. HU: For South Korea, home of Samsung and LG Electronics, super-fast Internet speeds and heavy government investment in science and tech, this international sporting event is also a tech showcase. LORNA CAMPBELL: The robotic things are amazing. HU: Lorna Campbell is a spokeswoman for the Pyeongchang Olympic organizing committee. In all, some 85 robots are working the games, doing everything from carpet cleaning to serving snacks. . . CAMPBELL: And you can open the front of them, and they've got cookies and biscuits inside. HU: . . . To painting murals of MVPs. CAMPBELL: It's very Korean. So Korea is known across Asia as, like, probably the most technologically advanced country. They've always been one step ahead. They've got all the research and development centers. HU: The games give Korean engineers a chance to really see how their robot creations interact. Airstar, their airport robot, can speak six languages. But things still get lost in translation. How do I get to the train station? COMPUTER-GENERATED VOICE: I did not understand what you said. HU: Just as we hit this glitch, we spot a young man in a blue hoodie tailing Airstar and typing into his tablet. He's Ee Joo-hyung (ph), and he tells us he's a robot caretaker. He follows Airstar around all day, logging its interactions as well as potential bugs. EE JOO-HYUNG: (Through interpreter) The robots were functioning well in the labs, but once they're actually interacting with people, things might be different. HU: He admits the robots still need a bit more work before they're perfect. And that makes sense, says Sam Byford, Asia editor for the tech site The Verge. SAM BYFORD: I tend to be skeptical of things that are sort of tied to major sporting events and that kind of thing because it's a pretty arbitrary time to show these things off, right? Like, it's not going to be tied to the actual research breakthrough. HU: But he says he expected Korea to use this opportunity to show and tell its tech story, especially since the next Olympics will be hosted by Japan, a fierce competitor in robotics. BYFORD: Robotics in general is always about baby steps. HU: Airstar the airport robot glides more than it steps, really. And it did eventually guide me to the Olympic help desk at the airport, where I found humans to talk with. And then now she says, forgive me, but I must return home. (SOUNDBITE OF MUSIC) HU: And her screen says bye. (SOUNDBITE OF MUSIC) HU: Elise Hu, NPR News, Incheon, South Korea.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-02-05-583367084": {"title": "Battle Over Self-Driving Cars Goes To Court In Case Between Google's Waymo And Uber : NPR", "url": "https://www.npr.org/2018/02/05/583367084/battle-over-self-driving-cars-goes-to-court-in-case-between-googles-waymo-and-ub", "author": "No author found", "published_date": "2018-02-05", "content": "", "section": "Business", "disclaimer": ""}, "2018-02-06-583778394": {"title": "Positive Train Control Technology Could Be A Solution For Amtrak After Recent Crashes : NPR", "url": "https://www.npr.org/2018/02/06/583778394/positive-train-control-technology-could-be-a-solution-for-amtrak-after-recent-cr", "author": "No author found", "published_date": "2018-02-06", "content": "AILSA CHANG, HOST:  This past weekend, there was another fatal crash involving a passenger train. It was the third since December. As the investigation got started, the head of the National Transportation Safety Board said the U. S. needs more of what's called Positive Train Control, or PTC. The board says this will ensure train safety, and if it had been in place this past weekend, it might have helped. Joining us to talk about safety and trains is William Vantuono. He's the editor-in-chief of Railway Age and joins us from our studios in New York. Thanks for joining us. WILLIAM VANTUONO: You're very welcome, glad to be here. CHANG: So Positive Train Control is a phrase that's been thrown around a lot. What exactly is it? VANTUONO: It is a safety overlay system on the existing signaling and train control system in the U. S. that is designed to prevent accidents caused by human error. CHANG: OK, but how? It's - that stops the train immediately when. . . VANTUONO: Yeah. Well, there are four parts to PTC. It's designed to prevent train-to-train collisions, overspeed derailments - or excessive speed derailments - unauthorized incursions onto track maintenance work zones and movement of a train through a track switch that has been left in the wrong position, which is what happened with the Amtrak Silver Star wreck. CHANG: And what are the limits of PTC, as you call it? I mean, give me an idea of what kinds of accident it cannot prevent. VANTUONO: Well, putting it into context, PTC-preventable accidents represent about 4 percent of all railroad accidents that occur - that have occurred. So what we're talking about here are accidents that are caused by human error. Overspeed condition where there is no enforcement, where - for example, with the wreck of the Amtrak train 501 in December, the Cascades train, the train entered a 30-mile-an-hour curve at close to 80 miles an hour. The engineer claims that he just basically missed the sign. There were signals, warning signs, and he kind of blew through them, which indicates inattention or distraction. And a Positive Train Control system would have sensed that the train was traveling too fast, that it was approaching the curve at too fast a rate of speed to break safely and then would have applied the brakes. CHANG: How much of the train activity in the country would be directly affected by Positive Train Control? VANTUONO: Approximately 25 percent, which is a significant chunk, yeah. And we're looking at around 60,000 route miles of railroad. CHANG: And it would be primarily for passenger trains. VANTUONO: Yeah, passenger traffic and, in terms of freight, hazmat traffic - chlorine, crude oil, ethanol, things that can catch fire or explode. CHANG: I know you attended a meeting yesterday for the Association of American Railroads about Positive Train Control. Did you get a sense of how likely is that PTC will get up and running anytime soon? VANTUONO: By the end of this year, by law, slightly over 50 percent of PTC territory or route - in terms of route miles - must be implemented, up and running. All hardware needs to be installed for the entire system. All radio spectrum needs to be acquired. And all employee training must be completed. So what this means is that two years following this year's deadline, the end of 2020, all testing will be completed and the entire PTC system will be up and running. CHANG: William Vantuono is editor-in-chief of Railway Age. Thanks for joining us. VANTUONO: You're very welcome. (SOUNDBITE OF MASSIVE ATTACK'S \"MONTAGE\") AILSA CHANG, HOST:   This past weekend, there was another fatal crash involving a passenger train. It was the third since December. As the investigation got started, the head of the National Transportation Safety Board said the U. S. needs more of what's called Positive Train Control, or PTC. The board says this will ensure train safety, and if it had been in place this past weekend, it might have helped. Joining us to talk about safety and trains is William Vantuono. He's the editor-in-chief of Railway Age and joins us from our studios in New York. Thanks for joining us. WILLIAM VANTUONO: You're very welcome, glad to be here. CHANG: So Positive Train Control is a phrase that's been thrown around a lot. What exactly is it? VANTUONO: It is a safety overlay system on the existing signaling and train control system in the U. S. that is designed to prevent accidents caused by human error. CHANG: OK, but how? It's - that stops the train immediately when. . . VANTUONO: Yeah. Well, there are four parts to PTC. It's designed to prevent train-to-train collisions, overspeed derailments - or excessive speed derailments - unauthorized incursions onto track maintenance work zones and movement of a train through a track switch that has been left in the wrong position, which is what happened with the Amtrak Silver Star wreck. CHANG: And what are the limits of PTC, as you call it? I mean, give me an idea of what kinds of accident it cannot prevent. VANTUONO: Well, putting it into context, PTC-preventable accidents represent about 4 percent of all railroad accidents that occur - that have occurred. So what we're talking about here are accidents that are caused by human error. Overspeed condition where there is no enforcement, where - for example, with the wreck of the Amtrak train 501 in December, the Cascades train, the train entered a 30-mile-an-hour curve at close to 80 miles an hour. The engineer claims that he just basically missed the sign. There were signals, warning signs, and he kind of blew through them, which indicates inattention or distraction. And a Positive Train Control system would have sensed that the train was traveling too fast, that it was approaching the curve at too fast a rate of speed to break safely and then would have applied the brakes. CHANG: How much of the train activity in the country would be directly affected by Positive Train Control? VANTUONO: Approximately 25 percent, which is a significant chunk, yeah. And we're looking at around 60,000 route miles of railroad. CHANG: And it would be primarily for passenger trains. VANTUONO: Yeah, passenger traffic and, in terms of freight, hazmat traffic - chlorine, crude oil, ethanol, things that can catch fire or explode. CHANG: I know you attended a meeting yesterday for the Association of American Railroads about Positive Train Control. Did you get a sense of how likely is that PTC will get up and running anytime soon? VANTUONO: By the end of this year, by law, slightly over 50 percent of PTC territory or route - in terms of route miles - must be implemented, up and running. All hardware needs to be installed for the entire system. All radio spectrum needs to be acquired. And all employee training must be completed. So what this means is that two years following this year's deadline, the end of 2020, all testing will be completed and the entire PTC system will be up and running. CHANG: William Vantuono is editor-in-chief of Railway Age. Thanks for joining us. VANTUONO: You're very welcome. (SOUNDBITE OF MASSIVE ATTACK'S \"MONTAGE\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-02-06-583648004": {"title": "Former Google Executive Warns Smartphones Keep Us Hooked : NPR", "url": "https://www.npr.org/2018/02/06/583648004/former-google-executive-warns-smartphones-keep-us-hooked", "author": "No author found", "published_date": "2018-02-06", "content": "STEVE INSKEEP, HOST: And I'm Steve Inskeep with a man who changed his mind about the Internet. His name is Tristan Harris. He once ran an Internet firm, and then Google bought him out. Suddenly, he was at work for them, for the big guys. And he was among those who believed that the Internet was a force for good. Now, Harris says, he has changed his mind. He is founder of the Center for Humane Technology, which is urging people this week to be aware of how they are manipulated. TRISTAN HARRIS: I always had this dream that technology was an empowering tool - you know, a bicycle for our minds. And then I, you know, in college, studied at this lab called the Persuasive Technology Lab that basically taught engineering students how to think about technology in a different way, in terms of how it can persuade us to do things. And I realized that it was less and less about actually trying to benefit people and more and more about just, how do we keep people hooked? INSKEEP: That's a beautiful analogy - the bicycle for the mind. I think you're telling me that you discovered, instead, that technology was a self-driving car that is taking us places without asking first. Isn't it true that even if the phone is manipulative - the apps on the phone are manipulative - people are still making a choice? HARRIS: Well, I think choice is a really interesting thing. You know, when I was a kid, I was a magician. And you know, you say, hey, so you picked, you know, whether it was a red card or a black card, did you not? And then you say that's right. And then they say, you picked whether it was a number card or a face card, did you not? And so therefore, you chose this card, correct? I didn't influence your choice in any way. And yet, at the same time, I know as a magician exactly what card you picked. And just like a magician, these products design menus in which all of the choices lead to more time on the screen. INSKEEP: Are you trying to be the equivalent of a cigarette warning label? HARRIS: No. We are trying to completely change the incentives away from addiction. And the way to do that is to change the business model which is possible for Facebook and other companies to make more money with subscription-based models. I think so many people would switch to that because it would be so much better for them at the beginning. So many people feel so distracting to have, you know, every aspect of the design to be built to distract them, to pull them in, to suck them in. You're not paying to remove the ads. You're paying to have the thousand engineers in the other side of the screen see you as the customer instead of seeing your eyeballs as the customer for advertisers. INSKEEP: Is there, in a sense, a hardware problem, though? Because the phone is right there. It's in your pocket. It's right with you. You may even sleep beside it. Since it's open, I'll check my text messages. And since I've done that, I'll check Facebook and Twitter and a couple of other things. HARRIS: Yeah, it's a very hard problem to solve. And actually, you know, one of the key players that can really make a difference here is not the attention companies. They're locked into this race to capture as much attention as possible. But I've been saying for four years that our biggest friends here and perhaps our only hope here are companies like Apple and Samsung because they make the devices, and they can design them differently. You know, one of the things we advocated for is change your phone to grayscale. INSKEEP: I'm doing that right now. So how do I do this? I'm going to settings. HARRIS: Opening it myself here. It's general, accessibility, display accommodations - color filters, excuse me. . . INSKEEP: OK, there we go. HARRIS: . . . And then turn that on, and then the first filter is called grayscale. INSKEEP: Done. And so this step is going to make my phone seem a little less appealing, a little less addictive. HARRIS: If you leave it in black and white for just long enough, you'll start to get used to it, and then color will feel overwhelming. It'll actually feel like, wow, this is just too much color. INSKEEP: That's OK. I mean, I like the black and white. It makes me feel like I'm in an old film noir. Would you be willing to go so far as to warn tech companies that if they don't address this problem, it will become a public health issue that will affect their bottom line? HARRIS: Well, absolutely. I mean, I think this has actually already happened. Apple had several of its shareholders basically write them a letter saying you have a huge public health problem on your hand. And so I think this legal risk is actually already happening, and companies again like Apple and Samsung, whose business models are not to maximize addiction or attention, can actually do a lot to help protect against this public health problem. That's what we're advocating for. INSKEEP: Can you foresee the day when the California attorney general, say, would file a lawsuit against Apple? HARRIS: That could be possible. I think there's many states that would consider such a thing. INSKEEP: We've been talking with Tristan Harris, who once worked for Google and now runs the Center for Humane Technology. Thanks. HARRIS: Thank you. INSKEEP: Their new campaign, together with the children's advocacy group Common Sense Media, is called Truth About Tech. STEVE INSKEEP, HOST:  And I'm Steve Inskeep with a man who changed his mind about the Internet. His name is Tristan Harris. He once ran an Internet firm, and then Google bought him out. Suddenly, he was at work for them, for the big guys. And he was among those who believed that the Internet was a force for good. Now, Harris says, he has changed his mind. He is founder of the Center for Humane Technology, which is urging people this week to be aware of how they are manipulated. TRISTAN HARRIS: I always had this dream that technology was an empowering tool - you know, a bicycle for our minds. And then I, you know, in college, studied at this lab called the Persuasive Technology Lab that basically taught engineering students how to think about technology in a different way, in terms of how it can persuade us to do things. And I realized that it was less and less about actually trying to benefit people and more and more about just, how do we keep people hooked? INSKEEP: That's a beautiful analogy - the bicycle for the mind. I think you're telling me that you discovered, instead, that technology was a self-driving car that is taking us places without asking first. Isn't it true that even if the phone is manipulative - the apps on the phone are manipulative - people are still making a choice? HARRIS: Well, I think choice is a really interesting thing. You know, when I was a kid, I was a magician. And you know, you say, hey, so you picked, you know, whether it was a red card or a black card, did you not? And then you say that's right. And then they say, you picked whether it was a number card or a face card, did you not? And so therefore, you chose this card, correct? I didn't influence your choice in any way. And yet, at the same time, I know as a magician exactly what card you picked. And just like a magician, these products design menus in which all of the choices lead to more time on the screen. INSKEEP: Are you trying to be the equivalent of a cigarette warning label? HARRIS: No. We are trying to completely change the incentives away from addiction. And the way to do that is to change the business model which is possible for Facebook and other companies to make more money with subscription-based models. I think so many people would switch to that because it would be so much better for them at the beginning. So many people feel so distracting to have, you know, every aspect of the design to be built to distract them, to pull them in, to suck them in. You're not paying to remove the ads. You're paying to have the thousand engineers in the other side of the screen see you as the customer instead of seeing your eyeballs as the customer for advertisers. INSKEEP: Is there, in a sense, a hardware problem, though? Because the phone is right there. It's in your pocket. It's right with you. You may even sleep beside it. Since it's open, I'll check my text messages. And since I've done that, I'll check Facebook and Twitter and a couple of other things. HARRIS: Yeah, it's a very hard problem to solve. And actually, you know, one of the key players that can really make a difference here is not the attention companies. They're locked into this race to capture as much attention as possible. But I've been saying for four years that our biggest friends here and perhaps our only hope here are companies like Apple and Samsung because they make the devices, and they can design them differently. You know, one of the things we advocated for is change your phone to grayscale. INSKEEP: I'm doing that right now. So how do I do this? I'm going to settings. HARRIS: Opening it myself here. It's general, accessibility, display accommodations - color filters, excuse me. . . INSKEEP: OK, there we go. HARRIS: . . . And then turn that on, and then the first filter is called grayscale. INSKEEP: Done. And so this step is going to make my phone seem a little less appealing, a little less addictive. HARRIS: If you leave it in black and white for just long enough, you'll start to get used to it, and then color will feel overwhelming. It'll actually feel like, wow, this is just too much color. INSKEEP: That's OK. I mean, I like the black and white. It makes me feel like I'm in an old film noir. Would you be willing to go so far as to warn tech companies that if they don't address this problem, it will become a public health issue that will affect their bottom line? HARRIS: Well, absolutely. I mean, I think this has actually already happened. Apple had several of its shareholders basically write them a letter saying you have a huge public health problem on your hand. And so I think this legal risk is actually already happening, and companies again like Apple and Samsung, whose business models are not to maximize addiction or attention, can actually do a lot to help protect against this public health problem. That's what we're advocating for. INSKEEP: Can you foresee the day when the California attorney general, say, would file a lawsuit against Apple? HARRIS: That could be possible. I think there's many states that would consider such a thing. INSKEEP: We've been talking with Tristan Harris, who once worked for Google and now runs the Center for Humane Technology. Thanks. HARRIS: Thank you. INSKEEP: Their new campaign, together with the children's advocacy group Common Sense Media, is called Truth About Tech.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-02-08-583693730": {"title": "How To Pack A Space Telescope : NPR", "url": "https://www.npr.org/2018/02/08/583693730/how-to-pack-a-space-telescope", "author": "No author found", "published_date": "2018-02-08", "content": "DAVID GREENE, HOST: The most-powerful space telescope ever built has been on a road trip across the U. S. When it launches next year, NASA expects this $8. 8 billion telescope to revolutionize astronomy, but it may have already revolutionized something else - the art of packing for a vacation. Because if you can pack up a massive telescope, I mean, cramming clothes, shoes, toiletries into a suitcase is nothing, right? NPR science correspondent Joe Palca was hanging out at the Johnson Space Center in Houston as people prepared a massive telescope for the latest leg of its journey. JOE PALCA, BYLINE: You don't just stuff an $8. 8 billion telescope into a box when you're ready to ship it. Everything has to be done in a clean room, so no dust or debris will get on the telescopes, mirrors or into its sensitive instruments. On this day, the telescope is sitting upright on a stand in a corner of the clean room while technicians are readying its shipping container. NEAL PATEL: So right now what they're doing is they're just lifting the lid off. They have this specially-designed beam to lift it off of there. PALCA: Neal Patel is a mechanical engineer. He specializes in moving spacecraft while they're on Earth. The shipping container is basically a flat pallet 110 feet long with a frame on top. There's a dome-like lid they're about to remove. The room grows silent. PATEL: Why everyone's really quiet right now is just 'cause it's a critical op. Any time they're lifting anything overhead, it's a critical op. PALCA: Engineers like Patel are being really, really careful in everything they do related to the telescope because they really hate to disappoint astrophysicists like Jackie Faherty. JACKIE FAHERTY: For my own personal science, I'm ridiculously excited. PALCA: Faherty is at the American Museum of Natural History. She studies space objects known as brown dwarfs. But Faherty says there's a ton more astronomers expect to learn using Webb. FAHERTY: From the first moments when the universe was created to what we're going to get to know about exoplanets that are orbiting other stars that may have habitable life. PALCA: Back in the clean room, I watch as the 10,000-pound container lid rises ever so slowly. (SOUNDBITE OF MECHANICAL LIFT)PALCA: Once it's clear of the pallet, things speed up. I wander over to the telescope sitting in the corner of the room. Folded up, it's about the size of a large school bus. It has a black carbon fiber body. But most striking is the 18 large gold hexagonal mirrors that will collect the starlight. There's three large mirrors right above my head. These mirrors will collect six times more light than the extremely successful Hubble Space Telescope. The next day, technicians began the process of unfastening the telescope from the stand it's been sitting on. The stand had been rotated so the telescope was now horizontal. Charlie Diaz is in charge of the entire move operation. He's been working on the logistics of moving the Webb telescope for more than a decade. Everything is meticulously planned. Diaz says the next step in today's operation is to remove the 16 bolts holding the telescope to its stand. CHARLIE DIAZ: You can't just, you know, remove a bolt. You have to loosen it up, then loosen another one. This way, they're uniformly released so there are no stresses. PALCA: It's a slow process, very slow. SANDRA IRISH: We like it to go slow and steady. PALCA: Sandra Irish is an aerospace engineer. Her job is to make sure the telescope isn't subjected to any stresses it can't handle. It was designed to operate in the essentially weightless environment of space. Here on Earth, gravity is a problem. Ultimately, the bolts come out. The telescope slides off the stand. It's raised up and then lowered ever so gently into its container. All the while, Sandra Irish has been watching the operation intently. I asked her if she was happy with the way the day was going. IRISH: I think I'll being the most happy once we put on the frame and the lid and we can ship out. So - but yeah, so far, so good. PALCA: The rest of the day went well. And a few days later, the telescope was loaded into a special C-5 cargo jet and flown to Los Angeles. It's now safely in another clean room where it will be readied for launch next year. Joe Palca, NPR News. DAVID GREENE, HOST:  The most-powerful space telescope ever built has been on a road trip across the U. S. When it launches next year, NASA expects this $8. 8 billion telescope to revolutionize astronomy, but it may have already revolutionized something else - the art of packing for a vacation. Because if you can pack up a massive telescope, I mean, cramming clothes, shoes, toiletries into a suitcase is nothing, right? NPR science correspondent Joe Palca was hanging out at the Johnson Space Center in Houston as people prepared a massive telescope for the latest leg of its journey. JOE PALCA, BYLINE: You don't just stuff an $8. 8 billion telescope into a box when you're ready to ship it. Everything has to be done in a clean room, so no dust or debris will get on the telescopes, mirrors or into its sensitive instruments. On this day, the telescope is sitting upright on a stand in a corner of the clean room while technicians are readying its shipping container. NEAL PATEL: So right now what they're doing is they're just lifting the lid off. They have this specially-designed beam to lift it off of there. PALCA: Neal Patel is a mechanical engineer. He specializes in moving spacecraft while they're on Earth. The shipping container is basically a flat pallet 110 feet long with a frame on top. There's a dome-like lid they're about to remove. The room grows silent. PATEL: Why everyone's really quiet right now is just 'cause it's a critical op. Any time they're lifting anything overhead, it's a critical op. PALCA: Engineers like Patel are being really, really careful in everything they do related to the telescope because they really hate to disappoint astrophysicists like Jackie Faherty. JACKIE FAHERTY: For my own personal science, I'm ridiculously excited. PALCA: Faherty is at the American Museum of Natural History. She studies space objects known as brown dwarfs. But Faherty says there's a ton more astronomers expect to learn using Webb. FAHERTY: From the first moments when the universe was created to what we're going to get to know about exoplanets that are orbiting other stars that may have habitable life. PALCA: Back in the clean room, I watch as the 10,000-pound container lid rises ever so slowly. (SOUNDBITE OF MECHANICAL LIFT) PALCA: Once it's clear of the pallet, things speed up. I wander over to the telescope sitting in the corner of the room. Folded up, it's about the size of a large school bus. It has a black carbon fiber body. But most striking is the 18 large gold hexagonal mirrors that will collect the starlight. There's three large mirrors right above my head. These mirrors will collect six times more light than the extremely successful Hubble Space Telescope. The next day, technicians began the process of unfastening the telescope from the stand it's been sitting on. The stand had been rotated so the telescope was now horizontal. Charlie Diaz is in charge of the entire move operation. He's been working on the logistics of moving the Webb telescope for more than a decade. Everything is meticulously planned. Diaz says the next step in today's operation is to remove the 16 bolts holding the telescope to its stand. CHARLIE DIAZ: You can't just, you know, remove a bolt. You have to loosen it up, then loosen another one. This way, they're uniformly released so there are no stresses. PALCA: It's a slow process, very slow. SANDRA IRISH: We like it to go slow and steady. PALCA: Sandra Irish is an aerospace engineer. Her job is to make sure the telescope isn't subjected to any stresses it can't handle. It was designed to operate in the essentially weightless environment of space. Here on Earth, gravity is a problem. Ultimately, the bolts come out. The telescope slides off the stand. It's raised up and then lowered ever so gently into its container. All the while, Sandra Irish has been watching the operation intently. I asked her if she was happy with the way the day was going. IRISH: I think I'll being the most happy once we put on the frame and the lid and we can ship out. So - but yeah, so far, so good. PALCA: The rest of the day went well. And a few days later, the telescope was loaded into a special C-5 cargo jet and flown to Los Angeles. It's now safely in another clean room where it will be readied for launch next year. Joe Palca, NPR News.", "section": "Joe's Big Idea", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-02-08-584144083": {"title": "Tillerson Warns Russia Is Meddling In 2018 Midterms, Methods Evolving : NPR", "url": "https://www.npr.org/2018/02/08/584144083/tracking-shows-russian-meddling-efforts-evolving-ahead-of-2018-midterms", "author": "No author found", "published_date": "2018-02-08", "content": "STEVE INSKEEP, HOST: With the midterm elections approaching, Secretary of State Rex Tillerson warned on Fox News this week that Russia is trying to meddle again. (SOUNDBITE OF FOX NEWS BROADCAST)REX TILLERSON: I think it's important we just continue to say to Russia, look, if you think we don't see what you're doing, we do see it, and you need to stop. INSKEEP: Tillerson's boss, President Trump, has downplayed Russian interference in the 2016 election and said that Russia's president, quote, \"means it\" when he denies meddling. Other officials say Russia did interfere and that its methods have evolved. NPR's Tim Mak went behind the scenes with a group tracking the latest tactics. (SOUNDBITE OF TV SHOW, \"HANNITY\")SEAN HANNITY: This makes Watergate like stealing a Snickers bar from a drugstore. SARA CARTER: Absolutely, and. . . TIM MAK, BYLINE: That's Sean Hannity, host of a popular Fox News show, talking about the Nunes memo. It's a document prepared by GOP members of the House intelligence committee that President Trump says vindicates him in ongoing investigations. As the debate over whether to release the memo seized Congress, an initiative monitoring Russian influence operations noticed a sharp uptick in the number of automated Twitter bots pushing the #ReleaseTheMemo hashtag. BRET SCHAFER: . . . At a level we have not seen with any other topic. MAK: That's Bret Schafer, an analyst for Hamilton 68, a bipartisan project backed by the German Marshall Fund of the United States. Schafer says #ReleaseTheMemo was an instance of Russian-linked networks trying to erode faith in American institutions. Hamilton 68 is led by an unlikely pair - Laura Rosenberger. . . LAURA ROSENBERGER: I was Hillary Clinton's foreign policy adviser during her 2016 presidential campaign, and before that I worked. . . MAK: . . . And Jamie Fly. JAMIE FLY: I was Marco Rubio's foreign policy adviser in the Senate and worked with him as his adviser during his presidential campaign. MAK: Starting six months ago, the project identified and monitored 600 Twitter accounts linked to Russian influence operations. Here's Fly discussing the primary purpose of these networks. FLY: These are not networks that are necessarily pushing always traditional propaganda. A lot of it is actually just trying to rip apart Americans, to sow chaos within our political system, to pit Americans of both parties against each other. MAK: The Hamilton 68 project saw a lot of Kremlin-linked activity on recent controversies like NFL players kneeling during the national anthem, the Charlottesville protests and the immigration debate swirling around DACA. Here's Rosenberger. ROSENBERGER: We have now realized that an adversary has discovered significant vulnerabilities that can be exploited, and we need to really wrestle with taking steps to make sure that those vulnerabilities can't be exploited and that the real potential of, you know, social media platforms can be sort of maximized. MAK: But the Russian influence networks are now doing more than just creating chaos. They are pushing Russian propaganda to Americans using intriguing techniques. SCHAFER: Oftentimes we see topics that really have no importance to Moscow or any of their interests. They're just shared and - as a way of gaining an audience. MAK: Schafer said he noticed the trend when he saw a number of Russian-linked accounts start tweeting about #MondayMotivation and #WednesdayWisdom popular hashtags used by actual Americans on Twitter. They were mixed in with content about Syria and Ukraine, two countries where Russia is involved with military interventions. It is, in essence, a Russian social media marketing technique using trending topics to get eyeballs on their core message. SCHAFER: If they just started shouting about Syria or Ukraine, less people would see it and would be less effective. MAK: Senator Mark Warner is the top Democrat on the Senate intelligence committee. Addressing the fact that there are still no serious efforts to combat these influence networks, he is pessimistic. MARK WARNER: This is really an ongoing national security issue, and I don't think we've come up with a legislative or policy solution yet that fully gets it right. MAK: Talking to Fox News this week, Secretary of State Rex Tillerson acknowledged that Russia's methods are evolving. (SOUNDBITE OF FOX NEWS BROADCAST)TILLERSON: The point is, if it's their intention to interfere, they're going find ways to do that, and. . . MAK: And, he added, it's going to be difficult to stop. Tim Mak, NPR News, Washington. (SOUNDBITE OF LOSCIL'S \"BLEEDING INK\") STEVE INSKEEP, HOST:  With the midterm elections approaching, Secretary of State Rex Tillerson warned on Fox News this week that Russia is trying to meddle again. (SOUNDBITE OF FOX NEWS BROADCAST) REX TILLERSON: I think it's important we just continue to say to Russia, look, if you think we don't see what you're doing, we do see it, and you need to stop. INSKEEP: Tillerson's boss, President Trump, has downplayed Russian interference in the 2016 election and said that Russia's president, quote, \"means it\" when he denies meddling. Other officials say Russia did interfere and that its methods have evolved. NPR's Tim Mak went behind the scenes with a group tracking the latest tactics. (SOUNDBITE OF TV SHOW, \"HANNITY\") SEAN HANNITY: This makes Watergate like stealing a Snickers bar from a drugstore. SARA CARTER: Absolutely, and. . . TIM MAK, BYLINE: That's Sean Hannity, host of a popular Fox News show, talking about the Nunes memo. It's a document prepared by GOP members of the House intelligence committee that President Trump says vindicates him in ongoing investigations. As the debate over whether to release the memo seized Congress, an initiative monitoring Russian influence operations noticed a sharp uptick in the number of automated Twitter bots pushing the #ReleaseTheMemo hashtag. BRET SCHAFER: . . . At a level we have not seen with any other topic. MAK: That's Bret Schafer, an analyst for Hamilton 68, a bipartisan project backed by the German Marshall Fund of the United States. Schafer says #ReleaseTheMemo was an instance of Russian-linked networks trying to erode faith in American institutions. Hamilton 68 is led by an unlikely pair - Laura Rosenberger. . . LAURA ROSENBERGER: I was Hillary Clinton's foreign policy adviser during her 2016 presidential campaign, and before that I worked. . . MAK: . . . And Jamie Fly. JAMIE FLY: I was Marco Rubio's foreign policy adviser in the Senate and worked with him as his adviser during his presidential campaign. MAK: Starting six months ago, the project identified and monitored 600 Twitter accounts linked to Russian influence operations. Here's Fly discussing the primary purpose of these networks. FLY: These are not networks that are necessarily pushing always traditional propaganda. A lot of it is actually just trying to rip apart Americans, to sow chaos within our political system, to pit Americans of both parties against each other. MAK: The Hamilton 68 project saw a lot of Kremlin-linked activity on recent controversies like NFL players kneeling during the national anthem, the Charlottesville protests and the immigration debate swirling around DACA. Here's Rosenberger. ROSENBERGER: We have now realized that an adversary has discovered significant vulnerabilities that can be exploited, and we need to really wrestle with taking steps to make sure that those vulnerabilities can't be exploited and that the real potential of, you know, social media platforms can be sort of maximized. MAK: But the Russian influence networks are now doing more than just creating chaos. They are pushing Russian propaganda to Americans using intriguing techniques. SCHAFER: Oftentimes we see topics that really have no importance to Moscow or any of their interests. They're just shared and - as a way of gaining an audience. MAK: Schafer said he noticed the trend when he saw a number of Russian-linked accounts start tweeting about #MondayMotivation and #WednesdayWisdom popular hashtags used by actual Americans on Twitter. They were mixed in with content about Syria and Ukraine, two countries where Russia is involved with military interventions. It is, in essence, a Russian social media marketing technique using trending topics to get eyeballs on their core message. SCHAFER: If they just started shouting about Syria or Ukraine, less people would see it and would be less effective. MAK: Senator Mark Warner is the top Democrat on the Senate intelligence committee. Addressing the fact that there are still no serious efforts to combat these influence networks, he is pessimistic. MARK WARNER: This is really an ongoing national security issue, and I don't think we've come up with a legislative or policy solution yet that fully gets it right. MAK: Talking to Fox News this week, Secretary of State Rex Tillerson acknowledged that Russia's methods are evolving. (SOUNDBITE OF FOX NEWS BROADCAST) TILLERSON: The point is, if it's their intention to interfere, they're going find ways to do that, and. . . MAK: And, he added, it's going to be difficult to stop. Tim Mak, NPR News, Washington. (SOUNDBITE OF LOSCIL'S \"BLEEDING INK\")", "section": "National Security", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-02-09-584640393": {"title": "Denmark's Tech Ambassador Is A World First And Isn't Tied To A Country : NPR", "url": "https://www.npr.org/2018/02/09/584640393/denmarks-tech-ambassador-is-a-world-first-and-isnt-tied-to-a-country", "author": "No author found", "published_date": "2018-02-09", "content": "", "section": "Technology", "disclaimer": ""}, "2018-02-09-584640400": {"title": "Why Printers Still Fail, Despite Advances In Technology : NPR", "url": "https://www.npr.org/2018/02/09/584640400/why-printers-still-fail-despite-advances-in-technology", "author": "No author found", "published_date": "2018-02-09", "content": "MARY LOUISE KELLY, HOST: Technology is marching on, bringing us smart speakers and smartphone updates and cars that drive themselves down the street. And yet it's the little challenges that bedevil. The dreaded blinking orange light. (SOUNDBITE OF PAPER RUSTLING)KELLY: All this paper and fiddling, and no joy. OK, full disclosure - we did deliberately wheel out NPR's most notoriously lousy printer for this moment. Why we even keep it is a question for another day. But the question to which we now apply ourselves - why do paper jams persist? Joshua Rothman takes on that question in The New Yorker, and he joins me now. Welcome. JOSHUA ROTHMAN: Hi. Thanks for having me. KELLY: How on earth did you get interested into digging into something that bedevils us all but seems so mundane? ROTHMAN: Well, pretty much exactly the way you just dramatized. I was printing something in my office, as one does, and the printer jammed. I had exactly that thought - why in the world is it still the case that we can send a man to the moon but we can't make a printer that doesn't jam? KELLY: What answer - as you started to investigate, what did you find? Why are - why is this so hard a challenge to solve? ROTHMAN: It has to do with sort of an elemental struggle between the natural and the mechanical. KELLY: Let me stop you there. An elemental struggle? ROTHMAN: Yeah. It really has to do with the fact that printing combines a natural thing, which is paper. It comes from trees. It's biological. And each sheet of paper is different. And on the other hand, there's the printer, which is made of metal and plastic. And every year, printers get faster. You know, as a result, the jam is a permanent part of our life. It just comes out of the fact that you can't have the combination of the biological and the mechanical and it goes perfectly forever. KELLY: As you investigated this, you discovered that one key is something known as the paper path. Explain. ROTHMAN: The paper path is what engineers call the route along which paper travels as it navigates the printer. And the main fact about the paper path is it is way more complicated than anybody imagines. It's twisty. It's high-speed. Paper gets superheated. We know how paper emerges from a printer and it's kind of toasty warm. That's because the paper path is so crazy. And it's almost something that paper has to survive. KELLY: You also tease out a couple of real-life examples where a printer paper jam had serious consequences. Tell me one. ROTHMAN: Well, one of the engineers that I talked to told me an amazing story, which basically involved - he was brought into a courthouse in Chicago where there were so many paper jams that attorneys were regularly filing their documents late. And it was resulting in prosecutions getting derailed because there was a really tight deadline. And by reducing the number of paper jams - basically he found out the courthouse was using substandard paper - he was able to increase the total number of cases of - that were prosecuted in this courthouse. KELLY: Wow. I mean, that's incredible. So literally upgrading to slightly more expensive paper meant that the rate of prosecutions in this particular courthouse was - shot right up? ROTHMAN: Yeah. It actually turns out that the quality of the paper really matters. And, you know, one thing that seems true about paper jams is they're just part of a larger set of technological problems that are just all about kind of greasing the wheels and keeping things going. KELLY: Will it ever be possible to build a jamless printer? ROTHMAN: You know, the conclusion that the engineers I talked to came to is that, no, you're never going to have a jamless printer, basically because you're never going to have a perfect machine. We're never going to perfect the world of objects that we've built. KELLY: And always a role for our baseball bats tucked in the corner the office just in case to. . . ROTHMAN: (Laughter). KELLY: . . . Attack the printers when they fail. ROTHMAN: Right. Right. Right. Totally. KELLY: Joshua Rothman, thanks so much. ROTHMAN: Thank you. KELLY: That's Joshua Rothman. He is The New Yorker's archive editor. And his news story is \"Why Paper Jams Persist. \"(SOUNDBITE OF SONG)UNIDENTIFIED SINGER: (Singing) Paper jam. MARY LOUISE KELLY, HOST:  Technology is marching on, bringing us smart speakers and smartphone updates and cars that drive themselves down the street. And yet it's the little challenges that bedevil. The dreaded blinking orange light. (SOUNDBITE OF PAPER RUSTLING) KELLY: All this paper and fiddling, and no joy. OK, full disclosure - we did deliberately wheel out NPR's most notoriously lousy printer for this moment. Why we even keep it is a question for another day. But the question to which we now apply ourselves - why do paper jams persist? Joshua Rothman takes on that question in The New Yorker, and he joins me now. Welcome. JOSHUA ROTHMAN: Hi. Thanks for having me. KELLY: How on earth did you get interested into digging into something that bedevils us all but seems so mundane? ROTHMAN: Well, pretty much exactly the way you just dramatized. I was printing something in my office, as one does, and the printer jammed. I had exactly that thought - why in the world is it still the case that we can send a man to the moon but we can't make a printer that doesn't jam? KELLY: What answer - as you started to investigate, what did you find? Why are - why is this so hard a challenge to solve? ROTHMAN: It has to do with sort of an elemental struggle between the natural and the mechanical. KELLY: Let me stop you there. An elemental struggle? ROTHMAN: Yeah. It really has to do with the fact that printing combines a natural thing, which is paper. It comes from trees. It's biological. And each sheet of paper is different. And on the other hand, there's the printer, which is made of metal and plastic. And every year, printers get faster. You know, as a result, the jam is a permanent part of our life. It just comes out of the fact that you can't have the combination of the biological and the mechanical and it goes perfectly forever. KELLY: As you investigated this, you discovered that one key is something known as the paper path. Explain. ROTHMAN: The paper path is what engineers call the route along which paper travels as it navigates the printer. And the main fact about the paper path is it is way more complicated than anybody imagines. It's twisty. It's high-speed. Paper gets superheated. We know how paper emerges from a printer and it's kind of toasty warm. That's because the paper path is so crazy. And it's almost something that paper has to survive. KELLY: You also tease out a couple of real-life examples where a printer paper jam had serious consequences. Tell me one. ROTHMAN: Well, one of the engineers that I talked to told me an amazing story, which basically involved - he was brought into a courthouse in Chicago where there were so many paper jams that attorneys were regularly filing their documents late. And it was resulting in prosecutions getting derailed because there was a really tight deadline. And by reducing the number of paper jams - basically he found out the courthouse was using substandard paper - he was able to increase the total number of cases of - that were prosecuted in this courthouse. KELLY: Wow. I mean, that's incredible. So literally upgrading to slightly more expensive paper meant that the rate of prosecutions in this particular courthouse was - shot right up? ROTHMAN: Yeah. It actually turns out that the quality of the paper really matters. And, you know, one thing that seems true about paper jams is they're just part of a larger set of technological problems that are just all about kind of greasing the wheels and keeping things going. KELLY: Will it ever be possible to build a jamless printer? ROTHMAN: You know, the conclusion that the engineers I talked to came to is that, no, you're never going to have a jamless printer, basically because you're never going to have a perfect machine. We're never going to perfect the world of objects that we've built. KELLY: And always a role for our baseball bats tucked in the corner the office just in case to. . . ROTHMAN: (Laughter). KELLY: . . . Attack the printers when they fail. ROTHMAN: Right. Right. Right. Totally. KELLY: Joshua Rothman, thanks so much. ROTHMAN: Thank you. KELLY: That's Joshua Rothman. He is The New Yorker's archive editor. And his news story is \"Why Paper Jams Persist. \" (SOUNDBITE OF SONG) UNIDENTIFIED SINGER: (Singing) Paper jam.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-02-12-585177775": {"title": "What Happens When You Fill A House With 'Smart' Technology : NPR", "url": "https://www.npr.org/2018/02/12/585177775/what-happens-when-you-fill-a-house-with-smart-technology", "author": "No author found", "published_date": "2018-02-12", "content": "MARY LOUISE KELLY, HOST: Lots of us are putting smart speakers in our homes. By NPR's own analysis, 1 in 6 Americans can now just using their voices call up the weather or a favorite song or turn on NPR. But why stop with speakers? A reporting team at Gizmodo decided to conduct an experiment and take the smart home concept to the extreme. Well, to explain quite how extreme, we've invited the team to come join us. Kashmir Hill joins us from San Francisco. Hi there. KASHMIR HILL: Hi. KELLY: And Surya Mattu from New York - hi to you, too. SURYA MATTU: Hi. KELLY: Kashmir, your home served as the site for this experiment. Walk us through what you did. HILL: Yeah, so my husband, my 1-year-old daughter and I were the guinea pigs for this experiment. KELLY: (Laughter). HILL: And - (laughter) where I smartened up our house and got a ton of devices that are Internet-connected. We already had a smart TV. We had the Amazon Echo smart speaker. KELLY: OK. HILL: But I also got an Internet-connected toothbrush, a sex toy, a photo frame, a coffeemaker, connected our bed to the Internet. KELLY: Your toothbrush was the one that got me - connected and reminding you when to brush and relaying that to the world. Why did this seem like an appealing idea? HILL: I wouldn't say it was appealing exactly, but we definitely thought it would help people anticipate the future. And so we wanted to see the state of the technology right now. And then what we were really interested in was whether my smart home would betray me. KELLY: Ooh (ph), OK. We're going to circle back to that. But, Surya Mattu, let me let you jump in here 'cause to be clear, you do not live in this smart home. MATTU: I do not. KELLY: You live on the other side of the country. What was your role in all this? MATTU: I was sort of the watchdog. So when Kash kind of came up with this idea originally, I was really curious to understand how often these devices are talking to their companies and how often those companies are asking for information from these devices. And so I just made a setup for us that allowed us to monitor that. KELLY: Basically you built a router so that you could see everything that was being communicated. . . MATTU: That's right. KELLY: . . . Back and forth. And worth noting - I think we all kind of know this in the back of our heads, but these devices are all communicating even if we're not using them, even if we're not home. MATTU: All the time. KELLY: So what were you able to see? MATTU: I was able to see a bunch of different things. I think the most surprising for me was just by kind of, like, correlating signals across different devices and, like, oh, they've probably just woken up or, oh, they were watching TV last night or, oh, Kash has maybe not brushed her teeth yet. KELLY: Kashmir, you said you set out to determine whether these devices would betray you. What do you mean? HILL: Well, I just wanted to find out how much information they would be sending out and where they would be sending it to. I was surprised by how often a lot of the devices that were sitting there unused were actually talking to their home servers. So the Amazon Echo was pinging Amazon every three minutes whether we were using it or not, whether it was muted or not. My smart TV was quite talkative about what we were doing (laughter). Surya was like, it looks like you've been watching - you watched TV all day on Christmas, which I was very embarrassed about, but my husband's family wanted to watch basketball all day. KELLY: Well, let me ask you both what your takeaway is from this. Surya, you start. MATTU: I think my main takeaway is the - our devices generate a lot of data about us. And while most of it isn't necessarily true, it still is, like, something people sell. KELLY: And, Kashmir, how about you? I mean, the whole fantasy of a smart home is that it's going to make our lives way easier and less stressful. Was that your experience? HILL: Yeah. I mean, I think my takeaway was that smart homes are not very convenient right now. They're very frustrating and annoying to live in. And so it's not worth the privacy tradeoff. KELLY: Yet. Maybe watch this space. . . HILL: Yet. KELLY: . . . As it all - as the technology. . . HILL: Yeah. KELLY: . . . Improves, and you can run this again in a year. HILL: Yeah, I think it will improve. And so one - a bigger takeaway I had from this is that I think that we all need what Surya bought for me, this router or some kind of interface that you can see what your products are doing and see what they're saying so that you're at least aware of the fact that you don't solely own these devices. You're really sharing custody with the company that made it. And we should be able to see what they're saying to those companies. KELLY: Thanks so much to both of you. MATTU: Thank you. HILL: Thanks. KELLY: Kashmir Hill and Surya Mattu are reporters at Gizmodo. Their story is \"The House That Spied On Me. \" MARY LOUISE KELLY, HOST:  Lots of us are putting smart speakers in our homes. By NPR's own analysis, 1 in 6 Americans can now just using their voices call up the weather or a favorite song or turn on NPR. But why stop with speakers? A reporting team at Gizmodo decided to conduct an experiment and take the smart home concept to the extreme. Well, to explain quite how extreme, we've invited the team to come join us. Kashmir Hill joins us from San Francisco. Hi there. KASHMIR HILL: Hi. KELLY: And Surya Mattu from New York - hi to you, too. SURYA MATTU: Hi. KELLY: Kashmir, your home served as the site for this experiment. Walk us through what you did. HILL: Yeah, so my husband, my 1-year-old daughter and I were the guinea pigs for this experiment. KELLY: (Laughter). HILL: And - (laughter) where I smartened up our house and got a ton of devices that are Internet-connected. We already had a smart TV. We had the Amazon Echo smart speaker. KELLY: OK. HILL: But I also got an Internet-connected toothbrush, a sex toy, a photo frame, a coffeemaker, connected our bed to the Internet. KELLY: Your toothbrush was the one that got me - connected and reminding you when to brush and relaying that to the world. Why did this seem like an appealing idea? HILL: I wouldn't say it was appealing exactly, but we definitely thought it would help people anticipate the future. And so we wanted to see the state of the technology right now. And then what we were really interested in was whether my smart home would betray me. KELLY: Ooh (ph), OK. We're going to circle back to that. But, Surya Mattu, let me let you jump in here 'cause to be clear, you do not live in this smart home. MATTU: I do not. KELLY: You live on the other side of the country. What was your role in all this? MATTU: I was sort of the watchdog. So when Kash kind of came up with this idea originally, I was really curious to understand how often these devices are talking to their companies and how often those companies are asking for information from these devices. And so I just made a setup for us that allowed us to monitor that. KELLY: Basically you built a router so that you could see everything that was being communicated. . . MATTU: That's right. KELLY: . . . Back and forth. And worth noting - I think we all kind of know this in the back of our heads, but these devices are all communicating even if we're not using them, even if we're not home. MATTU: All the time. KELLY: So what were you able to see? MATTU: I was able to see a bunch of different things. I think the most surprising for me was just by kind of, like, correlating signals across different devices and, like, oh, they've probably just woken up or, oh, they were watching TV last night or, oh, Kash has maybe not brushed her teeth yet. KELLY: Kashmir, you said you set out to determine whether these devices would betray you. What do you mean? HILL: Well, I just wanted to find out how much information they would be sending out and where they would be sending it to. I was surprised by how often a lot of the devices that were sitting there unused were actually talking to their home servers. So the Amazon Echo was pinging Amazon every three minutes whether we were using it or not, whether it was muted or not. My smart TV was quite talkative about what we were doing (laughter). Surya was like, it looks like you've been watching - you watched TV all day on Christmas, which I was very embarrassed about, but my husband's family wanted to watch basketball all day. KELLY: Well, let me ask you both what your takeaway is from this. Surya, you start. MATTU: I think my main takeaway is the - our devices generate a lot of data about us. And while most of it isn't necessarily true, it still is, like, something people sell. KELLY: And, Kashmir, how about you? I mean, the whole fantasy of a smart home is that it's going to make our lives way easier and less stressful. Was that your experience? HILL: Yeah. I mean, I think my takeaway was that smart homes are not very convenient right now. They're very frustrating and annoying to live in. And so it's not worth the privacy tradeoff. KELLY: Yet. Maybe watch this space. . . HILL: Yet. KELLY: . . . As it all - as the technology. . . HILL: Yeah. KELLY: . . . Improves, and you can run this again in a year. HILL: Yeah, I think it will improve. And so one - a bigger takeaway I had from this is that I think that we all need what Surya bought for me, this router or some kind of interface that you can see what your products are doing and see what they're saying so that you're at least aware of the fact that you don't solely own these devices. You're really sharing custody with the company that made it. And we should be able to see what they're saying to those companies. KELLY: Thanks so much to both of you. MATTU: Thank you. HILL: Thanks. KELLY: Kashmir Hill and Surya Mattu are reporters at Gizmodo. Their story is \"The House That Spied On Me. \"", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-02-13-585131439": {"title": "Taking A Page From 'Shark Tank' To Put Up Political Candidates : NPR", "url": "https://www.npr.org/2018/02/13/585131439/taking-a-page-from-shark-tank-to-put-up-political-candidates", "author": "No author found", "published_date": "2018-02-13", "content": "", "section": "Politics", "disclaimer": ""}, "2018-02-20-587375771": {"title": "Russian Bots Are Spreading False Information After The Florida Shooting : NPR", "url": "https://www.npr.org/2018/02/20/587375771/russian-bots-are-spreading-false-information-after-the-florida-shooting", "author": "No author found", "published_date": "2018-02-20", "content": "ARI SHAPIRO, HOST: Special counsel Robert Mueller's indictment of 13 Russians and three Russian companies detailed some of the ways in which Russian trolls used Facebook in an attempt to suppress African-American and Muslim voter turnout during the 2016 presidential election. And just last week, after the school shooting in Florida, trolls used Twitter to promote an extremist gun position. Those are just two examples of how easy it is to use social media to manipulate public opinion and voter attitudes. We're joined now by NPR's Aarti Shahani, who's following how social networks are responding. Hi, Aarti. AARTI SHAHANI, BYLINE: Hi. SHAPIRO: Let's start with last week's school shooting in Florida. How did trolls online exploit that tragedy? SHAHANI: Well, I spoke with a researcher at New Knowledge - that's a group that studies pro-Russia trolls - and he gave me a chilling timeline. By 2:30 p. m. Eastern that day he saw an uptick of messages by this one troll network. And they just started sharing breaking news, OK? But by 3 p. m. Eastern, a half hour later, they quickly pivoted to conspiracy theories, speculating it was a Democrat conspiracy and also advancing an extreme pro-gun position, saying teachers need to carry concealed handguns to protect students. Now, that latter talking point made its way into mainstream news. So it's an example of how anyone can drive the national conversation - you know, advocate strongly for an extreme position and manufacture a ton of retweets at the exact moment when pundits are looking to define the polls. SHAPIRO: That example didn't have to do with an election per se, but can these propaganda campaigns influence voters enough to have an impact on elections? SHAHANI: Yeah, well, I think the power of that example is to show that it - how quickly the mechanics work. And then it doesn't take a whole lot of voters to be swayed, right? I mean, we just had a Virginia state House election that ended in a tie. And our political system, because of the Electoral College and gerrymandering, is designed for a tiny group to sway an election. Then Facebook ad targeting is designed to let you reach that tiny group and slice and dice them according to their stance on guns or immigration or Trump, all for very little money. SHAPIRO: Tell us about some of the most important steps that social media platforms are taking to deal with this problem. SHAHANI: I think the single most important step seems to be coming out of Facebook, OK? They just announced that if you want to place a political ad in the U. S. , they're going to mail you a postcard to your U. S. address and you've got to give them the verification code on it to proceed. That's a significant step for them, but it only covers a tiny sliver of ads - OK? - the ones that name political candidates. So if I want to run an issue ad, say, to discourage certain minorities from voting, as you'd referred to happening before, that new system would not apply to me. I've spoken with several experts and no one believes Facebook or Twitter is close to ready for the midterm elections here. SHAPIRO: And Facebook was in some more controversy over the weekend when a senior employee tweeted that the main goal of Russian ads was not to sway U. S. elections. Explain what happened there. SHAHANI: Yeah, the employee is named Rob Goldman. He's vice president of advertising. And what he basically did was criticize the news coverage of Russian meddling, saying, hey, the stories keep talking about election interference, but there's just not a lot of evidence of that in the Mueller indictment. President Trump retweeted him because it's consistent with his position. Twitter got very angry. There was a massive backlash. And so then Facebook, they basically threw their guy under the bus and said, hey, he didn't clear his remarks with us. They don't represent our point of view here. And, you know, what I actually think is really unfortunate about this whole situation is that whether you agree or disagree with Goldman, he was doing exactly what Facebook needs to do more of, which is talking in public, OK? The company has been very secretive even though they know and we know it'll take a lot of outside partners to combat the problem of propaganda online. SHAPIRO: NPR's Aarti Shahani, thanks a lot. SHAHANI: Thank you. (SOUNDBITE OF BLACK MILK'S \"WHEN THE SKY FALLS\") ARI SHAPIRO, HOST:  Special counsel Robert Mueller's indictment of 13 Russians and three Russian companies detailed some of the ways in which Russian trolls used Facebook in an attempt to suppress African-American and Muslim voter turnout during the 2016 presidential election. And just last week, after the school shooting in Florida, trolls used Twitter to promote an extremist gun position. Those are just two examples of how easy it is to use social media to manipulate public opinion and voter attitudes. We're joined now by NPR's Aarti Shahani, who's following how social networks are responding. Hi, Aarti. AARTI SHAHANI, BYLINE: Hi. SHAPIRO: Let's start with last week's school shooting in Florida. How did trolls online exploit that tragedy? SHAHANI: Well, I spoke with a researcher at New Knowledge - that's a group that studies pro-Russia trolls - and he gave me a chilling timeline. By 2:30 p. m. Eastern that day he saw an uptick of messages by this one troll network. And they just started sharing breaking news, OK? But by 3 p. m. Eastern, a half hour later, they quickly pivoted to conspiracy theories, speculating it was a Democrat conspiracy and also advancing an extreme pro-gun position, saying teachers need to carry concealed handguns to protect students. Now, that latter talking point made its way into mainstream news. So it's an example of how anyone can drive the national conversation - you know, advocate strongly for an extreme position and manufacture a ton of retweets at the exact moment when pundits are looking to define the polls. SHAPIRO: That example didn't have to do with an election per se, but can these propaganda campaigns influence voters enough to have an impact on elections? SHAHANI: Yeah, well, I think the power of that example is to show that it - how quickly the mechanics work. And then it doesn't take a whole lot of voters to be swayed, right? I mean, we just had a Virginia state House election that ended in a tie. And our political system, because of the Electoral College and gerrymandering, is designed for a tiny group to sway an election. Then Facebook ad targeting is designed to let you reach that tiny group and slice and dice them according to their stance on guns or immigration or Trump, all for very little money. SHAPIRO: Tell us about some of the most important steps that social media platforms are taking to deal with this problem. SHAHANI: I think the single most important step seems to be coming out of Facebook, OK? They just announced that if you want to place a political ad in the U. S. , they're going to mail you a postcard to your U. S. address and you've got to give them the verification code on it to proceed. That's a significant step for them, but it only covers a tiny sliver of ads - OK? - the ones that name political candidates. So if I want to run an issue ad, say, to discourage certain minorities from voting, as you'd referred to happening before, that new system would not apply to me. I've spoken with several experts and no one believes Facebook or Twitter is close to ready for the midterm elections here. SHAPIRO: And Facebook was in some more controversy over the weekend when a senior employee tweeted that the main goal of Russian ads was not to sway U. S. elections. Explain what happened there. SHAHANI: Yeah, the employee is named Rob Goldman. He's vice president of advertising. And what he basically did was criticize the news coverage of Russian meddling, saying, hey, the stories keep talking about election interference, but there's just not a lot of evidence of that in the Mueller indictment. President Trump retweeted him because it's consistent with his position. Twitter got very angry. There was a massive backlash. And so then Facebook, they basically threw their guy under the bus and said, hey, he didn't clear his remarks with us. They don't represent our point of view here. And, you know, what I actually think is really unfortunate about this whole situation is that whether you agree or disagree with Goldman, he was doing exactly what Facebook needs to do more of, which is talking in public, OK? The company has been very secretive even though they know and we know it'll take a lot of outside partners to combat the problem of propaganda online. SHAPIRO: NPR's Aarti Shahani, thanks a lot. SHAHANI: Thank you. (SOUNDBITE OF BLACK MILK'S \"WHEN THE SKY FALLS\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-02-22-588069886": {"title": "Take A Peek Inside The Market For Stolen Usernames And Passwords : NPR", "url": "https://www.npr.org/2018/02/22/588069886/take-a-peek-inside-the-market-for-stolen-usernames-and-passwords", "author": "No author found", "published_date": "2018-02-22", "content": "ARI SHAPIRO, HOST: Most of us have a long list of usernames and passwords to sign into accounts online - eBay, Amazon, Expedia. Those credentials are valuable to hackers, and they're for sale online. Stacey Vanek Smith from our Planet Money team got a look into the market place for stolen passwords. STACEY VANEK SMITH, BYLINE: I have in front of me a list. It is four and a half pages long, and there are a bunch of company names on it all in alphabetical order. It has banks and airlines and clothing stores. And next to each company name is a price. This list comes from a site on the dark web where people buy and sell stolen usernames and passwords. It is a price list. I got a copy of this list from an investigative journalist named Brian Krebs. BRIAN KREBS: Author of the website krebsonsecurity. com. VANEK SMITH: And you spend a lot of time on the dark web. KREBS: Yeah. It's kind of an occupational hazard. VANEK SMITH: Krebs got this particular list from a site called Seller's Paradise. KREBS: It looks like a pretty nicely indexed e-commerce site where you might go and buy, you know, blenders or whatever it is you want to buy. VANEK SMITH: But in this case, instead of blenders, people are buying stolen usernames and passwords. Some account information like bank account passwords are obviously valuable. But for others, it can be kind of hard to know why anyone would be interested. There's Costco for 15, David's Bridal for 10. And what are you doing with these passwords if you buy them? So if you - if I buy someone's David's Bridal password for ten bucks, like, what am I doing with it? KREBS: (Laughter) One of the longest-running scams is the points. They go to use their points, and they're like, I don't have any points; I don't really know what's going on. VANEK SMITH: So, like, if you buy someone's, like - I'm looking at Best Buy - costs $13. KREBS: Right. I could in theory sign into your Best Buy account, change your address, and you would be none the wiser when they send me, you know, a set of $400 Bose headphones (laughter), you know? Cyber thieves think of really ingenious ways to cash these things out, and cash them out they do. VANEK SMITH: I mean, how scared should I be about this - about my passwords being out there? KREBS: Well, that depends. Are you the type of person who reuses the same password all over the place? Then you should. . . VANEK SMITH: Let's say that I were that kind of person (laughter). How scared should I be? KREBS: OK, yeah, I think you should be pretty concerned. I mean. . . VANEK SMITH: Really? KREBS: One of the biggest pieces of feedback I get from, you know, mere mortals who - you know, they take pride in the fact that they don't really understand computers or understand why anybody would want to hack their computer. And I just say, look; you have probably 20, 30 sets of credentials stored in your browser or on your computer that have value. You may not think that they do, but they absolutely do. And this service kind of, you know, puts a pretty fine point on that. VANEK SMITH: What does this mean - the existence of this marketplace - like, for most of us mere mortals? KREBS: It means that it's 2018, and we're all still stuck with the stupid passwords. VANEK SMITH: Krebs thinks we will eventually get to a post-password world. In that world, your phone could essentially become your password. After all, it has tons of data on you, your location, maybe even your fingerprints or your face. And that data can be used to verify your identity. So we'd essentially be carrying our passwords around in our pockets. But for now, we are stuck with these same old passwords and the same old advice we've been hearing for years. If you want to protect yourself from hackers, be sure to turn on two-factor authentication, and do not reuse the same passwords again and again and again like I do. Stacey Vanek Smith, NPR News. ARI SHAPIRO, HOST:  Most of us have a long list of usernames and passwords to sign into accounts online - eBay, Amazon, Expedia. Those credentials are valuable to hackers, and they're for sale online. Stacey Vanek Smith from our Planet Money team got a look into the market place for stolen passwords. STACEY VANEK SMITH, BYLINE: I have in front of me a list. It is four and a half pages long, and there are a bunch of company names on it all in alphabetical order. It has banks and airlines and clothing stores. And next to each company name is a price. This list comes from a site on the dark web where people buy and sell stolen usernames and passwords. It is a price list. I got a copy of this list from an investigative journalist named Brian Krebs. BRIAN KREBS: Author of the website krebsonsecurity. com. VANEK SMITH: And you spend a lot of time on the dark web. KREBS: Yeah. It's kind of an occupational hazard. VANEK SMITH: Krebs got this particular list from a site called Seller's Paradise. KREBS: It looks like a pretty nicely indexed e-commerce site where you might go and buy, you know, blenders or whatever it is you want to buy. VANEK SMITH: But in this case, instead of blenders, people are buying stolen usernames and passwords. Some account information like bank account passwords are obviously valuable. But for others, it can be kind of hard to know why anyone would be interested. There's Costco for 15, David's Bridal for 10. And what are you doing with these passwords if you buy them? So if you - if I buy someone's David's Bridal password for ten bucks, like, what am I doing with it? KREBS: (Laughter) One of the longest-running scams is the points. They go to use their points, and they're like, I don't have any points; I don't really know what's going on. VANEK SMITH: So, like, if you buy someone's, like - I'm looking at Best Buy - costs $13. KREBS: Right. I could in theory sign into your Best Buy account, change your address, and you would be none the wiser when they send me, you know, a set of $400 Bose headphones (laughter), you know? Cyber thieves think of really ingenious ways to cash these things out, and cash them out they do. VANEK SMITH: I mean, how scared should I be about this - about my passwords being out there? KREBS: Well, that depends. Are you the type of person who reuses the same password all over the place? Then you should. . . VANEK SMITH: Let's say that I were that kind of person (laughter). How scared should I be? KREBS: OK, yeah, I think you should be pretty concerned. I mean. . . VANEK SMITH: Really? KREBS: One of the biggest pieces of feedback I get from, you know, mere mortals who - you know, they take pride in the fact that they don't really understand computers or understand why anybody would want to hack their computer. And I just say, look; you have probably 20, 30 sets of credentials stored in your browser or on your computer that have value. You may not think that they do, but they absolutely do. And this service kind of, you know, puts a pretty fine point on that. VANEK SMITH: What does this mean - the existence of this marketplace - like, for most of us mere mortals? KREBS: It means that it's 2018, and we're all still stuck with the stupid passwords. VANEK SMITH: Krebs thinks we will eventually get to a post-password world. In that world, your phone could essentially become your password. After all, it has tons of data on you, your location, maybe even your fingerprints or your face. And that data can be used to verify your identity. So we'd essentially be carrying our passwords around in our pockets. But for now, we are stuck with these same old passwords and the same old advice we've been hearing for years. If you want to protect yourself from hackers, be sure to turn on two-factor authentication, and do not reuse the same passwords again and again and again like I do. Stacey Vanek Smith, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-02-22-587762926": {"title": "As Washington Gears Up To Tackle Foreign Influence, How Effective Can It Be? : NPR", "url": "https://www.npr.org/2018/02/22/587762926/as-washington-gears-up-to-tackle-foreign-influence-how-effective-can-it-be", "author": "No author found", "published_date": "2018-02-22", "content": "", "section": "National Security", "disclaimer": ""}, "2018-02-27-584650612": {"title": "Court Seems Unconvinced Of Microsoft's Argument To Shield Email Data Stored Over : NPR", "url": "https://www.npr.org/2018/02/27/584650612/new-front-in-data-privacy-at-the-supreme-court-can-u-s-seize-emails-stored-abroa", "author": "No author found", "published_date": "2018-02-27", "content": "STEVE INSKEEP, HOST: Every day, Americans send information to the cloud. That smartphone in your hand, or in the hand of the person next to you, is uploading photos and messages and all sorts of information. It's not literally in a cloud, but in a data center somewhere in the world so that you have a backup and some extra memory in your device. A case before the Supreme Court today asks if the U. S. government can use search warrants to get at that data even when it's stored outside the country. Here's NPR legal affairs correspondent Nina Totenberg. NINA TOTENBERG, BYLINE: The case involves a federal drug trafficking investigation in which U. S. law enforcement authorities obtained a search warrant for all data associated with the suspect's Microsoft account. Microsoft refused to give the feds the email content, which was stored in a data center in Ireland. If the company is forced to turn over the material, it could be fined three point six billion dollars for violating the European Union's new privacy protection rules. The company argues there's no legal authority for the U. S. warrant because it was issued under a law enacted by Congress in 1986, a full five years before the World Wide Web was even created. The only legal way to obtain this information, the company says, is through an international treaty between the U. S. and Ireland. But the U. S. government argues that process is extremely slow, cumbersome and would prove highly impractical, given that some providers break up data and move it around the world in pieces. Microsoft replies that it would set a dangerous precedent for other countries to follow if the U. S. government can reach into foreign territory to retrieve a user's private emails, even with a warrant. After all, the company asks, if the U. S. government can reach into data stored in foreign countries, why couldn't Russia or Iran reach into emails stored in the U. S. ? The U. S. government replies that any Microsoft email content stored in other countries can easily be transferred back to the U. S. by the click of a computer located at the company's headquarters in Redmond, Wash. , thus making the focus of such warrants domestic, not international. The case has huge potential ramifications for international law relations between countries and for the global economy. A decision is expected by summer. Nina Totenberg, NPR News, Washington. STEVE INSKEEP, HOST:  Every day, Americans send information to the cloud. That smartphone in your hand, or in the hand of the person next to you, is uploading photos and messages and all sorts of information. It's not literally in a cloud, but in a data center somewhere in the world so that you have a backup and some extra memory in your device. A case before the Supreme Court today asks if the U. S. government can use search warrants to get at that data even when it's stored outside the country. Here's NPR legal affairs correspondent Nina Totenberg. NINA TOTENBERG, BYLINE: The case involves a federal drug trafficking investigation in which U. S. law enforcement authorities obtained a search warrant for all data associated with the suspect's Microsoft account. Microsoft refused to give the feds the email content, which was stored in a data center in Ireland. If the company is forced to turn over the material, it could be fined three point six billion dollars for violating the European Union's new privacy protection rules. The company argues there's no legal authority for the U. S. warrant because it was issued under a law enacted by Congress in 1986, a full five years before the World Wide Web was even created. The only legal way to obtain this information, the company says, is through an international treaty between the U. S. and Ireland. But the U. S. government argues that process is extremely slow, cumbersome and would prove highly impractical, given that some providers break up data and move it around the world in pieces. Microsoft replies that it would set a dangerous precedent for other countries to follow if the U. S. government can reach into foreign territory to retrieve a user's private emails, even with a warrant. After all, the company asks, if the U. S. government can reach into data stored in foreign countries, why couldn't Russia or Iran reach into emails stored in the U. S. ? The U. S. government replies that any Microsoft email content stored in other countries can easily be transferred back to the U. S. by the click of a computer located at the company's headquarters in Redmond, Wash. , thus making the focus of such warrants domestic, not international. The case has huge potential ramifications for international law relations between countries and for the global economy. A decision is expected by summer. Nina Totenberg, NPR News, Washington.", "section": "Law", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-01-589464779": {"title": "Uber Launches Service To Get People To The Doctor's Office  : NPR", "url": "https://www.npr.org/2018/03/01/589464779/uber-launches-service-to-get-people-to-their-doctors-offices", "author": "No author found", "published_date": "2018-03-01", "content": "", "section": "Business", "disclaimer": ""}, "2018-03-03-590546371": {"title": "Rural Communities Take Broadband Into Their Own Hands : NPR", "url": "https://www.npr.org/2018/03/03/590546371/rural-communities-take-broadband-into-their-own-hands", "author": "No author found", "published_date": "2018-03-03", "content": "SCOTT SIMON, HOST:  A lot of us take high-speed internet for granted, but about 40 percent of rural Americans live where there are no options for broadband. From WMMT, Benny Becker reports on one community in Appalachian Kentucky that's struggling to get its residents connected. BENNY BECKER, BYLINE: I met Gemelia Lewis early last year near her home in Linefork, Ky. It's a little valley tucked away in the mountains of Letcher County, an Appalachian community with a strong sense of heritage. GEMELIA LEWIS: This is the house where I grew up. BECKER: Her grandfather built this house, and Lewis has stayed home to help take care of her parents. She has a background in accounting but has had a hard time finding steady work. LEWIS: I was actually offered a job where I could work from home, but I couldn't take the job because there's no internet. BECKER: From her house, Lewis had no options for broadband internet and not enough cellphone signal to get online or even make calls. The lack of connectivity has been hard on her family. Homework is almost impossible for her younger son, who's visually impaired. The school has loaned him an iPad that he can use to zoom in on the text in his assignments. . . LEWIS: But he can't do that here because we don't have any internet at all. I feel like he's getting left behind because he doesn't have what he needs to get his education, and that's not fair. BECKER: Across America, 23 million people live in rural areas where there's no broadband internet. The federal government has spent billions of dollars trying to expand access, but much of that has gone to large telecom companies. Those businesses make more money when they maximize customers, so they often target more densely populated areas. Christopher Mitchell directs the Community Broadband Networks Initiative in Minnesota. He advocates for local efforts to expand internet access, and he wants Congress to do more. CHRISTOPHER MITCHELL: It was estimated it would cost $350 billion to connect every last home in America. If most of that came through loans, you're looking at less than a $10-billion-per-year program for something that I think would supercharge the economy for decades to come. BECKER: Many communities say they're not willing to wait. More than 750 have already built their own broadband networks. That includes urban areas like Chattanooga, Tenn. , and Concord, Mass. , but also rural places like Powell, Wyo. , and Bellevue, Iowa. Last year, Letcher County, Ky. , decided to follow that model. Officials created a broadband board to try to bring affordable internet to isolated areas like Linefork. Harry Collins chairs the group. HARRY COLLINS: Let's face it - that pipe dream in the sky of the new interstate ain't going to roll right up through Linefork. But this group can bring you the information highway, and that's what we're here to do. BECKER: The board applied for a $1. 5 million federal grant to install a fiber optic network, but it was denied. Now the board is trying something different - a network that's quicker and cheaper to build. DON WHITE: Wireless broadband - so no cables, no phone lines. BECKER: That's Don White of FiSci Technologies. Last year, White helped install wireless broadband in another part of Letcher County. WHITE: It's a very viable approach to providing broadband into areas that are tough to get connectivity to. BECKER: The Letcher County Broadband Board estimates it could build a wireless network this year for $700,000 if it can find the money. And that's a problem for many rural communities already struggling from a declining population and many years of job losses. For those living in Linefork, there has been some good news. A small local provider expanded its network, and for the first time, a few people like Gemelia Lewis now have a fast internet connection at home. For NPR News, I'm Benny Becker in Whitesburg, Ky. (SOUNDBITE OF MUSIC)SIMON: And that story comes to us from the Ohio Valley Resource. SCOTT SIMON, HOST:   A lot of us take high-speed internet for granted, but about 40 percent of rural Americans live where there are no options for broadband. From WMMT, Benny Becker reports on one community in Appalachian Kentucky that's struggling to get its residents connected. BENNY BECKER, BYLINE: I met Gemelia Lewis early last year near her home in Linefork, Ky. It's a little valley tucked away in the mountains of Letcher County, an Appalachian community with a strong sense of heritage. GEMELIA LEWIS: This is the house where I grew up. BECKER: Her grandfather built this house, and Lewis has stayed home to help take care of her parents. She has a background in accounting but has had a hard time finding steady work. LEWIS: I was actually offered a job where I could work from home, but I couldn't take the job because there's no internet. BECKER: From her house, Lewis had no options for broadband internet and not enough cellphone signal to get online or even make calls. The lack of connectivity has been hard on her family. Homework is almost impossible for her younger son, who's visually impaired. The school has loaned him an iPad that he can use to zoom in on the text in his assignments. . . LEWIS: But he can't do that here because we don't have any internet at all. I feel like he's getting left behind because he doesn't have what he needs to get his education, and that's not fair. BECKER: Across America, 23 million people live in rural areas where there's no broadband internet. The federal government has spent billions of dollars trying to expand access, but much of that has gone to large telecom companies. Those businesses make more money when they maximize customers, so they often target more densely populated areas. Christopher Mitchell directs the Community Broadband Networks Initiative in Minnesota. He advocates for local efforts to expand internet access, and he wants Congress to do more. CHRISTOPHER MITCHELL: It was estimated it would cost $350 billion to connect every last home in America. If most of that came through loans, you're looking at less than a $10-billion-per-year program for something that I think would supercharge the economy for decades to come. BECKER: Many communities say they're not willing to wait. More than 750 have already built their own broadband networks. That includes urban areas like Chattanooga, Tenn. , and Concord, Mass. , but also rural places like Powell, Wyo. , and Bellevue, Iowa. Last year, Letcher County, Ky. , decided to follow that model. Officials created a broadband board to try to bring affordable internet to isolated areas like Linefork. Harry Collins chairs the group. HARRY COLLINS: Let's face it - that pipe dream in the sky of the new interstate ain't going to roll right up through Linefork. But this group can bring you the information highway, and that's what we're here to do. BECKER: The board applied for a $1. 5 million federal grant to install a fiber optic network, but it was denied. Now the board is trying something different - a network that's quicker and cheaper to build. DON WHITE: Wireless broadband - so no cables, no phone lines. BECKER: That's Don White of FiSci Technologies. Last year, White helped install wireless broadband in another part of Letcher County. WHITE: It's a very viable approach to providing broadband into areas that are tough to get connectivity to. BECKER: The Letcher County Broadband Board estimates it could build a wireless network this year for $700,000 if it can find the money. And that's a problem for many rural communities already struggling from a declining population and many years of job losses. For those living in Linefork, there has been some good news. A small local provider expanded its network, and for the first time, a few people like Gemelia Lewis now have a fast internet connection at home. For NPR News, I'm Benny Becker in Whitesburg, Ky. (SOUNDBITE OF MUSIC) SIMON: And that story comes to us from the Ohio Valley Resource.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-05-590803674": {"title": "YouTube Restores Account For Far-Right Activist Jerome Corsi : NPR", "url": "https://www.npr.org/2018/03/05/590803674/youtube-restores-account-for-far-right-activist-jerome-corsi", "author": "No author found", "published_date": "2018-03-05", "content": "RACHEL MARTIN, HOST: YouTube is locked in a battle over free speech rights. Last Thursday, the company banned far-right activist Jerome Corsi from their site for just a few hours. He is the Washington, D. C. , bureau chief of Infowars, which is headed by prominent conspiracy theorist Alex Jones. When Corsi got his access back to YouTube, he used the opportunity to rail against the company. Corsi's among those on the far-right fringe claiming that YouTube is engaging in a concerted effort to censor him. NPR's Aarti Shahani's been following this. And she joins us now. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. MARTIN: Can you just back up and tell us how this standoff got started in the first place? SHAHANI: Sure. So after the Parkland shooting, Infowars and a few other far-right outlets started suggesting, falsely, that one of the survivors, a young man speaking out against the NRA, that he's just an actor and he was stumbling around for words because he forgot his lines. One of these videos even made it into YouTube's trending list. Now, YouTube - it's a private network. It's owned by Google. And it doesn't claim to be an open platform for free speech. You know, one of the policies is you're not allowed to shame or attack victims after a violent incident. So the company basically sends out their version of a cease-and-desist notice. And on Thursday, the channel Jerome Corsi was running got taken down. MARTIN: And so what reason did they give, that he was violating the specific terms of YouTube? SHAHANI: Well, so YouTube, he says, didn't give him a stated reason. It was just their whim. And a YouTube spokesperson says that the company made a mistake, and it's because they've hired so many new people in the last few months to help them sort through videos and enforce the community standards that, you know, newbies are still learning what's OK and what's not OK and they got it wrong and also points out Corsi appealed and got his account restored that same day Thursday. You know, what I point out, though, is how much of a mixed message there is here. On the one hand, leaders at Google, YouTube's parent company, strongly support free speech. But then YouTube itself, just like Facebook, is defining and taking down content that's considered extremist or harassment. And those can be very subjective things left in the hands of a private company to define. MARTIN: But according to this guy, Corsi, he's accusing YouTube and other tech platforms of being organizations that are, essentially, run by a bunch of liberals who want to target people like him on the far-right fringe. Is there any truth to that? SHAHANI: There's anecdotal evidence of it, OK? There's his case. A couple weeks back, Twitter did a bot purge that included right-wing accounts. After Charlottesville, white supremacist Andrew Anglin got his website taken down. But all that said, Corsi is being far from forthcoming when it comes to what's really going on. Right after his account was restored - actually, over the weekend - he went on YouTube and gave an hour-long sermon lecture about, you know, what his takedown means. Have a listen to how he frames it. (SOUNDBITE OF ARCHIVED RECORDING)JEROME CORSI: The left has a full right to have its points argued. But the intolerance, the wanting to destroy conservatives - please give us the same courtesy of being able to express our views. And please, if your arguments on the left are stronger, defeat our arguments on the right. MARTIN: So sort of classic tropes in arguments about American free speech here. SHAHANI: Classic First Amendment - you know, debate and let the truth rise to the top in the marketplace of ideas. Only problem is, Infowars is producing a marketplace of fake news, a fact that he omits. And one way YouTube could handle this is rather than deciding what's real or fake and knocking him off, they could, like Google search, just push up to the top of their trending things that are trusted sources. That wouldn't have turned Corsi, then, into a hero among the far-right. MARTIN: NPR's Aarti Shahani. Aarti, thanks so much for sharing your reporting with us. SHAHANI: Thanks. RACHEL MARTIN, HOST:  YouTube is locked in a battle over free speech rights. Last Thursday, the company banned far-right activist Jerome Corsi from their site for just a few hours. He is the Washington, D. C. , bureau chief of Infowars, which is headed by prominent conspiracy theorist Alex Jones. When Corsi got his access back to YouTube, he used the opportunity to rail against the company. Corsi's among those on the far-right fringe claiming that YouTube is engaging in a concerted effort to censor him. NPR's Aarti Shahani's been following this. And she joins us now. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. MARTIN: Can you just back up and tell us how this standoff got started in the first place? SHAHANI: Sure. So after the Parkland shooting, Infowars and a few other far-right outlets started suggesting, falsely, that one of the survivors, a young man speaking out against the NRA, that he's just an actor and he was stumbling around for words because he forgot his lines. One of these videos even made it into YouTube's trending list. Now, YouTube - it's a private network. It's owned by Google. And it doesn't claim to be an open platform for free speech. You know, one of the policies is you're not allowed to shame or attack victims after a violent incident. So the company basically sends out their version of a cease-and-desist notice. And on Thursday, the channel Jerome Corsi was running got taken down. MARTIN: And so what reason did they give, that he was violating the specific terms of YouTube? SHAHANI: Well, so YouTube, he says, didn't give him a stated reason. It was just their whim. And a YouTube spokesperson says that the company made a mistake, and it's because they've hired so many new people in the last few months to help them sort through videos and enforce the community standards that, you know, newbies are still learning what's OK and what's not OK and they got it wrong and also points out Corsi appealed and got his account restored that same day Thursday. You know, what I point out, though, is how much of a mixed message there is here. On the one hand, leaders at Google, YouTube's parent company, strongly support free speech. But then YouTube itself, just like Facebook, is defining and taking down content that's considered extremist or harassment. And those can be very subjective things left in the hands of a private company to define. MARTIN: But according to this guy, Corsi, he's accusing YouTube and other tech platforms of being organizations that are, essentially, run by a bunch of liberals who want to target people like him on the far-right fringe. Is there any truth to that? SHAHANI: There's anecdotal evidence of it, OK? There's his case. A couple weeks back, Twitter did a bot purge that included right-wing accounts. After Charlottesville, white supremacist Andrew Anglin got his website taken down. But all that said, Corsi is being far from forthcoming when it comes to what's really going on. Right after his account was restored - actually, over the weekend - he went on YouTube and gave an hour-long sermon lecture about, you know, what his takedown means. Have a listen to how he frames it. (SOUNDBITE OF ARCHIVED RECORDING) JEROME CORSI: The left has a full right to have its points argued. But the intolerance, the wanting to destroy conservatives - please give us the same courtesy of being able to express our views. And please, if your arguments on the left are stronger, defeat our arguments on the right. MARTIN: So sort of classic tropes in arguments about American free speech here. SHAHANI: Classic First Amendment - you know, debate and let the truth rise to the top in the marketplace of ideas. Only problem is, Infowars is producing a marketplace of fake news, a fact that he omits. And one way YouTube could handle this is rather than deciding what's real or fake and knocking him off, they could, like Google search, just push up to the top of their trending things that are trusted sources. That wouldn't have turned Corsi, then, into a hero among the far-right. MARTIN: NPR's Aarti Shahani. Aarti, thanks so much for sharing your reporting with us. SHAHANI: Thanks.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-08-592046294": {"title": "What Research Says About Video Games And Violence In Children : NPR", "url": "https://www.npr.org/2018/03/08/592046294/what-research-says-about-video-games-and-violence-in-children", "author": "No author found", "published_date": "2018-03-08", "content": "ARI SHAPIRO, HOST: President Trump has held a series of White House meetings on gun violence, and the focus of today's was video games. Lawmakers, parent advocates and people from video game companies were invited to talk with the president. The press was not allowed in. Trump has been focused on this subject for a while now. Here's what he said a couple weeks ago. (SOUNDBITE OF ARCHIVED RECORDING)PRESIDENT DONALD TRUMP: I'm hearing more and more people say the level of violence on video games is really shaping young people's thoughts. SHAPIRO: The central question at the heart of this White House meeting is, does playing violent video games turn people into real-life shooters? Douglas Gentile has researched this issue. He's a psychology professor at Iowa State University. Thanks for joining us. DOUGLAS GENTILE: My pleasure. SHAPIRO: If you could just begin with the conclusion of your research - if every violent video game disappeared tomorrow, would there be fewer mass shootings? GENTILE: We don't know the answer to that, but that's because aggression is actually very complicated. It's multi-causal. No one single thing causes it. And when we've had a school shooting, we usually ask the wrong question. We ask, what was the cause? And then we point around at different things such as mental health or violent video games or poverty or whatever. And none of them is it. What is it is when you put them all together. And so would it reduce the risk - yes. How much - we don't know. SHAPIRO: So if we take a step back from mass shootings and say how much does playing violent video games increase real-life violence and aggression, do we have a clear answer to that? GENTILE: We have a clear answer when we're talking about aggression. So aggression is any behavior - that could be a verbal behavior, a physical behavior or a relational behavior - that is intended to harm someone else. So if you give someone the cold shoulder, that is aggressive. But that's different from violence, which is only physical and extreme such that if successful, it would cause severe bodily damage or death. And the research on media violence and aggression seems pretty clear - that the more children consume media violence, whether that's in video games, TV or movies, they do become more willing to behave aggressively when provoked. SHAPIRO: You sort of conflated video games, TV, movies there. In a video game, you're pretending to be the shooter. You're interacting with a virtual world. TV or movies is much more passive. Is there an important distinction there, or is violence violence in media no matter whether it's interactive or passive? GENTILE: We used to think that video games would have a much larger effect than passive media like TV or movies. But the research has not seemed to bear that out. It seems to be about the same size effect, which is somewhat surprising because they are active, and you are being rewarded for it. But basically what we're coming down to is learning. We can learn from all of these different ways. And it seems we don't learn particularly differently from video games than from TV or movies. SHAPIRO: Some people have offered a theory that videogames can be catharsis, and expressing violent impulses in a virtual world helps people not express those in the real world. Has that been disproven? GENTILE: That has been disproven. So how do you memorize a phone number? You repeat it. Does seeing it one more time take it out of your brain? That would be the catharsis idea, right? SHAPIRO: (Laughter) Right. GENTILE: But, no, each new time you see it burns it in a little deeper. So in fact, there's no possible way that catharsis can happen, at least not nearly the way people like to talk about it. SHAPIRO: Do you think the premise of this White House meeting is flawed? I mean, should video games be one focus of this debate over gun violence in America? GENTILE: I do think it's flawed. I think the problem is that we're seeking a simple solution to a complex problem. And I noticed there are no real aggression researchers at this White House meeting. So we're not even getting the real picture. What we're getting is just a very one-sided and very limited look into only one of the risk factors for aggression. SHAPIRO: Professor Gentile, thanks very much. GENTILE: My pleasure. SHAPIRO: Psychology professor Douglas Gentile of Iowa State University. ARI SHAPIRO, HOST:  President Trump has held a series of White House meetings on gun violence, and the focus of today's was video games. Lawmakers, parent advocates and people from video game companies were invited to talk with the president. The press was not allowed in. Trump has been focused on this subject for a while now. Here's what he said a couple weeks ago. (SOUNDBITE OF ARCHIVED RECORDING) PRESIDENT DONALD TRUMP: I'm hearing more and more people say the level of violence on video games is really shaping young people's thoughts. SHAPIRO: The central question at the heart of this White House meeting is, does playing violent video games turn people into real-life shooters? Douglas Gentile has researched this issue. He's a psychology professor at Iowa State University. Thanks for joining us. DOUGLAS GENTILE: My pleasure. SHAPIRO: If you could just begin with the conclusion of your research - if every violent video game disappeared tomorrow, would there be fewer mass shootings? GENTILE: We don't know the answer to that, but that's because aggression is actually very complicated. It's multi-causal. No one single thing causes it. And when we've had a school shooting, we usually ask the wrong question. We ask, what was the cause? And then we point around at different things such as mental health or violent video games or poverty or whatever. And none of them is it. What is it is when you put them all together. And so would it reduce the risk - yes. How much - we don't know. SHAPIRO: So if we take a step back from mass shootings and say how much does playing violent video games increase real-life violence and aggression, do we have a clear answer to that? GENTILE: We have a clear answer when we're talking about aggression. So aggression is any behavior - that could be a verbal behavior, a physical behavior or a relational behavior - that is intended to harm someone else. So if you give someone the cold shoulder, that is aggressive. But that's different from violence, which is only physical and extreme such that if successful, it would cause severe bodily damage or death. And the research on media violence and aggression seems pretty clear - that the more children consume media violence, whether that's in video games, TV or movies, they do become more willing to behave aggressively when provoked. SHAPIRO: You sort of conflated video games, TV, movies there. In a video game, you're pretending to be the shooter. You're interacting with a virtual world. TV or movies is much more passive. Is there an important distinction there, or is violence violence in media no matter whether it's interactive or passive? GENTILE: We used to think that video games would have a much larger effect than passive media like TV or movies. But the research has not seemed to bear that out. It seems to be about the same size effect, which is somewhat surprising because they are active, and you are being rewarded for it. But basically what we're coming down to is learning. We can learn from all of these different ways. And it seems we don't learn particularly differently from video games than from TV or movies. SHAPIRO: Some people have offered a theory that videogames can be catharsis, and expressing violent impulses in a virtual world helps people not express those in the real world. Has that been disproven? GENTILE: That has been disproven. So how do you memorize a phone number? You repeat it. Does seeing it one more time take it out of your brain? That would be the catharsis idea, right? SHAPIRO: (Laughter) Right. GENTILE: But, no, each new time you see it burns it in a little deeper. So in fact, there's no possible way that catharsis can happen, at least not nearly the way people like to talk about it. SHAPIRO: Do you think the premise of this White House meeting is flawed? I mean, should video games be one focus of this debate over gun violence in America? GENTILE: I do think it's flawed. I think the problem is that we're seeking a simple solution to a complex problem. And I noticed there are no real aggression researchers at this White House meeting. So we're not even getting the real picture. What we're getting is just a very one-sided and very limited look into only one of the risk factors for aggression. SHAPIRO: Professor Gentile, thanks very much. GENTILE: My pleasure. SHAPIRO: Psychology professor Douglas Gentile of Iowa State University.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-08-591884059": {"title": "Trump Pits Video Game Makers Against Harshest Critics In Closed-Door Meeting : NPR", "url": "https://www.npr.org/2018/03/08/591884059/trump-pits-video-game-makers-against-harshest-critics-in-another-made-for-tv-mee", "author": "No author found", "published_date": "2018-03-08", "content": "", "section": "Politics", "disclaimer": ""}, "2018-03-08-591141384": {"title": "America's Oil Boom Is Fueled By A Tech Boom  : NPR", "url": "https://www.npr.org/2018/03/08/591141384/americas-oil-boom-is-fueled-by-a-tech-boom", "author": "No author found", "published_date": "2018-03-08", "content": "ARI SHAPIRO, HOST: It's expected that the U. S. will be producing more oil than any other country by next year. That's thanks in big part to advances in technology. Mose Buchele of member station KUT looks at what those advances mean for the oil industry and its workers. MOSE BUCHELE, BYLINE: To understand what's changed, we're going to start with a trip to the past. JAMES WHITE: We have the Wichtex model 66. BUCHELE: I'm at the Permian Basin Petroleum Museum in Midland, Texas, with James White. He's showing me how workers used to get oil from the ground. WHITE: So I'm going to try to get this thing to start for you. BUCHELE: He's at a waist-high steel engine with two huge wheels attached. It's called a Johnny Popper. He grabs onto a wheel and spins it to get the machine working. This was used to move pump jacks. The Johnny Popper was outdated by the time White started work in the oil fields. He says since then, things have changed even more. WHITE: Guys don't have to get in pickups and drive out to their pump jacks every day like they used to have. It's done with a lot of modern technology. And, you know, that's cool. It just puts people out of work. BUCHELE: Next stop, a lab at UT Austin where Professor Eric van Oort shows me that modern technology. ERIC VAN OORT: So we're in the Real-Time Operating Center or remote collaboration room. BUCHELE: Students are sitting at computers where they can monitor information streaming in from oil wells hundreds of miles away. VAN OORT: More and more we see operators willing to share data with us. We analyze it, and we give advice back to them. BUCHELE: To help them get the most oil possible out of the ground. Another big change is automation. It's always taken brawn and skill to do the dangerous work of drilling wells and putting pipe down them. And those workers are called roughnecks. But more and more of them are being replaced by a robot called an iron roughneck. Companies are also turning to drones, smart drill bits, even autonomous rigs that can drill oil wells on their own. VAN OORT: And if you see those in operation, that is definitely an oh-wow moment. BUCHELE: A lot of these advances took off after oil prices cratered a few years ago and companies tried to cut cost. It worked. These days, the U. S. is producing more oil than ever but at a cost. The Labor Department finds 50,000 fewer people working in oil and gas extraction than at the height of the last boom. Van Oort does see an upside. VAN OORT: The good side is that we are creating more sophisticated higher-end jobs in the process. BUCHELE: And, he says, it's making the traditionally dangerous oil field a lot safer. But it's not just roughnecks in the oil field who worry about their jobs. I met R. T. Hale at a diner back in Midland. R T HALE: I've got a machine shop down here, and I've been there 47 year. BUCHELE: He repairs old oilfield equipment. But he says it's getting harder. HALE: My machines were made in the United States. They've all went out of business. Things I used to repair, they just throw it away and stick a new one on it. BUCHELE: These days, he says, it feels like his old shop belongs in a museum. For NPR News, I'm Mose Buchele in Midland, Texas. ARI SHAPIRO, HOST:  It's expected that the U. S. will be producing more oil than any other country by next year. That's thanks in big part to advances in technology. Mose Buchele of member station KUT looks at what those advances mean for the oil industry and its workers. MOSE BUCHELE, BYLINE: To understand what's changed, we're going to start with a trip to the past. JAMES WHITE: We have the Wichtex model 66. BUCHELE: I'm at the Permian Basin Petroleum Museum in Midland, Texas, with James White. He's showing me how workers used to get oil from the ground. WHITE: So I'm going to try to get this thing to start for you. BUCHELE: He's at a waist-high steel engine with two huge wheels attached. It's called a Johnny Popper. He grabs onto a wheel and spins it to get the machine working. This was used to move pump jacks. The Johnny Popper was outdated by the time White started work in the oil fields. He says since then, things have changed even more. WHITE: Guys don't have to get in pickups and drive out to their pump jacks every day like they used to have. It's done with a lot of modern technology. And, you know, that's cool. It just puts people out of work. BUCHELE: Next stop, a lab at UT Austin where Professor Eric van Oort shows me that modern technology. ERIC VAN OORT: So we're in the Real-Time Operating Center or remote collaboration room. BUCHELE: Students are sitting at computers where they can monitor information streaming in from oil wells hundreds of miles away. VAN OORT: More and more we see operators willing to share data with us. We analyze it, and we give advice back to them. BUCHELE: To help them get the most oil possible out of the ground. Another big change is automation. It's always taken brawn and skill to do the dangerous work of drilling wells and putting pipe down them. And those workers are called roughnecks. But more and more of them are being replaced by a robot called an iron roughneck. Companies are also turning to drones, smart drill bits, even autonomous rigs that can drill oil wells on their own. VAN OORT: And if you see those in operation, that is definitely an oh-wow moment. BUCHELE: A lot of these advances took off after oil prices cratered a few years ago and companies tried to cut cost. It worked. These days, the U. S. is producing more oil than ever but at a cost. The Labor Department finds 50,000 fewer people working in oil and gas extraction than at the height of the last boom. Van Oort does see an upside. VAN OORT: The good side is that we are creating more sophisticated higher-end jobs in the process. BUCHELE: And, he says, it's making the traditionally dangerous oil field a lot safer. But it's not just roughnecks in the oil field who worry about their jobs. I met R. T. Hale at a diner back in Midland. R T HALE: I've got a machine shop down here, and I've been there 47 year. BUCHELE: He repairs old oilfield equipment. But he says it's getting harder. HALE: My machines were made in the United States. They've all went out of business. Things I used to repair, they just throw it away and stick a new one on it. BUCHELE: These days, he says, it feels like his old shop belongs in a museum. For NPR News, I'm Mose Buchele in Midland, Texas.", "section": "Environment And Energy Collaborative", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-09-591879723": {"title": "Kang Lee: Can Technology Detect Our Hidden Emotions? : NPR", "url": "https://www.npr.org/2018/03/09/591879723/kang-lee-can-technology-detect-our-hidden-emotions", "author": "No author found", "published_date": "2018-03-09", "content": "GUY RAZ, HOST: What's the - what's the connection between lying and our emotions? KANG LEE: When we lie to cover up our own transgressions, you have this fear of being caught, and also you have this shame and guilt and also the delight they associate with that. RAZ: This is Kang Lee. LEE: I study developmental neuroscience with a special focus on emotion, lie telling, in children and adults. RAZ: And Kang says specifically when kids lie, they learn pretty early on how to hide all of these emotions and control their facial expressions. LEE: You have to manage all your facial expressions, your body language, the words you are going to choose to make your lie stick. RAZ: So say I guess I should mention, Kang, that. . . LEE: Yeah. RAZ: . . . Just like a couple days ago, the remote control on our Google Chrome thing was broken. Like, it was peeled back. Like, you had to use, like, you know, jaws of steel to peel it back. And I looked at my kids and I said, what happened? And they both looked at me and said I don't know. I said, you don't know what happened to this remote control? And they said, no. I said, did you break it? No. Did you touch it? Yes, but we didn't break it. LEE: Exactly. Yeah, and they - when they answer these questions, they actually look right into your eyes, right? They did not actually avert their eyes, right? RAZ: Right into my eyes - no, I didn't do it. LEE: Yes, indeed. And then the majority of kids, when they lie, they actually look into your face and very seriously say, no. I'm talking about very young kids. These could be 2 years old or 3 years old. So they are very good at managing their facial expressions. (SOUNDBITE OF MUSIC)RAZ: Now, remember how Lisa Feldman Barrett just said that our ability to detect emotions in other people is unreliable and that a facial expression doesn't necessarily tell you much about how someone's feeling? Well, Kang believes there might be a way to detect emotions without using our eyes. Kang Lee explains his research from the TED stage. (SOUNDBITE OF TED TALK)LEE: We know that underneath our facial skin, there's a rich network of blood vessels. When we experience different emotions, our facial blood flow changes subtly. So by looking at facial blood flow changes, we can reveal people's hidden emotions. We have developed a new imaging technology we call transdermal optical imaging. To do so, we use a regular video camera to record people when people experience various hidden emotions. And then, using our image processing technology, we can extract transdermal images of facial blood flow changes. And using this technology, we can now reveal the hidden emotion associated with lying and therefore detect people's lies with an accuracy at about 85 percent. (SOUNDBITE OF MUSIC)RAZ: So let me see if I understand this correctly. You can train a video camera onto a human face, and then, with your technology, you can analyze facial blood flow, and then that indicates how a person is feeling with an accuracy rate of 85 percent when it comes to lying. LEE: Yes. RAZ: But can you do that with other emotions? LEE: Well, right now, we can measure your emotion at about 93 percent accuracy to differentiate between three states - positive, neutral and negative. And then with regard to specific emotions, depends on the situations. For example, disgust - we can do about 89 percent accuracy. But some other emotions are not very good. Fear - fear is the most difficult one, and it's about 64 percent. But still, you know, we have a long way to go to be able to pick up information from your face to say, ah, you know, Guy now is - he's experiencing fear or something like that. (SOUNDBITE OF MUSIC)RAZ: In a minute, just how Kang Lee's technology works and whether we can actually keep our emotions private ever again. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC)RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, Decoding Our Emotions. And we were just hearing from psychologist Kang Lee describe a new technology that he's created in his lab by using something called transdermal optical imaging. LEE: Yes, and the technology actually is very simple. RAZ: So basically, it starts with our skin, which is translucent. LEE: So when lights come into your face, it does not bounce back right away. So it actually penetrates to the deeper layers of our face. RAZ: And just by using a regular video camera to record a face. . . LEE: You know, the cameras on our iPhone is amazingly good. RAZ: . . . That video footage - when it's run through Kang's machine learning algorithms, it can pick up a lot of physiological clues under our skin. LEE: So they actually can pick up these reflections from your face. RAZ: But what are they actually picking up? LEE: OK. So the vascular system and the heartbeat - heartbeat variability, which is extremely important to measure your stress. And then you have breathing. So these are very, very good indications of how much stress you are in. And they all come to modulate the blood flows on your whole body. And all these activities comes out as a symphony, basically, on your face. And from there, what we're doing right now is to basically decode it, like, decipher these secrets coming out of your face to know what's going on in your physiological system. RAZ: And over time, Kang has found markers for what happens in our bodies when we're, say, disgusted or surprised or angry. And I know what you're thinking. We just heard Lisa Feldman Barrett say that universal emotions don't exist and that emotions vary based on culture and context. And for the most part, Kang, well, he says. . . LEE: Yeah, so I agree with Lisa. Cognitive appraisal is a very, very important part of our emotional experiences. So, for example, the same physiologic activities but in two different situations would give rise to entirely different emotional experiences. So let's say you have short of breath, you know, your heart rate is pumping. One situation is you're fearful, but the other situation maybe you are in love. So - and this is all about you making this cognitive appraisal of the situation. RAZ: So is that why your team is still at, like, 64 percent accuracy when it comes to fear? Because the physical markers, the physiological markers for fear are so much harder to distinguish with, like, falling in love or anger. LEE: Yes. We tend to think about emotions as kind of natural, instinctive reactions. But rather, the brain plays a very important role to say, I'm falling in love. That's why my heart quickens. You know, I start to sweat. You know, my face turns red. On the other hand, you may say, I just feel angry. My heart quickens. My face turns red and my breathing become faster - something like that. But fortunately, though, we as humans tend to respond to a host of things in a very similar way. And because of this, then you say, OK, you know, these are the signatures of disgust. These are the signatures of joy. RAZ: I mean, if, in fact, you can connect certain physiological signals to specific emotions, surely it has to be adjusted for cultural context - right? - because there are some cultures that we know of that don't experience sadness because they don't have the language. The word doesn't exist. LEE: Totally. I mean, just - I grew up in China. So - and then I started to study psychology in college. And one of the words I came across was depression in English. And I had no idea what depression was because in China, in terms of diagnosis, it's called love sickness. So basically, the appraisal of these psychiatrists about your depression is not because your bio-chemical imbalance but just simply you cannot find a partner for life. RAZ: Wow. LEE: So just think about it. This is just one simple example. There are many, many other cultures that may have language for certain emotions but not for some others. So these differences must come to play. RAZ: So essentially, the thinking is that, over time, as this technology gets better and better and as the appraisals really do match the physiological signals, you can adjust this for different cultures and different contexts? LEE: Well, certainly. So what we're doing now, actually, is not only adjust according to your culture. We are adjusting according to you. Just imagine a few years down the road, there is a robot at home. And your robot basically is videotaping you and extracting physiological activities from you by remotely using our technology, for example. And then they figure out, you know, in these kinds of situations, oh, that's why you are happily reading books to your kids. This situation - you know, you get frustrated when you burn your toast or something like this, right? They learn from your past information about you. And then they can pretty much, much better about your personal emotional experiences. RAZ: OK, so there could be in the future a robot in your house that's just constantly assessing you based on your face and what's going on behind your skin - and just that that could be a reality in the future? LEE: Oh, yeah, definitely. It's in the very, very near future, not remote future. RAZ: So I don't want that future. Go back. Go back to the past. Go back, future. It's scary. That scares me. I don't want a robot knowing my emotions. LEE: Really? Because I thought this will be good - you know, I discovered the activities we can pick up are very, very useful for monitoring our health. For example, you know, we - by looking at the facial blood flow changes, we discover some people have arrhythmia. And we can tell them their blood pressures as well. And from there, we actually now can measure your stress very accurately. So this is, to me, very, very useful. And then from there - I'm thinking about my parents, right? They're in China. So they live by themselves. And they are in the 90s, you know. And sometimes they become lonely. So I try to call them every day. But that's still only like 10, 15 minutes. But they want to sometimes have conversations with someone. So this is what I'm kind of envisioning - you know, creating a robot for my parents, so they can converse them about their inner emotions. And sometimes they may feel depressed a little bit, and then the robot can help them to lift their spirit up. (SOUNDBITE OF MUSIC)RAZ: That's Kang Lee. He's a professor at the University of Toronto. You can see his entire talk at ted. com. GUY RAZ, HOST:  What's the - what's the connection between lying and our emotions? KANG LEE: When we lie to cover up our own transgressions, you have this fear of being caught, and also you have this shame and guilt and also the delight they associate with that. RAZ: This is Kang Lee. LEE: I study developmental neuroscience with a special focus on emotion, lie telling, in children and adults. RAZ: And Kang says specifically when kids lie, they learn pretty early on how to hide all of these emotions and control their facial expressions. LEE: You have to manage all your facial expressions, your body language, the words you are going to choose to make your lie stick. RAZ: So say I guess I should mention, Kang, that. . . LEE: Yeah. RAZ: . . . Just like a couple days ago, the remote control on our Google Chrome thing was broken. Like, it was peeled back. Like, you had to use, like, you know, jaws of steel to peel it back. And I looked at my kids and I said, what happened? And they both looked at me and said I don't know. I said, you don't know what happened to this remote control? And they said, no. I said, did you break it? No. Did you touch it? Yes, but we didn't break it. LEE: Exactly. Yeah, and they - when they answer these questions, they actually look right into your eyes, right? They did not actually avert their eyes, right? RAZ: Right into my eyes - no, I didn't do it. LEE: Yes, indeed. And then the majority of kids, when they lie, they actually look into your face and very seriously say, no. I'm talking about very young kids. These could be 2 years old or 3 years old. So they are very good at managing their facial expressions. (SOUNDBITE OF MUSIC) RAZ: Now, remember how Lisa Feldman Barrett just said that our ability to detect emotions in other people is unreliable and that a facial expression doesn't necessarily tell you much about how someone's feeling? Well, Kang believes there might be a way to detect emotions without using our eyes. Kang Lee explains his research from the TED stage. (SOUNDBITE OF TED TALK) LEE: We know that underneath our facial skin, there's a rich network of blood vessels. When we experience different emotions, our facial blood flow changes subtly. So by looking at facial blood flow changes, we can reveal people's hidden emotions. We have developed a new imaging technology we call transdermal optical imaging. To do so, we use a regular video camera to record people when people experience various hidden emotions. And then, using our image processing technology, we can extract transdermal images of facial blood flow changes. And using this technology, we can now reveal the hidden emotion associated with lying and therefore detect people's lies with an accuracy at about 85 percent. (SOUNDBITE OF MUSIC) RAZ: So let me see if I understand this correctly. You can train a video camera onto a human face, and then, with your technology, you can analyze facial blood flow, and then that indicates how a person is feeling with an accuracy rate of 85 percent when it comes to lying. LEE: Yes. RAZ: But can you do that with other emotions? LEE: Well, right now, we can measure your emotion at about 93 percent accuracy to differentiate between three states - positive, neutral and negative. And then with regard to specific emotions, depends on the situations. For example, disgust - we can do about 89 percent accuracy. But some other emotions are not very good. Fear - fear is the most difficult one, and it's about 64 percent. But still, you know, we have a long way to go to be able to pick up information from your face to say, ah, you know, Guy now is - he's experiencing fear or something like that. (SOUNDBITE OF MUSIC) RAZ: In a minute, just how Kang Lee's technology works and whether we can actually keep our emotions private ever again. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC) RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, Decoding Our Emotions. And we were just hearing from psychologist Kang Lee describe a new technology that he's created in his lab by using something called transdermal optical imaging. LEE: Yes, and the technology actually is very simple. RAZ: So basically, it starts with our skin, which is translucent. LEE: So when lights come into your face, it does not bounce back right away. So it actually penetrates to the deeper layers of our face. RAZ: And just by using a regular video camera to record a face. . . LEE: You know, the cameras on our iPhone is amazingly good. RAZ: . . . That video footage - when it's run through Kang's machine learning algorithms, it can pick up a lot of physiological clues under our skin. LEE: So they actually can pick up these reflections from your face. RAZ: But what are they actually picking up? LEE: OK. So the vascular system and the heartbeat - heartbeat variability, which is extremely important to measure your stress. And then you have breathing. So these are very, very good indications of how much stress you are in. And they all come to modulate the blood flows on your whole body. And all these activities comes out as a symphony, basically, on your face. And from there, what we're doing right now is to basically decode it, like, decipher these secrets coming out of your face to know what's going on in your physiological system. RAZ: And over time, Kang has found markers for what happens in our bodies when we're, say, disgusted or surprised or angry. And I know what you're thinking. We just heard Lisa Feldman Barrett say that universal emotions don't exist and that emotions vary based on culture and context. And for the most part, Kang, well, he says. . . LEE: Yeah, so I agree with Lisa. Cognitive appraisal is a very, very important part of our emotional experiences. So, for example, the same physiologic activities but in two different situations would give rise to entirely different emotional experiences. So let's say you have short of breath, you know, your heart rate is pumping. One situation is you're fearful, but the other situation maybe you are in love. So - and this is all about you making this cognitive appraisal of the situation. RAZ: So is that why your team is still at, like, 64 percent accuracy when it comes to fear? Because the physical markers, the physiological markers for fear are so much harder to distinguish with, like, falling in love or anger. LEE: Yes. We tend to think about emotions as kind of natural, instinctive reactions. But rather, the brain plays a very important role to say, I'm falling in love. That's why my heart quickens. You know, I start to sweat. You know, my face turns red. On the other hand, you may say, I just feel angry. My heart quickens. My face turns red and my breathing become faster - something like that. But fortunately, though, we as humans tend to respond to a host of things in a very similar way. And because of this, then you say, OK, you know, these are the signatures of disgust. These are the signatures of joy. RAZ: I mean, if, in fact, you can connect certain physiological signals to specific emotions, surely it has to be adjusted for cultural context - right? - because there are some cultures that we know of that don't experience sadness because they don't have the language. The word doesn't exist. LEE: Totally. I mean, just - I grew up in China. So - and then I started to study psychology in college. And one of the words I came across was depression in English. And I had no idea what depression was because in China, in terms of diagnosis, it's called love sickness. So basically, the appraisal of these psychiatrists about your depression is not because your bio-chemical imbalance but just simply you cannot find a partner for life. RAZ: Wow. LEE: So just think about it. This is just one simple example. There are many, many other cultures that may have language for certain emotions but not for some others. So these differences must come to play. RAZ: So essentially, the thinking is that, over time, as this technology gets better and better and as the appraisals really do match the physiological signals, you can adjust this for different cultures and different contexts? LEE: Well, certainly. So what we're doing now, actually, is not only adjust according to your culture. We are adjusting according to you. Just imagine a few years down the road, there is a robot at home. And your robot basically is videotaping you and extracting physiological activities from you by remotely using our technology, for example. And then they figure out, you know, in these kinds of situations, oh, that's why you are happily reading books to your kids. This situation - you know, you get frustrated when you burn your toast or something like this, right? They learn from your past information about you. And then they can pretty much, much better about your personal emotional experiences. RAZ: OK, so there could be in the future a robot in your house that's just constantly assessing you based on your face and what's going on behind your skin - and just that that could be a reality in the future? LEE: Oh, yeah, definitely. It's in the very, very near future, not remote future. RAZ: So I don't want that future. Go back. Go back to the past. Go back, future. It's scary. That scares me. I don't want a robot knowing my emotions. LEE: Really? Because I thought this will be good - you know, I discovered the activities we can pick up are very, very useful for monitoring our health. For example, you know, we - by looking at the facial blood flow changes, we discover some people have arrhythmia. And we can tell them their blood pressures as well. And from there, we actually now can measure your stress very accurately. So this is, to me, very, very useful. And then from there - I'm thinking about my parents, right? They're in China. So they live by themselves. And they are in the 90s, you know. And sometimes they become lonely. So I try to call them every day. But that's still only like 10, 15 minutes. But they want to sometimes have conversations with someone. So this is what I'm kind of envisioning - you know, creating a robot for my parents, so they can converse them about their inner emotions. And sometimes they may feel depressed a little bit, and then the robot can help them to lift their spirit up. (SOUNDBITE OF MUSIC) RAZ: That's Kang Lee. He's a professor at the University of Toronto. You can see his entire talk at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-11-592766459": {"title": "At SXSW, London Mayor Sadiq Khan To Touch On Policymakers' Role In Tech Revolution : NPR", "url": "https://www.npr.org/2018/03/11/592766459/at-sxsw-london-mayor-sadiq-khan-to-touch-on-policymakers-role-in-tech-revolution", "author": "No author found", "published_date": "2018-03-11", "content": "MICHEL MARTIN, HOST: Now to Austin, Texas, where the annual South by Southwest festival kicked off this past week. It's the 32nd annual festival. Now, South by Southwest used to be all about music, and music is still at the heart of it, so in a few minutes, we'll hear about a few of the featured artists who will be performing. But South by Southwest has also become a place to talk about cutting-edge technologies and some of the issues raised by those technologies. Our next guest is going to talk about that. He's the mayor of London, Sadiq Khan. He's in Austin to offer a keynote. Mayor Khan was elected in 2016 - the first Muslim mayor of London - and that, along with his inspiring personal story, has made him an international figure. He's the son of Pakistani immigrants. He grew up in public housing in South London. But he's also made it a mission to use government to address issues of the future as well as of the moment, which is what he'll talk about tomorrow. And we called him to see if we can persuade him to offer us a bit of a preview. Mayor Khan is with us now from Austin. Mr. Mayor, thank you so much for speaking with us. SADIQ KHAN: Absolute pleasure to be talking to NPR. Good to be talking to you. MARTIN: And, of course, two crucial questions - have you had any barbecue, and are you going to buy some boots? KHAN: (Laughter) I'll tell you all, it's a pleasure to be here at South by Southwest in Austin. And the hospitality and warmth of Austinites is legendary, but now I've experienced it at first hand. And the food - put aside, you know, barbecue food and, you know, the boots and the hats that they wear around here (laughter), but the food is amazing. . . MARTIN: (Laughter). KHAN: . . . All sorts of cuisine. I had Mexican last night. There's a number of mayors here in Austin - 40 mayors from across the U. S. Conference. And they're having a barbecue tonight, so I'm really looking forward to catching up with mates - we are friends - but also enjoying the Austin, Texas barbecue cuisine. MARTIN: Absolutely. But the boots, though. I mean. . . KHAN: (Laughter). MARTIN: I mean the boots. KHAN: I'm not sure. . . MARTIN: I'm just saying. KHAN: I've got two teenage daughters, and I'm not sure if they'd allow me back in the home with the boots, but I'll try my best. MARTIN: (Laughter) OK. But before we talk about your keynote, I did want to ask you about some of the issues in London. I mean. . . KHAN: Sure. MARTIN: . . . London has really been at the center of it. I mean, the shock of the Brexit vote, which most Londoners opposed, and then several terrorist attacks at Westminster and then London Bridge. And that sparked some back and forth between you and President Trump. I just - I know it's a difficult question, but how would you assess the public mood of Londoners? KHAN: Oh, we're a resilient city. We're a resilient people. And, you know, we've survived many previous difficult times, and we will survive this time as well. And one of the ways we do is because it's who we are. You know, we know that we will bounce back, and we've got friends, no matter how difficult the times. But the cultural links we have with other parts of the world, particularly America, are huge. And here in Austin, for example, every other accent I hear is a British London accent, which demonstrates (laughter) that we're everywhere. And long may that continue. MARTIN: Let's talk about your speech tomorrow. South by Southwest is known for being a showcase for innovation, and that's something you're going to talk about. Can you just give us one or two lines - maybe the executive summary, if you will - about what you're going to talk about tomorrow? KHAN: Well, one of the things that your listeners will recognize, I hope, is we are currently going through a fourth, if you like, industrial revolution - a tech revolution. And what I'll be saying during the course of my speech is politicians and policymakers have had their head in the sands while all this revolution's been taking place around us with huge advances in technology, in social media, in the shared economy, the peer-to-peer economy. And we've not been making sure that regulation evolves as fast as our economies. And the consequences, for example - something we've experienced on both sides of the Atlantic - where there is concerns about elections being interfered with through social media. There are concerns around fake news. There are concerns around how social media can be used to amplify messages of hatred and division. And I think tech companies, politicians, have a responsibility to respond to those concerns. MARTIN: Technology individuals in this field have, I think, tended to make the argument that they have thrived by being free - free to, you know, think wild thoughts. I mean, not to quote Rihanna here, but that's where they feel their strength has come from - is they can sort of think their thoughts, they can dream things that people have not imagined. And they have been very resistant to regulation as a consequence of that because they argue that that inhibits that ability to think kind of wildly and experimentally. What is your argument to them? KHAN: Well, disruptive pioneers is nothing new. For example, I think it's fantastic. You can get a date using social media. You can even fall in love, dare I say. You can book your holiday remotely from home without having to go to a travel agent, going through brochures and stuff. That's a great, great thing, and nobody wants to stop that. But at the same time, this same medium is being used to spread messages of hatred to divide communities. And just like we stop hate speech that's said face-to-face, we've got to work with the disrupters to stop hate speech that works via social media. MARTIN: Give me an example - I'm told that you have actually brought a small selection of some of the racist tweets that have been directed at you as mayor. Is that so - and that you might be sharing some of those? KHAN: I don't talk about some of the vile stuff that I receive on a daily basis - I mean, literally thousands and thousands of tweets. And. . . MARTIN: Really? You get thousands of hate messages every day? KHAN: Yeah. MARTIN: And are they local? Are they - do you have a sense that they're from around the world? I mean, I guess what I'm - what do you think is at play there? KHAN: I'm not the only person who receives, you know, these sorts of messages. You speak to a lot of women in public life, a lot of minorities in public life - they receive these sorts of messages. And my concern is with free speech, and with technology comes responsibility. And I think it's really important for mayors like me, politicians and policymakers to work with the tech companies to make sure we address this issue. MARTIN: But why is that a mayoral responsibility, per se? It seems, given that these algorithms are global, these companies are international - I mean, is this a primarily local responsibility, or is it mainly that you're using your platform to call attention to this as an issue? KHAN: Well, I'm using the bully pit of City Hall as a mayor to raise issues that matter to my citizens. But this morning, I met with 40 or so mayors from the U. S. during the U. S. Conference of Mayors. And what's clear to me is, actually, mayors can move far more nimbly and swifter. We've got far better relationships with these giant tech companies and with innovation. But also national governments in all countries just move far more slower. And so we as mayors, you know, bring often pressure on national governments to take swift action. One of the great things about South by Southwest is the sort of people who are here - you've got politicians, you've got policymakers, you've got innovators, you've got startups, you got scale-ups, you've got (unintelligible) from giant tech companies. And so when you're given the privilege of doing a keynote speech at South by Southwest, you know, what I'm using it for is, of course, to use it as a way to drum up investment for London. And, of course, I'll be reminding people that London is, you know, a great place to attract investment - the data, the connectivity, the innovation makes London a byword for a smart city - but also raising issues that are really important to citizens in London, citizens in the UK. But I know from the conversations I've heard it's troubling many citizens in the USA as well. MARTIN: Nice plug as we enter the travel season for your city, Mr. Mayor. I did take note of that, and. . . KHAN: (Laughter). MARTIN: . . . But you don't have cowboy boots so. . . KHAN: (Laughter). MARTIN: . . . Just saying. That was the mayor of London, Sadiq Khan, talking to us about the keynote speech he'll give tomorrow at the South by Southwest festival in Austin, Texas. We reached him there. Mr. Mayor, thank you so much for speaking with us. KHAN: Cheers. It's my pleasure, Michel. Take care. MICHEL MARTIN, HOST:  Now to Austin, Texas, where the annual South by Southwest festival kicked off this past week. It's the 32nd annual festival. Now, South by Southwest used to be all about music, and music is still at the heart of it, so in a few minutes, we'll hear about a few of the featured artists who will be performing. But South by Southwest has also become a place to talk about cutting-edge technologies and some of the issues raised by those technologies. Our next guest is going to talk about that. He's the mayor of London, Sadiq Khan. He's in Austin to offer a keynote. Mayor Khan was elected in 2016 - the first Muslim mayor of London - and that, along with his inspiring personal story, has made him an international figure. He's the son of Pakistani immigrants. He grew up in public housing in South London. But he's also made it a mission to use government to address issues of the future as well as of the moment, which is what he'll talk about tomorrow. And we called him to see if we can persuade him to offer us a bit of a preview. Mayor Khan is with us now from Austin. Mr. Mayor, thank you so much for speaking with us. SADIQ KHAN: Absolute pleasure to be talking to NPR. Good to be talking to you. MARTIN: And, of course, two crucial questions - have you had any barbecue, and are you going to buy some boots? KHAN: (Laughter) I'll tell you all, it's a pleasure to be here at South by Southwest in Austin. And the hospitality and warmth of Austinites is legendary, but now I've experienced it at first hand. And the food - put aside, you know, barbecue food and, you know, the boots and the hats that they wear around here (laughter), but the food is amazing. . . MARTIN: (Laughter). KHAN: . . . All sorts of cuisine. I had Mexican last night. There's a number of mayors here in Austin - 40 mayors from across the U. S. Conference. And they're having a barbecue tonight, so I'm really looking forward to catching up with mates - we are friends - but also enjoying the Austin, Texas barbecue cuisine. MARTIN: Absolutely. But the boots, though. I mean. . . KHAN: (Laughter). MARTIN: I mean the boots. KHAN: I'm not sure. . . MARTIN: I'm just saying. KHAN: I've got two teenage daughters, and I'm not sure if they'd allow me back in the home with the boots, but I'll try my best. MARTIN: (Laughter) OK. But before we talk about your keynote, I did want to ask you about some of the issues in London. I mean. . . KHAN: Sure. MARTIN: . . . London has really been at the center of it. I mean, the shock of the Brexit vote, which most Londoners opposed, and then several terrorist attacks at Westminster and then London Bridge. And that sparked some back and forth between you and President Trump. I just - I know it's a difficult question, but how would you assess the public mood of Londoners? KHAN: Oh, we're a resilient city. We're a resilient people. And, you know, we've survived many previous difficult times, and we will survive this time as well. And one of the ways we do is because it's who we are. You know, we know that we will bounce back, and we've got friends, no matter how difficult the times. But the cultural links we have with other parts of the world, particularly America, are huge. And here in Austin, for example, every other accent I hear is a British London accent, which demonstrates (laughter) that we're everywhere. And long may that continue. MARTIN: Let's talk about your speech tomorrow. South by Southwest is known for being a showcase for innovation, and that's something you're going to talk about. Can you just give us one or two lines - maybe the executive summary, if you will - about what you're going to talk about tomorrow? KHAN: Well, one of the things that your listeners will recognize, I hope, is we are currently going through a fourth, if you like, industrial revolution - a tech revolution. And what I'll be saying during the course of my speech is politicians and policymakers have had their head in the sands while all this revolution's been taking place around us with huge advances in technology, in social media, in the shared economy, the peer-to-peer economy. And we've not been making sure that regulation evolves as fast as our economies. And the consequences, for example - something we've experienced on both sides of the Atlantic - where there is concerns about elections being interfered with through social media. There are concerns around fake news. There are concerns around how social media can be used to amplify messages of hatred and division. And I think tech companies, politicians, have a responsibility to respond to those concerns. MARTIN: Technology individuals in this field have, I think, tended to make the argument that they have thrived by being free - free to, you know, think wild thoughts. I mean, not to quote Rihanna here, but that's where they feel their strength has come from - is they can sort of think their thoughts, they can dream things that people have not imagined. And they have been very resistant to regulation as a consequence of that because they argue that that inhibits that ability to think kind of wildly and experimentally. What is your argument to them? KHAN: Well, disruptive pioneers is nothing new. For example, I think it's fantastic. You can get a date using social media. You can even fall in love, dare I say. You can book your holiday remotely from home without having to go to a travel agent, going through brochures and stuff. That's a great, great thing, and nobody wants to stop that. But at the same time, this same medium is being used to spread messages of hatred to divide communities. And just like we stop hate speech that's said face-to-face, we've got to work with the disrupters to stop hate speech that works via social media. MARTIN: Give me an example - I'm told that you have actually brought a small selection of some of the racist tweets that have been directed at you as mayor. Is that so - and that you might be sharing some of those? KHAN: I don't talk about some of the vile stuff that I receive on a daily basis - I mean, literally thousands and thousands of tweets. And. . . MARTIN: Really? You get thousands of hate messages every day? KHAN: Yeah. MARTIN: And are they local? Are they - do you have a sense that they're from around the world? I mean, I guess what I'm - what do you think is at play there? KHAN: I'm not the only person who receives, you know, these sorts of messages. You speak to a lot of women in public life, a lot of minorities in public life - they receive these sorts of messages. And my concern is with free speech, and with technology comes responsibility. And I think it's really important for mayors like me, politicians and policymakers to work with the tech companies to make sure we address this issue. MARTIN: But why is that a mayoral responsibility, per se? It seems, given that these algorithms are global, these companies are international - I mean, is this a primarily local responsibility, or is it mainly that you're using your platform to call attention to this as an issue? KHAN: Well, I'm using the bully pit of City Hall as a mayor to raise issues that matter to my citizens. But this morning, I met with 40 or so mayors from the U. S. during the U. S. Conference of Mayors. And what's clear to me is, actually, mayors can move far more nimbly and swifter. We've got far better relationships with these giant tech companies and with innovation. But also national governments in all countries just move far more slower. And so we as mayors, you know, bring often pressure on national governments to take swift action. One of the great things about South by Southwest is the sort of people who are here - you've got politicians, you've got policymakers, you've got innovators, you've got startups, you got scale-ups, you've got (unintelligible) from giant tech companies. And so when you're given the privilege of doing a keynote speech at South by Southwest, you know, what I'm using it for is, of course, to use it as a way to drum up investment for London. And, of course, I'll be reminding people that London is, you know, a great place to attract investment - the data, the connectivity, the innovation makes London a byword for a smart city - but also raising issues that are really important to citizens in London, citizens in the UK. But I know from the conversations I've heard it's troubling many citizens in the USA as well. MARTIN: Nice plug as we enter the travel season for your city, Mr. Mayor. I did take note of that, and. . . KHAN: (Laughter). MARTIN: . . . But you don't have cowboy boots so. . . KHAN: (Laughter). MARTIN: . . . Just saying. That was the mayor of London, Sadiq Khan, talking to us about the keynote speech he'll give tomorrow at the South by Southwest festival in Austin, Texas. We reached him there. Mr. Mayor, thank you so much for speaking with us. KHAN: Cheers. It's my pleasure, Michel. Take care.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-15-593863645": {"title": "Robots Are Now 'Creating New Robots,' Tech Reporter Says : NPR", "url": "https://www.npr.org/2018/03/15/593863645/robots-are-now-creating-new-robots-tech-reporter-says", "author": "No author found", "published_date": "2018-03-15", "content": "TERRY GROSS, HOST: This is FRESH AIR. I'm Terry Gross. There's been a huge boon in the evolution of artificial intelligence over just the past five years due to more high-powered computers, new kinds of programs and an increasing pool of data to learn from. This has led to new developments such as driverless cars, machines that can carry on a conversation and robots that can interpret medical scans. Today, we're going to talk about this rapidly changing world of artificial intelligence and how these new high-tech developments may change our lives for better or for worse. I've also just described the beat that my guest, tech reporter Cade Metz, covers for The New York Times. He was formerly a staff writer at Wired magazine. He's currently at work on a book about the visionaries behind new developments in artificial intelligence. Cade Metz, welcome to FRESH AIR. CADE METZ: Thanks for having me. GROSS: So you've been writing a lot about artificial intelligence, and you're writing a book about it, too, now. So let's talk about that. What is artificial intelligence? What do we mean now when we say it? METZ: That's a really good question. And it's, on some levels, a hard question to answer just because people tend to throw around this term, and they have for a long time, to describe almost anything. And use of this term has accelerated in recent months, in recent years. Anything that is automated in some way or even any computing technology that's advancing the state of the art, even in a small or minor way, is described as artificial intelligence. So it creates all this confusion about what that means, right? That's a grandiose term. It has a long history. It dates back to the '50s when this group of academics got together to truly build artificial intelligence, a machine that would mimic the intelligence of a human. But amidst all that hype, a lot of it which can be sort of just pushed aside, there's a very real change going on now, and it started about five years ago. And essentially, it's a fundamental change in the way that we build technology. What has changed now is that a certain type of computer algorithm - it's called a neural network - has started to work in ways that it did not work in the past. And what this algorithm allows us to do, allows those engineers to do is build systems that can learn tasks on their own. So for instance, a neural network, if it analyzes millions of photos, it can learn to recognize faces and objects in those photos. Most of us have used Facebook. If you've ever used Facebook and you've posted a photo of friends and family, it can recognize those faces. Well, that's driven by one of these neural networks, which can learn that task on its own. We're also seeing these systems in speech recognition. So you can now bark commands into your cellphone. It can recognize what you say. It's used in machine translation where machines translate from one language to another. GROSS: So let's back up. These are systems that can - what did you say? They can learn on their own. METZ: Exactly. They're called neural networks. And that's a metaphor. They're meant to mimic the web of neurons in the brain. But really, they're just math. They're mathematical algorithms that analyze data. A group of millions of photos is just data, and these systems look for patterns in that data - the way a nose curves on a person's face, the way the lines of an eye come together. They can identify those patterns, the math of those visual patterns, for instance, and then learn to recognize what is in an image. In the same way, it can recognize patterns in voice data and learn to recognize what people say. GROSS: So another example you've mentioned in your reporting is how there are now some computers that can read scans to see, for instance, if there's a nodule in a lung that might be a sign of cancer. METZ: Exactly. It's the same basic technology. The same technology that allows Facebook to recognize faces on its social network can allow machines to analyze medical scans and recognize when there are signs of illness and disease. It can be applied to lung scans helping to identify cancer, retinal scans helping to identify signs of diabetic blindness. As time goes on, we're going to see machines improve in this capacity. And what they can kind of do is provide a first line of defense against these types of diseases. That's particularly useful in, say, the developing world, where you don't have as many doctors. You can use these types of systems to analyze the scans of patients that would otherwise require a human doctor. GROSS: And so smart speakers, where you could tell the speaker what to play or what to turn on, that's also artificial intelligence, the same kind of learning pattern that you're describing. METZ: It's exactly the same thing. So the - you know, the Amazon Echo, which sits on the coffee tables of so many people now, or Siri on the iPhone, they train these systems using real human speech, and they learn to recognize what you say. The next step is really understanding what you say. I think what you'll notice, and this shows the limitations of AI today, the next step is to really, truly understand what you say and act on it in a way that a human would. We're not there yet. These systems can recognize what you say and, on some level, understand. They can answer basic questions, respond to basic commands. But they can't really have a back-and-forth conversation the way you might expect them to, or you might want them to, or that might make it easier to really interact with machines. GROSS: On a related note, like, when you talk to, say, Siri or to a smart speaker, how is the voice created that's responding to you? Is that actually a person's voice that's been programmed in such a way so that it can be responding to what you're saying? Or is that, like, a combination, like, a mashup of a lot of different human voices that have become this one robotic voice? METZ: Well, it depends on which system you use. But for instance, Google's system is now, in much the same way, trained on real human speech. So it can - it has analyzed an enormous amount of human speech, and it can use that to create its own. This was just recently rolled into the new Google phones. And you can tell the difference. You know, when I was testing one of these phones late last year, I showed it to my 13-year-old daughter, who's a Apple iPhone user. She could hear the difference between her iPhone and this new Google phone. It sounded more like a human because it had indeed been trained on human speech. It's the same basic technology - those neural networks that I was talking about. It recognizes those patterns, including the other - the way your voice may rise at the end of the sentence, or you may exhale in certain places. It duplicates those kind of things and gets a little bit closer to human speech. But I will say, and my daughter will say the same thing. It's not identical to human speech. It's still very limited. And that's the case with all these systems. As much as things have progressed in all these areas over the past five years, they still have a very long way to go to truly mimic the way you and I interact. GROSS: What are some of the limitations that researchers are trying to overcome now? METZ: Well, there are myriad limitations. For instance, researchers have realized that you can fool these systems into, say, seeing things that aren't there or to thinking they see things when they aren't really. That's a real issue if you talk to people who are kind of on the front lines of this technology. And it becomes a greater worry when you realize that these same technologies are helping in the development of the driverless cars that are being tested in Silicon Valley and Arizona and other places. In many respects, this is how these cars see and respond to what is around them. If you have flaws like that in a self-driving car or, let's say, a surveillance camera, that becomes a big issue. GROSS: So you've described that these neural networks, the algorithms that are used for artificial intelligence that can learn to do things and kind of mimic human thought and behavior, they're meant to mimic the neurons in the brain. So what does that mean exactly? Like, how do you get algorithms to mimic the way the brain works? METZ: This is a key question. So let me make it clear that is just a metaphor in many respects, right? This is an old idea. It dates back to the '50s and the late '50s when they first built these types of algorithms. And they were designed to mimic the brain. The reality is they can't mimic the brain because we don't even know how the brain works. GROSS: Yeah. I was thinking that, too (laughter). Yeah. METZ: So let's get to the point where we understand the brain completely. Then we can think about truly rebuilding the brain in digital form. That's an impossibility at this point. But essentially, you know, these algorithms are complex networks of mathematical operations. And each mathematical operation is referred to among those that build it as a neuron. OK. You're passing information between these mathematical operations. And it's - they're sort of constructed like a pyramid. So you have, you know, neurons at the bottom, mathematical operations at the bottom of this pyramid. And it will start to recognize if you're, say, trying to identify a face in a photo. It'll recognize a line here or a line there. And then that information will bubble up to other mathematical operations that will then start to put those lines together with, say, an eye or a nose. As you work up that pyramid, you get to the point where all these operations are helping the system understand what a human face looks like. This is not exactly the way the brain works. It's a very loose approximation. That needs to be understood if you're going to understand how these systems work. The problem is if you start talking about neural networks, you start assuming that the brain has been mimicked. And you said earlier in describing this that these systems are mimicking human thought. They are they are actually not mimicking human thought. We're still a very long way away from that. What they're doing is they're - they are learning specific tasks, right? Identifying a photo is a task that these systems can learn and learn to do pretty well. In some cases, they can perform on par with a human. But we are complex biological systems that do a lot of things well beyond that. And machines are a very long way from mimicking everything that we can do. GROSS: So we should take a short break here, and then we'll talk some more. If you're just joining us, my guest is Cade Metz. We're talking about artificial intelligence and new breakthroughs in related technology. He's a tech reporter for The New York Times. We'll be right back. This is FRESH AIR. (SOUNDBITE OF TODD SICKAFOOSE'S \"TINY RESISTORS\")GROSS: This is FRESH AIR. And if you're just joining us, we're talking about artificial intelligence and other breakthroughs in high tech. My guest, Cade Metz, is a tech correspondent for The New York Times and is writing a book about artificial intelligence. So, you know, getting back to voice recognition and trying to make computers conversational, tell the story of the - I don't know what to call it, a computer or what - that was being taught speech and ended up picking up a lot of like racist terms and Holocaust-denying expressions. And the company ended up discontinuing this. METZ: That was an earlier Microsoft project called Tay. And it's kind of a famous example of these types of systems gone wrong. It was an earlier system. And it had a - it had a serious design flaw. In many cases, it was just designed to repeat what the person had said to it in some way. And so humans interacting with this service online quickly realized this was the case and they could kind of coax the system into saying these racist and xenophobic things. Microsoft immediately took it offline. And since then, the technology has seriously improved. These systems that learn on their own have come to the fore. And Microsoft sees a real path towards creating a conversational system. The problem is that Tay, this bot designed and deployed in the past by Microsoft, has created a real conundrum, not only for Microsoft but for anyone else trying to build these conversational technologies. The thinking is that if you can get these new systems out in front of people and they will interact with them on their own and generate more conversational data that these systems can use that data to train themselves in ever more proficient ways. But these companies are wary of putting them out there because they know they're going to make mistakes. They know that they're going to learn from those human biases and the data. They know they're going to offend people in the end. And, you know, these are big companies with brands and reputations to protect. And they see this as a real stumbling block to reaching the point where they can build truly conversational systems. GROSS: As we were saying before, you know, computers have to like - robots have to learn in order to function. They first have to learn. And you write about reinforcement learning, how some bots like learn tasks through trial and error. And you describe a project that was a Google project creating a bot that could beat the world's best player at the game of Go. So how did that bot learn through trial and error and reward? It sounds like a really behavioral kind of approach to behavioral psychology - reward and punishment type of approach without the punishment. METZ: Well, I mean, you could add the punishment, to tell you the truth. GROSS: Oh, really? METZ: This project was a project out of a lab in London called DeepMind, which is now owned by Google. They built a machine, like you said, to play the ancient game of Go, which is the Eastern version of chess. It dates back thousands of years. And it's a national game in China and Japan and Korea. Just a few years ago, even experts in the field of AI assumed it would be at least another decade before we could build a machine that could crack the game of Go just because it's so complex. People like to say, including the designers of this machine at DeepMind, they like to say that there are more possible moves on a Go board than atoms in the universe. You can never build a system that could explore all the possibilities on the board and reach a conclusion. You just couldn't build a computer that was that powerful. But with these neural networks, what they were able to do is build a system that could learn to play Go on its own. And the way they did that was to supply it with millions of moves from professional human players. Once it learned the game, they essentially created two versions of the system. OK. It's a decent player at this point. And they would pit the system against itself. And it would play millions of games against itself. This is the reinforcement-learning aspect. It learned which moves in these games of self-play - which moves were successful and which were not. And in that way, it reached a level that was well above any human player. They took it to Korea a couple of years ago, and they beat the man who was essentially the Roger Federer of the Go world over the past decade. And then they took it to China, and they beat the No. 1 player in the world handily. It's a system that really shows what is possible with these algorithms. GROSS: In which part - you've programmed the bot to try to get more points and not lose points, and that's its goal. And so it learns that some decisions lead to gaining points and some decisions lead to losing points. And it tries to orient itself toward its goal of gaining points. Is that. . . METZ: Exactly. And if you do that on an enormous scale, over millions of games, you learn which moves are going to gain you points versus which ones are going to lose points, which ones are going to get you closer to winning the game - having more points than your opponent - and which you're going to lose. The thinking is that you can then apply this type of reinforcement learning to the real world. So, for instance, people have already started research where they try to train cars in this way. I mean, they are literally training cars in games. So, you know, you have racing games, you know, video games that teenagers play. If you can train this virtual car to play that video game with the same method - right? - certain moves mean more points or less, then you can eventually train a car to drive real roads. But that's certainly easier said than done. The real world is more complicated even than a video game, of course. And kind of transferring that knowledge that a system has learned in a game to the real world is a big step. GROSS: My guest is Cade Metz, a technology correspondent for The New York Times. After a break, we'll talk about why some artificial intelligence designers are worried about the possibility of creating intelligent machines that will refuse to allow humans to turn them off. And Milo Miles will review two Kronos Quartet albums, one in collaboration with Laurie Anderson, the other in collaboration with a group from Mali. I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF EMAV'S \"TECHMO\")GROSS: This is FRESH AIR. I'm Terry Gross, back with Cade Metz, a tech correspondent for The New York Times. He writes about new developments in artificial intelligence and robotics like driverless cars, machines that can carry on a conversation, robots that can interpret medical scans and how these and other new high-tech developments may change our lives for better or worse. He's at work on a book about the visionaries behind new developments in artificial intelligence. Now that we've learned so much about hacking through Russian interference in our election with like fake news and fake accounts on social media, the thought of there being robots that can be hacked by a malevolent actor is really scary because if robots are designed to do important things, whether it's like, you know, reading a lung scan to see if a person has cancer or whether it's a driverless car, if somebody can hack into that for devious reasons, that's kind of terrifying. So are designers worried about being able to adequately design safeguards? METZ: They are certainly worried. There was a report recently, pulled together by a group of technologists and researchers across various companies, research labs and think tanks across the U. S. and the U. K. , that looked at these very issues. And, again, they are myriad. You know, you talk about the possibility of hacking in to an autonomous system. Or, you know, certainly you can exploit its mistakes. Like I said, there are situations where these neural networks can be fooled into seeing things that aren't there - or failing to see things that are. That can be exploited by bad actors. But there are other issues that may be even closer. One of the things that these systems are doing extremely well and increasingly well is essentially fabricating images that look real. There was a team out of a company called Nvidia - they're a chipmaker, but they have an AI lab in Finland. And this team of researchers put together a system where essentially they built a neural network that analyzed millions of celebrity photos. And this system learned to build its own celebrity - essentially taking all those patterns that you see in pictures of Gwyneth Paltrow, or whoever else you might imagine is in this database. And then it would build this celebrity - this fake celebrity that looked vaguely like someone you would see on the red carpet, but you couldn't quite identify. Think of the implications of that in the age of fake news, right? These techniques will soon be applied not only to still images like that but to video and virtual reality. But even in the near term, as these systems become better and better at building fake images that look real, we're really going to have to change the way we look at anything we see online. GROSS: Yeah, that's a really scary thought. I will say you had some of the images of these computer-generated celebrity faces online. And it's kind of hilarious because they look exactly like generic celebrities - like there's a certain generic celebrity look, you know? And I have to say that the computers - the artificial intelligence captured it perfectly. METZ: Well, that's what is doing. It's identifying patterns, right? There are patterns that you and I respond to. And we say, ah, that's a celebrity, right? And some cases, those are intuitive. We can't necessarily articulate them. But these systems can identify those patterns and then make use of them. And in a way, that's a scary thing as well, right? These systems are operating on such a large scale. They're analyzing so many photos. We can never be sure exactly why they're making certain decisions. And that's a big worry as well. Think about that self-driving car again. If the self-driving car makes a mistake, you want to know why it made a mistake. Well, these systems operate in ways that even the people who build them do not completely understand. GROSS: Why not? Why don't they understand them? METZ: Well, can you analyze millions of photos in a matter of minutes? You can't, right? You cannot identify the patterns that that system identified. You can't pinpoint the exact decisions it made over the course of that analysis because you cannot do it yourself. GROSS: So in other words, you have no idea why the size of the woman celebrity's lips are they are or why the length of stubble on the male celebrity's beard is - like the. . . METZ: Absolutely, or. . . GROSS: . . . Or, you know, what's considered like the perfect eyes for a celebrity. Like, the artificial intelligence has made a gazillion calculations, and you're just kind of generalizing when you say something. METZ: Absolutely, or think about it like this. Think back to what we discussed about the game of Go and that system that the DeepMind Lab in London built to play the game of Go. Some of the designers on that team were very good Go players. When that system was in the middle of one of those high-stakes games in Korea and China, they had no clue what it was doing. It was operating at a level that they could not understand. And these are some of the brightest people on earth literally. And over the course of those five-hour matches, where this machine is playing in ways that no human has ever played literally - and winning in that way, beating the best humans on Earth - the people who designed that machine are unclear what it's doing and why. It is playing at the level they could never play on their own. GROSS: And the machine isn't going to be rattled by anxiety. METZ: That's also true. And it doesn't need to sleep. GROSS: Right. OK, so let's take a short break here, and then we'll talk some more. If you're just joining us, my guest is Cade Metz. He's a tech correspondent for The New York Times. We're going to talk more about artificial intelligence after we take a short break. This is FRESH AIR. (SOUNDBITE OF OF MONTREAL SONG \"FABERGE FALLS FOR SHUGGIE\")GROSS: This is FRESH AIR. And if you're just joining us, my guest is Cade Metz. He's a tech correspondent for The New York Times where he writes about artificial intelligence, driverless cars, robotics, virtual reality and other emerging areas in high tech. OK, so another fear that you were right about - that some people have about artificial intelligence - and I think these are designers who are worried about it - is the fear that AI systems will learn to prevent humans from turning them off. When I read this, of course, the first thing I thought of is the computer HAL in \"2001: A Space Odyssey\" because HAL is refusing to be turned off. METZ: Right. GROSS: So how would you elaborate on what this fear is - what the danger is? METZ: You know, I like that you bring up HAL in \"2001\" - one of my favorite movies, certainly. And I have to say it's the favorite movie of many of these AI researchers who are designing these systems. If you go into the personal conference room of the top AI researcher at Facebook, he's got images from \"2001\" all over this conference room. My point is that pop cultural images, movies and books influence not only the way these designers think but the way you and I think and the people who run these companies think. We tend to think about those doomsday scenarios because we've thought about them a lot over the decades through pop culture, right? I think that's part of it - is that people on some level relate to that, and they've worried about that in some way. And so as these machines get better, you tend to worry about that as well. That said, among many of these researchers, there is a real concern that as time goes on this will be a big problem, for those reasons I talked about - that these machines learn in ways that we cannot completely understand. If you have a car, for instance, that learns by playing, you know, a video game, it happens on such a large scale. It plays the game so much that there are points where it's going to do things that we humans don't expect it to do. And as it reaches ever more for that higher point total, it's going to try to maximize its point total in every respect. The worry is that it's going to maximize it in ways that we don't want it to. That doomsday scenario is that if it is trying to get to the most points, well, it's not going to let us turn it off. Now, on some level, that again is not something that we can see happening any time soon. That said, these systems that learn on their own are getting better and better - meaning the algorithms are getting better and better. And the hardware that runs these algorithms, that allows them to train ever more - you know, with ever more speed, are getting better and better. And so the worry is that as we improve the algorithms, as we get faster computers, that we will somehow cross that threshold where the machines are really behaving in ways not only that we don't understand but that we can't really control. And among many thinkers in this area, they're worried - even at this early stage. And it is early. If that ever happens, it's years and years away. But they're worried at this early stage that we will cross that threshold and not know it. What they say is that we might as well start thinking about it now because it is potentially such a risk. And when they make that argument, it's hard to argue back. Why not start worrying now and try to build safeguards into these systems rather than wait? GROSS: Can you give us an example of the kind of scenario where a bot would not allow itself to be turned off because it was pursuing the reward that it was programmed to pursue, like getting points for making the right decision? METZ: Well, I mean, loosely speaking - right? - if you're trying to - I mean, again, this is largely theory. But if you're trying to maximize your points, you're not going to keep accumulating points if you're turned off, right? You could think about it in that respect. But think about - let's back up just a little bit. Think about it in this respect. There's a lab in San Francisco. It's called OpenAI. And it was founded by Elon Musk, the CEO of Tesla among others. And they specialize in this reinforcement learning technology, you know, where these systems learn by extreme trial and error. And they were recently, sometime last year, training the system to play a really old kind of laughably clunky video game from the '80s or '90s. It's like a boat racing game, OK? And they trained it through this reinforcement learning method where essentially it plays the game over and over and over and tries to maximize its points. Well, this system learned in ways the designers didn't understand. Rather than trying to finish the race, which it should have done, it realized it could gain more points if it just ran into things and then spun back around and ran into other things and sort of tried to grab these sort of baubles that were around the course that awarded more points. And so it got into this crazy loop where it was just wreaking havoc across this game solely in an effort to gain points and wasn't trying to actually win the game. That again, you know, is a metaphor for the type of thing these researchers are worried about. And what they ended up doing is they ended up building an algorithm that allowed for human input. When the machine started doing things like that that there were unexpected, the human designer could provide suggestions - give it a little nudge here or there, show that it needed to complete the race and not just wreak havoc. GROSS: Wow, that's so interesting. METZ: Isn't it? GROSS: Yeah. So are we at the point where robots are designing other robots - where artificial intelligence is creating new forms of artificial intelligence? METZ: In certain small ways - what's really interesting to me is that building these neural networks is very different than building other forms - traditional forms of computer software, OK? The people who build these things have a particular talent. Basically, you know, you're trying to coax a result out of this like vast sea of data. People often talk about it as a dark art. And they're like individuals - and they are now paid in the millions, by the way, by these companies to perform this dark art, as some call it. It is a real talent. And so what has happened is that because relatively few people know how to do this today - just because it wasn't done much in the past - that a lot of companies don't have this talent that can help them build these systems. And they want it. But what's interesting is that people are now building machine-learning algorithms that can help build these machine-learning algorithms if you can wrap your brain around that. And so essentially - let's say at Google, for instance - they now have a system that can build an image recognition system that beats the performance of a system built by their human designers. So in that respect, you do have AI building AI. They call it meta learning. Again, this work is early, and it's unclear how far this will progress. But that's a real viable area of research. GROSS: Why did you start writing about technology? METZ: Well, what I tell everybody is that I have kind of a dual background, right? My mother is a voracious reader - a fiction reader mostly. My father was an engineer. He was a career IBMer (ph), a programmer. And so when I went to college, I was an English major, but I also interned at IBM as a programmer. So I always had this kind of dual interest, and it really comes from my parents. GROSS: What was the most amazing piece of technology that you got first because of your father? METZ: Well, we had an IBM PC even before it was commercially available, as the way I remember it - about 1981 with the monochrome green display. I remember building a program that asked trivia questions. You know, it asked you who Pete Rose was, and you had to type in the answer. GROSS: (Laughter) So that helped lead you to the role that you play now. METZ: That's a big part of it. And it - a lot of it was the stories my father would tell. He worked on the original project at IBM that designed the UPC symbol, the bar codes that are on all our groceries. GROSS: No kidding. METZ: He worked to test that. And he was a great storyteller, had great stories about the development of that and all the myths that sort of surrounded. There were people who protested the UPC symbol because they said it was the sign of the beast. You know, they had proof in Revelations from the Bible that this was going to lead the world to ruin. He loved to tell those stories. And there was a certain, you know, pride that I had when you would pick up a can of peas and see that symbol on the can. GROSS: Wow. Cade Metz, it's been great to talk with you. Thank you so much. METZ: Thank you. GROSS: Cade Metz is a technology correspondent for The New York Times. After we take a short break, Milo Miles will review two new albums by the Kronos Quartet, one a collaboration with Laurie Anderson, the other with a group from Mali. This is FRESH AIR. (SOUNDBITE OF MUSIC) TERRY GROSS, HOST:  This is FRESH AIR. I'm Terry Gross. There's been a huge boon in the evolution of artificial intelligence over just the past five years due to more high-powered computers, new kinds of programs and an increasing pool of data to learn from. This has led to new developments such as driverless cars, machines that can carry on a conversation and robots that can interpret medical scans. Today, we're going to talk about this rapidly changing world of artificial intelligence and how these new high-tech developments may change our lives for better or for worse. I've also just described the beat that my guest, tech reporter Cade Metz, covers for The New York Times. He was formerly a staff writer at Wired magazine. He's currently at work on a book about the visionaries behind new developments in artificial intelligence. Cade Metz, welcome to FRESH AIR. CADE METZ: Thanks for having me. GROSS: So you've been writing a lot about artificial intelligence, and you're writing a book about it, too, now. So let's talk about that. What is artificial intelligence? What do we mean now when we say it? METZ: That's a really good question. And it's, on some levels, a hard question to answer just because people tend to throw around this term, and they have for a long time, to describe almost anything. And use of this term has accelerated in recent months, in recent years. Anything that is automated in some way or even any computing technology that's advancing the state of the art, even in a small or minor way, is described as artificial intelligence. So it creates all this confusion about what that means, right? That's a grandiose term. It has a long history. It dates back to the '50s when this group of academics got together to truly build artificial intelligence, a machine that would mimic the intelligence of a human. But amidst all that hype, a lot of it which can be sort of just pushed aside, there's a very real change going on now, and it started about five years ago. And essentially, it's a fundamental change in the way that we build technology. What has changed now is that a certain type of computer algorithm - it's called a neural network - has started to work in ways that it did not work in the past. And what this algorithm allows us to do, allows those engineers to do is build systems that can learn tasks on their own. So for instance, a neural network, if it analyzes millions of photos, it can learn to recognize faces and objects in those photos. Most of us have used Facebook. If you've ever used Facebook and you've posted a photo of friends and family, it can recognize those faces. Well, that's driven by one of these neural networks, which can learn that task on its own. We're also seeing these systems in speech recognition. So you can now bark commands into your cellphone. It can recognize what you say. It's used in machine translation where machines translate from one language to another. GROSS: So let's back up. These are systems that can - what did you say? They can learn on their own. METZ: Exactly. They're called neural networks. And that's a metaphor. They're meant to mimic the web of neurons in the brain. But really, they're just math. They're mathematical algorithms that analyze data. A group of millions of photos is just data, and these systems look for patterns in that data - the way a nose curves on a person's face, the way the lines of an eye come together. They can identify those patterns, the math of those visual patterns, for instance, and then learn to recognize what is in an image. In the same way, it can recognize patterns in voice data and learn to recognize what people say. GROSS: So another example you've mentioned in your reporting is how there are now some computers that can read scans to see, for instance, if there's a nodule in a lung that might be a sign of cancer. METZ: Exactly. It's the same basic technology. The same technology that allows Facebook to recognize faces on its social network can allow machines to analyze medical scans and recognize when there are signs of illness and disease. It can be applied to lung scans helping to identify cancer, retinal scans helping to identify signs of diabetic blindness. As time goes on, we're going to see machines improve in this capacity. And what they can kind of do is provide a first line of defense against these types of diseases. That's particularly useful in, say, the developing world, where you don't have as many doctors. You can use these types of systems to analyze the scans of patients that would otherwise require a human doctor. GROSS: And so smart speakers, where you could tell the speaker what to play or what to turn on, that's also artificial intelligence, the same kind of learning pattern that you're describing. METZ: It's exactly the same thing. So the - you know, the Amazon Echo, which sits on the coffee tables of so many people now, or Siri on the iPhone, they train these systems using real human speech, and they learn to recognize what you say. The next step is really understanding what you say. I think what you'll notice, and this shows the limitations of AI today, the next step is to really, truly understand what you say and act on it in a way that a human would. We're not there yet. These systems can recognize what you say and, on some level, understand. They can answer basic questions, respond to basic commands. But they can't really have a back-and-forth conversation the way you might expect them to, or you might want them to, or that might make it easier to really interact with machines. GROSS: On a related note, like, when you talk to, say, Siri or to a smart speaker, how is the voice created that's responding to you? Is that actually a person's voice that's been programmed in such a way so that it can be responding to what you're saying? Or is that, like, a combination, like, a mashup of a lot of different human voices that have become this one robotic voice? METZ: Well, it depends on which system you use. But for instance, Google's system is now, in much the same way, trained on real human speech. So it can - it has analyzed an enormous amount of human speech, and it can use that to create its own. This was just recently rolled into the new Google phones. And you can tell the difference. You know, when I was testing one of these phones late last year, I showed it to my 13-year-old daughter, who's a Apple iPhone user. She could hear the difference between her iPhone and this new Google phone. It sounded more like a human because it had indeed been trained on human speech. It's the same basic technology - those neural networks that I was talking about. It recognizes those patterns, including the other - the way your voice may rise at the end of the sentence, or you may exhale in certain places. It duplicates those kind of things and gets a little bit closer to human speech. But I will say, and my daughter will say the same thing. It's not identical to human speech. It's still very limited. And that's the case with all these systems. As much as things have progressed in all these areas over the past five years, they still have a very long way to go to truly mimic the way you and I interact. GROSS: What are some of the limitations that researchers are trying to overcome now? METZ: Well, there are myriad limitations. For instance, researchers have realized that you can fool these systems into, say, seeing things that aren't there or to thinking they see things when they aren't really. That's a real issue if you talk to people who are kind of on the front lines of this technology. And it becomes a greater worry when you realize that these same technologies are helping in the development of the driverless cars that are being tested in Silicon Valley and Arizona and other places. In many respects, this is how these cars see and respond to what is around them. If you have flaws like that in a self-driving car or, let's say, a surveillance camera, that becomes a big issue. GROSS: So you've described that these neural networks, the algorithms that are used for artificial intelligence that can learn to do things and kind of mimic human thought and behavior, they're meant to mimic the neurons in the brain. So what does that mean exactly? Like, how do you get algorithms to mimic the way the brain works? METZ: This is a key question. So let me make it clear that is just a metaphor in many respects, right? This is an old idea. It dates back to the '50s and the late '50s when they first built these types of algorithms. And they were designed to mimic the brain. The reality is they can't mimic the brain because we don't even know how the brain works. GROSS: Yeah. I was thinking that, too (laughter). Yeah. METZ: So let's get to the point where we understand the brain completely. Then we can think about truly rebuilding the brain in digital form. That's an impossibility at this point. But essentially, you know, these algorithms are complex networks of mathematical operations. And each mathematical operation is referred to among those that build it as a neuron. OK. You're passing information between these mathematical operations. And it's - they're sort of constructed like a pyramid. So you have, you know, neurons at the bottom, mathematical operations at the bottom of this pyramid. And it will start to recognize if you're, say, trying to identify a face in a photo. It'll recognize a line here or a line there. And then that information will bubble up to other mathematical operations that will then start to put those lines together with, say, an eye or a nose. As you work up that pyramid, you get to the point where all these operations are helping the system understand what a human face looks like. This is not exactly the way the brain works. It's a very loose approximation. That needs to be understood if you're going to understand how these systems work. The problem is if you start talking about neural networks, you start assuming that the brain has been mimicked. And you said earlier in describing this that these systems are mimicking human thought. They are they are actually not mimicking human thought. We're still a very long way away from that. What they're doing is they're - they are learning specific tasks, right? Identifying a photo is a task that these systems can learn and learn to do pretty well. In some cases, they can perform on par with a human. But we are complex biological systems that do a lot of things well beyond that. And machines are a very long way from mimicking everything that we can do. GROSS: So we should take a short break here, and then we'll talk some more. If you're just joining us, my guest is Cade Metz. We're talking about artificial intelligence and new breakthroughs in related technology. He's a tech reporter for The New York Times. We'll be right back. This is FRESH AIR. (SOUNDBITE OF TODD SICKAFOOSE'S \"TINY RESISTORS\") GROSS: This is FRESH AIR. And if you're just joining us, we're talking about artificial intelligence and other breakthroughs in high tech. My guest, Cade Metz, is a tech correspondent for The New York Times and is writing a book about artificial intelligence. So, you know, getting back to voice recognition and trying to make computers conversational, tell the story of the - I don't know what to call it, a computer or what - that was being taught speech and ended up picking up a lot of like racist terms and Holocaust-denying expressions. And the company ended up discontinuing this. METZ: That was an earlier Microsoft project called Tay. And it's kind of a famous example of these types of systems gone wrong. It was an earlier system. And it had a - it had a serious design flaw. In many cases, it was just designed to repeat what the person had said to it in some way. And so humans interacting with this service online quickly realized this was the case and they could kind of coax the system into saying these racist and xenophobic things. Microsoft immediately took it offline. And since then, the technology has seriously improved. These systems that learn on their own have come to the fore. And Microsoft sees a real path towards creating a conversational system. The problem is that Tay, this bot designed and deployed in the past by Microsoft, has created a real conundrum, not only for Microsoft but for anyone else trying to build these conversational technologies. The thinking is that if you can get these new systems out in front of people and they will interact with them on their own and generate more conversational data that these systems can use that data to train themselves in ever more proficient ways. But these companies are wary of putting them out there because they know they're going to make mistakes. They know that they're going to learn from those human biases and the data. They know they're going to offend people in the end. And, you know, these are big companies with brands and reputations to protect. And they see this as a real stumbling block to reaching the point where they can build truly conversational systems. GROSS: As we were saying before, you know, computers have to like - robots have to learn in order to function. They first have to learn. And you write about reinforcement learning, how some bots like learn tasks through trial and error. And you describe a project that was a Google project creating a bot that could beat the world's best player at the game of Go. So how did that bot learn through trial and error and reward? It sounds like a really behavioral kind of approach to behavioral psychology - reward and punishment type of approach without the punishment. METZ: Well, I mean, you could add the punishment, to tell you the truth. GROSS: Oh, really? METZ: This project was a project out of a lab in London called DeepMind, which is now owned by Google. They built a machine, like you said, to play the ancient game of Go, which is the Eastern version of chess. It dates back thousands of years. And it's a national game in China and Japan and Korea. Just a few years ago, even experts in the field of AI assumed it would be at least another decade before we could build a machine that could crack the game of Go just because it's so complex. People like to say, including the designers of this machine at DeepMind, they like to say that there are more possible moves on a Go board than atoms in the universe. You can never build a system that could explore all the possibilities on the board and reach a conclusion. You just couldn't build a computer that was that powerful. But with these neural networks, what they were able to do is build a system that could learn to play Go on its own. And the way they did that was to supply it with millions of moves from professional human players. Once it learned the game, they essentially created two versions of the system. OK. It's a decent player at this point. And they would pit the system against itself. And it would play millions of games against itself. This is the reinforcement-learning aspect. It learned which moves in these games of self-play - which moves were successful and which were not. And in that way, it reached a level that was well above any human player. They took it to Korea a couple of years ago, and they beat the man who was essentially the Roger Federer of the Go world over the past decade. And then they took it to China, and they beat the No. 1 player in the world handily. It's a system that really shows what is possible with these algorithms. GROSS: In which part - you've programmed the bot to try to get more points and not lose points, and that's its goal. And so it learns that some decisions lead to gaining points and some decisions lead to losing points. And it tries to orient itself toward its goal of gaining points. Is that. . . METZ: Exactly. And if you do that on an enormous scale, over millions of games, you learn which moves are going to gain you points versus which ones are going to lose points, which ones are going to get you closer to winning the game - having more points than your opponent - and which you're going to lose. The thinking is that you can then apply this type of reinforcement learning to the real world. So, for instance, people have already started research where they try to train cars in this way. I mean, they are literally training cars in games. So, you know, you have racing games, you know, video games that teenagers play. If you can train this virtual car to play that video game with the same method - right? - certain moves mean more points or less, then you can eventually train a car to drive real roads. But that's certainly easier said than done. The real world is more complicated even than a video game, of course. And kind of transferring that knowledge that a system has learned in a game to the real world is a big step. GROSS: My guest is Cade Metz, a technology correspondent for The New York Times. After a break, we'll talk about why some artificial intelligence designers are worried about the possibility of creating intelligent machines that will refuse to allow humans to turn them off. And Milo Miles will review two Kronos Quartet albums, one in collaboration with Laurie Anderson, the other in collaboration with a group from Mali. I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF EMAV'S \"TECHMO\") GROSS: This is FRESH AIR. I'm Terry Gross, back with Cade Metz, a tech correspondent for The New York Times. He writes about new developments in artificial intelligence and robotics like driverless cars, machines that can carry on a conversation, robots that can interpret medical scans and how these and other new high-tech developments may change our lives for better or worse. He's at work on a book about the visionaries behind new developments in artificial intelligence. Now that we've learned so much about hacking through Russian interference in our election with like fake news and fake accounts on social media, the thought of there being robots that can be hacked by a malevolent actor is really scary because if robots are designed to do important things, whether it's like, you know, reading a lung scan to see if a person has cancer or whether it's a driverless car, if somebody can hack into that for devious reasons, that's kind of terrifying. So are designers worried about being able to adequately design safeguards? METZ: They are certainly worried. There was a report recently, pulled together by a group of technologists and researchers across various companies, research labs and think tanks across the U. S. and the U. K. , that looked at these very issues. And, again, they are myriad. You know, you talk about the possibility of hacking in to an autonomous system. Or, you know, certainly you can exploit its mistakes. Like I said, there are situations where these neural networks can be fooled into seeing things that aren't there - or failing to see things that are. That can be exploited by bad actors. But there are other issues that may be even closer. One of the things that these systems are doing extremely well and increasingly well is essentially fabricating images that look real. There was a team out of a company called Nvidia - they're a chipmaker, but they have an AI lab in Finland. And this team of researchers put together a system where essentially they built a neural network that analyzed millions of celebrity photos. And this system learned to build its own celebrity - essentially taking all those patterns that you see in pictures of Gwyneth Paltrow, or whoever else you might imagine is in this database. And then it would build this celebrity - this fake celebrity that looked vaguely like someone you would see on the red carpet, but you couldn't quite identify. Think of the implications of that in the age of fake news, right? These techniques will soon be applied not only to still images like that but to video and virtual reality. But even in the near term, as these systems become better and better at building fake images that look real, we're really going to have to change the way we look at anything we see online. GROSS: Yeah, that's a really scary thought. I will say you had some of the images of these computer-generated celebrity faces online. And it's kind of hilarious because they look exactly like generic celebrities - like there's a certain generic celebrity look, you know? And I have to say that the computers - the artificial intelligence captured it perfectly. METZ: Well, that's what is doing. It's identifying patterns, right? There are patterns that you and I respond to. And we say, ah, that's a celebrity, right? And some cases, those are intuitive. We can't necessarily articulate them. But these systems can identify those patterns and then make use of them. And in a way, that's a scary thing as well, right? These systems are operating on such a large scale. They're analyzing so many photos. We can never be sure exactly why they're making certain decisions. And that's a big worry as well. Think about that self-driving car again. If the self-driving car makes a mistake, you want to know why it made a mistake. Well, these systems operate in ways that even the people who build them do not completely understand. GROSS: Why not? Why don't they understand them? METZ: Well, can you analyze millions of photos in a matter of minutes? You can't, right? You cannot identify the patterns that that system identified. You can't pinpoint the exact decisions it made over the course of that analysis because you cannot do it yourself. GROSS: So in other words, you have no idea why the size of the woman celebrity's lips are they are or why the length of stubble on the male celebrity's beard is - like the. . . METZ: Absolutely, or. . . GROSS: . . . Or, you know, what's considered like the perfect eyes for a celebrity. Like, the artificial intelligence has made a gazillion calculations, and you're just kind of generalizing when you say something. METZ: Absolutely, or think about it like this. Think back to what we discussed about the game of Go and that system that the DeepMind Lab in London built to play the game of Go. Some of the designers on that team were very good Go players. When that system was in the middle of one of those high-stakes games in Korea and China, they had no clue what it was doing. It was operating at a level that they could not understand. And these are some of the brightest people on earth literally. And over the course of those five-hour matches, where this machine is playing in ways that no human has ever played literally - and winning in that way, beating the best humans on Earth - the people who designed that machine are unclear what it's doing and why. It is playing at the level they could never play on their own. GROSS: And the machine isn't going to be rattled by anxiety. METZ: That's also true. And it doesn't need to sleep. GROSS: Right. OK, so let's take a short break here, and then we'll talk some more. If you're just joining us, my guest is Cade Metz. He's a tech correspondent for The New York Times. We're going to talk more about artificial intelligence after we take a short break. This is FRESH AIR. (SOUNDBITE OF OF MONTREAL SONG \"FABERGE FALLS FOR SHUGGIE\") GROSS: This is FRESH AIR. And if you're just joining us, my guest is Cade Metz. He's a tech correspondent for The New York Times where he writes about artificial intelligence, driverless cars, robotics, virtual reality and other emerging areas in high tech. OK, so another fear that you were right about - that some people have about artificial intelligence - and I think these are designers who are worried about it - is the fear that AI systems will learn to prevent humans from turning them off. When I read this, of course, the first thing I thought of is the computer HAL in \"2001: A Space Odyssey\" because HAL is refusing to be turned off. METZ: Right. GROSS: So how would you elaborate on what this fear is - what the danger is? METZ: You know, I like that you bring up HAL in \"2001\" - one of my favorite movies, certainly. And I have to say it's the favorite movie of many of these AI researchers who are designing these systems. If you go into the personal conference room of the top AI researcher at Facebook, he's got images from \"2001\" all over this conference room. My point is that pop cultural images, movies and books influence not only the way these designers think but the way you and I think and the people who run these companies think. We tend to think about those doomsday scenarios because we've thought about them a lot over the decades through pop culture, right? I think that's part of it - is that people on some level relate to that, and they've worried about that in some way. And so as these machines get better, you tend to worry about that as well. That said, among many of these researchers, there is a real concern that as time goes on this will be a big problem, for those reasons I talked about - that these machines learn in ways that we cannot completely understand. If you have a car, for instance, that learns by playing, you know, a video game, it happens on such a large scale. It plays the game so much that there are points where it's going to do things that we humans don't expect it to do. And as it reaches ever more for that higher point total, it's going to try to maximize its point total in every respect. The worry is that it's going to maximize it in ways that we don't want it to. That doomsday scenario is that if it is trying to get to the most points, well, it's not going to let us turn it off. Now, on some level, that again is not something that we can see happening any time soon. That said, these systems that learn on their own are getting better and better - meaning the algorithms are getting better and better. And the hardware that runs these algorithms, that allows them to train ever more - you know, with ever more speed, are getting better and better. And so the worry is that as we improve the algorithms, as we get faster computers, that we will somehow cross that threshold where the machines are really behaving in ways not only that we don't understand but that we can't really control. And among many thinkers in this area, they're worried - even at this early stage. And it is early. If that ever happens, it's years and years away. But they're worried at this early stage that we will cross that threshold and not know it. What they say is that we might as well start thinking about it now because it is potentially such a risk. And when they make that argument, it's hard to argue back. Why not start worrying now and try to build safeguards into these systems rather than wait? GROSS: Can you give us an example of the kind of scenario where a bot would not allow itself to be turned off because it was pursuing the reward that it was programmed to pursue, like getting points for making the right decision? METZ: Well, I mean, loosely speaking - right? - if you're trying to - I mean, again, this is largely theory. But if you're trying to maximize your points, you're not going to keep accumulating points if you're turned off, right? You could think about it in that respect. But think about - let's back up just a little bit. Think about it in this respect. There's a lab in San Francisco. It's called OpenAI. And it was founded by Elon Musk, the CEO of Tesla among others. And they specialize in this reinforcement learning technology, you know, where these systems learn by extreme trial and error. And they were recently, sometime last year, training the system to play a really old kind of laughably clunky video game from the '80s or '90s. It's like a boat racing game, OK? And they trained it through this reinforcement learning method where essentially it plays the game over and over and over and tries to maximize its points. Well, this system learned in ways the designers didn't understand. Rather than trying to finish the race, which it should have done, it realized it could gain more points if it just ran into things and then spun back around and ran into other things and sort of tried to grab these sort of baubles that were around the course that awarded more points. And so it got into this crazy loop where it was just wreaking havoc across this game solely in an effort to gain points and wasn't trying to actually win the game. That again, you know, is a metaphor for the type of thing these researchers are worried about. And what they ended up doing is they ended up building an algorithm that allowed for human input. When the machine started doing things like that that there were unexpected, the human designer could provide suggestions - give it a little nudge here or there, show that it needed to complete the race and not just wreak havoc. GROSS: Wow, that's so interesting. METZ: Isn't it? GROSS: Yeah. So are we at the point where robots are designing other robots - where artificial intelligence is creating new forms of artificial intelligence? METZ: In certain small ways - what's really interesting to me is that building these neural networks is very different than building other forms - traditional forms of computer software, OK? The people who build these things have a particular talent. Basically, you know, you're trying to coax a result out of this like vast sea of data. People often talk about it as a dark art. And they're like individuals - and they are now paid in the millions, by the way, by these companies to perform this dark art, as some call it. It is a real talent. And so what has happened is that because relatively few people know how to do this today - just because it wasn't done much in the past - that a lot of companies don't have this talent that can help them build these systems. And they want it. But what's interesting is that people are now building machine-learning algorithms that can help build these machine-learning algorithms if you can wrap your brain around that. And so essentially - let's say at Google, for instance - they now have a system that can build an image recognition system that beats the performance of a system built by their human designers. So in that respect, you do have AI building AI. They call it meta learning. Again, this work is early, and it's unclear how far this will progress. But that's a real viable area of research. GROSS: Why did you start writing about technology? METZ: Well, what I tell everybody is that I have kind of a dual background, right? My mother is a voracious reader - a fiction reader mostly. My father was an engineer. He was a career IBMer (ph), a programmer. And so when I went to college, I was an English major, but I also interned at IBM as a programmer. So I always had this kind of dual interest, and it really comes from my parents. GROSS: What was the most amazing piece of technology that you got first because of your father? METZ: Well, we had an IBM PC even before it was commercially available, as the way I remember it - about 1981 with the monochrome green display. I remember building a program that asked trivia questions. You know, it asked you who Pete Rose was, and you had to type in the answer. GROSS: (Laughter) So that helped lead you to the role that you play now. METZ: That's a big part of it. And it - a lot of it was the stories my father would tell. He worked on the original project at IBM that designed the UPC symbol, the bar codes that are on all our groceries. GROSS: No kidding. METZ: He worked to test that. And he was a great storyteller, had great stories about the development of that and all the myths that sort of surrounded. There were people who protested the UPC symbol because they said it was the sign of the beast. You know, they had proof in Revelations from the Bible that this was going to lead the world to ruin. He loved to tell those stories. And there was a certain, you know, pride that I had when you would pick up a can of peas and see that symbol on the can. GROSS: Wow. Cade Metz, it's been great to talk with you. Thank you so much. METZ: Thank you. GROSS: Cade Metz is a technology correspondent for The New York Times. After we take a short break, Milo Miles will review two new albums by the Kronos Quartet, one a collaboration with Laurie Anderson, the other with a group from Mali. This is FRESH AIR. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-16-593989347": {"title": "With Airport Face Scans More Widespread, Privacy And Accuracy Concerns Persist : NPR", "url": "https://www.npr.org/2018/03/16/593989347/facial-scanning-now-arriving-at-u-s-airports", "author": "No author found", "published_date": "2018-03-16", "content": "", "section": "Politics", "disclaimer": ""}, "2018-03-17-594537832": {"title": "Russian Trolling Continued After Election : NPR", "url": "https://www.npr.org/2018/03/17/594537832/russian-trolling-continued-after-election", "author": "No author found", "published_date": "2018-03-17", "content": "SCOTT SIMON, HOST:  The Trump administration has announced new sanctions on Russia, including on the Internet Research Agency. That's the troll farm in St. Petersburg, Russia, that engineered a massive online effort to post divisive political messages on American social media platforms during the 2016 presidential campaign. So far, the president has personally avoided condemning Russia for its actions despite the findings of U. S. intelligence agencies that U. S. citizens were targeted by Russian operatives. And the trolling continued after the election. Ajah Hales owns Golden Goddess Cosmetics in Cleveland. And early in 2017, she was contacted by a Facebook account that promised to promote black-owned businesses like hers. She joins us from WCPN in Cleveland. Thanks so much for being with us. AJAH HALES: Thanks for having me. SIMON: The account called itself Black4Black. What happened? How did they contact you? What did they say? HALES: They reached out to me on Facebook Messenger. And they said that they were interested in promoting black-owned businesses, that they were starting a directory. And would I like to be involved? SIMON: And you said? HALES: I said yes. I'm always looking for new ways to promote my business. So they asked me to complete a questionnaire because they wanted to do an article on my company. They did ask me for additional information, though - if I knew any other businesses that would be interested in free promotion. I'm involved with a local business group called Recycling Black Dollars. And so I forwarded them a spreadsheet with the businesses that had agreed to share their contact information. SIMON: Where did you think these people were? I mean, you had no inkling they were on the other side of the world? HALES: No, not at all. They said that they were with BlackMattersUS and that we would be able to tap into their social media network of over 300,000 followers. They sounded completely legitimate. SIMON: Well, how did you find out that this operation wasn't what it purported to be? HALES: Shelby Holliday from The Wall Street Journal reached out to me. I struggle with anxiety. And so my anxiety just skyrocketed as soon as I found that out because I'm obsessive about my brand. I don't want anyone to be using my brand or using any information I provide to do shady political dealings. SIMON: Why do you think they were interested in targeting you? HALES: Well, I think they targeted me because I'm a black-owned business. Black-owned businesses, especially black female-owned businesses, are some of the most vulnerable in the United States. So because we're vulnerable and because we're always going to be looking for ways to get the word out about our companies, we make easy marks. We're low-hanging fruit. SIMON: Well, let me put it this way. Do you think they did you any good? Did they do you no harm? HALES: I think that it's too soon to say. It's too soon to know what they intended to do with the information or how they might've used it. SIMON: Ajah Hales owns Golden Goddess Cosmetics in Cleveland. Thanks so much for being with us. HALES: Thanks for having me. (SOUNDBITE OF KODOMO'S \"CONCEPT 1\") SCOTT SIMON, HOST:   The Trump administration has announced new sanctions on Russia, including on the Internet Research Agency. That's the troll farm in St. Petersburg, Russia, that engineered a massive online effort to post divisive political messages on American social media platforms during the 2016 presidential campaign. So far, the president has personally avoided condemning Russia for its actions despite the findings of U. S. intelligence agencies that U. S. citizens were targeted by Russian operatives. And the trolling continued after the election. Ajah Hales owns Golden Goddess Cosmetics in Cleveland. And early in 2017, she was contacted by a Facebook account that promised to promote black-owned businesses like hers. She joins us from WCPN in Cleveland. Thanks so much for being with us. AJAH HALES: Thanks for having me. SIMON: The account called itself Black4Black. What happened? How did they contact you? What did they say? HALES: They reached out to me on Facebook Messenger. And they said that they were interested in promoting black-owned businesses, that they were starting a directory. And would I like to be involved? SIMON: And you said? HALES: I said yes. I'm always looking for new ways to promote my business. So they asked me to complete a questionnaire because they wanted to do an article on my company. They did ask me for additional information, though - if I knew any other businesses that would be interested in free promotion. I'm involved with a local business group called Recycling Black Dollars. And so I forwarded them a spreadsheet with the businesses that had agreed to share their contact information. SIMON: Where did you think these people were? I mean, you had no inkling they were on the other side of the world? HALES: No, not at all. They said that they were with BlackMattersUS and that we would be able to tap into their social media network of over 300,000 followers. They sounded completely legitimate. SIMON: Well, how did you find out that this operation wasn't what it purported to be? HALES: Shelby Holliday from The Wall Street Journal reached out to me. I struggle with anxiety. And so my anxiety just skyrocketed as soon as I found that out because I'm obsessive about my brand. I don't want anyone to be using my brand or using any information I provide to do shady political dealings. SIMON: Why do you think they were interested in targeting you? HALES: Well, I think they targeted me because I'm a black-owned business. Black-owned businesses, especially black female-owned businesses, are some of the most vulnerable in the United States. So because we're vulnerable and because we're always going to be looking for ways to get the word out about our companies, we make easy marks. We're low-hanging fruit. SIMON: Well, let me put it this way. Do you think they did you any good? Did they do you no harm? HALES: I think that it's too soon to say. It's too soon to know what they intended to do with the information or how they might've used it. SIMON: Ajah Hales owns Golden Goddess Cosmetics in Cleveland. Thanks so much for being with us. HALES: Thanks for having me. (SOUNDBITE OF KODOMO'S \"CONCEPT 1\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-18-594786383": {"title": "Facebook: Firm Working With Trump Campaign Stole 50 Million Users' Data : NPR", "url": "https://www.npr.org/2018/03/18/594786383/facebook-firm-working-with-trump-campaign-stole-50-million-users-data", "author": "No author found", "published_date": "2018-03-18", "content": "MICHEL MARTIN, HOST: Now to another explosive story that has potential implications for both politics and the law. Members of Congress are calling for an investigation into the data firm Cambridge Analytica after new reports from the New York Times and The Observer of London. According to these reports, the personal information belonging to some 50 million people was stolen and used to target them in ways they would not have seen and could not have known about. This according to a former insider named Christopher Wylie. He says this information was used to influence voters in the 2016 U. S. election. Cambridge Analytica has ties to the Trump campaign. In 2016, the firm was hired to run the campaign's data operations, and Steve Bannon is a former vice president of the firm. Facebook says the data was obtained legally but misused afterward, but they've suspended Cambridge Analytica from their platform. We wanted to understand how all this could have happened and what it might mean, so we've called Antonio Garcia Martinez. He is a Silicon Valley veteran and early Facebook employee who headed the company's targeting efforts, and he is now a contributor at Wired. Antonio Garcia Martinez, thank you so much for speaking to us. ANTIONIO GARCIA MARTINEZ: Thank you for having me. MARTIN: Well, as a person who has worked closely with big data, could you just tell us your reaction to these reports? MARTINEZ: Yeah. It's not totally surprising. I mean, the thing to keep clear here is that the data that's, kind of, leaked out isn't really, kind of, from the ad system but rather what's called platform-internally. And what that means is there are apps on Facebook, and, you know, you opt into these as a user, and that's, kind of, how this data breach happened. And it's really this weird thing where there is data coming out of one side of Facebook being used on the other, which is on - which is on the outside. But, you know, frankly, I wasn't hugely surprised. I mean, this is one of those things that it's been part of the platform for years now, and there's really not much Facebook can actually do about it is the hard truth about it. MARTIN: You know, I was going to ask you about this because this is being described as a data breach, which says that the information was stolen, and that is the word that the Observer report uses. It says that the - this information was stolen or that Facebook basically left the door open and allowed this to happen and didn't put enough controls into place to keep this from happening. Now, Facebook says that they are investigating but that people did consent to give out their information, so this was not a data breach. How do you respond to that? MARTINEZ: Yeah, no. I mean, how that term is typically used, this is not a data breach. That really gives the impression that there was some sort of security flaw and that Facebook somehow got hacked and literally everything from, you know, your messages to your partner to your, you know, childhood photos were potentially stolen, but that's not the case at all. You know, most of your listeners, probably, at some point, have opted into a Facebook app, and you do that even when, for example, you're using Facebook to log into a website or even on a mobile app. When you consent to a certain app accessing your Facebook data, that's kind of what you're opting into. So this wasn't that somebody broke into Facebook and stole data. This is users who opted into an app, and then the data that that app got via this opt-in was misused in ways that are not in accordance with Facebook's data policy. So I mean, to the extent that we can sort of distinguish between what a breach implies, which is kind of a break in Facebook security law, and, you know, data exiting in a way that user consented to that was then, kind of, misused, it's definitely the latter situation right here. MARTIN: So let me take this from another direction. One way to look at this is that the information may have allowed the Trump campaign to microtarget specific groups of voters, so how is this different from what all campaigns do these days? Or businesses for that matter? I mean, one of the key sources for this reporting, Christopher Wylie, was studying fashion trend forecasting, and he just applied the same kind of logic to this other enterprise. So is this different from what users could reasonably assume was being done with their data? MARTINEZ: You know, that's a great question. I mean, the short answer is no. I mean, this is kind of a - a somewhat obscure field of advertising targeting called psychographics, and that's, kind of, the buzz word for it. And what it basically means is it tries to capture a person's psychological state, you know, down to things like openness or neuroticism, and these are words that, you know, were quoted from some of the leaked emails that the Guardian leaked out, right? So they try to profile a person according to a certain psychological dimension, and then understand how likely that person was to vote for or against some issue or candidate. Right? And what they were trying to get at was using Facebook to understand how, you know, neurotic, say, you were. And if that neuroticism, in their minds, would incline you to vote for or against somebody, they would try to target you as a result of that. This isn't new. I mean, this sort of psychographic type stuff has been around even before the internet. It's been around for decades. It's also been on Facebook before. In the past, there have been other outside companies that have tried to psychologically profile users. What's strange is that it was - you know, that the data used to actually build these psychological models was kind of unethically arrived at. That, to me, is a critical issue. You know, the - how powerful this data was - I mean, to be honest, I think most ads professionals don't buy much into this whole psychographic thing. MARTIN: So could you just drill down on something you just said, which was you said there was something unethical or potentially unethical about the way this - this information was arrived at or it was taken by the firm that eventually used it. So what was unethical about that? MARTINEZ: Well, if the allegations are to be believed what happened was that Cambridge Analytica or, you know, their confederates and consultants paid test subjects to take a political poll, and in the course of taking that political poll, they had to opt in to a Facebook app as we often do when we log into other sites using Facebook, the difference being is that in this case, they took the results of those political polls - right? - their inclinations, their psychological profiles - and they joined that with all their Facebook data that they managed to suck out of Facebook via this Facebook app that the users had opted in to, not just those Facebook users but also their friends. On Facebook's platform - not anymore but until not too long ago - if you opted into an app, that gave access, potentially, to the app to your friends data as well. And that's why the number of users whose information has been compromised is as high as it is - 50 million people. Cambridge Analytica didn't pay 50 million people take a poll. It paid of order tens of thousands, but each of those individuals had hundreds or thousands of friends, which is why the number is so large. And that's what's unethical about it. They basically paid people to take a poll, but then they basically hoovered out all of their Facebook data and joined it with that political data. That's the illegal bit. MARTIN: That's Antonio Garcia Martinez. He's the former head of Facebook's effort to develop ad-targeting strategies. He's now a contributor to Wired. Antonio Garcia Martinez, thank you so much for speaking with us. MARTINEZ: It was great time. Thanks. MARTIN: We should also mention that we reached out to Facebook for comment. They said they are, quote, \"conducting an internal and external review,\" unquote, to make sure the data no longer exists. MICHEL MARTIN, HOST:  Now to another explosive story that has potential implications for both politics and the law. Members of Congress are calling for an investigation into the data firm Cambridge Analytica after new reports from the New York Times and The Observer of London. According to these reports, the personal information belonging to some 50 million people was stolen and used to target them in ways they would not have seen and could not have known about. This according to a former insider named Christopher Wylie. He says this information was used to influence voters in the 2016 U. S. election. Cambridge Analytica has ties to the Trump campaign. In 2016, the firm was hired to run the campaign's data operations, and Steve Bannon is a former vice president of the firm. Facebook says the data was obtained legally but misused afterward, but they've suspended Cambridge Analytica from their platform. We wanted to understand how all this could have happened and what it might mean, so we've called Antonio Garcia Martinez. He is a Silicon Valley veteran and early Facebook employee who headed the company's targeting efforts, and he is now a contributor at Wired. Antonio Garcia Martinez, thank you so much for speaking to us. ANTIONIO GARCIA MARTINEZ: Thank you for having me. MARTIN: Well, as a person who has worked closely with big data, could you just tell us your reaction to these reports? MARTINEZ: Yeah. It's not totally surprising. I mean, the thing to keep clear here is that the data that's, kind of, leaked out isn't really, kind of, from the ad system but rather what's called platform-internally. And what that means is there are apps on Facebook, and, you know, you opt into these as a user, and that's, kind of, how this data breach happened. And it's really this weird thing where there is data coming out of one side of Facebook being used on the other, which is on - which is on the outside. But, you know, frankly, I wasn't hugely surprised. I mean, this is one of those things that it's been part of the platform for years now, and there's really not much Facebook can actually do about it is the hard truth about it. MARTIN: You know, I was going to ask you about this because this is being described as a data breach, which says that the information was stolen, and that is the word that the Observer report uses. It says that the - this information was stolen or that Facebook basically left the door open and allowed this to happen and didn't put enough controls into place to keep this from happening. Now, Facebook says that they are investigating but that people did consent to give out their information, so this was not a data breach. How do you respond to that? MARTINEZ: Yeah, no. I mean, how that term is typically used, this is not a data breach. That really gives the impression that there was some sort of security flaw and that Facebook somehow got hacked and literally everything from, you know, your messages to your partner to your, you know, childhood photos were potentially stolen, but that's not the case at all. You know, most of your listeners, probably, at some point, have opted into a Facebook app, and you do that even when, for example, you're using Facebook to log into a website or even on a mobile app. When you consent to a certain app accessing your Facebook data, that's kind of what you're opting into. So this wasn't that somebody broke into Facebook and stole data. This is users who opted into an app, and then the data that that app got via this opt-in was misused in ways that are not in accordance with Facebook's data policy. So I mean, to the extent that we can sort of distinguish between what a breach implies, which is kind of a break in Facebook security law, and, you know, data exiting in a way that user consented to that was then, kind of, misused, it's definitely the latter situation right here. MARTIN: So let me take this from another direction. One way to look at this is that the information may have allowed the Trump campaign to microtarget specific groups of voters, so how is this different from what all campaigns do these days? Or businesses for that matter? I mean, one of the key sources for this reporting, Christopher Wylie, was studying fashion trend forecasting, and he just applied the same kind of logic to this other enterprise. So is this different from what users could reasonably assume was being done with their data? MARTINEZ: You know, that's a great question. I mean, the short answer is no. I mean, this is kind of a - a somewhat obscure field of advertising targeting called psychographics, and that's, kind of, the buzz word for it. And what it basically means is it tries to capture a person's psychological state, you know, down to things like openness or neuroticism, and these are words that, you know, were quoted from some of the leaked emails that the Guardian leaked out, right? So they try to profile a person according to a certain psychological dimension, and then understand how likely that person was to vote for or against some issue or candidate. Right? And what they were trying to get at was using Facebook to understand how, you know, neurotic, say, you were. And if that neuroticism, in their minds, would incline you to vote for or against somebody, they would try to target you as a result of that. This isn't new. I mean, this sort of psychographic type stuff has been around even before the internet. It's been around for decades. It's also been on Facebook before. In the past, there have been other outside companies that have tried to psychologically profile users. What's strange is that it was - you know, that the data used to actually build these psychological models was kind of unethically arrived at. That, to me, is a critical issue. You know, the - how powerful this data was - I mean, to be honest, I think most ads professionals don't buy much into this whole psychographic thing. MARTIN: So could you just drill down on something you just said, which was you said there was something unethical or potentially unethical about the way this - this information was arrived at or it was taken by the firm that eventually used it. So what was unethical about that? MARTINEZ: Well, if the allegations are to be believed what happened was that Cambridge Analytica or, you know, their confederates and consultants paid test subjects to take a political poll, and in the course of taking that political poll, they had to opt in to a Facebook app as we often do when we log into other sites using Facebook, the difference being is that in this case, they took the results of those political polls - right? - their inclinations, their psychological profiles - and they joined that with all their Facebook data that they managed to suck out of Facebook via this Facebook app that the users had opted in to, not just those Facebook users but also their friends. On Facebook's platform - not anymore but until not too long ago - if you opted into an app, that gave access, potentially, to the app to your friends data as well. And that's why the number of users whose information has been compromised is as high as it is - 50 million people. Cambridge Analytica didn't pay 50 million people take a poll. It paid of order tens of thousands, but each of those individuals had hundreds or thousands of friends, which is why the number is so large. And that's what's unethical about it. They basically paid people to take a poll, but then they basically hoovered out all of their Facebook data and joined it with that political data. That's the illegal bit. MARTIN: That's Antonio Garcia Martinez. He's the former head of Facebook's effort to develop ad-targeting strategies. He's now a contributor to Wired. Antonio Garcia Martinez, thank you so much for speaking with us. MARTINEZ: It was great time. Thanks. MARTIN: We should also mention that we reached out to Facebook for comment. They said they are, quote, \"conducting an internal and external review,\" unquote, to make sure the data no longer exists.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-18-594671296": {"title": "Report: Cambridge Analytica 'Harvested Private Information' : NPR", "url": "https://www.npr.org/2018/03/18/594671296/report-cambridge-analytica-harvested-private-information", "author": "No author found", "published_date": "2018-03-18", "content": "LULU GARCIA-NAVARRO, HOST: We're getting new information about a different kind of interference in the 2016 election. This time, it involves a data firm called Cambridge Analytica. A joint investigation from The New York Times and The London Observer found that the firm, quote, \"harvested private information from the Facebook profiles of more than 50 million users without their permission. \" It later used that data to help target American voters for its work with President Trump's campaign. Facebook has now suspended the company from its platform. For more, Matthew Rosenberg joins me in the studio. He covers intelligence and national security for The New York Times, and he co-authored this story. Welcome to the program. MATTHEW ROSENBERG: Morning. GARCIA-NAVARRO: So tell us what Cambridge Analytica is. ROSENBERG: It's a data firm, basically - founded by Robert Mercer and his daughter Rebekah, these billionaires - kind of Republican donors - and Steve Bannon. And the idea, the conceit was that they're going to use big data to map the personality of every voter. And by mapping the personality, they could then predict their behavior, sway them, influence them and change American political culture - shift it to the right, shift it to the views they wanted to establish. GARCIA-NAVARRO: Influence them how? ROSENBERG: You know, through a variety of advertising. So, like, let's say you find out this particular voter is neurotic. OK. Let's scare them with talk of crime and other issues. Let's say you find somebody who's open-minded and kind of optimistic. Well, how can we advertise at them? How can we appeal to that kind of point of view? It's a very - you know, they're using something called psychographics, which is a very new field and academic discipline that has a number of proponents and a lot of critics. And it's really uncertain how well this worked in 2014 when the firm first started going and then in 2016. But, you know, its proponents, like some of the guys at Cambridge Analytica, think this is the future, that we can really shape American politics and reshape them this way. GARCIA-NAVARRO: At the core of your investigation, though, there was this huge data breach - 50 million users, which is a number that we hadn't really heard before. Explain why you call it a data breach. ROSENBERG: Look. There was a bunch of information that was on Facebook and ended up with a private company where it wasn't supposed to, where most of those users had no idea it was going to end up. To us, that's a breach. GARCIA-NAVARRO: And what is exactly the information that they got? ROSENBERG: So this would be, you know, everything that was on, say, your Facebook profile, your basic bio data, everything you liked, who your friends were, where you lived. A lot of that information is private by default now, or only your friends can see it. And the way this app worked was that somebody would download the app. It would scrape all their information, and then it would scrape all their friends' information, which, in 2014, Facebook allowed. The thing is that the app said it was doing it for academic research, which it wasn't. It was doing it for Cambridge Analytica. The person - the professor who designed the app did this specifically for Cambridge Analytica. And throughout the process, Cambridge Analytica was paying the bills for the data collection. So it's not like somebody stumbled upon it and just sold it to them. GARCIA-NAVARRO: So basically, they misrepresented what they were really using it for. ROSENBERG: Exactly. GARCIA-NAVARRO: You and other news organizations got a lot of this new information from a leaker, Chris Wylie. He helped found Cambridge Analytica. And the U. K. 's Channel 4 actually spoke with him. And here is what he says. He says Cambridge Analytica basically weaponized your Facebook page. (SOUNDBITE OF ARCHIVED RECORDING)CHRISTOPHER WYLIE: So whenever you go, and you like something, you are giving me a clue as to who you are as a person. And so all of this can be captured very easily and run through an algorithm that learns who you are. When you go to work - right? - your co-workers only see one side of you. Your friends only see one side of you. But a computer sees all kinds of sides of you. And so we can get better than human level accuracy at predicting your behavior. GARCIA-NAVARRO: So tell me about Chris Wylie. ROSENBERG: Chris is a fascinating kid. He really believes that this is the future. He is a 28-year-old vegan from Canada who dropped out of high school, ended up getting into politics and data politics in Canada with Ken Strasma, who is Obama's big data guy. And he's got pictures of himself at Obama's 2012 inauguration. He ends up in London just into these circles with the people who eventually become Cambridge Analytica. And look. He is not a right-wing ideologue. He told me at one point that he quit the company because he wanted to be in fashion, not fascism, as he put it. And it's one of the interesting things about Cambridge Analytica. Their original data team was a lot of young, gay, liberal men. GARCIA-NAVARRO: But they did obviously get into bed with the Mercers and Steve Bannon. Let's hear more of Wylie speaking to Channel 4 about how Steve Bannon, who was Trump's chief strategist, wanted to use this technology. (SOUNDBITE OF ARCHIVED RECORDING)WYLIE: Steve wanted weapons for his culture war. That's what he wanted. We offered him a way to accomplish what he wanted to do, which was change the culture of America. GARCIA-NAVARRO: So did this actually change people's mind? Could it really have this bold effect across America? ROSENBERG: It's a big question. And I don't think we have the evidence to back that up at this point. You know, is this going to be able to work out in the future? Is the data going to be robust enough to be able to figure out the questions, figure out what personalities are and then advertise or market and micro-target at them? Perhaps. I mean, it certainly looks like we're moving in that direction. But I don't think in 2016 we really have the evidence to say that they had this dramatic change. GARCIA-NAVARRO: So how does what you uncovered link to Robert Mueller's investigation into election interference? ROSENBERG: So we know that that Mueller's team has asked Cambridge Analytica for all its documents and emails related to the Trump campaign. Exactly what they're looking for, we just don't know. You know, the Mueller investigation's a bit of a black box. GARCIA-NAVARRO: But your reporting showed that there is something interesting linking Cambridge Analytica to Russia. ROSENBERG: So there are all kinds of kind of unexplained connections. At one point, in the summer of 2014, Lukoil, which is a Russian oil company that's tight with Putin's inner circle, showed up and started to talk to them about American voter data, which was - they thought was very odd. What does Lukoil, which has, like, two gas stations in the United States, want with American voter data or consumer data? It was - it didn't make a lot of sense. You know, Alexander Nix, who's chief executive, was recently in front of Parliament. He said they've never done any business in Russia full stop. But we found brochures for their parent company that says they do have business in Russia. And so it's just questions that aren't really answered there. GARCIA-NAVARRO: And Facebook's reaction to your reporting has been that they have banned Cambridge Analytica from their platform. And now there are some Congress people who are asking for an investigation. ROSENBERG: I know Senator Klobuchar and Senator Warner have called for further investigation and to bring Facebook executives, even Zuckerberg, in to testify. GARCIA-NAVARRO: So not the end of this story - Matthew Rosenberg from the New York Times, thank you so much. ROSENBERG: Thank you. LULU GARCIA-NAVARRO, HOST:  We're getting new information about a different kind of interference in the 2016 election. This time, it involves a data firm called Cambridge Analytica. A joint investigation from The New York Times and The London Observer found that the firm, quote, \"harvested private information from the Facebook profiles of more than 50 million users without their permission. \" It later used that data to help target American voters for its work with President Trump's campaign. Facebook has now suspended the company from its platform. For more, Matthew Rosenberg joins me in the studio. He covers intelligence and national security for The New York Times, and he co-authored this story. Welcome to the program. MATTHEW ROSENBERG: Morning. GARCIA-NAVARRO: So tell us what Cambridge Analytica is. ROSENBERG: It's a data firm, basically - founded by Robert Mercer and his daughter Rebekah, these billionaires - kind of Republican donors - and Steve Bannon. And the idea, the conceit was that they're going to use big data to map the personality of every voter. And by mapping the personality, they could then predict their behavior, sway them, influence them and change American political culture - shift it to the right, shift it to the views they wanted to establish. GARCIA-NAVARRO: Influence them how? ROSENBERG: You know, through a variety of advertising. So, like, let's say you find out this particular voter is neurotic. OK. Let's scare them with talk of crime and other issues. Let's say you find somebody who's open-minded and kind of optimistic. Well, how can we advertise at them? How can we appeal to that kind of point of view? It's a very - you know, they're using something called psychographics, which is a very new field and academic discipline that has a number of proponents and a lot of critics. And it's really uncertain how well this worked in 2014 when the firm first started going and then in 2016. But, you know, its proponents, like some of the guys at Cambridge Analytica, think this is the future, that we can really shape American politics and reshape them this way. GARCIA-NAVARRO: At the core of your investigation, though, there was this huge data breach - 50 million users, which is a number that we hadn't really heard before. Explain why you call it a data breach. ROSENBERG: Look. There was a bunch of information that was on Facebook and ended up with a private company where it wasn't supposed to, where most of those users had no idea it was going to end up. To us, that's a breach. GARCIA-NAVARRO: And what is exactly the information that they got? ROSENBERG: So this would be, you know, everything that was on, say, your Facebook profile, your basic bio data, everything you liked, who your friends were, where you lived. A lot of that information is private by default now, or only your friends can see it. And the way this app worked was that somebody would download the app. It would scrape all their information, and then it would scrape all their friends' information, which, in 2014, Facebook allowed. The thing is that the app said it was doing it for academic research, which it wasn't. It was doing it for Cambridge Analytica. The person - the professor who designed the app did this specifically for Cambridge Analytica. And throughout the process, Cambridge Analytica was paying the bills for the data collection. So it's not like somebody stumbled upon it and just sold it to them. GARCIA-NAVARRO: So basically, they misrepresented what they were really using it for. ROSENBERG: Exactly. GARCIA-NAVARRO: You and other news organizations got a lot of this new information from a leaker, Chris Wylie. He helped found Cambridge Analytica. And the U. K. 's Channel 4 actually spoke with him. And here is what he says. He says Cambridge Analytica basically weaponized your Facebook page. (SOUNDBITE OF ARCHIVED RECORDING) CHRISTOPHER WYLIE: So whenever you go, and you like something, you are giving me a clue as to who you are as a person. And so all of this can be captured very easily and run through an algorithm that learns who you are. When you go to work - right? - your co-workers only see one side of you. Your friends only see one side of you. But a computer sees all kinds of sides of you. And so we can get better than human level accuracy at predicting your behavior. GARCIA-NAVARRO: So tell me about Chris Wylie. ROSENBERG: Chris is a fascinating kid. He really believes that this is the future. He is a 28-year-old vegan from Canada who dropped out of high school, ended up getting into politics and data politics in Canada with Ken Strasma, who is Obama's big data guy. And he's got pictures of himself at Obama's 2012 inauguration. He ends up in London just into these circles with the people who eventually become Cambridge Analytica. And look. He is not a right-wing ideologue. He told me at one point that he quit the company because he wanted to be in fashion, not fascism, as he put it. And it's one of the interesting things about Cambridge Analytica. Their original data team was a lot of young, gay, liberal men. GARCIA-NAVARRO: But they did obviously get into bed with the Mercers and Steve Bannon. Let's hear more of Wylie speaking to Channel 4 about how Steve Bannon, who was Trump's chief strategist, wanted to use this technology. (SOUNDBITE OF ARCHIVED RECORDING) WYLIE: Steve wanted weapons for his culture war. That's what he wanted. We offered him a way to accomplish what he wanted to do, which was change the culture of America. GARCIA-NAVARRO: So did this actually change people's mind? Could it really have this bold effect across America? ROSENBERG: It's a big question. And I don't think we have the evidence to back that up at this point. You know, is this going to be able to work out in the future? Is the data going to be robust enough to be able to figure out the questions, figure out what personalities are and then advertise or market and micro-target at them? Perhaps. I mean, it certainly looks like we're moving in that direction. But I don't think in 2016 we really have the evidence to say that they had this dramatic change. GARCIA-NAVARRO: So how does what you uncovered link to Robert Mueller's investigation into election interference? ROSENBERG: So we know that that Mueller's team has asked Cambridge Analytica for all its documents and emails related to the Trump campaign. Exactly what they're looking for, we just don't know. You know, the Mueller investigation's a bit of a black box. GARCIA-NAVARRO: But your reporting showed that there is something interesting linking Cambridge Analytica to Russia. ROSENBERG: So there are all kinds of kind of unexplained connections. At one point, in the summer of 2014, Lukoil, which is a Russian oil company that's tight with Putin's inner circle, showed up and started to talk to them about American voter data, which was - they thought was very odd. What does Lukoil, which has, like, two gas stations in the United States, want with American voter data or consumer data? It was - it didn't make a lot of sense. You know, Alexander Nix, who's chief executive, was recently in front of Parliament. He said they've never done any business in Russia full stop. But we found brochures for their parent company that says they do have business in Russia. And so it's just questions that aren't really answered there. GARCIA-NAVARRO: And Facebook's reaction to your reporting has been that they have banned Cambridge Analytica from their platform. And now there are some Congress people who are asking for an investigation. ROSENBERG: I know Senator Klobuchar and Senator Warner have called for further investigation and to bring Facebook executives, even Zuckerberg, in to testify. GARCIA-NAVARRO: So not the end of this story - Matthew Rosenberg from the New York Times, thank you so much. ROSENBERG: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-18-594671282": {"title": "Can You Choose A Romantic Partner Just By Their Voice? A Dating App Thinks So : NPR", "url": "https://www.npr.org/2018/03/18/594671282/determine-potential-partners-by-voice-in-new-dating-app", "author": "No author found", "published_date": "2018-03-18", "content": "LULU GARCIA-NAVARRO, HOST: Could you choose a romantic partner just based on their voice? UNIDENTIFIED WOMAN: I'm a very sexy baby. I can't help it if men are attracted to me. UNIDENTIFIED MAN: I have a sexy voice. Champagne, mountain range, hugs. GARCIA-NAVARRO: A new dating app called Waving lets you do just that. It's like Tinder. You know, swipe right for someone you're into. Swipe left for someone you're not. But no pictures. No long bios to help you decide - just a short recorded message. ROBERT BURRISS: Well, the first thing I thought when I heard about this app was, you know, it sounds like a bit of a gimmick. GARCIA-NAVARRO: That's Robert Burriss, an evolutionary psychologist. He studies human attraction. BURRISS: There's a lot of research being done to show that voices are really important when it comes to judging a partner. GARCIA-NAVARRO: We asked Burriss to give us the lowdown on what makes a voice hot or not. BURRISS: We know that men with deeper voices are found more attractive by women. Women with higher-pitched voices tend to be more attractive to men. But it's not an arbitrary preference. It's based on biology. GARCIA-NAVARRO: He says a deeper voice in a man could mean a high level of testosterone. And for a woman, a higher-pitched voice might signal high estrogen levels. Burriss did have one caveat. In one study of hunter-gatherer populations in Tanzania, pregnant women preferred men with higher-pitched voices. BURRISS: And this might be because women who are pregnant are sort of primed to want to affiliate with men who are more feminine, more friendly, perhaps. And these men tend to have a higher-pitched voice. GARCIA-NAVARRO: Convinced yet? If you're ready to sign up, Burriss offered this suggestion for your voice-dating profile - vary your tone. He says it's like smiling in a picture. BURRISS: If they go up and down like this, then they will sound more approachable, more extrovert. Whereas introverts - they will tend to sort of sink back into their voice. And their voice will be more monotonous. GARCIA-NAVARRO: So if you're looking to date an extrovert or host a radio program, this might not be attractive. But despite all his research, Burriss says don't delete those selfies just yet. BURRISS: Before that finger presses that button to say, yes, it's a match, they're probably going to want to see the face. Do I really think people are going to go on lots of dates with people they meet on this app? I'm not so sure. GARCIA-NAVARRO: That's Robert Burriss, a postdoctoral researcher at Basel University in Switzerland. (SOUNDBITE OF SONG, \"YOU HAD ME FROM HELLO\")KENNY CHESNEY: (Singing) Well, you had me from hello. I felt love start to grow. The moment that I looked into your eyes, you won me. It was over from the start. LULU GARCIA-NAVARRO, HOST:  Could you choose a romantic partner just based on their voice? UNIDENTIFIED WOMAN: I'm a very sexy baby. I can't help it if men are attracted to me. UNIDENTIFIED MAN: I have a sexy voice. Champagne, mountain range, hugs. GARCIA-NAVARRO: A new dating app called Waving lets you do just that. It's like Tinder. You know, swipe right for someone you're into. Swipe left for someone you're not. But no pictures. No long bios to help you decide - just a short recorded message. ROBERT BURRISS: Well, the first thing I thought when I heard about this app was, you know, it sounds like a bit of a gimmick. GARCIA-NAVARRO: That's Robert Burriss, an evolutionary psychologist. He studies human attraction. BURRISS: There's a lot of research being done to show that voices are really important when it comes to judging a partner. GARCIA-NAVARRO: We asked Burriss to give us the lowdown on what makes a voice hot or not. BURRISS: We know that men with deeper voices are found more attractive by women. Women with higher-pitched voices tend to be more attractive to men. But it's not an arbitrary preference. It's based on biology. GARCIA-NAVARRO: He says a deeper voice in a man could mean a high level of testosterone. And for a woman, a higher-pitched voice might signal high estrogen levels. Burriss did have one caveat. In one study of hunter-gatherer populations in Tanzania, pregnant women preferred men with higher-pitched voices. BURRISS: And this might be because women who are pregnant are sort of primed to want to affiliate with men who are more feminine, more friendly, perhaps. And these men tend to have a higher-pitched voice. GARCIA-NAVARRO: Convinced yet? If you're ready to sign up, Burriss offered this suggestion for your voice-dating profile - vary your tone. He says it's like smiling in a picture. BURRISS: If they go up and down like this, then they will sound more approachable, more extrovert. Whereas introverts - they will tend to sort of sink back into their voice. And their voice will be more monotonous. GARCIA-NAVARRO: So if you're looking to date an extrovert or host a radio program, this might not be attractive. But despite all his research, Burriss says don't delete those selfies just yet. BURRISS: Before that finger presses that button to say, yes, it's a match, they're probably going to want to see the face. Do I really think people are going to go on lots of dates with people they meet on this app? I'm not so sure. GARCIA-NAVARRO: That's Robert Burriss, a postdoctoral researcher at Basel University in Switzerland. (SOUNDBITE OF SONG, \"YOU HAD ME FROM HELLO\") KENNY CHESNEY: (Singing) Well, you had me from hello. I felt love start to grow. The moment that I looked into your eyes, you won me. It was over from the start.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-19-595018854": {"title": "Uber Say It Will Cooperate With Investigation After Pedestrian Killed In Arizona : NPR", "url": "https://www.npr.org/2018/03/19/595018854/uber-say-it-will-cooperate-with-investigation-after-pedestrian-killed-in-arizona", "author": "No author found", "published_date": "2018-03-19", "content": "AILSA CHANG, HOST: A self-driving car operated by Uber struck and killed a woman this weekend. It happened in Tempe, Ariz. , and is thought to be the first pedestrian death involving an autonomous vehicle on a public road. NPR's Laura Sydell covers tech, and she joins us now to talk about the accident. Hey, Laura. LAURA SYDELL, BYLINE: Hello. CHANG: What exactly happened? SYDELL: Well, the Tempe Police Department says at around 10 p. m. Sunday night, an Uber self-driving vehicle hit a pedestrian crossing the road outside of a crosswalk. She was walking her bicycle. They've identified her as a 49-year-old woman. She was taken to a local hospital where she died from injuries inflicted by the crash. CHANG: And what do we know about the car? SYDELL: Well, Uber's been testing self-driving vehicles around Tempe and Phoenix, and this was one of those cars. In this case, there was actually a person in the car. And the person is supposed to take over if there is an emergency. Unfortunately it doesn't look like that happened here. While we don't know what caused the accident, the victim was crossing outside of a crosswalk with her bike. And one of the big issues with autonomous vehicles is getting them to respond to unpredictable human behavior. CHANG: And that's not what happened here. How has Uber responded to all of this? SYDELL: The company says it's fully cooperating with authorities, and in a statement, it said, our hearts go out to the victim's family. For now, Uber says it is suspending testing of autonomous vehicles in North America. And that's around the Phoenix area. Uber also has vehicles in Pittsburgh, San Francisco and Toronto. CHANG: You know, one thing that a lot of the companies developing self-driving cars talk about is that these cars will be safer than cars with human drivers. Do you think an accident like this will slow down the adoption of these cars? SYDELL: Well, you know, we can't know for sure how this will impact the deployment of autonomous vehicles, but it certainly looks like the kind of event that's going to feed the critics. More than 20 states have authorized self-driving cars. Arizona's been particularly welcoming. And the mayor of Tempe, though, tweeted out in a statement saying that the city's been supportive of autonomous vehicles because they're promised for disabled residents and seniors. But he said testing's got to happen safely. And Democratic Senator Edward Markey of Massachusetts issued a statement saying that the accident underscores why we have to be cautious testing these technologies on public roads. CHANG: All right, that's NPR's Laura Sydell. Thank you so much, Laura. SYDELL: You're welcome. AILSA CHANG, HOST:  A self-driving car operated by Uber struck and killed a woman this weekend. It happened in Tempe, Ariz. , and is thought to be the first pedestrian death involving an autonomous vehicle on a public road. NPR's Laura Sydell covers tech, and she joins us now to talk about the accident. Hey, Laura. LAURA SYDELL, BYLINE: Hello. CHANG: What exactly happened? SYDELL: Well, the Tempe Police Department says at around 10 p. m. Sunday night, an Uber self-driving vehicle hit a pedestrian crossing the road outside of a crosswalk. She was walking her bicycle. They've identified her as a 49-year-old woman. She was taken to a local hospital where she died from injuries inflicted by the crash. CHANG: And what do we know about the car? SYDELL: Well, Uber's been testing self-driving vehicles around Tempe and Phoenix, and this was one of those cars. In this case, there was actually a person in the car. And the person is supposed to take over if there is an emergency. Unfortunately it doesn't look like that happened here. While we don't know what caused the accident, the victim was crossing outside of a crosswalk with her bike. And one of the big issues with autonomous vehicles is getting them to respond to unpredictable human behavior. CHANG: And that's not what happened here. How has Uber responded to all of this? SYDELL: The company says it's fully cooperating with authorities, and in a statement, it said, our hearts go out to the victim's family. For now, Uber says it is suspending testing of autonomous vehicles in North America. And that's around the Phoenix area. Uber also has vehicles in Pittsburgh, San Francisco and Toronto. CHANG: You know, one thing that a lot of the companies developing self-driving cars talk about is that these cars will be safer than cars with human drivers. Do you think an accident like this will slow down the adoption of these cars? SYDELL: Well, you know, we can't know for sure how this will impact the deployment of autonomous vehicles, but it certainly looks like the kind of event that's going to feed the critics. More than 20 states have authorized self-driving cars. Arizona's been particularly welcoming. And the mayor of Tempe, though, tweeted out in a statement saying that the city's been supportive of autonomous vehicles because they're promised for disabled residents and seniors. But he said testing's got to happen safely. And Democratic Senator Edward Markey of Massachusetts issued a statement saying that the accident underscores why we have to be cautious testing these technologies on public roads. CHANG: All right, that's NPR's Laura Sydell. Thank you so much, Laura. SYDELL: You're welcome.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-19-595018770": {"title": "Facebook Admits Data-Mining Firm Got Access To Millions Of Users' Personal Information : NPR", "url": "https://www.npr.org/2018/03/19/595018770/facebook-admits-data-mining-firm-got-access-to-millions-of-users-personal-inform", "author": "No author found", "published_date": "2018-03-19", "content": "AILSA CHANG, HOST: Facebook's share price tumbled today. The tech giant is on the defensive, and that's because it let the private information of some 50 million users wind up in the hands of a data mining firm that worked for the Trump campaign. From Capitol Hill to Westminster in London, lawmakers want to know why Facebook didn't do more to protect users' privacy. In a moment, we'll hear from a state attorney general looking into the matter. But now we're joined by NPR tech correspondent Aarti Shahani, who's at Facebook's headquarters in Menlo Park. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. CHANG: So how Facebook data wound up in the hands of this data mining firm is a pretty complicated story. Can you just start off by breaking that down for us? SHANANI: Yeah, so the story starts three years ago. There's this Russian-American professor, and he made an app, OK? He built it as a personality predictor. Give us your Facebook feed, your likes, your posts, and we'll tell you about yourself - OK, sounds innocuous. Well, about 270,000 people downloaded. But because of how Facebook set up its system, the app didn't just get their profiles. It sucked in friends' profiles, too. So that's you get up to 50 million users, the vast majority of whom did not opt in, right? CHANG: Right. SHANANI: And this guy turns around, and he passes the data on to Cambridge Analytica, which is against Facebook's terms. And, you know, again, Cambridge Analytica is a firm that helped the Trump campaign with social media targeting. So fast-forward to this past Friday. Out of the blue, a Facebook lawyer posts a blog saying, hey, back in 2015, we learned about this. We told the parties to delete the data, and we've discovered maybe that did not happen. What the lawyer failed to say was The New York Times and the U. K. 's Observer were about to publish an explosive report about it. CHANG: Right, OK. And I know you've been hanging out today at Facebook's headquarters. What has it been like there? SHANANI: Yes, yeah. Hi from the parking lot. (LAUGHTER)SHANANI: You know, the thing is I came here - Facebook is under attack from all over, right? And you'd expect the executives there to explain and defend themselves publicly. But that hasn't happened, you know? What they tend to do is hole up and go quiet at the very moment, you know, when the world is really asking them to speak up. So you know, we figured it made sense for me to show up here, come to their doorstep and ask them to talk to us. CHANG: And what's happened so far? SHANANI: Well, the social network is not being that social. CHANG: (Laughter). SHANANI: Security was kind enough to seat me. And a spokeswoman, Genevieve Grdina, brought me water and offered me tea and coffee. CHANG: That's nice. SHANANI: But, you know, I'm not here for that. I'm here for an interview. But she did tell me they're not going to grant one today. But she says they are working on scheduling one for tomorrow with NPR. CHANG: OK, well, what has Facebook said about all of this since the news broke a couple days ago? SHANANI: You know, I would just characterize it as a deafening silence, OK? There's been nothing - not a word, not a Facebook post - from the CEO, Mark Zuckerberg, or from Sheryl Sandberg. There was that blog post I mentioned from the Facebook lawyer. CHANG: Right. SHANANI: He also added an update there saying that, hey, some people are calling this a data breach. It's not a breach in the traditional sense. And so, you know, I'll be curious to see if they change strategy and, instead of a silent treatment, you know, move on to talking. CHANG: And just very briefly, this morning on NPR, Minnesota Senator Amy Klobuchar said that these platforms cannot police themselves. She's among the senators calling on Mark Zuckerberg to testify before Congress. But what do you see Congress doing? How do they take a role in this policy-wise? SHANANI: Namely consumer protection, OK? Right now, Facebook isn't really governed by laws. It's governed by competition and reputational damage. You know, what are the penalties that Facebook should face when it lets users' data fall in the wrong hands? CHANG: All right, that's NPR's Aarti Shahani. Thank you. SHANANI: Thank you. AILSA CHANG, HOST:  Facebook's share price tumbled today. The tech giant is on the defensive, and that's because it let the private information of some 50 million users wind up in the hands of a data mining firm that worked for the Trump campaign. From Capitol Hill to Westminster in London, lawmakers want to know why Facebook didn't do more to protect users' privacy. In a moment, we'll hear from a state attorney general looking into the matter. But now we're joined by NPR tech correspondent Aarti Shahani, who's at Facebook's headquarters in Menlo Park. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. CHANG: So how Facebook data wound up in the hands of this data mining firm is a pretty complicated story. Can you just start off by breaking that down for us? SHANANI: Yeah, so the story starts three years ago. There's this Russian-American professor, and he made an app, OK? He built it as a personality predictor. Give us your Facebook feed, your likes, your posts, and we'll tell you about yourself - OK, sounds innocuous. Well, about 270,000 people downloaded. But because of how Facebook set up its system, the app didn't just get their profiles. It sucked in friends' profiles, too. So that's you get up to 50 million users, the vast majority of whom did not opt in, right? CHANG: Right. SHANANI: And this guy turns around, and he passes the data on to Cambridge Analytica, which is against Facebook's terms. And, you know, again, Cambridge Analytica is a firm that helped the Trump campaign with social media targeting. So fast-forward to this past Friday. Out of the blue, a Facebook lawyer posts a blog saying, hey, back in 2015, we learned about this. We told the parties to delete the data, and we've discovered maybe that did not happen. What the lawyer failed to say was The New York Times and the U. K. 's Observer were about to publish an explosive report about it. CHANG: Right, OK. And I know you've been hanging out today at Facebook's headquarters. What has it been like there? SHANANI: Yes, yeah. Hi from the parking lot. (LAUGHTER) SHANANI: You know, the thing is I came here - Facebook is under attack from all over, right? And you'd expect the executives there to explain and defend themselves publicly. But that hasn't happened, you know? What they tend to do is hole up and go quiet at the very moment, you know, when the world is really asking them to speak up. So you know, we figured it made sense for me to show up here, come to their doorstep and ask them to talk to us. CHANG: And what's happened so far? SHANANI: Well, the social network is not being that social. CHANG: (Laughter). SHANANI: Security was kind enough to seat me. And a spokeswoman, Genevieve Grdina, brought me water and offered me tea and coffee. CHANG: That's nice. SHANANI: But, you know, I'm not here for that. I'm here for an interview. But she did tell me they're not going to grant one today. But she says they are working on scheduling one for tomorrow with NPR. CHANG: OK, well, what has Facebook said about all of this since the news broke a couple days ago? SHANANI: You know, I would just characterize it as a deafening silence, OK? There's been nothing - not a word, not a Facebook post - from the CEO, Mark Zuckerberg, or from Sheryl Sandberg. There was that blog post I mentioned from the Facebook lawyer. CHANG: Right. SHANANI: He also added an update there saying that, hey, some people are calling this a data breach. It's not a breach in the traditional sense. And so, you know, I'll be curious to see if they change strategy and, instead of a silent treatment, you know, move on to talking. CHANG: And just very briefly, this morning on NPR, Minnesota Senator Amy Klobuchar said that these platforms cannot police themselves. She's among the senators calling on Mark Zuckerberg to testify before Congress. But what do you see Congress doing? How do they take a role in this policy-wise? SHANANI: Namely consumer protection, OK? Right now, Facebook isn't really governed by laws. It's governed by competition and reputational damage. You know, what are the penalties that Facebook should face when it lets users' data fall in the wrong hands? CHANG: All right, that's NPR's Aarti Shahani. Thank you. SHANANI: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-20-595344888": {"title": "Former Facebook Insider Says Company Cannot Be Trusted To Regulate Itself : NPR", "url": "https://www.npr.org/2018/03/20/595344888/former-facebook-insider-says-company-cannot-be-trusted-to-regulate-itself", "author": "No author found", "published_date": "2018-03-20", "content": "AILSA CHANG, HOST: Lawmakers and regulators are demanding answers from Facebook. The company has acknowledged that the personal data of 50 million Facebook users was harvested by an outside group for political purposes. We'll hear more about that firm, Cambridge Analytica, in a few minutes. But first we're going to hear from a former Facebook employee who saw firsthand how the company treated user data. SANDY PARAKILAS: I would characterize it as insufficient and sometimes negligent. CHANG: That's Sandy Parakilas. Between 2011 and 2012, he was responsible for protecting the user data of people who used Facebook apps. Those are outside programs like \"FarmVille\" that plug directly into the platform. It was through a Facebook app two years after Parakilas left the company that Cambridge Analytica was able to get the data of those 50 million users. PARAKILAS: So the apps that use Facebook platform are only held in compliance by policies that Facebook writes. And there was no way for Facebook to see what was happening to user data once it passed to the developers of those apps - people like Aleksandr Kogan, who built the app that passed all this data to Cambridge Analytica. And I thought that was a big problem. CHANG: OK, so just to be clear, the way Cambridge Analytica was able to get this data is 270,000 users signed up for a personality quiz designed by Aleksandr Kogan, and then the app designed by Kogan was able to collect data on the friends of those 270,000 users, correct? PARAKILAS: That's correct. CHANG: And when you were there, did you see any kind of safeguards in place at Facebook to make sure the data wasn't being abused? There was - it was literally turn the other way. PARAKILAS: They really didn't want to know, to a certain extent, what was happening with the data once it left Facebook. And I pushed repeatedly for more audits, more protection. And I didn't get much traction. They didn't seem to prioritize protecting users over the growth of Facebook apps. CHANG: Give me a. . . PARAKILAS: And in fact. . . CHANG: . . . Specific example of how you pushed for a better response to privacy. PARAKILAS: So in mid-2012, I created a PowerPoint deck that outlined all the ways that Facebook data was vulnerable on Facebook platform, and I included a list of the kinds of bad actors who I had either seen trying to do bad things with Facebook data or who I hypothesized might do bad things with Facebook data. And some of the actors on the list included foreign state actors and data brokers. And I sent this PowerPoint deck to senior executives at the company. And I - you know, I didn't really see much change happen as a result. And what's been really frustrating to me is that what happened with Cambridge Analytica was something that I warned about years before it happened. And their response was exactly the same as the response that I saw them take when I was at the company. And I thought it was insufficient then, and I think it's insufficient now. CHANG: Are these people you're describing inside Facebook who are unresponsive - are they still with the company? PARAKILAS: Yes. CHANG: From what I understand, policies at Facebook have changed, right? So the data breach we saw with Cambridge Analytica wouldn't happen under current policies, right? PARAKILAS: Well, yes and no. The first issue is that Aleksandr Kogan had all of this data - both the people who had authorized the app and the people who hadn't - the friends. And he passed all of that information to Cambridge Analytica. Passing any of that information was a huge policy violation. What has changed is not the ability of Facebook to enforce on developers like Kogan who pass data. What's changed is they have restricted the amount of data that you can get on friends. So the entirety of the breach would not be possible today, but some elements of it still would be. CHANG: You left the company six years ago. Do you have any reason to believe that attitudes inside the company have changed? PARAKILAS: I think what happened with Cambridge Analytica shows that nothing has changed, and I think it's really important to understand the timeline of what happened. In 2014, Aleksandr Kogan made this app, and he misrepresented it to users as a psychological profile app. CHANG: Right. PARAKILAS: And he then harvested all of this data for commercial purposes not just from the users of the app but from their friends who had no idea this app even existed. Then he passed that data to Cambridge Analytica, which broke Facebook policy. Facebook discovered this in December 2015 when there was an article in The Guardian that described what was going on. And they evidently reached out to Kogan and Cambridge Analytica and said, we need you to delete the data. But they just took the word of those two parties that they had deleted the data. And in fact, they had not. They lied. And the problem is Facebook had a right to audit. They had the ability to go in and demand a physical audit of the disk drives and storage and the code of the application itself. They didn't use that, nor did they sue either of these parties at that point despite the fact that it was a serious violation. And they waited, and they did nothing until last Friday when they finally started to take real action against Cambridge Analytica and banned them from Facebook's platform. CHANG: And why do you think they chose that - to not act more aggressively back in 2015? What incentives do they have not to act more aggressively? PARAKILAS: I believe that their position is that they are in a better legal position if they don't uncover abuse - that they can say, well, we didn't know. And to me, that is negligent. If you can reasonably know something simply by reading the press and then by investigating but you choose not to because you believe that it protects you legally, I think that is negligence. CHANG: What do you want to happen inside the company to help address these privacy concerns? PARAKILAS: Well, I mean, the first thing that absolutely needs to happen is Mark Zuckerberg needs to testify to U. S. Congress and in parliaments and other government bodies in other countries that have serious concerns about this. And they must be dramatically more transparent. CHANG: Sandy Parakilas is a former manager at Facebook. Thanks very much for joining us. PARAKILAS: Thank you, Ailsa. CHANG: NPR did invite a Facebook executive to come on the air, but the company says it is not doing interviews right now. AILSA CHANG, HOST:  Lawmakers and regulators are demanding answers from Facebook. The company has acknowledged that the personal data of 50 million Facebook users was harvested by an outside group for political purposes. We'll hear more about that firm, Cambridge Analytica, in a few minutes. But first we're going to hear from a former Facebook employee who saw firsthand how the company treated user data. SANDY PARAKILAS: I would characterize it as insufficient and sometimes negligent. CHANG: That's Sandy Parakilas. Between 2011 and 2012, he was responsible for protecting the user data of people who used Facebook apps. Those are outside programs like \"FarmVille\" that plug directly into the platform. It was through a Facebook app two years after Parakilas left the company that Cambridge Analytica was able to get the data of those 50 million users. PARAKILAS: So the apps that use Facebook platform are only held in compliance by policies that Facebook writes. And there was no way for Facebook to see what was happening to user data once it passed to the developers of those apps - people like Aleksandr Kogan, who built the app that passed all this data to Cambridge Analytica. And I thought that was a big problem. CHANG: OK, so just to be clear, the way Cambridge Analytica was able to get this data is 270,000 users signed up for a personality quiz designed by Aleksandr Kogan, and then the app designed by Kogan was able to collect data on the friends of those 270,000 users, correct? PARAKILAS: That's correct. CHANG: And when you were there, did you see any kind of safeguards in place at Facebook to make sure the data wasn't being abused? There was - it was literally turn the other way. PARAKILAS: They really didn't want to know, to a certain extent, what was happening with the data once it left Facebook. And I pushed repeatedly for more audits, more protection. And I didn't get much traction. They didn't seem to prioritize protecting users over the growth of Facebook apps. CHANG: Give me a. . . PARAKILAS: And in fact. . . CHANG: . . . Specific example of how you pushed for a better response to privacy. PARAKILAS: So in mid-2012, I created a PowerPoint deck that outlined all the ways that Facebook data was vulnerable on Facebook platform, and I included a list of the kinds of bad actors who I had either seen trying to do bad things with Facebook data or who I hypothesized might do bad things with Facebook data. And some of the actors on the list included foreign state actors and data brokers. And I sent this PowerPoint deck to senior executives at the company. And I - you know, I didn't really see much change happen as a result. And what's been really frustrating to me is that what happened with Cambridge Analytica was something that I warned about years before it happened. And their response was exactly the same as the response that I saw them take when I was at the company. And I thought it was insufficient then, and I think it's insufficient now. CHANG: Are these people you're describing inside Facebook who are unresponsive - are they still with the company? PARAKILAS: Yes. CHANG: From what I understand, policies at Facebook have changed, right? So the data breach we saw with Cambridge Analytica wouldn't happen under current policies, right? PARAKILAS: Well, yes and no. The first issue is that Aleksandr Kogan had all of this data - both the people who had authorized the app and the people who hadn't - the friends. And he passed all of that information to Cambridge Analytica. Passing any of that information was a huge policy violation. What has changed is not the ability of Facebook to enforce on developers like Kogan who pass data. What's changed is they have restricted the amount of data that you can get on friends. So the entirety of the breach would not be possible today, but some elements of it still would be. CHANG: You left the company six years ago. Do you have any reason to believe that attitudes inside the company have changed? PARAKILAS: I think what happened with Cambridge Analytica shows that nothing has changed, and I think it's really important to understand the timeline of what happened. In 2014, Aleksandr Kogan made this app, and he misrepresented it to users as a psychological profile app. CHANG: Right. PARAKILAS: And he then harvested all of this data for commercial purposes not just from the users of the app but from their friends who had no idea this app even existed. Then he passed that data to Cambridge Analytica, which broke Facebook policy. Facebook discovered this in December 2015 when there was an article in The Guardian that described what was going on. And they evidently reached out to Kogan and Cambridge Analytica and said, we need you to delete the data. But they just took the word of those two parties that they had deleted the data. And in fact, they had not. They lied. And the problem is Facebook had a right to audit. They had the ability to go in and demand a physical audit of the disk drives and storage and the code of the application itself. They didn't use that, nor did they sue either of these parties at that point despite the fact that it was a serious violation. And they waited, and they did nothing until last Friday when they finally started to take real action against Cambridge Analytica and banned them from Facebook's platform. CHANG: And why do you think they chose that - to not act more aggressively back in 2015? What incentives do they have not to act more aggressively? PARAKILAS: I believe that their position is that they are in a better legal position if they don't uncover abuse - that they can say, well, we didn't know. And to me, that is negligent. If you can reasonably know something simply by reading the press and then by investigating but you choose not to because you believe that it protects you legally, I think that is negligence. CHANG: What do you want to happen inside the company to help address these privacy concerns? PARAKILAS: Well, I mean, the first thing that absolutely needs to happen is Mark Zuckerberg needs to testify to U. S. Congress and in parliaments and other government bodies in other countries that have serious concerns about this. And they must be dramatically more transparent. CHANG: Sandy Parakilas is a former manager at Facebook. Thanks very much for joining us. PARAKILAS: Thank you, Ailsa. CHANG: NPR did invite a Facebook executive to come on the air, but the company says it is not doing interviews right now.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-20-595344980": {"title": "Facebook's Cambridge Analytica Scandal Latest Blow To The Company's Reputation : NPR", "url": "https://www.npr.org/2018/03/20/595344980/can-facebook-move-past-latest-breach-of-user-data", "author": "No author found", "published_date": "2018-03-20", "content": "AILSA CHANG, HOST: Facebook continues to face blowback from users, regulators and investors. Its stock price fell for a second straight day. Lawmakers are demanding hearings. And some users say they are quitting the social media app. This all follows accusations that a company working with President Trump's 2016 campaign misused the personal data of millions of Facebook users. NPR's Yuki Noguchi reports. YUKI NOGUCHI, BYLINE: Over the weekend, angry former Facebook users turned to Twitter to express their discontent. David Chartier, a freelance writer in Chicago, was one of them. DAVID CHARTIER: You can and arguably should delete Facebook. NOGUCHI: Chartier was responding to news that a voter profiling firm called Cambridge Analytica allegedly collected and used data from 50 million Facebook users without their permission. Chartier's tweet, one of many like it, proved one of his most popular, eliciting 300 retweets and several responses. CHARTIER: People who have been thinking about it - and they're getting inspired to now. Or people who have done it recently - you know, I deleted mine a year ago, and I've never felt better - that kind of thing. NOGUCHI: Calls for a consumer boycott of Facebook is just one of the company's mounting problems. It failed to prevent the dissemination of fake news and misleading ads during the last presidential election. And now it appears it failed to keep its users' data safe. Tavis McGinn says he thinks the unauthorized use of data involves more companies than just Cambridge Analytica. TAVIS MCGINN: You have to assume that there are many, many other companies who are out there using the data in a similar way. NOGUCHI: McGinn now has his own research firm but last year worked for Facebook CEO Mark Zuckerberg and COO Sheryl Sandberg, conducting reputation research and polling for them. He says those problems have gotten markedly worse since he left. MCGINN: A lot of the damage has already been done, and it is very hard to rebuild trust. NOGUCHI: Motley Fool analyst Jason Moser says Facebook executives are hurting its stock price and making its brand problems far worse by not responding to the public outcry. Facebook shares are off nearly 10 percent says the Cambridge Analytica news broke. JASON MOSER: Mark Zuckerberg made his resolution for 2018 to fix the Facebook platform. Right now that's getting off to, I mean, I think we could all say probably a horrendous start. NOGUCHI: Facebook could not be reached for comment. Moser says Facebook is likely to face regulatory pressure. According to news reports, the Federal Trade Commission is investigating the incident. Facebook has grown through acquisition of companies such as Instagram and WhatsApp. But Moser says it isn't likely to win regulators' blessings on any future deals. MOSER: Regulators would put that under the microscope and give it a very thorough examination because of what we're seeing right now. NOGUCHI: Meanwhile, Moser says the Delete Facebook movement speaks to an overall disenchantment among users. He says he informally surveys friends and acquaintances about their recent Facebook use. Every one of them reports having cut back. MOSER: But then it also has to make you wonder, are Facebook's best days behind it? NOGUCHI: David Chartier, the Chicago writer, says he was inspired to quit Facebook after other friends quit. And now he says even more friends are following suit. CHARTIER: We just took a little bit of time to talk a different way. Maybe we message each other. Or some of them use Twitter a little more often. You know, maybe we're even calling each other. NOGUCHI: It's an adjustment, he says, but he's happier for it. Yuki Noguchi, NPR News, Washington. AILSA CHANG, HOST:  Facebook continues to face blowback from users, regulators and investors. Its stock price fell for a second straight day. Lawmakers are demanding hearings. And some users say they are quitting the social media app. This all follows accusations that a company working with President Trump's 2016 campaign misused the personal data of millions of Facebook users. NPR's Yuki Noguchi reports. YUKI NOGUCHI, BYLINE: Over the weekend, angry former Facebook users turned to Twitter to express their discontent. David Chartier, a freelance writer in Chicago, was one of them. DAVID CHARTIER: You can and arguably should delete Facebook. NOGUCHI: Chartier was responding to news that a voter profiling firm called Cambridge Analytica allegedly collected and used data from 50 million Facebook users without their permission. Chartier's tweet, one of many like it, proved one of his most popular, eliciting 300 retweets and several responses. CHARTIER: People who have been thinking about it - and they're getting inspired to now. Or people who have done it recently - you know, I deleted mine a year ago, and I've never felt better - that kind of thing. NOGUCHI: Calls for a consumer boycott of Facebook is just one of the company's mounting problems. It failed to prevent the dissemination of fake news and misleading ads during the last presidential election. And now it appears it failed to keep its users' data safe. Tavis McGinn says he thinks the unauthorized use of data involves more companies than just Cambridge Analytica. TAVIS MCGINN: You have to assume that there are many, many other companies who are out there using the data in a similar way. NOGUCHI: McGinn now has his own research firm but last year worked for Facebook CEO Mark Zuckerberg and COO Sheryl Sandberg, conducting reputation research and polling for them. He says those problems have gotten markedly worse since he left. MCGINN: A lot of the damage has already been done, and it is very hard to rebuild trust. NOGUCHI: Motley Fool analyst Jason Moser says Facebook executives are hurting its stock price and making its brand problems far worse by not responding to the public outcry. Facebook shares are off nearly 10 percent says the Cambridge Analytica news broke. JASON MOSER: Mark Zuckerberg made his resolution for 2018 to fix the Facebook platform. Right now that's getting off to, I mean, I think we could all say probably a horrendous start. NOGUCHI: Facebook could not be reached for comment. Moser says Facebook is likely to face regulatory pressure. According to news reports, the Federal Trade Commission is investigating the incident. Facebook has grown through acquisition of companies such as Instagram and WhatsApp. But Moser says it isn't likely to win regulators' blessings on any future deals. MOSER: Regulators would put that under the microscope and give it a very thorough examination because of what we're seeing right now. NOGUCHI: Meanwhile, Moser says the Delete Facebook movement speaks to an overall disenchantment among users. He says he informally surveys friends and acquaintances about their recent Facebook use. Every one of them reports having cut back. MOSER: But then it also has to make you wonder, are Facebook's best days behind it? NOGUCHI: David Chartier, the Chicago writer, says he was inspired to quit Facebook after other friends quit. And now he says even more friends are following suit. CHARTIER: We just took a little bit of time to talk a different way. Maybe we message each other. Or some of them use Twitter a little more often. You know, maybe we're even calling each other. NOGUCHI: It's an adjustment, he says, but he's happier for it. Yuki Noguchi, NPR News, Washington.", "section": "Business", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-21-595886212": {"title": "Zuckerberg's Former Mentor Weighs In On Cambridge Analytica Statement : NPR", "url": "https://www.npr.org/2018/03/21/595886212/zuckerbergs-former-mentor-weighs-in-on-cambridge-analytica-statement", "author": "No author found", "published_date": "2018-03-21", "content": "AILSA CHANG, HOST: Facebook CEO Mark Zuckerberg has finally broken his silence. He issued a statement, which he posted to his own Facebook page, addressing the controversy over how an outside firm harvested the profiles of 50 million Facebook users. Zuckerberg says the company made mistakes, that it will audit thousands of apps and it will put in more safeguards to protect user data. Roger McNamee is managing director at the private equity firm Elevation Partners. He was an early investor in Facebook. He's still a current investor. And he has been a mentor to Mark Zuckerberg. Welcome. ROGER MCNAMEE: It's a great pleasure, Ailsa. CHANG: What do you make of Zuckerberg's statement today? MCNAMEE: I think if he had made the statement in 2015, it would have been completely appropriate. 2015 is when they fully understood what had happened with Cambridge Analytica. And instead, they remained absolutely silent for more than two years knowing that this abuse had occurred. But it's actually worse than that, Ailsa, because in 2016, Facebook had employees embedded in the Trump campaign working side by side with Cambridge Analytica employees. And their job was to get Trump elected. And they did a lot of things together using this exact data set that was taken inappropriately from Facebook in order to get Trump elected. And the important thing is the senior executives at Facebook should have known all the details of that at the time because it was well-known that Cambridge Analytica was working for Trump. And so if Mark wants us to trust him, he's going to have to start with actions. CHANG: And about those actions that you think Zuckerberg should move forward with - when you read his statement, did you think that it outlined enough action? MCNAMEE: No. I think the problem here is it doesn't outline a reasonable plan at all. Again, there are a series of things he has to do simply to demonstrate good faith. He does need to turn over all of the data relative to 2016 with respect to the Russian interference. And he needs to give that to the investigating committees and of course to Robert Mueller. He also needs to reach out to every one of the 126 million Facebook users and 20-plus million Instagram users who were touched by the Russian interference. Explain what happened. Take the blame for essentially being a platform the Russians used to interfere in our election. CHANG: What would make you pull your money out of Facebook now? You're still an investor. MCNAMEE: I am, but I have been selling the stock because I really feel that they're destroying value, and they've harmed democracy. I think their strategy of fighting this issue makes absolutely no sense. There's no happy ending to the plan that they're on. I mean, let's be clear. In 2011, the company signed a consent decree with the Federal Trade Commission that required them to gain positive approval from users for any sharing of their information. You know, they needed to have what is known as informed consent, and Facebook should have hired a whole team to take care of that. And they chose not to do that. But I still feel terrible about it because, at the end of the day, these were my friends. I helped them be successful. I wanted them to be successful. And I was obviously really pleased with all the success they had. What I didn't know was that they were behaving in a manner that is just totally inappropriate. And had I had a chance to do it over again, I would have behaved differently. CHANG: Roger McNamee is managing director at the private equity firm Elevation Partners. He was an early investor in Facebook. Thank you very much for joining us. MCNAMEE: My pleasure. AILSA CHANG, HOST:  Facebook CEO Mark Zuckerberg has finally broken his silence. He issued a statement, which he posted to his own Facebook page, addressing the controversy over how an outside firm harvested the profiles of 50 million Facebook users. Zuckerberg says the company made mistakes, that it will audit thousands of apps and it will put in more safeguards to protect user data. Roger McNamee is managing director at the private equity firm Elevation Partners. He was an early investor in Facebook. He's still a current investor. And he has been a mentor to Mark Zuckerberg. Welcome. ROGER MCNAMEE: It's a great pleasure, Ailsa. CHANG: What do you make of Zuckerberg's statement today? MCNAMEE: I think if he had made the statement in 2015, it would have been completely appropriate. 2015 is when they fully understood what had happened with Cambridge Analytica. And instead, they remained absolutely silent for more than two years knowing that this abuse had occurred. But it's actually worse than that, Ailsa, because in 2016, Facebook had employees embedded in the Trump campaign working side by side with Cambridge Analytica employees. And their job was to get Trump elected. And they did a lot of things together using this exact data set that was taken inappropriately from Facebook in order to get Trump elected. And the important thing is the senior executives at Facebook should have known all the details of that at the time because it was well-known that Cambridge Analytica was working for Trump. And so if Mark wants us to trust him, he's going to have to start with actions. CHANG: And about those actions that you think Zuckerberg should move forward with - when you read his statement, did you think that it outlined enough action? MCNAMEE: No. I think the problem here is it doesn't outline a reasonable plan at all. Again, there are a series of things he has to do simply to demonstrate good faith. He does need to turn over all of the data relative to 2016 with respect to the Russian interference. And he needs to give that to the investigating committees and of course to Robert Mueller. He also needs to reach out to every one of the 126 million Facebook users and 20-plus million Instagram users who were touched by the Russian interference. Explain what happened. Take the blame for essentially being a platform the Russians used to interfere in our election. CHANG: What would make you pull your money out of Facebook now? You're still an investor. MCNAMEE: I am, but I have been selling the stock because I really feel that they're destroying value, and they've harmed democracy. I think their strategy of fighting this issue makes absolutely no sense. There's no happy ending to the plan that they're on. I mean, let's be clear. In 2011, the company signed a consent decree with the Federal Trade Commission that required them to gain positive approval from users for any sharing of their information. You know, they needed to have what is known as informed consent, and Facebook should have hired a whole team to take care of that. And they chose not to do that. But I still feel terrible about it because, at the end of the day, these were my friends. I helped them be successful. I wanted them to be successful. And I was obviously really pleased with all the success they had. What I didn't know was that they were behaving in a manner that is just totally inappropriate. And had I had a chance to do it over again, I would have behaved differently. CHANG: Roger McNamee is managing director at the private equity firm Elevation Partners. He was an early investor in Facebook. Thank you very much for joining us. MCNAMEE: My pleasure.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-21-595791387": {"title": "Blockbuster Plans To Close 1 Store In Alaska, Taking Total Number Down To 5  : NPR", "url": "https://www.npr.org/2018/03/21/595791387/blockbuster-plans-to-close-1-store-in-alaska-taking-total-number-down-to-5", "author": "No author found", "published_date": "2018-03-21", "content": "AILSA CHANG, HOST: A big moment is happening next month in North Pole, Alaska - population about 2,200. MICHEL MARTIN, HOST: There will be one less place to rent a movie. CHANG: A Blockbuster is closing. MARTIN: You heard that right - Blockbuster. Remember big blue signs, lots of movies? We used to rent movies before the Internet took over. At one time, there were 9,000 Blockbusters in the U. S. CHANG: Today there are only six. One is in Oregon, and the other five are in Alaska. MARTIN: And when the Blockbuster closes at North Pole, people like Taylor Neininger will have to adapt. Every week for the past seven years, she's taken her family there to pick out some movies. TAYLOR NEININGER: So the kids - as soon as we walk in, they're running. They know where the kids movies are. And so their excitement to go is pretty fun to watch. CHANG: Neininger says her internet is too slow for the likes of Netflix or Amazon. NEININGER: We've had a lot of issues with the streaming - just a lot of buffering. When it sort of buffers a lot, we kind of get tired of waiting. BRYCE WARD: One of the reasons why the Blockbuster stores have been around in Alaska or continue to be around in Alaska is the Internet's not as fast up here. And a lot of folks like to just go down and pick out a movie. MARTIN: That's North Pole Mayor Bryce Ward. He's among those who still like to browse in an actual store. WARD: Usually when my wife and I have picked out a movie, it's taken us longer to pick out a movie - just walking up and down, looking at all the options - than it has to actually watch it (laughter). MARTIN: Ward says the Internet speed in North Pole has improved, so it was not a surprise that their local Blockbuster will soon close. CHANG: We reached out to the owner of Alaska's remaining Blockbusters. Alan Payne turned down our request for an interview, but he did tell us that despite thousands of customers each week, traffic in his stores is declining. MARTIN: In an email, he writes this. We've loved serving the great people of Alaska for the past 30 years and will continue to do so in our remaining stores as long as we can. CHANG: For her part, Taylor Neininger says she's willing to drive farther to Fairbanks to use her Blockbuster membership card. NEININGER: Fairbanks is about 11 miles away, so there'll still be that. It's not the same, I don't think, but I'm sure we will go. MARTIN: And she sighs. Eventually she will have to start streaming. (SOUNDBITE OF NEON INDIAN'S \"HIT PARADE\") AILSA CHANG, HOST:  A big moment is happening next month in North Pole, Alaska - population about 2,200. MICHEL MARTIN, HOST:  There will be one less place to rent a movie. CHANG: A Blockbuster is closing. MARTIN: You heard that right - Blockbuster. Remember big blue signs, lots of movies? We used to rent movies before the Internet took over. At one time, there were 9,000 Blockbusters in the U. S. CHANG: Today there are only six. One is in Oregon, and the other five are in Alaska. MARTIN: And when the Blockbuster closes at North Pole, people like Taylor Neininger will have to adapt. Every week for the past seven years, she's taken her family there to pick out some movies. TAYLOR NEININGER: So the kids - as soon as we walk in, they're running. They know where the kids movies are. And so their excitement to go is pretty fun to watch. CHANG: Neininger says her internet is too slow for the likes of Netflix or Amazon. NEININGER: We've had a lot of issues with the streaming - just a lot of buffering. When it sort of buffers a lot, we kind of get tired of waiting. BRYCE WARD: One of the reasons why the Blockbuster stores have been around in Alaska or continue to be around in Alaska is the Internet's not as fast up here. And a lot of folks like to just go down and pick out a movie. MARTIN: That's North Pole Mayor Bryce Ward. He's among those who still like to browse in an actual store. WARD: Usually when my wife and I have picked out a movie, it's taken us longer to pick out a movie - just walking up and down, looking at all the options - than it has to actually watch it (laughter). MARTIN: Ward says the Internet speed in North Pole has improved, so it was not a surprise that their local Blockbuster will soon close. CHANG: We reached out to the owner of Alaska's remaining Blockbusters. Alan Payne turned down our request for an interview, but he did tell us that despite thousands of customers each week, traffic in his stores is declining. MARTIN: In an email, he writes this. We've loved serving the great people of Alaska for the past 30 years and will continue to do so in our remaining stores as long as we can. CHANG: For her part, Taylor Neininger says she's willing to drive farther to Fairbanks to use her Blockbuster membership card. NEININGER: Fairbanks is about 11 miles away, so there'll still be that. It's not the same, I don't think, but I'm sure we will go. MARTIN: And she sighs. Eventually she will have to start streaming. (SOUNDBITE OF NEON INDIAN'S \"HIT PARADE\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-21-595791340": {"title": "Will Facebook's Cambridge Analytica Scandal Actually Cause Users To Delete The App? : NPR", "url": "https://www.npr.org/2018/03/21/595791340/will-facebooks-cambridge-analytica-scandal-actually-cause-users-to-delete-the-ap", "author": "No author found", "published_date": "2018-03-21", "content": "MICHEL MARTIN, HOST: It's not really a surprise that people's personal data on Facebook was gathered and sold and spread around. Most people have known for quite some time now that our information online is far from secure. But the recent news about Cambridge Analytica getting millions of people's information seems to have hit a nerve. We wanted to get some perspective on this, so we've called Siva Vaidhyanathan. He is a media studies professor at the University of Virginia, and he's with us now. Professor, thanks so much for joining us. SIVA VAIDHYANATHAN: Thank you, Michel. It's good to talk to you. MARTIN: Well, this has sparked an outcry and some calls to #DeleteFacebook. Do you think this is a real thing? VAIDHYANATHAN: I think it's a thing among an elite strata of Twitter users. I don't think it's going to turn into a mass movement. Nor do I necessarily think it should. You know, people who study social media, people who do data science, privacy advocates, even the Federal Trade Commission - we've all known about this practice that Facebook has had since about 2010. You know, since about 2010, Facebook has been sort of promiscuously and notoriously duplicitous and irresponsible with our data. Now, people are upset about it, and they should be, but they should have been upset about it then, too. And some of us were. But nobody was paying attention because Facebook was this shiny, happy, new company. MARTIN: As you're suggesting that this has been an open secret for years, is there something about this episode, though, that feels different to you? VAIDHYANATHAN: Oh, it feels different in terms of the public awareness. You know, I think that's a major change. And the fact that there seems to be growing dissatisfaction with Facebook - that's been building since the 2016 election when it was pretty clear that President Trump had exploited Facebook successfully. But that was accompanied by serious propaganda efforts, disinformation and misinformation efforts from certain agents coming out of Russia, right? Facebook is just barely coming to terms with that. But I think there's public awareness that I wish had been around before the damage was done. MARTIN: So you're saying you think there is something about this episode that's hitting a nerve. You know, why now is this the episode that is getting all this attention? VAIDHYANATHAN: Well, I think there's a bit of spy novel (laughter) to this whole latest episode, right? Cambridge Analytica itself is this sort of secretive front company that is controlled by billionaire genius Robert Mercer. And Steve Bannon, you know, was on the board. And the people who run Cambridge Analytica seem to brag about their ability to manipulate minds. The spy novel elements of it are very attractive, right? Here's an actual narrative, an actual story for people to get shocked about. MARTIN: I think people are probably getting the clue right now that you are a bit of a Facebook critic. VAIDHYANATHAN: (Laughter) Yes. MARTIN: You have a book coming out soon called \"Anti-Social Media: How Facebook Disconnects Us And Undermines Democracy. \" But I do want to ask if you think that this issue is unique to Facebook. VAIDHYANATHAN: It's unique to Facebook because Facebook is so pervasive, right? Two-point-two billion people use Facebook every month around the world. That is a tremendous amount of power. It's cultural power. It's intellectual power. It's political power. It's financial power. It's an incredibly important part of people's lives around the world. MARTIN: Well, that - yeah, that kind of leads to the question I was going to conclude with. Is there anything that people can do if they are upset about this? If they are worried about their privacy being invaded or if they don't like the uses to which their - this information has been put, is there anything people can do? Can they. . . VAIDHYANATHAN: Sure. MARTIN: . . . Delete their profiles? VAIDHYANATHAN: Yeah. I mean, we could delete our profiles. A few hundred thousand Americans delete their profiles, doesn't matter because, you know, a few hundred thousand people in India just signed up for Facebook. So that's not going to hurt Facebook. But you should delete your Facebook profile if Facebook puts you in a bad mood, if it distracts you at work, if you're getting into nasty arguments with your uncle. You know, those are reasons to get off of Facebook. But don't get off of Facebook because you think it's going to make a difference to Facebook or to the world. If you have a problem with how Facebook has misled us, abused our privileges, abused our relationship with them, we need to use the instruments of regulation to curb its power. So if we act as citizens, we have a chance. If we act as Facebook users, we have no chance. MARTIN: That's Siva Vaidhyanathan. He is a media studies professor at the University of Virginia. Professor, thanks so much for speaking with us. VAIDHYANATHAN: Thank you, Michel. This was a pleasure. MARTIN: In a statement today, CEO Mark Zuckerberg announced a breach of trust, quote, \"between Facebook and the people who share their data with us and expect us to protect it,\" unquote. MICHEL MARTIN, HOST:  It's not really a surprise that people's personal data on Facebook was gathered and sold and spread around. Most people have known for quite some time now that our information online is far from secure. But the recent news about Cambridge Analytica getting millions of people's information seems to have hit a nerve. We wanted to get some perspective on this, so we've called Siva Vaidhyanathan. He is a media studies professor at the University of Virginia, and he's with us now. Professor, thanks so much for joining us. SIVA VAIDHYANATHAN: Thank you, Michel. It's good to talk to you. MARTIN: Well, this has sparked an outcry and some calls to #DeleteFacebook. Do you think this is a real thing? VAIDHYANATHAN: I think it's a thing among an elite strata of Twitter users. I don't think it's going to turn into a mass movement. Nor do I necessarily think it should. You know, people who study social media, people who do data science, privacy advocates, even the Federal Trade Commission - we've all known about this practice that Facebook has had since about 2010. You know, since about 2010, Facebook has been sort of promiscuously and notoriously duplicitous and irresponsible with our data. Now, people are upset about it, and they should be, but they should have been upset about it then, too. And some of us were. But nobody was paying attention because Facebook was this shiny, happy, new company. MARTIN: As you're suggesting that this has been an open secret for years, is there something about this episode, though, that feels different to you? VAIDHYANATHAN: Oh, it feels different in terms of the public awareness. You know, I think that's a major change. And the fact that there seems to be growing dissatisfaction with Facebook - that's been building since the 2016 election when it was pretty clear that President Trump had exploited Facebook successfully. But that was accompanied by serious propaganda efforts, disinformation and misinformation efforts from certain agents coming out of Russia, right? Facebook is just barely coming to terms with that. But I think there's public awareness that I wish had been around before the damage was done. MARTIN: So you're saying you think there is something about this episode that's hitting a nerve. You know, why now is this the episode that is getting all this attention? VAIDHYANATHAN: Well, I think there's a bit of spy novel (laughter) to this whole latest episode, right? Cambridge Analytica itself is this sort of secretive front company that is controlled by billionaire genius Robert Mercer. And Steve Bannon, you know, was on the board. And the people who run Cambridge Analytica seem to brag about their ability to manipulate minds. The spy novel elements of it are very attractive, right? Here's an actual narrative, an actual story for people to get shocked about. MARTIN: I think people are probably getting the clue right now that you are a bit of a Facebook critic. VAIDHYANATHAN: (Laughter) Yes. MARTIN: You have a book coming out soon called \"Anti-Social Media: How Facebook Disconnects Us And Undermines Democracy. \" But I do want to ask if you think that this issue is unique to Facebook. VAIDHYANATHAN: It's unique to Facebook because Facebook is so pervasive, right? Two-point-two billion people use Facebook every month around the world. That is a tremendous amount of power. It's cultural power. It's intellectual power. It's political power. It's financial power. It's an incredibly important part of people's lives around the world. MARTIN: Well, that - yeah, that kind of leads to the question I was going to conclude with. Is there anything that people can do if they are upset about this? If they are worried about their privacy being invaded or if they don't like the uses to which their - this information has been put, is there anything people can do? Can they. . . VAIDHYANATHAN: Sure. MARTIN: . . . Delete their profiles? VAIDHYANATHAN: Yeah. I mean, we could delete our profiles. A few hundred thousand Americans delete their profiles, doesn't matter because, you know, a few hundred thousand people in India just signed up for Facebook. So that's not going to hurt Facebook. But you should delete your Facebook profile if Facebook puts you in a bad mood, if it distracts you at work, if you're getting into nasty arguments with your uncle. You know, those are reasons to get off of Facebook. But don't get off of Facebook because you think it's going to make a difference to Facebook or to the world. If you have a problem with how Facebook has misled us, abused our privileges, abused our relationship with them, we need to use the instruments of regulation to curb its power. So if we act as citizens, we have a chance. If we act as Facebook users, we have no chance. MARTIN: That's Siva Vaidhyanathan. He is a media studies professor at the University of Virginia. Professor, thanks so much for speaking with us. VAIDHYANATHAN: Thank you, Michel. This was a pleasure. MARTIN: In a statement today, CEO Mark Zuckerberg announced a breach of trust, quote, \"between Facebook and the people who share their data with us and expect us to protect it,\" unquote.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-21-595305722": {"title": "Election Security Hearing Held By Senate Intelligence Committee : NPR", "url": "https://www.npr.org/2018/03/21/595305722/senators-hold-election-security-hearing-after-releasing-recommendations-for-2018", "author": "No author found", "published_date": "2018-03-21", "content": "", "section": "National Security", "disclaimer": ""}, "2018-03-22-596180037": {"title": "Mozilla Announces End To Advertising On Facebook Amid Cambridge Analytica Breach : NPR", "url": "https://www.npr.org/2018/03/22/596180037/mozilla-announces-end-to-advertising-on-facebook-amid-cambridge-analytica-breach", "author": "No author found", "published_date": "2018-03-22", "content": "AUDIE CORNISH, HOST: Facebook CEO Mark Zuckerberg says his company is redoubling efforts to safeguard users' data. He spoke out yesterday for the first time about how Cambridge Analytica, a political data mining firm, got the personal data of millions of Facebook users without their permission. Here he is last night on CNN. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: This is going to be an intensive process, but this is important. I mean, this is something that in retrospect we clearly should have done upfront with Cambridge Analytica. We should not have trusted the certification that they gave us. And we're not going to make that mistake again. CORNISH: Congress is still demanding hearings. Facebook shares are still on a rollercoaster, and at least one advertiser is having second thoughts. Mozilla, the maker of the Firefox browser, says it is pressing pause on its Facebook ads. Mozilla chief marketing officer Jascha Kaykas-Wolff joins us now to explain why. Welcome to the program. JASCHA KAYKAS-WOLFF: Thank you for having me. CORNISH: So on the one hand, you call Mark Zuckerberg's promises encouraging. And on the other hand, you say you want to hit pause on ads. What's going on? KAYKAS-WOLFF: That's right. Well, over the course of this week, Mozilla has done two things. First, we've launched a petition that is rallying a group of people who we represent that use our products and use Facebook as well to encourage Facebook to start to take a look at their default privacy settings. In addition to that, we also, as a marketing organization, have decided to support this petition that we have generally. So as of yesterday, we've paused our advertising on Facebook. CORNISH: So how much does Mozilla spend on advertising through Facebook? I mean, what kind of impact are we talking about here? KAYKAS-WOLFF: So while we make a product, Firefox, that tens and hundreds of millions of people use every day all over the world, we are not the same size as Google and Microsoft and Apple. And our advertising budget is much smaller. That being said, Facebook has made up a large percentage of our advertising spend over the course of the last few years, in the low seven-digit numbers in particular. CORNISH: You've suggested a blanket ban on third parties accessing information of their Facebook friends - right? - of people who use the app. Is anyone else joining you in that call? KAYKAS-WOLFF: Well, the discussion is just beginning, and that's what we're encouraged by. CORNISH: Is that a no? KAYKAS-WOLFF: In fact, we began our. . . CORNISH: (Laughter) Does that usually mean that nobody has called, no one's picked up the phone? KAYKAS-WOLFF: (Laughter) Well, very specifically, we are in discussions with Facebook. CORNISH: What have they told you? KAYKAS-WOLFF: Those are ongoing discussions, and we'll kind of leave those to the teams that are working through the policy and ultimately what's being said publicly. But we're really encouraged that the actions that we've taken have prompted discussions that we think can ultimately benefit end users. CORNISH: What would be a measure of success or moving the ball forward? What do you need to see from Facebook in order to feel like a difference is being made? KAYKAS-WOLFF: We'd like to see public clarity around the way that they're going to handle users' data on their platform in the default settings. So success for us is making sure that users are being - people are being represented well and that their data is being protected first and foremost. CORNISH: It seems like this is part of the business model, right? Essentially, we want to use the service. We give up data. And then people in the industry get to turn around and sell that data. I mean, is there a solution here that doesn't disrupt the business model? KAYKAS-WOLFF: Well, there's a responsibility of technology companies to make sure they represent exactly what is taking place with their data to their users. And it's not completely clear across the board right now. Think of the times that you or I have hit agree on terms of service without having actually read all those details. It's time, we believe, for technology companies to really represent the best interest of the users. It's OK to exchange data, especially if it's a core part of your business model. But it's really important to be upfront about exactly what information is being shared and exactly what you're going to be doing with it. CORNISH: Jascha Kaykas-Wolff is the chief marketing officer for Mozilla. Thank you for speaking with us. KAYKAS-WOLFF: Thanks very much for having me. AUDIE CORNISH, HOST:  Facebook CEO Mark Zuckerberg says his company is redoubling efforts to safeguard users' data. He spoke out yesterday for the first time about how Cambridge Analytica, a political data mining firm, got the personal data of millions of Facebook users without their permission. Here he is last night on CNN. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: This is going to be an intensive process, but this is important. I mean, this is something that in retrospect we clearly should have done upfront with Cambridge Analytica. We should not have trusted the certification that they gave us. And we're not going to make that mistake again. CORNISH: Congress is still demanding hearings. Facebook shares are still on a rollercoaster, and at least one advertiser is having second thoughts. Mozilla, the maker of the Firefox browser, says it is pressing pause on its Facebook ads. Mozilla chief marketing officer Jascha Kaykas-Wolff joins us now to explain why. Welcome to the program. JASCHA KAYKAS-WOLFF: Thank you for having me. CORNISH: So on the one hand, you call Mark Zuckerberg's promises encouraging. And on the other hand, you say you want to hit pause on ads. What's going on? KAYKAS-WOLFF: That's right. Well, over the course of this week, Mozilla has done two things. First, we've launched a petition that is rallying a group of people who we represent that use our products and use Facebook as well to encourage Facebook to start to take a look at their default privacy settings. In addition to that, we also, as a marketing organization, have decided to support this petition that we have generally. So as of yesterday, we've paused our advertising on Facebook. CORNISH: So how much does Mozilla spend on advertising through Facebook? I mean, what kind of impact are we talking about here? KAYKAS-WOLFF: So while we make a product, Firefox, that tens and hundreds of millions of people use every day all over the world, we are not the same size as Google and Microsoft and Apple. And our advertising budget is much smaller. That being said, Facebook has made up a large percentage of our advertising spend over the course of the last few years, in the low seven-digit numbers in particular. CORNISH: You've suggested a blanket ban on third parties accessing information of their Facebook friends - right? - of people who use the app. Is anyone else joining you in that call? KAYKAS-WOLFF: Well, the discussion is just beginning, and that's what we're encouraged by. CORNISH: Is that a no? KAYKAS-WOLFF: In fact, we began our. . . CORNISH: (Laughter) Does that usually mean that nobody has called, no one's picked up the phone? KAYKAS-WOLFF: (Laughter) Well, very specifically, we are in discussions with Facebook. CORNISH: What have they told you? KAYKAS-WOLFF: Those are ongoing discussions, and we'll kind of leave those to the teams that are working through the policy and ultimately what's being said publicly. But we're really encouraged that the actions that we've taken have prompted discussions that we think can ultimately benefit end users. CORNISH: What would be a measure of success or moving the ball forward? What do you need to see from Facebook in order to feel like a difference is being made? KAYKAS-WOLFF: We'd like to see public clarity around the way that they're going to handle users' data on their platform in the default settings. So success for us is making sure that users are being - people are being represented well and that their data is being protected first and foremost. CORNISH: It seems like this is part of the business model, right? Essentially, we want to use the service. We give up data. And then people in the industry get to turn around and sell that data. I mean, is there a solution here that doesn't disrupt the business model? KAYKAS-WOLFF: Well, there's a responsibility of technology companies to make sure they represent exactly what is taking place with their data to their users. And it's not completely clear across the board right now. Think of the times that you or I have hit agree on terms of service without having actually read all those details. It's time, we believe, for technology companies to really represent the best interest of the users. It's OK to exchange data, especially if it's a core part of your business model. But it's really important to be upfront about exactly what information is being shared and exactly what you're going to be doing with it. CORNISH: Jascha Kaykas-Wolff is the chief marketing officer for Mozilla. Thank you for speaking with us. KAYKAS-WOLFF: Thanks very much for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-23-596044821": {"title": "Russia Hacked U.S.Power Grid And Other Critical Infrastructure. So Now What? : NPR", "url": "https://www.npr.org/2018/03/23/596044821/russia-hacked-u-s-power-grid-so-what-will-the-trump-administration-do-about-it", "author": "No author found", "published_date": "2018-03-23", "content": "", "section": "Politics", "disclaimer": ""}, "2018-03-24-596647504": {"title": "'Cow Clicker' Developer: Facebook's Response To Complaints Is Too Late : NPR", "url": "https://www.npr.org/2018/03/24/596647504/cow-clicker-developer-facebooks-response-to-complaints-is-too-late", "author": "No author found", "published_date": "2018-03-24", "content": "SCOTT SIMON, HOST: The revelations that Cambridge Analytica paid for data scraped from millions of users surprised many people - not our next guest. In 2010, Ian Bogost created an app called \"Cow Clicker\" - you clicked on cows. The app harvested troves of data about users. Mr. Bogost is a contributing editor at The Atlantic - still has that information today. Thanks so much for being with us. IAN BOGOST: Thanks for having me. SIMON: So why would you click a cow? BOGOST: (Laughter) It's surprisingly delightful to click on a picture of a cute cow. But the impetus for the app were these social games that were very popular around this time - 2009, 2010 on Facebook - games like like \"FarmVille. \" And that's really where the cow idea came from. SIMON: And what kind of data did you get? BOGOST: So back in this era, Facebook handed off a lot of data by default, whether or not I asked for it or whether or not I made use of it. So that data - the data that Facebook provided to every app at this time included users' names, their profile pictures, their genders, a list of their friends and anything else that they might have shared publicly. And because I needed to store some of it to make the game work, to make it operate I still have all of those unique Facebook ID's for almost 200,000 users without the intention of, you know, selling it to data brokers but rather just because it came along the wire. SIMON: But could you sell it now? BOGOST: It's a great question. One of the things that I speculated about in the article is whether all of this press over the Cambridge Analytica Facebook controversy might be sending developers of old apps that maybe are even defunct back to their databases and kind of asking, well, what data do I have, and what might it be worth? And this idea that, you know, Facebook apps handed over enormous piles of data to millions of apps that were active for years and years. That's something that's not completely captured in the Cambridge Analytica story. SIMON: Doesn't Facebook say, look - there are privacy policies? People get a chance to choose. BOGOST: Facebook tells you that they have provided notice to their application developers and then to the users as they kind of match make them with these applications. And that sounds great. And to some extent, it's true if you read the privacy policies, for one, but then also if you understand when you're moving from Facebook's privacy terms into the privacy policy terms of some third party. I think that's really unclear. It was unclear in 2010 when I was making \"Cow Clicker. \" And it's still somewhat unclear today. So notice itself may not be enough to reasonably expect users to make informed decisions about how their information is being used. SIMON: What do you make of what Mark Zuckerberg or Sheryl Sandberg said this week in response to complaints about Facebook? BOGOST: These statements came very late. And they were very vague. One of the statements that Zuckerberg made and a promise that Facebook has agreed to perform as a result of it is to, you know - to look at what data got out and to audit companies who may have used that data improperly. The fact is, as I've shown, with this silly example of \"Cow Clicker,\" it's too late. The data is out, and you kind of can't put the genie back in the bottle anymore. SIMON: Ian Bogost is a contributing editor at \"The Atlantic. \" Thanks very much for being with us. BOGOST: Thanks so much. SIMON: And we know that NPR is also a Facebook app developer. Our listeners use their Facebook logins to access NPR One and our website. NPR can then see information that users choose to provide in their public profile - may include names, email addresses, gender, age range. And some of that information is stored in NPR's databases. Like several other media companies, NPR received payment from Facebook to experiment with Facebook Live video content in 2016. SCOTT SIMON, HOST:  The revelations that Cambridge Analytica paid for data scraped from millions of users surprised many people - not our next guest. In 2010, Ian Bogost created an app called \"Cow Clicker\" - you clicked on cows. The app harvested troves of data about users. Mr. Bogost is a contributing editor at The Atlantic - still has that information today. Thanks so much for being with us. IAN BOGOST: Thanks for having me. SIMON: So why would you click a cow? BOGOST: (Laughter) It's surprisingly delightful to click on a picture of a cute cow. But the impetus for the app were these social games that were very popular around this time - 2009, 2010 on Facebook - games like like \"FarmVille. \" And that's really where the cow idea came from. SIMON: And what kind of data did you get? BOGOST: So back in this era, Facebook handed off a lot of data by default, whether or not I asked for it or whether or not I made use of it. So that data - the data that Facebook provided to every app at this time included users' names, their profile pictures, their genders, a list of their friends and anything else that they might have shared publicly. And because I needed to store some of it to make the game work, to make it operate I still have all of those unique Facebook ID's for almost 200,000 users without the intention of, you know, selling it to data brokers but rather just because it came along the wire. SIMON: But could you sell it now? BOGOST: It's a great question. One of the things that I speculated about in the article is whether all of this press over the Cambridge Analytica Facebook controversy might be sending developers of old apps that maybe are even defunct back to their databases and kind of asking, well, what data do I have, and what might it be worth? And this idea that, you know, Facebook apps handed over enormous piles of data to millions of apps that were active for years and years. That's something that's not completely captured in the Cambridge Analytica story. SIMON: Doesn't Facebook say, look - there are privacy policies? People get a chance to choose. BOGOST: Facebook tells you that they have provided notice to their application developers and then to the users as they kind of match make them with these applications. And that sounds great. And to some extent, it's true if you read the privacy policies, for one, but then also if you understand when you're moving from Facebook's privacy terms into the privacy policy terms of some third party. I think that's really unclear. It was unclear in 2010 when I was making \"Cow Clicker. \" And it's still somewhat unclear today. So notice itself may not be enough to reasonably expect users to make informed decisions about how their information is being used. SIMON: What do you make of what Mark Zuckerberg or Sheryl Sandberg said this week in response to complaints about Facebook? BOGOST: These statements came very late. And they were very vague. One of the statements that Zuckerberg made and a promise that Facebook has agreed to perform as a result of it is to, you know - to look at what data got out and to audit companies who may have used that data improperly. The fact is, as I've shown, with this silly example of \"Cow Clicker,\" it's too late. The data is out, and you kind of can't put the genie back in the bottle anymore. SIMON: Ian Bogost is a contributing editor at \"The Atlantic. \" Thanks very much for being with us. BOGOST: Thanks so much. SIMON: And we know that NPR is also a Facebook app developer. Our listeners use their Facebook logins to access NPR One and our website. NPR can then see information that users choose to provide in their public profile - may include names, email addresses, gender, age range. And some of that information is stored in NPR's databases. Like several other media companies, NPR received payment from Facebook to experiment with Facebook Live video content in 2016.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-25-596805347": {"title": "How Does Cambridge Analytica Flap Compare With Obama's Campaign Tactics? : NPR", "url": "https://www.npr.org/2018/03/25/596805347/how-does-cambridge-analytica-flap-compare-with-obama-s-campaign-tactics", "author": "No author found", "published_date": "2018-03-25", "content": "LULU GARCIA-NAVARRO, HOST: The data company Cambridge Analytica stands accused of gathering people's Facebook data through misrepresenting what it was for and, in the case of millions of users, without their consent. The purpose was to influence their vote in favor of Donald Trump in the 2016 election. Now, conservatives have drawn parallels to the data operation of the Obama campaign in 2012. And to talk about that, we're joined now by Betsy Hoover. She was the online organizing director for Barack Obama's 2012 presidential campaign. Welcome to the program. BETSY HOOVER: Thanks for having me. Happy to be here. GARCIA-NAVARRO: So take us back to the 2012 campaign. Facebook was central to your digital strategy. Briefly tell us how you gathered data from Facebook and what you did with it. HOOVER: Yeah. So it's important to remember that, you know, Facebook and technology has changed a ton over the past 10 years. So in 2008, Facebook was one-tenth the size of what it was in 2012, for example. But Facebook was a huge part of our digital strategy. We had a, you know, massive number of people following Barack Obama on Facebook. We also, though, knew that the vast majority of voters, something like 97 percent of the U. S. electorate, could be reached if everyone who was following Barack Obama on Facebook shared a piece of content. GARCIA-NAVARRO: Wow. Ninety-seven percent? HOOVER: Yeah. And so that's a super powerful tool. GARCIA-NAVARRO: So conservatives say this about your strategy. And this was written in the National Review. The Obama team created social interactions you wouldn't have otherwise had. So you did get access through Facebook, apparently, of the friends of people who logged onto your site without their consent. And you used that information, right? So what's different than what Cambridge Analytica did? HOOVER: Well, their statement in the National Review is factually inaccurate. So the app that everyone's referring to in this moment was an app called Targeted Sharing. It was an app that we created on Facebook that fully followed Facebook's terms of service. And any individual could decide to use the app. When they clicked on the app, a screen would pop up that would say what data they're authorizing the app was giving us access to and exactly how we were going to use that data. And so at that time, it was totally legitimate on Facebook to say you're giving us access to your social network. You're giving us access to your friends on Facebook. GARCIA-NAVARRO: We should say that Facebook changed its terms of service, so that is no longer the case. But back then, it was. HOOVER: Back then, it was. It was totally legal and something that we were allowed to do. But we were really minimalist in how we used that data. So, you know, we got your list of friends. And then we matched it to our model, our list of voters that we didn't build with Facebook data. We built with voter history and, you know, all of the other data points that Democratic campaigns use to build models. But we matched the data of your friends to that model and then reflected it back to the person who had authorized the app and said, if you want to reach out to your friends about this election on Facebook, here are the ones that you should reach out to first. And that was it. . . GARCIA-NAVARRO: So you gave them names of their friends that might be persuadable? HOOVER: Yeah. We basically just ordered the list that they gave us, and we didn't use that data for any other purpose. So the difference between that and what Cambridge Analytica did in my mind is that it's a totally different ballpark. We had consent. It was within Facebook's terms of service, and we were very minimalist with how we used that data. GARCIA-NAVARRO: Cambridge Analytica was trying to build these psychographic profiles and use that to microtarget people with ads that would push them in a certain direction. If we set aside the alleged unethical ways in which Cambridge Analytica obtained their data, what would you say about how they used their data? It seems to be where campaigns are headed. HOOVER: Yeah. I mean, honestly, that doesn't concern me. And I know that people have different feelings on this. But the goal of campaigns is to help voters understand your candidate and what your candidate represents in a way that resonates with the voter. And so, you know, targeting messages based on what the campaign knows about voting segments feels smart to me. And it's not different at all from what companies are doing when they're marketing products to you. And, yeah, it is something that we should be exploring on both sides and figuring out if that can help us, you know, run campaigns that are authentically meeting voters where they are. GARCIA-NAVARRO: Do you think the average Internet user actually understands what's happening to their data? HOOVER: I think we're getting smarter about it. And that's part of what this debate is about. But a lot of times, the answer is no. You can give your data away on the Internet, and companies and campaigns and lots of organizations are going to use it in a variety of ways. And so. . . GARCIA-NAVARRO: So they benefit from that ignorance. HOOVER: Yeah. A little bit. And so I think that as - like, for us, as Americans, like, my message to the public is, like, we should be smarter about how we're using the Internet and where our data's going. And, you know, some of my data I give away. That's fine. I would rather see products that I like than products I don't like. Or I'd rather see candidates that I'm more likely to vote for than not. That's OK. But it is important that you know where your data's going, and that's part of being, I think, educated Internet users. GARCIA-NAVARRO: Should it be legislated? HOOVER: My sense is a lot of this should be legislated. Like, I think it's hard to say to Facebook or Google or Amazon or the Obama campaign or the Trump campaign, it is your responsibility to decide what data is good to have access to and what isn't. Like, technology is a huge part of the way that people are engaging in our public space. And we should have laws that legislate pieces of that because this is the space that is playing a huge role in how people interact with society. GARCIA-NAVARRO: Betsy Hoover worked on the Obama campaigns in 2008 and 2012. And she's a founding partner of the political consulting firm 270 Strategies and a founder of Higher Ground Labs. Thank you so much. HOOVER: Thank you. LULU GARCIA-NAVARRO, HOST:  The data company Cambridge Analytica stands accused of gathering people's Facebook data through misrepresenting what it was for and, in the case of millions of users, without their consent. The purpose was to influence their vote in favor of Donald Trump in the 2016 election. Now, conservatives have drawn parallels to the data operation of the Obama campaign in 2012. And to talk about that, we're joined now by Betsy Hoover. She was the online organizing director for Barack Obama's 2012 presidential campaign. Welcome to the program. BETSY HOOVER: Thanks for having me. Happy to be here. GARCIA-NAVARRO: So take us back to the 2012 campaign. Facebook was central to your digital strategy. Briefly tell us how you gathered data from Facebook and what you did with it. HOOVER: Yeah. So it's important to remember that, you know, Facebook and technology has changed a ton over the past 10 years. So in 2008, Facebook was one-tenth the size of what it was in 2012, for example. But Facebook was a huge part of our digital strategy. We had a, you know, massive number of people following Barack Obama on Facebook. We also, though, knew that the vast majority of voters, something like 97 percent of the U. S. electorate, could be reached if everyone who was following Barack Obama on Facebook shared a piece of content. GARCIA-NAVARRO: Wow. Ninety-seven percent? HOOVER: Yeah. And so that's a super powerful tool. GARCIA-NAVARRO: So conservatives say this about your strategy. And this was written in the National Review. The Obama team created social interactions you wouldn't have otherwise had. So you did get access through Facebook, apparently, of the friends of people who logged onto your site without their consent. And you used that information, right? So what's different than what Cambridge Analytica did? HOOVER: Well, their statement in the National Review is factually inaccurate. So the app that everyone's referring to in this moment was an app called Targeted Sharing. It was an app that we created on Facebook that fully followed Facebook's terms of service. And any individual could decide to use the app. When they clicked on the app, a screen would pop up that would say what data they're authorizing the app was giving us access to and exactly how we were going to use that data. And so at that time, it was totally legitimate on Facebook to say you're giving us access to your social network. You're giving us access to your friends on Facebook. GARCIA-NAVARRO: We should say that Facebook changed its terms of service, so that is no longer the case. But back then, it was. HOOVER: Back then, it was. It was totally legal and something that we were allowed to do. But we were really minimalist in how we used that data. So, you know, we got your list of friends. And then we matched it to our model, our list of voters that we didn't build with Facebook data. We built with voter history and, you know, all of the other data points that Democratic campaigns use to build models. But we matched the data of your friends to that model and then reflected it back to the person who had authorized the app and said, if you want to reach out to your friends about this election on Facebook, here are the ones that you should reach out to first. And that was it. . . GARCIA-NAVARRO: So you gave them names of their friends that might be persuadable? HOOVER: Yeah. We basically just ordered the list that they gave us, and we didn't use that data for any other purpose. So the difference between that and what Cambridge Analytica did in my mind is that it's a totally different ballpark. We had consent. It was within Facebook's terms of service, and we were very minimalist with how we used that data. GARCIA-NAVARRO: Cambridge Analytica was trying to build these psychographic profiles and use that to microtarget people with ads that would push them in a certain direction. If we set aside the alleged unethical ways in which Cambridge Analytica obtained their data, what would you say about how they used their data? It seems to be where campaigns are headed. HOOVER: Yeah. I mean, honestly, that doesn't concern me. And I know that people have different feelings on this. But the goal of campaigns is to help voters understand your candidate and what your candidate represents in a way that resonates with the voter. And so, you know, targeting messages based on what the campaign knows about voting segments feels smart to me. And it's not different at all from what companies are doing when they're marketing products to you. And, yeah, it is something that we should be exploring on both sides and figuring out if that can help us, you know, run campaigns that are authentically meeting voters where they are. GARCIA-NAVARRO: Do you think the average Internet user actually understands what's happening to their data? HOOVER: I think we're getting smarter about it. And that's part of what this debate is about. But a lot of times, the answer is no. You can give your data away on the Internet, and companies and campaigns and lots of organizations are going to use it in a variety of ways. And so. . . GARCIA-NAVARRO: So they benefit from that ignorance. HOOVER: Yeah. A little bit. And so I think that as - like, for us, as Americans, like, my message to the public is, like, we should be smarter about how we're using the Internet and where our data's going. And, you know, some of my data I give away. That's fine. I would rather see products that I like than products I don't like. Or I'd rather see candidates that I'm more likely to vote for than not. That's OK. But it is important that you know where your data's going, and that's part of being, I think, educated Internet users. GARCIA-NAVARRO: Should it be legislated? HOOVER: My sense is a lot of this should be legislated. Like, I think it's hard to say to Facebook or Google or Amazon or the Obama campaign or the Trump campaign, it is your responsibility to decide what data is good to have access to and what isn't. Like, technology is a huge part of the way that people are engaging in our public space. And we should have laws that legislate pieces of that because this is the space that is playing a huge role in how people interact with society. GARCIA-NAVARRO: Betsy Hoover worked on the Obama campaigns in 2008 and 2012. And she's a founding partner of the political consulting firm 270 Strategies and a founder of Higher Ground Labs. Thank you so much. HOOVER: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-26-597100707": {"title": "How Facebook Users Are Responding To The Cambridge Analytica Scandal : NPR", "url": "https://www.npr.org/2018/03/26/597100707/how-facebook-users-are-responding-to-the-cambridge-analytica-scandal", "author": "No author found", "published_date": "2018-03-26", "content": "AILSA CHANG, HOST: On this week's All Tech Considered, what are you doing about your Facebook account after the Cambridge Analytica revelations? (SOUNDBITE OF MUSIC)CHANG: After learning that the political firm Cambridge Analytica improperly got 50 million Facebook profiles, lots of people wrote in to NPR with their questions. Here's a sample of what we've heard. JONI SHERMAN: My name is Joni Sherman (ph). I'm from Orlando, Fla. I'm wondering how in the heck this went on for so long without bells and whistles going on in their shop. And then I also wonder if I'm one of the 50 million Facebook users who's information was breached because I'd like to be able to react to that with my friends and such. STEVE BEGAY: My name's Steve Begay (ph). I live in Rochester, N. Y. I'd like to know, from Facebook, where is my data. Who has taken it? Do they have an audit trail for what apps or other things have scraped my data? GABRIELA SILK: My name is Gabriela Silk (ph). And I live in Longmont, Colo. Does Facebook even have the ability to tackle the enormity of this issue? And if they don't, what should I and other Facebook users be doing in the meantime? CHANG: We're going to put some of those questions to Manoush Zomorodi now. She hosts WNYC's podcast Note To Self. She's been reporting on Cambridge Analytica and Facebook for a while now. And she's taking some steps to protect her own data. Manoush, welcome. MANOUSH ZOMORODI, BYLINE: Hello, Ailsa. CHANG: Currently, how do we know if, say, any of us are one of the 50 million people? ZOMORODI: We don't yet. Mark Zuckerberg came out last week and said that Facebook will alert the 50 million people whose data were taken by Cambridge Analytica. But we haven't heard from him yet. But I think for now we should just all assume that our data has been taken. . . CHANG: Yeah. ZOMORODI: . . . Because even if our data didn't go to Cambridge Analytica, who knows the other companies that may have had access to it? CHANG: Is there a way for someone to find out all of the data that Facebook has on us? ZOMORODI: Yes. On Facebook, if you go into settings, there is a way to download a copy of your information. It could include all the contacts in your address book, maybe your calendar. There are even some people saying that they have seen that it has metadata from cellphone calls that they made. That is because when you put Facebook on your phone, or you signed up for a new account, it said, would you like to connect with all your contacts? And if you said yes, it hasn't been made clear just how deeply it has gotten into your personal data. But you can download a zip file for yourself. CHANG: So I heard that you went about trying to figure out what data Cambridge Analytica might have gotten on you. ZOMORODI: Yeah, I was really curious. I was like, well, if there's a psychographic profile of me out there, I want to see it. I went to datarequest. cambridgeanalytica. org last year, and I put in a request. I had to give two forms of ID. I had to pay a 10-pound fee. And so I got back a file. It was not the 5,000 data points that Cambridge Analytica claims to have on the majority of U. S. voters. But it did have the GPS points of my home. CHANG: Wow. ZOMORODI: It had my voting history - days that I've gone to vote in elections. The point of all of this is though our personal data is floating around the world. We don't know who has it and where there have been no safeguards - particularly here in the U. S. CHANG: OK. So Facebook says it is going to be rolling out new privacy settings. But does it actually have the ability to tackle this issue? I mean, does any social media site have the ability to protect your privacy? ZOMORODI: Well, I think to protect our privacy, they have to change the very business models that they are built on. There's the old saying, which is if the product is free, then you are the product. In this case, why don't we have to pay for Facebook or Twitter or Instagram or any of those - because we are giving them so much data. And the data is how they are making so much money off of advertising. So in order for us to really tackle this problem, these companies either have to be nonprofits, like the encryption-based texting service signal. Or we'd have to be willing to pay for them, which, of course, means that it would be only people who could afford to pay for closed social networks, which, of course, defeats the very purpose of these social networks. It's a crucial existential question for Silicon Valley, absolutely. CHANG: Manoush Zomorodi is the host of WNYC's podcast Note To Self. Thank you very much for joining us. ZOMORODI: Oh, Ailsa, it was great to be here. AILSA CHANG, HOST:  On this week's All Tech Considered, what are you doing about your Facebook account after the Cambridge Analytica revelations? (SOUNDBITE OF MUSIC) CHANG: After learning that the political firm Cambridge Analytica improperly got 50 million Facebook profiles, lots of people wrote in to NPR with their questions. Here's a sample of what we've heard. JONI SHERMAN: My name is Joni Sherman (ph). I'm from Orlando, Fla. I'm wondering how in the heck this went on for so long without bells and whistles going on in their shop. And then I also wonder if I'm one of the 50 million Facebook users who's information was breached because I'd like to be able to react to that with my friends and such. STEVE BEGAY: My name's Steve Begay (ph). I live in Rochester, N. Y. I'd like to know, from Facebook, where is my data. Who has taken it? Do they have an audit trail for what apps or other things have scraped my data? GABRIELA SILK: My name is Gabriela Silk (ph). And I live in Longmont, Colo. Does Facebook even have the ability to tackle the enormity of this issue? And if they don't, what should I and other Facebook users be doing in the meantime? CHANG: We're going to put some of those questions to Manoush Zomorodi now. She hosts WNYC's podcast Note To Self. She's been reporting on Cambridge Analytica and Facebook for a while now. And she's taking some steps to protect her own data. Manoush, welcome. MANOUSH ZOMORODI, BYLINE: Hello, Ailsa. CHANG: Currently, how do we know if, say, any of us are one of the 50 million people? ZOMORODI: We don't yet. Mark Zuckerberg came out last week and said that Facebook will alert the 50 million people whose data were taken by Cambridge Analytica. But we haven't heard from him yet. But I think for now we should just all assume that our data has been taken. . . CHANG: Yeah. ZOMORODI: . . . Because even if our data didn't go to Cambridge Analytica, who knows the other companies that may have had access to it? CHANG: Is there a way for someone to find out all of the data that Facebook has on us? ZOMORODI: Yes. On Facebook, if you go into settings, there is a way to download a copy of your information. It could include all the contacts in your address book, maybe your calendar. There are even some people saying that they have seen that it has metadata from cellphone calls that they made. That is because when you put Facebook on your phone, or you signed up for a new account, it said, would you like to connect with all your contacts? And if you said yes, it hasn't been made clear just how deeply it has gotten into your personal data. But you can download a zip file for yourself. CHANG: So I heard that you went about trying to figure out what data Cambridge Analytica might have gotten on you. ZOMORODI: Yeah, I was really curious. I was like, well, if there's a psychographic profile of me out there, I want to see it. I went to datarequest. cambridgeanalytica. org last year, and I put in a request. I had to give two forms of ID. I had to pay a 10-pound fee. And so I got back a file. It was not the 5,000 data points that Cambridge Analytica claims to have on the majority of U. S. voters. But it did have the GPS points of my home. CHANG: Wow. ZOMORODI: It had my voting history - days that I've gone to vote in elections. The point of all of this is though our personal data is floating around the world. We don't know who has it and where there have been no safeguards - particularly here in the U. S. CHANG: OK. So Facebook says it is going to be rolling out new privacy settings. But does it actually have the ability to tackle this issue? I mean, does any social media site have the ability to protect your privacy? ZOMORODI: Well, I think to protect our privacy, they have to change the very business models that they are built on. There's the old saying, which is if the product is free, then you are the product. In this case, why don't we have to pay for Facebook or Twitter or Instagram or any of those - because we are giving them so much data. And the data is how they are making so much money off of advertising. So in order for us to really tackle this problem, these companies either have to be nonprofits, like the encryption-based texting service signal. Or we'd have to be willing to pay for them, which, of course, means that it would be only people who could afford to pay for closed social networks, which, of course, defeats the very purpose of these social networks. It's a crucial existential question for Silicon Valley, absolutely. CHANG: Manoush Zomorodi is the host of WNYC's podcast Note To Self. Thank you very much for joining us. ZOMORODI: Oh, Ailsa, it was great to be here.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-27-597390569": {"title": "FTC Investigating Whether Facebook Violated Consent Decree : NPR", "url": "https://www.npr.org/2018/03/27/597390569/ftc-investigating-whether-facebook-violated-consent-decree", "author": "No author found", "published_date": "2018-03-27", "content": "AILSA CHANG, HOST: Things are not getting any easier for Facebook as the tech giant continues to face questions about how the data of 50 million users got into unauthorized hands. Congress wants Facebook CEO Mark Zuckerberg himself to testify while the Federal Trade Commission continues to investigate whether the company violated a 2011 consent decree. Demands for the government to do something to protect user privacy raise the question, what might effective regulation of Facebook even look like? With us now to explore that question is Jessica Rich, the former head of the FTC's Bureau of Consumer Protection. Welcome. JESSICA RICH: Hello, Ailsa. CHANG: Jessica, you helped shape the FTC consent decree back in 2011, right? RICH: I did. CHANG: What went through your mind when you were first hearing these stories about what happened with Cambridge Analytica and Facebook? RICH: Well, like many people, my reaction was are you kidding? The facts here of allowing third parties to have unfettered access to user data and not exercising the kind of care for Facebook users that they should were the exact same facts that drove us to take action against them in 2011 and that led to the order they're now under. CHANG: And can you just remind us what those facts were that led to the 2011 order? RICH: Well, it was pretty similar to the facts here. It was all about sharing data contrary to user expectations and preferences. They overrode consumers' preferences to make private information public, including your friends lists. They allowed third-party apps to access virtually everything. They claim to verify the security of third-party apps and they didn't. And they said they didn't share information with advertisers when they did. CHANG: It's eerily familiar. RICH: Eerily familiar. It's all about allowing third parties unrestricted access to user data contrary to user preferences and expectations. CHANG: And the penalties for violating that 2011 consent decree, they're huge. It's $40,000. . . RICH: Per violation. CHANG: So if you multiply that across 50 million users, we're talking about billions of dollars, potentially, that Facebook faces in fines. Why would a number like that, billions of dollars of potential fines, not serve as enough of a deterrent for a company like Facebook? RICH: I really can't answer that question. It must be lack of proper compliance procedures or literally a culture that is not one that really cares about its users. CHANG: Do you think the FTC is well-equipped to regulate social media companies like Facebook and push them to better protect user privacy? RICH: I think the FTC is very well-equipped to do enforcement on a case-by-case basis, which is what it did here. And I have every confidence that the enforcement division that oversees order compliance will get to the bottom of this. CHANG: But is the FTC effectively enforcing if Facebook did indeed violate a consent decree from seven years ago? RICH: The FTC can't be expected to know every detail of companies' actions all along the way when it is monitoring an order. If the FTC takes action here against Facebook for violating the order, it will be enforcing the order now that it has these facts in hand. And if it assesses huge penalties against Facebook, it will have made an example of Facebook. It will deter Facebook hopefully in the future. And it will be an effective action. What the FTC can't do is police the entire tech marketplace for violations. It does not have the resources to do that. CHANG: OK. So because it doesn't have the resources to do that, what more broadly should be done? RICH: What I would propose would be simple standardize information about company practices that allow consumers to easily compare companies. There needs to be a requirement that companies secure the data they collect. And there needs to be a strong enforcer and strong penalties for violations. CHANG: Jessica Rich is the former head of the FTC's Bureau of Consumer Protection. She's now a vice president of advocacy at Consumer Reports. Thanks very much for joining us. RICH: Thanks for having me, Ailsa. AILSA CHANG, HOST:  Things are not getting any easier for Facebook as the tech giant continues to face questions about how the data of 50 million users got into unauthorized hands. Congress wants Facebook CEO Mark Zuckerberg himself to testify while the Federal Trade Commission continues to investigate whether the company violated a 2011 consent decree. Demands for the government to do something to protect user privacy raise the question, what might effective regulation of Facebook even look like? With us now to explore that question is Jessica Rich, the former head of the FTC's Bureau of Consumer Protection. Welcome. JESSICA RICH: Hello, Ailsa. CHANG: Jessica, you helped shape the FTC consent decree back in 2011, right? RICH: I did. CHANG: What went through your mind when you were first hearing these stories about what happened with Cambridge Analytica and Facebook? RICH: Well, like many people, my reaction was are you kidding? The facts here of allowing third parties to have unfettered access to user data and not exercising the kind of care for Facebook users that they should were the exact same facts that drove us to take action against them in 2011 and that led to the order they're now under. CHANG: And can you just remind us what those facts were that led to the 2011 order? RICH: Well, it was pretty similar to the facts here. It was all about sharing data contrary to user expectations and preferences. They overrode consumers' preferences to make private information public, including your friends lists. They allowed third-party apps to access virtually everything. They claim to verify the security of third-party apps and they didn't. And they said they didn't share information with advertisers when they did. CHANG: It's eerily familiar. RICH: Eerily familiar. It's all about allowing third parties unrestricted access to user data contrary to user preferences and expectations. CHANG: And the penalties for violating that 2011 consent decree, they're huge. It's $40,000. . . RICH: Per violation. CHANG: So if you multiply that across 50 million users, we're talking about billions of dollars, potentially, that Facebook faces in fines. Why would a number like that, billions of dollars of potential fines, not serve as enough of a deterrent for a company like Facebook? RICH: I really can't answer that question. It must be lack of proper compliance procedures or literally a culture that is not one that really cares about its users. CHANG: Do you think the FTC is well-equipped to regulate social media companies like Facebook and push them to better protect user privacy? RICH: I think the FTC is very well-equipped to do enforcement on a case-by-case basis, which is what it did here. And I have every confidence that the enforcement division that oversees order compliance will get to the bottom of this. CHANG: But is the FTC effectively enforcing if Facebook did indeed violate a consent decree from seven years ago? RICH: The FTC can't be expected to know every detail of companies' actions all along the way when it is monitoring an order. If the FTC takes action here against Facebook for violating the order, it will be enforcing the order now that it has these facts in hand. And if it assesses huge penalties against Facebook, it will have made an example of Facebook. It will deter Facebook hopefully in the future. And it will be an effective action. What the FTC can't do is police the entire tech marketplace for violations. It does not have the resources to do that. CHANG: OK. So because it doesn't have the resources to do that, what more broadly should be done? RICH: What I would propose would be simple standardize information about company practices that allow consumers to easily compare companies. There needs to be a requirement that companies secure the data they collect. And there needs to be a strong enforcer and strong penalties for violations. CHANG: Jessica Rich is the former head of the FTC's Bureau of Consumer Protection. She's now a vice president of advocacy at Consumer Reports. Thanks very much for joining us. RICH: Thanks for having me, Ailsa.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-27-597390633": {"title": "European Commission Outlines Plan To Change How Internet Companies Are Taxed : NPR", "url": "https://www.npr.org/2018/03/27/597390633/european-commission-outlines-plan-to-change-how-internet-companies-are-taxed", "author": "No author found", "published_date": "2018-03-27", "content": "ARI SHAPIRO, HOST: The European Commission has outlined a plan to change the system of taxation for large Internet companies. The argument is that huge digital companies like Google and Amazon have been paying an unfair, low tax rate in Europe. Most of the biggest firms are American, and they say they're being unfairly targeted at a time when transatlantic tensions are already strained. NPR's Eleanor Beardsley reports from Paris. ELEANOR BEARDSLEY, BYLINE: With 500 million well-connected consumers, the European Union is a lucrative market place for Internet companies. But European leaders say large digital firms like Google, Amazon, Facebook and Apple are not paying their share of taxes. In a recent interview with NPR, French Finance Minister Bruno Le Maire said that has to change. (SOUNDBITE OF ARCHIVED BROADCAST)BRUNO LE MAIRE: We want to ensure fair taxation for everybody. I'm asking the small and medium-sized French companies to pay their due taxes either in France or in Europe. I just want to ask the same to the Internet giants. They have to pay their taxes like any other private company. BEARDSLEY: The EU complains that foreign Internet companies do most of their business in large countries like Germany, France and Britain, yet locate their headquarters in small, low-tax countries such as Ireland and Luxembourg. Peter Chase studies at the German Marshall Fund in Brussels. He says the EU is looking to change the basis of corporate taxation. PETER CHASE: They want to say that if a company does a lot of digital business, if it works like a marketplace to bring people together and then gets a fee for that, if they do all that business in their territory, then that should be enough to say they have a significant digital presence. BEARDSLEY: And so that's where the EU thinks companies should pay their taxes, says Chase. (SOUNDBITE OF ARCHIVED RECORDING)PIERRE MOSCOVICI: (Speaking French). BEARDSLEY: Last week, Pierre Moscovici, the European commissioner in charge of taxation, said in the last 20 years, the digital revolution has up-ended economies and changes the way companies create value. He told journalists tax law needs to keep up with those changes. (SOUNDBITE OF ARCHIVED RECORDING)MOSCOVICI: That those companies pay in the EU something like 9 percent whilst the rest of the economy pays 23 percent. And people don't accept that. That's why we are acting now decisively. BEARDSLEY: Moscovici says until the EU can make permanent changes to the way Internet companies are taxed, it will create a temporary solution, a 3 percent tax on digital company revenues, which will raise about 5 billion euros a year. (SOUNDBITE OF ARCHIVED RECORDING)MOSCOVICI: This interim solution will focus on turnover from activities where user participation plays a central role in value creation, meaning most of the digital services we use in our daily life. BEARDSLEY: Moscovici said in order to avoid damaging startups, the measure will concentrate on large Internet firms. But he insisted the EU is not targeting American companies. (SOUNDBITE OF ARCHIVED RECORDING)MOSCOVICI: This is not an anti-American approach. It is not an anti-American tax. BEARDSLEY: Moscovici said about 150 companies will be affected by this tax and only half are American. Not all EU members are likely to be on board in changing the basis of corporate taxation, says Chase. CHASE: Small countries that are really adept at technologies, these are the ones that are going to have a problem with the redistribution of what should be their government revenues to big countries like Germany. BEARDSLEY: With so many conflicting points of view, Chase says it might take a long time to get approval from all 27 EU member states. But in the meantime, he says, the debate over digital taxation in Europe is straining transatlantic trade relations. Eleanor Beardsley, NPR News, Paris. (SOUNDBITE OF REKI'S \"SANCTUM\") ARI SHAPIRO, HOST:  The European Commission has outlined a plan to change the system of taxation for large Internet companies. The argument is that huge digital companies like Google and Amazon have been paying an unfair, low tax rate in Europe. Most of the biggest firms are American, and they say they're being unfairly targeted at a time when transatlantic tensions are already strained. NPR's Eleanor Beardsley reports from Paris. ELEANOR BEARDSLEY, BYLINE: With 500 million well-connected consumers, the European Union is a lucrative market place for Internet companies. But European leaders say large digital firms like Google, Amazon, Facebook and Apple are not paying their share of taxes. In a recent interview with NPR, French Finance Minister Bruno Le Maire said that has to change. (SOUNDBITE OF ARCHIVED BROADCAST) BRUNO LE MAIRE: We want to ensure fair taxation for everybody. I'm asking the small and medium-sized French companies to pay their due taxes either in France or in Europe. I just want to ask the same to the Internet giants. They have to pay their taxes like any other private company. BEARDSLEY: The EU complains that foreign Internet companies do most of their business in large countries like Germany, France and Britain, yet locate their headquarters in small, low-tax countries such as Ireland and Luxembourg. Peter Chase studies at the German Marshall Fund in Brussels. He says the EU is looking to change the basis of corporate taxation. PETER CHASE: They want to say that if a company does a lot of digital business, if it works like a marketplace to bring people together and then gets a fee for that, if they do all that business in their territory, then that should be enough to say they have a significant digital presence. BEARDSLEY: And so that's where the EU thinks companies should pay their taxes, says Chase. (SOUNDBITE OF ARCHIVED RECORDING) PIERRE MOSCOVICI: (Speaking French). BEARDSLEY: Last week, Pierre Moscovici, the European commissioner in charge of taxation, said in the last 20 years, the digital revolution has up-ended economies and changes the way companies create value. He told journalists tax law needs to keep up with those changes. (SOUNDBITE OF ARCHIVED RECORDING) MOSCOVICI: That those companies pay in the EU something like 9 percent whilst the rest of the economy pays 23 percent. And people don't accept that. That's why we are acting now decisively. BEARDSLEY: Moscovici says until the EU can make permanent changes to the way Internet companies are taxed, it will create a temporary solution, a 3 percent tax on digital company revenues, which will raise about 5 billion euros a year. (SOUNDBITE OF ARCHIVED RECORDING) MOSCOVICI: This interim solution will focus on turnover from activities where user participation plays a central role in value creation, meaning most of the digital services we use in our daily life. BEARDSLEY: Moscovici said in order to avoid damaging startups, the measure will concentrate on large Internet firms. But he insisted the EU is not targeting American companies. (SOUNDBITE OF ARCHIVED RECORDING) MOSCOVICI: This is not an anti-American approach. It is not an anti-American tax. BEARDSLEY: Moscovici said about 150 companies will be affected by this tax and only half are American. Not all EU members are likely to be on board in changing the basis of corporate taxation, says Chase. CHASE: Small countries that are really adept at technologies, these are the ones that are going to have a problem with the redistribution of what should be their government revenues to big countries like Germany. BEARDSLEY: With so many conflicting points of view, Chase says it might take a long time to get approval from all 27 EU member states. But in the meantime, he says, the debate over digital taxation in Europe is straining transatlantic trade relations. Eleanor Beardsley, NPR News, Paris. (SOUNDBITE OF REKI'S \"SANCTUM\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-03-27-597021235": {"title": "Tumblr's Ban Of Russian Accounts Adds Detail To Targeting Of Black Americans : NPR", "url": "https://www.npr.org/2018/03/27/597021235/tumblrs-ban-of-russian-accounts-adds-detail-to-targeting-of-black-americans", "author": "No author found", "published_date": "2018-03-27", "content": "", "section": "National Security", "disclaimer": ""}, "2018-03-28-597444394": {"title": "CLOUD Act, Tucked Into Omnibus, Likely To Derail Supreme Court Tech Privacy Case : NPR", "url": "https://www.npr.org/2018/03/28/597444394/a-needle-in-a-legal-haystack-could-sink-a-major-supreme-court-privacy-case", "author": "No author found", "published_date": "2018-03-28", "content": "", "section": "Law", "disclaimer": ""}, "2018-04-01-598394986": {"title": "Facebook Complaints Are A New Kind Of 911 Call In The Platform's Hometown : NPR", "url": "https://www.npr.org/2018/04/01/598394986/facebook-complaints-are-a-new-kind-of-911-call-in-the-platforms-hometown", "author": "No author found", "published_date": "2018-04-01", "content": "KORVA COLEMAN, HOST: Menlo Park, Calif. , was a sleepy bedroom community until Facebook came to town. One unexpected thing Facebook has brought to Menlo Park is complaints from all over the world about Facebook. As Jake Warga reports, those complaints have become a distraction for the town's 911 service. CHARLIE MANNING: 911 emergency. JAKE WARGA, BYLINE: Menlo Park Police get a lot of calls they can't do anything about. MANNING: A lot of people call and say that my account has been hacked. I want you to go and find who this person is that hacks my account. WARGA: That's Charlie Manning, senior dispatcher for Menlo Park Police, where they take calls like this one. . . UNIDENTIFIED PERSON #1: Menlo Park Police. UNIDENTIFIED PERSON #2: Yes, I'm trying to get in contact with Facebook. UNIDENTIFIED PERSON #1: OK. UNIDENTIFIED PERSON #2: Is there support there? UNIDENTIFIED PERSON #1: No, there's no support. You're going to have to make a report online. WARGA: The police department has a direct business line that anyone can call, no matter where they are or how late it is. And at night, anyone who calls the number gets routed to dispatch. MANNING: They're up late at night at 11 o'clock, and they're looking at Facebook. And they realize they've been hacked. Who are they going to call? They're going to call us. So it's almost like we're famous for having Facebook in our city. WARGA: Facebook, the actual company and its flurry of new building construction, is physically located in Menlo Park. So say you think someone is breaking the law on Facebook, and you want to call the cops on them, like the nurse. MANNING: She was an RN calling from, like, North Carolina. She didn't like that there were advertisements that were on Facebook - a product that you have to get by prescription only. And they were giving out free samples. And it was to grow your eyelashes. UNIDENTIFIED PERSON #3: (Unintelligible), please. MANNING: They feel it's an emergency because it's personally affecting their lives. They want action right then and there. UNIDENTIFIED PERSON #4: 911 Emergency. UNIDENTIFIED PERSON #5: My business online and everything was hacked. MANNING: It can be very devastating for that person. You know, you get teenagers that - the world is crashing in on them. And, yeah, I do feel sorry for them. I do. It calms them down as long as somebody has heard what's happening. UNIDENTIFIED PERSON #5: I'm looking for the legal department because I need to see Mr. Z personally. UNIDENTIFIED PERSON #4: OK. UNIDENTIFIED PERSON #5: And I need to have somebody to call him. WARGA: And since people around the world have Facebook accounts. . . MANNING: Some of them have accents, and they're really hard to understand because they call from all over, all over the world. WARGA: This is Karen Salas, the other dispatcher on duty. KAREN SALAS: I think a lot of people calling have this grandiose view of California to begin with. Facebook's here, and Facebook's big. So therefore, Facebook's giant. Then we must be big and giant, too. 911 emergency. WARGA: A 911 call comes in while we're talking. Someone has parked illegally in a disabled space. They try to make it clear to callers there's nothing they can do about your account. They can only direct you to instructions online to report your issues directly to Facebook. Facebook has responded to how it's changing Menlo Park. The company is paid for a community police substation and is helping the city pay for more police officers. SALAS: 911 emergency. WARGA: Another 911 call comes in for the same parked car in a disabled space. SALAS: OK. Bye. WARGA: For NPR News, I'm Jake Warga in Menlo Park. (SOUNDBITE OF BADBADNOTGOOD AND GHOSTFACE KILLAH SONG, \"SOUR SOUL\") KORVA COLEMAN, HOST:  Menlo Park, Calif. , was a sleepy bedroom community until Facebook came to town. One unexpected thing Facebook has brought to Menlo Park is complaints from all over the world about Facebook. As Jake Warga reports, those complaints have become a distraction for the town's 911 service. CHARLIE MANNING: 911 emergency. JAKE WARGA, BYLINE: Menlo Park Police get a lot of calls they can't do anything about. MANNING: A lot of people call and say that my account has been hacked. I want you to go and find who this person is that hacks my account. WARGA: That's Charlie Manning, senior dispatcher for Menlo Park Police, where they take calls like this one. . . UNIDENTIFIED PERSON #1: Menlo Park Police. UNIDENTIFIED PERSON #2: Yes, I'm trying to get in contact with Facebook. UNIDENTIFIED PERSON #1: OK. UNIDENTIFIED PERSON #2: Is there support there? UNIDENTIFIED PERSON #1: No, there's no support. You're going to have to make a report online. WARGA: The police department has a direct business line that anyone can call, no matter where they are or how late it is. And at night, anyone who calls the number gets routed to dispatch. MANNING: They're up late at night at 11 o'clock, and they're looking at Facebook. And they realize they've been hacked. Who are they going to call? They're going to call us. So it's almost like we're famous for having Facebook in our city. WARGA: Facebook, the actual company and its flurry of new building construction, is physically located in Menlo Park. So say you think someone is breaking the law on Facebook, and you want to call the cops on them, like the nurse. MANNING: She was an RN calling from, like, North Carolina. She didn't like that there were advertisements that were on Facebook - a product that you have to get by prescription only. And they were giving out free samples. And it was to grow your eyelashes. UNIDENTIFIED PERSON #3: (Unintelligible), please. MANNING: They feel it's an emergency because it's personally affecting their lives. They want action right then and there. UNIDENTIFIED PERSON #4: 911 Emergency. UNIDENTIFIED PERSON #5: My business online and everything was hacked. MANNING: It can be very devastating for that person. You know, you get teenagers that - the world is crashing in on them. And, yeah, I do feel sorry for them. I do. It calms them down as long as somebody has heard what's happening. UNIDENTIFIED PERSON #5: I'm looking for the legal department because I need to see Mr. Z personally. UNIDENTIFIED PERSON #4: OK. UNIDENTIFIED PERSON #5: And I need to have somebody to call him. WARGA: And since people around the world have Facebook accounts. . . MANNING: Some of them have accents, and they're really hard to understand because they call from all over, all over the world. WARGA: This is Karen Salas, the other dispatcher on duty. KAREN SALAS: I think a lot of people calling have this grandiose view of California to begin with. Facebook's here, and Facebook's big. So therefore, Facebook's giant. Then we must be big and giant, too. 911 emergency. WARGA: A 911 call comes in while we're talking. Someone has parked illegally in a disabled space. They try to make it clear to callers there's nothing they can do about your account. They can only direct you to instructions online to report your issues directly to Facebook. Facebook has responded to how it's changing Menlo Park. The company is paid for a community police substation and is helping the city pay for more police officers. SALAS: 911 emergency. WARGA: Another 911 call comes in for the same parked car in a disabled space. SALAS: OK. Bye. WARGA: For NPR News, I'm Jake Warga in Menlo Park. (SOUNDBITE OF BADBADNOTGOOD AND GHOSTFACE KILLAH SONG, \"SOUR SOUL\")", "section": "Strange News", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-02-598916525": {"title": "Meet The Companies Behind Facial Recognition Technology : NPR", "url": "https://www.npr.org/2018/04/02/598916525/meet-the-companies-behind-facial-recognition-technology", "author": "No author found", "published_date": "2018-04-02", "content": "MARY LOUISE KELLY, HOST: Facial recognition technology is becoming more common here in the U. S. as anyone who uses Facebook or an iPhone 10 knows. But it's far more widespread in China. We take a look in this week's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")KELLY: In China, facial recognition technology is used for lots of things from ride hailing and shopping to surveillance. NPR's Rob Schmitz takes us on a tour of the Chinese companies developing this technology. ROB SCHMITZ, BYLINE: When I take out my microphone at the Beijing headquarters of SenseTime, China's largest artificial intelligence company, employee Katherine Xue looks nervous. KATHERINE XUE: I'm a little scared (laughter). SCHMITZ: You're scared? You have cameras all over this room, and you're scared of the microphone (laughter). XUE: A little bit. SCHMITZ: Actually, I'm a little scared, too. Dozens of cameras are pointed at me. On a big screen in SenseTime's showroom, my face is covered with lines that detect my age, identity and my attractiveness score. It gave me a 98 on my attractiveness. XUE: Yes. SCHMITZ: And is that a high score, or is that a low score? XUE: It's a very high score. SCHMITZ: It's a very high score. A moment later Katherine, who scored a 99 and didn't feign surprise at all, whispers that everyone receives a high attractiveness score. This is a marketing gimmick that flatters you so you'll buy merchandise based on data gleaned from your face. In my case, the machine determined that I'd likely buy a cheap brand of Chinese grain alcohol. Thanks, machine. This is just one way SenseTime uses facial recognition technology. The other starts with a camera pointed at a street. What are we looking at here? QIAN CHEN: Real-time video surveillance system. Actually. . . SCHMITZ: SenseTime AI researcher Qian Chen shows me video of the street below. Each car and person is surrounded by a square that displays information like a car's model and license plate and a person's gender and clothing. Qian says if their data is in the system, it could figure out who they are. SenseTime's June Jin says the company sells these applications to Chinese police. JUNE JIN: We have been working with about, you know, 40 public security bureaus, and they've been working on this, leveling up the city's security level. SCHMITZ: State surveillance makes up a third of SenseTime's $3 billion business, says Jin. Megvii, China's second-largest AI company, serves the government, too. Vice President Xie Yinan. . . XIE YINAN: (Through interpreter) The government is pushing the need for this technology from the top so companies don't have big obstacles in making it happen. In America, people are too busy discussing how they should use it. SCHMITZ: China's government has laid out goals to build an artificial intelligence industry worth nearly $150 billion by 2030, much of it to enhance domestic security. When I question how Chinese police will use Megvii's technology, Xie says there are limitations. XIE: (Through interpreter) We just provide the government the technology, and they do their job with it. Cameras are set in China at 2. 8 meters above the ground. That means they won't be able to capture human faces. That's a rule. Chinese citizens know that, so they don't think about it too much. SCHMITZ: Ji Feng thinks about it all the time. He's a poet and an activist who police escort out of Beijing each year on the anniversary of the Tiananmen Square massacre and on World Human Rights Day. Ji says after a fellow activist visited his home recently, police used facial recognition cameras to identify him, inform his landlord, who then threatened to kick him out of his apartment. JI FENG: (Through interpreter) The government's using this technology to catch people who are considered threats to social stability. Are they using it to catch thieves? Yes, but it's mostly used to maintain stability. SCHMITZ: Megvii's Xie Yinan insists China is using its technology to keep cities safe. When I ask him what he'd think if foreign governments used his technology to crack down on its citizens, he quotes Google's founders. XIE: (Through interpreter) Our founders also think don't be evil is the No. 1 principle. If a government is using it to control locals, we'd think twice about doing business with them. Our principle is to empower humans, not to control them. SCHMITZ: Plus, Xie says, the algorithm capacity of the fastest servers isn't enough to support data from thousands of cameras capturing hundreds of millions of people at any given time. But it can give you a high score on your attractiveness. Rob Schmitz, NPR News, Beijing. MARY LOUISE KELLY, HOST:  Facial recognition technology is becoming more common here in the U. S. as anyone who uses Facebook or an iPhone 10 knows. But it's far more widespread in China. We take a look in this week's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") KELLY: In China, facial recognition technology is used for lots of things from ride hailing and shopping to surveillance. NPR's Rob Schmitz takes us on a tour of the Chinese companies developing this technology. ROB SCHMITZ, BYLINE: When I take out my microphone at the Beijing headquarters of SenseTime, China's largest artificial intelligence company, employee Katherine Xue looks nervous. KATHERINE XUE: I'm a little scared (laughter). SCHMITZ: You're scared? You have cameras all over this room, and you're scared of the microphone (laughter). XUE: A little bit. SCHMITZ: Actually, I'm a little scared, too. Dozens of cameras are pointed at me. On a big screen in SenseTime's showroom, my face is covered with lines that detect my age, identity and my attractiveness score. It gave me a 98 on my attractiveness. XUE: Yes. SCHMITZ: And is that a high score, or is that a low score? XUE: It's a very high score. SCHMITZ: It's a very high score. A moment later Katherine, who scored a 99 and didn't feign surprise at all, whispers that everyone receives a high attractiveness score. This is a marketing gimmick that flatters you so you'll buy merchandise based on data gleaned from your face. In my case, the machine determined that I'd likely buy a cheap brand of Chinese grain alcohol. Thanks, machine. This is just one way SenseTime uses facial recognition technology. The other starts with a camera pointed at a street. What are we looking at here? QIAN CHEN: Real-time video surveillance system. Actually. . . SCHMITZ: SenseTime AI researcher Qian Chen shows me video of the street below. Each car and person is surrounded by a square that displays information like a car's model and license plate and a person's gender and clothing. Qian says if their data is in the system, it could figure out who they are. SenseTime's June Jin says the company sells these applications to Chinese police. JUNE JIN: We have been working with about, you know, 40 public security bureaus, and they've been working on this, leveling up the city's security level. SCHMITZ: State surveillance makes up a third of SenseTime's $3 billion business, says Jin. Megvii, China's second-largest AI company, serves the government, too. Vice President Xie Yinan. . . XIE YINAN: (Through interpreter) The government is pushing the need for this technology from the top so companies don't have big obstacles in making it happen. In America, people are too busy discussing how they should use it. SCHMITZ: China's government has laid out goals to build an artificial intelligence industry worth nearly $150 billion by 2030, much of it to enhance domestic security. When I question how Chinese police will use Megvii's technology, Xie says there are limitations. XIE: (Through interpreter) We just provide the government the technology, and they do their job with it. Cameras are set in China at 2. 8 meters above the ground. That means they won't be able to capture human faces. That's a rule. Chinese citizens know that, so they don't think about it too much. SCHMITZ: Ji Feng thinks about it all the time. He's a poet and an activist who police escort out of Beijing each year on the anniversary of the Tiananmen Square massacre and on World Human Rights Day. Ji says after a fellow activist visited his home recently, police used facial recognition cameras to identify him, inform his landlord, who then threatened to kick him out of his apartment. JI FENG: (Through interpreter) The government's using this technology to catch people who are considered threats to social stability. Are they using it to catch thieves? Yes, but it's mostly used to maintain stability. SCHMITZ: Megvii's Xie Yinan insists China is using its technology to keep cities safe. When I ask him what he'd think if foreign governments used his technology to crack down on its citizens, he quotes Google's founders. XIE: (Through interpreter) Our founders also think don't be evil is the No. 1 principle. If a government is using it to control locals, we'd think twice about doing business with them. Our principle is to empower humans, not to control them. SCHMITZ: Plus, Xie says, the algorithm capacity of the fastest servers isn't enough to support data from thousands of cameras capturing hundreds of millions of people at any given time. But it can give you a high score on your attractiveness. Rob Schmitz, NPR News, Beijing.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-02-598916380": {"title": "Technologies To Create Fake Audio And Video Are Quickly Evolving : NPR", "url": "https://www.npr.org/2018/04/02/598916380/technologies-to-create-fake-audio-and-video-are-quickly-evolving", "author": "No author found", "published_date": "2018-04-02", "content": "MARY LOUISE KELLY, HOST: Amid all the talk of fake news, the technologies to create fake audio and video are quickly evolving. NPR's Tim Mak has been looking into this, and he brings us this report of how these technologies could impact our politics. TIM MAK, BYLINE: This is not a real audio clip of President Trump. (SOUNDBITE OF ARCHIVED RECORDING)COMPUTER-GENERATED VOICE: South Korea's finding, as I have told them, that their talk of appeasement with North Korea will not work. They only understand one thing. MAK: Trump did write that on Twitter, but he never once said that. A Montreal startup called Lyrebird has released a product which allows users to create an audio clip of anyone saying anything. Here's the company using a fake clip of President Obama to market their technology. (SOUNDBITE OF ARCHIVED RECORDING)COMPUTER-GENERATED VOICE: They want to use this technology to change the life of everyone that lost their voice to a disease by helping them recover this part of their identities. Let's help them achieve this goal. MAK: Again, Obama never actually said that. These technologies process the limited number of distinct sounds in the human voice, and using a process called machine learning, it then imitates them. YOSHUA BENGIO: We can record a few minutes of somebody's voice and then be able to generate speech of that person speaking, saying things that have been typed in the computer. MAK: Professor Yoshua Bengio is an adviser to Lyrebird. He touts such positive uses as restoring voices to those who have lost them to illness. BENGIO: I think it's better if companies, which work to, you know, try to do it in a way that's going to be beneficial for society, actually build those products and try to put as much as possible of the safeguards that I think are necessary and also raise the awareness rather than doing these things in secret. MAK: And so how much does this matter? HANY FARID: I don't think it's an overstatement to say that it is a potential threat to democracy. MAK: Hany Farid is the chair of computer science at Dartmouth College. An example that illustrates Farid's concern took place during the 2008 election, when rumors circulated that there was a tape of Michelle Obama using a derogatory term for white people. There's no evidence that it existed, but using these technologies, a fake could be made. It can also give public figures a chance to call real audio a forgery. Farid recalls those \"Access Hollywood\" tapes during the 2016 campaign. FARID: Eighteen months ago, when that audio recording of President Trump came out on the bus, if that was today, you can guarantee it, he would've said it's fake. And he would've had some reasonable credibility in saying that as well 'cause there was no video associated with it. MAK: The threat of falsified audio, video and photo is a national security issue that has gotten the interest of the Defense Advanced Research Projects Agency, or DARPA, which is part of the Department of Defense. David Doermann runs the media forensics program at DARPA. His disaster scenario is a mass misinformation campaign - creating an event that never even occurred. DAVID DOERMANN: And that might lead to political unrest or riots or, at worst, some nations acting all based on this bad information. MAK: Doermann's team is putting together a platform that automatically determines whether images, video or audio has been manipulated. Here's Mark Kozak, an engineer who works for PAR Government Systems. He helps create falsified audio that Doermann and his team then use to develop their platform. MARK KOZAK: It wasn't that long ago that you could easily assume that if you have photographic evidence of something, that can be used as evidence and no one's going to question it. I think people have to learn to be questioning everything that you hear and see. MAK: And we may be headed for an endless back-and-forth between those who create fake media and those who want to catch them. Tim Mak, NPR News, Washington. (SOUNDBITE OF MUSIC) MARY LOUISE KELLY, HOST:  Amid all the talk of fake news, the technologies to create fake audio and video are quickly evolving. NPR's Tim Mak has been looking into this, and he brings us this report of how these technologies could impact our politics. TIM MAK, BYLINE: This is not a real audio clip of President Trump. (SOUNDBITE OF ARCHIVED RECORDING) COMPUTER-GENERATED VOICE: South Korea's finding, as I have told them, that their talk of appeasement with North Korea will not work. They only understand one thing. MAK: Trump did write that on Twitter, but he never once said that. A Montreal startup called Lyrebird has released a product which allows users to create an audio clip of anyone saying anything. Here's the company using a fake clip of President Obama to market their technology. (SOUNDBITE OF ARCHIVED RECORDING) COMPUTER-GENERATED VOICE: They want to use this technology to change the life of everyone that lost their voice to a disease by helping them recover this part of their identities. Let's help them achieve this goal. MAK: Again, Obama never actually said that. These technologies process the limited number of distinct sounds in the human voice, and using a process called machine learning, it then imitates them. YOSHUA BENGIO: We can record a few minutes of somebody's voice and then be able to generate speech of that person speaking, saying things that have been typed in the computer. MAK: Professor Yoshua Bengio is an adviser to Lyrebird. He touts such positive uses as restoring voices to those who have lost them to illness. BENGIO: I think it's better if companies, which work to, you know, try to do it in a way that's going to be beneficial for society, actually build those products and try to put as much as possible of the safeguards that I think are necessary and also raise the awareness rather than doing these things in secret. MAK: And so how much does this matter? HANY FARID: I don't think it's an overstatement to say that it is a potential threat to democracy. MAK: Hany Farid is the chair of computer science at Dartmouth College. An example that illustrates Farid's concern took place during the 2008 election, when rumors circulated that there was a tape of Michelle Obama using a derogatory term for white people. There's no evidence that it existed, but using these technologies, a fake could be made. It can also give public figures a chance to call real audio a forgery. Farid recalls those \"Access Hollywood\" tapes during the 2016 campaign. FARID: Eighteen months ago, when that audio recording of President Trump came out on the bus, if that was today, you can guarantee it, he would've said it's fake. And he would've had some reasonable credibility in saying that as well 'cause there was no video associated with it. MAK: The threat of falsified audio, video and photo is a national security issue that has gotten the interest of the Defense Advanced Research Projects Agency, or DARPA, which is part of the Department of Defense. David Doermann runs the media forensics program at DARPA. His disaster scenario is a mass misinformation campaign - creating an event that never even occurred. DAVID DOERMANN: And that might lead to political unrest or riots or, at worst, some nations acting all based on this bad information. MAK: Doermann's team is putting together a platform that automatically determines whether images, video or audio has been manipulated. Here's Mark Kozak, an engineer who works for PAR Government Systems. He helps create falsified audio that Doermann and his team then use to develop their platform. MARK KOZAK: It wasn't that long ago that you could easily assume that if you have photographic evidence of something, that can be used as evidence and no one's going to question it. I think people have to learn to be questioning everything that you hear and see. MAK: And we may be headed for an endless back-and-forth between those who create fake media and those who want to catch them. Tim Mak, NPR News, Washington. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-03-599077747": {"title": "Why A French Attorney Is On A Digital Privacy Crusade Against Tech Titans : NPR", "url": "https://www.npr.org/2018/04/03/599077747/why-a-french-attorney-is-on-a-digital-privacy-crusade-against-tech-titans", "author": "No author found", "published_date": "2018-04-03", "content": "STEVE INSKEEP, HOST: Type your name into Google. . . (SOUNDBITE OF TYPING)INSKEEP: . . . And you may find out something about yourself. You may find out how much of your life is public. And you may even find claims that are not true. People do. What do you do if you know that anybody who types your name in a search engine could come up with something false? There is a global movement based in Europe designed to stop Internet firms from letting this happen. NPR's Aarti Shahani visited a man in Paris who is at the center of the movement. UNIDENTIFIED PERSON #1: Bonjour. DAN SHEFET: Bonjour. AARTI SHAHANI, BYLINE: Dan Shefet wants Americans to wake up and realize, life can be better. We can have what the Europeans have. And by that, he doesn't mean - or just mean - a zesty tuna tartare sculpted atop a bed of steamed, perfectly salted spinach, paired with a dry, French white wine - apricot undertones. SHEFET: We'll take two Chablis. SHAHANI: Perfect. UNIDENTIFIED PERSON #2: (Speaking French). SHEFET: (Speaking French). SHAHANI: Shefet means rights - legal rights, human rights in the digital age. And this time, it's not a government standing in the way. It's Internet giants like Google. SHEFET: How far than they - can they control your life? When does it go too far? SHAHANI: For Shefet, it's intensely personal, but more on that later. It is also political. Shefet, a small man with a big laugh, jet-sets around the world advising ministers on how they can punish tech titans for the harmful speech that goes viral. He counsels victims of slander, too, and politicians looking to bury a youthful indiscretion. Call him the all-purpose speech slayer, striking when the tech titans are down. Facebook is under investigation for violating user privacy. Google's been fined billions for illegally killing off competition. SHEFET: These guys are being attacked from everywhere. And they understand that the time has come to be realistic (laughter). SHAHANI: And he knows he's being overconfident. The powerful do not give up power easily. Shefet believes these titans have gotten a free pass. No other industry is so unregulated. Banks, for example, spend billions making sure the information in their system is correct. SHEFET: If you don't, you lose your license. You go to prison. It's not in any way optional. And that's what they don't have. SHAHANI: Shefet, a 63-year-old lawyer from Denmark, has a penthouse office down the street from France's White House. His campaign against Silicon Valley started with a single case - his own. It's Friday evening. The macarons have arrived. SHEFET: Ah, yeah, you got to come in. SHAHANI: . . . Pistachio and chocolate. And Shefet breaks out the champagne. (SOUNDBITE OF CHAMPAGNE FIZZING)SHEFET: You got to cheers. Come on. SHAHANI: Leaning back in his black leather chair, he touches his right temple and begins to tell his story. In 2013, an enemy of a client created websites claiming Shefet was a member of the Serbian mafia. They rose to the top of his Google search results. Colleagues started to question Shefet. He realized his reputation was at risk, so he sued Google in Paris and won. The court ordered Google to stop highlighting the lies. But Google did not respond. SHEFET: Not even a letter saying, we have received your injunction, but for the following reasons, we won't comply - nothing at all. Fine. SHAHANI: Soon after, another court, the European Court of Justice, issued a landmark opinion about privacy on the Internet. And buried in it, there was a line Shefet grabbed to make a novel argument. Google's office in Paris could be forced to pay a fine for every day that Google headquarters in California ignored his court order. That is, the child could be punished for the sins of the corporate parent. SHEFET: Suddenly, we in Europe have a remedy - efficient remedy, real remedy. SHAHANI: Google finally scrubbed Shefet's results. And since that victory, his inbox has been flooded with cries for help, including from the U. S. - a state assemblyman in New York who wants advice on a bill to bring similar privacy rights to New Yorkers; consumer groups want expert testimony; individuals want help. SHEFET: Why do American citizens call me in Europe to get their life back? Why? Isn't that strange. SHAHANI: NPR reached out to Google, and they gave us their top privacy lawyer - Peter Fleischer, the man they put in Paris to counter the movement Shefet has helped build. Should Americans have the same right that the Europeans have? PETER FLEISCHER: Clearly, they don't. SHAHANI: My question was, should they? FLEISCHER: Well, the first question is, can they? Not, should they? And that's a legal question. SHAHANI: Fleischer doesn't want Americans to be seduced. He makes two big points. First, America, more so than any other country on earth, values free speech so much so that it's the first of our amendments. What Europe's doing. . . FLEISCHER: Would be inconsistent with the U. S. Constitution. SHAHANI: And second, with its new laws, Europe is creating an Internet that people can exploit to hide the truth - people with criminal records who want to bury accurate news reports, lawyers and dentists who want to hide the complaints of unhappy customers, business owners obsessed with projecting a certain image. FLEISCHER: I think a lot of people would think that's troubling to think that the more wealthy and powerful in the world can use this tool to polish up their online reputations far more than, you know, the average Joe. (SOUNDBITE OF MUSIC)SHAHANI: I'm having my last lunch with Dan Shefet - this time, blackened salmon, arugula with a wasabi drizzle. SHEFET: That's why I never answer my phone. It's too good. Hello? SHAHANI: Actually, he always answers his phone. And these days, he's getting new callers - not just Western democracies, but also authoritarian states like Singapore and Saudi Arabia. Shefet wants to make something clear. He is not Google's enemy. In fact, they share values, like don't be evil. And he brings up another case - not from the courts, but from personal history - his mom and dad. SHEFET: My mother divorced my father just after I was born. I don't know whether there was a cause and effect on that. SHAHANI: It was a time when almost no one divorced. He says they hated each other. And for many years, Shefet didn't have contact with his father, which made him sad because his dad was a vast ocean of knowledge. But then. . . SHEFET: The funny thing is, I started seeing him again. They remarried each other. They were divorced for, like, 15 years, and then they got married again (laughter). SHAHANI: They found a way to see each other's point of view. Shefet hopes it takes the tech titans and their disaffected users less time than it took his parents, and he feels he can be a peacemaker in the process. Aarti Shahani, NPR News, Paris. INSKEEP: That's the first in a series on the clash between privacy and free speech. We'll have more later this week. (SOUNDBITE OF SECEDE'S \"BORN IN A TROPICAL SWAMP\") STEVE INSKEEP, HOST:  Type your name into Google. . . (SOUNDBITE OF TYPING) INSKEEP: . . . And you may find out something about yourself. You may find out how much of your life is public. And you may even find claims that are not true. People do. What do you do if you know that anybody who types your name in a search engine could come up with something false? There is a global movement based in Europe designed to stop Internet firms from letting this happen. NPR's Aarti Shahani visited a man in Paris who is at the center of the movement. UNIDENTIFIED PERSON #1: Bonjour. DAN SHEFET: Bonjour. AARTI SHAHANI, BYLINE: Dan Shefet wants Americans to wake up and realize, life can be better. We can have what the Europeans have. And by that, he doesn't mean - or just mean - a zesty tuna tartare sculpted atop a bed of steamed, perfectly salted spinach, paired with a dry, French white wine - apricot undertones. SHEFET: We'll take two Chablis. SHAHANI: Perfect. UNIDENTIFIED PERSON #2: (Speaking French). SHEFET: (Speaking French). SHAHANI: Shefet means rights - legal rights, human rights in the digital age. And this time, it's not a government standing in the way. It's Internet giants like Google. SHEFET: How far than they - can they control your life? When does it go too far? SHAHANI: For Shefet, it's intensely personal, but more on that later. It is also political. Shefet, a small man with a big laugh, jet-sets around the world advising ministers on how they can punish tech titans for the harmful speech that goes viral. He counsels victims of slander, too, and politicians looking to bury a youthful indiscretion. Call him the all-purpose speech slayer, striking when the tech titans are down. Facebook is under investigation for violating user privacy. Google's been fined billions for illegally killing off competition. SHEFET: These guys are being attacked from everywhere. And they understand that the time has come to be realistic (laughter). SHAHANI: And he knows he's being overconfident. The powerful do not give up power easily. Shefet believes these titans have gotten a free pass. No other industry is so unregulated. Banks, for example, spend billions making sure the information in their system is correct. SHEFET: If you don't, you lose your license. You go to prison. It's not in any way optional. And that's what they don't have. SHAHANI: Shefet, a 63-year-old lawyer from Denmark, has a penthouse office down the street from France's White House. His campaign against Silicon Valley started with a single case - his own. It's Friday evening. The macarons have arrived. SHEFET: Ah, yeah, you got to come in. SHAHANI: . . . Pistachio and chocolate. And Shefet breaks out the champagne. (SOUNDBITE OF CHAMPAGNE FIZZING) SHEFET: You got to cheers. Come on. SHAHANI: Leaning back in his black leather chair, he touches his right temple and begins to tell his story. In 2013, an enemy of a client created websites claiming Shefet was a member of the Serbian mafia. They rose to the top of his Google search results. Colleagues started to question Shefet. He realized his reputation was at risk, so he sued Google in Paris and won. The court ordered Google to stop highlighting the lies. But Google did not respond. SHEFET: Not even a letter saying, we have received your injunction, but for the following reasons, we won't comply - nothing at all. Fine. SHAHANI: Soon after, another court, the European Court of Justice, issued a landmark opinion about privacy on the Internet. And buried in it, there was a line Shefet grabbed to make a novel argument. Google's office in Paris could be forced to pay a fine for every day that Google headquarters in California ignored his court order. That is, the child could be punished for the sins of the corporate parent. SHEFET: Suddenly, we in Europe have a remedy - efficient remedy, real remedy. SHAHANI: Google finally scrubbed Shefet's results. And since that victory, his inbox has been flooded with cries for help, including from the U. S. - a state assemblyman in New York who wants advice on a bill to bring similar privacy rights to New Yorkers; consumer groups want expert testimony; individuals want help. SHEFET: Why do American citizens call me in Europe to get their life back? Why? Isn't that strange. SHAHANI: NPR reached out to Google, and they gave us their top privacy lawyer - Peter Fleischer, the man they put in Paris to counter the movement Shefet has helped build. Should Americans have the same right that the Europeans have? PETER FLEISCHER: Clearly, they don't. SHAHANI: My question was, should they? FLEISCHER: Well, the first question is, can they? Not, should they? And that's a legal question. SHAHANI: Fleischer doesn't want Americans to be seduced. He makes two big points. First, America, more so than any other country on earth, values free speech so much so that it's the first of our amendments. What Europe's doing. . . FLEISCHER: Would be inconsistent with the U. S. Constitution. SHAHANI: And second, with its new laws, Europe is creating an Internet that people can exploit to hide the truth - people with criminal records who want to bury accurate news reports, lawyers and dentists who want to hide the complaints of unhappy customers, business owners obsessed with projecting a certain image. FLEISCHER: I think a lot of people would think that's troubling to think that the more wealthy and powerful in the world can use this tool to polish up their online reputations far more than, you know, the average Joe. (SOUNDBITE OF MUSIC) SHAHANI: I'm having my last lunch with Dan Shefet - this time, blackened salmon, arugula with a wasabi drizzle. SHEFET: That's why I never answer my phone. It's too good. Hello? SHAHANI: Actually, he always answers his phone. And these days, he's getting new callers - not just Western democracies, but also authoritarian states like Singapore and Saudi Arabia. Shefet wants to make something clear. He is not Google's enemy. In fact, they share values, like don't be evil. And he brings up another case - not from the courts, but from personal history - his mom and dad. SHEFET: My mother divorced my father just after I was born. I don't know whether there was a cause and effect on that. SHAHANI: It was a time when almost no one divorced. He says they hated each other. And for many years, Shefet didn't have contact with his father, which made him sad because his dad was a vast ocean of knowledge. But then. . . SHEFET: The funny thing is, I started seeing him again. They remarried each other. They were divorced for, like, 15 years, and then they got married again (laughter). SHAHANI: They found a way to see each other's point of view. Shefet hopes it takes the tech titans and their disaffected users less time than it took his parents, and he feels he can be a peacemaker in the process. Aarti Shahani, NPR News, Paris. INSKEEP: That's the first in a series on the clash between privacy and free speech. We'll have more later this week. (SOUNDBITE OF SECEDE'S \"BORN IN A TROPICAL SWAMP\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-04-599604485": {"title": "Facebook Says Cambridge Analytica May Have Obtained Data On As Many As 87M Users : NPR", "url": "https://www.npr.org/2018/04/04/599604485/facebook-says-cambridge-analytica-may-have-obtained-data-on-as-many-as-87m-users", "author": "No author found", "published_date": "2018-04-04", "content": "MARY LOUISE KELLY, HOST: All right. To another story now, a story involving the number 87 million. Facebook says that's the number of people whose information may have been improperly shared with the political data mining firm Cambridge Analytica. This disclosure comes on the same day that lawmakers announced that Facebook CEO Mark Zuckerberg has agreed to testify before Congress. Well, here to discuss the most recent developments is NPR tech reporter Aarti Shahani. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. KELLY: So Mark Zuckerberg today held a conference call with journalists. I want to start by just listening in for a moment to what he had to say. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: I think the reality here is that we need to take a broader view of our responsibility rather than just the legal responsibility. So, you know, we're focused on doing the right thing and making sure that people's information is protected. We're doing the investigations. We're locking down the platform, et cetera. KELLY: Aarti, locking down the platform, doing the right thing - do we know what he means? SHAHANI: Well, he means a lot of things. First of all, he's got a short-term and a longer-term view on what he's doing. He said that Facebook is one year into a massive three-year shift. So we're going to keep hearing about changes. They're going to last beyond one news cycle. There was a lot of questioning his leadership on the call. It was pretty blunt. You know, one question that came up was, has the board discussed you, Mark Zuckerberg, stepping down as chairman? He said, uncomfortably, not that I'm aware of. Another question was, are you the best person to head Facebook? And what he said there - he was like, yes, I am, and when you're building something like Facebook, there are going to be things that you mess up, and you learn from your mistakes. He also said that so far no one has been fired to his knowledge around the Cambridge Analytica scandal that broke most recently. KELLY: Let me drill down on that number I just put out there - 87 million people, the number that the company now says of people whose data was compromised in this Cambridge Analytica scandal. They'd originally said it was 50 million people - already pretty huge. We learned this from this blog post that came out today. Do we know how Facebook has arrived at this new, even more staggering number? SHAHANI: Yeah. They are continuing to investigate. And now what they're saying about this new even more staggering number is that it's the upper limit. So they're like, we believe that this is the highest it's going to get, i. e. don't expect next week that number to go up. KELLY: OK. SHAHANI: And the blog posts that you just mentioned, you know, that came out by Facebook's chief technology officer basically outlining steps that Facebook is going to take to guard user data more aggressively. OK. One ongoing issue is that Facebook is one massive platform, but on top of that, you've got other apps, other, what are called, third-party developers that for years have looked to Facebook to be a data feeder to them, to give them information about users, and everyone's kind of in the data sharing together. The blog post announced that they're going to stop certain kinds of data sharing. So, for example, if I'm advertising an event, if I'm organizing an event on Facebook, it used to be that the people who are going to join my event would be visible to third-party developers, but Facebook is changing that so that now the guest list is guarded. The comments for the event are guarded as one step. And really, you know, what you're seeing is Facebook sending out the message incrementally that, hey, we know that our borders were porous, but we're building a wall. KELLY: And, Aarti, quickly, we mentioned that Mark Zuckerberg will appear before the House Energy and Commerce Committee. He's going to be testifying next Wednesday. What are you watching for? SHAHANI: Yeah. You know, the thing is that there's a real conversation here to be had about regulation. So far, Zuckerberg admits that Facebook has been lax with handling user data. In part, that's his fault, but in part, you know, you've got to look at what are the laws, what are the regulations making him do it. If a mistake costs you nothing versus a mistake costs you a billion dollars, well, what do you more - when are you more likely to make a mistake? So I'm looking to see what are the regulatory conversations that really take shape here. KELLY: That's NPR's Aarti Shahani. Thanks so much. SHAHANI: Thank you. (SOUNDBITE OF HOT SUGAR'S \"SINKIES\") MARY LOUISE KELLY, HOST:  All right. To another story now, a story involving the number 87 million. Facebook says that's the number of people whose information may have been improperly shared with the political data mining firm Cambridge Analytica. This disclosure comes on the same day that lawmakers announced that Facebook CEO Mark Zuckerberg has agreed to testify before Congress. Well, here to discuss the most recent developments is NPR tech reporter Aarti Shahani. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. KELLY: So Mark Zuckerberg today held a conference call with journalists. I want to start by just listening in for a moment to what he had to say. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: I think the reality here is that we need to take a broader view of our responsibility rather than just the legal responsibility. So, you know, we're focused on doing the right thing and making sure that people's information is protected. We're doing the investigations. We're locking down the platform, et cetera. KELLY: Aarti, locking down the platform, doing the right thing - do we know what he means? SHAHANI: Well, he means a lot of things. First of all, he's got a short-term and a longer-term view on what he's doing. He said that Facebook is one year into a massive three-year shift. So we're going to keep hearing about changes. They're going to last beyond one news cycle. There was a lot of questioning his leadership on the call. It was pretty blunt. You know, one question that came up was, has the board discussed you, Mark Zuckerberg, stepping down as chairman? He said, uncomfortably, not that I'm aware of. Another question was, are you the best person to head Facebook? And what he said there - he was like, yes, I am, and when you're building something like Facebook, there are going to be things that you mess up, and you learn from your mistakes. He also said that so far no one has been fired to his knowledge around the Cambridge Analytica scandal that broke most recently. KELLY: Let me drill down on that number I just put out there - 87 million people, the number that the company now says of people whose data was compromised in this Cambridge Analytica scandal. They'd originally said it was 50 million people - already pretty huge. We learned this from this blog post that came out today. Do we know how Facebook has arrived at this new, even more staggering number? SHAHANI: Yeah. They are continuing to investigate. And now what they're saying about this new even more staggering number is that it's the upper limit. So they're like, we believe that this is the highest it's going to get, i. e. don't expect next week that number to go up. KELLY: OK. SHAHANI: And the blog posts that you just mentioned, you know, that came out by Facebook's chief technology officer basically outlining steps that Facebook is going to take to guard user data more aggressively. OK. One ongoing issue is that Facebook is one massive platform, but on top of that, you've got other apps, other, what are called, third-party developers that for years have looked to Facebook to be a data feeder to them, to give them information about users, and everyone's kind of in the data sharing together. The blog post announced that they're going to stop certain kinds of data sharing. So, for example, if I'm advertising an event, if I'm organizing an event on Facebook, it used to be that the people who are going to join my event would be visible to third-party developers, but Facebook is changing that so that now the guest list is guarded. The comments for the event are guarded as one step. And really, you know, what you're seeing is Facebook sending out the message incrementally that, hey, we know that our borders were porous, but we're building a wall. KELLY: And, Aarti, quickly, we mentioned that Mark Zuckerberg will appear before the House Energy and Commerce Committee. He's going to be testifying next Wednesday. What are you watching for? SHAHANI: Yeah. You know, the thing is that there's a real conversation here to be had about regulation. So far, Zuckerberg admits that Facebook has been lax with handling user data. In part, that's his fault, but in part, you know, you've got to look at what are the laws, what are the regulations making him do it. If a mistake costs you nothing versus a mistake costs you a billion dollars, well, what do you more - when are you more likely to make a mistake? So I'm looking to see what are the regulatory conversations that really take shape here. KELLY: That's NPR's Aarti Shahani. Thanks so much. SHAHANI: Thank you. (SOUNDBITE OF HOT SUGAR'S \"SINKIES\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-04-599579218": {"title": "Attack At YouTube Offices Brings Company's Content Policy Into Question : NPR", "url": "https://www.npr.org/2018/04/04/599579218/attack-at-youtube-offices-brings-companys-content-policy-into-question", "author": "No author found", "published_date": "2018-04-04", "content": "MARY LOUISE KELLY, HOST: More information is coming to light about what led to the shooting yesterday at YouTube headquarters in San Bruno, Calif. Three people were wounded and the shooter, a woman, died from a self-inflicted gunshot. She appears to have been motivated by her anger at YouTube. NPR's digital culture correspondent Laura Sydell joins me now to discuss the latest. And, Laura, let's start with the shooter. Since you and I spoke yesterday, we've gotten some details. We've gotten a name. LAURA SYDELL, BYLINE: Right. Her name is Nasim Aghdam, and she was 39 years old from Southern California. Police say they had actually gotten a call from her family before the shooting because she'd been missing for several days. And police found her sleeping in her car not far from YouTube headquarters the early morning of the day of the shooting. Police say they actually questioned her. She told them she was up there looking for work. They ran her license plate and, in fact, discovered that the family had been looking for her, called the family, the family says they called them back and here there's some dispute. The family says they warned the police that she was very angry at YouTube. The police say that they didn't hear anything about that and they spoke with her for 20 minutes and didn't notice a gun or anything of the sort. KELLY: I want to ask you more about the possible motive at play here in a second, but first, would you give us a quick update on how the victims are doing? SYDELL: Yeah. There were three shooting victims who were taken to Zuckerberg San Francisco General Hospital yesterday. Two of them were women, a 32-year-old, who was admitted in serious condition, and a 27-year-old, who was admitted in fair condition. And according to the hospital, both have now been released. The third victim, a 36-year-old man, has improved from critical to serious condition, but he is still in hospital. KELLY: OK. Now this question of motive because police are saying they believe she was motivated because she was upset with YouTube. How firm is that link? SYDELL: Well, it looks pretty firm. Aghdam had a YouTube channel from which she made money from ads running against her content, and her channel was a mix of videos about topics like animal rights, vegan eating, exercise videos to music. Some of the videos were in Farsi. But there are some videos where she explicitly expresses anger at YouTube. Apparently YouTube had actually put an age restriction on her workout videos and limited it to people who were 18 or above. And she started to make less money, she says, got fewer views on her channel. Here's a clip from a video that appears to be Aghdam speaking, saying what - you know, that YouTube really wanted to censor her views. (SOUNDBITE OF ARCHIVED RECORDING)NASIM AGHDAM: This is what they are doing to weaken activists and many other people who try to promote healthy, humane and smart living. People like me are not good for big businesses like for animal business. KELLY: So she's saying - she's accusing YouTube of trying to weaken activists like her, of trying to censor or filter her in some way. What do we make of this complaint? SYDELL: Well, this is a pretty extreme example of - and very troubled example - of anger at YouTube that a lot of creators on the site have been complaining about. They recently changed a policy about what kinds of videos qualify to get ads run against them and how they rank videos and search results. And YouTube is in a tough position. They're trying to keep violent, hateful content off their site and keep the platform as open as possible. And every day, there are millions of videos being uploaded to this site. The company's tried to balance the right of people to express themself with complaints about violent, objectionable videos, fake news, Russian propaganda. And, you know, all the social media companies now are facing this same dilemma. KELLY: That's NPR's Laura Sydell. Thanks very much. SYDELL: You're welcome. MARY LOUISE KELLY, HOST:  More information is coming to light about what led to the shooting yesterday at YouTube headquarters in San Bruno, Calif. Three people were wounded and the shooter, a woman, died from a self-inflicted gunshot. She appears to have been motivated by her anger at YouTube. NPR's digital culture correspondent Laura Sydell joins me now to discuss the latest. And, Laura, let's start with the shooter. Since you and I spoke yesterday, we've gotten some details. We've gotten a name. LAURA SYDELL, BYLINE: Right. Her name is Nasim Aghdam, and she was 39 years old from Southern California. Police say they had actually gotten a call from her family before the shooting because she'd been missing for several days. And police found her sleeping in her car not far from YouTube headquarters the early morning of the day of the shooting. Police say they actually questioned her. She told them she was up there looking for work. They ran her license plate and, in fact, discovered that the family had been looking for her, called the family, the family says they called them back and here there's some dispute. The family says they warned the police that she was very angry at YouTube. The police say that they didn't hear anything about that and they spoke with her for 20 minutes and didn't notice a gun or anything of the sort. KELLY: I want to ask you more about the possible motive at play here in a second, but first, would you give us a quick update on how the victims are doing? SYDELL: Yeah. There were three shooting victims who were taken to Zuckerberg San Francisco General Hospital yesterday. Two of them were women, a 32-year-old, who was admitted in serious condition, and a 27-year-old, who was admitted in fair condition. And according to the hospital, both have now been released. The third victim, a 36-year-old man, has improved from critical to serious condition, but he is still in hospital. KELLY: OK. Now this question of motive because police are saying they believe she was motivated because she was upset with YouTube. How firm is that link? SYDELL: Well, it looks pretty firm. Aghdam had a YouTube channel from which she made money from ads running against her content, and her channel was a mix of videos about topics like animal rights, vegan eating, exercise videos to music. Some of the videos were in Farsi. But there are some videos where she explicitly expresses anger at YouTube. Apparently YouTube had actually put an age restriction on her workout videos and limited it to people who were 18 or above. And she started to make less money, she says, got fewer views on her channel. Here's a clip from a video that appears to be Aghdam speaking, saying what - you know, that YouTube really wanted to censor her views. (SOUNDBITE OF ARCHIVED RECORDING) NASIM AGHDAM: This is what they are doing to weaken activists and many other people who try to promote healthy, humane and smart living. People like me are not good for big businesses like for animal business. KELLY: So she's saying - she's accusing YouTube of trying to weaken activists like her, of trying to censor or filter her in some way. What do we make of this complaint? SYDELL: Well, this is a pretty extreme example of - and very troubled example - of anger at YouTube that a lot of creators on the site have been complaining about. They recently changed a policy about what kinds of videos qualify to get ads run against them and how they rank videos and search results. And YouTube is in a tough position. They're trying to keep violent, hateful content off their site and keep the platform as open as possible. And every day, there are millions of videos being uploaded to this site. The company's tried to balance the right of people to express themself with complaints about violent, objectionable videos, fake news, Russian propaganda. And, you know, all the social media companies now are facing this same dilemma. KELLY: That's NPR's Laura Sydell. Thanks very much. SYDELL: You're welcome.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-05-599761391": {"title": "Facebook's Sheryl Sandberg On Cambridge Analytica, Protecting User Data : NPR", "url": "https://www.npr.org/2018/04/05/599761391/full-transcript-facebook-coo-sheryl-sandberg-on-protecting-user-data", "author": "No author found", "published_date": "2018-04-05", "content": "", "section": "Technology", "disclaimer": ""}, "2018-04-05-599997683": {"title": "Facebook Will Notify 87M Users Whose Data May Have Been Used By Cambridge Analytica : NPR", "url": "https://www.npr.org/2018/04/05/599997683/facebook-will-notify-87m-users-whose-data-may-have-been-used-by-cambridge-analyt", "author": "No author found", "published_date": "2018-04-05", "content": "AUDIE CORNISH, HOST:  On Monday, users of Facebook will see a notification at the top of their news feeds. The social media company's under pressure for just how much it let other companies use the information of millions of Facebook users, so it will tell people how to have their information spread to fewer places. Today the company's chief operating officer added this. Sheryl Sandberg tells NPR that Facebook will begin notifying the 87 million individuals whose information may have gone to Cambridge Analytica. Most never consented to give up that data. The company later worked for the election of President Trump. Sandberg spoke with Steve Inskeep of NPR's Morning Edition. He's on the line now from Menlo Park, Calif. And, Steve, to begin, why take this step? STEVE INSKEEP, BYLINE: The company is under a lot of pressure, Audie, to be more transparent. They've had one awkward disclosure after another just about Cambridge Analytica. Just this week, the number of people whose data may have been compromised went from 50 million to 87 million, although Sheryl Sandberg went on to emphasize she's not really even sure about that. But it's clear that this is a vast company that's now under vast pressure for a series of stories and scandals, of which this is only the latest. CORNISH: So they're not even sure about the number, and yet they're saying each one of these individuals will learn somehow that their data has been compromised. INSKEEP: Yeah, yeah, will learn in a personal and direct way beginning on Monday. All 87 million won't hear on Monday apparently. But they're going to begin putting these notifications to these individuals that if Cambridge Analytica got your data, supposedly you will be told. And this is one of many announcements they've made in recent days that really get to the heart of the contradictions of Facebook. It's a huge company that says it's going to give individuals tremendous freedom and power to connect with others, but Facebook itself has ended up having immense power of course. And Mark Zuckerberg, the founder, faces questions before Congress next week. CORNISH: These are folks who don't normally make themselves available to talk, right? Sandberg is another one of the top people there. How did she talk about this past year of public relations disasters for the company? INSKEEP: Oh, my, gosh, with lots of regret, repeatedly said that she and other executives were responsible for failures and didn't do enough. And they've been criticized for ignoring the demands to protect people's privacy for years. And we talked about that. Let's listen to some. I'm curious. When you're talking with Mark Zuckerberg or whoever else you may talk with around this company, have you had moments when you've asked the question, are we as a company too powerful? SHERYL SANDBERG: It's an important question. And people have that question about us and others particularly as our size and scope - and we've had a lot of long and thoughtful conversations about what that means. We know that a lot of regulators have that question. We know that consumers around the world have that question. INSKEEP: Do you take it seriously, or does it seem ridiculous to you? SANDBERG: Oh, we take it very seriously. We've always had a deep responsibility for people. But at our size and scope with billions of people using our products, we have a very deep responsibility. We're having conversations with regulators around the world. But we're not even waiting for regulation. INSKEEP: Given that the Federal Trade Commission reached a consent agreement with Facebook in 2011 to better protect people's privacy, should you have taken these steps years ago? SANDBERG: Well, we're in constant conversation with the FTC. And that consent decree was important, and we've taken every step we know how to make sure we're in accordance with it. But the bigger answer is, should we have taken these steps years ago anyway? And the answer to that is yes - like, a very clear, a very firm yes. We really believed in social experiences. We really believed in protecting privacy. But we were way too idealistic. We did not think enough about the abuse cases. INSKEEP: And Sandberg is trying to prove that her company's attitude has changed, Audie. CORNISH: NPR's Steve Inskeep - we'll hear more of that interview tomorrow on Morning Edition. Steve, thanks so much. INSKEEP: Glad to do it. AUDIE CORNISH, HOST:   On Monday, users of Facebook will see a notification at the top of their news feeds. The social media company's under pressure for just how much it let other companies use the information of millions of Facebook users, so it will tell people how to have their information spread to fewer places. Today the company's chief operating officer added this. Sheryl Sandberg tells NPR that Facebook will begin notifying the 87 million individuals whose information may have gone to Cambridge Analytica. Most never consented to give up that data. The company later worked for the election of President Trump. Sandberg spoke with Steve Inskeep of NPR's Morning Edition. He's on the line now from Menlo Park, Calif. And, Steve, to begin, why take this step? STEVE INSKEEP, BYLINE: The company is under a lot of pressure, Audie, to be more transparent. They've had one awkward disclosure after another just about Cambridge Analytica. Just this week, the number of people whose data may have been compromised went from 50 million to 87 million, although Sheryl Sandberg went on to emphasize she's not really even sure about that. But it's clear that this is a vast company that's now under vast pressure for a series of stories and scandals, of which this is only the latest. CORNISH: So they're not even sure about the number, and yet they're saying each one of these individuals will learn somehow that their data has been compromised. INSKEEP: Yeah, yeah, will learn in a personal and direct way beginning on Monday. All 87 million won't hear on Monday apparently. But they're going to begin putting these notifications to these individuals that if Cambridge Analytica got your data, supposedly you will be told. And this is one of many announcements they've made in recent days that really get to the heart of the contradictions of Facebook. It's a huge company that says it's going to give individuals tremendous freedom and power to connect with others, but Facebook itself has ended up having immense power of course. And Mark Zuckerberg, the founder, faces questions before Congress next week. CORNISH: These are folks who don't normally make themselves available to talk, right? Sandberg is another one of the top people there. How did she talk about this past year of public relations disasters for the company? INSKEEP: Oh, my, gosh, with lots of regret, repeatedly said that she and other executives were responsible for failures and didn't do enough. And they've been criticized for ignoring the demands to protect people's privacy for years. And we talked about that. Let's listen to some. I'm curious. When you're talking with Mark Zuckerberg or whoever else you may talk with around this company, have you had moments when you've asked the question, are we as a company too powerful? SHERYL SANDBERG: It's an important question. And people have that question about us and others particularly as our size and scope - and we've had a lot of long and thoughtful conversations about what that means. We know that a lot of regulators have that question. We know that consumers around the world have that question. INSKEEP: Do you take it seriously, or does it seem ridiculous to you? SANDBERG: Oh, we take it very seriously. We've always had a deep responsibility for people. But at our size and scope with billions of people using our products, we have a very deep responsibility. We're having conversations with regulators around the world. But we're not even waiting for regulation. INSKEEP: Given that the Federal Trade Commission reached a consent agreement with Facebook in 2011 to better protect people's privacy, should you have taken these steps years ago? SANDBERG: Well, we're in constant conversation with the FTC. And that consent decree was important, and we've taken every step we know how to make sure we're in accordance with it. But the bigger answer is, should we have taken these steps years ago anyway? And the answer to that is yes - like, a very clear, a very firm yes. We really believed in social experiences. We really believed in protecting privacy. But we were way too idealistic. We did not think enough about the abuse cases. INSKEEP: And Sandberg is trying to prove that her company's attitude has changed, Audie. CORNISH: NPR's Steve Inskeep - we'll hear more of that interview tomorrow on Morning Edition. Steve, thanks so much. INSKEEP: Glad to do it.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-05-599895248": {"title": "What Facebook Is Changing About Its Data-Sharing Practices : NPR", "url": "https://www.npr.org/2018/04/05/599895248/what-facebook-is-changing-about-its-data-sharing-practices", "author": "No author found", "published_date": "2018-04-05", "content": "MARY LOUISE KELLY, HOST:  Facebook had a big day yesterday. In one breath, it acknowledged that 87 million people may have had their personal data shared improperly with the political firm Cambridge Analytica. Facebook also announced some changes to its data-sharing practices. All this comes a week before CEO, Mark Zuckerberg, is set to testify before multiple congressional committees here in Washington. Joining us to talk about Facebook's new policies, Shira Ovide. She's a tech columnist for Bloomberg. Hey there. SHIRA OVIDE: Hello. KELLY: So Facebook says it's going to do some things differently - protect people's data better. Walk me through the, you know, top one or two significant changes as you see them. OVIDE: Right, so I think the broad category of changes Facebook made was to change the relationship with the outside companies - like \"Words With Friends\" or like Tinder - that mesh with your Facebook account. KELLY: This is things like, when I want to order dinner from some online company, and they say, do you want to open an account with us, or do you want to just login through Facebook? OVIDE: That's exactly it. KELLY: Got it. OK. OVIDE: And Facebook has a number of these outside companies that they've agreed to allow to use your Facebook account details to login. And Facebook now says they're going to tighten the rules around that. And they're going to decide on a case-by-case basis when to allow those outside companies to let you use your Facebook login, the idea being that that will restrict the number of companies that have visibility into all of the data that you have on Facebook. KELLY: This question of whether Facebook will be a good arbiter in deciding which companies to grant access to our personal information - I think there are a lot of legitimate questions being raised about that right now. OVIDE: I agree. That's a big concern of mine as well that - look, in a sense, Facebook is giving itself more responsibility and more power. We've seen over the last 18 months that Facebook is not always a responsible steward of information and does not always make good decisions about what relationships companies should have with Facebook users. KELLY: And you said two big changes. What's the other one? OVIDE: Facebook has this feature called reverse look-up that lets you use somebody's telephone number or email address to find them on Facebook. But Facebook said that companies abuse that feature basically to do wholesale scraping of user information. And the company said on Wednesday that it found probably all of Facebook's two-billion-plus users have had their accounts scraped in that way, using the reverse look-up feature. So it's turning it off. KELLY: I mean, you've reported on Facebook for a long time. How significant are these changes? How far do they go in addressing all of these questions that have been raised about protecting our data? OVIDE: So I would say it matters to a point. What Facebook hasn't done is really restrict or limit the amount of data that Facebook itself collects about people. It is putting some locks on what outsiders can do, and that's good. But Facebook still collects reams of data about you from not just what you do on Facebook but from what you do in the real world - from purchases that you make, for example, from other websites that you visit all over the Internet. So Facebook really hasn't committed to limiting much at all that kind of information that Facebook itself is collecting. KELLY: Well, so then how reassured should we be by these changes? How much safer is our data on Facebook than it was a week ago before these changes or say a year ago? OVIDE: I do think the company has learned a lesson. Again, though, the company's whole business model and ethos is built around open communication, widespread data collection and using that data to target people based on their interests or activities. KELLY: That's the inherent tension here, right? OVIDE: Correct. And it's one of the best businesses ever built on the Internet, but it may also be an indefensible business. KELLY: Bloomberg tech columnist Shira Ovide, thanks so much. OVIDE: Thank you. MARY LOUISE KELLY, HOST:   Facebook had a big day yesterday. In one breath, it acknowledged that 87 million people may have had their personal data shared improperly with the political firm Cambridge Analytica. Facebook also announced some changes to its data-sharing practices. All this comes a week before CEO, Mark Zuckerberg, is set to testify before multiple congressional committees here in Washington. Joining us to talk about Facebook's new policies, Shira Ovide. She's a tech columnist for Bloomberg. Hey there. SHIRA OVIDE: Hello. KELLY: So Facebook says it's going to do some things differently - protect people's data better. Walk me through the, you know, top one or two significant changes as you see them. OVIDE: Right, so I think the broad category of changes Facebook made was to change the relationship with the outside companies - like \"Words With Friends\" or like Tinder - that mesh with your Facebook account. KELLY: This is things like, when I want to order dinner from some online company, and they say, do you want to open an account with us, or do you want to just login through Facebook? OVIDE: That's exactly it. KELLY: Got it. OK. OVIDE: And Facebook has a number of these outside companies that they've agreed to allow to use your Facebook account details to login. And Facebook now says they're going to tighten the rules around that. And they're going to decide on a case-by-case basis when to allow those outside companies to let you use your Facebook login, the idea being that that will restrict the number of companies that have visibility into all of the data that you have on Facebook. KELLY: This question of whether Facebook will be a good arbiter in deciding which companies to grant access to our personal information - I think there are a lot of legitimate questions being raised about that right now. OVIDE: I agree. That's a big concern of mine as well that - look, in a sense, Facebook is giving itself more responsibility and more power. We've seen over the last 18 months that Facebook is not always a responsible steward of information and does not always make good decisions about what relationships companies should have with Facebook users. KELLY: And you said two big changes. What's the other one? OVIDE: Facebook has this feature called reverse look-up that lets you use somebody's telephone number or email address to find them on Facebook. But Facebook said that companies abuse that feature basically to do wholesale scraping of user information. And the company said on Wednesday that it found probably all of Facebook's two-billion-plus users have had their accounts scraped in that way, using the reverse look-up feature. So it's turning it off. KELLY: I mean, you've reported on Facebook for a long time. How significant are these changes? How far do they go in addressing all of these questions that have been raised about protecting our data? OVIDE: So I would say it matters to a point. What Facebook hasn't done is really restrict or limit the amount of data that Facebook itself collects about people. It is putting some locks on what outsiders can do, and that's good. But Facebook still collects reams of data about you from not just what you do on Facebook but from what you do in the real world - from purchases that you make, for example, from other websites that you visit all over the Internet. So Facebook really hasn't committed to limiting much at all that kind of information that Facebook itself is collecting. KELLY: Well, so then how reassured should we be by these changes? How much safer is our data on Facebook than it was a week ago before these changes or say a year ago? OVIDE: I do think the company has learned a lesson. Again, though, the company's whole business model and ethos is built around open communication, widespread data collection and using that data to target people based on their interests or activities. KELLY: That's the inherent tension here, right? OVIDE: Correct. And it's one of the best businesses ever built on the Internet, but it may also be an indefensible business. KELLY: Bloomberg tech columnist Shira Ovide, thanks so much. OVIDE: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-05-599723063": {"title": "Facebook Says 87 Million Users' Data May Have Been Improperly Accessed  : NPR", "url": "https://www.npr.org/2018/04/05/599723063/facebook-says-87-million-users-data-may-have-been-improperly-accessed", "author": "No author found", "published_date": "2018-04-05", "content": "RACHEL MARTIN, HOST: We are learning more about Facebook's struggle over the control of users' data. According to the tech giant, malicious actors took advantage of Facebook's search tools to collect information on users. Facebook also says the data grabbed by Cambridge Analytica may be much bigger than first reported. It announced that the data analytics firm that was used by the Trump campaign in 2016 could have improperly accessed up to 87 million users and data from those users. Facebook CEO Mark Zuckerberg has now agreed to testify before committees in both the House and the Senate. He also addressed the issues in a conference call with reporters yesterday. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: It's clear now that we didn't do enough. We didn't focus enough on preventing abuse and thinking through how people could use these tools to do harm as well. And that goes for fake news, foreign interference in elections, in addition to developers and data privacy. MARTIN: I'm joined now by David Ingram. He's a tech correspondent for Reuters. David, thanks for being with us. DAVID INGRAM: My pleasure. MARTIN: So Mark Zuckerberg doesn't talk a lot to the media, so the fact that he held this conference call with reporters indicates just how serious this is for the company and the level of damage control they're doing right now, right? INGRAM: That's correct. He's been doing a series of media interviews. His No. 2, Sheryl Sandberg - who, in her own right, is a famous American business executive - is doing interviews later this week, as well. And in these interviews, they are apologizing. They are saying that they are going to notify these 87 million people who had their data grabbed and make other changes to try to protect privacy. MARTIN: Before we get to those changes, I mean, what about the data that's already been abused by Cambridge Analytica? I mean, what do they tell the users who've had that data violated? INGRAM: So they're going to tell them that their data may have been harvested by this consultancy, Cambridge Analytica, and the academic they were working with to get all these Facebook profiles. But beyond that, there's really not much that Facebook can say. According. . . MARTIN: They can't get it back. They can't ensure that it's not being used in another way. INGRAM: Exactly. The cat is out of the barn. And once data is - has left Facebook or any other place, it is nearly impossible to track it down or ensure that it has been deleted. MARTIN: So, I mean, what can they do to make sure that this doesn't happen again? What kind of solutions are they talking about? INGRAM: Well, so we're really talking about apps that sort of build onto Facebook that you connect to your Facebook profile. There are thousands of those out there. And they are clamping down and have been since 2015 on the kinds of data that those apps can get about you and about your friends. The key period we're talking about here was 2010 to 2015, when Facebook's rules on this were relatively lax. MARTIN: So, I mean, some have suggested that Zuckerberg's leadership of Facebook is in question now. I mean, did you get a sense on this call that he is trying to defend himself personally? INGRAM: He was asked about this, I believe, at least twice on the call, about whether he was the right person to lead Facebook or whether his corporate board was discussing stepping - him stepping down. That is almost impossible to imagine, since he founded the company and is still, to this day, the controlling shareholder. So he has complete control over who is on the board and who gets to be the CEO. So if he is no longer the CEO, it'll be because of his decision. MARTIN: Right. So he's going to go to Capitol Hill. He has conceded that he will come and ask - and answer questions from lawmakers. Where do you think that line of questioning is going to go? INGRAM: Well, I think we're going to hear from some lawmakers who really want to create more of a European-style legal structure in the United States to protect people's personal information. And they're going to press Mark Zuckerberg to try to get his support behind something like that. There's a new European law taking effect in May. But I think even among senators who aren't going to support going that far, there's just a sense that Facebook has gotten so enormous and there doesn't seem to be someone who's really accountable outside of Mark Zuckerberg, and he has complete control of the company, of course. MARTIN: You know, Tim Wu of Columbia University wrote about this recently, that it's kind of surprising there's not any competition for Facebook. You would think that at this point, some other companies would say, we can do this better; we can provide this service and give you more privacy. INGRAM: It is surprising, and that's relatively new. I mean, I think a few years ago, we were talking about Twitter as a competitor to Facebook or Snapchat, but Facebook has grown by leaps and bounds over the past few years. You could consider Google to be your competitor in some respects, but it is going to be really hard for any other company to get to the 2-billion-user level that Facebook is at right now. MARTIN: Right. David Ingram - he's a tech reporter with Reuters news. He joined us on Skype this morning. David, thank you so much. INGRAM: My pleasure. (SOUNDBITE OF PHILANTHROPE'S \"CITY LIGHTS\") RACHEL MARTIN, HOST:  We are learning more about Facebook's struggle over the control of users' data. According to the tech giant, malicious actors took advantage of Facebook's search tools to collect information on users. Facebook also says the data grabbed by Cambridge Analytica may be much bigger than first reported. It announced that the data analytics firm that was used by the Trump campaign in 2016 could have improperly accessed up to 87 million users and data from those users. Facebook CEO Mark Zuckerberg has now agreed to testify before committees in both the House and the Senate. He also addressed the issues in a conference call with reporters yesterday. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: It's clear now that we didn't do enough. We didn't focus enough on preventing abuse and thinking through how people could use these tools to do harm as well. And that goes for fake news, foreign interference in elections, in addition to developers and data privacy. MARTIN: I'm joined now by David Ingram. He's a tech correspondent for Reuters. David, thanks for being with us. DAVID INGRAM: My pleasure. MARTIN: So Mark Zuckerberg doesn't talk a lot to the media, so the fact that he held this conference call with reporters indicates just how serious this is for the company and the level of damage control they're doing right now, right? INGRAM: That's correct. He's been doing a series of media interviews. His No. 2, Sheryl Sandberg - who, in her own right, is a famous American business executive - is doing interviews later this week, as well. And in these interviews, they are apologizing. They are saying that they are going to notify these 87 million people who had their data grabbed and make other changes to try to protect privacy. MARTIN: Before we get to those changes, I mean, what about the data that's already been abused by Cambridge Analytica? I mean, what do they tell the users who've had that data violated? INGRAM: So they're going to tell them that their data may have been harvested by this consultancy, Cambridge Analytica, and the academic they were working with to get all these Facebook profiles. But beyond that, there's really not much that Facebook can say. According. . . MARTIN: They can't get it back. They can't ensure that it's not being used in another way. INGRAM: Exactly. The cat is out of the barn. And once data is - has left Facebook or any other place, it is nearly impossible to track it down or ensure that it has been deleted. MARTIN: So, I mean, what can they do to make sure that this doesn't happen again? What kind of solutions are they talking about? INGRAM: Well, so we're really talking about apps that sort of build onto Facebook that you connect to your Facebook profile. There are thousands of those out there. And they are clamping down and have been since 2015 on the kinds of data that those apps can get about you and about your friends. The key period we're talking about here was 2010 to 2015, when Facebook's rules on this were relatively lax. MARTIN: So, I mean, some have suggested that Zuckerberg's leadership of Facebook is in question now. I mean, did you get a sense on this call that he is trying to defend himself personally? INGRAM: He was asked about this, I believe, at least twice on the call, about whether he was the right person to lead Facebook or whether his corporate board was discussing stepping - him stepping down. That is almost impossible to imagine, since he founded the company and is still, to this day, the controlling shareholder. So he has complete control over who is on the board and who gets to be the CEO. So if he is no longer the CEO, it'll be because of his decision. MARTIN: Right. So he's going to go to Capitol Hill. He has conceded that he will come and ask - and answer questions from lawmakers. Where do you think that line of questioning is going to go? INGRAM: Well, I think we're going to hear from some lawmakers who really want to create more of a European-style legal structure in the United States to protect people's personal information. And they're going to press Mark Zuckerberg to try to get his support behind something like that. There's a new European law taking effect in May. But I think even among senators who aren't going to support going that far, there's just a sense that Facebook has gotten so enormous and there doesn't seem to be someone who's really accountable outside of Mark Zuckerberg, and he has complete control of the company, of course. MARTIN: You know, Tim Wu of Columbia University wrote about this recently, that it's kind of surprising there's not any competition for Facebook. You would think that at this point, some other companies would say, we can do this better; we can provide this service and give you more privacy. INGRAM: It is surprising, and that's relatively new. I mean, I think a few years ago, we were talking about Twitter as a competitor to Facebook or Snapchat, but Facebook has grown by leaps and bounds over the past few years. You could consider Google to be your competitor in some respects, but it is going to be really hard for any other company to get to the 2-billion-user level that Facebook is at right now. MARTIN: Right. David Ingram - he's a tech reporter with Reuters news. He joined us on Skype this morning. David, thank you so much. INGRAM: My pleasure. (SOUNDBITE OF PHILANTHROPE'S \"CITY LIGHTS\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-07-600482813": {"title": "The Ethics Of Tech : NPR", "url": "https://www.npr.org/2018/04/07/600482813/the-ethics-of-tech", "author": "No author found", "published_date": "2018-04-07", "content": "SCOTT DETROW, HOST: Facebook CEO Mark Zuckerberg will be facing tough questions when he appears before Congress in the coming days. At the top of the list, the scandal involving Cambridge Analytica. That's the company that's been accused of improperly obtaining data from millions of Facebook users, then using that information for its work on political campaigns, reportedly including the Trump campaign. Let's hear now from someone with a long history in Silicon Valley. Yonatan Zunger, the former Google engineer, recently wrote in The Boston Globe that this scandal is just more evidence that the entire tech industry faces an ethical crisis. YONATAN ZUNGER: The method by which Cambridge Analytica got the data from Facebook was a system Facebook built almost specifically for the purpose of making it easy for companies to harvest information about networks of individuals. DETROW: Right. And you write that over and over again throughout history, and also in recent years, in the tech field, companies work on something with a specific intention, and then the product is used with - only a slightly degree off from that intention in a way that nobody thought about and causes a lot of harm. ZUNGER: Absolutely. Something I always tell people is that any idiot can build a system. Any amateur can make it perform. Professionals think about how a system will fail. It's very common for people to think about how a system will work if it's used the way they imagine it, but they don't think about how that system might work if it were used by a bad actor, or it could be used by just a perfectly ordinary person who's just a little different from what the person designing it is like. DETROW: How do companies have those conversations like you mentioned about the downsides of the services they're coming up with? ZUNGER: This is the single most important thing that most companies can be doing right now. First and foremost, companies need to pay attention. And, in fact, individuals working at these companies need to be thinking about how each product could actually be used in the real world. If you build a product that works great for men and is going to lead to harassment of women, you have a problem. If you build a product that makes everyone's address books 5 percent more efficient and then gets three people killed because it happened to leak their personal information to their stalker, that's a problem. What you need is a very diverse working group that can recognize a wide range of problems, that knows which questions to ask and has the support both inside the company and in the broader community to surface these issues and make sure that they're taken seriously and considered as genuine safety issues before a product is released to the public, as well as after. This is different from a traditional compliance function, where they come in at the very end and say, no, I'm sorry. You can't launch this, at which point a business leader is just going to say, well, we need to launch it and it's too late to change it. Because they were in there from the room from day one, it makes a huge difference. DETROW: A lot of people would say, especially here in Washington, where we are, that the answer could be federal regulation. Do you think that's the right way? ZUNGER: I think regulation has a place, but it's important to handle it very carefully. In particular, everyone agrees that building codes are a great idea, and I think most people also agree that our elected representatives are not the right people to decide what kind of insulation is appropriate for use in the garage. What you want is regulation and other mandatory mechanisms, like ethics standards or review boards or whatever processes you have, that specifies goals and objectives, which we can discuss as a society. And then the actual translation of that into implementation is something that should be done by people who deeply understand the field. DETROW: Well, Yonatan Zunger, formerly of Google. Now he works for the tech company Humu. Thank you so much for joining us. ZUNGER: Thank you very much. SCOTT DETROW, HOST:  Facebook CEO Mark Zuckerberg will be facing tough questions when he appears before Congress in the coming days. At the top of the list, the scandal involving Cambridge Analytica. That's the company that's been accused of improperly obtaining data from millions of Facebook users, then using that information for its work on political campaigns, reportedly including the Trump campaign. Let's hear now from someone with a long history in Silicon Valley. Yonatan Zunger, the former Google engineer, recently wrote in The Boston Globe that this scandal is just more evidence that the entire tech industry faces an ethical crisis. YONATAN ZUNGER: The method by which Cambridge Analytica got the data from Facebook was a system Facebook built almost specifically for the purpose of making it easy for companies to harvest information about networks of individuals. DETROW: Right. And you write that over and over again throughout history, and also in recent years, in the tech field, companies work on something with a specific intention, and then the product is used with - only a slightly degree off from that intention in a way that nobody thought about and causes a lot of harm. ZUNGER: Absolutely. Something I always tell people is that any idiot can build a system. Any amateur can make it perform. Professionals think about how a system will fail. It's very common for people to think about how a system will work if it's used the way they imagine it, but they don't think about how that system might work if it were used by a bad actor, or it could be used by just a perfectly ordinary person who's just a little different from what the person designing it is like. DETROW: How do companies have those conversations like you mentioned about the downsides of the services they're coming up with? ZUNGER: This is the single most important thing that most companies can be doing right now. First and foremost, companies need to pay attention. And, in fact, individuals working at these companies need to be thinking about how each product could actually be used in the real world. If you build a product that works great for men and is going to lead to harassment of women, you have a problem. If you build a product that makes everyone's address books 5 percent more efficient and then gets three people killed because it happened to leak their personal information to their stalker, that's a problem. What you need is a very diverse working group that can recognize a wide range of problems, that knows which questions to ask and has the support both inside the company and in the broader community to surface these issues and make sure that they're taken seriously and considered as genuine safety issues before a product is released to the public, as well as after. This is different from a traditional compliance function, where they come in at the very end and say, no, I'm sorry. You can't launch this, at which point a business leader is just going to say, well, we need to launch it and it's too late to change it. Because they were in there from the room from day one, it makes a huge difference. DETROW: A lot of people would say, especially here in Washington, where we are, that the answer could be federal regulation. Do you think that's the right way? ZUNGER: I think regulation has a place, but it's important to handle it very carefully. In particular, everyone agrees that building codes are a great idea, and I think most people also agree that our elected representatives are not the right people to decide what kind of insulation is appropriate for use in the garage. What you want is regulation and other mandatory mechanisms, like ethics standards or review boards or whatever processes you have, that specifies goals and objectives, which we can discuss as a society. And then the actual translation of that into implementation is something that should be done by people who deeply understand the field. DETROW: Well, Yonatan Zunger, formerly of Google. Now he works for the tech company Humu. Thank you so much for joining us. ZUNGER: Thank you very much.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-07-600138358": {"title": "Analysis: In Trump's Twitter Feed, A Tale Of Sound And Fury  : NPR", "url": "https://www.npr.org/2018/04/07/600138358/analysis-in-trumps-twitter-feed-a-tale-of-sound-and-fury", "author": "No author found", "published_date": "2018-04-07", "content": "", "section": "The New Clash Between Free Speech And Privacy", "disclaimer": ""}, "2018-04-08-600616534": {"title": "Facebook CEO Mark Zuckerberg Readies To Testify On Capitol Hill : NPR", "url": "https://www.npr.org/2018/04/08/600616534/facebook-ceo-mark-zuckerberg-readies-to-testify-on-capitol-hill", "author": "No author found", "published_date": "2018-04-08", "content": "LULU GARCIA-NAVARRO, HOST: Facebook CEO Mark Zuckerberg is set to testify on Capitol Hill this week. And it's a big deal for a company considered a tech darling not so long ago. It's been only three weeks since reports emerged about a political data firm getting access to information on tens of millions of Facebook users. But a lot has happened in those three weeks. NPR's Alina Selyukh walks us through it. ALINA SELYUKH, BYLINE: The news stories exploded on March 17 in the New York Times, working with British media. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #1: U. K. data firm Cambridge Analytica issued a statement. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #2: Cambridge Analytica. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #3: Cambridge Analytica drilled deep. (SOUNDBITE OF ARCHIVED BROADCAST)DAVID GREENE, BYLINE: Questions are mounting about Facebook's role. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #4: It may have mishandled data from more than 50 million. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #5: Fifty million. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #6: Fifty million Facebook users. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #7: Fifty million-plus. SELYUKH: The British political consulting firm got its hands on Facebook data of millions of people a few years back. Some of the people had participated in a personality quiz. But most of them just happened to be friends of the quiz-takers. This news broke in the middle of another long-running Facebook crisis, the company's role in the 2016 election. Facebook had said more than 140 million people may have been exposed to Russia-linked propaganda during the campaign season. After years of presenting itself as this open and neutral platform for all ideas, Facebook faced accusations of profiting from divisive ads and the spread of bogus stories. (SOUNDBITE OF ARCHIVED RECORDING)RON WYDEN: Fake users posting stories on Facebook, videos on YouTube, links on Twitter can be used by foreign and domestic enemies to undermine our society. SELYUKH: That's Democratic Senator Ron Wyden at a November hearing laying into Facebook's top lawyer. (SOUNDBITE OF ARCHIVED RECORDING)WYDEN: You need to stop paying lip service to shutting down bad actors using these accounts. SELYUKH: The Cambridge Analytica story landed with a thud. It was personal. Reports said some 50 million users had their data scooped up - much of it without their permission. Plus, it was political. In secret videos, a Cambridge Analytica executive boasted that his firm helped Donald Trump win the election - though the company officially denies using the data in the 2016 election. Facebook responded that they banned Cambridge Analytica and had already put in place restrictions on those kinds of data scoops years ago. But Mark Zuckerberg for days remained silent. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #8: How has Mark Zuckerberg avoided the spotlight? (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #9: Zuckerberg released a powerful denial saying. . . (SOUNDBITE OF WIND)SELYUKH: In response, people began a boycott movement - #deleteFacebook - and painted dystopian pictures of the future where all of our lives are controlled by tech giants - you know, like Tom Hanks in that movie \"The Circle. \"(SOUNDBITE OF FILM, \"THE CIRCLE\")TOM HANKS: We will see it all because knowing is good. But knowing everything is better. (APPLAUSE)SELYUKH: Four days after the story broke, Zuckerberg emerged with a Facebook post and a rare round of interviews, including CNN. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: This was a major breach of trust. And I'm really sorry that this happened. You know, we have a basic responsibility to protect people's data. And if we can't do that, then we don't deserve to have the opportunity to serve people. SELYUKH: Meanwhile, Facebook's share price was spiraling downward. The Federal Trade Commission said it was investigating Facebook's privacy practices. Lawmakers were summoning Zuckerberg to Washington. Here's Democratic Senator Ed Markey on NPR's Morning Edition. (SOUNDBITE OF ARCHIVED BROADCAST)ED MARKEY: The CEO of Facebook has to come in so that we can ask him the questions which the American people want to have the answer to. How did Facebook allow this to happen? SELYUKH: Ten days after the scandal broke, a news report suggested Zuckerberg agreed to testify. And Facebook came out with a new list of changes to the website - for example, making it easier for users to see what information they share with apps. But last Wednesday came a new twist to the story. Facebook said the number of users affected by the Cambridge Analytica data grab was 87 million not 50 million. Facebook dispatched executive Sheryl Sandberg for a new round of interviews, including with NPR, once again apologizing and pledging to do better. (SOUNDBITE OF ARCHIVED BROADCAST)SHERYL SANDBERG: Starting Monday, we're going to start rolling out to everyone in the world, right on the top of their news feed, a place where you can see all the apps you've shared your data with and a really easy way to delete them. SELYUKH: These simplified controls are among the numerous privacy changes Facebook has delivered over the years - often in response to scandals or complaints. Some critics would say Facebook's strategy is an effort to stave off regulations. But in that first CNN interview, Zuckerberg said something unprecedented about this. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: I actually am not sure we shouldn't be regulated. I actually think the question is more what is the right regulation rather than - yes or no - should it be regulated? SELYUKH: And that's the question many lawmakers will ask when Zuckerberg comes to Washington this week. Alina Selyukh, NPR News. LULU GARCIA-NAVARRO, HOST:  Facebook CEO Mark Zuckerberg is set to testify on Capitol Hill this week. And it's a big deal for a company considered a tech darling not so long ago. It's been only three weeks since reports emerged about a political data firm getting access to information on tens of millions of Facebook users. But a lot has happened in those three weeks. NPR's Alina Selyukh walks us through it. ALINA SELYUKH, BYLINE: The news stories exploded on March 17 in the New York Times, working with British media. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #1: U. K. data firm Cambridge Analytica issued a statement. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #2: Cambridge Analytica. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #3: Cambridge Analytica drilled deep. (SOUNDBITE OF ARCHIVED BROADCAST) DAVID GREENE, BYLINE: Questions are mounting about Facebook's role. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #4: It may have mishandled data from more than 50 million. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #5: Fifty million. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #6: Fifty million Facebook users. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #7: Fifty million-plus. SELYUKH: The British political consulting firm got its hands on Facebook data of millions of people a few years back. Some of the people had participated in a personality quiz. But most of them just happened to be friends of the quiz-takers. This news broke in the middle of another long-running Facebook crisis, the company's role in the 2016 election. Facebook had said more than 140 million people may have been exposed to Russia-linked propaganda during the campaign season. After years of presenting itself as this open and neutral platform for all ideas, Facebook faced accusations of profiting from divisive ads and the spread of bogus stories. (SOUNDBITE OF ARCHIVED RECORDING) RON WYDEN: Fake users posting stories on Facebook, videos on YouTube, links on Twitter can be used by foreign and domestic enemies to undermine our society. SELYUKH: That's Democratic Senator Ron Wyden at a November hearing laying into Facebook's top lawyer. (SOUNDBITE OF ARCHIVED RECORDING) WYDEN: You need to stop paying lip service to shutting down bad actors using these accounts. SELYUKH: The Cambridge Analytica story landed with a thud. It was personal. Reports said some 50 million users had their data scooped up - much of it without their permission. Plus, it was political. In secret videos, a Cambridge Analytica executive boasted that his firm helped Donald Trump win the election - though the company officially denies using the data in the 2016 election. Facebook responded that they banned Cambridge Analytica and had already put in place restrictions on those kinds of data scoops years ago. But Mark Zuckerberg for days remained silent. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #8: How has Mark Zuckerberg avoided the spotlight? (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #9: Zuckerberg released a powerful denial saying. . . (SOUNDBITE OF WIND) SELYUKH: In response, people began a boycott movement - #deleteFacebook - and painted dystopian pictures of the future where all of our lives are controlled by tech giants - you know, like Tom Hanks in that movie \"The Circle. \" (SOUNDBITE OF FILM, \"THE CIRCLE\") TOM HANKS: We will see it all because knowing is good. But knowing everything is better. (APPLAUSE) SELYUKH: Four days after the story broke, Zuckerberg emerged with a Facebook post and a rare round of interviews, including CNN. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: This was a major breach of trust. And I'm really sorry that this happened. You know, we have a basic responsibility to protect people's data. And if we can't do that, then we don't deserve to have the opportunity to serve people. SELYUKH: Meanwhile, Facebook's share price was spiraling downward. The Federal Trade Commission said it was investigating Facebook's privacy practices. Lawmakers were summoning Zuckerberg to Washington. Here's Democratic Senator Ed Markey on NPR's Morning Edition. (SOUNDBITE OF ARCHIVED BROADCAST) ED MARKEY: The CEO of Facebook has to come in so that we can ask him the questions which the American people want to have the answer to. How did Facebook allow this to happen? SELYUKH: Ten days after the scandal broke, a news report suggested Zuckerberg agreed to testify. And Facebook came out with a new list of changes to the website - for example, making it easier for users to see what information they share with apps. But last Wednesday came a new twist to the story. Facebook said the number of users affected by the Cambridge Analytica data grab was 87 million not 50 million. Facebook dispatched executive Sheryl Sandberg for a new round of interviews, including with NPR, once again apologizing and pledging to do better. (SOUNDBITE OF ARCHIVED BROADCAST) SHERYL SANDBERG: Starting Monday, we're going to start rolling out to everyone in the world, right on the top of their news feed, a place where you can see all the apps you've shared your data with and a really easy way to delete them. SELYUKH: These simplified controls are among the numerous privacy changes Facebook has delivered over the years - often in response to scandals or complaints. Some critics would say Facebook's strategy is an effort to stave off regulations. But in that first CNN interview, Zuckerberg said something unprecedented about this. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: I actually am not sure we shouldn't be regulated. I actually think the question is more what is the right regulation rather than - yes or no - should it be regulated? SELYUKH: And that's the question many lawmakers will ask when Zuckerberg comes to Washington this week. Alina Selyukh, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-09-600866342": {"title": "The Rise \u2014 And Stall \u2014 Of Facebook Founder Mark Zuckerberg : NPR", "url": "https://www.npr.org/2018/04/09/600866342/the-rise-and-stall-of-facebook-foundermark-zuckerberg", "author": "No author found", "published_date": "2018-04-09", "content": "AUDIE CORNISH, HOST: This week Facebook CEO Mark Zuckerberg tells Congress he's sorry. MARY LOUISE KELLY, HOST: He'll testify twice about how Facebook let data on up to 87 million users get into the hands of the political firm Cambridge Analytica. In prepared remarks, Zuckerberg says the company did not take a broad enough view of its responsibility. He says that was his mistake because he started Facebook and he runs it. CORNISH: But this is just the latest controversy for the 33-year-old billionaire who famously started the social network from his Harvard dorm room. For a look at Zuckerberg's trajectory we've called upon Mitch Kapor. He's a Silicon Valley entrepreneur himself. He now runs Kapor Capital and the Kapor Center for Social Impact with his wife Freada. Welcome to the program. MITCH KAPOR: Pleasure to be here. CORNISH: Now, Mark Zuckerberg in a way is like the original hoodie-wearing savant CEO. I mean, this was kind of the image of him when people talked about the Harvard dorm room. KAPOR: Absolutely. yes. CORNISH: Was it all that common, or was it even myth at the time? KAPOR: No, I think that he was very smart, technically minded undergraduate with a lot of ability to code and a bunch of ideas to try things, the first of which was to scrape data from Harvard servers without anybody's permission, and then asking other people at Harvard to rate the hotness of student pictures. CORNISH: It's interesting because when Facebook's origin story was then told in the 2010 film \"The Social Network,\" that - (laughter) I think that very - there was a scene that spoke to this. (SOUNDBITE OF FILM, \"THE SOCIAL NETWORK\")JESSE EISENBERG: (As Mark Zuckerberg) The Kirkland Facebook is open on my desktop, and some of these people have pretty horrendous Facebook pics. Billy Olson's sitting here and had the idea of putting some of the pictures next to pictures of farm animals and have people vote on who's hotter. CORNISH: Obviously this is not a documentary. But how did this change the way the public viewed Zuckerberg, do you think? KAPOR: Well, he became a kind of cultural icon as a result of the movie. And while obviously it wasn't entirely a flattering portrait, my sense is that the hero worship aspect of our culture fastened onto him even more tightly as a result of the exposure of the Zuckerberg character. CORNISH: There's also, as they say, some danger in believing your own press. And in the years after, we saw Mark Zuckerberg really seize the reins in terms of being the face of the company and embracing that role. Were there some pitfalls there? KAPOR: I think so in that when you're the king - and he really is the king - it's very difficult for your subjects to speak the truth to you. By becoming so visible and so much out there, I think it really made it even more difficult to have the kind of real internal dialogue that's needed to keep this hypergrowth company on some sort of track. CORNISH: Fast-forward a few years, and we have seen Mark Zuckerberg try to change the narrative - right? - around people. People stopped seeing him as so benevolent, frankly - right? - or as a naive kid, so to speak. And I remember when he and his wife promised to give away most of their wealth. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: You know, what does it take to make it so that people don't get sick anymore? Can we build more inclusive and welcoming communities? Can people in the next generation learn and experience a hundred times more things than we can? I think the answer to that is yes. CORNISH: How would you mark that moment in sort of the growth of a leader? KAPOR: Well, I think he's been struggling to figure out what to do now that he's king. I will say that we have to distinguish between good intentions and what actually results. For instance, on this pledge to give away the money, if you look at the details of that, in fact, it's not going into a philanthropy, into a foundation the way Bill Gates did with his money at Microsoft. It's going into a limited liability corporation. And they can do anything they want with that. And they have no accountability. I don't doubt that he's sincere. But I certainly don't take it at face value because what really matters is what Facebook actually does doesn't have much to do with what he says. CORNISH: Since the Cambridge Analytica story broke last month, we've seen different sides of Mark Zuckerberg. First there was silence. He waited a while before even speaking with his own employees. And then a kind of apology tour where he did interviews with a handful of news outlets. Here's part of a press call he made last week. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: I think the reality here is that we need to take a broader view of our responsibility rather than just the legal responsibilities. So, you know, we're focused on doing the right thing and making sure that people's information is protected. We're doing the investigations. We're locking down the platform, et cetera. CORNISH: Again, looking at the arc of a leader, what kind of moment is this? KAPOR: Well, what I would say is it's not his first apology. And so the thing that really matters is what happens next and what they actually do about all this. CORNISH: Is Mark Zuckerberg part of a larger story about leadership in Silicon Valley, about an industry that hasn't been taking responsibility for its own power? KAPOR: Oh, absolutely. That Zuckerberg and the Facebook story really exemplifies a great deal about how Silicon Valley operates. One thing I would point out is all along this wasn't a secret that they were engaging in various disreputable behaviors. There were opportunities to speak up. But none of the investors who were on the Facebook gravy train wanted to do that for fear of either being thrown out or not having access to the incredible half trillion dollars of wealth that was being created. And so they remained silent. And the silence of the investors and the board, I think, is also part of what is wrong with Silicon Valley. CORNISH: So what is your let's say optimistic take on what Mark Zuckerberg will do next? KAPOR: Well, the optimistic take is that this really becomes a crisis of spirit for him and leads to a kind of genuine and deep personal transformation. He becomes a leader that is more driven by some set of principles and values and sense of obligation to all of his stakeholders, including mostly the 2 billion users, and that he refocuses the company by taking a long-term view. I think that's going to be hard because there'll be a loss of status and a loss of billions of dollars to do that because their current business model is a devil's bargain that's based on exploiting Facebook users. But the optimistic scenario is he can rise to that occasion. CORNISH: Tech entrepreneur and investor Mitch Kapor - he runs Kapor Capital and the Kapor Center for Social Impact. Thank you for speaking with ALL THINGS CONSIDERED. KAPOR: Absolutely. Thank you. AUDIE CORNISH, HOST:  This week Facebook CEO Mark Zuckerberg tells Congress he's sorry. MARY LOUISE KELLY, HOST:  He'll testify twice about how Facebook let data on up to 87 million users get into the hands of the political firm Cambridge Analytica. In prepared remarks, Zuckerberg says the company did not take a broad enough view of its responsibility. He says that was his mistake because he started Facebook and he runs it. CORNISH: But this is just the latest controversy for the 33-year-old billionaire who famously started the social network from his Harvard dorm room. For a look at Zuckerberg's trajectory we've called upon Mitch Kapor. He's a Silicon Valley entrepreneur himself. He now runs Kapor Capital and the Kapor Center for Social Impact with his wife Freada. Welcome to the program. MITCH KAPOR: Pleasure to be here. CORNISH: Now, Mark Zuckerberg in a way is like the original hoodie-wearing savant CEO. I mean, this was kind of the image of him when people talked about the Harvard dorm room. KAPOR: Absolutely. yes. CORNISH: Was it all that common, or was it even myth at the time? KAPOR: No, I think that he was very smart, technically minded undergraduate with a lot of ability to code and a bunch of ideas to try things, the first of which was to scrape data from Harvard servers without anybody's permission, and then asking other people at Harvard to rate the hotness of student pictures. CORNISH: It's interesting because when Facebook's origin story was then told in the 2010 film \"The Social Network,\" that - (laughter) I think that very - there was a scene that spoke to this. (SOUNDBITE OF FILM, \"THE SOCIAL NETWORK\") JESSE EISENBERG: (As Mark Zuckerberg) The Kirkland Facebook is open on my desktop, and some of these people have pretty horrendous Facebook pics. Billy Olson's sitting here and had the idea of putting some of the pictures next to pictures of farm animals and have people vote on who's hotter. CORNISH: Obviously this is not a documentary. But how did this change the way the public viewed Zuckerberg, do you think? KAPOR: Well, he became a kind of cultural icon as a result of the movie. And while obviously it wasn't entirely a flattering portrait, my sense is that the hero worship aspect of our culture fastened onto him even more tightly as a result of the exposure of the Zuckerberg character. CORNISH: There's also, as they say, some danger in believing your own press. And in the years after, we saw Mark Zuckerberg really seize the reins in terms of being the face of the company and embracing that role. Were there some pitfalls there? KAPOR: I think so in that when you're the king - and he really is the king - it's very difficult for your subjects to speak the truth to you. By becoming so visible and so much out there, I think it really made it even more difficult to have the kind of real internal dialogue that's needed to keep this hypergrowth company on some sort of track. CORNISH: Fast-forward a few years, and we have seen Mark Zuckerberg try to change the narrative - right? - around people. People stopped seeing him as so benevolent, frankly - right? - or as a naive kid, so to speak. And I remember when he and his wife promised to give away most of their wealth. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: You know, what does it take to make it so that people don't get sick anymore? Can we build more inclusive and welcoming communities? Can people in the next generation learn and experience a hundred times more things than we can? I think the answer to that is yes. CORNISH: How would you mark that moment in sort of the growth of a leader? KAPOR: Well, I think he's been struggling to figure out what to do now that he's king. I will say that we have to distinguish between good intentions and what actually results. For instance, on this pledge to give away the money, if you look at the details of that, in fact, it's not going into a philanthropy, into a foundation the way Bill Gates did with his money at Microsoft. It's going into a limited liability corporation. And they can do anything they want with that. And they have no accountability. I don't doubt that he's sincere. But I certainly don't take it at face value because what really matters is what Facebook actually does doesn't have much to do with what he says. CORNISH: Since the Cambridge Analytica story broke last month, we've seen different sides of Mark Zuckerberg. First there was silence. He waited a while before even speaking with his own employees. And then a kind of apology tour where he did interviews with a handful of news outlets. Here's part of a press call he made last week. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: I think the reality here is that we need to take a broader view of our responsibility rather than just the legal responsibilities. So, you know, we're focused on doing the right thing and making sure that people's information is protected. We're doing the investigations. We're locking down the platform, et cetera. CORNISH: Again, looking at the arc of a leader, what kind of moment is this? KAPOR: Well, what I would say is it's not his first apology. And so the thing that really matters is what happens next and what they actually do about all this. CORNISH: Is Mark Zuckerberg part of a larger story about leadership in Silicon Valley, about an industry that hasn't been taking responsibility for its own power? KAPOR: Oh, absolutely. That Zuckerberg and the Facebook story really exemplifies a great deal about how Silicon Valley operates. One thing I would point out is all along this wasn't a secret that they were engaging in various disreputable behaviors. There were opportunities to speak up. But none of the investors who were on the Facebook gravy train wanted to do that for fear of either being thrown out or not having access to the incredible half trillion dollars of wealth that was being created. And so they remained silent. And the silence of the investors and the board, I think, is also part of what is wrong with Silicon Valley. CORNISH: So what is your let's say optimistic take on what Mark Zuckerberg will do next? KAPOR: Well, the optimistic take is that this really becomes a crisis of spirit for him and leads to a kind of genuine and deep personal transformation. He becomes a leader that is more driven by some set of principles and values and sense of obligation to all of his stakeholders, including mostly the 2 billion users, and that he refocuses the company by taking a long-term view. I think that's going to be hard because there'll be a loss of status and a loss of billions of dollars to do that because their current business model is a devil's bargain that's based on exploiting Facebook users. But the optimistic scenario is he can rise to that occasion. CORNISH: Tech entrepreneur and investor Mitch Kapor - he runs Kapor Capital and the Kapor Center for Social Impact. Thank you for speaking with ALL THINGS CONSIDERED. KAPOR: Absolutely. Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-09-600938197": {"title": "How Facebook Has Changed Since NPR First Reported On It 13 Years Ago : NPR", "url": "https://www.npr.org/2018/04/09/600938197/how-facebook-has-changed-since-npr-first-reported-on-it-13-years-ago", "author": "No author found", "published_date": "2018-04-09", "content": "AUDIE CORNISH, HOST: When Facebook CEO Mark Zuckerberg testifies before Congress tomorrow and Wednesday, it will be a spectacle. Every lawmaker wants to ask him about how the world's largest social network treats its users' personal information. Facebook and Zuckerberg are household names. MARY LOUISE KELLY, HOST: Of course, it wasn't always that way. Let's travel back to that time when the word, Facebook, meant a real book with students' photos on pages. It's time for. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: First mention. KELLY: A search through NPR's story archives takes us back to the morning of August 19, 2004. That is when NPR's Renee Montagne introduced a report about a website just a few months old that lets students at Harvard University learn a bit about each other. (SOUNDBITE OF ARCHIVED BROADCAST)RENEE MONTAGNE, BYLINE: Thanks to the Internet and some crafty underclassmen, now you can get to know your roommate or mates without the inconvenience of actually meeting them. CHRIS HUGHES: You know, later on in the all-freshman dining hall or at one of those super awkward ice cream socials, it might be much easier to start up a conversation. MONTAGNE: That is Chris Hughes, a junior at Harvard. He is co-founder of the getting-to-know-you website thefacebook. com. CORNISH: No mention of the other co-founder of The Facebook in that story. NPR listeners would have to wait two months more for that. On October 11, 2004, we aired a story by reporter Andrea Shea. Gone was the cheery mood from that first report. Harvard students had begun to point fingers at each other. (SOUNDBITE OF ARCHIVED BROADCAST)ANDREA SHEA, BYLINE: It's a classic he-said-she-said scenario. Last winter, when senior Tyler Winklevoss and his partners decided to get serious with their concept for ConnectU, they asked Mark Zuckerberg, a technically savvy sophomore, to write code for the fledgling site. He agreed. This is where it starts to get hazy and ugly. KELLY: Zuckerberg denied stealing the idea for what would become Facebook from the twins Tyler and Cameron Winklevoss. A legal battle dragged out for years. All of this is dramatized in the film \"The Social Network,\" which was released in 2010. CORNISH: Since those first mentions of Mark Zuckerberg and The Facebook, NPR has found many occasions to talk about both the man and the company. And with Zuckerberg's testimony on Capitol Hill, this week will be no exception. (SOUNDBITE OF MUSIC) AUDIE CORNISH, HOST:  When Facebook CEO Mark Zuckerberg testifies before Congress tomorrow and Wednesday, it will be a spectacle. Every lawmaker wants to ask him about how the world's largest social network treats its users' personal information. Facebook and Zuckerberg are household names. MARY LOUISE KELLY, HOST:  Of course, it wasn't always that way. Let's travel back to that time when the word, Facebook, meant a real book with students' photos on pages. It's time for. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: First mention. KELLY: A search through NPR's story archives takes us back to the morning of August 19, 2004. That is when NPR's Renee Montagne introduced a report about a website just a few months old that lets students at Harvard University learn a bit about each other. (SOUNDBITE OF ARCHIVED BROADCAST) RENEE MONTAGNE, BYLINE: Thanks to the Internet and some crafty underclassmen, now you can get to know your roommate or mates without the inconvenience of actually meeting them. CHRIS HUGHES: You know, later on in the all-freshman dining hall or at one of those super awkward ice cream socials, it might be much easier to start up a conversation. MONTAGNE: That is Chris Hughes, a junior at Harvard. He is co-founder of the getting-to-know-you website thefacebook. com. CORNISH: No mention of the other co-founder of The Facebook in that story. NPR listeners would have to wait two months more for that. On October 11, 2004, we aired a story by reporter Andrea Shea. Gone was the cheery mood from that first report. Harvard students had begun to point fingers at each other. (SOUNDBITE OF ARCHIVED BROADCAST) ANDREA SHEA, BYLINE: It's a classic he-said-she-said scenario. Last winter, when senior Tyler Winklevoss and his partners decided to get serious with their concept for ConnectU, they asked Mark Zuckerberg, a technically savvy sophomore, to write code for the fledgling site. He agreed. This is where it starts to get hazy and ugly. KELLY: Zuckerberg denied stealing the idea for what would become Facebook from the twins Tyler and Cameron Winklevoss. A legal battle dragged out for years. All of this is dramatized in the film \"The Social Network,\" which was released in 2010. CORNISH: Since those first mentions of Mark Zuckerberg and The Facebook, NPR has found many occasions to talk about both the man and the company. And with Zuckerberg's testimony on Capitol Hill, this week will be no exception. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-09-600819760": {"title": "Facebook To Notify Users Whose Information Was Used By Cambridge Analytica : NPR", "url": "https://www.npr.org/2018/04/09/600819760/was-your-facebook-information-used-or-shared-by-cambridge-analytica", "author": "No author found", "published_date": "2018-04-09", "content": "", "section": "Technology", "disclaimer": ""}, "2018-04-09-600233409": {"title": "As Testimony Of Facebook's Zuckerberg Awaits, A Look At Tech Lobbying In DC : NPR", "url": "https://www.npr.org/2018/04/09/600233409/lobbyists-campaign-cash-and-think-tanks-how-silicon-valley-tackled-politics", "author": "No author found", "published_date": "2018-04-09", "content": "", "section": "National Security", "disclaimer": ""}, "2018-04-10-601148172": {"title": "Most Americans Feel They've Lost Control Of Their Online Data : NPR", "url": "https://www.npr.org/2018/04/10/601148172/most-americans-feel-theyve-lost-control-control-of-their-online-data", "author": "No author found", "published_date": "2018-04-10", "content": "MARY LOUISE KELLY, HOST: As Facebook's Mark Zuckerberg testified for Congress today, we sat wondering, how much do Americans care about their online privacy? What's OK for companies to collect about you, and what's not? So we went out here in Washington, D. C. , and asked. MICHAEL WATSON: Things like purchasing preferences probably seem OK. But anything of a personal nature I would think should be off-limits. ASHLEY WILLIAMS: Of course Social Securities and bank information, all that - that's a little bit more private. WATSON: I think I have some control. But I tend to trust the companies to protect me a little bit. WILLIAMS: To me, the whole them knowing what I like - you know, the ads and stuff. . . MELANIE FINE: When you, like, search something and then you go on Facebook and there's an ad for it, that's really creepy. WILLIAMS: . . . To me that's not that big of a deal. That's fine. Whatever. FINE: I mean, in a perfect world they wouldn't use anything that I didn't specifically tell them, sure, here's some information about me. But we don't really live in that world. KELLY: That's Melanie Fine (ph) along with Michael Watson (ph) and Ashley Williams (ph). For a broader view we turned to Lee Rainie. He directs Internet and technology research at Pew. And he's been polling Americans on these issues for years. He told us the picture that the data paints is complicated. LEE RAINIE: Americans are all over the place when it comes to privacy. The most fundamental level, when you ask them the straight-on question, do you care about it or not, they do care. When you then talk to them about specific tradeoffs, they're a little bit more in a transactional frame of mind. I'm going to give up a little bit of personal information. What am I going to get in return? But the one thing I think that's predominant in our data now is that Americans are confused about what's happening. They don't exactly know what's being collected. They don't know what's being done with it once the data are collected. And they're totally freaked out about the number of data breaches that have occurred. KELLY: So it sounds like Americans are not feeling in control of their online data. RAINIE: Exactly. Three-quarters of Americans say that control matters a lot to them, but 91 percent of Americans say that they feel that all Americans have lost control of their data and don't really know what they would do to recapture some of that control. KELLY: Well, let's drill down on some of the specifics. What type of data were people just fine with having shared, and what seems to cross a line? RAINIE: So their basic purchasing habits - they are not very worried about them being captured and used. The cultural tastes they have, even their political views and religious views don't register very high on things that they consider sensitive. They are very concerned about their Social Security numbers, their health information, the content of private communications - what's going on in their email and texts like that. So not all information is created equal for people, and not all people share the same sort of boundary lines. KELLY: What about generational differences? Did you find that, say, millennials are more blase about their online privacy than the baby boomers? RAINIE: There's a really interesting generational story that isn't quite what the stereotype would have it. Yes, younger people are much more active online, much more forgiving of some of the circumstances when their data are captured and used in some ways to deliver products and services to them. But they're also more vigilant than their elders in monitoring. They watch what's posted about them. They watch what pictures their name is tagged in. And they're very concerned about the way that they present themselves online. So they curate their identity and their reputation very aggressively. KELLY: Mark Zuckerberg, as we mentioned, he's before Congress today. He'll be back there tomorrow. Lots of questions about the role that regulation could play in preserving people's privacy. Do you have any advice to lawmakers based from your research as lawmakers think about where to draw the line and how to police a company like Facebook? RAINIE: One of the reasons we intensively studied privacy since 2013 and the first revelations by Edward Snowden about government surveillance was that we wanted to see for ourselves what the bright lines were that Americans would draw because we knew that both companies and the government would love to have some guidance about where the bright lines are. And the maddeningly complicated thing about this process is that different Americans draw their lines at different places. Contexts and the specific conditions under which people share their information matters a lot. And their time of life matters a lot. The bargain that they're being offered matters a lot. So there aren't very pronouncedly clear bright lines for all Americans in all ways. KELLY: You're kind of saying as Congress seeks to fix this, it's not quite clear what this is. Different people define this problem really differently. RAINIE: Yes, there are certain broad tendencies. And I think Americans would very clearly love a lot more transparency, a lot more control over their data, a lot more information about when problems occur. But there are still great variances even on those questions. So it would be hard for anybody wanting to regulate in this space to sort of say, OK, these are the absolute moments when all Americans would be happy with regulation, and these are the moments when people would be happy with the companies being in control of things. It's a much more fluid kind of situation that depends on people in their own individual circumstances. KELLY: That's Lee Rainie, director of Internet and technology research at Pew. Thanks so much for talking to us today. RAINIE: Thanks, Mary Louise. MARY LOUISE KELLY, HOST:  As Facebook's Mark Zuckerberg testified for Congress today, we sat wondering, how much do Americans care about their online privacy? What's OK for companies to collect about you, and what's not? So we went out here in Washington, D. C. , and asked. MICHAEL WATSON: Things like purchasing preferences probably seem OK. But anything of a personal nature I would think should be off-limits. ASHLEY WILLIAMS: Of course Social Securities and bank information, all that - that's a little bit more private. WATSON: I think I have some control. But I tend to trust the companies to protect me a little bit. WILLIAMS: To me, the whole them knowing what I like - you know, the ads and stuff. . . MELANIE FINE: When you, like, search something and then you go on Facebook and there's an ad for it, that's really creepy. WILLIAMS: . . . To me that's not that big of a deal. That's fine. Whatever. FINE: I mean, in a perfect world they wouldn't use anything that I didn't specifically tell them, sure, here's some information about me. But we don't really live in that world. KELLY: That's Melanie Fine (ph) along with Michael Watson (ph) and Ashley Williams (ph). For a broader view we turned to Lee Rainie. He directs Internet and technology research at Pew. And he's been polling Americans on these issues for years. He told us the picture that the data paints is complicated. LEE RAINIE: Americans are all over the place when it comes to privacy. The most fundamental level, when you ask them the straight-on question, do you care about it or not, they do care. When you then talk to them about specific tradeoffs, they're a little bit more in a transactional frame of mind. I'm going to give up a little bit of personal information. What am I going to get in return? But the one thing I think that's predominant in our data now is that Americans are confused about what's happening. They don't exactly know what's being collected. They don't know what's being done with it once the data are collected. And they're totally freaked out about the number of data breaches that have occurred. KELLY: So it sounds like Americans are not feeling in control of their online data. RAINIE: Exactly. Three-quarters of Americans say that control matters a lot to them, but 91 percent of Americans say that they feel that all Americans have lost control of their data and don't really know what they would do to recapture some of that control. KELLY: Well, let's drill down on some of the specifics. What type of data were people just fine with having shared, and what seems to cross a line? RAINIE: So their basic purchasing habits - they are not very worried about them being captured and used. The cultural tastes they have, even their political views and religious views don't register very high on things that they consider sensitive. They are very concerned about their Social Security numbers, their health information, the content of private communications - what's going on in their email and texts like that. So not all information is created equal for people, and not all people share the same sort of boundary lines. KELLY: What about generational differences? Did you find that, say, millennials are more blase about their online privacy than the baby boomers? RAINIE: There's a really interesting generational story that isn't quite what the stereotype would have it. Yes, younger people are much more active online, much more forgiving of some of the circumstances when their data are captured and used in some ways to deliver products and services to them. But they're also more vigilant than their elders in monitoring. They watch what's posted about them. They watch what pictures their name is tagged in. And they're very concerned about the way that they present themselves online. So they curate their identity and their reputation very aggressively. KELLY: Mark Zuckerberg, as we mentioned, he's before Congress today. He'll be back there tomorrow. Lots of questions about the role that regulation could play in preserving people's privacy. Do you have any advice to lawmakers based from your research as lawmakers think about where to draw the line and how to police a company like Facebook? RAINIE: One of the reasons we intensively studied privacy since 2013 and the first revelations by Edward Snowden about government surveillance was that we wanted to see for ourselves what the bright lines were that Americans would draw because we knew that both companies and the government would love to have some guidance about where the bright lines are. And the maddeningly complicated thing about this process is that different Americans draw their lines at different places. Contexts and the specific conditions under which people share their information matters a lot. And their time of life matters a lot. The bargain that they're being offered matters a lot. So there aren't very pronouncedly clear bright lines for all Americans in all ways. KELLY: You're kind of saying as Congress seeks to fix this, it's not quite clear what this is. Different people define this problem really differently. RAINIE: Yes, there are certain broad tendencies. And I think Americans would very clearly love a lot more transparency, a lot more control over their data, a lot more information about when problems occur. But there are still great variances even on those questions. So it would be hard for anybody wanting to regulate in this space to sort of say, OK, these are the absolute moments when all Americans would be happy with regulation, and these are the moments when people would be happy with the companies being in control of things. It's a much more fluid kind of situation that depends on people in their own individual circumstances. KELLY: That's Lee Rainie, director of Internet and technology research at Pew. Thanks so much for talking to us today. RAINIE: Thanks, Mary Louise.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-10-601268117": {"title": "Facebook Founder And CEO Mark Zuckerberg Testifies On Capitol Hill : NPR", "url": "https://www.npr.org/2018/04/10/601268117/facebook-founder-and-ceo-mark-zuckerberg-testifies-on-capitol-hill", "author": "No author found", "published_date": "2018-04-10", "content": "MARY LOUISE KELLY, HOST: Facebook CEO Mark Zuckerberg faced nearly five hours of grilling on Capitol Hill today. Forty-some senators were putting questions to him on how the personal information of up to 87 million Facebook users got into the hands of a political data mining firm, Cambridge Analytica. Zuckerberg acknowledged his company had messed up. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: This episode has clearly hurt us and has clearly made it harder for us to achieve the social mission that we care about. And we now have to do a lot of work around building trust. KELLY: Senators also raised questions about other controversies involving Facebook. NPR's Alina Selyukh is back with us. She was there for all five hours of the testimony. Congratulations on surviving, Alina. The hearing's wrapped up now. What was the big takeaway for you? What was the headline? ALINA SELYUKH, BYLINE: So the big theme of the hearing definitely had everything to do with this kind of unclear to most users scope of access that Facebook has to their lives. Remember; this is - this was all started because of Cambridge Analytica, as you were saying. Suddenly people were realizing that their clicks and likes were kind of taking a life on their own once they were on Facebook. And so this hearing specifically highlighted just how little people on Facebook understand the amounts of - the amount of control that they have over information. There was one very telling moment when Senator Dick Durbin, an Illinois Democrat, asked Zuckerberg about this. (SOUNDBITE OF ARCHIVED RECORDING)DICK DURBIN: Mr. Zuckerberg, would you be comfortable sharing with us the name of the hotel you stayed in last night? ZUCKERBERG: (Laughter) No. (LAUGHTER)DURBIN: If you've messaged anybody this week, would you share with us the names of the people you've messaged? ZUCKERBERG: Senator, no, I would probably not choose to do that publicly here. DURBIN: I think that may be what this is all about. SELYUKH: Right. It's the amount of trust that people put into Facebook, and now questions being raised about what Facebook does with that trust. KELLY: Did we ever find out where Mark Zuckerberg - what hotel he was in last night? Not that you're on it. SELYUKH: (Laughter) I'm on it right after this. KELLY: Well, see if lawmakers get to that tomorrow. For today's hearing, what was the overall tone? I mean, there were - there's been a lot of pent-up frustration with Facebook on Capitol Hill, a lot of lawmakers waiting for years to put questions with him. How did it all unfold? SELYUKH: You know, it could have been a lot worse. There was a lot of fascination with the sort of origin story of Facebook, a lot of references to his dorm room. There were of course some moments of real scrutiny. Senator Lindsey Graham, Republican from South Carolina, asked whether Facebook or Zuckerberg considered Facebook a monopoly, which is a big word to utter. And there was this moment - very notable - at the end of the hearing with Senator John Kennedy, Republican of Louisiana. (SOUNDBITE OF ARCHIVED RECORDING)JOHN KENNEDY: Here's what everybody's been trying to tell you, and I say this gently. Your user agreement sucks. (LAUGHTER)KENNEDY: You can spot me 75 IQ points. If I can figure it out, you can figure it out. The purpose of that user agreement is to cover Facebook's rear end. It's not to inform your users about their rights. SELYUKH: What he's talking about is Facebook's fine print. And he - which he says Facebook should go back and rewrite. And he says he doesn't really want to regulate Facebook, but, you know, he just might. KELLY: What about this central question? If you're thinking about regulating Facebook, you have to figure out what Facebook is. This has been an ongoing question. . . SELYUKH: Right. KELLY: . . . How Facebook defines itself. Is it a publisher? Is it a media company? Is it a. . . SELYUKH: Neutral platform. KELLY: . . . Tech company? Yeah. Is it a utility? What is it? SELYUKH: This came up a number of times. And there was one specific exchange after a question with - from Senator Dan Sullivan, Republican of Alaska. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: I agree that we're responsible for the content. But we don't produce the content. I think that when people ask us if we're a media company or a publisher, my understanding of what the heart of what they're really getting at is, do we feel responsibility for the content on our platform? The answer to that I think is clearly yes. SELYUKH: This was a very rehearsed, very prepared answer to this long-running question. For years Facebook has bristled at a comparison to a media company. And this is sort of the slow acceptance of some responsibility for the content that lives on its platform. KELLY: What about Russia. . . SELYUKH: Right. KELLY: . . . And the attempts by Russia to try to influence American voters in the run-up to the 2016 election, try to influence American politics, you know, past the election and right up to this moment right now? What did Zuckerberg say about that? SELYUKH: So he apologized for being too slow to respond. They have been shutting down some accounts. There was a bit of news sort of on that front, and it had to do with the special counsel Robert Mueller. He said that - he was asked whether some employees - whether he actually was interviewed as part of the Mueller investigation into Russia and potential meddling. And he said he himself was not, but some employees were. This was a political story for Democrats. But Republicans were actually more interested in accusations of a liberal bias on Facebook. Senator Ted Cruz, Senator Ben Sasse brought up this issue and kind of grilled Zuckerberg on the stories in the 2016 cycle about Facebook employees allegedly suppressing conservative stories. And that was sort of a political point on that end. KELLY: All right. Quickly, Alina, this was round one. SELYUKH: Yes. KELLY: Round two is tomorrow. He'll be on the House side. What are we watching for? SELYUKH: We are watching for more of the same. And you know what? The shares of Facebook were up today. So if he does it again, Facebook will be breathing a sigh of relief. KELLY: All right. SELYUKH: (Laughter). KELLY: NPR's Alina Selyukh, thank you so much. SELYUKH: Thank you. MARY LOUISE KELLY, HOST:  Facebook CEO Mark Zuckerberg faced nearly five hours of grilling on Capitol Hill today. Forty-some senators were putting questions to him on how the personal information of up to 87 million Facebook users got into the hands of a political data mining firm, Cambridge Analytica. Zuckerberg acknowledged his company had messed up. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: This episode has clearly hurt us and has clearly made it harder for us to achieve the social mission that we care about. And we now have to do a lot of work around building trust. KELLY: Senators also raised questions about other controversies involving Facebook. NPR's Alina Selyukh is back with us. She was there for all five hours of the testimony. Congratulations on surviving, Alina. The hearing's wrapped up now. What was the big takeaway for you? What was the headline? ALINA SELYUKH, BYLINE: So the big theme of the hearing definitely had everything to do with this kind of unclear to most users scope of access that Facebook has to their lives. Remember; this is - this was all started because of Cambridge Analytica, as you were saying. Suddenly people were realizing that their clicks and likes were kind of taking a life on their own once they were on Facebook. And so this hearing specifically highlighted just how little people on Facebook understand the amounts of - the amount of control that they have over information. There was one very telling moment when Senator Dick Durbin, an Illinois Democrat, asked Zuckerberg about this. (SOUNDBITE OF ARCHIVED RECORDING) DICK DURBIN: Mr. Zuckerberg, would you be comfortable sharing with us the name of the hotel you stayed in last night? ZUCKERBERG: (Laughter) No. (LAUGHTER) DURBIN: If you've messaged anybody this week, would you share with us the names of the people you've messaged? ZUCKERBERG: Senator, no, I would probably not choose to do that publicly here. DURBIN: I think that may be what this is all about. SELYUKH: Right. It's the amount of trust that people put into Facebook, and now questions being raised about what Facebook does with that trust. KELLY: Did we ever find out where Mark Zuckerberg - what hotel he was in last night? Not that you're on it. SELYUKH: (Laughter) I'm on it right after this. KELLY: Well, see if lawmakers get to that tomorrow. For today's hearing, what was the overall tone? I mean, there were - there's been a lot of pent-up frustration with Facebook on Capitol Hill, a lot of lawmakers waiting for years to put questions with him. How did it all unfold? SELYUKH: You know, it could have been a lot worse. There was a lot of fascination with the sort of origin story of Facebook, a lot of references to his dorm room. There were of course some moments of real scrutiny. Senator Lindsey Graham, Republican from South Carolina, asked whether Facebook or Zuckerberg considered Facebook a monopoly, which is a big word to utter. And there was this moment - very notable - at the end of the hearing with Senator John Kennedy, Republican of Louisiana. (SOUNDBITE OF ARCHIVED RECORDING) JOHN KENNEDY: Here's what everybody's been trying to tell you, and I say this gently. Your user agreement sucks. (LAUGHTER) KENNEDY: You can spot me 75 IQ points. If I can figure it out, you can figure it out. The purpose of that user agreement is to cover Facebook's rear end. It's not to inform your users about their rights. SELYUKH: What he's talking about is Facebook's fine print. And he - which he says Facebook should go back and rewrite. And he says he doesn't really want to regulate Facebook, but, you know, he just might. KELLY: What about this central question? If you're thinking about regulating Facebook, you have to figure out what Facebook is. This has been an ongoing question. . . SELYUKH: Right. KELLY: . . . How Facebook defines itself. Is it a publisher? Is it a media company? Is it a. . . SELYUKH: Neutral platform. KELLY: . . . Tech company? Yeah. Is it a utility? What is it? SELYUKH: This came up a number of times. And there was one specific exchange after a question with - from Senator Dan Sullivan, Republican of Alaska. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: I agree that we're responsible for the content. But we don't produce the content. I think that when people ask us if we're a media company or a publisher, my understanding of what the heart of what they're really getting at is, do we feel responsibility for the content on our platform? The answer to that I think is clearly yes. SELYUKH: This was a very rehearsed, very prepared answer to this long-running question. For years Facebook has bristled at a comparison to a media company. And this is sort of the slow acceptance of some responsibility for the content that lives on its platform. KELLY: What about Russia. . . SELYUKH: Right. KELLY: . . . And the attempts by Russia to try to influence American voters in the run-up to the 2016 election, try to influence American politics, you know, past the election and right up to this moment right now? What did Zuckerberg say about that? SELYUKH: So he apologized for being too slow to respond. They have been shutting down some accounts. There was a bit of news sort of on that front, and it had to do with the special counsel Robert Mueller. He said that - he was asked whether some employees - whether he actually was interviewed as part of the Mueller investigation into Russia and potential meddling. And he said he himself was not, but some employees were. This was a political story for Democrats. But Republicans were actually more interested in accusations of a liberal bias on Facebook. Senator Ted Cruz, Senator Ben Sasse brought up this issue and kind of grilled Zuckerberg on the stories in the 2016 cycle about Facebook employees allegedly suppressing conservative stories. And that was sort of a political point on that end. KELLY: All right. Quickly, Alina, this was round one. SELYUKH: Yes. KELLY: Round two is tomorrow. He'll be on the House side. What are we watching for? SELYUKH: We are watching for more of the same. And you know what? The shares of Facebook were up today. So if he does it again, Facebook will be breathing a sigh of relief. KELLY: All right. SELYUKH: (Laughter). KELLY: NPR's Alina Selyukh, thank you so much. SELYUKH: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-10-600917264": {"title": "Mark Zuckerberg On Capitol Hill: What To Expect From The Facebook Hearings : NPR", "url": "https://www.npr.org/2018/04/10/600917264/facebook-in-congress-what-to-expect-when-zuckerberg-goes-to-capitol-hill", "author": "No author found", "published_date": "2018-04-10", "content": "", "section": "Politics", "disclaimer": ""}, "2018-04-11-601560213": {"title": "Media Or Tech Company? Facebook's Profile Is Blurry : NPR", "url": "https://www.npr.org/2018/04/11/601560213/media-or-tech-company-facebooks-profile-is-blurry", "author": "No author found", "published_date": "2018-04-11", "content": "MARY LOUISE KELLY, HOST:  Let's zero in on one of the more existential questions Zuckerberg had to wrestle with as he testified this week. Here is Republican Senator Dan Sullivan of Alaska wondering. . . (SOUNDBITE OF ARCHIVED RECORDING)DAN SULLIVAN: What exactly Facebook is. KELLY: The senator pressed, is Facebook a tech company? Is it a publisher? Here's Zuckerberg. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: I think that when people ask us if we're a media company or a publisher, my understanding of what the heart of what they're really getting at is, do we feel responsibility for the content on our platform? The answer to that I think is clearly yes. KELLY: That answer matters because what Facebook is determines how lawmakers may go about trying to regulate it. Issie Lapowsky is a senior writer for Wired magazine, and she joins me now. Welcome. ISSIE LAPOWSKY: Thanks for having me. KELLY: Is it clear to you whether Facebook is a media company or a publisher or what? LAPOWSKY: Well, I think Facebook is a lot of things. Mark Zuckerberg would like to tell you that Facebook is a tech company, but as he noted today, you know, Facebook does a lot of things, including building drones that can beam the Internet to parts of the developing world. They build tools that allow you to send money to friends. So are they a financial institution? Are they an aerospace company? I think it's pretty clear that Facebook has completely changed the way the media industry works, and that's why regulators and sometimes the public have such a tough time really defining whether Facebook is a news entity because we haven't had a platform like this that is both so dominant in news but that also is not committing journalism itself. It's really just pulling in all this news from the rest of the web, some of that legitimate news, some of it not. KELLY: Well, you're describing a company that is in some ways sui generis. Lawmakers can't figure out how to regulate Facebook because nothing like Facebook has ever existed before. LAPOWSKY: Exactly. And yesterday, Mark Zuckerberg was asked, who are your competitors? Lindsey Graham asked him this question. (SOUNDBITE OF ARCHIVED RECORDING)LINDSEY GRAHAM: Is there an alternative to Facebook in the private sector? ZUCKERBERG: Yes, Senator. The average American uses eight different apps. . . GRAHAM: OK. ZUCKERBERG: . . . To communicate with their friends and stay in touch with people. . . GRAHAM: OK, which is. . . ZUCKERBERG: . . . Ranging from texting apps to email to. . . GRAHAM: Does it have the same service you provide? ZUCKERBERG: Well, we provide a number of different services. GRAHAM: Is Twitter the same as what you do? ZUCKERBERG: It overlaps with a portion of what we do. GRAHAM: You don't think you have a monopoly. ZUCKERBERG: It certainly doesn't feel like that to me. GRAHAM: OK. (LAUGHTER)LAPOWSKY: That might be true. He might feel that Facebook's dominance is constantly under threat, but that doesn't mean that in fact it is. KELLY: How does existing law treat Facebook? Is it as a utility, as a publisher or what? LAPOWSKY: So the laws that allow the Internet to really become what the Internet has become give platforms wide latitude in terms of what responsibility they have for what people publish on their platforms. So that's why Mark Zuckerberg and certainly the heads of other social networks like this have really maintained that, we are the platform, not the publisher because they want to be a neutral platform, which means that they are not subject to any laws requiring that they monitor illegal activity and things like that. KELLY: Which is why I thought that statement from Mark Zuckerberg was so interesting - him acknowledging Facebook feel responsibility for the content on our platform. That's a departure from where Facebook and other giants of Silicon Valley have landed in past. LAPOWSKY: That is an absolute departure. So many of these tech CEOs have started repeating this cliche. We don't want to be the arbiters of truth. And I think that Mark Zuckerberg is seeing that that is not playing so well in the public. And time and again, they've seen how these problems really escalate. And the more extreme examples that we see of ways that it could go wrong, I think the more Mark Zuckerberg has had to come around to the idea that, yes, they are responsible for this content. KELLY: So at the end of these two days of much-anticipated, much-watched testimony, it sounds like in your view, this question of what Facebook is remains an intriguing and somewhat open question. LAPOWSKY: It's definitely an open question. I think this has to be sort of the beginning of the conversation, not the end because lawmakers seem to come to these hearings somewhat uninformed about how Facebook works. And so a lot of what Mark Zuckerberg has been doing has been explaining how Facebook works and at times dodging their tough questions about it. But obviously these hearings were just a matter of Washington getting a firm grasp on how Facebook works in order to figure out how to address it because you're right; they haven't seen anything like Facebook before. KELLY: That's Issie Lapowsky. She covers tech, politics and national affairs for Wired magazine. Thanks so much. LAPOWSKY: Thank you. MARY LOUISE KELLY, HOST:   Let's zero in on one of the more existential questions Zuckerberg had to wrestle with as he testified this week. Here is Republican Senator Dan Sullivan of Alaska wondering. . . (SOUNDBITE OF ARCHIVED RECORDING) DAN SULLIVAN: What exactly Facebook is. KELLY: The senator pressed, is Facebook a tech company? Is it a publisher? Here's Zuckerberg. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: I think that when people ask us if we're a media company or a publisher, my understanding of what the heart of what they're really getting at is, do we feel responsibility for the content on our platform? The answer to that I think is clearly yes. KELLY: That answer matters because what Facebook is determines how lawmakers may go about trying to regulate it. Issie Lapowsky is a senior writer for Wired magazine, and she joins me now. Welcome. ISSIE LAPOWSKY: Thanks for having me. KELLY: Is it clear to you whether Facebook is a media company or a publisher or what? LAPOWSKY: Well, I think Facebook is a lot of things. Mark Zuckerberg would like to tell you that Facebook is a tech company, but as he noted today, you know, Facebook does a lot of things, including building drones that can beam the Internet to parts of the developing world. They build tools that allow you to send money to friends. So are they a financial institution? Are they an aerospace company? I think it's pretty clear that Facebook has completely changed the way the media industry works, and that's why regulators and sometimes the public have such a tough time really defining whether Facebook is a news entity because we haven't had a platform like this that is both so dominant in news but that also is not committing journalism itself. It's really just pulling in all this news from the rest of the web, some of that legitimate news, some of it not. KELLY: Well, you're describing a company that is in some ways sui generis. Lawmakers can't figure out how to regulate Facebook because nothing like Facebook has ever existed before. LAPOWSKY: Exactly. And yesterday, Mark Zuckerberg was asked, who are your competitors? Lindsey Graham asked him this question. (SOUNDBITE OF ARCHIVED RECORDING) LINDSEY GRAHAM: Is there an alternative to Facebook in the private sector? ZUCKERBERG: Yes, Senator. The average American uses eight different apps. . . GRAHAM: OK. ZUCKERBERG: . . . To communicate with their friends and stay in touch with people. . . GRAHAM: OK, which is. . . ZUCKERBERG: . . . Ranging from texting apps to email to. . . GRAHAM: Does it have the same service you provide? ZUCKERBERG: Well, we provide a number of different services. GRAHAM: Is Twitter the same as what you do? ZUCKERBERG: It overlaps with a portion of what we do. GRAHAM: You don't think you have a monopoly. ZUCKERBERG: It certainly doesn't feel like that to me. GRAHAM: OK. (LAUGHTER) LAPOWSKY: That might be true. He might feel that Facebook's dominance is constantly under threat, but that doesn't mean that in fact it is. KELLY: How does existing law treat Facebook? Is it as a utility, as a publisher or what? LAPOWSKY: So the laws that allow the Internet to really become what the Internet has become give platforms wide latitude in terms of what responsibility they have for what people publish on their platforms. So that's why Mark Zuckerberg and certainly the heads of other social networks like this have really maintained that, we are the platform, not the publisher because they want to be a neutral platform, which means that they are not subject to any laws requiring that they monitor illegal activity and things like that. KELLY: Which is why I thought that statement from Mark Zuckerberg was so interesting - him acknowledging Facebook feel responsibility for the content on our platform. That's a departure from where Facebook and other giants of Silicon Valley have landed in past. LAPOWSKY: That is an absolute departure. So many of these tech CEOs have started repeating this cliche. We don't want to be the arbiters of truth. And I think that Mark Zuckerberg is seeing that that is not playing so well in the public. And time and again, they've seen how these problems really escalate. And the more extreme examples that we see of ways that it could go wrong, I think the more Mark Zuckerberg has had to come around to the idea that, yes, they are responsible for this content. KELLY: So at the end of these two days of much-anticipated, much-watched testimony, it sounds like in your view, this question of what Facebook is remains an intriguing and somewhat open question. LAPOWSKY: It's definitely an open question. I think this has to be sort of the beginning of the conversation, not the end because lawmakers seem to come to these hearings somewhat uninformed about how Facebook works. And so a lot of what Mark Zuckerberg has been doing has been explaining how Facebook works and at times dodging their tough questions about it. But obviously these hearings were just a matter of Washington getting a firm grasp on how Facebook works in order to figure out how to address it because you're right; they haven't seen anything like Facebook before. KELLY: That's Issie Lapowsky. She covers tech, politics and national affairs for Wired magazine. Thanks so much. LAPOWSKY: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-11-601630161": {"title": "Facebook's Zuckerberg Ditches His Signature Hoodie For A Suit At Congress : NPR", "url": "https://www.npr.org/2018/04/11/601630161/facebooks-zuckerberg-ditches-his-signature-hoodie-for-a-suit-at-congress", "author": "No author found", "published_date": "2018-04-11", "content": "MARY LOUISE KELLY, HOST:  Pull up a picture of Mark Zuckerberg. Pick any occasion other than this week's testimony, and as likely as not, he's wearing a gray T-shirt and a hoodie. The Facebook founder may have made billions, but his sartorial style has not evolved much from his student days back in Harvard's dorms, which piqued Washington Post fashion critic Robin Givhan's curiosity as to what kind of an effort Zuckerberg would make for his turn on Capitol Hill. We reached Robin Givhan via Skype. Robin, welcome back to the program. ROBIN GIVHAN: Thank you. Nice to be here. KELLY: Nice to have you here. OK, so blue suit, blue tie - anything more to say about the outfit? Describe it for us. GIVHAN: This was sort of a grudging suit and tie. The tie wasn't really knotted very well. It kind of hung loosely from his neck. His shirt looked like it was a bit too big. The suit kind of looked like, OK, here's the most basic suit I can find. KELLY: So this is - he wore a tie, but you're suggesting it kind of looked like he didn't want to be wearing a tie. And he showed up in the uniform known to, you know, generations of Washington interns, not in the billionaire's version of a dashing navy suit and tie. GIVHAN: No, and that's not to say that the suit wasn't expensive. It simply wasn't tailored. And this was not a situation that he wanted to find himself in, and the clothing reflected that. KELLY: What is his traditional uniform - the T-shirt, the hoodie - what is that meant to telegraph to the world? GIVHAN: Well, he has said that one of the reasons why he wears the same thing is because it frees him from having to make that decision because there are so many other decisions that he has to make any given day. It is extraordinarily youthful and boyish, which underscores the sort of next-generation nature of his business. And it also has an element of rebelliousness which, you know, speaks to what his tech universe is all about. KELLY: You make the argument in your column in The Washington Post Mark Zuckerberg - he's a suit now. He's one of them, so he better get comfortable wearing one. GIVHAN: This was a moment when this 33-year-old sort of disruptor really had to come face to face with the fact that he was no longer disrupting. He was in a position in which he had to fix things. And the suit really just underscored very visually that he was crossing from being an outsider into now being an insider. KELLY: Let me ask you the contrarian question that I can hear my dad shouting at the radio right now. Why does it matter what Mark Zuckerberg wears to Washington? GIVHAN: Well, it matters because he has, one, used fashion as a way to distinguish himself and to send a message about what it is that he believes he's doing and where his company is situated in the broader cultural context. But I also think it matters because one of the reasons these hearings are in fact televised is because they are political theater. Part of theater is the costuming, and that helps us understand who the players are, what their goals are and what the messaging is. KELLY: Will you be watching for him to revert to usual uniform? GIVHAN: I will be curious to see if he can wait until he's on California soil before he slips into his T-shirt or. . . (LAUGHTER)GIVHAN: . . . Or if he heads right back into, you know, his hotel and pulls off that suit jacket as soon as possible. KELLY: That's Robin Givhan, fashion critic for The Washington Post. Thanks very much. GIVHAN: (Laughter) Thank you. MARY LOUISE KELLY, HOST:   Pull up a picture of Mark Zuckerberg. Pick any occasion other than this week's testimony, and as likely as not, he's wearing a gray T-shirt and a hoodie. The Facebook founder may have made billions, but his sartorial style has not evolved much from his student days back in Harvard's dorms, which piqued Washington Post fashion critic Robin Givhan's curiosity as to what kind of an effort Zuckerberg would make for his turn on Capitol Hill. We reached Robin Givhan via Skype. Robin, welcome back to the program. ROBIN GIVHAN: Thank you. Nice to be here. KELLY: Nice to have you here. OK, so blue suit, blue tie - anything more to say about the outfit? Describe it for us. GIVHAN: This was sort of a grudging suit and tie. The tie wasn't really knotted very well. It kind of hung loosely from his neck. His shirt looked like it was a bit too big. The suit kind of looked like, OK, here's the most basic suit I can find. KELLY: So this is - he wore a tie, but you're suggesting it kind of looked like he didn't want to be wearing a tie. And he showed up in the uniform known to, you know, generations of Washington interns, not in the billionaire's version of a dashing navy suit and tie. GIVHAN: No, and that's not to say that the suit wasn't expensive. It simply wasn't tailored. And this was not a situation that he wanted to find himself in, and the clothing reflected that. KELLY: What is his traditional uniform - the T-shirt, the hoodie - what is that meant to telegraph to the world? GIVHAN: Well, he has said that one of the reasons why he wears the same thing is because it frees him from having to make that decision because there are so many other decisions that he has to make any given day. It is extraordinarily youthful and boyish, which underscores the sort of next-generation nature of his business. And it also has an element of rebelliousness which, you know, speaks to what his tech universe is all about. KELLY: You make the argument in your column in The Washington Post Mark Zuckerberg - he's a suit now. He's one of them, so he better get comfortable wearing one. GIVHAN: This was a moment when this 33-year-old sort of disruptor really had to come face to face with the fact that he was no longer disrupting. He was in a position in which he had to fix things. And the suit really just underscored very visually that he was crossing from being an outsider into now being an insider. KELLY: Let me ask you the contrarian question that I can hear my dad shouting at the radio right now. Why does it matter what Mark Zuckerberg wears to Washington? GIVHAN: Well, it matters because he has, one, used fashion as a way to distinguish himself and to send a message about what it is that he believes he's doing and where his company is situated in the broader cultural context. But I also think it matters because one of the reasons these hearings are in fact televised is because they are political theater. Part of theater is the costuming, and that helps us understand who the players are, what their goals are and what the messaging is. KELLY: Will you be watching for him to revert to usual uniform? GIVHAN: I will be curious to see if he can wait until he's on California soil before he slips into his T-shirt or. . . (LAUGHTER) GIVHAN: . . . Or if he heads right back into, you know, his hotel and pulls off that suit jacket as soon as possible. KELLY: That's Robin Givhan, fashion critic for The Washington Post. Thanks very much. GIVHAN: (Laughter) Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-11-601323233": {"title": "Did Fake News On Facebook Help Elect President Trump? Here's What We Know : NPR", "url": "https://www.npr.org/2018/04/11/601323233/6-facts-we-know-about-fake-news-in-the-2016-election", "author": "No author found", "published_date": "2018-04-11", "content": "", "section": "Politics", "disclaimer": ""}, "2018-04-12-601881444": {"title": "Facebook And Other Firms Have A Ton Of Data On You. Here's How To Limit That : NPR", "url": "https://www.npr.org/2018/04/12/601881444/facebook-and-other-firms-have-a-ton-of-data-on-you-heres-how-to-limit-that", "author": "No author found", "published_date": "2018-04-12", "content": "AUDIE CORNISH, HOST: Facebook CEO Mark Zuckerberg is back in Silicon Valley after two days of testimony on Capitol Hill. And lawmakers are focused on other issues today. But Facebook users and really anyone who uses the Internet, well, we're still wondering if there's any such thing as online privacy. Listen to this exchange from the House hearing yesterday between Representative Ben Ray Lujan of New Mexico and Zuckerberg. (SOUNDBITE OF ARCHIVED RECORDING)BEN RAY LUJAN: Facebook has detailed profiles on people who have never signed up for Facebook - yes or no. MARK ZUCKERBERG: Congressman, in general, we collect data on people have not signed up for Facebook for security purposes to prevent the kind of scraping that you were just referring to. CORNISH: To talk about the many ways Facebook collects data on people and what, if anything, we can do about it, we've reached out to Julia Angwin. She's an investigative reporter formerly at ProPublica. Welcome to the program. JULIA ANGWIN: It's great to be here. CORNISH: Whether you're on Facebook or not, what are the kinds of attributes Facebook is looking for? What are they looking for in the way of data? ANGWIN: Facebook is looking to know basically as much as possible about its users, things like this person likes grass or this person is really interested in a certain type of wrestling. There was one ad category they had which was a person who likes to pretend to text in awkward situations. You know, these are the kinds of things they'd like to know about you because they're hoping some advertiser will want to buy that ad category. CORNISH: I went and downloaded my Facebook archive to see what they had on me, and some things were not so surprising - right? - topics that I had clicked on, not surprisingly a lot of public radio stations and ads and news sites, a tremendous number of advertisers from various places I had shopped, whether it was, like, ASOS or Netflix or AAA or whatever. But then when I went to the contact info section, they didn't have anything. And I wonder if, like, there is some measure here of personal responsibility, right? Like, I didn't put that data in, and so they don't have it. ANGWIN: Yeah. You didn't put it in maybe, and so that's why they don't have it. But that's not to say that your contact information isn't in other people's files, right? So that's the problem is that even if you limit what you put in, other people may contribute more. CORNISH: For people who hear this and decide that maybe they don't want Facebook to collect all of this information, is there anything they can actually do about it? ANGWIN: Well, there's a couple of things you can do. One is you can go and work your way through all of Facebook's privacy settings and try to lock them down as much as possible. Another thing you can do to get rid of the tracking that happens when you're not on Facebook, you can use ad tracking blocking tools. So I use something called Ghostery. There's something - Privacy Badger. There's a bunch of different tools like this, and they will say there are 15 different trackers on this website. Do you want to allow any of them? And you can say no. CORNISH: Is there too much focus on Facebook? I mean, when I think about consumer data brokers - right? - who are buying and selling information, every brand I've ever dealt with has collected my information, and I'm now a customer on one of their customer lists, which they buy and sell, it just seems like this is a small part of the problem that people are focusing on. ANGWIN: I mean, I think yes and no. Facebook is probably better at collecting so much data about everyone. Until two weeks ago when they announced they were ending it, they were actually buying data from data brokers about your off-line transactions and your level of income, your car ownership. They've just said they're going to end that. But the truth is that we're in a world where every business is racing to collect as much data as possible because Google and Facebook have shown that monetizing that data can be extremely lucrative. And so unfortunately, it's true that everyone is racing to get enormous amounts of data about us, and we don't have a huge amount of control over that until perhaps Congress maybe steps in with some of the things that they've been discussing in the past few days. CORNISH: That's Julia Angwin. She's an investigative reporter and author of the book \"Dragnet Nation: A Quest For Privacy, Security, And Freedom In A World Of Relentless Surveillance. \" Julia, thanks for speaking with us. ANGWIN: Thank you. AUDIE CORNISH, HOST:  Facebook CEO Mark Zuckerberg is back in Silicon Valley after two days of testimony on Capitol Hill. And lawmakers are focused on other issues today. But Facebook users and really anyone who uses the Internet, well, we're still wondering if there's any such thing as online privacy. Listen to this exchange from the House hearing yesterday between Representative Ben Ray Lujan of New Mexico and Zuckerberg. (SOUNDBITE OF ARCHIVED RECORDING) BEN RAY LUJAN: Facebook has detailed profiles on people who have never signed up for Facebook - yes or no. MARK ZUCKERBERG: Congressman, in general, we collect data on people have not signed up for Facebook for security purposes to prevent the kind of scraping that you were just referring to. CORNISH: To talk about the many ways Facebook collects data on people and what, if anything, we can do about it, we've reached out to Julia Angwin. She's an investigative reporter formerly at ProPublica. Welcome to the program. JULIA ANGWIN: It's great to be here. CORNISH: Whether you're on Facebook or not, what are the kinds of attributes Facebook is looking for? What are they looking for in the way of data? ANGWIN: Facebook is looking to know basically as much as possible about its users, things like this person likes grass or this person is really interested in a certain type of wrestling. There was one ad category they had which was a person who likes to pretend to text in awkward situations. You know, these are the kinds of things they'd like to know about you because they're hoping some advertiser will want to buy that ad category. CORNISH: I went and downloaded my Facebook archive to see what they had on me, and some things were not so surprising - right? - topics that I had clicked on, not surprisingly a lot of public radio stations and ads and news sites, a tremendous number of advertisers from various places I had shopped, whether it was, like, ASOS or Netflix or AAA or whatever. But then when I went to the contact info section, they didn't have anything. And I wonder if, like, there is some measure here of personal responsibility, right? Like, I didn't put that data in, and so they don't have it. ANGWIN: Yeah. You didn't put it in maybe, and so that's why they don't have it. But that's not to say that your contact information isn't in other people's files, right? So that's the problem is that even if you limit what you put in, other people may contribute more. CORNISH: For people who hear this and decide that maybe they don't want Facebook to collect all of this information, is there anything they can actually do about it? ANGWIN: Well, there's a couple of things you can do. One is you can go and work your way through all of Facebook's privacy settings and try to lock them down as much as possible. Another thing you can do to get rid of the tracking that happens when you're not on Facebook, you can use ad tracking blocking tools. So I use something called Ghostery. There's something - Privacy Badger. There's a bunch of different tools like this, and they will say there are 15 different trackers on this website. Do you want to allow any of them? And you can say no. CORNISH: Is there too much focus on Facebook? I mean, when I think about consumer data brokers - right? - who are buying and selling information, every brand I've ever dealt with has collected my information, and I'm now a customer on one of their customer lists, which they buy and sell, it just seems like this is a small part of the problem that people are focusing on. ANGWIN: I mean, I think yes and no. Facebook is probably better at collecting so much data about everyone. Until two weeks ago when they announced they were ending it, they were actually buying data from data brokers about your off-line transactions and your level of income, your car ownership. They've just said they're going to end that. But the truth is that we're in a world where every business is racing to collect as much data as possible because Google and Facebook have shown that monetizing that data can be extremely lucrative. And so unfortunately, it's true that everyone is racing to get enormous amounts of data about us, and we don't have a huge amount of control over that until perhaps Congress maybe steps in with some of the things that they've been discussing in the past few days. CORNISH: That's Julia Angwin. She's an investigative reporter and author of the book \"Dragnet Nation: A Quest For Privacy, Security, And Freedom In A World Of Relentless Surveillance. \" Julia, thanks for speaking with us. ANGWIN: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-12-601951556": {"title": "Can Social Media Have A Structure That Does More Good Than Harm? : NPR", "url": "https://www.npr.org/2018/04/12/601951556/can-social-media-have-a-structure-that-does-more-good-than-harm", "author": "No author found", "published_date": "2018-04-12", "content": "MARY LOUISE KELLY, HOST: I had dinner with friends a few nights ago, and people were talking about quitting Facebook. Some of them already had. They were freaked out by the revelations that led to Mark Zuckerberg testifying in front of Congress this week - revelations that a political data company called Cambridge Analytica had gotten access to personal information - personal information of about 90 million of us. Well, I haven't quit Facebook. But I think a lot about what happens when I open the app - the good and bad about my complicated relationship with Facebook and social media in general. Well, we're going to talk now with someone whose job it is to think about all this. Zeynep Tufekci is a techno sociologist at the University of North Carolina at Chapel Hill. Welcome to the program. ZEYNEP TUFEKCI: Thank you for inviting me. KELLY: Does the bad outweigh the good? TUFEKCI: That's a tough question. I think in some ways, this particular business model - surveillance and targeted advertising this way - is not compatible with a healthy democracy. On the other hand, it's a great connectivity tool. So rather than does the good outweigh the bad, the question for me is, how do we make the good outweigh the bad? You know, how do we proceed so that - just like earlier technologies - we no longer have lead in paint. Our cars have seatbelts and emission controls and airbags, right? We need to put the safety and oversight to our digital tools so that they're not a surveillance machine. KELLY: We'll stay with this point. If you're trying to design from scratch a social media app where the good would outweigh the bad, what would it look like? TUFEKCI: It would absolutely be one that has the person using it as the customer. Right now, when I use Facebook, I'm being sold. My attention is being sold, and my data is being used to sell me to advertisers. That's just not healthy. If I was starting from scratch, I would make the app a small amount of money that is - we pay for phones. We pay for SIM cards. We pay for our Netflix. We pay for all sorts of things we purchase. And a couple years ago, I wrote an op-ed for The New York Times where I looked at their SEC filings. And three years ago, it was about 20 cents per month to run the platform per person in the United States. That wasn't a lot of money. And if you consider the fact that ad technology is costing them a lot, it's quite plausible to have this level of connectivity that is affordable almost anywhere in the world. And you can subsidize a little bit, and you can have somewhat freer version. There's things you can do. KELLY: So you're talking about designing a social media platform with a fundamentally different business model than the existing ones have. TUFEKCI: Absolutely - because once it's infrastructure like this, you need to fix it rather than walk away. It's the same way you can't just walk away from dangerous roads. It needs to be safer. And that's the hopeful part. These technologies are really young. And what we've done is we've allowed this enormous data collection and this targeted advertising to be the main business model. So this should be a wakeup call that we shouldn't allow this to continue. And that we shouldn't feel bad about wanting to use it to connect with people. I mean, I think it's perfectly fine. A lot of people are feeling guilty that they're using Facebook, and my response is don't. It's a great product in many ways. We just want it to be safer, and we just want it to have the seatbelts. KELLY: But you're sounding somewhat optimistic, though, that this technology. . . TUFEKCI: I'm an optimistic. KELLY: . . . Is young enough and that there is room for this landscape to change quite a lot. TUFEKCI: Absolutely. I'm an optimist because if we did bring some oversight, if we did sort of break up this kind of power, if we limited data retention - we limited the surveillance and all these things - I think there would be this enormous boon to innovation. There would be these new business models. It's such a young technology. For all the talk of innovation, Silicon Valley right now is a very boring place. Everybody's trying to get purchased by Facebook or Google. That is not an innovative landscape. And we've got two giant ad brokers basically determining the whole economy - digital economy. We should not be cynical. We should not be resigned. We should say wait a minute, this is not the only way to do this. Let's keep all the good, and let's get rid of as much of the bad as we possibly can. KELLY: Dr. Tufekci, thank you. TUFEKCI: Thank you. KELLY: Zeynep Tufekci of the University of North Carolina at Chapel Hill. MARY LOUISE KELLY, HOST:  I had dinner with friends a few nights ago, and people were talking about quitting Facebook. Some of them already had. They were freaked out by the revelations that led to Mark Zuckerberg testifying in front of Congress this week - revelations that a political data company called Cambridge Analytica had gotten access to personal information - personal information of about 90 million of us. Well, I haven't quit Facebook. But I think a lot about what happens when I open the app - the good and bad about my complicated relationship with Facebook and social media in general. Well, we're going to talk now with someone whose job it is to think about all this. Zeynep Tufekci is a techno sociologist at the University of North Carolina at Chapel Hill. Welcome to the program. ZEYNEP TUFEKCI: Thank you for inviting me. KELLY: Does the bad outweigh the good? TUFEKCI: That's a tough question. I think in some ways, this particular business model - surveillance and targeted advertising this way - is not compatible with a healthy democracy. On the other hand, it's a great connectivity tool. So rather than does the good outweigh the bad, the question for me is, how do we make the good outweigh the bad? You know, how do we proceed so that - just like earlier technologies - we no longer have lead in paint. Our cars have seatbelts and emission controls and airbags, right? We need to put the safety and oversight to our digital tools so that they're not a surveillance machine. KELLY: We'll stay with this point. If you're trying to design from scratch a social media app where the good would outweigh the bad, what would it look like? TUFEKCI: It would absolutely be one that has the person using it as the customer. Right now, when I use Facebook, I'm being sold. My attention is being sold, and my data is being used to sell me to advertisers. That's just not healthy. If I was starting from scratch, I would make the app a small amount of money that is - we pay for phones. We pay for SIM cards. We pay for our Netflix. We pay for all sorts of things we purchase. And a couple years ago, I wrote an op-ed for The New York Times where I looked at their SEC filings. And three years ago, it was about 20 cents per month to run the platform per person in the United States. That wasn't a lot of money. And if you consider the fact that ad technology is costing them a lot, it's quite plausible to have this level of connectivity that is affordable almost anywhere in the world. And you can subsidize a little bit, and you can have somewhat freer version. There's things you can do. KELLY: So you're talking about designing a social media platform with a fundamentally different business model than the existing ones have. TUFEKCI: Absolutely - because once it's infrastructure like this, you need to fix it rather than walk away. It's the same way you can't just walk away from dangerous roads. It needs to be safer. And that's the hopeful part. These technologies are really young. And what we've done is we've allowed this enormous data collection and this targeted advertising to be the main business model. So this should be a wakeup call that we shouldn't allow this to continue. And that we shouldn't feel bad about wanting to use it to connect with people. I mean, I think it's perfectly fine. A lot of people are feeling guilty that they're using Facebook, and my response is don't. It's a great product in many ways. We just want it to be safer, and we just want it to have the seatbelts. KELLY: But you're sounding somewhat optimistic, though, that this technology. . . TUFEKCI: I'm an optimistic. KELLY: . . . Is young enough and that there is room for this landscape to change quite a lot. TUFEKCI: Absolutely. I'm an optimist because if we did bring some oversight, if we did sort of break up this kind of power, if we limited data retention - we limited the surveillance and all these things - I think there would be this enormous boon to innovation. There would be these new business models. It's such a young technology. For all the talk of innovation, Silicon Valley right now is a very boring place. Everybody's trying to get purchased by Facebook or Google. That is not an innovative landscape. And we've got two giant ad brokers basically determining the whole economy - digital economy. We should not be cynical. We should not be resigned. We should say wait a minute, this is not the only way to do this. Let's keep all the good, and let's get rid of as much of the bad as we possibly can. KELLY: Dr. Tufekci, thank you. TUFEKCI: Thank you. KELLY: Zeynep Tufekci of the University of North Carolina at Chapel Hill.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-12-601111484": {"title": "Really Random Numbers : NPR", "url": "https://www.npr.org/2018/04/12/601111484/really-random-numbers", "author": "No author found", "published_date": "2018-04-12", "content": "NOEL KING, HOST: A team of physicists has come up with a way to secure communications over the Internet, which is not a small deal in this day and age. Their approach involves creating truly random numbers. NPR's Joe Palca has the story. JOE PALCA, BYLINE: It's not hard to come up with an example of why randomness is key to online security. Peter Bierhorst is a mathematician at the National Institute of Standards and Technology in Boulder, Colo. PETER BIERHORST: Suppose you're logging into your bank's website from a computer, and it says, we don't recognize this computer; can we send you a six-digit, little one-time PIN through a text message? And then type it into your computer, and then we'll know it's you. PALCA: Now, with a six-digit PIN, if each digit is picked randomly, the chance of someone guessing the PIN on the first try is one in a million. But what if the PIN the bank sends is not really random? 3. 1415926535897932. . . Pi is known as an irrational number. It has an infinite number of digits. Bierhorst says, suppose you took six digits from pi. BIERHORST: That would look pretty unpatterned. But if you had some adversary, and they started figuring out what the bank was doing, which was just using the digits of pi in sequence one after the other for all of these little PINs, then that adversary would pretty soon figure that out and be able to hack into your secure communication with the bank. PALCA: So truly random numbers are what you need to assure as much security as possible. To get such numbers, Bierhorst and his colleagues turned to the weird and wonderful world of quantum physics. As they report in the journal Nature, the researchers used particles of light known as photons to test for randomness. BIERHORST: The way we set this experiment up is very subtle. PALCA: Without going into too much detail, the experiment made measurements of the photons that had been joined in a spooky physical state known as quantum entanglement. The measurements were made far enough apart so that the speed of light could not have affected the outcome. BIERHORST: And so if you don't believe that signaling faster than the speed of light is possible, you have to accept the random character in our measurement outcomes. PALCA: Bierhorst says now that they are sure their photon measurements are truly random, they plan to use the setup as a random number generator. Umesh Vazirani works with quantum physics at the University of California, Berkeley. He says Bierhorst and his colleagues aren't the first to try to use quantum physics to generate random numbers. UMESH VAZIRANI: It's such a surprise. In a classical world, you could never, ever certify something as random. It's something that the quantum world allows us to do. PALCA: The quantum world may allow us to do many more things, such as make unspeakably fast computers. But that's a story for another day. Joe Palca, NPR News. (SOUNDBITE OF SONG, \"ONE\")THREE DOG NIGHT: (Singing) One is the loneliest number that you ever do. Two can be as bad as one. It's the loneliest number since. . . NOEL KING, HOST:  A team of physicists has come up with a way to secure communications over the Internet, which is not a small deal in this day and age. Their approach involves creating truly random numbers. NPR's Joe Palca has the story. JOE PALCA, BYLINE: It's not hard to come up with an example of why randomness is key to online security. Peter Bierhorst is a mathematician at the National Institute of Standards and Technology in Boulder, Colo. PETER BIERHORST: Suppose you're logging into your bank's website from a computer, and it says, we don't recognize this computer; can we send you a six-digit, little one-time PIN through a text message? And then type it into your computer, and then we'll know it's you. PALCA: Now, with a six-digit PIN, if each digit is picked randomly, the chance of someone guessing the PIN on the first try is one in a million. But what if the PIN the bank sends is not really random? 3. 1415926535897932. . . Pi is known as an irrational number. It has an infinite number of digits. Bierhorst says, suppose you took six digits from pi. BIERHORST: That would look pretty unpatterned. But if you had some adversary, and they started figuring out what the bank was doing, which was just using the digits of pi in sequence one after the other for all of these little PINs, then that adversary would pretty soon figure that out and be able to hack into your secure communication with the bank. PALCA: So truly random numbers are what you need to assure as much security as possible. To get such numbers, Bierhorst and his colleagues turned to the weird and wonderful world of quantum physics. As they report in the journal Nature, the researchers used particles of light known as photons to test for randomness. BIERHORST: The way we set this experiment up is very subtle. PALCA: Without going into too much detail, the experiment made measurements of the photons that had been joined in a spooky physical state known as quantum entanglement. The measurements were made far enough apart so that the speed of light could not have affected the outcome. BIERHORST: And so if you don't believe that signaling faster than the speed of light is possible, you have to accept the random character in our measurement outcomes. PALCA: Bierhorst says now that they are sure their photon measurements are truly random, they plan to use the setup as a random number generator. Umesh Vazirani works with quantum physics at the University of California, Berkeley. He says Bierhorst and his colleagues aren't the first to try to use quantum physics to generate random numbers. UMESH VAZIRANI: It's such a surprise. In a classical world, you could never, ever certify something as random. It's something that the quantum world allows us to do. PALCA: The quantum world may allow us to do many more things, such as make unspeakably fast computers. But that's a story for another day. Joe Palca, NPR News. (SOUNDBITE OF SONG, \"ONE\") THREE DOG NIGHT: (Singing) One is the loneliest number that you ever do. Two can be as bad as one. It's the loneliest number since. . .", "section": "Joe's Big Idea", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-12-601217231": {"title": "Government Tests Its Response To Cyberattacks Amid Increasing Threats : NPR", "url": "https://www.npr.org/2018/04/12/601217231/cyber-storm-tests-government-reaction-to-cyberattacks-amid-increasing-threats", "author": "No author found", "published_date": "2018-04-12", "content": "", "section": "Politics", "disclaimer": ""}, "2018-04-13-602091004": {"title": "Facebook Users React To Privacy Breaches : NPR", "url": "https://www.npr.org/2018/04/13/602091004/facebook-users-react-to-privacy-breaches", "author": "No author found", "published_date": "2018-04-13", "content": "NOEL KING, HOST: Facebook has started letting around 87 million of its users know that their data may have been scooped up by the political data firm Cambridge Analytica. NPR's Laura Sydell talked to some Facebook users who got the notification. LAURA SYDELL, BYLINE: When Mark Zuckerberg testified before Congress, he said there'd been no mass exodus from the site since the revelations about improper use of personal data. Senator Ron Johnson, a Republican from Wisconsin, had this to say. (SOUNDBITE OF ARCHIVED RECORDING)RON JOHNSON: So it's kind of safe to say that Facebook users don't seem to be overly concerned about all these revelations, although obviously Congress apparently is. MARK ZUCKERBERG: Well, Senator, I think people are concerned about it. And I think these are incredibly important issues that people want us to address. SYDELL: In fact, a Gallup poll from last week shows that 55 percent of Americans are concerned about their personal data being sold and used by other companies. NPR reached out, on social media of course, to see how the people who got an alert this week felt. We got over 70 responses. KAREN CASEY: It made me feel violated and angry. SYDELL: Karen Casey, a 60-year-old who lives in Louisiana, got a Facebook alert this week that her information had been shared with Cambridge Analytica. She was bothered that they might have tried to manipulate her. CASEY: Using this data to study all these people psychologically and then to use that information to target people with a certain message - there's just something dark about that to me. SYDELL: Casey's been thinking about deleting her Facebook account, but it's the place where she connects with all her old high school pals. CASEY: What could I do differently? I'll get everybody's email and make sure I have everybody's phone number and then shut it down. SYDELL: That's a lot of work, and she hasn't done it yet. She's not the only one who's upset and yet finds it hard to part with Facebook. Carolyn Williamson works in government in Dallas, Texas, and she's involved in politics. For her, Facebook is an important communication platform that makes her hesitate to delete it. CAROLYN WILLIAMSON: I'm still not going to be getting the news. I'm still not going to be seeing what's going on. And if I do have a message that I want to get out, how do I get it out? SYDELL: This feeling of resignation was common to the more than 70 people who responded to NPR's call out. NICHOLAS MCDOWELL: I would love to be able to leave Facebook. SYDELL: Nicholas McDowell, a 46-year-old who lives in Silicon Valley, has tried to get off Facebook. He says every time he hears about a competitor, he tries it. MCDOWELL: But unless I can convince my current set of friends, my friends from high school, my friends from college, my parents, my siblings to kind of move over there en masse, it doesn't seem like going over there will be a similar experience. SYDELL: McDowell hopes that Congress will come up with a way to regulate consumer privacy online. So did others who spoke to NPR, though they were all skeptical that this Congress would manage to get it done. Laura Sydell, NPR News. NOEL KING, HOST:  Facebook has started letting around 87 million of its users know that their data may have been scooped up by the political data firm Cambridge Analytica. NPR's Laura Sydell talked to some Facebook users who got the notification. LAURA SYDELL, BYLINE: When Mark Zuckerberg testified before Congress, he said there'd been no mass exodus from the site since the revelations about improper use of personal data. Senator Ron Johnson, a Republican from Wisconsin, had this to say. (SOUNDBITE OF ARCHIVED RECORDING) RON JOHNSON: So it's kind of safe to say that Facebook users don't seem to be overly concerned about all these revelations, although obviously Congress apparently is. MARK ZUCKERBERG: Well, Senator, I think people are concerned about it. And I think these are incredibly important issues that people want us to address. SYDELL: In fact, a Gallup poll from last week shows that 55 percent of Americans are concerned about their personal data being sold and used by other companies. NPR reached out, on social media of course, to see how the people who got an alert this week felt. We got over 70 responses. KAREN CASEY: It made me feel violated and angry. SYDELL: Karen Casey, a 60-year-old who lives in Louisiana, got a Facebook alert this week that her information had been shared with Cambridge Analytica. She was bothered that they might have tried to manipulate her. CASEY: Using this data to study all these people psychologically and then to use that information to target people with a certain message - there's just something dark about that to me. SYDELL: Casey's been thinking about deleting her Facebook account, but it's the place where she connects with all her old high school pals. CASEY: What could I do differently? I'll get everybody's email and make sure I have everybody's phone number and then shut it down. SYDELL: That's a lot of work, and she hasn't done it yet. She's not the only one who's upset and yet finds it hard to part with Facebook. Carolyn Williamson works in government in Dallas, Texas, and she's involved in politics. For her, Facebook is an important communication platform that makes her hesitate to delete it. CAROLYN WILLIAMSON: I'm still not going to be getting the news. I'm still not going to be seeing what's going on. And if I do have a message that I want to get out, how do I get it out? SYDELL: This feeling of resignation was common to the more than 70 people who responded to NPR's call out. NICHOLAS MCDOWELL: I would love to be able to leave Facebook. SYDELL: Nicholas McDowell, a 46-year-old who lives in Silicon Valley, has tried to get off Facebook. He says every time he hears about a competitor, he tries it. MCDOWELL: But unless I can convince my current set of friends, my friends from high school, my friends from college, my parents, my siblings to kind of move over there en masse, it doesn't seem like going over there will be a similar experience. SYDELL: McDowell hopes that Congress will come up with a way to regulate consumer privacy online. So did others who spoke to NPR, though they were all skeptical that this Congress would manage to get it done. Laura Sydell, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-15-602494594": {"title": "Diamond And Silk Controversy Speaks To Broader Cultural Issues About The Right And Elites : NPR", "url": "https://www.npr.org/2018/04/15/602494594/facebook-admits-enforcement-error-in-how-it-handled-content-from-pro-trump-duo", "author": "No author found", "published_date": "2018-04-15", "content": "", "section": "Politics", "disclaimer": ""}, "2018-04-17-603093373": {"title": "Facebook To Face Class Action Over Facial Recognition : NPR", "url": "https://www.npr.org/2018/04/17/603093373/facebook-to-face-class-action-over-facial-recognition", "author": "No author found", "published_date": "2018-04-17", "content": "NOEL KING, HOST: A federal judge in San Francisco has ruled that Facebook users can go ahead with a class action lawsuit against the company. The lawsuit alleges that Facebook broke the law by creating facial templates for users without their permission. NPR's Aarti Shahani is with us. Good morning, Aarti. AARTI SHAHANI, BYLINE: Good morning. KING: All right. Let's talk about these plaintiffs. Who are they, and what is a facial template and why are they upset about it? SHAHANI: OK. So they are three Facebook users from Illinois, and they sued under a state law from 2008 called the Biometric Information Privacy Act. And it says that private entities like Facebook are not allowed to collect or retain people's biometrics, which can include things like retina scans, fingerprints, DNA, face geometry, without written consent. And so the wisdom of that is, like, you know, biometrics are unique. If a bank's computers get hacked, right, you get to have a new account or credit card number. If Facebook is hacked, you can't get a new face. So, you know, the plaintiffs who filed, it was three years ago, they said Facebook was harvesting face templates and tagging their faces for more than a billion people worldwide without informed, written consent. And they're seeking up to $5,000 in damages for each violation. So that would be billions of dollars total. KING: Billions of dollars. How does Facebook feel about all this? SHAHANI: (Laughter), well, you know what Facebook has to say in court is actually very different from what their CEO Mark Zuckerberg has said when he went to Capitol Hill last week. You know, there he was over and over saying he was sorry for his company's missteps and suggested his main fault was being too idealistic or too optimistic. But in court, in this case, his lawyers have been arguing, hey, no one was harmed in this procedure, and, arguably, you know, studying and tagging faces when pictures are uploaded is a service that lots of users like. It's been available for several years, and you can opt out of it if you want to. Facebook actually did manage to get the case moved to California, which is their home turf. But the judge, it's U. S. District Court James Donato in San Francisco, he says a class action is the most efficient way to resolve the dispute, and so it can proceed. A company spokesperson says in an email that Facebook is reviewing the ruling, that the case has, quote, \"no merit,\" and they'll defend against it vigorously. KING: And they have defended against lawsuits vigorously in the past, right? This isn't the first class action lawsuit against Facebook. Is this one different somehow? SHAHANI: Yeah. You know, it's not. And what stands out to me about this case isn't really the plaintiffs or Facebook so much as the judge. And so let me take you back seven years, OK? Back then Facebook was trying to get more users to connect to each other on Facebook, OK, to make it more of a go-to place. So the company, they did a little trick. They'd comb through your emails - I'm going to say you use Gmail - and they'd match the people you're emailing with their existing user base. So, like, let me give you an example. Say I'm emailing Grandpa, but he's not my quote-unquote \"friend. \" Well, Facebook would go and ping him with my profile photo and say, hey, do you want to be friends with her? It was called the Friend Finder feature, and lots of users didn't like it. They thought it was creepy. They were like, why are you talking to my grandfather? So they sued, saying that Facebook was using their images for commercial purposes. Then Facebook made a couple of counter arguments like, hey, you users accepted the terms, but also, you guys aren't celebrities. There's no commercial value to your pictures. There's no cognizable injury. And in that case, the judge sided with Facebook. So, you know, what struck me yesterday, it was like, the harm argument they've been making for years didn't manage to kill the class. Maybe the times are changing. KING: All right. Aarti, very quickly, the latest NPR/NewsHour/Marist poll is out. Gives us some insight into how the public feels about Facebook. How do people feel? SHAHANI: Yeah. Well, the numbers are stark. Only 12 percent of people polled have confidence in Facebook to protect privacy and personal information. KING: Twelve percent. All right. We're going to have to leave it there. NPR's Aarti Shahani, thanks so much. SHAHANI: Thank you. NOEL KING, HOST:  A federal judge in San Francisco has ruled that Facebook users can go ahead with a class action lawsuit against the company. The lawsuit alleges that Facebook broke the law by creating facial templates for users without their permission. NPR's Aarti Shahani is with us. Good morning, Aarti. AARTI SHAHANI, BYLINE: Good morning. KING: All right. Let's talk about these plaintiffs. Who are they, and what is a facial template and why are they upset about it? SHAHANI: OK. So they are three Facebook users from Illinois, and they sued under a state law from 2008 called the Biometric Information Privacy Act. And it says that private entities like Facebook are not allowed to collect or retain people's biometrics, which can include things like retina scans, fingerprints, DNA, face geometry, without written consent. And so the wisdom of that is, like, you know, biometrics are unique. If a bank's computers get hacked, right, you get to have a new account or credit card number. If Facebook is hacked, you can't get a new face. So, you know, the plaintiffs who filed, it was three years ago, they said Facebook was harvesting face templates and tagging their faces for more than a billion people worldwide without informed, written consent. And they're seeking up to $5,000 in damages for each violation. So that would be billions of dollars total. KING: Billions of dollars. How does Facebook feel about all this? SHAHANI: (Laughter), well, you know what Facebook has to say in court is actually very different from what their CEO Mark Zuckerberg has said when he went to Capitol Hill last week. You know, there he was over and over saying he was sorry for his company's missteps and suggested his main fault was being too idealistic or too optimistic. But in court, in this case, his lawyers have been arguing, hey, no one was harmed in this procedure, and, arguably, you know, studying and tagging faces when pictures are uploaded is a service that lots of users like. It's been available for several years, and you can opt out of it if you want to. Facebook actually did manage to get the case moved to California, which is their home turf. But the judge, it's U. S. District Court James Donato in San Francisco, he says a class action is the most efficient way to resolve the dispute, and so it can proceed. A company spokesperson says in an email that Facebook is reviewing the ruling, that the case has, quote, \"no merit,\" and they'll defend against it vigorously. KING: And they have defended against lawsuits vigorously in the past, right? This isn't the first class action lawsuit against Facebook. Is this one different somehow? SHAHANI: Yeah. You know, it's not. And what stands out to me about this case isn't really the plaintiffs or Facebook so much as the judge. And so let me take you back seven years, OK? Back then Facebook was trying to get more users to connect to each other on Facebook, OK, to make it more of a go-to place. So the company, they did a little trick. They'd comb through your emails - I'm going to say you use Gmail - and they'd match the people you're emailing with their existing user base. So, like, let me give you an example. Say I'm emailing Grandpa, but he's not my quote-unquote \"friend. \" Well, Facebook would go and ping him with my profile photo and say, hey, do you want to be friends with her? It was called the Friend Finder feature, and lots of users didn't like it. They thought it was creepy. They were like, why are you talking to my grandfather? So they sued, saying that Facebook was using their images for commercial purposes. Then Facebook made a couple of counter arguments like, hey, you users accepted the terms, but also, you guys aren't celebrities. There's no commercial value to your pictures. There's no cognizable injury. And in that case, the judge sided with Facebook. So, you know, what struck me yesterday, it was like, the harm argument they've been making for years didn't manage to kill the class. Maybe the times are changing. KING: All right. Aarti, very quickly, the latest NPR/NewsHour/Marist poll is out. Gives us some insight into how the public feels about Facebook. How do people feel? SHAHANI: Yeah. Well, the numbers are stark. Only 12 percent of people polled have confidence in Facebook to protect privacy and personal information. KING: Twelve percent. All right. We're going to have to leave it there. NPR's Aarti Shahani, thanks so much. SHAHANI: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-18-603629455": {"title": "Basketball, Marijuana And Poetry: These Police Tweet More Than Crime Alerts  : NPR", "url": "https://www.npr.org/2018/04/18/603629455/basketball-marijuana-and-poetry-these-police-tweet-more-than-crime-alerts", "author": "No author found", "published_date": "2018-04-18", "content": "AILSA CHANG, HOST: This Friday is 4/20, as in April 20. ARI SHAPIRO, HOST: Also as in 4:20, like marijuana. The police in Lawrence, Kan. have been tweeting some tips. DREW FENNELLY: (Reading) If it's 4:45 on 4/20 and you get pulled over and you've got orange or red Cheeto dust on your fingertips, that's what we call evidence. CHANG: (Laughter) OK. That is a joke, if it is not clear. Officer Drew Fennelly runs the department's official Twitter account. Here's one of his other tweets from yesterday. FENNELLY: (Reading) This post does not condone smoking Mary Jane. But my boss says I have to clarify that, as a police department, we do not condone smoking reefer. Again, driving high is still DUI, and that ain't no joke. CHANG: His tweets get thousands of likes. SHAPIRO: Officer Fennelly began the account in 2015. Back then he had to get approval for almost everything he posted. FENNELLY: Initially our tweets were bad. So we didn't have much of a following in the beginning. SHAPIRO: Then in March of 2016, he tweeted this about the University of Kansas's loss to Villanova. CHANG: (Reading) Sorry we can't investigate Villanova ripping your heart out of your chest. The crime occurred outside our jurisdiction. FENNELLY: It was the first tweet that we went with humor to try to get better engagement, and it ended up getting a couple thousand likes, over a thousand retweets. SHAPIRO: We started scrolling through the feed, and before we knew it, our staff was reciting the officer's tweets from this past Valentine's Day. UNIDENTIFIED PERSON #1: (Reading) Cops are not poets. So is this poem a haiku - absolutely not. Don't break the law, criminals. UNIDENTIFIED PERSON #2: (Reading) Shall I compare thee to a summer's day? Thou art more lovely and more temperate. Thou wander'st into a store and pays for thou's stuff before leaving. CHANG: Officer Drew Fennelly says he and his colleagues know that people in Lawrence are reading the posts. FENNELLY: People on the street, when they come up to officers, they want to talk about the Twitter account. And I feel like their familiarity with what we're putting out on Twitter makes them feel they can go up to an officer and say, hey, just want to say, love your Twitter account. For officers to be able to have that interaction, I think that's the best thing that we can hope for. CHANG: He says it's part of the department's community policing efforts. SHAPIRO: Lawrence Police Twitter account now has more than 100,000 followers - by some counts, more than the population of the city itself. AILSA CHANG, HOST:  This Friday is 4/20, as in April 20. ARI SHAPIRO, HOST:  Also as in 4:20, like marijuana. The police in Lawrence, Kan. have been tweeting some tips. DREW FENNELLY: (Reading) If it's 4:45 on 4/20 and you get pulled over and you've got orange or red Cheeto dust on your fingertips, that's what we call evidence. CHANG: (Laughter) OK. That is a joke, if it is not clear. Officer Drew Fennelly runs the department's official Twitter account. Here's one of his other tweets from yesterday. FENNELLY: (Reading) This post does not condone smoking Mary Jane. But my boss says I have to clarify that, as a police department, we do not condone smoking reefer. Again, driving high is still DUI, and that ain't no joke. CHANG: His tweets get thousands of likes. SHAPIRO: Officer Fennelly began the account in 2015. Back then he had to get approval for almost everything he posted. FENNELLY: Initially our tweets were bad. So we didn't have much of a following in the beginning. SHAPIRO: Then in March of 2016, he tweeted this about the University of Kansas's loss to Villanova. CHANG: (Reading) Sorry we can't investigate Villanova ripping your heart out of your chest. The crime occurred outside our jurisdiction. FENNELLY: It was the first tweet that we went with humor to try to get better engagement, and it ended up getting a couple thousand likes, over a thousand retweets. SHAPIRO: We started scrolling through the feed, and before we knew it, our staff was reciting the officer's tweets from this past Valentine's Day. UNIDENTIFIED PERSON #1: (Reading) Cops are not poets. So is this poem a haiku - absolutely not. Don't break the law, criminals. UNIDENTIFIED PERSON #2: (Reading) Shall I compare thee to a summer's day? Thou art more lovely and more temperate. Thou wander'st into a store and pays for thou's stuff before leaving. CHANG: Officer Drew Fennelly says he and his colleagues know that people in Lawrence are reading the posts. FENNELLY: People on the street, when they come up to officers, they want to talk about the Twitter account. And I feel like their familiarity with what we're putting out on Twitter makes them feel they can go up to an officer and say, hey, just want to say, love your Twitter account. For officers to be able to have that interaction, I think that's the best thing that we can hope for. CHANG: He says it's part of the department's community policing efforts. SHAPIRO: Lawrence Police Twitter account now has more than 100,000 followers - by some counts, more than the population of the city itself.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-19-602252391": {"title": "After Alert On Russian Hacks, Bigger Push To Protect Power Grid : NPR", "url": "https://www.npr.org/2018/04/19/602252391/after-alert-on-russian-hacks-bigger-push-to-protect-power-grid", "author": "No author found", "published_date": "2018-04-19", "content": "RACHEL MARTIN, HOST: U. S. energy companies are on alert for cyberattacks. Earlier this week, American and British officials warned that Russian hackers are targeting global Internet equipment. Russian hackers have also been blamed for a series of hacks against American power plants. StateImpact Pennsylvania's Marie Cusick looks at how the industry is trying to make sure your power does not get shut off. MARIE CUSICK, BYLINE: At the nation's largest grid operator, the report on the Russian attacks was no surprise. TOM O'BRIEN: You will never stop people from trying to get into your systems. CUSICK: Tom O'Brien works for PJM Interconnection. It serves 65 million people in the mid-Atlantic and Midwest. O'BRIEN: The question is what controls do you have to not allow them to penetrate and how do you respond in the event that they actually do get into your system? CUSICK: The constant threats are one reason why PJM has so many layers of security around its control center. UNIDENTIFIED PERSON #1: You are going to use your red badge against that and card in. Excellent, come on in. CUSICK: Recording equipment is banned. So the microphone is zipped into a bag and left at the security desk. Inside the control room, about 10 people are monitoring floor-to-ceiling displays showing real-time information from power plants. As one manager puts it, this is a highly orchestrated, 24/7 effort that goes unnoticed by most people. And that's the way they like it. But the industry definitely did take note two years ago when this happened. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: The first known hacker-caused power outage has occurred. So thousands of people in the Ukraine left in the dark, literally. CUSICK: Those attacks were widely blamed on Russia. O'Brien doesn't want to get into specifics about how PJM deals with cyberthreats, but one of the many lessons of the Ukraine attacks was a reminder to keep an eye out for odd communications. O'BRIEN: A very large percentage of entry points to attacks are coming through email. And that's why PJM as well as many others have aggressive phishing campaigns, we're training our employees. CUSICK: One way to limit exposure is by having separate systems. For example, industrial controls in a power plant aren't connected to corporate business networks. And since 2011, North American grid operators and government agencies have done large security exercises every two years with thousands of people practicing how they'd respond to a coordinated physical or cyber event. So far, nothing like that has happened. And it's not very likely, says Robert M. Lee. He's a former military intelligence analyst who runs his own cybersecurity firm called Dragos. ROBERT M. LEE: The more complex the system, the harder it is to have a scalable attack. CUSICK: Knocking out power to the entire East Coast for a week or a month would be very hard, he says. But briefly disrupting a major city is easier. That's what keeps him up at night. LEE: I worry about an adversary getting into maybe Washington, D. C. 's, portion of the grid, taking down power for maybe 30 minutes. CUSICK: The Department of Energy is trying to create a new office focused on cybersecurity and emergency response. But deterrence may be one reason why there has not yet been a major attack on the U. S. grid, says John MacWilliams. He's with Columbia University's Center on Global Energy Policy. JOHN MACWILLIAMS: That's obviously an act of war, and we have the capability of responding either through cyber mechanisms or kinetic military. CUSICK: In the meantime, small-scale incidents keep happening. This spring, another cyberattack targeted natural gas pipelines. Four companies shut down their computer systems just in case, but they say no service was disrupted. For NPR News, I'm Marie Cusick. RACHEL MARTIN, HOST:  U. S. energy companies are on alert for cyberattacks. Earlier this week, American and British officials warned that Russian hackers are targeting global Internet equipment. Russian hackers have also been blamed for a series of hacks against American power plants. StateImpact Pennsylvania's Marie Cusick looks at how the industry is trying to make sure your power does not get shut off. MARIE CUSICK, BYLINE: At the nation's largest grid operator, the report on the Russian attacks was no surprise. TOM O'BRIEN: You will never stop people from trying to get into your systems. CUSICK: Tom O'Brien works for PJM Interconnection. It serves 65 million people in the mid-Atlantic and Midwest. O'BRIEN: The question is what controls do you have to not allow them to penetrate and how do you respond in the event that they actually do get into your system? CUSICK: The constant threats are one reason why PJM has so many layers of security around its control center. UNIDENTIFIED PERSON #1: You are going to use your red badge against that and card in. Excellent, come on in. CUSICK: Recording equipment is banned. So the microphone is zipped into a bag and left at the security desk. Inside the control room, about 10 people are monitoring floor-to-ceiling displays showing real-time information from power plants. As one manager puts it, this is a highly orchestrated, 24/7 effort that goes unnoticed by most people. And that's the way they like it. But the industry definitely did take note two years ago when this happened. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #2: The first known hacker-caused power outage has occurred. So thousands of people in the Ukraine left in the dark, literally. CUSICK: Those attacks were widely blamed on Russia. O'Brien doesn't want to get into specifics about how PJM deals with cyberthreats, but one of the many lessons of the Ukraine attacks was a reminder to keep an eye out for odd communications. O'BRIEN: A very large percentage of entry points to attacks are coming through email. And that's why PJM as well as many others have aggressive phishing campaigns, we're training our employees. CUSICK: One way to limit exposure is by having separate systems. For example, industrial controls in a power plant aren't connected to corporate business networks. And since 2011, North American grid operators and government agencies have done large security exercises every two years with thousands of people practicing how they'd respond to a coordinated physical or cyber event. So far, nothing like that has happened. And it's not very likely, says Robert M. Lee. He's a former military intelligence analyst who runs his own cybersecurity firm called Dragos. ROBERT M. LEE: The more complex the system, the harder it is to have a scalable attack. CUSICK: Knocking out power to the entire East Coast for a week or a month would be very hard, he says. But briefly disrupting a major city is easier. That's what keeps him up at night. LEE: I worry about an adversary getting into maybe Washington, D. C. 's, portion of the grid, taking down power for maybe 30 minutes. CUSICK: The Department of Energy is trying to create a new office focused on cybersecurity and emergency response. But deterrence may be one reason why there has not yet been a major attack on the U. S. grid, says John MacWilliams. He's with Columbia University's Center on Global Energy Policy. JOHN MACWILLIAMS: That's obviously an act of war, and we have the capability of responding either through cyber mechanisms or kinetic military. CUSICK: In the meantime, small-scale incidents keep happening. This spring, another cyberattack targeted natural gas pipelines. Four companies shut down their computer systems just in case, but they say no service was disrupted. For NPR News, I'm Marie Cusick.", "section": "Environment And Energy Collaborative", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-19-603844604": {"title": "Robot Puts Together IKEA Chair : NPR", "url": "https://www.npr.org/2018/04/19/603844604/robot-puts-together-ikea-chair", "author": "No author found", "published_date": "2018-04-19", "content": "DAVID GREENE, HOST: Good morning. I'm David Greene. Do you know how to curse in Swedish? Me neither, though I feel like I do after building so much Ikea furniture in my life. I've put the wrong parts together and yelled out the Swedish name of that bedframe in anger, which makes this news especially painful. A robot successfully built an Ikea Stefan chair. Wired magazine says it took 20 minutes compared to the human average of a lifetime of misery. What's Swedish for I hate you? It's MORNING EDITION. DAVID GREENE, HOST:  Good morning. I'm David Greene. Do you know how to curse in Swedish? Me neither, though I feel like I do after building so much Ikea furniture in my life. I've put the wrong parts together and yelled out the Swedish name of that bedframe in anger, which makes this news especially painful. A robot successfully built an Ikea Stefan chair. Wired magazine says it took 20 minutes compared to the human average of a lifetime of misery. What's Swedish for I hate you? It's MORNING EDITION.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-20-595564470": {"title": "Artificial Intelligence For Natural Disasters : NPR", "url": "https://www.npr.org/2018/04/20/595564470/betting-on-artificial-intelligence-to-guide-earthquake-response", "author": "No author found", "published_date": "2018-04-20", "content": "AILSA CHANG, HOST: Planning for natural disasters involves dealing with a lot of uncertainty, but advances in artificial intelligence may allow computers to take some of the guesswork out of the planning process. NPR's science correspondent Joe Palca traveled to Palo Alto, Calif. That's where a startup company is using machine learning and artificial intelligence to advise fire departments on how best to respond in an earthquake. JOE PALCA, BYLINE: Computers may play a role in preparing for and responding to natural disasters like earthquakes, but Nicole Hu knows they'll never play the most important role. NICOLE HU: The true heroes on the ground are the ones who actually know how to respond in terms of a disaster. PALCA: Hu is chief technology officer for a startup called One Concern. Her company is trying to make the job of first responders easier when an earthquake hits. I met Hu at her well-appointed, open-plan office in Palo Alto. We sat in her conference room as she sketched out how she is using artificial intelligence to do this. The key is providing computers with mountains of data relevant to predicting what's at risk in an earthquake - for example, data about the built environment, things like homes and buildings. HU: We have almost every information about most buildings in the United States. PALCA: What they're made of, when they were built, how likely they are to collapse when the ground starts shaking. There's also a lot of data about the natural environment. HU: What is the soil like? What's the elevation like? What's the general humidity like? PALCA: What's the underlying geology in an area like? All this data lets you build maps of what regions are most vulnerable in an earthquake. HU: Machine learning does help you do that. PALCA: Here's basically how it works. The computer takes all this data and makes a prediction about what will happen in an earthquake. It then uses data from past earthquakes to see whether its predictions are any good and then revises its predictive models. In other words, it learns as it goes. Stanford University earthquake engineer Gregory Deierlein is an adviser for One Concern. He says one of the most remarkable things about the company's software is it can incorporate data from an earthquake as it's happening and adjust its predictions in real time. GREGORY DEIERLEIN: Those sort of things used to be research projects where after an event, we would collect data, and a few years later we'd produce new models. PALCA: Now the new models appear in a matter of minutes. Still, even though he's an adviser to One Concern, Deierlein says he doesn't know all the details of their approach. DEIERLEIN: Like many kind of startup companies, they're not fully transparent in everything they're doing. I mean, (laughter) that's their kind of proprietary knowledge that they're bringing to it. PALCA: Even if their methods are a bit opaque, some first responders are already convinced the software will be useful. I went to visit Fire Chief Dan Ghiorso of the Woodside Fire Protection District just up the road from Palo Alto. Ghiorso's district is about 32 square miles. DAN GHIORSO: So it's quite a large district. And we have the San Andreas fault running a couple hundred feet behind us. PALCA: Ghiorso says in the past when an earthquake hit, he'd have to make educated guesses about what parts of his district might have suffered the most damage and then drive out and have a look. The software changes that. He calls up a map of how his fire district might look if a 6. 9-magnitude earthquake struck nearby. GHIORSO: Instead of all of it being blue where there's no damage, we have some of what they call level two and then level three. PALCA: Level three are areas predicted to have bad damage. They show up as red on the map. GHIORSO: So what I would do is, if this was a live scenario, I would look at this and say, hey, we need to get some people out into those red areas and see, what is the damage? Is there really damage? PALCA: Now, these are just computer-generated predictions. But Ghiorso says he's sure they'll be extremely useful in a real disaster for showing where help is needed the most. GHIORSO: Instead of me taking my educated guess, they're putting science behind this. So I'm very confident. PALCA: Unfortunately, it's going to take a natural disaster to see if his confidence is fully justified. Joe Palca, NPR News. AILSA CHANG, HOST:  Planning for natural disasters involves dealing with a lot of uncertainty, but advances in artificial intelligence may allow computers to take some of the guesswork out of the planning process. NPR's science correspondent Joe Palca traveled to Palo Alto, Calif. That's where a startup company is using machine learning and artificial intelligence to advise fire departments on how best to respond in an earthquake. JOE PALCA, BYLINE: Computers may play a role in preparing for and responding to natural disasters like earthquakes, but Nicole Hu knows they'll never play the most important role. NICOLE HU: The true heroes on the ground are the ones who actually know how to respond in terms of a disaster. PALCA: Hu is chief technology officer for a startup called One Concern. Her company is trying to make the job of first responders easier when an earthquake hits. I met Hu at her well-appointed, open-plan office in Palo Alto. We sat in her conference room as she sketched out how she is using artificial intelligence to do this. The key is providing computers with mountains of data relevant to predicting what's at risk in an earthquake - for example, data about the built environment, things like homes and buildings. HU: We have almost every information about most buildings in the United States. PALCA: What they're made of, when they were built, how likely they are to collapse when the ground starts shaking. There's also a lot of data about the natural environment. HU: What is the soil like? What's the elevation like? What's the general humidity like? PALCA: What's the underlying geology in an area like? All this data lets you build maps of what regions are most vulnerable in an earthquake. HU: Machine learning does help you do that. PALCA: Here's basically how it works. The computer takes all this data and makes a prediction about what will happen in an earthquake. It then uses data from past earthquakes to see whether its predictions are any good and then revises its predictive models. In other words, it learns as it goes. Stanford University earthquake engineer Gregory Deierlein is an adviser for One Concern. He says one of the most remarkable things about the company's software is it can incorporate data from an earthquake as it's happening and adjust its predictions in real time. GREGORY DEIERLEIN: Those sort of things used to be research projects where after an event, we would collect data, and a few years later we'd produce new models. PALCA: Now the new models appear in a matter of minutes. Still, even though he's an adviser to One Concern, Deierlein says he doesn't know all the details of their approach. DEIERLEIN: Like many kind of startup companies, they're not fully transparent in everything they're doing. I mean, (laughter) that's their kind of proprietary knowledge that they're bringing to it. PALCA: Even if their methods are a bit opaque, some first responders are already convinced the software will be useful. I went to visit Fire Chief Dan Ghiorso of the Woodside Fire Protection District just up the road from Palo Alto. Ghiorso's district is about 32 square miles. DAN GHIORSO: So it's quite a large district. And we have the San Andreas fault running a couple hundred feet behind us. PALCA: Ghiorso says in the past when an earthquake hit, he'd have to make educated guesses about what parts of his district might have suffered the most damage and then drive out and have a look. The software changes that. He calls up a map of how his fire district might look if a 6. 9-magnitude earthquake struck nearby. GHIORSO: Instead of all of it being blue where there's no damage, we have some of what they call level two and then level three. PALCA: Level three are areas predicted to have bad damage. They show up as red on the map. GHIORSO: So what I would do is, if this was a live scenario, I would look at this and say, hey, we need to get some people out into those red areas and see, what is the damage? Is there really damage? PALCA: Now, these are just computer-generated predictions. But Ghiorso says he's sure they'll be extremely useful in a real disaster for showing where help is needed the most. GHIORSO: Instead of me taking my educated guess, they're putting science behind this. So I'm very confident. PALCA: Unfortunately, it's going to take a natural disaster to see if his confidence is fully justified. Joe Palca, NPR News.", "section": "Joe's Big Idea", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-20-604106157": {"title": "Master File Glitch Caused IRS Computer Shutdown : NPR", "url": "https://www.npr.org/2018/04/20/604106157/irs-computer-glitch-caused-by-master-file-issue", "author": "No author found", "published_date": "2018-04-20", "content": "", "section": "Politics", "disclaimer": ""}, "2018-04-22-604702022": {"title": "DEA Adds More Technology To Its Fight Against Heroin  : NPR", "url": "https://www.npr.org/2018/04/22/604702022/dea-adds-more-technology-to-its-fight-against-heroin", "author": "No author found", "published_date": "2018-04-22", "content": "LULU GARCIA-NAVARRO, HOST:  Another reason President Trump says he wants to build a wall along the Mexico border is because of drug trafficking, specifically in heroin. The Drug Enforcement Administration says more than 90 percent of the heroin coming into the United States arrives from Mexico. And that's where the DEA is sending drones and geolocation technology to find out where Mexican farmers are growing poppies, the crop used to make the drug. MIKE VIGIL: Once you attack the problem in one location, it's the balloon effect, where it shifts into another area. GARCIA-NAVARRO: That's Mike Vigil, the DEA's former chief of international operations and one of the agency's longest-serving agents in Mexico. VIGIL: I remember being in Mexico many, many years ago as a young agent. And the opium poppy crops were being planted throughout the mountains. You could just see massive amounts of fields, huge fields. And as the eradication efforts started to take place, those fields started to become smaller and smaller. They started to hide them in ravines. And they're very difficult to locate in certain areas. GARCIA-NAVARRO: You said it would be more efficient to create markets for other crops in Mexico. VIGIL: Eradication is not going to work because people depend on opium poppy cultivation to support themselves and to support their families. We have to give them an alternative. GARCIA-NAVARRO: And he stresses that America's relationship with Mexico is more valuable than any tech the DEA could employ. VIGIL: Mexico has always collaborated with the United States in terms of counterdrug efforts and other issues. One of the big factors is the exchange of information. And we need that because as far as I'm concerned, when it comes to stopping drugs and other issues that we have to deal with here in the United States, Mexico is the virtual wall, OK? GARCIA-NAVARRO: And what you mean by that is Mexico is the virtual wall because they have their own law enforcement acting as a barrier and protecting us from some of these things. . . VIGIL: That is absolutely correct. But when you have an individual as caustic as Donald Trump, you know, accusing Mexicans of being murderers and rapists and the fact that he's going to do away with the North American Free Trade Agreement, that he's going to build a wall, that - obviously, sending the National Guard troops to the border - you know, the Mexican government takes great offense at that. And if he continues, I can tell you that Mexico will probably not be as cooperative. And if they stop cooperating with the U. S. government, we are going to be in a world of hurt. GARCIA-NAVARRO: Mike Vigil is the former chief of international operations at the DEA and one of the longest-serving U. S. agents in Mexico. Thank you very much. VIGIL: Thank you, Lulu. (SOUNDBITE OF STAN FOREBEE'S \"THROUGH YOUR EYES\") LULU GARCIA-NAVARRO, HOST:   Another reason President Trump says he wants to build a wall along the Mexico border is because of drug trafficking, specifically in heroin. The Drug Enforcement Administration says more than 90 percent of the heroin coming into the United States arrives from Mexico. And that's where the DEA is sending drones and geolocation technology to find out where Mexican farmers are growing poppies, the crop used to make the drug. MIKE VIGIL: Once you attack the problem in one location, it's the balloon effect, where it shifts into another area. GARCIA-NAVARRO: That's Mike Vigil, the DEA's former chief of international operations and one of the agency's longest-serving agents in Mexico. VIGIL: I remember being in Mexico many, many years ago as a young agent. And the opium poppy crops were being planted throughout the mountains. You could just see massive amounts of fields, huge fields. And as the eradication efforts started to take place, those fields started to become smaller and smaller. They started to hide them in ravines. And they're very difficult to locate in certain areas. GARCIA-NAVARRO: You said it would be more efficient to create markets for other crops in Mexico. VIGIL: Eradication is not going to work because people depend on opium poppy cultivation to support themselves and to support their families. We have to give them an alternative. GARCIA-NAVARRO: And he stresses that America's relationship with Mexico is more valuable than any tech the DEA could employ. VIGIL: Mexico has always collaborated with the United States in terms of counterdrug efforts and other issues. One of the big factors is the exchange of information. And we need that because as far as I'm concerned, when it comes to stopping drugs and other issues that we have to deal with here in the United States, Mexico is the virtual wall, OK? GARCIA-NAVARRO: And what you mean by that is Mexico is the virtual wall because they have their own law enforcement acting as a barrier and protecting us from some of these things. . . VIGIL: That is absolutely correct. But when you have an individual as caustic as Donald Trump, you know, accusing Mexicans of being murderers and rapists and the fact that he's going to do away with the North American Free Trade Agreement, that he's going to build a wall, that - obviously, sending the National Guard troops to the border - you know, the Mexican government takes great offense at that. And if he continues, I can tell you that Mexico will probably not be as cooperative. And if they stop cooperating with the U. S. government, we are going to be in a world of hurt. GARCIA-NAVARRO: Mike Vigil is the former chief of international operations at the DEA and one of the longest-serving U. S. agents in Mexico. Thank you very much. VIGIL: Thank you, Lulu. (SOUNDBITE OF STAN FOREBEE'S \"THROUGH YOUR EYES\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-23-603140766": {"title": "#ScootersBehavingBadly: U.S. Cities Race To Keep Up With Small Vehicle Shares  : NPR", "url": "https://www.npr.org/2018/04/23/603140766/-scootersbehavingbadly-u-s-cities-race-to-keep-up-with-small-vehicle-shares", "author": "No author found", "published_date": "2018-04-23", "content": "ARI SHAPIRO, HOST: It's springtime in Washington, D. C. And around NPR's headquarters, there is more popping up than just flowers. All over the sidewalks there are shared bicycles, scooters, electric bikes. Lots of little companies are popping up to do this kind of transportation in major cities in the U. S. , and bigger companies are buying some of these smaller companies. In a moment, we're going to talk with a reporter for Curbed about what's going on. First we're going to try out some of these little vehicles. Our first stop is a scooter company called Bird. And there's one not far from here. We're going to head that way. (SOUNDBITE OF ESQUIVEL'S \"CIELITO LINDO\")SHAPIRO: OK, scooter's here. It's black. It says Bird. We're going to use the app to unlock it. (SOUNDBITE OF BEEPING)SHAPIRO: I'm going to take this one for a ride around the block and try not to kill myself. What makes it go? (SOUNDBITE OF MUSIC)UNIDENTIFIED PERSON: How'd it go? SHAPIRO: I did not fall. I did get some strange looks. I did not feel like I was going to die. It was a nice little ride around the block. Next up, Capital Bikeshare. This is a program that's been in Washington for a few years and lots of other cities, too. The bikes all go into docking stations, so there isn't the same problem with sidewalk clutter. (SOUNDBITE OF BEEPING)SHAPIRO: It's a beautiful day for a bike ride. (SOUNDBITE OF ESQUIVEL'S \"PRIMAVERA\")SHAPIRO: OK, our last ride of this little tour is an electric scooter company called LimeBike. We are in a rundown housing complex. I can't find the Lime scooter. The app says there's a Lime scooter right here, but it is nowhere to be seen. So I guess this is one of the kinks to be worked out in this new technology. We're going to head back to the studio and talk with Patrick Sisson, a journalist who's been writing about this for the website Curbed. (SOUNDBITE OF ESQUIVEL'S \"TICO TICO\")SHAPIRO: Hi, Patrick. PATRICK SISSON: Hey, Ari. SHAPIRO: We just heard a little bit of what it's like here in Washington, D. C. You're in California. There has been a lot of drama in San Francisco and Santa Monica. Tell us what's going on there. SISSON: Yeah. It's what some have called scootergeddon (ph). In San Francisco especially, people have been complaining quite a bit about these scooters, that they're blocking the right of way. San Francisco has issued a lot of declarations and passed laws that are looking for permitting of the scooters. They're actually even asking scooter riders to take photos of where they park their scooters to make sure they're not blocking the right of way. In Santa Monica, the company Bird settled for $300,000 with the city after being fined for safety violations and blocking the right of way. SHAPIRO: People are clearly annoyed. There's a lot of #ScootersBehavingBadly on Twitter. SISSON: Yeah, exactly. And this is also spreading to cities like Washington, D. C. , and Austin where people are having the same complaints. SHAPIRO: Well, speaking of Austin, in Austin, LimeBike deployed 200 scooters before the company had reached an agreement about rules with the city. And so I'm wondering whether local governments are ready to deal with these new technologies or if they're just playing catch-up in one city after another. SISSON: Well, in a lot of ways they are. And it sort of follows the same blueprint we saw with Uber and Lyft where tech companies are looking to provide transportation and mobility solutions, but they're asking for forgiveness after the fact instead of permission before the fact. SHAPIRO: So while you've got the inconvenience, the awkwardness, the frustration with local cities, is there also a problem that's being solved here? If somebody takes the train into work or takes the bus, that last mile-first mile problem of getting to where you're going from the public transportation hub sounds like a good use for these things. SISSON: Yeah. I mean, to be totally fair to the tech companies, they're on to something. I mean, there's a great desire for people to have car-free last mile transportation solutions. I remember speaking to someone in Santa Monica city government that says, you know, we really like the idea of having these around. It's just there's existing transportation infrastructure. There's safety issues. We've just got to make sure it's properly regulated and operating the way it should be. SHAPIRO: We're talking so much about scooters. D. C. is also cluttered with dockless bikes. There are several different brands out there. Are dockless bikes part of this equation, or are they already being eclipsed by scooters? SISSON: I think they're all part of the same equation. It's just a matter of urban mobility solutions, trying to get people around the city without cars and sort of solving that last mile problem. SHAPIRO: Are there any cities that are coming up with really creative solutions to these problems you're talking about? SISSON: There's a couple trials going on that I think are kind of interesting. Seattle's been trialing a sort of dockless bike parking space that maybe is showing what could be done in scooters and other situations. You're also seeing in San Francisco that Lyft is trialing a drop-off and pickup system where they would designate certain parts of the neighborhood for dropping off and picking up riders. There's a lot going on. And just, you know, I think we're still sort of in early days in terms of how this regulation is going to work. SHAPIRO: Patrick Sisson is a senior reporter for the website Curbed covering cities, transportation and architecture. Thanks so much. SISSON: Thanks. ARI SHAPIRO, HOST:  It's springtime in Washington, D. C. And around NPR's headquarters, there is more popping up than just flowers. All over the sidewalks there are shared bicycles, scooters, electric bikes. Lots of little companies are popping up to do this kind of transportation in major cities in the U. S. , and bigger companies are buying some of these smaller companies. In a moment, we're going to talk with a reporter for Curbed about what's going on. First we're going to try out some of these little vehicles. Our first stop is a scooter company called Bird. And there's one not far from here. We're going to head that way. (SOUNDBITE OF ESQUIVEL'S \"CIELITO LINDO\") SHAPIRO: OK, scooter's here. It's black. It says Bird. We're going to use the app to unlock it. (SOUNDBITE OF BEEPING) SHAPIRO: I'm going to take this one for a ride around the block and try not to kill myself. What makes it go? (SOUNDBITE OF MUSIC) UNIDENTIFIED PERSON: How'd it go? SHAPIRO: I did not fall. I did get some strange looks. I did not feel like I was going to die. It was a nice little ride around the block. Next up, Capital Bikeshare. This is a program that's been in Washington for a few years and lots of other cities, too. The bikes all go into docking stations, so there isn't the same problem with sidewalk clutter. (SOUNDBITE OF BEEPING) SHAPIRO: It's a beautiful day for a bike ride. (SOUNDBITE OF ESQUIVEL'S \"PRIMAVERA\") SHAPIRO: OK, our last ride of this little tour is an electric scooter company called LimeBike. We are in a rundown housing complex. I can't find the Lime scooter. The app says there's a Lime scooter right here, but it is nowhere to be seen. So I guess this is one of the kinks to be worked out in this new technology. We're going to head back to the studio and talk with Patrick Sisson, a journalist who's been writing about this for the website Curbed. (SOUNDBITE OF ESQUIVEL'S \"TICO TICO\") SHAPIRO: Hi, Patrick. PATRICK SISSON: Hey, Ari. SHAPIRO: We just heard a little bit of what it's like here in Washington, D. C. You're in California. There has been a lot of drama in San Francisco and Santa Monica. Tell us what's going on there. SISSON: Yeah. It's what some have called scootergeddon (ph). In San Francisco especially, people have been complaining quite a bit about these scooters, that they're blocking the right of way. San Francisco has issued a lot of declarations and passed laws that are looking for permitting of the scooters. They're actually even asking scooter riders to take photos of where they park their scooters to make sure they're not blocking the right of way. In Santa Monica, the company Bird settled for $300,000 with the city after being fined for safety violations and blocking the right of way. SHAPIRO: People are clearly annoyed. There's a lot of #ScootersBehavingBadly on Twitter. SISSON: Yeah, exactly. And this is also spreading to cities like Washington, D. C. , and Austin where people are having the same complaints. SHAPIRO: Well, speaking of Austin, in Austin, LimeBike deployed 200 scooters before the company had reached an agreement about rules with the city. And so I'm wondering whether local governments are ready to deal with these new technologies or if they're just playing catch-up in one city after another. SISSON: Well, in a lot of ways they are. And it sort of follows the same blueprint we saw with Uber and Lyft where tech companies are looking to provide transportation and mobility solutions, but they're asking for forgiveness after the fact instead of permission before the fact. SHAPIRO: So while you've got the inconvenience, the awkwardness, the frustration with local cities, is there also a problem that's being solved here? If somebody takes the train into work or takes the bus, that last mile-first mile problem of getting to where you're going from the public transportation hub sounds like a good use for these things. SISSON: Yeah. I mean, to be totally fair to the tech companies, they're on to something. I mean, there's a great desire for people to have car-free last mile transportation solutions. I remember speaking to someone in Santa Monica city government that says, you know, we really like the idea of having these around. It's just there's existing transportation infrastructure. There's safety issues. We've just got to make sure it's properly regulated and operating the way it should be. SHAPIRO: We're talking so much about scooters. D. C. is also cluttered with dockless bikes. There are several different brands out there. Are dockless bikes part of this equation, or are they already being eclipsed by scooters? SISSON: I think they're all part of the same equation. It's just a matter of urban mobility solutions, trying to get people around the city without cars and sort of solving that last mile problem. SHAPIRO: Are there any cities that are coming up with really creative solutions to these problems you're talking about? SISSON: There's a couple trials going on that I think are kind of interesting. Seattle's been trialing a sort of dockless bike parking space that maybe is showing what could be done in scooters and other situations. You're also seeing in San Francisco that Lyft is trialing a drop-off and pickup system where they would designate certain parts of the neighborhood for dropping off and picking up riders. There's a lot going on. And just, you know, I think we're still sort of in early days in terms of how this regulation is going to work. SHAPIRO: Patrick Sisson is a senior reporter for the website Curbed covering cities, transportation and architecture. Thanks so much. SISSON: Thanks.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-23-604384871": {"title": "Alexa, Tell Me A National Security Secret: Amazon's Reach Goes Beyond The Post O : NPR", "url": "https://www.npr.org/2018/04/23/604384871/alexa-tell-me-a-national-security-secret-amazons-reach-goes-beyond-the-post-offi", "author": "No author found", "published_date": "2018-04-23", "content": "ARI SHAPIRO, HOST: One of President Trump's favorite targets on Twitter is Amazon. He accuses the company of not collecting taxes, even though they do. And he charges the company with putting other retailers out of business. But Trump might be surprised to hear that one of Amazon's biggest customers is, in fact, the federal government. NPR's Brian Naylor reports. BRIAN NAYLOR, BYLINE: While Amazon still makes most of its money selling stuff online, a growing part of the company's business is storing stuff in the cloud. Amazon Web Services, or AWS, sells its cloud services to lots of companies, and increasingly to the government. Agencies from the Smithsonian to the CIA have awarded contracts to AWS. CIA Chief Information Officer John Edwards lavished praise on the company at a conference last year. (SOUNDBITE OF ARCHIVED RECORDING)JOHN EDWARDS: It's the best decision we ever made. It's the most innovative thing we've ever done. It is having a material difference - or making a material difference and having a material impact on both CIA and the IC. NAYLOR: For the IC, the Intelligence Community, and the CIA to trust Amazon with their data and their secrets is a big deal in government contracting circles. I'll tell you why in a minute. Daniel Ives, an analyst with GBH Insights, says Amazon Cloud Services is a growing part of the company's overall business, thanks, in part, to its pursuit of government contracts. DANIEL IVES: Amazon has strong tentacles into the federal government on AWS on their core cloud products. A number of years ago, they were basically background noise, but they've really become the key cloud player in the government. NAYLOR: Ives estimates Amazon could have $3 to $4 billion worth of business with the government annually in the next few years. That business could grow even more if the company is successful in winning a big contract with the Pentagon to host its data. The contract, called the Joint Enterprise Defense Infrastructure, or JEDI, could be as much as a $10 billion deal for whomever wins it. And that's why Amazon's success with the CIA is so important. At a recent congressional hearing, Defense Secretary James Mattis outlined his agency's needs. (SOUNDBITE OF ARCHIVED RECORDING)JAMES MATTIS: We have over 400 different basic data centers that we have to protect, and we have watched very closely what CIA got in terms of security and service from their movement to the cloud. It is a fair and open competition. NAYLOR: Still, Chris Cummiskey, a senior fellow at the George Washington University Center for Cyber and Homeland Security says Amazon may have an advantage, and that has competitors like Oracle, Microsoft and others upset and raising questions about risk. CHRIS CUMMISKEY: There is concern in the industry that if it's a sole source award, which is basically what they're talking about - awarding to a single company - that that would be not good for competition, not good for security and not good for the overall health of, you know, the competitive marketplace. NAYLOR: The JEDI contract is expected to be awarded this fall. Brian Naylor, NPR News, Washington. (SOUNDBITE OF MUSIC) ARI SHAPIRO, HOST:  One of President Trump's favorite targets on Twitter is Amazon. He accuses the company of not collecting taxes, even though they do. And he charges the company with putting other retailers out of business. But Trump might be surprised to hear that one of Amazon's biggest customers is, in fact, the federal government. NPR's Brian Naylor reports. BRIAN NAYLOR, BYLINE: While Amazon still makes most of its money selling stuff online, a growing part of the company's business is storing stuff in the cloud. Amazon Web Services, or AWS, sells its cloud services to lots of companies, and increasingly to the government. Agencies from the Smithsonian to the CIA have awarded contracts to AWS. CIA Chief Information Officer John Edwards lavished praise on the company at a conference last year. (SOUNDBITE OF ARCHIVED RECORDING) JOHN EDWARDS: It's the best decision we ever made. It's the most innovative thing we've ever done. It is having a material difference - or making a material difference and having a material impact on both CIA and the IC. NAYLOR: For the IC, the Intelligence Community, and the CIA to trust Amazon with their data and their secrets is a big deal in government contracting circles. I'll tell you why in a minute. Daniel Ives, an analyst with GBH Insights, says Amazon Cloud Services is a growing part of the company's overall business, thanks, in part, to its pursuit of government contracts. DANIEL IVES: Amazon has strong tentacles into the federal government on AWS on their core cloud products. A number of years ago, they were basically background noise, but they've really become the key cloud player in the government. NAYLOR: Ives estimates Amazon could have $3 to $4 billion worth of business with the government annually in the next few years. That business could grow even more if the company is successful in winning a big contract with the Pentagon to host its data. The contract, called the Joint Enterprise Defense Infrastructure, or JEDI, could be as much as a $10 billion deal for whomever wins it. And that's why Amazon's success with the CIA is so important. At a recent congressional hearing, Defense Secretary James Mattis outlined his agency's needs. (SOUNDBITE OF ARCHIVED RECORDING) JAMES MATTIS: We have over 400 different basic data centers that we have to protect, and we have watched very closely what CIA got in terms of security and service from their movement to the cloud. It is a fair and open competition. NAYLOR: Still, Chris Cummiskey, a senior fellow at the George Washington University Center for Cyber and Homeland Security says Amazon may have an advantage, and that has competitors like Oracle, Microsoft and others upset and raising questions about risk. CHRIS CUMMISKEY: There is concern in the industry that if it's a sole source award, which is basically what they're talking about - awarding to a single company - that that would be not good for competition, not good for security and not good for the overall health of, you know, the competitive marketplace. NAYLOR: The JEDI contract is expected to be awarded this fall. Brian Naylor, NPR News, Washington. (SOUNDBITE OF MUSIC)", "section": "Politics", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-24-605401427": {"title": "How Web Browsers Have Changed 25 Years After The Introduction Of Mosaic : NPR", "url": "https://www.npr.org/2018/04/24/605401427/how-web-browsers-have-changed-25-years-after-the-introduction-of-mosaic", "author": "No author found", "published_date": "2018-04-24", "content": "ARI SHAPIRO, HOST: Twenty-five years ago this week, a web browser called Mosaic was released to the public. AILSA CHANG, HOST: Before Mosaic existed, you had to know what you were looking for. With Mosaic, you had an easy way to discover things on the Internet. SHAPIRO: So on this anniversary, we travel back in time with our regular feature. . . (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PEOPLE: First mention. SHAPIRO: NPR listeners would have to wait two years after its launch to hear about Mosaic on the radio. CHANG: On March 4, 1995, gardening maven Ketzel Levine brought it to our attention not long after she discovered fellow gardeners were using computers to study up on gardening practices. (SOUNDBITE OF ARCHIVED BROADCAST)KETZEL LEVINE, BYLINE: What I discovered is this whole world of maniacs. And they're all communicating nonverbally on the Internet, in cyberspace, talking about anything they might talk about across the back garden gate. And this you need to have links or Veronica or Mosaic. SHAPIRO: NPR didn't really get around to explaining what Mosaic was until June of 1995 in this report from correspondent John McChesney. (SOUNDBITE OF ARCHIVED BROADCAST)JOHN MCCHESNEY, BYLINE: That part of the Internet called the World Wide Web is growing at a blistering pace. That's largely because of an easy-to-use graphical interface called Mosaic developed by a group of students at the University of Illinois at Champaign-Urbana. CHANG: By the time NPR was reporting on Mosaic in 1995, the students who had perfected it were already onto bigger things. They founded a new company that eventually became Netscape. Netscape morphed into Mozilla, which today makes the Firefox browser. ARI SHAPIRO, HOST:  Twenty-five years ago this week, a web browser called Mosaic was released to the public. AILSA CHANG, HOST:  Before Mosaic existed, you had to know what you were looking for. With Mosaic, you had an easy way to discover things on the Internet. SHAPIRO: So on this anniversary, we travel back in time with our regular feature. . . (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PEOPLE: First mention. SHAPIRO: NPR listeners would have to wait two years after its launch to hear about Mosaic on the radio. CHANG: On March 4, 1995, gardening maven Ketzel Levine brought it to our attention not long after she discovered fellow gardeners were using computers to study up on gardening practices. (SOUNDBITE OF ARCHIVED BROADCAST) KETZEL LEVINE, BYLINE: What I discovered is this whole world of maniacs. And they're all communicating nonverbally on the Internet, in cyberspace, talking about anything they might talk about across the back garden gate. And this you need to have links or Veronica or Mosaic. SHAPIRO: NPR didn't really get around to explaining what Mosaic was until June of 1995 in this report from correspondent John McChesney. (SOUNDBITE OF ARCHIVED BROADCAST) JOHN MCCHESNEY, BYLINE: That part of the Internet called the World Wide Web is growing at a blistering pace. That's largely because of an easy-to-use graphical interface called Mosaic developed by a group of students at the University of Illinois at Champaign-Urbana. CHANG: By the time NPR was reporting on Mosaic in 1995, the students who had perfected it were already onto bigger things. They founded a new company that eventually became Netscape. Netscape morphed into Mozilla, which today makes the Firefox browser.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-24-605107093": {"title": "Facebook Updates Community Standards, Expands Appeals Process : NPR", "url": "https://www.npr.org/2018/04/24/605107093/facebook-updates-community-standards-expands-appeals-process", "author": "No author found", "published_date": "2018-04-24", "content": "STEVE INSKEEP, HOST: Facebook says it will be more open about the posts it takes down. The company tells NPR that today it is publishing internal details of its community standards. That's the term for what's allowed on Facebook and what is not. Monika Bickert is a Facebook vice president. MONIKA BICKERT: So we've always had a set of community standards that the public can see that explain, for instance, no harassment, no bullying, no terror propaganda. But now we're actually explaining how we define those terms for our review teams and how we enforce those policies. INSKEEP: She says users want more openness, which is an understatement. The company is under unprecedented pressure. It's been roiled by two years of questions - which news did it promote during the last election? How widely did it share users' data? - and more. Now it is revealing definitions used by internal monitors who check up on complaints about posts around the world, like, what exactly constitutes a genuine death threat? If it names a person, location or weapon, that should come down. Or what exactly amounts to hate speech? BICKERT: Where we have drawn the line is that we will allow attacks or negative commentary about institutions or countries or religions, but we don't allow attacks against people. So if somebody is criticizing or attacking all members of a religion, that's where we would draw the line. INSKEEP: I wonder if one of the gray areas there might be someone who criticizes Islam but in an extreme way that somebody might argue is inciting people against Muslims. BICKERT: We do try to allow as much speech as possible about institutions, religions, countries, and we know sometimes that might make people uncomfortable. That's one of the reasons we give people a lot of choice and control over what they see on Facebook. You can unfollow pages, you can unfollow people, and you can block people that you don't want to communicate with. INSKEEP: How are you thinking about the environment as the 2018 election approaches and, of course, there will once again be lots of political speech on Facebook? BICKERT: Well, we know there are a lot of very serious issues, and it's important to get them right. We're focused on combating fake news. We're also focused on providing increased transparency into political advertisements and pages that have political content. And we're also investing a lot in our technical tools that help keep inauthentic accounts off the site. INSKEEP: Are you already going after fake accounts in that larger, more specific way in the United States here in 2018? BICKERT: Yes. The tools that we have developed to more effectively catch fake accounts - they've improved a lot, and we are using them globally. We now are able to stop more than a million fake accounts at the time of creation every day. INSKEEP: The publication of its internal standards is another signal that Facebook is having to acknowledge that it is effectively a publisher. It wants to find itself as a technology company, just a platform for other people's speech, but the founder, Mark Zuckerberg, now accepts some responsibility for what is posted. Facebook was embarrassed when a famous old Vietnam War photo was mistakenly censored and then put back up. It's also had to tussle with authoritarian governments like Russia and Turkey that demand some posts be taken down. Just last weekend, Sri Lankan officials complained to The New York Times that Facebook was not responsive enough to complaints of hate speech. Monika Bickert says that when pressured by governments, the company at least tries to keep up speech that meets its standards. What does this announcement suggest about the power your company has? BICKERT: I think what it suggests is that we really want to respond to what the community wants. What we're hearing is that they want more clarity, and they want to know how we enforce these rules. That's why we're doing this. And we're actually hopeful that this is going to spark a conversation. INSKEEP: But this is also a reminder, you've got this enormous fire hose of speech, maybe the world's largest fire hose of speech, and you can turn that fire hose on or off. It's your choice. BICKERT: I want to be very clear that when we make these policies, we don't do it in a vacuum. This is not my team sitting in a room in California saying, these will be the policies. Every time we adjust a policy, we have external input from experts around the world. INSKEEP: The company that claims some 2 billion users around the world insists it is straining to work within the laws of every country while still allowing as much speech as it can. STEVE INSKEEP, HOST:  Facebook says it will be more open about the posts it takes down. The company tells NPR that today it is publishing internal details of its community standards. That's the term for what's allowed on Facebook and what is not. Monika Bickert is a Facebook vice president. MONIKA BICKERT: So we've always had a set of community standards that the public can see that explain, for instance, no harassment, no bullying, no terror propaganda. But now we're actually explaining how we define those terms for our review teams and how we enforce those policies. INSKEEP: She says users want more openness, which is an understatement. The company is under unprecedented pressure. It's been roiled by two years of questions - which news did it promote during the last election? How widely did it share users' data? - and more. Now it is revealing definitions used by internal monitors who check up on complaints about posts around the world, like, what exactly constitutes a genuine death threat? If it names a person, location or weapon, that should come down. Or what exactly amounts to hate speech? BICKERT: Where we have drawn the line is that we will allow attacks or negative commentary about institutions or countries or religions, but we don't allow attacks against people. So if somebody is criticizing or attacking all members of a religion, that's where we would draw the line. INSKEEP: I wonder if one of the gray areas there might be someone who criticizes Islam but in an extreme way that somebody might argue is inciting people against Muslims. BICKERT: We do try to allow as much speech as possible about institutions, religions, countries, and we know sometimes that might make people uncomfortable. That's one of the reasons we give people a lot of choice and control over what they see on Facebook. You can unfollow pages, you can unfollow people, and you can block people that you don't want to communicate with. INSKEEP: How are you thinking about the environment as the 2018 election approaches and, of course, there will once again be lots of political speech on Facebook? BICKERT: Well, we know there are a lot of very serious issues, and it's important to get them right. We're focused on combating fake news. We're also focused on providing increased transparency into political advertisements and pages that have political content. And we're also investing a lot in our technical tools that help keep inauthentic accounts off the site. INSKEEP: Are you already going after fake accounts in that larger, more specific way in the United States here in 2018? BICKERT: Yes. The tools that we have developed to more effectively catch fake accounts - they've improved a lot, and we are using them globally. We now are able to stop more than a million fake accounts at the time of creation every day. INSKEEP: The publication of its internal standards is another signal that Facebook is having to acknowledge that it is effectively a publisher. It wants to find itself as a technology company, just a platform for other people's speech, but the founder, Mark Zuckerberg, now accepts some responsibility for what is posted. Facebook was embarrassed when a famous old Vietnam War photo was mistakenly censored and then put back up. It's also had to tussle with authoritarian governments like Russia and Turkey that demand some posts be taken down. Just last weekend, Sri Lankan officials complained to The New York Times that Facebook was not responsive enough to complaints of hate speech. Monika Bickert says that when pressured by governments, the company at least tries to keep up speech that meets its standards. What does this announcement suggest about the power your company has? BICKERT: I think what it suggests is that we really want to respond to what the community wants. What we're hearing is that they want more clarity, and they want to know how we enforce these rules. That's why we're doing this. And we're actually hopeful that this is going to spark a conversation. INSKEEP: But this is also a reminder, you've got this enormous fire hose of speech, maybe the world's largest fire hose of speech, and you can turn that fire hose on or off. It's your choice. BICKERT: I want to be very clear that when we make these policies, we don't do it in a vacuum. This is not my team sitting in a room in California saying, these will be the policies. Every time we adjust a policy, we have external input from experts around the world. INSKEEP: The company that claims some 2 billion users around the world insists it is straining to work within the laws of every country while still allowing as much speech as it can.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-24-604241476": {"title": "Sounding The Alarm About A New Russian Cyber Threat : NPR", "url": "https://www.npr.org/2018/04/24/604241476/sounding-the-alarm-about-a-new-russian-cyber-threat", "author": "No author found", "published_date": "2018-04-24", "content": "DAVID GREENE, HOST: The United States and the U. K. issued this rare joint alert last week warning that Russia is actively preparing for a future cyberwar against the West. Jeanette Manfra is the Department of Homeland Security's cybersecurity chief, and she says one dangerous technique is called spoofing. JEANETTE MANFRA: It allows an actor to pretend that they're the computer or the device that you think you're talking to. So they get into the middle of the connection between two different devices, and they can spy traffic that is going back and forth. They can manipulate the traffic. GREENE: Rachel Martin spoke to Manfra about this threat. MANFRA: This is very focused on what we call enterprise, or even small office or home office routers and switches. So these are the devices that basically make networks work. And what that means is that if somebody is sitting on those routers or those switches, they have full access to all of that data, all of your communications. They can see that. They can potentially manipulate that. And they have pretty broad access then to your network. RACHEL MARTIN, HOST: So that sounds horrible. MANFRA: I agree. MARTIN: Can I ask what has transpired to make this threat more severe now? MANFRA: We've issued previous alerts, but what we saw was that it was not reaching far enough and wide enough. Not enough people had access to this and knew to take action, so we felt that we needed to get it out to as many businesses, as many even home offices as possible, which necessitated a public alert. MARTIN: All right, so now let's tackle the solutions because there will be a lot of people out there who hear this and start to get real nervous about the idea of a cyberwarrior out of Russia getting into their home computer network. So how do you fix this? MANFRA: So it's reasonably simple. The vendor of the network infrastructure device, whether that's a router or a switch - the vendors are putting out guidance or have been putting out guidance that are specific to the make and the model of their network device. So organizations - you know, they need to go check what the vendor is, the make and the model. You can get online. You can download the vendor guidance for how to address it. MARTIN: So that's what an individual can do. What are you doing? What is the federal government's responsibility in trying to prevent these kind of attacks? And what can you do? MANFRA: We've been issuing guidance and alerts on whether it's vulnerabilities that we see - we've been issuing a series of alerts on North Korean activity. But, of course, we want to continue to ensure that there are consequences for malicious behavior. My department is focused on defense and ensuring that network defenders have what they need, but there are other tools that the government has to deter this activity, whether that's sanctions, criminal penalties, diplomatic engagement. There's a lot that the government is doing to try to impose consequences on this type of irresponsible behavior. MARTIN: If you see the threat increasing, though, are you satisfied with the punitive measures that have been put in place against Russia? Should the sanctions be more severe? Should there be more targeted repercussions? MANFRA: I believe the sanctions are pretty severe. And I also believe that publicly naming government for this type of behavior is important. And then, of course, I believe in the continued efforts of law enforcement to identify and prosecute those who are breaking our laws. MARTIN: Does the U. S. view a cyberwar as an actual war or a cyberattack in the same way that they perceive, for example, a physical attack on American infrastructure? If a power grid is disabled because of a cyberattack, and the result is that the power grid is down, how is that different than if it's bombed? MANFRA: That is a great question. And I would say this has been a question that our government has been thinking about for some time. I think about it in terms of actions against our critical infrastructure and our country that would have consequences about public health or safety or economic security. We would take that very, very seriously. MARTIN: What's the scenario that troubles you most? MANFRA: That we will miss something. We are doing everything that we can to ensure that that doesn't happen, but we need individuals - consumers, citizens - and we need companies to all recognize that they have a role to play in keeping this Internet ecosystem safe. MARTIN: Jeanette Manfra is in charge of cybersecurity for the Department of Homeland Security. Thank you so much for talking with us. MANFRA: Thank you. DAVID GREENE, HOST:  The United States and the U. K. issued this rare joint alert last week warning that Russia is actively preparing for a future cyberwar against the West. Jeanette Manfra is the Department of Homeland Security's cybersecurity chief, and she says one dangerous technique is called spoofing. JEANETTE MANFRA: It allows an actor to pretend that they're the computer or the device that you think you're talking to. So they get into the middle of the connection between two different devices, and they can spy traffic that is going back and forth. They can manipulate the traffic. GREENE: Rachel Martin spoke to Manfra about this threat. MANFRA: This is very focused on what we call enterprise, or even small office or home office routers and switches. So these are the devices that basically make networks work. And what that means is that if somebody is sitting on those routers or those switches, they have full access to all of that data, all of your communications. They can see that. They can potentially manipulate that. And they have pretty broad access then to your network. RACHEL MARTIN, HOST:  So that sounds horrible. MANFRA: I agree. MARTIN: Can I ask what has transpired to make this threat more severe now? MANFRA: We've issued previous alerts, but what we saw was that it was not reaching far enough and wide enough. Not enough people had access to this and knew to take action, so we felt that we needed to get it out to as many businesses, as many even home offices as possible, which necessitated a public alert. MARTIN: All right, so now let's tackle the solutions because there will be a lot of people out there who hear this and start to get real nervous about the idea of a cyberwarrior out of Russia getting into their home computer network. So how do you fix this? MANFRA: So it's reasonably simple. The vendor of the network infrastructure device, whether that's a router or a switch - the vendors are putting out guidance or have been putting out guidance that are specific to the make and the model of their network device. So organizations - you know, they need to go check what the vendor is, the make and the model. You can get online. You can download the vendor guidance for how to address it. MARTIN: So that's what an individual can do. What are you doing? What is the federal government's responsibility in trying to prevent these kind of attacks? And what can you do? MANFRA: We've been issuing guidance and alerts on whether it's vulnerabilities that we see - we've been issuing a series of alerts on North Korean activity. But, of course, we want to continue to ensure that there are consequences for malicious behavior. My department is focused on defense and ensuring that network defenders have what they need, but there are other tools that the government has to deter this activity, whether that's sanctions, criminal penalties, diplomatic engagement. There's a lot that the government is doing to try to impose consequences on this type of irresponsible behavior. MARTIN: If you see the threat increasing, though, are you satisfied with the punitive measures that have been put in place against Russia? Should the sanctions be more severe? Should there be more targeted repercussions? MANFRA: I believe the sanctions are pretty severe. And I also believe that publicly naming government for this type of behavior is important. And then, of course, I believe in the continued efforts of law enforcement to identify and prosecute those who are breaking our laws. MARTIN: Does the U. S. view a cyberwar as an actual war or a cyberattack in the same way that they perceive, for example, a physical attack on American infrastructure? If a power grid is disabled because of a cyberattack, and the result is that the power grid is down, how is that different than if it's bombed? MANFRA: That is a great question. And I would say this has been a question that our government has been thinking about for some time. I think about it in terms of actions against our critical infrastructure and our country that would have consequences about public health or safety or economic security. We would take that very, very seriously. MARTIN: What's the scenario that troubles you most? MANFRA: That we will miss something. We are doing everything that we can to ensure that that doesn't happen, but we need individuals - consumers, citizens - and we need companies to all recognize that they have a role to play in keeping this Internet ecosystem safe. MARTIN: Jeanette Manfra is in charge of cybersecurity for the Department of Homeland Security. Thank you so much for talking with us. MANFRA: Thank you.", "section": "National Security", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-26-605944175": {"title": "Amazon's Alexa Will Help Kids Learn Manners : NPR", "url": "https://www.npr.org/2018/04/26/605944175/amazons-alexa-will-help-kids-learn-manners", "author": "No author found", "published_date": "2018-04-26", "content": "RACHEL MARTIN, HOST: Good morning. I'm Rachel Martin. In its never-ending quest to be more and more involved in our lives, Amazon is now trying to help us get our kids to use their manners. Seems a lot of parents didn't like the fact that in order to activate Alexa, all it took was a bossy command. Amazon has updated the software so now if a kid asks Alexa to do something with a please attached, she'll say, thanks for asking nicely. Positive reinforcement, right? As far as we know, Alexa cannot yet get them to clean up their rooms. It's MORNING EDITION. RACHEL MARTIN, HOST:  Good morning. I'm Rachel Martin. In its never-ending quest to be more and more involved in our lives, Amazon is now trying to help us get our kids to use their manners. Seems a lot of parents didn't like the fact that in order to activate Alexa, all it took was a bossy command. Amazon has updated the software so now if a kid asks Alexa to do something with a please attached, she'll say, thanks for asking nicely. Positive reinforcement, right? As far as we know, Alexa cannot yet get them to clean up their rooms. It's MORNING EDITION.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-27-606393983": {"title": "Wikipedia Founder Says Internet Users Are Adrift In The 'Fake News' Era : NPR", "url": "https://www.npr.org/2018/04/27/606393983/wikipedia-founder-says-internet-users-are-adrift-in-the-fake-news-era", "author": "No author found", "published_date": "2018-04-27", "content": "ARI SHAPIRO, HOST:  How do you tell what's real from what's not on the Internet, especially when paid foreign agents and political operatives are working hard to create believable stories that are complete fiction? This is something Wikipedia co-founder Jimmy Wales has been thinking about. This week, he delivered the keynote address at an international media forum on the topic, and he joins us now from London. Welcome. JIMMY WALES: Thank you. SHAPIRO: I think a lot of the Internet giants - Facebook, YouTube, Wikipedia - were founded on a kind of idealistic hope that the more open and connected people were online, the better things would be for everyone. Do you still hold that belief? WALES: Yeah, very much. In this era where we've seen the rise of these fake news websites and so forth, Wikipedia has had almost no problems with this at all simply because our community is quite - you know, it's their hobby to debate about the quality of sources. And it's very difficult to fool the Wikipedia community with this. SHAPIRO: The Internet has certainly had a problem with this. And some would say it is a problem of too many people having too much access being able to offer too freely ideas that are completely made up. WALES: (Laughter) Yeah, well, that's just human life. All of us have a few idiot friends, and now they can share stuff on Facebook. The thing is, it's easy to be a little bit condescending about these people and to joke about idiots sharing nonsense. But the truth is, in free societies, people have a right to not be interested in the news. But when you're not that interested in the news and you do decide, hey, I think I want to find out some information, you still deserve to get quality information. And that's what we've really been lacking. SHAPIRO: There is now so much money and power resting in the distortion of the narrative, whether you're talking about a country like Russia acting in its geopolitical interest or a political activist who's willing to spend millions of dollars to get someone elected who will pursue a specific policy. When there is that much at stake, aren't the people who are fighting for factual accuracy going to be forever outgunned? WALES: We're getting there. I mean, a lot of people say they've lost trust in the media, and they think everything is propaganda. And then it becomes very hard to come to any kind of a consensus in society where we say, well, look; you and I disagree on a certain policy, but here are some facts underlying it, and we can at least agree on those facts, whereas now I feel like a lot of people are just adrift. They don't know what to believe. SHAPIRO: Is there one step that you think people could do that would go some distance towards undoing some of this problem? WALES: Maybe one step that a lot of people could do is immediately take a more skeptical attitude towards things that you're sharing online. Just take a second, and look in a search engine. Check some of the keywords and terms, and just make sure you're not adding to the problem. SHAPIRO: Do you think hundreds of millions of Americans are actually going to do that? WALES: It's not required that everybody do it. As long as some people in our social circles are vigilant about this sort of thing, they can raise the alarm. I'll give an example. I nearly posted something. So here was this story that said scientists have confirmed that if your cat was big enough, it would eat you. And that's a funny story, right? SHAPIRO: (Laughing) It's a great story. WALES: And we all kind of - we kind of believe it about cats, right? SHAPIRO: Right. WALES: And I was about to share it to my daughter. And I just thought, I'm going to google this quickly. And I did. And what I found was it linked to an original study. I opened that PDF, and I looked through it. And you know what? the study absolutely said nothing of the sort. So I didn't share it, and that was a good thing. And I felt like, whew, I nearly got duped by a great story. I mean, that's the thing. A lot of the things that do go viral, they go viral because you hear it, and you chuckle. And you're like, that's great. And it confirms something you believe is probably true anyway. So you just blast it off, and you've just been fooled. SHAPIRO: That's a great story, but I can also imagine a lot of listeners saying, I would never have taken those steps - open a PDF, read a scientific study. I see something funny on Facebook. I send it to my kid. WALES: (Laughter) Yeah, well, think again. You might be sharing fake news. SHAPIRO: Jimmy Wales, thank you so much for talking with us today. WALES: Yes. Thank you. SHAPIRO: He's the co-founder of Wikipedia. And this week, he gave the keynote address at the Westminster Media Forum titled \"Next Steps For Tackling Fake News. \" ARI SHAPIRO, HOST:   How do you tell what's real from what's not on the Internet, especially when paid foreign agents and political operatives are working hard to create believable stories that are complete fiction? This is something Wikipedia co-founder Jimmy Wales has been thinking about. This week, he delivered the keynote address at an international media forum on the topic, and he joins us now from London. Welcome. JIMMY WALES: Thank you. SHAPIRO: I think a lot of the Internet giants - Facebook, YouTube, Wikipedia - were founded on a kind of idealistic hope that the more open and connected people were online, the better things would be for everyone. Do you still hold that belief? WALES: Yeah, very much. In this era where we've seen the rise of these fake news websites and so forth, Wikipedia has had almost no problems with this at all simply because our community is quite - you know, it's their hobby to debate about the quality of sources. And it's very difficult to fool the Wikipedia community with this. SHAPIRO: The Internet has certainly had a problem with this. And some would say it is a problem of too many people having too much access being able to offer too freely ideas that are completely made up. WALES: (Laughter) Yeah, well, that's just human life. All of us have a few idiot friends, and now they can share stuff on Facebook. The thing is, it's easy to be a little bit condescending about these people and to joke about idiots sharing nonsense. But the truth is, in free societies, people have a right to not be interested in the news. But when you're not that interested in the news and you do decide, hey, I think I want to find out some information, you still deserve to get quality information. And that's what we've really been lacking. SHAPIRO: There is now so much money and power resting in the distortion of the narrative, whether you're talking about a country like Russia acting in its geopolitical interest or a political activist who's willing to spend millions of dollars to get someone elected who will pursue a specific policy. When there is that much at stake, aren't the people who are fighting for factual accuracy going to be forever outgunned? WALES: We're getting there. I mean, a lot of people say they've lost trust in the media, and they think everything is propaganda. And then it becomes very hard to come to any kind of a consensus in society where we say, well, look; you and I disagree on a certain policy, but here are some facts underlying it, and we can at least agree on those facts, whereas now I feel like a lot of people are just adrift. They don't know what to believe. SHAPIRO: Is there one step that you think people could do that would go some distance towards undoing some of this problem? WALES: Maybe one step that a lot of people could do is immediately take a more skeptical attitude towards things that you're sharing online. Just take a second, and look in a search engine. Check some of the keywords and terms, and just make sure you're not adding to the problem. SHAPIRO: Do you think hundreds of millions of Americans are actually going to do that? WALES: It's not required that everybody do it. As long as some people in our social circles are vigilant about this sort of thing, they can raise the alarm. I'll give an example. I nearly posted something. So here was this story that said scientists have confirmed that if your cat was big enough, it would eat you. And that's a funny story, right? SHAPIRO: (Laughing) It's a great story. WALES: And we all kind of - we kind of believe it about cats, right? SHAPIRO: Right. WALES: And I was about to share it to my daughter. And I just thought, I'm going to google this quickly. And I did. And what I found was it linked to an original study. I opened that PDF, and I looked through it. And you know what? the study absolutely said nothing of the sort. So I didn't share it, and that was a good thing. And I felt like, whew, I nearly got duped by a great story. I mean, that's the thing. A lot of the things that do go viral, they go viral because you hear it, and you chuckle. And you're like, that's great. And it confirms something you believe is probably true anyway. So you just blast it off, and you've just been fooled. SHAPIRO: That's a great story, but I can also imagine a lot of listeners saying, I would never have taken those steps - open a PDF, read a scientific study. I see something funny on Facebook. I send it to my kid. WALES: (Laughter) Yeah, well, think again. You might be sharing fake news. SHAPIRO: Jimmy Wales, thank you so much for talking with us today. WALES: Yes. Thank you. SHAPIRO: He's the co-founder of Wikipedia. And this week, he gave the keynote address at the Westminster Media Forum titled \"Next Steps For Tackling Fake News. \"", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-27-606346395": {"title": "This Is Your Brain On Ads | Hidden Brain  : NPR", "url": "https://www.npr.org/2018/04/27/606346395/this-is-your-brain-on-ads-how-media-companies-hijack-your-attention", "author": "No author found", "published_date": "2018-04-27", "content": "", "section": "Hidden Brain", "disclaimer": ""}, "2018-04-29-606382240": {"title": "In 'Emergency Contact,' Finding A Safe Space In Texts : NPR", "url": "https://www.npr.org/2018/04/29/606382240/in-emergency-contact-finding-a-safe-space-in-texts", "author": "No author found", "published_date": "2018-04-29", "content": "LULU GARCIA-NAVARRO, HOST:  How do we find a real connection in a device-reliant world? In Mary H. K. Choi's debut novel, \"Emergency Contact,\" Penny and Sam strike up a text-based romance and soon become take-your-phone-to-the-bathroom inseparable. But for different reasons, they have trouble making it real. Mary H. K. Choi joins me now from our studios in Culver City, Calif. , to talk about her book. Hi. MARY H K CHOI: Hi, how are you? GARCIA-NAVARRO: I'm great. So, I guess let's start with these two characters, Penny and Sam. Who are they at the start of this book? CHOI: So, at its start, Penny, who is a Korean-American person, is going off to college and Sam is in the throes of the worst breakup. He's kind of - he's homeless adjacent, and he's dealing with a lot of anxiety and panic. GARCIA-NAVARRO: And they kind of have this - not a meet-cute, which is what they're normally called when you have romantic leads meeting. It was a meet-anxiety. CHOI: Yes, it's definitely sort of meet-harrowing rather than meet-cute. Sam sort of has this panic attack on the street. And Penny, who is the type of person who would normally just sort of keep it moving and be like, ooh, someone else will deal with this, but she sees him, and she reaches out to him. And they become each other's emergency contact. GARCIA-NAVARRO: What does that mean to you - emergency contact? CHOI: It's that you have someone holding you down. And it might not be the person that you've thought it would be in terms of, like, it might not be your parent. It might not be your caregiver. It might not even be the most, I guess, Orthodox definition of who you're, like, besty is. It could be just someone else who makes the world feel like a safer space. It's like the tether to the spaceship when you're kind of like freefall floating out in outer space. And I really liked the idea, too, of having that be someone who lives inside your phone. . . GARCIA-NAVARRO: Right. CHOI: . . . Because you have that person in your, you know, literal back pocket but it's unencumbered by all this pressure of how you look, how they look. Like, is it romantic? Like, am I funny enough? Is that person thinking about how my hair is greasy? Do I have a zit? Like, it doesn't have all of the stress of that, especially if you're, like, cognitively a little bit atypical and you struggle with different visual cues or timing for who's speaking next. And, you know, I find texting to be kind of a safe space. GARCIA-NAVARRO: There are a lot of other themes in this book, which I want to talk to you about - race, class, friendship. Why did you want to tackle all of that in a YA novel? CHOI: The thing that I find interesting about teens now is that no matter how desperate we seem to be taxonomically othering them for one reason or another - because the internet, because whatever - you know, I feel like a lot of the benchmarks and the experiences are, you know, same for teens through time immemorial. GARCIA-NAVARRO: Hm. CHOI: And I wanted an old-school teen story that still had technology and felt very, very contemporary but with a lot of the sort of, like, bigger themes that are very real. Because teens now, it's this dual thing where they're either like super precocious because they're so good at the Internet and they're like YouTube billionaires or they're hopeless and they're depressed and, like, anxiety-ridden and overmedicated. And I wanted to give them credit. And I wanted to let them know that they're seen in some way. GARCIA-NAVARRO: There's also another powerful theme in this book, and that is sexual assault. And Penny has an experience in her past. CHOI: Right. It's funny - well, not funny, but people have described \"Emergency Contact\" as funny. And while it is really, really funny in moments, I definitely always kind of want to throw an asterisk on that because, to your point, there should be a trigger warning to this book about the sexual assault in it. And Penny has an experience where, you know - and she's like me. She's an indoor cat. Like, she is very into, like, climate conditioning. She loves computer - like, she loves the Internet. And, you know, she has a lot of social issues outside. And, so, she befriends someone who is a trusted person. And basically he betrays that trust and sexually assaults her. And for a long, long time, her brain can't compute that. And I feel like if someone is exposed to the #MeToo movement or Weinstein or Bill Cosby through the Internet, and if someone is in a position of seeing what they think, like, sexual trauma looks like or what the right type of victim is, it can be really confusing about how to define their own experiences. And I had a personal experience where, for a long, long time, I was gaslighting myself into thinking that one experience was not as big a deal as I thought it was. And. . . GARCIA-NAVARRO: I'm assuming you're saying that you had an experience with sexual assault. CHOI: Sexual assault, yeah. And it was really, really painful because I just - there is just a discord. And I was thinking about how there's so much conversation about mutually affirmative consent, and we all know the language, and we all know the conversations, but in each moment, when it's just two people and you're wildy inexperienced, like, you don't know what that's supposed to feel like a lot of the time. And I wanted to introduce some of that ambiguity back in like a blameless way where I wanted it to be OK for Penny to admit that she was sexually assaulted. And I wanted her to be OK with the fact that she did not want to tell anyone because I do think that there are situations in which you feel pressure to do one thing or another. GARCIA-NAVARRO: Yeah, to just speak out. CHOI: To speak out. GARCIA-NAVARRO: There is a lot of pressure to be public about what's happened to you. CHOI: There is a lot of pressure. And the thing I'm not saying - I'm not saying that you shouldn't say anything. But what I'm saying is that you don't have to be any type of person in that moment because you are absolutely blameless. And that's something that took me a really, really long time and also a lot of therapy to sort of get to. I say that it's kind of a time capsule in terms of me writing YA. It kind of does go both ways. It's more like a portal, which is like if I can at all share any wisdom that I've collected over my many, many years on planet Earth and if I can tell it to someone who's younger than me that they can use, like, that's great. It's like, you know, \"Back To The Future\" when Biff has, like, the almanac and makes all those bets and is, like, suddenly really, really rich in the future. It's like kind of that. If I can enrich anyone before they get to where I am at, like, I think that that is time well spent. And I would love to keep putting my weight behind it. GARCIA-NAVARRO: Mary H. K. Choi's new novel is \"Emergency Contact. \" Thank you very much. CHOI: Thank you. LULU GARCIA-NAVARRO, HOST:   How do we find a real connection in a device-reliant world? In Mary H. K. Choi's debut novel, \"Emergency Contact,\" Penny and Sam strike up a text-based romance and soon become take-your-phone-to-the-bathroom inseparable. But for different reasons, they have trouble making it real. Mary H. K. Choi joins me now from our studios in Culver City, Calif. , to talk about her book. Hi. MARY H K CHOI: Hi, how are you? GARCIA-NAVARRO: I'm great. So, I guess let's start with these two characters, Penny and Sam. Who are they at the start of this book? CHOI: So, at its start, Penny, who is a Korean-American person, is going off to college and Sam is in the throes of the worst breakup. He's kind of - he's homeless adjacent, and he's dealing with a lot of anxiety and panic. GARCIA-NAVARRO: And they kind of have this - not a meet-cute, which is what they're normally called when you have romantic leads meeting. It was a meet-anxiety. CHOI: Yes, it's definitely sort of meet-harrowing rather than meet-cute. Sam sort of has this panic attack on the street. And Penny, who is the type of person who would normally just sort of keep it moving and be like, ooh, someone else will deal with this, but she sees him, and she reaches out to him. And they become each other's emergency contact. GARCIA-NAVARRO: What does that mean to you - emergency contact? CHOI: It's that you have someone holding you down. And it might not be the person that you've thought it would be in terms of, like, it might not be your parent. It might not be your caregiver. It might not even be the most, I guess, Orthodox definition of who you're, like, besty is. It could be just someone else who makes the world feel like a safer space. It's like the tether to the spaceship when you're kind of like freefall floating out in outer space. And I really liked the idea, too, of having that be someone who lives inside your phone. . . GARCIA-NAVARRO: Right. CHOI: . . . Because you have that person in your, you know, literal back pocket but it's unencumbered by all this pressure of how you look, how they look. Like, is it romantic? Like, am I funny enough? Is that person thinking about how my hair is greasy? Do I have a zit? Like, it doesn't have all of the stress of that, especially if you're, like, cognitively a little bit atypical and you struggle with different visual cues or timing for who's speaking next. And, you know, I find texting to be kind of a safe space. GARCIA-NAVARRO: There are a lot of other themes in this book, which I want to talk to you about - race, class, friendship. Why did you want to tackle all of that in a YA novel? CHOI: The thing that I find interesting about teens now is that no matter how desperate we seem to be taxonomically othering them for one reason or another - because the internet, because whatever - you know, I feel like a lot of the benchmarks and the experiences are, you know, same for teens through time immemorial. GARCIA-NAVARRO: Hm. CHOI: And I wanted an old-school teen story that still had technology and felt very, very contemporary but with a lot of the sort of, like, bigger themes that are very real. Because teens now, it's this dual thing where they're either like super precocious because they're so good at the Internet and they're like YouTube billionaires or they're hopeless and they're depressed and, like, anxiety-ridden and overmedicated. And I wanted to give them credit. And I wanted to let them know that they're seen in some way. GARCIA-NAVARRO: There's also another powerful theme in this book, and that is sexual assault. And Penny has an experience in her past. CHOI: Right. It's funny - well, not funny, but people have described \"Emergency Contact\" as funny. And while it is really, really funny in moments, I definitely always kind of want to throw an asterisk on that because, to your point, there should be a trigger warning to this book about the sexual assault in it. And Penny has an experience where, you know - and she's like me. She's an indoor cat. Like, she is very into, like, climate conditioning. She loves computer - like, she loves the Internet. And, you know, she has a lot of social issues outside. And, so, she befriends someone who is a trusted person. And basically he betrays that trust and sexually assaults her. And for a long, long time, her brain can't compute that. And I feel like if someone is exposed to the #MeToo movement or Weinstein or Bill Cosby through the Internet, and if someone is in a position of seeing what they think, like, sexual trauma looks like or what the right type of victim is, it can be really confusing about how to define their own experiences. And I had a personal experience where, for a long, long time, I was gaslighting myself into thinking that one experience was not as big a deal as I thought it was. And. . . GARCIA-NAVARRO: I'm assuming you're saying that you had an experience with sexual assault. CHOI: Sexual assault, yeah. And it was really, really painful because I just - there is just a discord. And I was thinking about how there's so much conversation about mutually affirmative consent, and we all know the language, and we all know the conversations, but in each moment, when it's just two people and you're wildy inexperienced, like, you don't know what that's supposed to feel like a lot of the time. And I wanted to introduce some of that ambiguity back in like a blameless way where I wanted it to be OK for Penny to admit that she was sexually assaulted. And I wanted her to be OK with the fact that she did not want to tell anyone because I do think that there are situations in which you feel pressure to do one thing or another. GARCIA-NAVARRO: Yeah, to just speak out. CHOI: To speak out. GARCIA-NAVARRO: There is a lot of pressure to be public about what's happened to you. CHOI: There is a lot of pressure. And the thing I'm not saying - I'm not saying that you shouldn't say anything. But what I'm saying is that you don't have to be any type of person in that moment because you are absolutely blameless. And that's something that took me a really, really long time and also a lot of therapy to sort of get to. I say that it's kind of a time capsule in terms of me writing YA. It kind of does go both ways. It's more like a portal, which is like if I can at all share any wisdom that I've collected over my many, many years on planet Earth and if I can tell it to someone who's younger than me that they can use, like, that's great. It's like, you know, \"Back To The Future\" when Biff has, like, the almanac and makes all those bets and is, like, suddenly really, really rich in the future. It's like kind of that. If I can enrich anyone before they get to where I am at, like, I think that that is time well spent. And I would love to keep putting my weight behind it. GARCIA-NAVARRO: Mary H. K. Choi's new novel is \"Emergency Contact. \" Thank you very much. CHOI: Thank you.", "section": "Author Interviews", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-29-606773813": {"title": "What's An 'Incel'? The Online Community Behind The Toronto Van Attack : NPR", "url": "https://www.npr.org/2018/04/29/606773813/whats-an-incel-the-online-community-behind-the-toronto-van-attack", "author": "No author found", "published_date": "2018-04-29", "content": "LULU GARCIA-NAVARRO, HOST:  Last week, a van plowed into a busy Toronto sidewalk, killing 10 people, in what appeared to be a deliberate act. The suspect in the attack, Alek Minassian, was quickly linked to an online community of trolls and violent misogynists that called themselves incels, a term that stands for involuntarily celibate. We wanted to know more. Arshy Mann is a reporter at Xtra, a Toronto-based LGBTQ online magazine, and he's been covering the incel community. And he joins me now. Hi. ARSHY MANN: Thank you for having me. GARCIA-NAVARRO: What is the incel community? MANN: It's a online subculture of young men who feel very frustrated with their sexual and romantic lives. They get together online to kind of talk about this. But the way that they express that isn't through, kind of, working through their feelings or talking through their issues. Instead, they've moved towards a kind of really virulent misogyny. And they spend a lot of their time engaging in really violent ideation about the horrible things that they want to do to women and to sexually successful men. And they venerate a lot of these violent figures who have kind of come before, especially Elliot Rodger, who murdered a number of people in 2014 on the campus of the University of California, Santa Barbara. And they see him as a kind of hero. And they talk about replicating that violence. It's quite a disturbing part of the Internet. GARCIA-NAVARRO: You wrote in one of your articles about the incel community that, while misogyny is nothing new, the way these men organize and express it is. MANN: Yeah, exactly. I think what really distinguishes them is the fact that they find each other online, and they escalate their rhetoric consistently until it reaches this kind of violent feverish pitch. Obviously, there are a lot of young men and women out there who are not happy with either their romantic or sexual lives. But what distinguishes the incel subculture is that they get together, and they really egg each other on. It's a kind of process that is similar to other forms of radicalization that we might associate with terrorism, whether it's, say, ISIS or white supremacist terror. These men are told that they're worthless. And they're told that by their peers in these communities. And if they try to improve or leave these kinds of communities, they're pushed back against even more by their fellow community members. GARCIA-NAVARRO: And does it cross with racist rhetoric, like we may see in the \"alt-right\" and other communities? MANN: Yes. And so it often has a kind of racial tinge or racial panic element to it. Elliot Rodger, in his manifesto, talked about being especially horrified seeing, say, black men with white women. He had a feeling of entitlement to white women's bodies in particular. However, we shouldn't just - when it comes to incels specifically, they are not just limited to kind of young white men. There are men of all ethnicities who are kind of involved in this subculture. But, at least for a good number of them, these more misogynistic movements can often be an entry point into the more racialized or anti-Semitic branches of the \"alt-right. \"GARCIA-NAVARRO: Are these online communities, as far as you know, being monitored by law enforcement like an ISIS forum would be? MANN: We really don't have a sense of whether or not law enforcement had been monitoring these communities. But from the ways in which law enforcement in the past have treated online misogyny, which generally has been to not take it very seriously, I would be shocked, frankly, if these groups and these forums had been monitored in any kind of systematic or substantial way. GARCIA-NAVARRO: Arshy Mann is a reporter at Xtra. Thank you very much. MANN: Thank you. LULU GARCIA-NAVARRO, HOST:   Last week, a van plowed into a busy Toronto sidewalk, killing 10 people, in what appeared to be a deliberate act. The suspect in the attack, Alek Minassian, was quickly linked to an online community of trolls and violent misogynists that called themselves incels, a term that stands for involuntarily celibate. We wanted to know more. Arshy Mann is a reporter at Xtra, a Toronto-based LGBTQ online magazine, and he's been covering the incel community. And he joins me now. Hi. ARSHY MANN: Thank you for having me. GARCIA-NAVARRO: What is the incel community? MANN: It's a online subculture of young men who feel very frustrated with their sexual and romantic lives. They get together online to kind of talk about this. But the way that they express that isn't through, kind of, working through their feelings or talking through their issues. Instead, they've moved towards a kind of really virulent misogyny. And they spend a lot of their time engaging in really violent ideation about the horrible things that they want to do to women and to sexually successful men. And they venerate a lot of these violent figures who have kind of come before, especially Elliot Rodger, who murdered a number of people in 2014 on the campus of the University of California, Santa Barbara. And they see him as a kind of hero. And they talk about replicating that violence. It's quite a disturbing part of the Internet. GARCIA-NAVARRO: You wrote in one of your articles about the incel community that, while misogyny is nothing new, the way these men organize and express it is. MANN: Yeah, exactly. I think what really distinguishes them is the fact that they find each other online, and they escalate their rhetoric consistently until it reaches this kind of violent feverish pitch. Obviously, there are a lot of young men and women out there who are not happy with either their romantic or sexual lives. But what distinguishes the incel subculture is that they get together, and they really egg each other on. It's a kind of process that is similar to other forms of radicalization that we might associate with terrorism, whether it's, say, ISIS or white supremacist terror. These men are told that they're worthless. And they're told that by their peers in these communities. And if they try to improve or leave these kinds of communities, they're pushed back against even more by their fellow community members. GARCIA-NAVARRO: And does it cross with racist rhetoric, like we may see in the \"alt-right\" and other communities? MANN: Yes. And so it often has a kind of racial tinge or racial panic element to it. Elliot Rodger, in his manifesto, talked about being especially horrified seeing, say, black men with white women. He had a feeling of entitlement to white women's bodies in particular. However, we shouldn't just - when it comes to incels specifically, they are not just limited to kind of young white men. There are men of all ethnicities who are kind of involved in this subculture. But, at least for a good number of them, these more misogynistic movements can often be an entry point into the more racialized or anti-Semitic branches of the \"alt-right. \" GARCIA-NAVARRO: Are these online communities, as far as you know, being monitored by law enforcement like an ISIS forum would be? MANN: We really don't have a sense of whether or not law enforcement had been monitoring these communities. But from the ways in which law enforcement in the past have treated online misogyny, which generally has been to not take it very seriously, I would be shocked, frankly, if these groups and these forums had been monitored in any kind of systematic or substantial way. GARCIA-NAVARRO: Arshy Mann is a reporter at Xtra. Thank you very much. MANN: Thank you.", "section": "News", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-04-30-607036918": {"title": "Ex-CIA Director On National Security, Post-Truth 'Assault On Intelligence'  : NPR", "url": "https://www.npr.org/2018/04/30/607036918/ex-cia-director-on-national-security-post-truth-assault-on-intelligence", "author": "No author found", "published_date": "2018-04-30", "content": "MARY LOUISE KELLY, HOST: A possible summit with the leader of North Korea. A deadline to decide whether to restore sanctions on Iran. The move of the embassy from Tel Aviv to Jerusalem and Israel. President Trump has a heaping plate of foreign policy to consume in the month of May. At this point, most past presidents would be leaning on the intelligence community for guidance and context. But President Trump has made plain his differences with the CIA, FBI and NSA, and some members of the intelligence community say they feel sidelined by this White House. In his new book, former CIA Director Michael Hayden rips into President Trump for waging a war on U. S. spy agencies. And he calls out the Trump administration for ushering in what he calls a post-truth world. General Hayden spoke with our colleague Ailsa Chang about his book earlier today. AILSA CHANG, BYLINE: Welcome. MICHAEL HAYDEN: Thank you. CHANG: You say that intelligence in its best form is the objective truth, and that the goal of intelligence is to get inside the head of the president and of lawmakers when they're deciding policy. So what happens when policymakers don't believe in their intelligence apparatus? HAYDEN: Yeah, so they begin to make decisions based upon other inputs. Now, Ailsa, to be fair, it's rare that intelligence is the sole determinant of correct policy action. A lot of other legitimate things impact on a policymaker. CHANG: Sure. HAYDEN: But I've always viewed the role of intelligence as creating the left and the right-hand boundaries of logical policy discussion. So if you're beyond one side, you're dividing by zero or something, and if you're beyond the other side, you're assuming water can be made to run uphill. In other words, you've gone in your policymaking to places where objective reality just won't allow it to succeed. And so if you deny intelligence its role, you deny yourself these boundaries. And that of course, I think, leads to some really serious situations. CHANG: But you don't only say that President Trump distrusts the intel community. You go a level deeper. You say that Trump actually lacks the ability to distinguish truth from non-truth. What basis do you have for saying that? HAYDEN: So let me begin with the distrust of the community, which is a national tragedy. We always have a challenge establishing a relationship with a new president. President Trump was always going to see the world quite differently than the way we did. But then the relationship building, that's stunted by the Russian interference. The first time we had to seriously talk with this president, we were trying to convince him of something that other Americans were using to discredit his legitimacy as president. So we began this in a ditch. Now, over to the way I think God made the president, Michael Gerson, President George W. Bush's best speechwriter, said that the president lives in the eternal now. And the purpose of intelligence is to provide, what's the history? What are the likely consequences? And so we have a president here who is spontaneous, has almost a preternatural confidence in his own instincts. And to be fair to the president, those were the instincts that got him elected when a lot of experts said that wouldn't happen. CHANG: But are you saying that President Trump doesn't believe the intelligence, or he just finds it irrelevant to final decision-making? HAYDEN: I think it's more the latter. Look; we have a lot of presidents who've argued about intelligence. And if that were the only case here, this would be just one other story. But here's a case of a president who's shown evidence that he bases his decisions not on objective reality. A classic - he claimed that Barack Obama wiretapped Trump Tower. John Dickerson of CBS was pursuing him on this question, saying, do you have evidence, Mr. President? What's the evidence? And the president's only response was, a lot of people were saying; a lot of people agree with me. All right? That is decision-making based upon something other than objective reality. CHANG: But when you say the White House is causing great harm to the public's trust in the intel community, is that where all the blame should lie? I mean, you observe in your book there have been a lot of leaks from the intelligence community during this administration, many probably from career professionals. Along with that, you and several of your former colleagues have become some of Trump's most outspoken critics now. Can you blame the public if trust in the intel community is eroding when that community looks angry and fractured and willing to undermine this particular White House? HAYDEN: I get that we occasionally look angry, although I try to stay with the fact-based case. But I don't think we look fractured. I think we all share a very broad concern with what it is that the president is doing. CHANG: Fractured with the White House. HAYDEN: Yes. CHANG: And willing to undermine the White House publicly. HAYDEN: That's different. I don't know that. Now, I do freely admit in the book that this creates a great challenge for us because what we have here is a president who does not seem to be controlled by the traditional norms of the office. We have never seen a president speak like this, act like this, disparage people like this. And of course that generates a response from people who have been career professionals because we think what the president's doing is very harmful. The challenge we have is, how do we push back against that without breaking our own norms? And so for intelligence, it might be leaking. And so I have to be careful, people like me have to be careful when we are pushing back - which we think is quite legitimate - we don't make the problem worse by undercutting our own legitimacy. CHANG: You talk about Trump's disparaging style. You have criticized him and his use of Twitter. He's gone really hard on Twitter after Kim Jong Un, the North Korean leader, calling him names like Little Rocket Man. But here we are. Kim Jong Un and President Trump are trying to figure out a date and place to meet face to face for the first time. What do you think now? Is Trump's Twitter diplomacy, his belligerent style actually working here? HAYDEN: So a couple of thoughts. Number one, word is we are - now is the fourth time we've been here. This has happened with three other administrations broadly pretty much with the same promise and the same difficulties going forward. I want this to work. Full credit to President Trump and his team for amping up the pressure on the North Koreans - diplomatic isolation, military demonstrations, economic sanctions. That is great. But I don't know that even now - I'm looking back on that Twitter feud. I actually think that reduced the margin for error with regard to a very young and inexperienced North Korean leader during a very difficult time. So credit for getting us where we are, but I don't know that the Twitter use was what did it. CHANG: You lay a lot of blame with President Trump when it comes to dismissing truth, normalizing lying, propelling us into a so-called post-truth world, as you name it. How much is Trump singularly to blame for all of that? HAYDEN: Well, we have. And this is the core issue. We plural, the big we, are in a post-truth world, a world in which decisions are far more based upon emotion and preference than they are on objective reality, data and details. And that's an overturning of the Western way of thought since the Enlightenment. President Trump identified it. President Trump exploited it. And I think President Trump worsens it by some of the things he does in office. But the trend is well beyond him and well beyond America. CHANG: Michael Hayden is the former director of the CIA and of the National Security Agency. His new book is called \"The Assault On Intelligence: American National Security In An Age Of Lies. \" Thank you very much for joining us. HAYDEN: Thank you. MARY LOUISE KELLY, HOST:  A possible summit with the leader of North Korea. A deadline to decide whether to restore sanctions on Iran. The move of the embassy from Tel Aviv to Jerusalem and Israel. President Trump has a heaping plate of foreign policy to consume in the month of May. At this point, most past presidents would be leaning on the intelligence community for guidance and context. But President Trump has made plain his differences with the CIA, FBI and NSA, and some members of the intelligence community say they feel sidelined by this White House. In his new book, former CIA Director Michael Hayden rips into President Trump for waging a war on U. S. spy agencies. And he calls out the Trump administration for ushering in what he calls a post-truth world. General Hayden spoke with our colleague Ailsa Chang about his book earlier today. AILSA CHANG, BYLINE: Welcome. MICHAEL HAYDEN: Thank you. CHANG: You say that intelligence in its best form is the objective truth, and that the goal of intelligence is to get inside the head of the president and of lawmakers when they're deciding policy. So what happens when policymakers don't believe in their intelligence apparatus? HAYDEN: Yeah, so they begin to make decisions based upon other inputs. Now, Ailsa, to be fair, it's rare that intelligence is the sole determinant of correct policy action. A lot of other legitimate things impact on a policymaker. CHANG: Sure. HAYDEN: But I've always viewed the role of intelligence as creating the left and the right-hand boundaries of logical policy discussion. So if you're beyond one side, you're dividing by zero or something, and if you're beyond the other side, you're assuming water can be made to run uphill. In other words, you've gone in your policymaking to places where objective reality just won't allow it to succeed. And so if you deny intelligence its role, you deny yourself these boundaries. And that of course, I think, leads to some really serious situations. CHANG: But you don't only say that President Trump distrusts the intel community. You go a level deeper. You say that Trump actually lacks the ability to distinguish truth from non-truth. What basis do you have for saying that? HAYDEN: So let me begin with the distrust of the community, which is a national tragedy. We always have a challenge establishing a relationship with a new president. President Trump was always going to see the world quite differently than the way we did. But then the relationship building, that's stunted by the Russian interference. The first time we had to seriously talk with this president, we were trying to convince him of something that other Americans were using to discredit his legitimacy as president. So we began this in a ditch. Now, over to the way I think God made the president, Michael Gerson, President George W. Bush's best speechwriter, said that the president lives in the eternal now. And the purpose of intelligence is to provide, what's the history? What are the likely consequences? And so we have a president here who is spontaneous, has almost a preternatural confidence in his own instincts. And to be fair to the president, those were the instincts that got him elected when a lot of experts said that wouldn't happen. CHANG: But are you saying that President Trump doesn't believe the intelligence, or he just finds it irrelevant to final decision-making? HAYDEN: I think it's more the latter. Look; we have a lot of presidents who've argued about intelligence. And if that were the only case here, this would be just one other story. But here's a case of a president who's shown evidence that he bases his decisions not on objective reality. A classic - he claimed that Barack Obama wiretapped Trump Tower. John Dickerson of CBS was pursuing him on this question, saying, do you have evidence, Mr. President? What's the evidence? And the president's only response was, a lot of people were saying; a lot of people agree with me. All right? That is decision-making based upon something other than objective reality. CHANG: But when you say the White House is causing great harm to the public's trust in the intel community, is that where all the blame should lie? I mean, you observe in your book there have been a lot of leaks from the intelligence community during this administration, many probably from career professionals. Along with that, you and several of your former colleagues have become some of Trump's most outspoken critics now. Can you blame the public if trust in the intel community is eroding when that community looks angry and fractured and willing to undermine this particular White House? HAYDEN: I get that we occasionally look angry, although I try to stay with the fact-based case. But I don't think we look fractured. I think we all share a very broad concern with what it is that the president is doing. CHANG: Fractured with the White House. HAYDEN: Yes. CHANG: And willing to undermine the White House publicly. HAYDEN: That's different. I don't know that. Now, I do freely admit in the book that this creates a great challenge for us because what we have here is a president who does not seem to be controlled by the traditional norms of the office. We have never seen a president speak like this, act like this, disparage people like this. And of course that generates a response from people who have been career professionals because we think what the president's doing is very harmful. The challenge we have is, how do we push back against that without breaking our own norms? And so for intelligence, it might be leaking. And so I have to be careful, people like me have to be careful when we are pushing back - which we think is quite legitimate - we don't make the problem worse by undercutting our own legitimacy. CHANG: You talk about Trump's disparaging style. You have criticized him and his use of Twitter. He's gone really hard on Twitter after Kim Jong Un, the North Korean leader, calling him names like Little Rocket Man. But here we are. Kim Jong Un and President Trump are trying to figure out a date and place to meet face to face for the first time. What do you think now? Is Trump's Twitter diplomacy, his belligerent style actually working here? HAYDEN: So a couple of thoughts. Number one, word is we are - now is the fourth time we've been here. This has happened with three other administrations broadly pretty much with the same promise and the same difficulties going forward. I want this to work. Full credit to President Trump and his team for amping up the pressure on the North Koreans - diplomatic isolation, military demonstrations, economic sanctions. That is great. But I don't know that even now - I'm looking back on that Twitter feud. I actually think that reduced the margin for error with regard to a very young and inexperienced North Korean leader during a very difficult time. So credit for getting us where we are, but I don't know that the Twitter use was what did it. CHANG: You lay a lot of blame with President Trump when it comes to dismissing truth, normalizing lying, propelling us into a so-called post-truth world, as you name it. How much is Trump singularly to blame for all of that? HAYDEN: Well, we have. And this is the core issue. We plural, the big we, are in a post-truth world, a world in which decisions are far more based upon emotion and preference than they are on objective reality, data and details. And that's an overturning of the Western way of thought since the Enlightenment. President Trump identified it. President Trump exploited it. And I think President Trump worsens it by some of the things he does in office. But the trend is well beyond him and well beyond America. CHANG: Michael Hayden is the former director of the CIA and of the National Security Agency. His new book is called \"The Assault On Intelligence: American National Security In An Age Of Lies. \" Thank you very much for joining us. HAYDEN: Thank you.", "section": "Author Interviews", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-01-607483459": {"title": "Facebook's Mark Zuckerberg Tries To Move Company Forward After Data Scandal : NPR", "url": "https://www.npr.org/2018/05/01/607483459/facebooks-mark-zuckerberg-tries-to-move-company-forward-after-data-scandal", "author": "No author found", "published_date": "2018-05-01", "content": "AUDIE CORNISH, HOST: Once a year, Facebook holds a splashy conference for developers called Facebook F8. The social media giant rolls out its new features and generally promotes goodwill among its business partners. This year, there's more at stake. Today was the first time Facebook CEO Mark Zuckerberg spoke directly to developers who are at the heart of a controversy over Facebook's data sharing practices. NPR's Laura Sydell is at the F8 conference. Hey there, Laura. LAURA SYDELL, BYLINE: Hello. CORNISH: So you heard this speech Mark Zuckerberg gave today. What was the tone? SYDELL: Well, I would say that overall, he was trying to balance being optimistic with apologizing for some of the things that have happened. You know, Zuckerberg came up on stage, and the first thing he addressed was what happened over the past few months, which was that an app developer gave information of some 87 million Facebook users to Cambridge Analytica, a firm that was working with the Trump campaign. He said they've already changed their privacy policies, and he said that what happened would never happen today. That said, they're still making sure no apps are giving anything away. And he said no one should expect perfection. Here he is. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: Now, there's no guarantee that we get this right. This is hard stuff. We will make mistakes. And they will have consequences, and we will need to fix them. But what I can guarantee is that if we don't work on this, the world isn't moving in this direction by itself. SYDELL: And what he means there is that no one else out there is going to connect the world the way Facebook can. So he's trying to keep this balance between getting everybody excited about the mission of building Facebook and bringing the world together, which is what he always talks about. CORNISH: How did developers respond? SYDELL: You know, I spoke with a few of them, and some of them have actually lost customers since the whole privacy scandal began. One developer, Pete Haas, who helps small businesses on Facebook so that they can reach the right advertisers said many of them have pulled back, and they have a kind of wait-and-see feeling about the whole thing. He's concerned that if there are more controls on privacy, it's going to make it harder to target the users he thinks might buy their products. PETE HAAS: We use Facebook as the identity provider of the Internet and so that when you log in with Facebook, you can just do that easily. And when that happens, you request certain features. I just hope that they don't restrict those more. SYDELL: And what he means here is that Facebook might tighten its privacy controls even more and make it easier for people to opt out and therefore harder for them to target the exact customers they want to target. CORNISH: In the meantime, we know Facebook usually rolls out new features at this conference. Did they do that today? SYDELL: Yeah, and I would say the biggest thing is that they're getting in to the dating business. If I were match. com or other dating sites, I would be worried. What they're doing is they're setting up kind of a separate area. It's separate from regular Facebook where you can communicate. If you're a single person, you can put up your profile. Your other friends don't have to see it. If someone single is going to an event near you, you'll be able to chat with them first, and maybe you want to meet up. Now, here's the thing that's interesting - is Facebook really has the potential to connect people who are really going to share interests 'cause they have all this personal data on you. They haven't said how much of it they're going to use. But of course it's ironic that right now after they've had this huge privacy scandal, they now are putting themself out there as a dating site. And as such, they'll be better at it the more of your personal data that they actually have. CORNISH: That's NPR's Laura Sydell. Laura, thank you. SYDELL: You are quite welcome. AUDIE CORNISH, HOST:  Once a year, Facebook holds a splashy conference for developers called Facebook F8. The social media giant rolls out its new features and generally promotes goodwill among its business partners. This year, there's more at stake. Today was the first time Facebook CEO Mark Zuckerberg spoke directly to developers who are at the heart of a controversy over Facebook's data sharing practices. NPR's Laura Sydell is at the F8 conference. Hey there, Laura. LAURA SYDELL, BYLINE: Hello. CORNISH: So you heard this speech Mark Zuckerberg gave today. What was the tone? SYDELL: Well, I would say that overall, he was trying to balance being optimistic with apologizing for some of the things that have happened. You know, Zuckerberg came up on stage, and the first thing he addressed was what happened over the past few months, which was that an app developer gave information of some 87 million Facebook users to Cambridge Analytica, a firm that was working with the Trump campaign. He said they've already changed their privacy policies, and he said that what happened would never happen today. That said, they're still making sure no apps are giving anything away. And he said no one should expect perfection. Here he is. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: Now, there's no guarantee that we get this right. This is hard stuff. We will make mistakes. And they will have consequences, and we will need to fix them. But what I can guarantee is that if we don't work on this, the world isn't moving in this direction by itself. SYDELL: And what he means there is that no one else out there is going to connect the world the way Facebook can. So he's trying to keep this balance between getting everybody excited about the mission of building Facebook and bringing the world together, which is what he always talks about. CORNISH: How did developers respond? SYDELL: You know, I spoke with a few of them, and some of them have actually lost customers since the whole privacy scandal began. One developer, Pete Haas, who helps small businesses on Facebook so that they can reach the right advertisers said many of them have pulled back, and they have a kind of wait-and-see feeling about the whole thing. He's concerned that if there are more controls on privacy, it's going to make it harder to target the users he thinks might buy their products. PETE HAAS: We use Facebook as the identity provider of the Internet and so that when you log in with Facebook, you can just do that easily. And when that happens, you request certain features. I just hope that they don't restrict those more. SYDELL: And what he means here is that Facebook might tighten its privacy controls even more and make it easier for people to opt out and therefore harder for them to target the exact customers they want to target. CORNISH: In the meantime, we know Facebook usually rolls out new features at this conference. Did they do that today? SYDELL: Yeah, and I would say the biggest thing is that they're getting in to the dating business. If I were match. com or other dating sites, I would be worried. What they're doing is they're setting up kind of a separate area. It's separate from regular Facebook where you can communicate. If you're a single person, you can put up your profile. Your other friends don't have to see it. If someone single is going to an event near you, you'll be able to chat with them first, and maybe you want to meet up. Now, here's the thing that's interesting - is Facebook really has the potential to connect people who are really going to share interests 'cause they have all this personal data on you. They haven't said how much of it they're going to use. But of course it's ironic that right now after they've had this huge privacy scandal, they now are putting themself out there as a dating site. And as such, they'll be better at it the more of your personal data that they actually have. CORNISH: That's NPR's Laura Sydell. Laura, thank you. SYDELL: You are quite welcome.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-01-607321023": {"title": "Would We Be Better Off If We Didn't Rely On 1 Social Network? : NPR", "url": "https://www.npr.org/2018/05/01/607321023/would-we-be-better-off-if-we-didnt-rely-on-1-social-network", "author": "No author found", "published_date": "2018-05-01", "content": "STEVE INSKEEP, HOST:  Some technology experts are asking what it would mean to create an alternative to Facebook. The social network has had a rough couple of years. It faces criticism for the way it shared users' data and the way it was used during the presidential campaign. It faces a movement to quit Facebook. But, of course, the question is, quit Facebook in favor of what? If the people you know are all on Facebook, quitting it would be like quitting the power grid - possible, but not so easy - for now. NPR's Laurel Wamsley reports. LAUREL WAMSLEY, BYLINE: When Mark Zuckerberg was grilled last month on Capitol Hill, Senator Lindsey Graham asked him a question a lot of people are asking in one way or another. (SOUNDBITE OF ARCHIVED RECORDING)LINDSEY GRAHAM: If I buy a Ford, and it doesn't work well and I don't like it, I can buy a Chevy. If I'm upset with Facebook, what's the equivalent product that I can go sign up for? WAMSLEY: Zuckerberg said, well, there are a lot of companies that do some form of some part of what Facebook does. But the short answer to the question of - what's a Facebook that's not Facebook? - is, there isn't one. Two-thirds of American adults use Facebook, and three-quarters of those use it every day. I'm one of them, even though I'm uncomfortable with how much the company knows about me and how I'm targeted with advertising based on that data. But I use Facebook for my job, and I also don't want to miss out on party invites from my friends. So despite my misgivings, it's hard to think of quitting the platform for good. But Facebook is only 14 years old, and knowing what we know now about the flaws in its design, how might we go about creating the next social network, one that doesn't have those same problems? CATHY O'NEIL: I really think Facebook is destructive. WAMSLEY: That's Cathy O'Neil. She's a mathematician and the author of a book about the potentially dangerous consequences of algorithms. She says she imagines the next social network as having the best parts of Facebook without the worst parts of Facebook. O'NEIL: Basically, a town square where people can interact, they can keep up with each other but without the sort of commercial, predatory aspect. WAMSLEY: Her envisioned social network would have a moderator curating the conversation, and it would be a nonprofit. But she doesn't really want the government to run the system either because then they'd probably collect too much data about all of us. Another person thinking about building a better social network as Ethan Zuckerman. He's director of the Center for Civic Media at the MIT Media Lab. He thinks it's a problem that Facebook is so big. ETHAN ZUCKERMAN: Facebook has an awful lot of power by virtue of the fact that you have a single company making decisions for about 2 billion people. WAMSLEY: He says the next iteration of social networks could be decentralized instead of run by one company. A good example of this is a network that already exists, an open-source software called Mastodon. ZUCKERMAN: You can install it, put it up on a Web server. At that point, you're running a Mastodon node. WAMSLEY: It's a sort of replacement Twitter where anyone can create their own community with their own rules. Another problem that he'd fix. . . ZUCKERMAN: We don't really have control over the algorithms that sort our information and choose what we see or don't see. WAMSLEY: Zuckerman and his MIT colleagues have built an experimental platform called Gobo, which allows you to tinker with the algorithms on your Facebook and Twitter feeds as you see fit. ZUCKERMAN: So you can say things like, I'd like to hear from more women. Mute all the men. I'd like less rude content and more civil content. WAMSLEY: It's both fun and tricky to think about what a better social network looks like. Should it be bounded by geography so it's easier to meet your neighbors, or is it a place to meet far-flung kindred spirits? Is it a space to share news articles and funny videos, or is it politics- and meme-free? Do you use your real name, or do you keep personal data out of it? Maybe you pay to use it, or maybe it pays you to use it. Zuckerman says we might be better off if we didn't rely on one social network to do everything. ZUCKERMAN: We might choose to have a bunch of different networks and figure out how to link them together. It's a bit crazy that we have one social network that tries to do everything. WAMSLEY: It can be hard for a new social network to take off because of something called the network effect, which says we tend to go where our friends already are. And when innovative social networks have threatened Facebook's dominance, it's simply bought them, as it did with Instagram and WhatsApp. ZUCKERMAN: The trick is right now, Facebook has a quite effective monopoly, so one possibility on this could be to try to constrain Facebook from swallowing other competitors. WAMSLEY: And that network effect can cut both ways. So if my friends started leaving Facebook, it'd be easier for me to leave, too. I asked Cathy O'Neil if she could imagine a scenario in which a real alternative to Facebook emerges, and she said yes, including one where people simply lose interest. O'NEIL: It was kind of a temporary insanity that we all went through where we wanted to do this in the first place. WAMSLEY: And in that future, perhaps we'll look up from our phones, walk outside and hang out together in real life. DAVID GREENE, HOST:  Laurel Wamsley reporting here on MORNING EDITION from NPR News. STEVE INSKEEP, HOST:   Some technology experts are asking what it would mean to create an alternative to Facebook. The social network has had a rough couple of years. It faces criticism for the way it shared users' data and the way it was used during the presidential campaign. It faces a movement to quit Facebook. But, of course, the question is, quit Facebook in favor of what? If the people you know are all on Facebook, quitting it would be like quitting the power grid - possible, but not so easy - for now. NPR's Laurel Wamsley reports. LAUREL WAMSLEY, BYLINE: When Mark Zuckerberg was grilled last month on Capitol Hill, Senator Lindsey Graham asked him a question a lot of people are asking in one way or another. (SOUNDBITE OF ARCHIVED RECORDING) LINDSEY GRAHAM: If I buy a Ford, and it doesn't work well and I don't like it, I can buy a Chevy. If I'm upset with Facebook, what's the equivalent product that I can go sign up for? WAMSLEY: Zuckerberg said, well, there are a lot of companies that do some form of some part of what Facebook does. But the short answer to the question of - what's a Facebook that's not Facebook? - is, there isn't one. Two-thirds of American adults use Facebook, and three-quarters of those use it every day. I'm one of them, even though I'm uncomfortable with how much the company knows about me and how I'm targeted with advertising based on that data. But I use Facebook for my job, and I also don't want to miss out on party invites from my friends. So despite my misgivings, it's hard to think of quitting the platform for good. But Facebook is only 14 years old, and knowing what we know now about the flaws in its design, how might we go about creating the next social network, one that doesn't have those same problems? CATHY O'NEIL: I really think Facebook is destructive. WAMSLEY: That's Cathy O'Neil. She's a mathematician and the author of a book about the potentially dangerous consequences of algorithms. She says she imagines the next social network as having the best parts of Facebook without the worst parts of Facebook. O'NEIL: Basically, a town square where people can interact, they can keep up with each other but without the sort of commercial, predatory aspect. WAMSLEY: Her envisioned social network would have a moderator curating the conversation, and it would be a nonprofit. But she doesn't really want the government to run the system either because then they'd probably collect too much data about all of us. Another person thinking about building a better social network as Ethan Zuckerman. He's director of the Center for Civic Media at the MIT Media Lab. He thinks it's a problem that Facebook is so big. ETHAN ZUCKERMAN: Facebook has an awful lot of power by virtue of the fact that you have a single company making decisions for about 2 billion people. WAMSLEY: He says the next iteration of social networks could be decentralized instead of run by one company. A good example of this is a network that already exists, an open-source software called Mastodon. ZUCKERMAN: You can install it, put it up on a Web server. At that point, you're running a Mastodon node. WAMSLEY: It's a sort of replacement Twitter where anyone can create their own community with their own rules. Another problem that he'd fix. . . ZUCKERMAN: We don't really have control over the algorithms that sort our information and choose what we see or don't see. WAMSLEY: Zuckerman and his MIT colleagues have built an experimental platform called Gobo, which allows you to tinker with the algorithms on your Facebook and Twitter feeds as you see fit. ZUCKERMAN: So you can say things like, I'd like to hear from more women. Mute all the men. I'd like less rude content and more civil content. WAMSLEY: It's both fun and tricky to think about what a better social network looks like. Should it be bounded by geography so it's easier to meet your neighbors, or is it a place to meet far-flung kindred spirits? Is it a space to share news articles and funny videos, or is it politics- and meme-free? Do you use your real name, or do you keep personal data out of it? Maybe you pay to use it, or maybe it pays you to use it. Zuckerman says we might be better off if we didn't rely on one social network to do everything. ZUCKERMAN: We might choose to have a bunch of different networks and figure out how to link them together. It's a bit crazy that we have one social network that tries to do everything. WAMSLEY: It can be hard for a new social network to take off because of something called the network effect, which says we tend to go where our friends already are. And when innovative social networks have threatened Facebook's dominance, it's simply bought them, as it did with Instagram and WhatsApp. ZUCKERMAN: The trick is right now, Facebook has a quite effective monopoly, so one possibility on this could be to try to constrain Facebook from swallowing other competitors. WAMSLEY: And that network effect can cut both ways. So if my friends started leaving Facebook, it'd be easier for me to leave, too. I asked Cathy O'Neil if she could imagine a scenario in which a real alternative to Facebook emerges, and she said yes, including one where people simply lose interest. O'NEIL: It was kind of a temporary insanity that we all went through where we wanted to do this in the first place. WAMSLEY: And in that future, perhaps we'll look up from our phones, walk outside and hang out together in real life. DAVID GREENE, HOST:   Laurel Wamsley reporting here on MORNING EDITION from NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-04-604140045": {"title": "Amazon HQ2, Washington D.C. Battles Its Suburbs In Virginia And Maryland : NPR", "url": "https://www.npr.org/2018/05/04/604140045/d-c-un-united-amazons-second-hq-pits-city-vs-its-suburbs", "author": "No author found", "published_date": "2018-05-04", "content": "DAVID GREENE, HOST: OK. The suspense goes on. Amazon is soon going to announce a new shorter list of locations it is considering to host a second headquarters, and the bidding race is super competitive. At stake are some 50,000 jobs with an average salary of six figures. The bids from the Washington, D. C. , area really stand out for a particular lack of teamwork. NPR's Alina Selyukh met up with Martin Austermuhle from member station WAMU. He's been covering the politics of wooing Amazon. ALINA SELYUKH, BYLINE: Martin and I are on a bridge behind a landmark transit hub in Washington, D. C. It's called Union Station. This is a fast-growing neighborhood, one of several being pitched for Amazon's HQ2. MARTIN AUSTERMUHLE, BYLINE: Union Station is behind us. The train tracks kind of stretch in front of us heading north out of the city. SELYUKH: How far are we from Maryland and Virginia here? AUSTERMUHLE: I think five or six miles in any one direction. SELYUKH: In another universe, the District of Columbia and nearby parts of Maryland and Virginia could easily be one giant city. Everyone here rides the same Metro, comes out to the same baseball and football games, either works for the federal government or knows someone who does. But when it came to wooing Amazon, three parts of this one metro area - Washington, Northern Virginia and the Montgomery County in Maryland - decided to compete. AUSTERMUHLE: It would stand to reason that this is exactly the sort of place - these are elected officials that interact on a daily basis - they could've sat down and had a conversation and said, listen, we all want Amazon HQ2 to come to the region. Let's not make it more expensive for each of us individually by one-upping each other with incentives. SELYUKH: Did that happen? AUSTERMUHLE: No. SELYUKH: It did happen in other large metro areas. For example, places like Denver or Dallas drew the entire region into one pitch. In fact, Amazon encouraged local governments to join forces for a unified bid. But the nation's capital is special with so many suburbs belonging to two completely different states and so different taxpayers and different governors who all want to bring home their own bacon. VICTOR HOSKINS: You have to see that you can't do it alone. You know, you can't confront the challenge alone. You have to see you've got to confront it together. SELYUKH: Victor Hoskins is with Arlington County, Northern Va. He says it takes a major breakthrough, something like tackling the regional subway system to convince separate states to spend their taxpayer money on one joint mission. HOSKINS: That's the kind of conclusion that the jurisdictions have to come to, and it's not an easy one to come to. People sometimes can't agree on what movie to go to. SELYUKH: Hoskins has a word for the relationship in the region. HOSKINS: Co-opitition (ph). And I call us frenemies. SELYUKH: And not just any kind of frenemies but frenemies who generously borrow each other's backyards. All three finalist bids around here tout many of the same perks - airports in Virginia and Maryland, universities and museums in D. C. and some of the smartest workers in the country. And what I haven't mentioned yet is that there was a fourth bid from the D. C. metro region, one that didn't make the finalist list. It came from Maryland's Prince George's County. Economic Development chief David Iannucci says they crafted their own pitch because Maryland's governor basically told all counties to fend for themselves. DAVID IANNUCCI: So I think there's a very strong chance we didn't make the cut because of the way that happened. SELYUKH: When you met with Amazon, did they. . . IANNUCCI: They cited the No. 1 reason that we didn't make the cut was an absence of senior software development engineers. SELYUKH: This is a bit ironic because we're talking about a county that's home to the University of Maryland, College Park, which has a top-rated computer science program and definitely features in the bids that did make the cut. Iannucci says he tried to explain this to Amazon, but. . . IANNUCCI: It's like saying your teenage date, take me back. SELYUKH: Lately, the three D. C. -area finalists have been taking great pains to say that they aren't actually rivals. The governors of Maryland and Virginia, along with the mayor of Washington, wrote a letter to Amazon CEO Jeff Bezos reassuring him they are, quote, \"much more than just neighbors. \" They say they are partners with a shared vision for the future. Meanwhile, Martin has been digging into how the three are competing financially. AUSTERMUHLE: Maryland is offering one of the biggest buckets of money that anyone knows about - some $5 billion worth of tax breaks and other perks. We don't really know what Virginia is offering. And D. C. shared a document about its financial incentives but redacted some of the juiciest details. SELYUKH: The bottom line is this for local officials - even if Amazon's workers come from the whole region, and even if the whole region benefits from the prestige, it's the winning location that gets most of those sweet tax dollars. And that's why Amazon now has to choose not just among 20 metro areas but three neighbors in one metro area, though some folks around Washington would say that's three times the chances of getting picked. Alina Selyukh, NPR News. DAVID GREENE, HOST:  OK. The suspense goes on. Amazon is soon going to announce a new shorter list of locations it is considering to host a second headquarters, and the bidding race is super competitive. At stake are some 50,000 jobs with an average salary of six figures. The bids from the Washington, D. C. , area really stand out for a particular lack of teamwork. NPR's Alina Selyukh met up with Martin Austermuhle from member station WAMU. He's been covering the politics of wooing Amazon. ALINA SELYUKH, BYLINE: Martin and I are on a bridge behind a landmark transit hub in Washington, D. C. It's called Union Station. This is a fast-growing neighborhood, one of several being pitched for Amazon's HQ2. MARTIN AUSTERMUHLE, BYLINE: Union Station is behind us. The train tracks kind of stretch in front of us heading north out of the city. SELYUKH: How far are we from Maryland and Virginia here? AUSTERMUHLE: I think five or six miles in any one direction. SELYUKH: In another universe, the District of Columbia and nearby parts of Maryland and Virginia could easily be one giant city. Everyone here rides the same Metro, comes out to the same baseball and football games, either works for the federal government or knows someone who does. But when it came to wooing Amazon, three parts of this one metro area - Washington, Northern Virginia and the Montgomery County in Maryland - decided to compete. AUSTERMUHLE: It would stand to reason that this is exactly the sort of place - these are elected officials that interact on a daily basis - they could've sat down and had a conversation and said, listen, we all want Amazon HQ2 to come to the region. Let's not make it more expensive for each of us individually by one-upping each other with incentives. SELYUKH: Did that happen? AUSTERMUHLE: No. SELYUKH: It did happen in other large metro areas. For example, places like Denver or Dallas drew the entire region into one pitch. In fact, Amazon encouraged local governments to join forces for a unified bid. But the nation's capital is special with so many suburbs belonging to two completely different states and so different taxpayers and different governors who all want to bring home their own bacon. VICTOR HOSKINS: You have to see that you can't do it alone. You know, you can't confront the challenge alone. You have to see you've got to confront it together. SELYUKH: Victor Hoskins is with Arlington County, Northern Va. He says it takes a major breakthrough, something like tackling the regional subway system to convince separate states to spend their taxpayer money on one joint mission. HOSKINS: That's the kind of conclusion that the jurisdictions have to come to, and it's not an easy one to come to. People sometimes can't agree on what movie to go to. SELYUKH: Hoskins has a word for the relationship in the region. HOSKINS: Co-opitition (ph). And I call us frenemies. SELYUKH: And not just any kind of frenemies but frenemies who generously borrow each other's backyards. All three finalist bids around here tout many of the same perks - airports in Virginia and Maryland, universities and museums in D. C. and some of the smartest workers in the country. And what I haven't mentioned yet is that there was a fourth bid from the D. C. metro region, one that didn't make the finalist list. It came from Maryland's Prince George's County. Economic Development chief David Iannucci says they crafted their own pitch because Maryland's governor basically told all counties to fend for themselves. DAVID IANNUCCI: So I think there's a very strong chance we didn't make the cut because of the way that happened. SELYUKH: When you met with Amazon, did they. . . IANNUCCI: They cited the No. 1 reason that we didn't make the cut was an absence of senior software development engineers. SELYUKH: This is a bit ironic because we're talking about a county that's home to the University of Maryland, College Park, which has a top-rated computer science program and definitely features in the bids that did make the cut. Iannucci says he tried to explain this to Amazon, but. . . IANNUCCI: It's like saying your teenage date, take me back. SELYUKH: Lately, the three D. C. -area finalists have been taking great pains to say that they aren't actually rivals. The governors of Maryland and Virginia, along with the mayor of Washington, wrote a letter to Amazon CEO Jeff Bezos reassuring him they are, quote, \"much more than just neighbors. \" They say they are partners with a shared vision for the future. Meanwhile, Martin has been digging into how the three are competing financially. AUSTERMUHLE: Maryland is offering one of the biggest buckets of money that anyone knows about - some $5 billion worth of tax breaks and other perks. We don't really know what Virginia is offering. And D. C. shared a document about its financial incentives but redacted some of the juiciest details. SELYUKH: The bottom line is this for local officials - even if Amazon's workers come from the whole region, and even if the whole region benefits from the prestige, it's the winning location that gets most of those sweet tax dollars. And that's why Amazon now has to choose not just among 20 metro areas but three neighbors in one metro area, though some folks around Washington would say that's three times the chances of getting picked. Alina Selyukh, NPR News.", "section": "Business", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-05-608802917": {"title": "Some Users Wary Of Facebook's Newest Venture In Online Dating : NPR", "url": "https://www.npr.org/2018/05/05/608802917/some-users-weary-of-facebook-s-newest-venture-in-online-dating", "author": "No author found", "published_date": "2018-05-05", "content": "MICHEL MARTIN, HOST:  Mark Zuckerberg made a big announcement this week. Facebook is soon going to launch a new dating service. And he emphasized, it is for meaningful relationships but that is a hard standard to meet on a site that's full of people with very different intentions from love, to adultery, to annoying come-ons. NPR's Aarti Shahani, reports. AARTI SHAHANI, BYLINE: Fifteen years ago, Joe Miller of Oak Ridge, Tenn. , met a nice lady at his book club. JOE MILLER: She was there with her husband. SHAHANI: Then, years passed. Facebook came along, and Miller spotted her name in a comment. Turns out, she was divorced. He reached out, they started chatting, eventually made dinner plans. MILLER: It was unclear if we old friends getting together for dinner or if this was a date. SHAHANI: He wanted it to be a date so he said so. And today, a year and a half later, they're a couple. MILLER: I've never got a real interest in anybody else. She's intelligent, very kind, and sweet person. She's pretty much I think anything a person could want to run into. SHAHANI: That is a Facebook love story. Then there's the experience of Tiffany Keith in Houston, Texas. She says she gets friend requests from men who claim to be in the military. This one guy said he was a four-star general and a medical doctor, but his pictures were odd. TIFFANY KEITH: One particular one he's wearing scrubs and he's standing in front of a door and the door has lettering, which is a dentist's office. SHAHANI: A dental office with a name that was not his. KEITH: I'm like, my friend, you are a dental hygienist. You are not a four-star general. SHAHANI: Keith is not a fan of Facebook's latest foray. She doesn't want her dating profile to be linked to her Facebook identity. She worries that'll make it too easy for potential matches to peer into the intimate details of her life. KEITH: I don't want someone that I might want to date seeing pictures of my kid and my family. SHAHANI: In recent weeks, Mark Zuckerberg has admitted he can be too optimistic. That's in part how 87 million Facebook user profiles fell into the hands of a political operative. This week, the 33-year-old CEO's optimism was on display once more. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: So today, we are announcing a new set of features, coming soon, around dating. SHAHANI: He was on stage at a Facebook conference. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: Now this is going to be - going to be for building real long-term relationships, not just hookups. SHAHANI: Not just hookups. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: We want Facebook to be somewhere where you can start meaningful relationships. SHAHANI: There are two issues with this. First, casual dating is now a big part of many people's lives. And second. . . MISIEK PISKORSKI: Unfortunately, there is no really a way to prescribe this. People will really appropriate this service in any way they like. SHAHANI: Misiek Piskorski, a professor at IMD Business School, has studied social networks for 15 years. And with past platforms, he observed the practice of married men pretending to be single. PISKORSKI: This is not something that I sort of dreamed up or I said, oh, this is sort of like possibilities. You actually see this in the data. SHAHANI: Arum Kang is co-CEO of a popular dating app on Facebook. It's called Coffee Meets Bagel. Her app matches users with friends of friends. Now Facebook, a longtime partner, will become a competitor. But Kang has a prediction that Facebook's dating app will suffer from being too easy to use. ARUM KANG: A product that's probably more of a low commitment, easy for the, you know, the majority of the users to access. I think naturally it will be more of a casual interaction than anything else. SHAHANI: That said, Kang expects Facebook to take advantage of the depth of its user data to give its own dating product a leg up against others. Aarti Shahani, NPR News, San Francisco. MICHEL MARTIN, HOST:   Mark Zuckerberg made a big announcement this week. Facebook is soon going to launch a new dating service. And he emphasized, it is for meaningful relationships but that is a hard standard to meet on a site that's full of people with very different intentions from love, to adultery, to annoying come-ons. NPR's Aarti Shahani, reports. AARTI SHAHANI, BYLINE: Fifteen years ago, Joe Miller of Oak Ridge, Tenn. , met a nice lady at his book club. JOE MILLER: She was there with her husband. SHAHANI: Then, years passed. Facebook came along, and Miller spotted her name in a comment. Turns out, she was divorced. He reached out, they started chatting, eventually made dinner plans. MILLER: It was unclear if we old friends getting together for dinner or if this was a date. SHAHANI: He wanted it to be a date so he said so. And today, a year and a half later, they're a couple. MILLER: I've never got a real interest in anybody else. She's intelligent, very kind, and sweet person. She's pretty much I think anything a person could want to run into. SHAHANI: That is a Facebook love story. Then there's the experience of Tiffany Keith in Houston, Texas. She says she gets friend requests from men who claim to be in the military. This one guy said he was a four-star general and a medical doctor, but his pictures were odd. TIFFANY KEITH: One particular one he's wearing scrubs and he's standing in front of a door and the door has lettering, which is a dentist's office. SHAHANI: A dental office with a name that was not his. KEITH: I'm like, my friend, you are a dental hygienist. You are not a four-star general. SHAHANI: Keith is not a fan of Facebook's latest foray. She doesn't want her dating profile to be linked to her Facebook identity. She worries that'll make it too easy for potential matches to peer into the intimate details of her life. KEITH: I don't want someone that I might want to date seeing pictures of my kid and my family. SHAHANI: In recent weeks, Mark Zuckerberg has admitted he can be too optimistic. That's in part how 87 million Facebook user profiles fell into the hands of a political operative. This week, the 33-year-old CEO's optimism was on display once more. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: So today, we are announcing a new set of features, coming soon, around dating. SHAHANI: He was on stage at a Facebook conference. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: Now this is going to be - going to be for building real long-term relationships, not just hookups. SHAHANI: Not just hookups. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: We want Facebook to be somewhere where you can start meaningful relationships. SHAHANI: There are two issues with this. First, casual dating is now a big part of many people's lives. And second. . . MISIEK PISKORSKI: Unfortunately, there is no really a way to prescribe this. People will really appropriate this service in any way they like. SHAHANI: Misiek Piskorski, a professor at IMD Business School, has studied social networks for 15 years. And with past platforms, he observed the practice of married men pretending to be single. PISKORSKI: This is not something that I sort of dreamed up or I said, oh, this is sort of like possibilities. You actually see this in the data. SHAHANI: Arum Kang is co-CEO of a popular dating app on Facebook. It's called Coffee Meets Bagel. Her app matches users with friends of friends. Now Facebook, a longtime partner, will become a competitor. But Kang has a prediction that Facebook's dating app will suffer from being too easy to use. ARUM KANG: A product that's probably more of a low commitment, easy for the, you know, the majority of the users to access. I think naturally it will be more of a casual interaction than anything else. SHAHANI: That said, Kang expects Facebook to take advantage of the depth of its user data to give its own dating product a leg up against others. Aarti Shahani, NPR News, San Francisco.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-06-608868142": {"title": "European Data Privacy Rules To Go Into Effect : NPR", "url": "https://www.npr.org/2018/05/06/608868142/european-data-privacy-rules-to-go-into-effect", "author": "No author found", "published_date": "2018-05-06", "content": "LULU GARCIA-NAVARRO, HOST: The political consulting firm Cambridge Analytica announced this past week it's closing up shop, citing a loss of business after its alleged role in improperly harvesting the Facebook data of up to 87 million users. That closure is a victory of sorts for privacy advocates, but what's to stop the next company from doing something similar? Some point to new European data protection rules that go into effect May 25. The GDPR, General Data Protection Regulation, was designed to protect EU citizens. But because it applies to any business that has their data, it's having a worldwide impact. Joining us to talk about the new rules and more is Matt Hancock, secretary of state for digital, culture, media and sport in the U. K. That's a lot. Secretary Hancock, welcome. MATT HANCOCK: It's great to be here. GARCIA-NAVARRO: First, I'd like your reaction to the closure of Cambridge Analytica and its British affiliate, SCL Group. HANCOCK: I don't mourn the loss of Cambridge Analytica. The allegations against it were very serious. And what's important is that we can continue to get to the bottom of what happened. And it is vital that its closure does not impede the investigation into the use and abuse of data by Cambridge Analytica. And we'll make sure that doesn't happen. GARCIA-NAVARRO: All right. Enter the GDPR, General Data Protection Regulation, which is - could prevent something like this from happening again. I'd like to start with the basics, for our American audience. HANCOCK: Well, the idea behind these new laws is that the data that you put into the Internet belongs to you as a citizen, and you should therefore have consent over how it's used. And that consent should be in plain, straightforward language. And the aim is to make sure that when companies use your data, you know what they're doing with it. And, crucially, that - unless they've got very good reason - if you want them to delete it, then they do. GARCIA-NAVARRO: Of course, Facebook is one of the companies at the heart of all this. And CEO Mark Zuckerberg said that the GDPR will be applied \"in spirit\" - we're using quotes here - to all users, not just those in the EU. But is Facebook, frankly, too big to regulate effectively? HANCOCK: Oh, no. I don't think any company is. There's been this debate saying that, you know, we're a medium-sized country. How can we possibly have an impact on the Web as a whole because the Web is global? And I really reject that argument. I want the Internet and this new technology to work as a force for good in the world. GARCIA-NAVARRO: Great. But isn't Facebook a monopoly? I mean, shouldn't they possibly, as some people have suggested, be forced to split off Instagram and WhatsApp and all these other companies that make this digital behemoth? I mean, shouldn't it be regulated, in fact, more forcefully than it has been? HANCOCK: It's an interesting question because one of the features of monopoly is normally they put prices up for their customers 'cause they're the only people who can supply the product. In this case, they're very large, but they also provide things for free. So, how you think about that is actually - is quite a difficult intellectual question. GARCIA-NAVARRO: But do they? I mean, isn't that. . . HANCOCK: Ahh. GARCIA-NAVARRO: But isn't that the heart of this? I mean. . . HANCOCK: Well. . . GARCIA-NAVARRO: . . . Isn't it at the heart of this? I mean. . . HANCOCK: No, it's a barter - right? Instead of - it's. . . GARCIA-NAVARRO: We're giving them our data. HANCOCK: Yeah. Yeah. GARCIA-NAVARRO: And in return, we get the things that Facebook provides, but it's not exactly free. HANCOCK: Well, hence the principle in the new data rules, the GDPR, saying that you have to give consent over your use of data because then they have to ask you permission for how you use the data. And there's another provision that we're bringing in, which is that under the new rules, if you want to take all your data off one platform and give it to another, that has to be very simple. So Facebook are bringing in a simple button to be able to download all of your data off Facebook. And if you want, either delete it or move it to another platform. And that will bring in a bit of competitive tension that doesn't exist at the moment. GARCIA-NAVARRO: You are secretary of state for digital, culture, media and sport. Media is in there, too. And let's talk briefly about Facebook and the media. HANCOCK: Yeah. GARCIA-NAVARRO: They're certainly, some would say, a media company. Should they not be bound to the same ethics and rules? HANCOCK: Yeah. Well, it's a - this is a - I think this is a really important question because they're not the same as a publisher. When you post on a social media platform, you immediately express your view to the world. And anybody can go on and do that. And if you therefore made them have the same rules and responsibilities and liabilities as a media company, then they wouldn't be able to exist. You know, even I. . . GARCIA-NAVARRO: That's what they like to say. HANCOCK: Well. . . GARCIA-NAVARRO: They're a platform. They're not a media company. But Facebook made $40 billion in advertising revenue last year. HANCOCK: So. . . GARCIA-NAVARRO: Is that healthy for democracy? HANCOCK: So - exactly. So I was going to come on to that. One half of my answer is that they're not the same because, you know, even I, as a very responsible person, they would - they couldn't allow me to post or you, and I'm sure you're very responsible, too. GARCIA-NAVARRO: Very. HANCOCK: If they were fully liable for everything you said - because if you put up something that was illegal, then they'd be liable. And therefore, they wouldn't let anybody post, and the whole social media concept would collapse. On the other hand, did they have no responsibility at all for what people post or for taking down the most egregious content? Well, they do take those responsibilities seriously because they take down, for instance, child abuse images. And so they're not full-blown publishers. But when you come to the impact they've had on the whole of the rest of the media, I think there's something in there. The old balance between having broadcasters and newspapers, there is now a third category, which is social media. And making sure that we can have a healthy political debate is one of the big challenges of our age. GARCIA-NAVARRO: One of the big challenges. And you, sir, are indeed the secretary in charge of this, so what is your. . . HANCOCK: Yeah. Yeah. GARCIA-NAVARRO: What is your solution for that? HANCOCK: So we currently have a review out into how we pay for high-quality journalism. With newspapers, you know, in the past, the advertising on one side of the page paid for the writing and the journalism on the other. Now the advertising revenue from the adverts goes to the platform, largely, not entirely. GARCIA-NAVARRO: Right. I mean, they basically eviscerated the business model for journalism. HANCOCK: Yeah. Yeah. And journalism has to find a new business model. And we want to help them to do that. And we don't rule out changing the law. GARCIA-NAVARRO: Do you think Facebook needs to pay? HANCOCK: Well, that is a very interesting question. GARCIA-NAVARRO: Hmm, and you're not answering it. (LAUGHTER)HANCOCK: Well, we're looking into these things. GARCIA-NAVARRO: Secretary Matt Hancock, thank you so much. HANCOCK: Thank you. It's been a great pleasure. LULU GARCIA-NAVARRO, HOST:  The political consulting firm Cambridge Analytica announced this past week it's closing up shop, citing a loss of business after its alleged role in improperly harvesting the Facebook data of up to 87 million users. That closure is a victory of sorts for privacy advocates, but what's to stop the next company from doing something similar? Some point to new European data protection rules that go into effect May 25. The GDPR, General Data Protection Regulation, was designed to protect EU citizens. But because it applies to any business that has their data, it's having a worldwide impact. Joining us to talk about the new rules and more is Matt Hancock, secretary of state for digital, culture, media and sport in the U. K. That's a lot. Secretary Hancock, welcome. MATT HANCOCK: It's great to be here. GARCIA-NAVARRO: First, I'd like your reaction to the closure of Cambridge Analytica and its British affiliate, SCL Group. HANCOCK: I don't mourn the loss of Cambridge Analytica. The allegations against it were very serious. And what's important is that we can continue to get to the bottom of what happened. And it is vital that its closure does not impede the investigation into the use and abuse of data by Cambridge Analytica. And we'll make sure that doesn't happen. GARCIA-NAVARRO: All right. Enter the GDPR, General Data Protection Regulation, which is - could prevent something like this from happening again. I'd like to start with the basics, for our American audience. HANCOCK: Well, the idea behind these new laws is that the data that you put into the Internet belongs to you as a citizen, and you should therefore have consent over how it's used. And that consent should be in plain, straightforward language. And the aim is to make sure that when companies use your data, you know what they're doing with it. And, crucially, that - unless they've got very good reason - if you want them to delete it, then they do. GARCIA-NAVARRO: Of course, Facebook is one of the companies at the heart of all this. And CEO Mark Zuckerberg said that the GDPR will be applied \"in spirit\" - we're using quotes here - to all users, not just those in the EU. But is Facebook, frankly, too big to regulate effectively? HANCOCK: Oh, no. I don't think any company is. There's been this debate saying that, you know, we're a medium-sized country. How can we possibly have an impact on the Web as a whole because the Web is global? And I really reject that argument. I want the Internet and this new technology to work as a force for good in the world. GARCIA-NAVARRO: Great. But isn't Facebook a monopoly? I mean, shouldn't they possibly, as some people have suggested, be forced to split off Instagram and WhatsApp and all these other companies that make this digital behemoth? I mean, shouldn't it be regulated, in fact, more forcefully than it has been? HANCOCK: It's an interesting question because one of the features of monopoly is normally they put prices up for their customers 'cause they're the only people who can supply the product. In this case, they're very large, but they also provide things for free. So, how you think about that is actually - is quite a difficult intellectual question. GARCIA-NAVARRO: But do they? I mean, isn't that. . . HANCOCK: Ahh. GARCIA-NAVARRO: But isn't that the heart of this? I mean. . . HANCOCK: Well. . . GARCIA-NAVARRO: . . . Isn't it at the heart of this? I mean. . . HANCOCK: No, it's a barter - right? Instead of - it's. . . GARCIA-NAVARRO: We're giving them our data. HANCOCK: Yeah. Yeah. GARCIA-NAVARRO: And in return, we get the things that Facebook provides, but it's not exactly free. HANCOCK: Well, hence the principle in the new data rules, the GDPR, saying that you have to give consent over your use of data because then they have to ask you permission for how you use the data. And there's another provision that we're bringing in, which is that under the new rules, if you want to take all your data off one platform and give it to another, that has to be very simple. So Facebook are bringing in a simple button to be able to download all of your data off Facebook. And if you want, either delete it or move it to another platform. And that will bring in a bit of competitive tension that doesn't exist at the moment. GARCIA-NAVARRO: You are secretary of state for digital, culture, media and sport. Media is in there, too. And let's talk briefly about Facebook and the media. HANCOCK: Yeah. GARCIA-NAVARRO: They're certainly, some would say, a media company. Should they not be bound to the same ethics and rules? HANCOCK: Yeah. Well, it's a - this is a - I think this is a really important question because they're not the same as a publisher. When you post on a social media platform, you immediately express your view to the world. And anybody can go on and do that. And if you therefore made them have the same rules and responsibilities and liabilities as a media company, then they wouldn't be able to exist. You know, even I. . . GARCIA-NAVARRO: That's what they like to say. HANCOCK: Well. . . GARCIA-NAVARRO: They're a platform. They're not a media company. But Facebook made $40 billion in advertising revenue last year. HANCOCK: So. . . GARCIA-NAVARRO: Is that healthy for democracy? HANCOCK: So - exactly. So I was going to come on to that. One half of my answer is that they're not the same because, you know, even I, as a very responsible person, they would - they couldn't allow me to post or you, and I'm sure you're very responsible, too. GARCIA-NAVARRO: Very. HANCOCK: If they were fully liable for everything you said - because if you put up something that was illegal, then they'd be liable. And therefore, they wouldn't let anybody post, and the whole social media concept would collapse. On the other hand, did they have no responsibility at all for what people post or for taking down the most egregious content? Well, they do take those responsibilities seriously because they take down, for instance, child abuse images. And so they're not full-blown publishers. But when you come to the impact they've had on the whole of the rest of the media, I think there's something in there. The old balance between having broadcasters and newspapers, there is now a third category, which is social media. And making sure that we can have a healthy political debate is one of the big challenges of our age. GARCIA-NAVARRO: One of the big challenges. And you, sir, are indeed the secretary in charge of this, so what is your. . . HANCOCK: Yeah. Yeah. GARCIA-NAVARRO: What is your solution for that? HANCOCK: So we currently have a review out into how we pay for high-quality journalism. With newspapers, you know, in the past, the advertising on one side of the page paid for the writing and the journalism on the other. Now the advertising revenue from the adverts goes to the platform, largely, not entirely. GARCIA-NAVARRO: Right. I mean, they basically eviscerated the business model for journalism. HANCOCK: Yeah. Yeah. And journalism has to find a new business model. And we want to help them to do that. And we don't rule out changing the law. GARCIA-NAVARRO: Do you think Facebook needs to pay? HANCOCK: Well, that is a very interesting question. GARCIA-NAVARRO: Hmm, and you're not answering it. (LAUGHTER) HANCOCK: Well, we're looking into these things. GARCIA-NAVARRO: Secretary Matt Hancock, thank you so much. HANCOCK: Thank you. It's been a great pleasure.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-07-608590826": {"title": "In 'The Perfectionists' Simon Winchester Looks At History Of Precision Engineering : NPR", "url": "https://www.npr.org/2018/05/07/608590826/in-the-perfectionists-simon-winchester-looks-at-history-of-precision-engineering", "author": "No author found", "published_date": "2018-05-07", "content": "MARY LOUISE KELLY, HOST: Today's technology demands perfection, precise and accurate measurements, perfect timing. On this week's All Tech Considered, we explore the history of perfection from 18th century England to today's Silicon Valley. (SOUNDBITE OF MUSIC)KELLY: The author Simon Winchester has taken on subjects as diverse as the volcanic explosion of Krakatoa to the creation of the Oxford English Dictionary. You may remember his book \"The Professor And The Madman. \" Well, Winchester's latest is titled \"The Perfectionists: How Precision Engineers Created The Modern World. \" And Simon Winchester joins us now from our New York studios. Welcome. SIMON WINCHESTER: Thank you. KELLY: I love the idea for where this book came from. You write that you got an email one day from a complete stranger who said, you ought to write a book about the history of precision. You obviously liked the idea. WINCHESTER: I did. And he was an extraordinary man. He's a scientific glassblower and has this passion for numbers and things that are perfect and precise and exact and accurate. And he thinks this is invisible. It's sort of hidden in plain sight, but it's essential to our world. So why not write a book about it? And I thought, my gosh, that's not a bad idea. KELLY: So you actually pick one figure from history who you say is widely regarded as the father of precision. This is an 18th century Englishman John Wilkinson. Who was he? WINCHESTER: Yes, John Iron-Mad Wilkinson, as he was called, was an ironmaster, which is not a profession you hear often today. But he was obsessed with the metal. And he had an iron coffin, which he kept in his workshop, and would lie in and jump out of occasionally to startle people. He had an iron boat. He had iron everything. Anyway, in order to make one particular thing ordered from James Watt, whose name you'll know as a creator of the steam engine. KELLY: Sure, the Scottish engineer. WINCHESTER: Indeed. He ordered an engine and he said, the problem with that is the cylinder. There's a piston which goes up and down, but it's so sloppily inside the cylinder that it really doesn't work at all. He said, I can make out of a solid piece of iron and using a drill mounted on wheels, which are moved into the face of the piece of iron, a cylinder which is precise. And he drilled a hole, which was perfectly straight and true. The piston fitted into it. And lo and behold, the steam engine worked, and we're off to the races. The industrial revolution begins. KELLY: Now, you track when precision and the pursuit of perfection started to make its way across the Atlantic. What happened in the history of gun making that was so revolutionary and brought precision engineering here to the U. S. ? WINCHESTER: Well, the first important thing is that it was all down to Thomas Jefferson. He was in Paris. And a man called Onnoy Blanc (ph), a Frenchman who had taken the ideas of John Wilkinson and started to run with them, realized that there was a good way, an efficient way of making the parts of a flintlock gun. A flintlock, the thing that produces the spark, has got about 10 pieces in it. What previously happened is that if a gun broke, you'd have to go to the gunmaker and make a completely new one from scratch. But he said, what if all the parts were exactly the same, if they were interchangeable? And that's the key phrase. KELLY: And, Simon Winchester, no one had thought of that before? WINCHESTER: This sounds simple to us today but nobody in the 18th century had thought of it. Thomas Jefferson saw the process, he was invited to a demonstration in Paris, wrote a letter, which was hand carried in a packet across the Atlantic to the then secretary of war in Washington. Eventually, a demonstration was arranged in Harper's Ferry, Va. , and then the American gun industry started with a vengeance. KELLY: Let me zip you all the way up to present day and the matter of jet engines and specifically, to the story of a Qantas jet, which had a horrific malfunction in the air. Nobody was hurt, but it's quite a tale. Tell us briefly the story and how that fits into this theme. WINCHESTER: Well, this was a Qantas Airbus A380, one of these gigantic double-decker aircraft. It was going from Singapore to Sydney. KELLY: Give us the year. WINCHESTER: Twenty-ten. KELLY: OK. WINCHESTER: It was early in the morning. It took off, went up to 7,000 feet, massive explosion in the inboard port side, left side engine - shrapnel all over the place, much like what occurred in the Southwest Airlines flight the other day. KELLY: And what did they figure out had gone wrong? WINCHESTER: What had happened is that an oil feed pipe had been incorrectly machined in a factory in England. And it broke and sprayed hot oil onto the rotor blades. This caused the rotor itself to wobble and then to break and then everything in the middle of the engine broke, spun right out of the casing of the engine, mercifully didn't hit the fuselage, but destroyed much of the wing, most of the hydraulics and made the plane almost unflyable. KELLY: So what is the lesson here? WINCHESTER: The lesson here is that if you're making things to such incredible tolerances, A, you've got to be really careful, which the engineer in Hucknall in Northamptonshire was not. He only made a fraction of a millimeter of an error, but it was enough to bring that plane nearly down and to kill 500 people nearly. But also, to question are we pushing precision, the limits, too far? Because we're reaching limits now. I mean, in my and your iPhone, there are 4 billion, with a B, transistors. A transistor when it was invented in 1948 was about as big as your hand. There are more transistors in the world today than there are leaves on all the trees in all the world. That's an incredible fact. KELLY: Really? WINCHESTER: But it means we're operating at levels of precision which are right down at atomic levels where if you know your Heisenberg, it all gets really weird. Is it waves? Is it matter? Is it photons? Is it electrons? KELLY: You're talking dimensions that even a machine would struggle to measure. WINCHESTER: Yes. I mean, there's a classic example. A modern transistor is much smaller than the wavelength of light, which means you can't possibly even see it. And that makes one wonder, are we going too far? Are there limits? And maybe with robotics and artificial intelligence and so forth coming down the pike, are we in danger of fetishizing precision, making it too important a part of our life and losing our respect and admiration for craftsmanship, for working with wood and all those sorts of things? KELLY: You write about that in such a personal way in the book. You include the detail of your Sunday morning ritual, of walking room to room in this old farmhouse you live in and correcting all the clocks, you know, pushing one hand forward a little bit and another hand back, and that you love the inaccuracy of that, that they're all chiming totally out of sync by midway through the next week. WINCHESTER: Yes, a lovely description in Dorothy L. Sayers' book \"Gaudy Night\" about the clocks in Oxford chiming midnight in friendly disagreement. Well, I like clocks having friendly disagreement. I loathe digital watches with, I mean, the one I dare say you're looking at in the studio now, with microsecond countdowns. Let's take it a bit easier. Let's be a bit fuzzy in our needs and desires and wishes. KELLY: Well, it has been a pleasure speaking with you about precision and maybe the limits of precision. Simon Winchester, thank you. WINCHESTER: Thank you very much. KELLY: His new book is \"The Perfectionist: How Precision Engineers Created The Modern World. \"[POST-BROADCAST CORRECTION: Our guest incorrectly says the transistor was invented in 1948. It was actually invented in 1947 and then announced the following year. ] MARY LOUISE KELLY, HOST:  Today's technology demands perfection, precise and accurate measurements, perfect timing. On this week's All Tech Considered, we explore the history of perfection from 18th century England to today's Silicon Valley. (SOUNDBITE OF MUSIC) KELLY: The author Simon Winchester has taken on subjects as diverse as the volcanic explosion of Krakatoa to the creation of the Oxford English Dictionary. You may remember his book \"The Professor And The Madman. \" Well, Winchester's latest is titled \"The Perfectionists: How Precision Engineers Created The Modern World. \" And Simon Winchester joins us now from our New York studios. Welcome. SIMON WINCHESTER: Thank you. KELLY: I love the idea for where this book came from. You write that you got an email one day from a complete stranger who said, you ought to write a book about the history of precision. You obviously liked the idea. WINCHESTER: I did. And he was an extraordinary man. He's a scientific glassblower and has this passion for numbers and things that are perfect and precise and exact and accurate. And he thinks this is invisible. It's sort of hidden in plain sight, but it's essential to our world. So why not write a book about it? And I thought, my gosh, that's not a bad idea. KELLY: So you actually pick one figure from history who you say is widely regarded as the father of precision. This is an 18th century Englishman John Wilkinson. Who was he? WINCHESTER: Yes, John Iron-Mad Wilkinson, as he was called, was an ironmaster, which is not a profession you hear often today. But he was obsessed with the metal. And he had an iron coffin, which he kept in his workshop, and would lie in and jump out of occasionally to startle people. He had an iron boat. He had iron everything. Anyway, in order to make one particular thing ordered from James Watt, whose name you'll know as a creator of the steam engine. KELLY: Sure, the Scottish engineer. WINCHESTER: Indeed. He ordered an engine and he said, the problem with that is the cylinder. There's a piston which goes up and down, but it's so sloppily inside the cylinder that it really doesn't work at all. He said, I can make out of a solid piece of iron and using a drill mounted on wheels, which are moved into the face of the piece of iron, a cylinder which is precise. And he drilled a hole, which was perfectly straight and true. The piston fitted into it. And lo and behold, the steam engine worked, and we're off to the races. The industrial revolution begins. KELLY: Now, you track when precision and the pursuit of perfection started to make its way across the Atlantic. What happened in the history of gun making that was so revolutionary and brought precision engineering here to the U. S. ? WINCHESTER: Well, the first important thing is that it was all down to Thomas Jefferson. He was in Paris. And a man called Onnoy Blanc (ph), a Frenchman who had taken the ideas of John Wilkinson and started to run with them, realized that there was a good way, an efficient way of making the parts of a flintlock gun. A flintlock, the thing that produces the spark, has got about 10 pieces in it. What previously happened is that if a gun broke, you'd have to go to the gunmaker and make a completely new one from scratch. But he said, what if all the parts were exactly the same, if they were interchangeable? And that's the key phrase. KELLY: And, Simon Winchester, no one had thought of that before? WINCHESTER: This sounds simple to us today but nobody in the 18th century had thought of it. Thomas Jefferson saw the process, he was invited to a demonstration in Paris, wrote a letter, which was hand carried in a packet across the Atlantic to the then secretary of war in Washington. Eventually, a demonstration was arranged in Harper's Ferry, Va. , and then the American gun industry started with a vengeance. KELLY: Let me zip you all the way up to present day and the matter of jet engines and specifically, to the story of a Qantas jet, which had a horrific malfunction in the air. Nobody was hurt, but it's quite a tale. Tell us briefly the story and how that fits into this theme. WINCHESTER: Well, this was a Qantas Airbus A380, one of these gigantic double-decker aircraft. It was going from Singapore to Sydney. KELLY: Give us the year. WINCHESTER: Twenty-ten. KELLY: OK. WINCHESTER: It was early in the morning. It took off, went up to 7,000 feet, massive explosion in the inboard port side, left side engine - shrapnel all over the place, much like what occurred in the Southwest Airlines flight the other day. KELLY: And what did they figure out had gone wrong? WINCHESTER: What had happened is that an oil feed pipe had been incorrectly machined in a factory in England. And it broke and sprayed hot oil onto the rotor blades. This caused the rotor itself to wobble and then to break and then everything in the middle of the engine broke, spun right out of the casing of the engine, mercifully didn't hit the fuselage, but destroyed much of the wing, most of the hydraulics and made the plane almost unflyable. KELLY: So what is the lesson here? WINCHESTER: The lesson here is that if you're making things to such incredible tolerances, A, you've got to be really careful, which the engineer in Hucknall in Northamptonshire was not. He only made a fraction of a millimeter of an error, but it was enough to bring that plane nearly down and to kill 500 people nearly. But also, to question are we pushing precision, the limits, too far? Because we're reaching limits now. I mean, in my and your iPhone, there are 4 billion, with a B, transistors. A transistor when it was invented in 1948 was about as big as your hand. There are more transistors in the world today than there are leaves on all the trees in all the world. That's an incredible fact. KELLY: Really? WINCHESTER: But it means we're operating at levels of precision which are right down at atomic levels where if you know your Heisenberg, it all gets really weird. Is it waves? Is it matter? Is it photons? Is it electrons? KELLY: You're talking dimensions that even a machine would struggle to measure. WINCHESTER: Yes. I mean, there's a classic example. A modern transistor is much smaller than the wavelength of light, which means you can't possibly even see it. And that makes one wonder, are we going too far? Are there limits? And maybe with robotics and artificial intelligence and so forth coming down the pike, are we in danger of fetishizing precision, making it too important a part of our life and losing our respect and admiration for craftsmanship, for working with wood and all those sorts of things? KELLY: You write about that in such a personal way in the book. You include the detail of your Sunday morning ritual, of walking room to room in this old farmhouse you live in and correcting all the clocks, you know, pushing one hand forward a little bit and another hand back, and that you love the inaccuracy of that, that they're all chiming totally out of sync by midway through the next week. WINCHESTER: Yes, a lovely description in Dorothy L. Sayers' book \"Gaudy Night\" about the clocks in Oxford chiming midnight in friendly disagreement. Well, I like clocks having friendly disagreement. I loathe digital watches with, I mean, the one I dare say you're looking at in the studio now, with microsecond countdowns. Let's take it a bit easier. Let's be a bit fuzzy in our needs and desires and wishes. KELLY: Well, it has been a pleasure speaking with you about precision and maybe the limits of precision. Simon Winchester, thank you. WINCHESTER: Thank you very much. KELLY: His new book is \"The Perfectionist: How Precision Engineers Created The Modern World. \" [POST-BROADCAST CORRECTION: Our guest incorrectly says the transistor was invented in 1948. It was actually invented in 1947 and then announced the following year. ]", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-08-609437180": {"title": "New Jersey Town Restricts Streets From Commuters To Stop Waze Traffic Nightmare : NPR", "url": "https://www.npr.org/2018/05/08/609437180/new-jersey-town-restricts-streets-from-commuters-to-stop-waze-traffic-nightmare", "author": "No author found", "published_date": "2018-05-08", "content": "", "section": "Here & Now Compass", "disclaimer": ""}, "2018-05-08-599452050": {"title": "U.S. Voting System Still Vulnerable To Cyberattacks 6 Months Before Election Day : NPR", "url": "https://www.npr.org/2018/05/08/599452050/the-u-s-voting-system-remains-vulnerable-6-months-before-election-day-what-now", "author": "No author found", "published_date": "2018-05-08", "content": "", "section": "Politics", "disclaimer": ""}, "2018-05-10-609117134": {"title": "Chinese Robocalls Bombarding The U.S. Are Part Of An International Phone Scam : NPR", "url": "https://www.npr.org/2018/05/10/609117134/chinese-robocalls-bombarding-the-u-s-are-part-of-an-international-phone-scam", "author": "No author found", "published_date": "2018-05-10", "content": "AUDIE CORNISH, HOST: People who live in places with lots of Chinese immigrants are receiving robocalls that sound like this one. (SOUNDBITE OF ARCHIVED RECORDING)COMPUTER-GENERATED VOICE #1: (Speaking Mandarin). CORNISH: A person who doesn't speak Mandarin might probably hang up. But reporter Stephen Nessen of member station WNYC in New York stayed on the line. And he discovered an international extortion scheme. STEPHEN NESSEN, BYLINE: On a recent afternoon here at WNYC, all the office phones began ringing at the same time. On the other end was a robotic voice. (CROSSTALK)NESSEN: It says you have a package or document at the Chinese Consulate that might affect your immigration status. Press two to speak with a specialist. So a few Mandarin speakers in the office pressed two. KATHY TU, BYLINE: I was like, but what is this for, really? What is this for? And then they hung up on me. NESSEN: That's my colleague Kathy Tu, co-host of the podcast Nancy. I happened to catch newsroom producer Richard Yeh the moment he got the call. RICHARD YEH, BYLINE: (Speaking Mandarin). NESSEN: And then it got weirder. YEH: He said, what do you want? This is Panda Express. (Speaking Mandarin) Panda Express. I said, you called me, something about a message in the consulate in Chinese. NESSEN: Strange, but as I learned later, people like Yeh and Tu, while they are Mandarin speakers, are not the intended targets of this type of robocall. Neither is the New York Police Department. DONALD MCCAFFREY: I get them also. In the NYPD building, I get them also. NESSEN: Officer Donald McCaffrey started getting them several times a day on his work and cellphone. He's investigating the calls and says he first heard about them last December. MCCAFFREY: There was a Chinese lady, an elderly woman. She's 65. She called to report that someone from the Chinese Consulate basically called and said that she needs to call the Beijing Police Department because she's being investigated for financial crimes over in China. NESSEN: It was not the Chinese Consulate. It was a scam spoofing the consulate's phone number. McCaffrey says the woman transferred $1. 3 million to a Hong Kong bank account. He says 30 cases like this have been reported to the NYPD and estimates $3 million has been lost. The Chinese Consulate in New York says they've reported this scam to the Chinese police but were told it's difficult to track down suspects. Several other Chinese consulates around the country and the embassy in D. C. have posted warnings about this scam. And the U. S. is not alone. People in Australia and New Zealand get the calls. It hit Canada last year. (SOUNDBITE OF PRESS CONFERENCE)ANNIE LINTEAU: Complaints continue to be reported. And the nature of incidents appear to be evolving. NESSEN: That's Staff Sergeant Annie Linteau of the Royal Canadian Mounted Police speaking at a press conference. (SOUNDBITE OF PRESS CONFERENCE)LINTEAU: The victims are then instructed to isolate themselves. And their family in China are then made to believe that their loved one in Canada has been abducted. NESSEN: Ben Yates is a cybersecurity expert and lawyer based in Hong Kong. He says these types of calls have been targeting mainland Chinese there for over three years. BEN YATES: If you're based overseas and you're from mainland China, then you may be concerned about the authorities questioning what you're doing. NESSEN: He says one reason the scheme has been successful is because it isn't out of the realm of possibility that the Chinese government would be looking into what overseas Chinese are doing. YATES: Whether it involves transferring money out of China or other aspects of your activities, it could be all kinds of things that the authorities could have an interest in. NESSEN: He says most people in Hong Kong install an app on their phones to block suspected robocalls. But the mystery remains - who or what is behind this phone scam? The FBI is looking into it. In the meantime, the best advice is. . . (SOUNDBITE OF ARCHIVED RECORDING)COMPUTER-GENERATED VOICE #2: (Speaking Mandarin). NESSEN: . . . Just hang up when you get the call. For NPR News, I'm Stephen Nessen in New York. AUDIE CORNISH, HOST:  People who live in places with lots of Chinese immigrants are receiving robocalls that sound like this one. (SOUNDBITE OF ARCHIVED RECORDING) COMPUTER-GENERATED VOICE #1: (Speaking Mandarin). CORNISH: A person who doesn't speak Mandarin might probably hang up. But reporter Stephen Nessen of member station WNYC in New York stayed on the line. And he discovered an international extortion scheme. STEPHEN NESSEN, BYLINE: On a recent afternoon here at WNYC, all the office phones began ringing at the same time. On the other end was a robotic voice. (CROSSTALK) NESSEN: It says you have a package or document at the Chinese Consulate that might affect your immigration status. Press two to speak with a specialist. So a few Mandarin speakers in the office pressed two. KATHY TU, BYLINE: I was like, but what is this for, really? What is this for? And then they hung up on me. NESSEN: That's my colleague Kathy Tu, co-host of the podcast Nancy. I happened to catch newsroom producer Richard Yeh the moment he got the call. RICHARD YEH, BYLINE: (Speaking Mandarin). NESSEN: And then it got weirder. YEH: He said, what do you want? This is Panda Express. (Speaking Mandarin) Panda Express. I said, you called me, something about a message in the consulate in Chinese. NESSEN: Strange, but as I learned later, people like Yeh and Tu, while they are Mandarin speakers, are not the intended targets of this type of robocall. Neither is the New York Police Department. DONALD MCCAFFREY: I get them also. In the NYPD building, I get them also. NESSEN: Officer Donald McCaffrey started getting them several times a day on his work and cellphone. He's investigating the calls and says he first heard about them last December. MCCAFFREY: There was a Chinese lady, an elderly woman. She's 65. She called to report that someone from the Chinese Consulate basically called and said that she needs to call the Beijing Police Department because she's being investigated for financial crimes over in China. NESSEN: It was not the Chinese Consulate. It was a scam spoofing the consulate's phone number. McCaffrey says the woman transferred $1. 3 million to a Hong Kong bank account. He says 30 cases like this have been reported to the NYPD and estimates $3 million has been lost. The Chinese Consulate in New York says they've reported this scam to the Chinese police but were told it's difficult to track down suspects. Several other Chinese consulates around the country and the embassy in D. C. have posted warnings about this scam. And the U. S. is not alone. People in Australia and New Zealand get the calls. It hit Canada last year. (SOUNDBITE OF PRESS CONFERENCE) ANNIE LINTEAU: Complaints continue to be reported. And the nature of incidents appear to be evolving. NESSEN: That's Staff Sergeant Annie Linteau of the Royal Canadian Mounted Police speaking at a press conference. (SOUNDBITE OF PRESS CONFERENCE) LINTEAU: The victims are then instructed to isolate themselves. And their family in China are then made to believe that their loved one in Canada has been abducted. NESSEN: Ben Yates is a cybersecurity expert and lawyer based in Hong Kong. He says these types of calls have been targeting mainland Chinese there for over three years. BEN YATES: If you're based overseas and you're from mainland China, then you may be concerned about the authorities questioning what you're doing. NESSEN: He says one reason the scheme has been successful is because it isn't out of the realm of possibility that the Chinese government would be looking into what overseas Chinese are doing. YATES: Whether it involves transferring money out of China or other aspects of your activities, it could be all kinds of things that the authorities could have an interest in. NESSEN: He says most people in Hong Kong install an app on their phones to block suspected robocalls. But the mystery remains - who or what is behind this phone scam? The FBI is looking into it. In the meantime, the best advice is. . . (SOUNDBITE OF ARCHIVED RECORDING) COMPUTER-GENERATED VOICE #2: (Speaking Mandarin). NESSEN: . . . Just hang up when you get the call. For NPR News, I'm Stephen Nessen in New York.", "section": "National", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-10-609422158": {"title": "Real-Time Facial Recognition Is Available, But Will U.S. Police Buy It? : NPR", "url": "https://www.npr.org/2018/05/10/609422158/real-time-facial-recognition-is-available-but-will-u-s-police-buy-it", "author": "No author found", "published_date": "2018-05-10", "content": "MARY LOUISE KELLY, HOST: Police in the U. S. have been using facial recognition software for years, usually after a suspect is caught on camera during a crime. Now real-time facial recognition is on the horizon. In China, authorities are touting a new system's ability to spot people as they're walking down the street. Similar software is being tried by police in Russia, in India, even the United Kingdom. So when might it reach American streets? NPR's law enforcement correspondent Martin Kaste went to find out. MARTIN KASTE, BYLINE: This is connect:ID. It's a convention for the biometrics industry that's held in Washington. And the vendors' exhibit hall is just what you'd expect. Everywhere you look, there are big screens with live views of your face as you go by as computers track you and categorize you by age and sex. If they're connected to the right database, they could also guess your name. Terry Hartmann is at the booth for a German company called Cognitec. And what's new, he says, is how good these systems are getting at recognizing faces in real-world conditions. TERRY HARTMANN: You can see all of these matches are different poses. The people aren't facing the camera straight on. You've got people with glasses. You've got the lady looking down. She's matched looking in a different direction. KASTE: Link that to a national photo database, and it's pretty much the end of anonymity in public places. That's clearly the appeal in autocratic societies like China. But these systems are also being pitched to Western governments. Police in the United Kingdom are now scanning crowds for known troublemakers or wanted criminals. And the same tech is being offered to American police, says Clare Garvie. She tracks this issue for the Center on Privacy and Technology at Georgetown Law. CLARE GARVIE: Every major company that sells to law enforcement in the U. S. advertises the ability to do real time. And we've seen a fair amount of interest on the part of law enforcement in purchasing these systems. KASTE: American police aren't buying the real-time systems yet, but Garvie says a few departments have requested funding to try them out. If police are hesitant, it's because the systems are pricey, and there's also the risk of a public backlash. But when it comes to legal barriers, that's less of a concern. JONATHAN TURLEY: There's not a lot standing between instantaneous facial recognition technology and its ubiquitous use by police departments or cities. KASTE: Jonathan Turley is a civil libertarian and law professor at George Washington University. He just gave a speech here at this convention appealing to the industry to set up better privacy protections. That's because he's not sure he can count on the courts to limit how police use facial recognition, especially if Americans get used to this tech in other settings such as on their phones or in shopping malls. TURLEY: As businesses recognize you coming into stores and coffee shops, at what point do our expectations fall to the point that the extension of the government into the area becomes less problematic? KASTE: In other words, why would Americans expect not to be scanned by the police if they come to expect it from, say, the customer loyalty program at their favorite coffee chain? This prediction about the gradual normalization of facial recognition is something you hear from the industry, too. Here again is Terry Hartmann from Cognitec. HARTMANN: If organizations like casinos, security at stadiums are putting in this technology at a level that embarrasses the police where they don't have it themselves, that puts pressure on them. KASTE: The flashiest booth at this convention belongs to NEC Corporation of America. It's a major supplier of still photo facial recognition. They say they have not yet sold any real-time systems to American police, but executive Benji Hutchinson says a few departments are starting to window shop. BENJI HUTCHINSON: I can't say which ones. But it's been the large cities that you might imagine, some of the large coastal areas. KASTE: One sticking point, Hutchinson says, is that American cities often don't have enough of the kind of high-def digital security cameras that you need for facial recognition. Though he says that may not matter in the long term. As if to illustrate his point, a body camera clipped to a nearby mannequin starts beeping. (SOUNDBITE OF BEEPING)KASTE: Is that me, or is that. . . HUTCHINSON: Oh, that's this thing. KASTE: Oh, OK. HUTCHINSON: Yeah. Yeah. KASTE: (Laughter) I never know what's beeping. Oh, it's matching someone. OK. A body camera that NEC has on display has recognized a passing face. Hutchinson predicts this is how the technology will come to American streets, by piggybacking on the thousands of new body cameras that departments are buying for their officers. At first, he says, it'll just be used as a way to recognize the faces of innocent people and blur them out. But it won't stop there. HUTCHINSON: I think the second step is obviously they're going to look at ways to implement facial recognition against known or suspected terrorists or people who are wanted. That'll be the obvious next step. And it is a pretty simple leap. KASTE: The question is, which law enforcement agency will be the first to take that leap? Hutchinson predicts it'll likely be a deep-pocketed federal department such as Customs and Border Protection, which is already testing facial recognition at passport control in airports. And as he mentions this, two CBP officials stop by the NEC booth, and the company's sales team scrambles to show them what the body cameras can do. Martin Kaste, NPR News, Washington. MARY LOUISE KELLY, HOST:  Police in the U. S. have been using facial recognition software for years, usually after a suspect is caught on camera during a crime. Now real-time facial recognition is on the horizon. In China, authorities are touting a new system's ability to spot people as they're walking down the street. Similar software is being tried by police in Russia, in India, even the United Kingdom. So when might it reach American streets? NPR's law enforcement correspondent Martin Kaste went to find out. MARTIN KASTE, BYLINE: This is connect:ID. It's a convention for the biometrics industry that's held in Washington. And the vendors' exhibit hall is just what you'd expect. Everywhere you look, there are big screens with live views of your face as you go by as computers track you and categorize you by age and sex. If they're connected to the right database, they could also guess your name. Terry Hartmann is at the booth for a German company called Cognitec. And what's new, he says, is how good these systems are getting at recognizing faces in real-world conditions. TERRY HARTMANN: You can see all of these matches are different poses. The people aren't facing the camera straight on. You've got people with glasses. You've got the lady looking down. She's matched looking in a different direction. KASTE: Link that to a national photo database, and it's pretty much the end of anonymity in public places. That's clearly the appeal in autocratic societies like China. But these systems are also being pitched to Western governments. Police in the United Kingdom are now scanning crowds for known troublemakers or wanted criminals. And the same tech is being offered to American police, says Clare Garvie. She tracks this issue for the Center on Privacy and Technology at Georgetown Law. CLARE GARVIE: Every major company that sells to law enforcement in the U. S. advertises the ability to do real time. And we've seen a fair amount of interest on the part of law enforcement in purchasing these systems. KASTE: American police aren't buying the real-time systems yet, but Garvie says a few departments have requested funding to try them out. If police are hesitant, it's because the systems are pricey, and there's also the risk of a public backlash. But when it comes to legal barriers, that's less of a concern. JONATHAN TURLEY: There's not a lot standing between instantaneous facial recognition technology and its ubiquitous use by police departments or cities. KASTE: Jonathan Turley is a civil libertarian and law professor at George Washington University. He just gave a speech here at this convention appealing to the industry to set up better privacy protections. That's because he's not sure he can count on the courts to limit how police use facial recognition, especially if Americans get used to this tech in other settings such as on their phones or in shopping malls. TURLEY: As businesses recognize you coming into stores and coffee shops, at what point do our expectations fall to the point that the extension of the government into the area becomes less problematic? KASTE: In other words, why would Americans expect not to be scanned by the police if they come to expect it from, say, the customer loyalty program at their favorite coffee chain? This prediction about the gradual normalization of facial recognition is something you hear from the industry, too. Here again is Terry Hartmann from Cognitec. HARTMANN: If organizations like casinos, security at stadiums are putting in this technology at a level that embarrasses the police where they don't have it themselves, that puts pressure on them. KASTE: The flashiest booth at this convention belongs to NEC Corporation of America. It's a major supplier of still photo facial recognition. They say they have not yet sold any real-time systems to American police, but executive Benji Hutchinson says a few departments are starting to window shop. BENJI HUTCHINSON: I can't say which ones. But it's been the large cities that you might imagine, some of the large coastal areas. KASTE: One sticking point, Hutchinson says, is that American cities often don't have enough of the kind of high-def digital security cameras that you need for facial recognition. Though he says that may not matter in the long term. As if to illustrate his point, a body camera clipped to a nearby mannequin starts beeping. (SOUNDBITE OF BEEPING) KASTE: Is that me, or is that. . . HUTCHINSON: Oh, that's this thing. KASTE: Oh, OK. HUTCHINSON: Yeah. Yeah. KASTE: (Laughter) I never know what's beeping. Oh, it's matching someone. OK. A body camera that NEC has on display has recognized a passing face. Hutchinson predicts this is how the technology will come to American streets, by piggybacking on the thousands of new body cameras that departments are buying for their officers. At first, he says, it'll just be used as a way to recognize the faces of innocent people and blur them out. But it won't stop there. HUTCHINSON: I think the second step is obviously they're going to look at ways to implement facial recognition against known or suspected terrorists or people who are wanted. That'll be the obvious next step. And it is a pretty simple leap. KASTE: The question is, which law enforcement agency will be the first to take that leap? Hutchinson predicts it'll likely be a deep-pocketed federal department such as Customs and Border Protection, which is already testing facial recognition at passport control in airports. And as he mentions this, two CBP officials stop by the NEC booth, and the company's sales team scrambles to show them what the body cameras can do. Martin Kaste, NPR News, Washington.", "section": "Criminal Justice Collaborative", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-12-610632088": {"title": "Axon Considers Combining Body Cameras With Facial Recognition : NPR", "url": "https://www.npr.org/2018/05/12/610632088/what-artificial-intelligence-can-do-for-local-cops", "author": "No author found", "published_date": "2018-05-12", "content": "SCOTT SIMON, HOST: Axon, formerly known as TASER International, makes tasers and body cameras for police departments. And in the near future, body cams may be equipped with facial recognition software. The company has created a new ethics board to consider some of the implications of this software and the emerging use of artificial intelligence in local policing. Axon CEO Rick Smith joins us now from their offices in Scottsdale, Ariz. Thanks so much for being with us. RICK SMITH: Thanks for having me on today. SIMON: As I don't have to tell you, Mr. Smith, you announced your ethics board, and a group of more than 40 civil rights and tech groups wrote you a letter in which they said you should never develop real-time facial recognition through police body cameras because they say the risks of misidentification are too high. Innocent people could be pursued by the police - sometimes suffer fatal consequences. How do you respond to that? SMITH: Well, we agree philosophically with the issues that were raised. But I think it's counterproductive to say that a technology is unethical and should never be developed. I think what we need to do is take a look at how this technology could evolve. What are the risks? But basically today, an individual officer might have to make life-or-death decisions based only on their own perceptions and prejudices. Do we think that computers getting information to those officers that could them make better decisions would move the world in the right direction? And I think the answer is unequivocally, yes, that could happen. SIMON: As the technology stands now - as we've heard reported any number of times, the technology's especially faulty when it comes to seeing the differences in darker faces. SMITH: I think that has to do with the types of training data sets that have been used historically. Certainly those are one of the issues that before we developed anything to be deployed in the field, we would take a very hard look at that. SIMON: I gather Chinese police, for example, routinely use facial recognition technology. Some of them even have sunglasses that come equipped with cameras that can identify faces in real time. They say it's allowed them to arrest suspected criminals. And in, you know, China criminals can include people who just believe in free speech. Do you have reservations about using that technology here? SMITH: Well, you know, for example, there are police forces around the world that use batons and guns in very abusive ways. And yet ultimately, we know that our police, in order to do their job, need to have those same types of tools. We understand that these technologies could be used in ways that we don't want to see happening in our society. However, I think it's too blunt an instrument to say that because there is a risk of misuse, we should just write them off. I think we need to dig a layer deeper and understand what are the benefits and what are the risks. SIMON: What are the benefits in your mind? SMITH: Well, I mean, you could imagine many benefits. I think one example you can look at is DNA technology. You know, when DNA was first being introduced, there was much concern about false positives and false matches. And yet ultimately, I think DNA technology has done more than any other key technology in exonerating people that were wrongfully convicted. I think we'll see other biometrics, including facial recognition technology, that properly deployed with the right oversight over the coming decades could ultimately reduce prejudice in policing and help catch dangerous people that we all agree we don't want out in our communities and do it in a way that, at the same time, respects police transparency and rights of privacy of the average citizen. SIMON: Maybe this is generational, but, Mr. Smith, how do you feel about the fact that we might soon have a technology that - well, when you leave the office today, it'll recognize you and know when you get into the elevator. It will recognize you when you're in the parking lot. It will recognize you when you stop at a stoplight on your way home and know where you are all the time. SMITH: Well, it's certainly an interesting world that we're moving into where notions of privacy are changing pretty dramatically. And what's most interesting is I think people are actually opting into these systems. Knowingly and willingly, they're deploying these types of technologies for the convenience that they offer to themselves. And then that opens questions of what does privacy mean in the world we live in today. And frankly, what's it going to mean in another 10 or 20 years? SIMON: Rick Smith, CEO of Axon, formerly known as TASER International. Thanks so much for being with us, sir. SMITH: Great. And thank you for the thoughtful questions. SCOTT SIMON, HOST:  Axon, formerly known as TASER International, makes tasers and body cameras for police departments. And in the near future, body cams may be equipped with facial recognition software. The company has created a new ethics board to consider some of the implications of this software and the emerging use of artificial intelligence in local policing. Axon CEO Rick Smith joins us now from their offices in Scottsdale, Ariz. Thanks so much for being with us. RICK SMITH: Thanks for having me on today. SIMON: As I don't have to tell you, Mr. Smith, you announced your ethics board, and a group of more than 40 civil rights and tech groups wrote you a letter in which they said you should never develop real-time facial recognition through police body cameras because they say the risks of misidentification are too high. Innocent people could be pursued by the police - sometimes suffer fatal consequences. How do you respond to that? SMITH: Well, we agree philosophically with the issues that were raised. But I think it's counterproductive to say that a technology is unethical and should never be developed. I think what we need to do is take a look at how this technology could evolve. What are the risks? But basically today, an individual officer might have to make life-or-death decisions based only on their own perceptions and prejudices. Do we think that computers getting information to those officers that could them make better decisions would move the world in the right direction? And I think the answer is unequivocally, yes, that could happen. SIMON: As the technology stands now - as we've heard reported any number of times, the technology's especially faulty when it comes to seeing the differences in darker faces. SMITH: I think that has to do with the types of training data sets that have been used historically. Certainly those are one of the issues that before we developed anything to be deployed in the field, we would take a very hard look at that. SIMON: I gather Chinese police, for example, routinely use facial recognition technology. Some of them even have sunglasses that come equipped with cameras that can identify faces in real time. They say it's allowed them to arrest suspected criminals. And in, you know, China criminals can include people who just believe in free speech. Do you have reservations about using that technology here? SMITH: Well, you know, for example, there are police forces around the world that use batons and guns in very abusive ways. And yet ultimately, we know that our police, in order to do their job, need to have those same types of tools. We understand that these technologies could be used in ways that we don't want to see happening in our society. However, I think it's too blunt an instrument to say that because there is a risk of misuse, we should just write them off. I think we need to dig a layer deeper and understand what are the benefits and what are the risks. SIMON: What are the benefits in your mind? SMITH: Well, I mean, you could imagine many benefits. I think one example you can look at is DNA technology. You know, when DNA was first being introduced, there was much concern about false positives and false matches. And yet ultimately, I think DNA technology has done more than any other key technology in exonerating people that were wrongfully convicted. I think we'll see other biometrics, including facial recognition technology, that properly deployed with the right oversight over the coming decades could ultimately reduce prejudice in policing and help catch dangerous people that we all agree we don't want out in our communities and do it in a way that, at the same time, respects police transparency and rights of privacy of the average citizen. SIMON: Maybe this is generational, but, Mr. Smith, how do you feel about the fact that we might soon have a technology that - well, when you leave the office today, it'll recognize you and know when you get into the elevator. It will recognize you when you're in the parking lot. It will recognize you when you stop at a stoplight on your way home and know where you are all the time. SMITH: Well, it's certainly an interesting world that we're moving into where notions of privacy are changing pretty dramatically. And what's most interesting is I think people are actually opting into these systems. Knowingly and willingly, they're deploying these types of technologies for the convenience that they offer to themselves. And then that opens questions of what does privacy mean in the world we live in today. And frankly, what's it going to mean in another 10 or 20 years? SIMON: Rick Smith, CEO of Axon, formerly known as TASER International. Thanks so much for being with us, sir. SMITH: Great. And thank you for the thoughtful questions.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-13-610777754": {"title": "Taking A Look At Those Russia-Linked Facebook Ads : NPR", "url": "https://www.npr.org/2018/05/13/610777754/taking-a-look-at-those-russia-linked-facebook-ads", "author": "No author found", "published_date": "2018-05-13", "content": "LULU GARCIA-NAVARRO, HOST: This past week, Democrats on the House intelligence committee released over 3,500 Russia-linked Facebook and Instagram ads - the primary tool through which Russian operatives meddled in the 2016 election. It's the most of these ads that we've been able to get a look at so far. Issie Lapowsky is a senior writer at Wired. And she spent some time parsing through the thousands of ads. She joins us from our studios in New York. Welcome to the program. ISSIE LAPOWSKY: Thanks for having me. GARCIA-NAVARRO: Well, let's get to it. Can you give us some examples of the things that you saw? LAPOWSKY: Yeah, it's some wild stuff. I'll categorize it by saying that there are a few buckets that these ads fell into. A lot of them had to do with Black Lives Matter issues, immigration issues, Second Amendment issues. There was also a slew of ads devoted to Texas secession. There were ads devoted to both Hillary Clinton and Donald Trump and a good number of Bernie Sanders ads, as well. In other cases, it looked like they were sort of fishing for people's attention. So maybe these were just meme accounts where they would share memes completely unrelated to politics. And you look through them. And you're, like, what does this have to do with politics, right? But then you find one lingering ad in there that is a meme about Hillary Clinton's emails. GARCIA-NAVARRO: You know, one could argue that these were threads that were already in the election. I mean, these are things that the American public were debating and the presidential candidates themselves. LAPOWSKY: Absolutely. You see them really repeating a lot of the rhetorical devices that were being used in the election by some of the candidates. For instance, this is a really interesting one. It was called Williams and Kalvin. And these were real youtubers that the Russian trolls hired to create their own channel and talk about - you know, they purported to talk about, you know, black culture, the black community. But what you found was a lot of what they shared was negative content about Hillary Clinton. So one of their ads - it said, why should someone vote for a criminal? And then it quoted from the book \"Clinton Cash,\" which, again, was a big talking point of the Trump campaign. And it specifically targeted Hispanic people on Facebook and people who, for instance, followed or were interested in Maya Angelou or Mother Jones. So you can see how they were really going after what would be sort of a traditional Democratic base and trying to turn them against Hillary Clinton, using some of the talking points that were already being used in the election. GARCIA-NAVARRO: So Williams and Kalvin were Americans hired by Russians? I mean - and who are they? LAPOWSKY: They were American video bloggers, essentially, on YouTube that were hired. And you found this in a lot of cases. The Internet Research Agency would approach, for instance, physical trainers and actually pay them money to teach self-defense classes. And then they would advertise these self-defense classes on Facebook in order to get people visiting their community. I guess they wanted to get people together and at least thinking about the fact that people were racially divided and, in some sense, needed to defend themselves. GARCIA-NAVARRO: Thirty-five hundred ads. That's a lot of ads. But how much of what the IRA did does that represent? LAPOWSKY: A small slice of what they did. Thirty-five hundred ads. But on Facebook alone, there were 80,000 pieces of organic content. So that's, you know, just the content that these accounts were posting. On Instagram, there were 120,000 pieces of organic content. So a lot of researchers have been sounding the alarm about the fact that the ads are a bit of a red herring. Yes, they do get more exposure. But if people were organically following any of these accounts, and they had, you know, thousands of followers, they were getting much more exposure to all of this content. GARCIA-NAVARRO: Issie Lapowsky's story is up on wired. com now. And she's a senior writer there. Thank you so much. LAPOWSKY: Thank you. LULU GARCIA-NAVARRO, HOST:  This past week, Democrats on the House intelligence committee released over 3,500 Russia-linked Facebook and Instagram ads - the primary tool through which Russian operatives meddled in the 2016 election. It's the most of these ads that we've been able to get a look at so far. Issie Lapowsky is a senior writer at Wired. And she spent some time parsing through the thousands of ads. She joins us from our studios in New York. Welcome to the program. ISSIE LAPOWSKY: Thanks for having me. GARCIA-NAVARRO: Well, let's get to it. Can you give us some examples of the things that you saw? LAPOWSKY: Yeah, it's some wild stuff. I'll categorize it by saying that there are a few buckets that these ads fell into. A lot of them had to do with Black Lives Matter issues, immigration issues, Second Amendment issues. There was also a slew of ads devoted to Texas secession. There were ads devoted to both Hillary Clinton and Donald Trump and a good number of Bernie Sanders ads, as well. In other cases, it looked like they were sort of fishing for people's attention. So maybe these were just meme accounts where they would share memes completely unrelated to politics. And you look through them. And you're, like, what does this have to do with politics, right? But then you find one lingering ad in there that is a meme about Hillary Clinton's emails. GARCIA-NAVARRO: You know, one could argue that these were threads that were already in the election. I mean, these are things that the American public were debating and the presidential candidates themselves. LAPOWSKY: Absolutely. You see them really repeating a lot of the rhetorical devices that were being used in the election by some of the candidates. For instance, this is a really interesting one. It was called Williams and Kalvin. And these were real youtubers that the Russian trolls hired to create their own channel and talk about - you know, they purported to talk about, you know, black culture, the black community. But what you found was a lot of what they shared was negative content about Hillary Clinton. So one of their ads - it said, why should someone vote for a criminal? And then it quoted from the book \"Clinton Cash,\" which, again, was a big talking point of the Trump campaign. And it specifically targeted Hispanic people on Facebook and people who, for instance, followed or were interested in Maya Angelou or Mother Jones. So you can see how they were really going after what would be sort of a traditional Democratic base and trying to turn them against Hillary Clinton, using some of the talking points that were already being used in the election. GARCIA-NAVARRO: So Williams and Kalvin were Americans hired by Russians? I mean - and who are they? LAPOWSKY: They were American video bloggers, essentially, on YouTube that were hired. And you found this in a lot of cases. The Internet Research Agency would approach, for instance, physical trainers and actually pay them money to teach self-defense classes. And then they would advertise these self-defense classes on Facebook in order to get people visiting their community. I guess they wanted to get people together and at least thinking about the fact that people were racially divided and, in some sense, needed to defend themselves. GARCIA-NAVARRO: Thirty-five hundred ads. That's a lot of ads. But how much of what the IRA did does that represent? LAPOWSKY: A small slice of what they did. Thirty-five hundred ads. But on Facebook alone, there were 80,000 pieces of organic content. So that's, you know, just the content that these accounts were posting. On Instagram, there were 120,000 pieces of organic content. So a lot of researchers have been sounding the alarm about the fact that the ads are a bit of a red herring. Yes, they do get more exposure. But if people were organically following any of these accounts, and they had, you know, thousands of followers, they were getting much more exposure to all of this content. GARCIA-NAVARRO: Issie Lapowsky's story is up on wired. com now. And she's a senior writer there. Thank you so much. LAPOWSKY: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-14-611097647": {"title": "Google's 'Duplex' Raises Ethical Questions : NPR", "url": "https://www.npr.org/2018/05/14/611097647/googles-duplex-raises-ethical-questions", "author": "No author found", "published_date": "2018-05-14", "content": "MARY LOUISE KELLY, HOST: As robots continue their relentless march towards doing all kinds of things better than humans, we consider this question. Should a bot sound like a human? It's time for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")KELLY: We're talking today about Google Duplex. It's a technology that mimics natural speech - like, really natural speech. Here's Duplex calling for a dinner reservation. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: How may I help you? COMPUTER-GENERATED VOICE #1: Hi. I'd like to reserve a table for Wednesday the 7th. UNIDENTIFIED PERSON #1: For seven people? COMPUTER-GENERATED VOICE #1: It's for four people. KELLY: Google Duplex is designed to handle tasks like booking dinner or scheduling your dental cleaning. It launched an ethics debate when Google demoed it at a conference last week. So into the fray of that debate, let's bring Shane Mac. He is CEO and co-founder of Assist. That is an automated assistant platform for messaging and voice. Shane Mac has spent a lot of time thinking about the technology and the ethics involved here. Shane, welcome. SHANE MAC: Thank you for having me on the show. KELLY: We're glad to have you here. That piece of tape that we just heard, I think part of what throws me is just how natural it sounds - this bot saying mmm hmm and um. What is the reason for making it sound quite so natural? MAC: I think that was just technology for technology's sake, to be honest. And you see Google actually - after that announcement, they came out and said, we're going to have a disclosure that it is a robot before it gets on the phone. KELLY: This was a big question at this conference over whether Google had some responsibility to tell people, hey, this is a robot that you're talking to. MAC: A hundred percent. And I think you're going to see that. You know, this call is being recorded for quality purposes, what you see today. I think there's going to be a lot more of that needed to disclose when it's a robot, when it's not. KELLY: Is it as basic as saying, hi, by the way, you're talking to a bot? MAC: Or even think about it in the assistant world is like, this is Shane's assistant. It is an automated assistant, and it is trying to help Shane get things done. KELLY: I want to play one more sample. This is Duplex calling a hair salon. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: Hello. How can I help you? COMPUTER-GENERATED VOICE #2: Hi. I'm calling to book a woman's haircut for a client. I'm looking for something on May 3. UNIDENTIFIED PERSON #2: Sure. Give me one second - sure. What time are you looking for around? COMPUTER-GENERATED VOICE #2: At 12 p. m. KELLY: Is there a big market for this? I mean, one of the things that blew my mind about this was that many people were still making phone calls to do things like schedule brunch in an era where we're all used to apps and just punching in the reservation time we want. MAC: It's crazy how big it is. Like, 60 percent of people still call. I think this is where it has a real advantage for small businesses. They don't have the resources today to have a great scheduling system. The other side of this is Google - what if they just said, hey, next time instead of us calling you, would you like us to respond with a robot on your side? And then bots are going to be talking to bots. KELLY: Which is further mind-blowing, yes, that the person answering the phone and carrying out this reservation process would also be a robot. And then it all just auto-populates on your calendar. Is that where this is all headed? MAC: Probably, yes. I think what the dream of Google is really saving time on both sides so it becomes an intent world. Shane wants to get a haircut; the salon wants to cut hair. Everything in the middle is just saving people time. KELLY: Although there was a real backlash when Google rolled this out at this conference. I noticed Zeynep Tufekci, who's a professor at University of North Carolina, Chapel Hill. She studies the social impacts of technology. She was tweeting saying calling this is horrifying, saying Silicon Valley is ethically lost. Are you sympathetic to those concerns? MAC: I think it's actually very important. There's huge implications here when you think about this being used to totally fake as someone. If the robot can sound like John Legend and John Legend calls me and I think it's John Legend and that is now doing things that are unethical, there are so many unanswered questions here that I think we have to really get ahead of. And new laws need to be created of disclosure, of the intent. And is it a machine, or is it not? KELLY: That's Shane Mac, or at least we think it's Shane Mac. (LAUGHTER)MAC: That is the thing. I mean. . . KELLY: That's the thing. All right, we think we've been talking with Shane Mac, the CEO and co-founder of Assist. Shane Mac, thank you. MAC: Thank you very much. MARY LOUISE KELLY, HOST:  As robots continue their relentless march towards doing all kinds of things better than humans, we consider this question. Should a bot sound like a human? It's time for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") KELLY: We're talking today about Google Duplex. It's a technology that mimics natural speech - like, really natural speech. Here's Duplex calling for a dinner reservation. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: How may I help you? COMPUTER-GENERATED VOICE #1: Hi. I'd like to reserve a table for Wednesday the 7th. UNIDENTIFIED PERSON #1: For seven people? COMPUTER-GENERATED VOICE #1: It's for four people. KELLY: Google Duplex is designed to handle tasks like booking dinner or scheduling your dental cleaning. It launched an ethics debate when Google demoed it at a conference last week. So into the fray of that debate, let's bring Shane Mac. He is CEO and co-founder of Assist. That is an automated assistant platform for messaging and voice. Shane Mac has spent a lot of time thinking about the technology and the ethics involved here. Shane, welcome. SHANE MAC: Thank you for having me on the show. KELLY: We're glad to have you here. That piece of tape that we just heard, I think part of what throws me is just how natural it sounds - this bot saying mmm hmm and um. What is the reason for making it sound quite so natural? MAC: I think that was just technology for technology's sake, to be honest. And you see Google actually - after that announcement, they came out and said, we're going to have a disclosure that it is a robot before it gets on the phone. KELLY: This was a big question at this conference over whether Google had some responsibility to tell people, hey, this is a robot that you're talking to. MAC: A hundred percent. And I think you're going to see that. You know, this call is being recorded for quality purposes, what you see today. I think there's going to be a lot more of that needed to disclose when it's a robot, when it's not. KELLY: Is it as basic as saying, hi, by the way, you're talking to a bot? MAC: Or even think about it in the assistant world is like, this is Shane's assistant. It is an automated assistant, and it is trying to help Shane get things done. KELLY: I want to play one more sample. This is Duplex calling a hair salon. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #2: Hello. How can I help you? COMPUTER-GENERATED VOICE #2: Hi. I'm calling to book a woman's haircut for a client. I'm looking for something on May 3. UNIDENTIFIED PERSON #2: Sure. Give me one second - sure. What time are you looking for around? COMPUTER-GENERATED VOICE #2: At 12 p. m. KELLY: Is there a big market for this? I mean, one of the things that blew my mind about this was that many people were still making phone calls to do things like schedule brunch in an era where we're all used to apps and just punching in the reservation time we want. MAC: It's crazy how big it is. Like, 60 percent of people still call. I think this is where it has a real advantage for small businesses. They don't have the resources today to have a great scheduling system. The other side of this is Google - what if they just said, hey, next time instead of us calling you, would you like us to respond with a robot on your side? And then bots are going to be talking to bots. KELLY: Which is further mind-blowing, yes, that the person answering the phone and carrying out this reservation process would also be a robot. And then it all just auto-populates on your calendar. Is that where this is all headed? MAC: Probably, yes. I think what the dream of Google is really saving time on both sides so it becomes an intent world. Shane wants to get a haircut; the salon wants to cut hair. Everything in the middle is just saving people time. KELLY: Although there was a real backlash when Google rolled this out at this conference. I noticed Zeynep Tufekci, who's a professor at University of North Carolina, Chapel Hill. She studies the social impacts of technology. She was tweeting saying calling this is horrifying, saying Silicon Valley is ethically lost. Are you sympathetic to those concerns? MAC: I think it's actually very important. There's huge implications here when you think about this being used to totally fake as someone. If the robot can sound like John Legend and John Legend calls me and I think it's John Legend and that is now doing things that are unethical, there are so many unanswered questions here that I think we have to really get ahead of. And new laws need to be created of disclosure, of the intent. And is it a machine, or is it not? KELLY: That's Shane Mac, or at least we think it's Shane Mac. (LAUGHTER) MAC: That is the thing. I mean. . . KELLY: That's the thing. All right, we think we've been talking with Shane Mac, the CEO and co-founder of Assist. Shane Mac, thank you. MAC: Thank you very much.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-16-611742303": {"title": "SEC Creates Spoof Cryptocurrency Website To Warn Investors : NPR", "url": "https://www.npr.org/2018/05/16/611742303/sec-creates-spoof-cryptocurrency-website-to-warn-investors", "author": "No author found", "published_date": "2018-05-16", "content": "", "section": "Business", "disclaimer": ""}, "2018-05-16-611727847": {"title": "So Which Is It, Yanny Or Laurel? : NPR", "url": "https://www.npr.org/2018/05/16/611727847/so-which-is-it-yanny-or-laurel", "author": "No author found", "published_date": "2018-05-16", "content": "ARI SHAPIRO, HOST:  We all know that America is a divided country. Well, this week, it became a little more divided thanks to this word. (SOUNDBITE OF ARCHIVED RECORDING)COMPUTER-GENERATED VOICE: Laurel. SHAPIRO: Obviously Yanny. AUDIE CORNISH, HOST:  It's Laurel. (SOUNDBITE OF ARCHIVED RECORDING)COMPUTER-GENERATED VOICE: Laurel. CORNISH: Ari. . . SHAPIRO: It's definitely not Laurel. CORNISH: No, I'm - me and the rest of America, I think, think it's Laurel. SHAPIRO: I've listened to this word so many times, and I cannot hear Laurel. CORNISH: OK. We are not the only ones on social media staking out positions on this. We decided to dig into what's actually happening. And on the line, we have Lee Miller. He's the associate professor in neurobiology, physiology and behavior at UC Davis. Welcome to the program. LEE MILLER: Thank you. Glad to be here. CORNISH: All right. Lee Miller, just so we can get all of our biases out on the table, what do you hear when we play this word? (SOUNDBITE OF ARCHIVED RECORDING)COMPUTER-GENERATED VOICE: Laurel. SHAPIRO: Yanny, obviously. MILLER: It's obviously Laurel. No question. SHAPIRO: What (laughter)? CORNISH: OK. All right. So Ari claims he's hearing Yanny. SHAPIRO: I'm definitely hearing - not Yonny (ph), Yanny. CORNISH: Yanny. OK. I'll take your word for it. But I understand that it is possible for people to hear two completely different words, and it has to do with the frequency? MILLER: Evidently that's what differentiates the Laurel versus Yanny, or Yanny here. Yeah. What you say is true. I mean, we don't really experience the world as it is, but we experience our brain's best guess. And the cues that we get from the world are often more ambiguous than we realize. So not only is our brain constantly unconsciously filling in gaps, repairing errors, noisy bits based on the context, but it's also making assumptions about a talker's voice - their accent or their idiosyncrasies in pronunciation. I think, in this case, we don't have much context, for one. We can't hear the word in a sentence. There's some noise in higher frequencies that might seem to influence people to hear the false perceptive, Yanny. SHAPIRO: Wait; wait; wait; wait. You just described Yanny as false, and I would like to say that it is not false. Yanny is very real. I understand that an L sound and an R sound are kind of similar. A B and a P are kind of similar. But the N in Yanny is nothing like the R in Laurel. So why are we hearing an N as an R, and vice versa? MILLER: Yeah, I agree. And I was looking at - before we talked here, I was looking really quick at where the power, the energy is in the different frequencies - during the N and the R. And I think that's one of the moments where there's some of this noise in the recording, which isn't coming from his pronunciation, but it's in there nonetheless. (SOUNDBITE OF ARCHIVED RECORDING)COMPUTER-GENERATED VOICE: Laurel. MILLER: And that's what's throwing your ear off. But it's not throwing my ear off because not only do we - is there ambiguity in the physics of the world, but we also - our perceptual systems have different sensitivities, too. So I think in my case, I can't hear high frequencies very well, but you can. And that's probably going to go some way to explain the difference in our percepts, too. SHAPIRO: Is there a right answer to this question? MILLER: I think, in this case, if we can track down the actual recording, we can establish that. But it's not going to stop us from arguing about it, is it? It's like the gold and the blue dress, right? CORNISH: So true. MILLER: Which is obviously white and gold. CORNISH: Yes, exactly (laughter). MILLER: Obviously. CORNISH: And not blue and black, as the other half of the Internet thought at the time. MILLER: No. No, no. SHAPIRO: Lee Miller of the University of California at Davis. Thank you, although, no thank you for siding with Audie over me. CORNISH: And what he means by that is thank you for proving me correct. MILLER: You're very welcome. Thanks for having me. SHAPIRO: OK, OK, OK. It's come to my attention that the New York Times and Wired magazine each tracked down what they call the actual answer to this question. CORNISH: And the answer is? SHAPIRO: It pains me to say, apparently, there was a high school student who played the online dictionary audio file for the word. . . (SOUNDBITE OF ARCHIVED RECORDING)COMPUTER-GENERATED VOICE: Laurel. SHAPIRO: . . . Laurel, and that that is what we are listening to - somebody who falsely recorded the sound Yanny on the word Laurel. CORNISH: I don't think that was the answer there (laughter). SHAPIRO: That's the answer I'm going with. CORNISH: Yeah. OK. Well, Ari, I appreciate being right. ARI SHAPIRO, HOST:   We all know that America is a divided country. Well, this week, it became a little more divided thanks to this word. (SOUNDBITE OF ARCHIVED RECORDING) COMPUTER-GENERATED VOICE: Laurel. SHAPIRO: Obviously Yanny. AUDIE CORNISH, HOST:   It's Laurel. (SOUNDBITE OF ARCHIVED RECORDING) COMPUTER-GENERATED VOICE: Laurel. CORNISH: Ari. . . SHAPIRO: It's definitely not Laurel. CORNISH: No, I'm - me and the rest of America, I think, think it's Laurel. SHAPIRO: I've listened to this word so many times, and I cannot hear Laurel. CORNISH: OK. We are not the only ones on social media staking out positions on this. We decided to dig into what's actually happening. And on the line, we have Lee Miller. He's the associate professor in neurobiology, physiology and behavior at UC Davis. Welcome to the program. LEE MILLER: Thank you. Glad to be here. CORNISH: All right. Lee Miller, just so we can get all of our biases out on the table, what do you hear when we play this word? (SOUNDBITE OF ARCHIVED RECORDING) COMPUTER-GENERATED VOICE: Laurel. SHAPIRO: Yanny, obviously. MILLER: It's obviously Laurel. No question. SHAPIRO: What (laughter)? CORNISH: OK. All right. So Ari claims he's hearing Yanny. SHAPIRO: I'm definitely hearing - not Yonny (ph), Yanny. CORNISH: Yanny. OK. I'll take your word for it. But I understand that it is possible for people to hear two completely different words, and it has to do with the frequency? MILLER: Evidently that's what differentiates the Laurel versus Yanny, or Yanny here. Yeah. What you say is true. I mean, we don't really experience the world as it is, but we experience our brain's best guess. And the cues that we get from the world are often more ambiguous than we realize. So not only is our brain constantly unconsciously filling in gaps, repairing errors, noisy bits based on the context, but it's also making assumptions about a talker's voice - their accent or their idiosyncrasies in pronunciation. I think, in this case, we don't have much context, for one. We can't hear the word in a sentence. There's some noise in higher frequencies that might seem to influence people to hear the false perceptive, Yanny. SHAPIRO: Wait; wait; wait; wait. You just described Yanny as false, and I would like to say that it is not false. Yanny is very real. I understand that an L sound and an R sound are kind of similar. A B and a P are kind of similar. But the N in Yanny is nothing like the R in Laurel. So why are we hearing an N as an R, and vice versa? MILLER: Yeah, I agree. And I was looking at - before we talked here, I was looking really quick at where the power, the energy is in the different frequencies - during the N and the R. And I think that's one of the moments where there's some of this noise in the recording, which isn't coming from his pronunciation, but it's in there nonetheless. (SOUNDBITE OF ARCHIVED RECORDING) COMPUTER-GENERATED VOICE: Laurel. MILLER: And that's what's throwing your ear off. But it's not throwing my ear off because not only do we - is there ambiguity in the physics of the world, but we also - our perceptual systems have different sensitivities, too. So I think in my case, I can't hear high frequencies very well, but you can. And that's probably going to go some way to explain the difference in our percepts, too. SHAPIRO: Is there a right answer to this question? MILLER: I think, in this case, if we can track down the actual recording, we can establish that. But it's not going to stop us from arguing about it, is it? It's like the gold and the blue dress, right? CORNISH: So true. MILLER: Which is obviously white and gold. CORNISH: Yes, exactly (laughter). MILLER: Obviously. CORNISH: And not blue and black, as the other half of the Internet thought at the time. MILLER: No. No, no. SHAPIRO: Lee Miller of the University of California at Davis. Thank you, although, no thank you for siding with Audie over me. CORNISH: And what he means by that is thank you for proving me correct. MILLER: You're very welcome. Thanks for having me. SHAPIRO: OK, OK, OK. It's come to my attention that the New York Times and Wired magazine each tracked down what they call the actual answer to this question. CORNISH: And the answer is? SHAPIRO: It pains me to say, apparently, there was a high school student who played the online dictionary audio file for the word. . . (SOUNDBITE OF ARCHIVED RECORDING) COMPUTER-GENERATED VOICE: Laurel. SHAPIRO: . . . Laurel, and that that is what we are listening to - somebody who falsely recorded the sound Yanny on the word Laurel. CORNISH: I don't think that was the answer there (laughter). SHAPIRO: That's the answer I'm going with. CORNISH: Yeah. OK. Well, Ari, I appreciate being right.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-16-611412562": {"title": "Whistleblower: Cambridge Analytica Aimed To Trigger Paranoia And Racial Biases : NPR", "url": "https://www.npr.org/2018/05/16/611412562/whistleblower-cambridge-analytica-aimed-to-trigger-paranoia-and-racial-biases", "author": "No author found", "published_date": "2018-05-16", "content": "", "section": "Technology", "disclaimer": ""}, "2018-05-16-611538490": {"title": "Senate Votes On Net Neutrality : NPR", "url": "https://www.npr.org/2018/05/16/611538490/senate-votes-on-net-neutrality", "author": "No author found", "published_date": "2018-05-16", "content": "RACHEL MARTIN, HOST: Democrats are staging an insurgency of sorts today in the Senate. They're forcing a vote on net neutrality. It's a last-ditch effort to keep Obama-era regulations on internet service providers in place. Here's Senator Ed Markey last week pushing for the vote. (SOUNDBITE OF ARCHIVED RECORDING)ED MARKEY: A free and open internet means an internet free from corporate control and open to anyone who wants to connect, communicate or innovate. MARTIN: NPR's Alina Selyukh is here in the studio to walk us through what's going on. Hey, Alina. ALINA SELYUKH, BYLINE: Hi. Good morning. MARTIN: Seems like every few months we talk about net neutrality. SELYUKH: It really does. MARTIN: So what exactly are lawmakers voting on today? SELYUKH: So I'll give you my short recap. Politically what's happening here, it's Democrats trying to stop the Trump administration from loosening regulations written by the Obama administration. Policywise, the policy issue at stake is, how strictly should the federal government regulate your internet provider? This is a question that the Federal Communications Commission has struggled with for more than a decade. MARTIN: Right. SELYUKH: For example, should your broadband provider be able to stream some video faster than other video? Or should your phone company be able to give you free data in exchange for using a very specific music app? The Obama FCC, three years ago, wrote rules that put internet providers under pretty serious oversight. They imposed the basic net neutrality rules like no blocking, no throttling. Now the Trump FCC has walked all that back. And they're saying, as long as companies are transparent, all the government should do is punish bad behavior after it happens. Senate Democrats don't want this repeal to go into effect, and that's what they're trying to stop with this vote today. MARTIN: Although Democrats don't have control of Congress - so what are the chances of this actually getting through? SELYUKH: It's a very unique situation in the Senate. They have the votes for this. All they need is a simple majority. With Senator John McCain being ill, all they need is just one Republican vote. And they have this vote from Maine Republican Senator Susan Collins. But then, to your point, to actually stop the repeal from going into effect, the House has to approve the same measure. And on the House side, there's zero plans to pick this up. MARTIN: All right. SELYUKH: And so in Congress, this push is looking pretty much dead after today. MARTIN: But they must know that. Right? SELYUKH: Yes. MARTIN: So Democrats in the Senate must see some value in holding the vote anyway. SELYUKH: One word - midterms. The Democrats are putting the stake in the ground as sort of the party that supports net neutrality. They're hoping this gets people voting on Election Day. And we are starting to see this big political move on net neutrality. People are putting together trackers showing how lawmakers are voting on this, doing polls that show just how excited people are - how passionate they feel about the internet, how much they worry that prices might go up - all these things. MARTIN: This is actually animating voters this year? SELYUKH: I've been covering net neutrality for almost six years now, and this is a fascinating move to me. We're definitely going from, like, conversations in the backrooms of think tanks to now, you know, campaign-type slogans, Twitter one-liners. You know, the government is meddling with your internet - or it's the end of the internet as we know it. Those have been around for a while. But now we are definitely in political territory, sort of further away from the specifics of the policy and into the politics of two divided parties offering completely conflicting visions of what will happen in June when the net neutrality rules go away. MARTIN: All right, NPR's Alina Selyukh breaking it down for us this morning. Alina thank you so much. SELYUKH: Thank you. RACHEL MARTIN, HOST:  Democrats are staging an insurgency of sorts today in the Senate. They're forcing a vote on net neutrality. It's a last-ditch effort to keep Obama-era regulations on internet service providers in place. Here's Senator Ed Markey last week pushing for the vote. (SOUNDBITE OF ARCHIVED RECORDING) ED MARKEY: A free and open internet means an internet free from corporate control and open to anyone who wants to connect, communicate or innovate. MARTIN: NPR's Alina Selyukh is here in the studio to walk us through what's going on. Hey, Alina. ALINA SELYUKH, BYLINE: Hi. Good morning. MARTIN: Seems like every few months we talk about net neutrality. SELYUKH: It really does. MARTIN: So what exactly are lawmakers voting on today? SELYUKH: So I'll give you my short recap. Politically what's happening here, it's Democrats trying to stop the Trump administration from loosening regulations written by the Obama administration. Policywise, the policy issue at stake is, how strictly should the federal government regulate your internet provider? This is a question that the Federal Communications Commission has struggled with for more than a decade. MARTIN: Right. SELYUKH: For example, should your broadband provider be able to stream some video faster than other video? Or should your phone company be able to give you free data in exchange for using a very specific music app? The Obama FCC, three years ago, wrote rules that put internet providers under pretty serious oversight. They imposed the basic net neutrality rules like no blocking, no throttling. Now the Trump FCC has walked all that back. And they're saying, as long as companies are transparent, all the government should do is punish bad behavior after it happens. Senate Democrats don't want this repeal to go into effect, and that's what they're trying to stop with this vote today. MARTIN: Although Democrats don't have control of Congress - so what are the chances of this actually getting through? SELYUKH: It's a very unique situation in the Senate. They have the votes for this. All they need is a simple majority. With Senator John McCain being ill, all they need is just one Republican vote. And they have this vote from Maine Republican Senator Susan Collins. But then, to your point, to actually stop the repeal from going into effect, the House has to approve the same measure. And on the House side, there's zero plans to pick this up. MARTIN: All right. SELYUKH: And so in Congress, this push is looking pretty much dead after today. MARTIN: But they must know that. Right? SELYUKH: Yes. MARTIN: So Democrats in the Senate must see some value in holding the vote anyway. SELYUKH: One word - midterms. The Democrats are putting the stake in the ground as sort of the party that supports net neutrality. They're hoping this gets people voting on Election Day. And we are starting to see this big political move on net neutrality. People are putting together trackers showing how lawmakers are voting on this, doing polls that show just how excited people are - how passionate they feel about the internet, how much they worry that prices might go up - all these things. MARTIN: This is actually animating voters this year? SELYUKH: I've been covering net neutrality for almost six years now, and this is a fascinating move to me. We're definitely going from, like, conversations in the backrooms of think tanks to now, you know, campaign-type slogans, Twitter one-liners. You know, the government is meddling with your internet - or it's the end of the internet as we know it. Those have been around for a while. But now we are definitely in political territory, sort of further away from the specifics of the policy and into the politics of two divided parties offering completely conflicting visions of what will happen in June when the net neutrality rules go away. MARTIN: All right, NPR's Alina Selyukh breaking it down for us this morning. Alina thank you so much. SELYUKH: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-17-611869599": {"title": "Tennessee Hack Shows Election Websites Are Vulnerable : NPR", "url": "https://www.npr.org/2018/05/17/611869599/not-just-ballots-tennessee-hack-shows-election-websites-are-vulnerable-too", "author": "No author found", "published_date": "2018-05-17", "content": "RACHEL MARTIN, HOST: House Speaker Paul Ryan has called every member of the House of Representatives to an important meeting next week on election security. Top intelligence and law enforcement officials will brief Congress on the risks and threats to this year's midterm elections. Local election websites are particularly under threat, and one of them was breached this month on election day in Tennessee. NPR's Miles Parks has more. MILES PARKS, BYLINE: Officials in Knox County, Tenn. , knew their county primary would get more attention than normal. Here's Chris Davis, the county's assistant administrator of elections. CHRIS DAVIS: One of the candidates in this election who actually ended up winning the Republican primary for county mayor was a gentleman by the name of Glenn Jacobs, who - I'm not sure if NPR listeners are pro wrestling aficionados. Maybe, maybe not, but - was a pro wrestler by the name of Kane. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: Get down. Get down. UNIDENTIFIED PERSON #2: It's Kane. Kane is back. (CHEERING)UNIDENTIFIED PERSON #2: The Big Red Machine is here. PARKS: But right as election results started coming in, an incident even scarier than Cain's tombstone piledriver took officials and voters by surprise. DAVE BALL: Around 8 o'clock, we suddenly started experiencing a very high load of traffic. PARKS: That's Dave Ball, the deputy IT director for Knox County. BALL: Shortly thereafter, the Web server hit as much capacity as it could take, and it went down. PARKS: The website, which was supposed to be showing election results, stayed down for about an hour. The server wasn't connected to the system that records or counts votes, but it's the way many voters learn about the outcome. The Senate Intelligence Committee says six states had their election websites attacked by Russian operatives in 2016. Doug Jones is an election cybersecurity expert at the University of Iowa. He says local election websites are more vulnerable to attacks than, say, a voting machine because the sites are connected to the Internet. DOUG JONES: The diversity, the huge number of counties, means it's expensive to do a broad attack because there are so many of them. But it also means it's really unlikely that there isn't some vulnerable county out there. And the first thing an attacker would do would be to start probing all the county election offices and finding the ones that are weak. PARKS: Knox County took care of the vulnerability that the attackers exploited, and, again, the website was only down for an hour. No votes were in any way affected. But Chris Davis says it's just as much about a voter's confidence in the entire election system. DAVIS: It's from a public perception standpoint as much as anything. We want to make sure that all of this data is secure, and if somebody logs on to our website to look at something that they can trust that that information is correct. PARKS: He said county officials will be extra alert heading into this year's midterm elections. Tennessee may have a competitive Senate race this year. DAVIS: Something happens in Nebraska, it's like, OK. That's one thing. But if it happens in your backyard, it's, well, this can happen. Well, you know, if this can happen in little old Knox County, Tenn. , it can happen anywhere. PARKS: On August 2, voters in Knox County will go to the polls again to decide whether Glenn Jacobs, Kane, will be their next mayor. Miles Parks, NPR News, Washington. RACHEL MARTIN, HOST:  House Speaker Paul Ryan has called every member of the House of Representatives to an important meeting next week on election security. Top intelligence and law enforcement officials will brief Congress on the risks and threats to this year's midterm elections. Local election websites are particularly under threat, and one of them was breached this month on election day in Tennessee. NPR's Miles Parks has more. MILES PARKS, BYLINE: Officials in Knox County, Tenn. , knew their county primary would get more attention than normal. Here's Chris Davis, the county's assistant administrator of elections. CHRIS DAVIS: One of the candidates in this election who actually ended up winning the Republican primary for county mayor was a gentleman by the name of Glenn Jacobs, who - I'm not sure if NPR listeners are pro wrestling aficionados. Maybe, maybe not, but - was a pro wrestler by the name of Kane. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: Get down. Get down. UNIDENTIFIED PERSON #2: It's Kane. Kane is back. (CHEERING) UNIDENTIFIED PERSON #2: The Big Red Machine is here. PARKS: But right as election results started coming in, an incident even scarier than Cain's tombstone piledriver took officials and voters by surprise. DAVE BALL: Around 8 o'clock, we suddenly started experiencing a very high load of traffic. PARKS: That's Dave Ball, the deputy IT director for Knox County. BALL: Shortly thereafter, the Web server hit as much capacity as it could take, and it went down. PARKS: The website, which was supposed to be showing election results, stayed down for about an hour. The server wasn't connected to the system that records or counts votes, but it's the way many voters learn about the outcome. The Senate Intelligence Committee says six states had their election websites attacked by Russian operatives in 2016. Doug Jones is an election cybersecurity expert at the University of Iowa. He says local election websites are more vulnerable to attacks than, say, a voting machine because the sites are connected to the Internet. DOUG JONES: The diversity, the huge number of counties, means it's expensive to do a broad attack because there are so many of them. But it also means it's really unlikely that there isn't some vulnerable county out there. And the first thing an attacker would do would be to start probing all the county election offices and finding the ones that are weak. PARKS: Knox County took care of the vulnerability that the attackers exploited, and, again, the website was only down for an hour. No votes were in any way affected. But Chris Davis says it's just as much about a voter's confidence in the entire election system. DAVIS: It's from a public perception standpoint as much as anything. We want to make sure that all of this data is secure, and if somebody logs on to our website to look at something that they can trust that that information is correct. PARKS: He said county officials will be extra alert heading into this year's midterm elections. Tennessee may have a competitive Senate race this year. DAVIS: Something happens in Nebraska, it's like, OK. That's one thing. But if it happens in your backyard, it's, well, this can happen. Well, you know, if this can happen in little old Knox County, Tenn. , it can happen anywhere. PARKS: On August 2, voters in Knox County will go to the polls again to decide whether Glenn Jacobs, Kane, will be their next mayor. Miles Parks, NPR News, Washington.", "section": "Politics", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-22-613115969": {"title": "Orlando Police Testing Amazon's Real-Time Facial Recognition : NPR", "url": "https://www.npr.org/2018/05/22/613115969/orlando-police-testing-amazons-real-time-facial-recognition", "author": "No author found", "published_date": "2018-05-22", "content": "AUDIE CORNISH, HOST: An update now on a story we brought you a couple of weeks ago about facial recognition technology. NPR's Martin Kaste reported that American police have been reluctant to adopt, quote, \"real-time systems,\" the kind that ID people as they walk down the street. Now Martin says it appears at least one police department is trying that out with the help of a very familiar tech company. MARTIN KASTE, BYLINE: Believe it or not, that company is Amazon. It sells a video analytics tool called Rekognition. It's cloud-based. You upload video along with the faces that you want it to scan for. Anybody can use it, but recently the company started pitching it to local governments. (SOUNDBITE OF ARCHIVED RECORDING)RANJU DAS: City of Orlando is a launch partner of ours. It's a smart city. They have cameras all over the city. KASTE: This is Amazon employee Ranju Das talking up the product's public safety uses at a recent conference. (SOUNDBITE OF ARCHIVED RECORDING)DAS: We analyze that video in real time, search against the collection of faces that they have. These could be their mayor if they want to know if the mayor of the city is in a place, they want to know if - persons of interest they want to track. KASTE: That sounds like the real-time systems that police in China have been bragging about recently - networks of cameras that let them track where someone is or has been. American police have not had this ability yet as far as it's been documented, and the privacy laws here are not clear about whether the police would need a warrant to do this kind of scanning. In a written statement, Orlando Police called this a pilot program but would not take follow-up questions. Later, after the web version of this story was published, they sent another statement saying real-time facial recognition have been tested on only eight cameras in the city and did not involve the identification of members of the general public. The department has not explained its future plans for the system. MATT CAGLE: We're talking about a new technology that supercharges surveillance. KASTE: Matt Cagle is with the ACLU, which first noticed Amazon's push to sell facial recognition to police. It's now asking Amazon to stop doing so. While there are other companies that sell facial recognition to law enforcement, he says Amazon is different. CAGLE: Amazon is one of the largest technology companies and one of the largest cloud computing providers. So we find it really difficult to overstate Amazon's ability to influence whether facial recognition becomes widespread in the United States or not. KASTE: In a written statement, Amazon says it, quote, \"requires that customers comply with the law and be responsible when using the service. \" The company also played down just how real time the facial recognition is in Orlando. But for the ACLU, the concern isn't so much about what these systems can do now. It's about what they could grow into. The cloud-based nature of Amazon's service makes it especially easy to expand and adapt facial recognition. Deputy Jeff Talbot is a spokesman for the Washington County Sheriff's Office near Portland, Ore. Deputies there use Amazon to compare photos taken in the field to a database of mug shots. JEFF TALBOT: They've created a little tool where you upload. It sends you your results, and you can continue on with your investigation. KASTE: And this service is astonishingly cheap. TALBOT: We pay literally a couple of dollars a month. KASTE: That kind of automatic mug shot comparison used to require systems that cost tens of thousands of dollars. Facial recognition is now becoming just another digital commodity available to even the most cash-strapped police departments despite the fact that there has been very little public debate about what the rules should be for how police use it. Martin Kaste, NPR News. AUDIE CORNISH, HOST:  An update now on a story we brought you a couple of weeks ago about facial recognition technology. NPR's Martin Kaste reported that American police have been reluctant to adopt, quote, \"real-time systems,\" the kind that ID people as they walk down the street. Now Martin says it appears at least one police department is trying that out with the help of a very familiar tech company. MARTIN KASTE, BYLINE: Believe it or not, that company is Amazon. It sells a video analytics tool called Rekognition. It's cloud-based. You upload video along with the faces that you want it to scan for. Anybody can use it, but recently the company started pitching it to local governments. (SOUNDBITE OF ARCHIVED RECORDING) RANJU DAS: City of Orlando is a launch partner of ours. It's a smart city. They have cameras all over the city. KASTE: This is Amazon employee Ranju Das talking up the product's public safety uses at a recent conference. (SOUNDBITE OF ARCHIVED RECORDING) DAS: We analyze that video in real time, search against the collection of faces that they have. These could be their mayor if they want to know if the mayor of the city is in a place, they want to know if - persons of interest they want to track. KASTE: That sounds like the real-time systems that police in China have been bragging about recently - networks of cameras that let them track where someone is or has been. American police have not had this ability yet as far as it's been documented, and the privacy laws here are not clear about whether the police would need a warrant to do this kind of scanning. In a written statement, Orlando Police called this a pilot program but would not take follow-up questions. Later, after the web version of this story was published, they sent another statement saying real-time facial recognition have been tested on only eight cameras in the city and did not involve the identification of members of the general public. The department has not explained its future plans for the system. MATT CAGLE: We're talking about a new technology that supercharges surveillance. KASTE: Matt Cagle is with the ACLU, which first noticed Amazon's push to sell facial recognition to police. It's now asking Amazon to stop doing so. While there are other companies that sell facial recognition to law enforcement, he says Amazon is different. CAGLE: Amazon is one of the largest technology companies and one of the largest cloud computing providers. So we find it really difficult to overstate Amazon's ability to influence whether facial recognition becomes widespread in the United States or not. KASTE: In a written statement, Amazon says it, quote, \"requires that customers comply with the law and be responsible when using the service. \" The company also played down just how real time the facial recognition is in Orlando. But for the ACLU, the concern isn't so much about what these systems can do now. It's about what they could grow into. The cloud-based nature of Amazon's service makes it especially easy to expand and adapt facial recognition. Deputy Jeff Talbot is a spokesman for the Washington County Sheriff's Office near Portland, Ore. Deputies there use Amazon to compare photos taken in the field to a database of mug shots. JEFF TALBOT: They've created a little tool where you upload. It sends you your results, and you can continue on with your investigation. KASTE: And this service is astonishingly cheap. TALBOT: We pay literally a couple of dollars a month. KASTE: That kind of automatic mug shot comparison used to require systems that cost tens of thousands of dollars. Facial recognition is now becoming just another digital commodity available to even the most cash-strapped police departments despite the fact that there has been very little public debate about what the rules should be for how police use it. Martin Kaste, NPR News.", "section": "Criminal Justice Collaborative", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-24-613270172": {"title": "Facebook Project Wants You To 'Buy Nothing' And Ask For What You Need : NPR", "url": "https://www.npr.org/2018/05/24/613270172/facebook-project-wants-you-to-buy-nothing-and-ask-for-what-you-need", "author": "No author found", "published_date": "2018-05-24", "content": "AUDIE CORNISH, HOST: Facebook makes money through advertising - ads that are trying to get you to buy things. But it's also spawned hundreds of groups where the goal is to give things away. The Buy Nothing Project encourages people to share what they have with others all without money changing hands. NPR's Jeff Brady reports that in five years, the project has spread around the world with the help of more than 3,000 volunteers. JEFF BRADY, BYLINE: At a townhouse in Philadelphia, Johanna Humphrey is headed upstairs to show me something. JOHANNA HUMPHREY: Amazon sent me a ton of crayons by mistake. BRADY: She ordered 24 boxes for her 3-year-old son's birthday party, but the company sent her twice that many and doesn't want the extras back. HUMPHREY: Parents don't need this many crayons in their house. BRADY: So she's going to list them on her neighborhood Buy Nothing group on Facebook and offer them to a teacher. Humphrey actually runs the group. She started it a few years ago as a volunteer. HUMPHREY: I really primarily do it for the environmental reasons, right? If there's a way that I can stop some of my stuff from ending up in a trash heap somewhere, I feel like I've done my job. BRADY: Humphrey's group has grown to nearly 2,000 members. That many people can get unwieldy, so soon it will split into two groups. That's one way the Buy Nothing Project has grown over the last five years - from one group outside Seattle to more than 2,000 around the world. The project has no paid staff. LIESL CLARK: We don't want to monetize this in any way possible even as a nonprofit. BRADY: Liesl Clark is a filmmaker and co-founder of the Buy Nothing Project. CLARK: I wanted to try a social experiment which was to get people to literally ask themselves whether they can buy nothing. And I don't care if it's just for an hour or for a day or for a week. BRADY: Clark says she wants people to give things away but also ask for what they need instead of going to the store. She believes relying on and helping neighbors creates healthier communities and happier people. The project does have a subversive element. A frequent question is why the project uses Facebook. Clark says that's where the people are and, quote, \"we need to use the master's tools to dismantle the castle. \"CLARK: And this next step is going to be backing off and trying to render ourselves obsolete by moving off of Facebook and encouraging people to share in person and do less so on their devices. BRADY: That does not seem to be happening yet. For now, the Buy Nothing Project is still growing quickly on Facebook. Back at Johanna Humphrey's townhouse in Philadelphia, she's selected a teacher from the nearly 100 people who expressed interest in the crayons. HUMPHREY: She posted a picture of her classroom. She said, I'm a kindergarten teacher, and we would love, love, love to have these. And so just seeing a picture of a ton of little kids' faces - that was what sold it for me. BRADY: The next day, the two meet in person for the crayon hand-off. HUMPHREY: Hi. LAURA SMITH: Hello. HUMPHREY: How are you? SMITH: Good. How are you? HUMPHREY: Johanna - nice to meet you. SMITH: Nice to meet you - Laura. HUMPHREY: How are you? Very good to meet you. BRADY: Laura Smith teaches at a nearby charter school. SMITH: And I was so excited to get these crayons because I have 23 students who literally cry over a broken crayon (laughing). So. . . BRADY: The two women chat for a few minutes, and Smith is off. SMITH: Thank you so much. HUMPHREY: You are very welcome. SMITH: Thank you. BRADY: A few days later, a cute video clip arrives in Humphrey's email showing the students in class with their crayon. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED CHILDREN: Thank you. BRADY: This is what the Buy Nothing Project founders say they want to create - new connections by sharing things and interactions that extend into the future. Jeff Brady, NPR News, Philadelphia. AUDIE CORNISH, HOST:  Facebook makes money through advertising - ads that are trying to get you to buy things. But it's also spawned hundreds of groups where the goal is to give things away. The Buy Nothing Project encourages people to share what they have with others all without money changing hands. NPR's Jeff Brady reports that in five years, the project has spread around the world with the help of more than 3,000 volunteers. JEFF BRADY, BYLINE: At a townhouse in Philadelphia, Johanna Humphrey is headed upstairs to show me something. JOHANNA HUMPHREY: Amazon sent me a ton of crayons by mistake. BRADY: She ordered 24 boxes for her 3-year-old son's birthday party, but the company sent her twice that many and doesn't want the extras back. HUMPHREY: Parents don't need this many crayons in their house. BRADY: So she's going to list them on her neighborhood Buy Nothing group on Facebook and offer them to a teacher. Humphrey actually runs the group. She started it a few years ago as a volunteer. HUMPHREY: I really primarily do it for the environmental reasons, right? If there's a way that I can stop some of my stuff from ending up in a trash heap somewhere, I feel like I've done my job. BRADY: Humphrey's group has grown to nearly 2,000 members. That many people can get unwieldy, so soon it will split into two groups. That's one way the Buy Nothing Project has grown over the last five years - from one group outside Seattle to more than 2,000 around the world. The project has no paid staff. LIESL CLARK: We don't want to monetize this in any way possible even as a nonprofit. BRADY: Liesl Clark is a filmmaker and co-founder of the Buy Nothing Project. CLARK: I wanted to try a social experiment which was to get people to literally ask themselves whether they can buy nothing. And I don't care if it's just for an hour or for a day or for a week. BRADY: Clark says she wants people to give things away but also ask for what they need instead of going to the store. She believes relying on and helping neighbors creates healthier communities and happier people. The project does have a subversive element. A frequent question is why the project uses Facebook. Clark says that's where the people are and, quote, \"we need to use the master's tools to dismantle the castle. \" CLARK: And this next step is going to be backing off and trying to render ourselves obsolete by moving off of Facebook and encouraging people to share in person and do less so on their devices. BRADY: That does not seem to be happening yet. For now, the Buy Nothing Project is still growing quickly on Facebook. Back at Johanna Humphrey's townhouse in Philadelphia, she's selected a teacher from the nearly 100 people who expressed interest in the crayons. HUMPHREY: She posted a picture of her classroom. She said, I'm a kindergarten teacher, and we would love, love, love to have these. And so just seeing a picture of a ton of little kids' faces - that was what sold it for me. BRADY: The next day, the two meet in person for the crayon hand-off. HUMPHREY: Hi. LAURA SMITH: Hello. HUMPHREY: How are you? SMITH: Good. How are you? HUMPHREY: Johanna - nice to meet you. SMITH: Nice to meet you - Laura. HUMPHREY: How are you? Very good to meet you. BRADY: Laura Smith teaches at a nearby charter school. SMITH: And I was so excited to get these crayons because I have 23 students who literally cry over a broken crayon (laughing). So. . . BRADY: The two women chat for a few minutes, and Smith is off. SMITH: Thank you so much. HUMPHREY: You are very welcome. SMITH: Thank you. BRADY: A few days later, a cute video clip arrives in Humphrey's email showing the students in class with their crayon. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED CHILDREN: Thank you. BRADY: This is what the Buy Nothing Project founders say they want to create - new connections by sharing things and interactions that extend into the future. Jeff Brady, NPR News, Philadelphia.", "section": "Environment And Energy Collaborative", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-24-613960551": {"title": "Sweeping Internet Privacy Protection Regulations To Take Effect : NPR", "url": "https://www.npr.org/2018/05/24/613960551/sweeping-internet-privacy-protection-regulations-to-take-effect", "author": "No author found", "published_date": "2018-05-24", "content": "DAVID GREENE, HOST: The United States takes credit for inventing the Internet. The EU seems determined to govern it. This Friday, a sweeping new directive goes into effect. It's called GDPR, the General Data Protection Regulation. And here to explain to us what this mouthful means for the European Union and also for the United States, we have NPR's Aarti Shahani. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. GREENE: I love acronyms. It's great for radio. GDPR - what exactly is it? SHAHANI: Rolls off the tongue, right? GREENE: Yeah, totally. SHAHANI: (Laughter) So it is a directive that protects residents of the European Union, OK, people living there, even Americans living there. But if you're in the U. S. , even if you're European, you're not protected. U. S. -based companies, all companies, like Google, Facebook and Microsoft, they have to comply for any and all businesses that - or business they're conducting over there. At the most basic level, GDPR expands what counts as personal data and your rights over that data. So your data is, for example, what you post on social media, your electronic medical records, your mailing address. It's also your IP address. That string of number is unique to your smartphone or tablet. It also allows GPS location. GREENE: It's like everything it sounds like. SHAHANI: That's right. It's more comprehensive. And the directive says people have to give permission for a company to collect your data. A company can't just sign you up without explicitly asking, so, hey, can I store your phone number or collect your web browsing history? It has to be asked. And if it's more personal, say it's your biometrics, the ask has to be even more clear, red alert language. Now, as a European, if you don't want a company keeping your data, you have a right to deletion. They have to delete the data without any undue delay or face a penalty. GREENE: OK. I just want to underscore something. This means nothing if we are here in the United States. This doesn't apply to us at all or it might somehow. SHAHANI: It's not legally binding in the U. S. So, you know, if you're an American, you're probably getting a lot of emails and push notifications from your apps, you know, maybe even some newsletters you forgot you signed up for. That's what I'm getting. GREENE: Yeah. SHAHANI: You know, I had a lawyer take a look at the legalese that I got from Spotify and eBay. And what he did is he pointed out they're saying that me - we here in the U. S. - can request to delete our data, but it's just a request. There is nothing binding about it, OK? Here, we've only got three laws that protect medical and financial records and kids. But other than that, we're pretty much the Wild West. We are unregulated. That's how that scandal happened where 87 million Facebook users had their profiles land in the hands of a political operative - unregulated marketplace. You know, and last month, in testimony before Congress, Mark Zuckerberg, CEO of Facebook, said he'd give Americans all the same controls Europeans have. Let's have a quick listen to his words. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: We believe that everyone around the world deserves good privacy controls. We've had a lot of these controls in place for years. The GDPR requires us to do a few more things, and we're going to extend that to the world. SHAHANI: And so actually on Facebook there will be a big difference between Europe and the U. S. when it comes to what's collected by default. In Europe, Facebook has to get permission to do facial recognition. It's not on by default, but in the U. S. , it is. So American users have to click through screens to opt out. GREENE: OK. So, Aarti, I mean, explain one of the - one of the arguments against these laws is that the sky is going to fall down. It's going to really hurt businesses like Facebook because they rely so heavily on collecting information automatically. Is that a legitimate argument? SHAHANI: So that is the key debate, and one side is arguing GDPR will be terrible for competition and small startups won't be able to afford all the big expenses that come with managing and protecting data so just the big companies will be able to survive. Another camp says, no, consumers don't trust the Internet right now. That's a problem, and it will be good for us. GREENE: All right. NPR's Aarti Shahani. Aarti, thanks a lot. SHAHANI: Thank you. DAVID GREENE, HOST:  The United States takes credit for inventing the Internet. The EU seems determined to govern it. This Friday, a sweeping new directive goes into effect. It's called GDPR, the General Data Protection Regulation. And here to explain to us what this mouthful means for the European Union and also for the United States, we have NPR's Aarti Shahani. Hey, Aarti. AARTI SHAHANI, BYLINE: Hi. GREENE: I love acronyms. It's great for radio. GDPR - what exactly is it? SHAHANI: Rolls off the tongue, right? GREENE: Yeah, totally. SHAHANI: (Laughter) So it is a directive that protects residents of the European Union, OK, people living there, even Americans living there. But if you're in the U. S. , even if you're European, you're not protected. U. S. -based companies, all companies, like Google, Facebook and Microsoft, they have to comply for any and all businesses that - or business they're conducting over there. At the most basic level, GDPR expands what counts as personal data and your rights over that data. So your data is, for example, what you post on social media, your electronic medical records, your mailing address. It's also your IP address. That string of number is unique to your smartphone or tablet. It also allows GPS location. GREENE: It's like everything it sounds like. SHAHANI: That's right. It's more comprehensive. And the directive says people have to give permission for a company to collect your data. A company can't just sign you up without explicitly asking, so, hey, can I store your phone number or collect your web browsing history? It has to be asked. And if it's more personal, say it's your biometrics, the ask has to be even more clear, red alert language. Now, as a European, if you don't want a company keeping your data, you have a right to deletion. They have to delete the data without any undue delay or face a penalty. GREENE: OK. I just want to underscore something. This means nothing if we are here in the United States. This doesn't apply to us at all or it might somehow. SHAHANI: It's not legally binding in the U. S. So, you know, if you're an American, you're probably getting a lot of emails and push notifications from your apps, you know, maybe even some newsletters you forgot you signed up for. That's what I'm getting. GREENE: Yeah. SHAHANI: You know, I had a lawyer take a look at the legalese that I got from Spotify and eBay. And what he did is he pointed out they're saying that me - we here in the U. S. - can request to delete our data, but it's just a request. There is nothing binding about it, OK? Here, we've only got three laws that protect medical and financial records and kids. But other than that, we're pretty much the Wild West. We are unregulated. That's how that scandal happened where 87 million Facebook users had their profiles land in the hands of a political operative - unregulated marketplace. You know, and last month, in testimony before Congress, Mark Zuckerberg, CEO of Facebook, said he'd give Americans all the same controls Europeans have. Let's have a quick listen to his words. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: We believe that everyone around the world deserves good privacy controls. We've had a lot of these controls in place for years. The GDPR requires us to do a few more things, and we're going to extend that to the world. SHAHANI: And so actually on Facebook there will be a big difference between Europe and the U. S. when it comes to what's collected by default. In Europe, Facebook has to get permission to do facial recognition. It's not on by default, but in the U. S. , it is. So American users have to click through screens to opt out. GREENE: OK. So, Aarti, I mean, explain one of the - one of the arguments against these laws is that the sky is going to fall down. It's going to really hurt businesses like Facebook because they rely so heavily on collecting information automatically. Is that a legitimate argument? SHAHANI: So that is the key debate, and one side is arguing GDPR will be terrible for competition and small startups won't be able to afford all the big expenses that come with managing and protecting data so just the big companies will be able to survive. Another camp says, no, consumers don't trust the Internet right now. That's a problem, and it will be good for us. GREENE: All right. NPR's Aarti Shahani. Aarti, thanks a lot. SHAHANI: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-25-614075065": {"title": "Tristan Harris: What Is The Cost Of Infinite Distractions? : NPR", "url": "https://www.npr.org/2018/05/25/614075065/tristan-harris-what-is-the-cost-of-infinite-distractions", "author": "No author found", "published_date": "2018-05-25", "content": "(SOUNDBITE OF MUSIC)GUY RAZ, HOST: Is it strange to think of attention as a commodity? TRISTAN HARRIS: You know, it's interesting to think of it as a commodity, given how important it is. I mean, attention is just consciousness. It's experience. And to think of it as a commodity is itself so dehumanizing. It's like taking the - you know, whatever it is that's unexplainable about this experience, and saying, yeah, let's sell that. Let's sell the core thing to our human experience. RAZ: This is Tristan Harris. He runs an organization called the Center for Humane Technology that's trying to free us from our addiction to tech. But before that, Tristan actually used to work in the tech industry at Google, creating the very things that try to compete for our attention. HARRIS: People have been competing for attention for a long time. But, you know, I think we have a very new environment for that competition that's unprecedented. We're not built for this. And I think we have to start by looking in the mirror at how human instincts really work. Right? We have this, actually, this image on our website of, you know, the ape turning into the Neanderthal turning into the Homo sapiens, the man. And then the next thing is this human who's sort of turning around, like, looking in the mirror and seeing what we are, where all those instincts come from and how they're configured. Because we're not going to get out of the mind-body meat suit that's configured and calibrated the way that it was for, you know, thousands of years ago on the savannah. RAZ: No. HARRIS: So we're either going to build environments that are humane, that work with that architecture, or we're going to build environments that do not respect that architecture. And I think that it's really possible to do that, but it really just requires really a science of ourselves, like, looking in the mirror and saying, how do we make stuff work with us? (SOUNDBITE OF MUSIC)RAZ: Here's more from Tristan Harris on the TED stage. (SOUNDBITE OF TED TALK)HARRIS: What do you think makes more money in the United States than movies, game parks and baseball combined? Slot machines. How can slot machines make all this money when we play with such small amounts of money? We play with coins. How is this possible? Well, the thing is, my phone is a slot machine. Every time I check my phone, I'm playing the slot machine to see, what am I going to get? Every time I check my email, I'm playing the slot machine to see, what what am I going to get? Every time I scroll a news feed, I'm playing the slot machine to see, what am I going to get next, right? And the thing is that, again, knowing exactly how this works - and I'm a designer. I know exactly how the psychology of this works. I know exactly what's going on. But it doesn't leave me with any choice. I still just get sucked into it. So what are we going to do? Because it leaves us with this all-or-nothing relationship with technology, right? You're either on and you're connected and distracted all the time, or you're off but then you're wondering, am I missing something important? In other words, you're either distracted or you have a fear of missing out. Right? (SOUNDBITE OF MUSIC)RAZ: In just a moment, how we can break free from our addiction to technology and take back the right to choose. Stay with us. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC)RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today - ideas about the consequences of living in an age of constant information and infinite distractions. And just before the break, we were hearing from Tristan Harris. After working at Google, Tristan went on to found an organization that's trying to rethink and improve our relationship with technology because right now, he says, we don't have much of a choice. HARRIS: I think we have meta choices. I mean, I think - what I mean by that is if you knew that there is some other environment that's really distracting, like a casino, and it's all the way in Vegas and you have to get in a car and drive 60 miles to get there or something like that, you could say, OK, distraction's a choice. I just won't go to Vegas. It's another thing when you wake up in the morning and you turn off your alarm and right next to your alarm clock in your phone when you undo the app switcher are, like, a hundred different, invisible, crazy things that all want your attention - all casinos. And so one of the problems is we don't have the ability to separate the casinos and the crazy distractions from the things that are not the distractions. Like, when you do that alarm clock, you - what's different about that, right? I mean, you chose for that thing to interrupt your attention. You tied your hands behind your back and said, I want you to interrupt my entire attentional cycle at 6:30 in the morning tomorrow. But then all these other things you think you chose because you downloaded that app and you think you hit allow notifications, but did you really allow for all of those supercomputers and engineers on the other side of the screen to try and, you know, figure out what's going to keep you coming back? You know, that's not what we signed up for. It's not quite the same thing. (SOUNDBITE OF TED TALK)HARRIS: And that's what we're doing all the time. We're bulldozing each other's attention left and right. And there's serious cost to this because every time we interrupt each other, it takes us about 23 minutes on average to refocus our attention. We actually cycle through two different projects before we come back to the original thing we were doing. This is Gloria Mark's research combined with Microsoft research that showed this. And her research also shows that it actually trains bad habits. The more interruptions we get externally, it's conditioning and training us to interrupt ourselves. We actually self-interrupt about every 3 1/2 minutes. (SOUNDBITE OF MUSIC)HARRIS: And just imagine that, you know, that study I think was done, you know, in a very specific context. It was done in a workplace setting. I think it was done at NASA. And, you know, that was done, I think, in 2008 - 10 years ago. And so imagine how much more often we hear our buzz, dings, new emails, you know, pouncing at us from a thousand different directions that are completely unrelated. I think something that's really undervalued is the cost of unrelated, different things coming at you all the time so your mind has to switch and switch and switch. (SOUNDBITE OF MUSIC)RAZ: So how do we solve this? How do we even begin to fix this? HARRIS: Well, we've been advocating for five years for a different kind of design, our humane design, that pays attention to the way that the human mind and the human instincts really work. And you design to accommodate them. RAZ: Yeah, so the idea is through, I mean, design. The solution is in design. HARRIS: Design is one part of it. But it's an important part. I mean, a simple example is - a lot of people have been doing this recently - you turn your phone to grayscale. Like, why would you do that? RAZ: Yeah, yeah. HARRIS: Like, why does that do anything? Like, well, that's a crazy idea. RAZ: Yes. I've done that. I've done it. HARRIS: (Laughter) Well, in a way, it's an experiment because you get to see the difference between what's the difference when I look at my phone and all those colors light up my brain? I didn't choose for those colors to light up my brain. It just did that. RAZ: Yeah. HARRIS: When it's gray, it has a different effect on me, and I didn't choose that either. It just - I feel a little bit calmer. It feels a little bit less appealing. Imagine if all home screens were gray. It would reduce some of that addictiveness just probably about 15 percent. But that's one tiny, little example. I mean, another example is interruptions. How often should you get interrupted? Should we - you know, if you had to choose, if you had a lever for two billion people - this kind of what I was thinking about is Google. It's like you have this two billion person ant colony called humanity. And then you've got this phone in their pocket that's going to steer their attention. Now, if you're the operator, you're the one in the control room and you get to choose, do two billion people get interrupted every, you know, on a random schedule, every one to 15 minutes with little buzzes? Or would you prefer to have a world where they get interrupted once a day at 5 p. m. , except if there's something extraordinarily important? Like you'd probably pick the latter if you're defining a default setting for two billion people. RAZ: What are the consequences if we do nothing, if nothing changes or it just continues to get worse? HARRIS: I think the consequences are incredibly serious because there's a temptation to believe - I remember I had this moment I was at a conference and I met some people who worked for a security - what are they called? - three-letter acronym security agencies. And I gave a talk about the manipulative design stuff, you know, the way the companies use these techniques and - or hooked them. Someone came up to me and said, yeah, totally, I'm so hooked by this stuff. And I said, oh, no. Like, I always thought there was this other special group of people who work on climate change or some special class of people who work on, you know, national security or some special class of people who work on inequality, and they're immune to all this stuff, and they're making really good decisions to get us out of all these messes that we're in as a global system. And then what strikes me and what terrifies me is this is the fabric through which all human thinking and choices happen is your mind. And if that capacity for thinking and choosing is corrupted, that's everything that disables our ability to do everything - climate change, solving inequality, agreeing on what the truth is, our political process, elections. Everything comes down to the fabric of our mind. What are we thinking and believing? And when you turn your phone over in the morning and you see a bunch of stuff, thoughts start streaming into our heads that we didn't choose, and that is the environment that we have immersed 2 billion human animals. And that's why I look at this and say, we need to change course right now. You have to see the structure of this, that this is too much of a threat, and we can't survive it if we don't. RAZ: Tristan Harris - he's the co-founder of the Center for Humane Technology. You can see all of his talks at ted. com. (SOUNDBITE OF MUSIC) GUY RAZ, HOST:  Is it strange to think of attention as a commodity? TRISTAN HARRIS: You know, it's interesting to think of it as a commodity, given how important it is. I mean, attention is just consciousness. It's experience. And to think of it as a commodity is itself so dehumanizing. It's like taking the - you know, whatever it is that's unexplainable about this experience, and saying, yeah, let's sell that. Let's sell the core thing to our human experience. RAZ: This is Tristan Harris. He runs an organization called the Center for Humane Technology that's trying to free us from our addiction to tech. But before that, Tristan actually used to work in the tech industry at Google, creating the very things that try to compete for our attention. HARRIS: People have been competing for attention for a long time. But, you know, I think we have a very new environment for that competition that's unprecedented. We're not built for this. And I think we have to start by looking in the mirror at how human instincts really work. Right? We have this, actually, this image on our website of, you know, the ape turning into the Neanderthal turning into the Homo sapiens, the man. And then the next thing is this human who's sort of turning around, like, looking in the mirror and seeing what we are, where all those instincts come from and how they're configured. Because we're not going to get out of the mind-body meat suit that's configured and calibrated the way that it was for, you know, thousands of years ago on the savannah. RAZ: No. HARRIS: So we're either going to build environments that are humane, that work with that architecture, or we're going to build environments that do not respect that architecture. And I think that it's really possible to do that, but it really just requires really a science of ourselves, like, looking in the mirror and saying, how do we make stuff work with us? (SOUNDBITE OF MUSIC) RAZ: Here's more from Tristan Harris on the TED stage. (SOUNDBITE OF TED TALK) HARRIS: What do you think makes more money in the United States than movies, game parks and baseball combined? Slot machines. How can slot machines make all this money when we play with such small amounts of money? We play with coins. How is this possible? Well, the thing is, my phone is a slot machine. Every time I check my phone, I'm playing the slot machine to see, what am I going to get? Every time I check my email, I'm playing the slot machine to see, what what am I going to get? Every time I scroll a news feed, I'm playing the slot machine to see, what am I going to get next, right? And the thing is that, again, knowing exactly how this works - and I'm a designer. I know exactly how the psychology of this works. I know exactly what's going on. But it doesn't leave me with any choice. I still just get sucked into it. So what are we going to do? Because it leaves us with this all-or-nothing relationship with technology, right? You're either on and you're connected and distracted all the time, or you're off but then you're wondering, am I missing something important? In other words, you're either distracted or you have a fear of missing out. Right? (SOUNDBITE OF MUSIC) RAZ: In just a moment, how we can break free from our addiction to technology and take back the right to choose. Stay with us. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC) RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today - ideas about the consequences of living in an age of constant information and infinite distractions. And just before the break, we were hearing from Tristan Harris. After working at Google, Tristan went on to found an organization that's trying to rethink and improve our relationship with technology because right now, he says, we don't have much of a choice. HARRIS: I think we have meta choices. I mean, I think - what I mean by that is if you knew that there is some other environment that's really distracting, like a casino, and it's all the way in Vegas and you have to get in a car and drive 60 miles to get there or something like that, you could say, OK, distraction's a choice. I just won't go to Vegas. It's another thing when you wake up in the morning and you turn off your alarm and right next to your alarm clock in your phone when you undo the app switcher are, like, a hundred different, invisible, crazy things that all want your attention - all casinos. And so one of the problems is we don't have the ability to separate the casinos and the crazy distractions from the things that are not the distractions. Like, when you do that alarm clock, you - what's different about that, right? I mean, you chose for that thing to interrupt your attention. You tied your hands behind your back and said, I want you to interrupt my entire attentional cycle at 6:30 in the morning tomorrow. But then all these other things you think you chose because you downloaded that app and you think you hit allow notifications, but did you really allow for all of those supercomputers and engineers on the other side of the screen to try and, you know, figure out what's going to keep you coming back? You know, that's not what we signed up for. It's not quite the same thing. (SOUNDBITE OF TED TALK) HARRIS: And that's what we're doing all the time. We're bulldozing each other's attention left and right. And there's serious cost to this because every time we interrupt each other, it takes us about 23 minutes on average to refocus our attention. We actually cycle through two different projects before we come back to the original thing we were doing. This is Gloria Mark's research combined with Microsoft research that showed this. And her research also shows that it actually trains bad habits. The more interruptions we get externally, it's conditioning and training us to interrupt ourselves. We actually self-interrupt about every 3 1/2 minutes. (SOUNDBITE OF MUSIC) HARRIS: And just imagine that, you know, that study I think was done, you know, in a very specific context. It was done in a workplace setting. I think it was done at NASA. And, you know, that was done, I think, in 2008 - 10 years ago. And so imagine how much more often we hear our buzz, dings, new emails, you know, pouncing at us from a thousand different directions that are completely unrelated. I think something that's really undervalued is the cost of unrelated, different things coming at you all the time so your mind has to switch and switch and switch. (SOUNDBITE OF MUSIC) RAZ: So how do we solve this? How do we even begin to fix this? HARRIS: Well, we've been advocating for five years for a different kind of design, our humane design, that pays attention to the way that the human mind and the human instincts really work. And you design to accommodate them. RAZ: Yeah, so the idea is through, I mean, design. The solution is in design. HARRIS: Design is one part of it. But it's an important part. I mean, a simple example is - a lot of people have been doing this recently - you turn your phone to grayscale. Like, why would you do that? RAZ: Yeah, yeah. HARRIS: Like, why does that do anything? Like, well, that's a crazy idea. RAZ: Yes. I've done that. I've done it. HARRIS: (Laughter) Well, in a way, it's an experiment because you get to see the difference between what's the difference when I look at my phone and all those colors light up my brain? I didn't choose for those colors to light up my brain. It just did that. RAZ: Yeah. HARRIS: When it's gray, it has a different effect on me, and I didn't choose that either. It just - I feel a little bit calmer. It feels a little bit less appealing. Imagine if all home screens were gray. It would reduce some of that addictiveness just probably about 15 percent. But that's one tiny, little example. I mean, another example is interruptions. How often should you get interrupted? Should we - you know, if you had to choose, if you had a lever for two billion people - this kind of what I was thinking about is Google. It's like you have this two billion person ant colony called humanity. And then you've got this phone in their pocket that's going to steer their attention. Now, if you're the operator, you're the one in the control room and you get to choose, do two billion people get interrupted every, you know, on a random schedule, every one to 15 minutes with little buzzes? Or would you prefer to have a world where they get interrupted once a day at 5 p. m. , except if there's something extraordinarily important? Like you'd probably pick the latter if you're defining a default setting for two billion people. RAZ: What are the consequences if we do nothing, if nothing changes or it just continues to get worse? HARRIS: I think the consequences are incredibly serious because there's a temptation to believe - I remember I had this moment I was at a conference and I met some people who worked for a security - what are they called? - three-letter acronym security agencies. And I gave a talk about the manipulative design stuff, you know, the way the companies use these techniques and - or hooked them. Someone came up to me and said, yeah, totally, I'm so hooked by this stuff. And I said, oh, no. Like, I always thought there was this other special group of people who work on climate change or some special class of people who work on, you know, national security or some special class of people who work on inequality, and they're immune to all this stuff, and they're making really good decisions to get us out of all these messes that we're in as a global system. And then what strikes me and what terrifies me is this is the fabric through which all human thinking and choices happen is your mind. And if that capacity for thinking and choosing is corrupted, that's everything that disables our ability to do everything - climate change, solving inequality, agreeing on what the truth is, our political process, elections. Everything comes down to the fabric of our mind. What are we thinking and believing? And when you turn your phone over in the morning and you see a bunch of stuff, thoughts start streaming into our heads that we didn't choose, and that is the environment that we have immersed 2 billion human animals. And that's why I look at this and say, we need to change course right now. You have to see the structure of this, that this is too much of a threat, and we can't survive it if we don't. RAZ: Tristan Harris - he's the co-founder of the Center for Humane Technology. You can see all of his talks at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-25-614063234": {"title": "Manoush Zomorodi: Has Constant Stimulation Replaced Boredom? : NPR", "url": "https://www.npr.org/2018/05/25/614063234/manoush-zomorodi-has-constant-stimulation-replaced-boredom", "author": "No author found", "published_date": "2018-05-25", "content": "GUY RAZ, HOST: Do you remember when you were a kid, like, in the summertime - and there was no school, and it was hot, and the days would go on forever, and it was so boring? MANOUSH ZOMORODI: So, so boring. RAZ: Do you remember that? ZOMORODI: Oh, yeah. I remember it. I had a babysitter actually who used to lock us out of the house (laughter) in the afternoons. . . RAZ: Wow. ZOMORODI: . . . So she could watch her soap opera. And I remember, like, walking - so we would play this game where we would walk around the house and try to find ways to get back into the house. And that just would go on for hours and hours. And then you'd find ourselves, like, sitting in, like, a rut by the side of the house just looking at the dirt. . . RAZ: Yeah. ZOMORODI: . . . And doing very little. (Laughter) I don't think I've ever told my mother that story, actually. RAZ: This is a Manoush Zomorodi. ZOMORODI: I'm Manoush Zomorodi. I am a podcast host. I am a tech journalist. And I am the author of the book \"Bored And Brilliant. \"RAZ: And Manoush argues that the feeling of being bored isn't actually something people experience anymore. ZOMORODI: I definitely don't think that my children have ever experienced that sensation (laughter). Their minds are constantly being stimulated by STEM camp or, you know, adventure camp and afterschool guitar lessons and, well, iPad games that are allegedly educational. There's always - (laughter) every moment is filled. RAZ: And it's not just kids. All of us - we're filling the time when we used to be bored, when our minds would just wander, with never-ending stimulation, stimulation that captures our attention. And that's whether we're waiting in line at the airport. . . (SOUNDBITE OF VARIOUS NOTIFICATION TONES)ZOMORODI: Oh, I check my to-do list. I text my husband. I read the headlines. RAZ: . . . Or taking a break from work. ZOMORODI: I go back and check my to-do list again. I answer an email. I maybe play a quick game. (SOUNDBITE OF GAME, \"CANDY CRUSH SODA SAGA\")UNIDENTIFIED VOICE ACTOR: (As character) Sodalicious (ph). RAZ: It's a relatively recent phenomenon that we'll do almost anything to keep from being bored. ZOMORODI: Like, you know, how many kids were told, only boring people get bored. . . RAZ: Yeah. ZOMORODI: . . . As though this was something to be avoided? Boredom - oh, my God - avoid that at all costs. RAZ: But by constantly preventing ourselves from becoming bored, we actually might be missing out on something bigger. Manoush Zomorodi explains from the TED stage. (SOUNDBITE OF TED TALK)ZOMORODI: I started talking to neuroscientists and cognitive psychologists. And what they told me was fascinating. It turns out that when you get bored, you ignite a network in your brain called the default mode. So our body, it goes on autopilot while we're folding the laundry or we're walking to work. But actually, that is when our brain gets really busy. Here's boredom researcher Dr. Sandi Mann. (SOUNDBITE OF ARCHIVED RECORDING)SANDI MANN: Once you start daydreaming and allow your mind to really wander, you start thinking a little bit beyond the conscious, a little bit into the subconscious, which allows sort of different connections to take place. It's really awesome, actually. ZOMORODI: Totally awesome, right? So this is my brain in an fMRI. And I learned that in the default mode, that is when we connect disparate ideas, we solve some of our most nagging problems, and we do something called autobiographical planning. This is when we look back at our lives. We take note of the big moments. We create a personal narrative. And then we set goals, and we figure out what steps we need to take to reach them. But now we chill out on the couch - also while updating a Google Doc or replying to email. The average person checks email 74 times a day and switches tasks on their computer 566 times a day. I discovered all this talking to professor of informatics Dr. Gloria Mark. (SOUNDBITE OF ARCHIVED RECORDING)GLORIA MARK: So we find that when people are stressed, they tend to shift their attention more rapidly. We also found - strangely enough, we find that the shorter amount of sleep that a person gets, the more likely they are to check Facebook. So we're in this vicious habitual cycle. ZOMORODI: But could this cycle be broken? Like, what would happen if we broke this vicious cycle? What if we reclaimed those cracks in our day? Could it help us jump-start our creativity? Maybe my listeners could help me find out. We called the project Bored and Brilliant. (SOUNDBITE OF MUSIC)ZOMORODI: And within 48 hours, 20,000 people signed up, Guy. RAZ: Wow. ZOMORODI: Yeah (laughter). I was like, oh, not a special snowflake - this is a thing. People are feeling this. (SOUNDBITE OF MUSIC)RAZ: In just a moment, what happens when 20,000 people pledge to stop paying attention to their phones and start being bored? Stay with us. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, Competing for Your Attention, what living in an age of constant information and infinite distractions can do to our brains, our culture and our lives. And just before the break, we were hearing from podcast host Manoush Zomorodi, who, on an episode of her show, asked listeners to put down their phones for a week to see what happens. ZOMORODI: So one day, take the app that your thumb always seems to gravitate towards, take it off your phone and observe what it feels like. And then decide, do you want it back on your phone? Cool. Go for it if you do. But do not let the tech companies decide as their decision-making. Don't let that be the default which it very much has become, I think, for consumers. RAZ: So out of the tens of thousands of people who signed up for the challenge, some of them called her up because they started to realize that their relationship with their phone had kind of become co-dependent. UNIDENTIFIED PERSON #1: The relationship between a baby and its teddy bear, or a baby and its binky or a baby that wants its mother's cradle when it's done being held by a stranger, (laughter), that's the relationship between me and my phone. UNIDENTIFIED PERSON #2: I think of my phone like a power tool - extremely useful, but dangerous if I'm not handling it properly. If I don't pay close attention, I'll suddenly realize that I've lost an hour of time doing something totally mindless. (SOUNDBITE OF TED TALK)ZOMORODI: OK. But to really measure any improvement, we needed data, right? 'Cause that's what we do these days. So we partnered with some apps that would measure how much time we were spending every day on our phone. And if you're thinking it's ironic that I asked people to download another app so that they would spend less time on their phones, yeah. But, you've got to meet people where they are. But when the data came in, it turned out that we had cut down on average just six minutes, from 120 minutes a day on our phones to 114. Yeah. Whoop-dee-do (ph). RAZ: So it's amazing that you got so many people involved, and then looked at the data and it turned out that people just saved six minutes a day, which is sort of, like, deflating, right? I mean, after all this effort, people are only saving six minutes a day, which tells us something about ourselves. ZOMORODI: Yeah, I mean - well, first of all, it tells me that I have been trained to expect 10x returns, right? Like, you know, we expect these huge numbers, and I thought six minutes was nothing. But when I went back to the scientists and researchers who were advising me on this, they - I'm not joking. They laughed in my face. They were like, who says six minutes isn't significant? And, frankly, like, you know, the fact that you got people to change their behavior at all over a week is extraordinary. And listen to the stories because the stories will tell you so much more than any data can. And that's what people told me. They told me stories about how they realized they used to relax by playing their guitar, and that they suddenly understood that they hadn't played it in years. Or, things bigger than that, that people had sat down, just thought about what the family dynamics were and get to a better place in their relationship. There were all these amazing stories that people told us. And I thought, you know what? You're right. F the six minutes, right? RAZ: Yeah. ZOMORODI: (Laughter). RAZ: Yeah. Totally. ZOMORODI: Or, like, let's stop giving boredom such a bad rap. It actually is an extremely important human function that we are starting to just sort of breed out of our daily lives. And I sort of look around, and I see there's lots of things like that. Downtime, eye contact, conversations out loud where people stutter or make mistakes or take more than a quick, you know, 140 characters to figure out what they want to say. We've lost the capacity in many ways, I think, for patience. If we want to have excellent ideas, the best ideas, we need to let them take the time to take root and then blossom, and that does not happen in a tap of an app. RAZ: Yeah. ZOMORODI: We're humans. We need time. And that's the one thing that our phones can't give us more of. (SOUNDBITE OF MUSIC)RAZ: Manoush Zomorodi. She's a podcast host and co-founder of the media company Stable Genius Productions. You can see her full talk at ted. com. GUY RAZ, HOST:  Do you remember when you were a kid, like, in the summertime - and there was no school, and it was hot, and the days would go on forever, and it was so boring? MANOUSH ZOMORODI: So, so boring. RAZ: Do you remember that? ZOMORODI: Oh, yeah. I remember it. I had a babysitter actually who used to lock us out of the house (laughter) in the afternoons. . . RAZ: Wow. ZOMORODI: . . . So she could watch her soap opera. And I remember, like, walking - so we would play this game where we would walk around the house and try to find ways to get back into the house. And that just would go on for hours and hours. And then you'd find ourselves, like, sitting in, like, a rut by the side of the house just looking at the dirt. . . RAZ: Yeah. ZOMORODI: . . . And doing very little. (Laughter) I don't think I've ever told my mother that story, actually. RAZ: This is a Manoush Zomorodi. ZOMORODI: I'm Manoush Zomorodi. I am a podcast host. I am a tech journalist. And I am the author of the book \"Bored And Brilliant. \" RAZ: And Manoush argues that the feeling of being bored isn't actually something people experience anymore. ZOMORODI: I definitely don't think that my children have ever experienced that sensation (laughter). Their minds are constantly being stimulated by STEM camp or, you know, adventure camp and afterschool guitar lessons and, well, iPad games that are allegedly educational. There's always - (laughter) every moment is filled. RAZ: And it's not just kids. All of us - we're filling the time when we used to be bored, when our minds would just wander, with never-ending stimulation, stimulation that captures our attention. And that's whether we're waiting in line at the airport. . . (SOUNDBITE OF VARIOUS NOTIFICATION TONES) ZOMORODI: Oh, I check my to-do list. I text my husband. I read the headlines. RAZ: . . . Or taking a break from work. ZOMORODI: I go back and check my to-do list again. I answer an email. I maybe play a quick game. (SOUNDBITE OF GAME, \"CANDY CRUSH SODA SAGA\") UNIDENTIFIED VOICE ACTOR: (As character) Sodalicious (ph). RAZ: It's a relatively recent phenomenon that we'll do almost anything to keep from being bored. ZOMORODI: Like, you know, how many kids were told, only boring people get bored. . . RAZ: Yeah. ZOMORODI: . . . As though this was something to be avoided? Boredom - oh, my God - avoid that at all costs. RAZ: But by constantly preventing ourselves from becoming bored, we actually might be missing out on something bigger. Manoush Zomorodi explains from the TED stage. (SOUNDBITE OF TED TALK) ZOMORODI: I started talking to neuroscientists and cognitive psychologists. And what they told me was fascinating. It turns out that when you get bored, you ignite a network in your brain called the default mode. So our body, it goes on autopilot while we're folding the laundry or we're walking to work. But actually, that is when our brain gets really busy. Here's boredom researcher Dr. Sandi Mann. (SOUNDBITE OF ARCHIVED RECORDING) SANDI MANN: Once you start daydreaming and allow your mind to really wander, you start thinking a little bit beyond the conscious, a little bit into the subconscious, which allows sort of different connections to take place. It's really awesome, actually. ZOMORODI: Totally awesome, right? So this is my brain in an fMRI. And I learned that in the default mode, that is when we connect disparate ideas, we solve some of our most nagging problems, and we do something called autobiographical planning. This is when we look back at our lives. We take note of the big moments. We create a personal narrative. And then we set goals, and we figure out what steps we need to take to reach them. But now we chill out on the couch - also while updating a Google Doc or replying to email. The average person checks email 74 times a day and switches tasks on their computer 566 times a day. I discovered all this talking to professor of informatics Dr. Gloria Mark. (SOUNDBITE OF ARCHIVED RECORDING) GLORIA MARK: So we find that when people are stressed, they tend to shift their attention more rapidly. We also found - strangely enough, we find that the shorter amount of sleep that a person gets, the more likely they are to check Facebook. So we're in this vicious habitual cycle. ZOMORODI: But could this cycle be broken? Like, what would happen if we broke this vicious cycle? What if we reclaimed those cracks in our day? Could it help us jump-start our creativity? Maybe my listeners could help me find out. We called the project Bored and Brilliant. (SOUNDBITE OF MUSIC) ZOMORODI: And within 48 hours, 20,000 people signed up, Guy. RAZ: Wow. ZOMORODI: Yeah (laughter). I was like, oh, not a special snowflake - this is a thing. People are feeling this. (SOUNDBITE OF MUSIC) RAZ: In just a moment, what happens when 20,000 people pledge to stop paying attention to their phones and start being bored? Stay with us. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, Competing for Your Attention, what living in an age of constant information and infinite distractions can do to our brains, our culture and our lives. And just before the break, we were hearing from podcast host Manoush Zomorodi, who, on an episode of her show, asked listeners to put down their phones for a week to see what happens. ZOMORODI: So one day, take the app that your thumb always seems to gravitate towards, take it off your phone and observe what it feels like. And then decide, do you want it back on your phone? Cool. Go for it if you do. But do not let the tech companies decide as their decision-making. Don't let that be the default which it very much has become, I think, for consumers. RAZ: So out of the tens of thousands of people who signed up for the challenge, some of them called her up because they started to realize that their relationship with their phone had kind of become co-dependent. UNIDENTIFIED PERSON #1: The relationship between a baby and its teddy bear, or a baby and its binky or a baby that wants its mother's cradle when it's done being held by a stranger, (laughter), that's the relationship between me and my phone. UNIDENTIFIED PERSON #2: I think of my phone like a power tool - extremely useful, but dangerous if I'm not handling it properly. If I don't pay close attention, I'll suddenly realize that I've lost an hour of time doing something totally mindless. (SOUNDBITE OF TED TALK) ZOMORODI: OK. But to really measure any improvement, we needed data, right? 'Cause that's what we do these days. So we partnered with some apps that would measure how much time we were spending every day on our phone. And if you're thinking it's ironic that I asked people to download another app so that they would spend less time on their phones, yeah. But, you've got to meet people where they are. But when the data came in, it turned out that we had cut down on average just six minutes, from 120 minutes a day on our phones to 114. Yeah. Whoop-dee-do (ph). RAZ: So it's amazing that you got so many people involved, and then looked at the data and it turned out that people just saved six minutes a day, which is sort of, like, deflating, right? I mean, after all this effort, people are only saving six minutes a day, which tells us something about ourselves. ZOMORODI: Yeah, I mean - well, first of all, it tells me that I have been trained to expect 10x returns, right? Like, you know, we expect these huge numbers, and I thought six minutes was nothing. But when I went back to the scientists and researchers who were advising me on this, they - I'm not joking. They laughed in my face. They were like, who says six minutes isn't significant? And, frankly, like, you know, the fact that you got people to change their behavior at all over a week is extraordinary. And listen to the stories because the stories will tell you so much more than any data can. And that's what people told me. They told me stories about how they realized they used to relax by playing their guitar, and that they suddenly understood that they hadn't played it in years. Or, things bigger than that, that people had sat down, just thought about what the family dynamics were and get to a better place in their relationship. There were all these amazing stories that people told us. And I thought, you know what? You're right. F the six minutes, right? RAZ: Yeah. ZOMORODI: (Laughter). RAZ: Yeah. Totally. ZOMORODI: Or, like, let's stop giving boredom such a bad rap. It actually is an extremely important human function that we are starting to just sort of breed out of our daily lives. And I sort of look around, and I see there's lots of things like that. Downtime, eye contact, conversations out loud where people stutter or make mistakes or take more than a quick, you know, 140 characters to figure out what they want to say. We've lost the capacity in many ways, I think, for patience. If we want to have excellent ideas, the best ideas, we need to let them take the time to take root and then blossom, and that does not happen in a tap of an app. RAZ: Yeah. ZOMORODI: We're humans. We need time. And that's the one thing that our phones can't give us more of. (SOUNDBITE OF MUSIC) RAZ: Manoush Zomorodi. She's a podcast host and co-founder of the media company Stable Genius Productions. You can see her full talk at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-25-614079247": {"title": "Jaron Lanier: How Can We Repair The Mistakes Of The Digital Era? : NPR", "url": "https://www.npr.org/2018/05/25/614079247/jaron-lanier-how-can-we-repair-the-mistakes-of-the-digital-era", "author": "No author found", "published_date": "2018-05-25", "content": "GUY RAZ, HOST: Do you use Facebook? JARON LANIER: No. No. RAZ: Do you use Twitter? LANIER: No, no, no. RAZ: Snapchat? LANIER: Nope, nope, nope, nope, nope. RAZ: This is Jaron Lanier. And although you might guess it from his social media presence, Jaron's a technologist - actually, a pioneer who's thought a lot about how and why the Internet evolved into a place where advertisers compete for everyone's attention. Jaron's worked in tech pretty much his whole life. LANIER: I showed up in Silicon Valley at the start of the '80s and became a freelance video game designer in the very early days. RAZ: For a while, he worked at Atari. LANIER: I had a hit game in '83, I think, that nobody remembers anymore - in the 8-bit era. It was called Moondust. RAZ: And Jaron actually founded the first virtual reality company. LANIER: It was the most thrilling way to spend one's 20s that you could imagine. It just felt like we were at this moment of the creation of a new universe. It was an extraordinary feeling that doesn't exist today, although we still kind of try to evoke it in our rhetoric. It was an amazing time. RAZ: But even with all of that optimism, there were some concerns about the dark side of what might evolve from all that new technology. Here's Jaron Lanier on the TED stage. (SOUNDBITE OF TED TALK)LANIER: The idealism of digital culture back then was all about starting with that recognition of the possible darkness and trying to imagine a way to transcend it with beauty and creativity. It was a beautiful vision, and it's one I still believe in. I suppose I could mention one of the very earliest computer scientists, whose name was Norbert Wiener, and he wrote a book back in the '50s from before I was even born. And in the book, he described the potential to create a computer system that would be gathering data from people and providing feedback to those people in real time. And he has this amazing line where he says, one could imagine a global computer system where everybody has devices on them all the time, and the devices are giving them feedback based on what they did, and whole the population is subject to a degree of behavior modification, and such a society would be insane, could not survive, could not face its problems. And then he says, but this is only a thought experiment, and such a future is technologically infeasible. And yet, of course, it's what we have created. What my friends and I desperately wanted was exactly the opposite of that. We wanted a world of great creativity where individuals would find themselves and surprise everybody with brilliance. It was clear, though, that computers could go either way. And the thing is, it seems like we lost that war. You know, I - it's kind of the tragedy of my generation. So here we are, and now what we're trying to do is figure out how to unravel our mistakes. RAZ: So what happened, in your view? Like, how did we get to a world where advertising algorithms are constantly battling for our attention and tech companies are, you know, essentially manipulating our behavior? LANIER: All right, so the core problem of companies like Facebook is perverse incentives. It's that the way they make money is from manipulating people. It's from exacting behavioral change. This advertising model arose because everybody was backed into a corner. And the way I remember it - I was around Google when it was really just starting. I don't think people wanted it. I think it was just the only available solution because on the one hand, we loved our entrepreneurs. We worshipped Steve Jobs, for instance. But on the other hand, we wanted everything to be free. This idea of the advertising model was the only solution. And computers got faster, the algorithms got better, the customers and the users got more sophisticated, and the whole thing evolved into this massive behavior-modification scheme. And I don't think we can survive on this design. RAZ: So do you think that attention - and I asked this question to Zeynep and Tristan - I mean, do you think that attention has become commodified - like, that it's this huge resource now that big companies are just trying to capture? LANIER: Well, attention is, in a way, a laundered term. I don't think anybody cares what you're paying attention to because that in itself doesn't do anything for anyone else. What they really care about is the behavior that results. And the behavior might be negative. Like, it might be failing to vote in an election. Or it might be positive, like you might be purchasing something. So, you know, every penny that a company like Facebook or Google earns is because somebody believes that they're successfully manipulating someone else. We've created a society where if two people wish to have contact or collaborate or be aware of each other, the only way that can be financed is because there's a third party who believes they can successfully manipulate those first two people, and so attention is only a precedent to that. (SOUNDBITE OF TED TALK)LANIER: What started out as advertising really can't be called advertising anymore. It turned into behavior modification. And so this is the dilemma we've gotten ourselves into. The alternative is to turn back the clock and remake that decision. Remaking it would mean two things. It would mean, first, that many people - those who could afford to - would actually pay for these things. You'd pay for search. You'd pay for social networking. How would you pay? Maybe with a subscription fee, maybe with micropayments as you use them. There's a lot of options. If some of you are recoiling and you're thinking oh, my God, I would never pay for these things, I want to remind you of something that just happened. Around this same time that companies like Google and Facebook were formulating their free idea, a lot of cyberculture also believed that in the future, televisions and movies would be created in the same way, kind of like the Wikipedia. But then companies like Netflix, Amazon, HBO said, actually, you know, subscribe; we'll give you great TV. And it worked. We now are in this period called Peak TV, right? So sometimes, when you pay for stuff, things get better. (APPLAUSE)LANIER: We can imagine a hypothetical world of peak social media. What would that be like? It would mean when you get on, you can get really useful, authoritative medical advice instead of cranks. It could mean when you want to get factual information, there's not a bunch of weird, paranoid conspiracy theories. We can imagine this wonderful other possibility. I dream of it. I believe it's possible. I'm certain it's possible. And I'm certain that the companies - the Googles and the Facebooks - would actually do better in this world. I don't believe we need to punish Silicon Valley. We just need to remake the decision. RAZ: If we do nothing, and the way we consume the Internet and the way we interact with it just continues, what happens? What happens to us, to our behavior? LANIER: If we do nothing and we continue as we are, democracies will continue to devolve, and we'll end up in a world of autocrats who are somehow connected to the biggest computers. And we'll enter into the kind of dystopia that's been foreseen in science fiction. Eventually, something will come along that the society can't deal with, and we'll be extinguished. So if the species is to survive and is to be creative, we cannot fall into that trap. We simply cannot. RAZ: You know, Jaron, humans have been around as a species for, like, 300,000 years, right? And we haven't really changed all that much. But do you think that we humans are equipped to take in all of these inputs - like, to give all of our attention to things like social media? Like, is part of the problem that we just - our brains aren't built to juggle all this incoming information? LANIER: You know, I think we're designed for tremendous amount of input and a tremendous amount of memory. I think the problem with the digital era is, we get a lot of signals that are actually not real signals, like, you know, buy these shoes; go to this party; oh, you're not as hot as the other person - I don't know - just this endless stream of stuff. And so I don't think it's so much that we're being overwhelmed by genuine detail, but by pseudo-detail. And it's like we're in this behaviorist experiment where we're in this maze where instead of the world of sense and hues and shades and the subtleties of nature that are ever-changing and infinitely deep, instead, we're in this world of little buttons and lights and treat dispensers, and it's actually a curtailed, simplified world that seems complex just because having a lot of it kind of takes up our time, takes up our attention. But I actually think we are built for great deal of stimulus and detail, and we're not getting it. I think that the more accurate description of modern times is that we're starved for reality. RAZ: Jaron Lanier - he's a computer scientist who teaches and writes a lot about technology. You can see his entire talk at ted. com. (SOUNDBITE OF SONG, \"PAY ATTENTION\")POMPLAMOOSE: (Singing) Sometimes I don't pay attention, and I hit cars. Sometimes I don't pay attention, and I start wars. Sometimes I don't pay attention. You know what I mean. You know what I mean. RAZ: Hey, thanks for listening to our show on attention this week. If you want to find out more about who was on it, go to ted. npr. org. To see hundreds more TED Talks, check out ted. com or the TED app. Our production staff here at NPR includes Jeff Rogers, Sanaz Meshkinpour, Jinae West, Neva Grant, Rund Abdelfatah, Casey Herman and Rachel Faulkner, with help from Daniel Shukin, Lawrence Wu and Diba Mohtasham. Our partners at TED are Chris Anderson, Colin Helms, Anna Phelan and Janet Lee. If you want to let us know what you think about our show, please go to Apple Podcasts and write a review. And you can tweet at us. It's at @TEDRadioHour. I'm Guy Raz, and you've been listening to ideas worth spreading right here on the TED Radio Hour from NPR. GUY RAZ, HOST:  Do you use Facebook? JARON LANIER: No. No. RAZ: Do you use Twitter? LANIER: No, no, no. RAZ: Snapchat? LANIER: Nope, nope, nope, nope, nope. RAZ: This is Jaron Lanier. And although you might guess it from his social media presence, Jaron's a technologist - actually, a pioneer who's thought a lot about how and why the Internet evolved into a place where advertisers compete for everyone's attention. Jaron's worked in tech pretty much his whole life. LANIER: I showed up in Silicon Valley at the start of the '80s and became a freelance video game designer in the very early days. RAZ: For a while, he worked at Atari. LANIER: I had a hit game in '83, I think, that nobody remembers anymore - in the 8-bit era. It was called Moondust. RAZ: And Jaron actually founded the first virtual reality company. LANIER: It was the most thrilling way to spend one's 20s that you could imagine. It just felt like we were at this moment of the creation of a new universe. It was an extraordinary feeling that doesn't exist today, although we still kind of try to evoke it in our rhetoric. It was an amazing time. RAZ: But even with all of that optimism, there were some concerns about the dark side of what might evolve from all that new technology. Here's Jaron Lanier on the TED stage. (SOUNDBITE OF TED TALK) LANIER: The idealism of digital culture back then was all about starting with that recognition of the possible darkness and trying to imagine a way to transcend it with beauty and creativity. It was a beautiful vision, and it's one I still believe in. I suppose I could mention one of the very earliest computer scientists, whose name was Norbert Wiener, and he wrote a book back in the '50s from before I was even born. And in the book, he described the potential to create a computer system that would be gathering data from people and providing feedback to those people in real time. And he has this amazing line where he says, one could imagine a global computer system where everybody has devices on them all the time, and the devices are giving them feedback based on what they did, and whole the population is subject to a degree of behavior modification, and such a society would be insane, could not survive, could not face its problems. And then he says, but this is only a thought experiment, and such a future is technologically infeasible. And yet, of course, it's what we have created. What my friends and I desperately wanted was exactly the opposite of that. We wanted a world of great creativity where individuals would find themselves and surprise everybody with brilliance. It was clear, though, that computers could go either way. And the thing is, it seems like we lost that war. You know, I - it's kind of the tragedy of my generation. So here we are, and now what we're trying to do is figure out how to unravel our mistakes. RAZ: So what happened, in your view? Like, how did we get to a world where advertising algorithms are constantly battling for our attention and tech companies are, you know, essentially manipulating our behavior? LANIER: All right, so the core problem of companies like Facebook is perverse incentives. It's that the way they make money is from manipulating people. It's from exacting behavioral change. This advertising model arose because everybody was backed into a corner. And the way I remember it - I was around Google when it was really just starting. I don't think people wanted it. I think it was just the only available solution because on the one hand, we loved our entrepreneurs. We worshipped Steve Jobs, for instance. But on the other hand, we wanted everything to be free. This idea of the advertising model was the only solution. And computers got faster, the algorithms got better, the customers and the users got more sophisticated, and the whole thing evolved into this massive behavior-modification scheme. And I don't think we can survive on this design. RAZ: So do you think that attention - and I asked this question to Zeynep and Tristan - I mean, do you think that attention has become commodified - like, that it's this huge resource now that big companies are just trying to capture? LANIER: Well, attention is, in a way, a laundered term. I don't think anybody cares what you're paying attention to because that in itself doesn't do anything for anyone else. What they really care about is the behavior that results. And the behavior might be negative. Like, it might be failing to vote in an election. Or it might be positive, like you might be purchasing something. So, you know, every penny that a company like Facebook or Google earns is because somebody believes that they're successfully manipulating someone else. We've created a society where if two people wish to have contact or collaborate or be aware of each other, the only way that can be financed is because there's a third party who believes they can successfully manipulate those first two people, and so attention is only a precedent to that. (SOUNDBITE OF TED TALK) LANIER: What started out as advertising really can't be called advertising anymore. It turned into behavior modification. And so this is the dilemma we've gotten ourselves into. The alternative is to turn back the clock and remake that decision. Remaking it would mean two things. It would mean, first, that many people - those who could afford to - would actually pay for these things. You'd pay for search. You'd pay for social networking. How would you pay? Maybe with a subscription fee, maybe with micropayments as you use them. There's a lot of options. If some of you are recoiling and you're thinking oh, my God, I would never pay for these things, I want to remind you of something that just happened. Around this same time that companies like Google and Facebook were formulating their free idea, a lot of cyberculture also believed that in the future, televisions and movies would be created in the same way, kind of like the Wikipedia. But then companies like Netflix, Amazon, HBO said, actually, you know, subscribe; we'll give you great TV. And it worked. We now are in this period called Peak TV, right? So sometimes, when you pay for stuff, things get better. (APPLAUSE) LANIER: We can imagine a hypothetical world of peak social media. What would that be like? It would mean when you get on, you can get really useful, authoritative medical advice instead of cranks. It could mean when you want to get factual information, there's not a bunch of weird, paranoid conspiracy theories. We can imagine this wonderful other possibility. I dream of it. I believe it's possible. I'm certain it's possible. And I'm certain that the companies - the Googles and the Facebooks - would actually do better in this world. I don't believe we need to punish Silicon Valley. We just need to remake the decision. RAZ: If we do nothing, and the way we consume the Internet and the way we interact with it just continues, what happens? What happens to us, to our behavior? LANIER: If we do nothing and we continue as we are, democracies will continue to devolve, and we'll end up in a world of autocrats who are somehow connected to the biggest computers. And we'll enter into the kind of dystopia that's been foreseen in science fiction. Eventually, something will come along that the society can't deal with, and we'll be extinguished. So if the species is to survive and is to be creative, we cannot fall into that trap. We simply cannot. RAZ: You know, Jaron, humans have been around as a species for, like, 300,000 years, right? And we haven't really changed all that much. But do you think that we humans are equipped to take in all of these inputs - like, to give all of our attention to things like social media? Like, is part of the problem that we just - our brains aren't built to juggle all this incoming information? LANIER: You know, I think we're designed for tremendous amount of input and a tremendous amount of memory. I think the problem with the digital era is, we get a lot of signals that are actually not real signals, like, you know, buy these shoes; go to this party; oh, you're not as hot as the other person - I don't know - just this endless stream of stuff. And so I don't think it's so much that we're being overwhelmed by genuine detail, but by pseudo-detail. And it's like we're in this behaviorist experiment where we're in this maze where instead of the world of sense and hues and shades and the subtleties of nature that are ever-changing and infinitely deep, instead, we're in this world of little buttons and lights and treat dispensers, and it's actually a curtailed, simplified world that seems complex just because having a lot of it kind of takes up our time, takes up our attention. But I actually think we are built for great deal of stimulus and detail, and we're not getting it. I think that the more accurate description of modern times is that we're starved for reality. RAZ: Jaron Lanier - he's a computer scientist who teaches and writes a lot about technology. You can see his entire talk at ted. com. (SOUNDBITE OF SONG, \"PAY ATTENTION\") POMPLAMOOSE: (Singing) Sometimes I don't pay attention, and I hit cars. Sometimes I don't pay attention, and I start wars. Sometimes I don't pay attention. You know what I mean. You know what I mean. RAZ: Hey, thanks for listening to our show on attention this week. If you want to find out more about who was on it, go to ted. npr. org. To see hundreds more TED Talks, check out ted. com or the TED app. Our production staff here at NPR includes Jeff Rogers, Sanaz Meshkinpour, Jinae West, Neva Grant, Rund Abdelfatah, Casey Herman and Rachel Faulkner, with help from Daniel Shukin, Lawrence Wu and Diba Mohtasham. Our partners at TED are Chris Anderson, Colin Helms, Anna Phelan and Janet Lee. If you want to let us know what you think about our show, please go to Apple Podcasts and write a review. And you can tweet at us. It's at @TEDRadioHour. I'm Guy Raz, and you've been listening to ideas worth spreading right here on the TED Radio Hour from NPR.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-25-614073254": {"title": "Amishi Jha: How Can We Pay Better Attention To Our Attention? : NPR", "url": "https://www.npr.org/2018/05/25/614073254/amishi-jha-how-can-we-pay-better-attention-to-our-attention", "author": "No author found", "published_date": "2018-05-25", "content": "GUY RAZ, HOST: On the show today, ideas about how the things that compete for our attention might actually be hurting it. AMISHI JHA: This is this whole myth of multitasking, like, I'll keep all of my alerts on and sit down to do something important. It never works out. RAZ: This is neuroscientist Amishi Jha. She's a professor at the University of Miami, where she studies attention. JHA: Deciding to pick up your phone to see who just texted you, or looking at that website just because you wanted to see if you want to buy those new pair of shoes, whatever it is, it's pulling your attention away and now you have to expend even more capacity to get it back on track. RAZ: Can you break this down from a neuroscience perspective? Like, what actually is attention? JHA: One way we can think about attention, I see it as sort of like a flashlight. So just like a flashlight, you know, wherever we direct it in a darkened room, that part of our visual scene will be processed better. It allows us to willfully direct our brains' resources to particular things, whether it's the external environment, or we can even direct that flashlight internally to memories or emotions, if we'd like. RAZ: For the past 15 years, Amishi's been studying why it's harder to pay attention the longer we have to do it. JHA: And what is making people more likely to decline in their performance the longer they have to stay attentive? RAZ: And a lot of her experiments start with participants putting on these hats that kind of look like swimming caps. And those caps have electrodes embedded in them to measure attention. JHA: With things like functional MRI and brainwave recordings. RAZ: And then participants are asked to do a series of tasks. JHA: Something as simple as, you're going to see a number on the screen. Every time you see a number, press a button. Except, when that number's three, don't press the button. RAZ: Sounds pretty easy, right? JHA: Right. It's so simple. RAZ: You see the number three. You don't press the button. JHA: But what happens within - I would say within two minutes of having people do this task, they mess up. RAZ: So when the number three does pop up. . . JHA: Sure enough, they press the button. And you wonder, why? I mean, you didn't make the numbers hard to see. No. But what we did is make the task so boring that people willfully will start mind-wandering. RAZ: Amishi Jha picks up her idea from the TED stage. (SOUNDBITE OF TED TALK)JHA: So what do all of these studies tell us? They tell us that attention is very powerful in terms of affecting our perception, and things like stress and mind wandering diminish its power. But that's all in the context of these very controlled laboratory settings. What about in the real world? What about in our actual day-to-day life? What about now? Where is your attention right now? I'd like to make a prediction about your attention for the remainder of my talk. You will be unaware of what I'm saying four out of the next eight minutes. Now, why am I saying this? A growing body of literature suggests that we mind-wander. We take our mind away from the task at hand about 50 percent of our waking moments. And when this mind-wandering happens, it can be problematic. Now, I don't think there'll be any dire consequences with you all sitting here today, but imagine a military leader missing four minutes of a military briefing, or a judge missing four minutes of testimony. The consequences in those cases could be dire. So one question we might ask is, why do we do this? Why do we mind-wander so much? Well, part of the answer is that our mind is an exquisite, time-traveling master. We can rewind the mind to the past to reflect on events that have already happened. Right? Or we can go in fast-future to plan for the next thing that we want to do. And we land in this mental time-travel mode of the past or the future very frequently, most times without our awareness, even if we want to be paying attention. RAZ: I feel like we humans are in this grand experiment where increasingly we're all becoming like cats in front of a laser beam. JHA: (Laughter). RAZ: Like, we're just like squirrels in a park. You know, it's, like, every direction. We're just, like, looking around. JHA: Right. RAZ: And so that means we're not attentive. We're not focusing on a task or a person or a conversation or something. But then there's this whole group of people, like, you know, like, Manoush Zomorodi, who we heard from earlier, who say, well, you need to let your mind wander, right, because that's where creativity comes from. And I don't quite know how to reconcile those two different ideas. JHA: Yeah. Both are true. Both are tied to spontaneous thought. But when I use the term mind-wandering, I'm really referring to that happening when we don't want it to. We want to be focused on something, and another thought pops in. The other category would be something we more colloquially would call daydreaming. That's when you let this happen without cost. RAZ: Right. Yeah. JHA: That capacity to let the mind engage in spontaneous thought is so generative. Positive mood increases. Creativity increases. And the key is that we have the space to do that. So if every moment the attention system is being occupied by some other demand, there are fewer opportunities to let that spontaneous thought arise. And I think that's why we need to set ourselves up to have daily practices that help give us that space back. RAZ: Yeah. JHA: And you see this already. I mean, to have people actually sign up to go into a room and sit quietly with their eyes closed on purpose. RAZ: Like one of those meditation retreats. JHA: Yeah. But that wouldn't have happened before we were constantly bombarded with information. But people do this all the time. RAZ: Yeah. JHA: You know, I did it. You know, I decided to take a week trip to India, not have my phone, and sit with my eyes closed and not talk to anybody. RAZ: I mean, did you go there just as a researcher to think, you know, I'm a neuroscientist, and I study this stuff. I should go check it out? Or did you - were you like, I am losing my mind. I need to, like, get control over it. JHA: Well, the whole reason mindfulness training entered my laboratory is because of my direct personal interest in it. And the interest came grudgingly. I mean, I'm an Indian woman. I grew up in a family where my parents meditated probably since I was a child. RAZ: Wow. JHA: And growing up, I pretty much disregarded it. It's like, it's that thing they do. RAZ: You were like, it's that weird thing my parents do. JHA: Exactly. I'm a Western scientist. Like, unless there's evidence, I don't care about it or I don't trust it. RAZ: Right. JHA: And I was probably in my late 30s and realized that being in a highly demanding academic profession, having young children, a husband who was in grad school, it was really hard to manage my own stress. And that's what made me interested in engaging in some tools that I could implement daily to help my attention. Here I was studying it in a lab, but I felt like I had no access to it. RAZ: So what was it like? JHA: (Laughter). It was terrifying at first. It was kind of like a boot camp for attention. RAZ: (Laughter). JHA: And it was kind of funny because after I was done with the retreat, you know, I had to go back to the airport, return to regular life. And I'm like, I wonder if all this time did anything? And one of the first things that happened to me, I was in the aisle seat. And a woman came up to me, and she's like, I'm in the middle seat. So I got up to let her in. As she was sitting down, she poured the entire contents of a jumbo size beverage on me. (Laughter). And my response was just - surprised me. I was just like, that's OK. You didn't mean to do that. It's going to be all right. RAZ: Wow. JHA: And that was the test. Just, like, I didn't get angry. RAZ: You didn't freak out. JHA: I didn't freak out. It was such a sense of balance I had, even as I was experiencing something potentially unpleasant. RAZ: How long did that last? JHA: (Laughter). You know, it's funny because my children comment on that. They're like, we think the retreat effect is gone now. RAZ: (Laughter). JHA: You know? RAZ: Go back. JHA: Yeah. Go back. I guess, subjectively, it really depends on how my commitment to continuing to practice remains intact as I return to normal life. RAZ: So, I mean, it's interesting because this has been - like, the idea of mindfulness and meditation, this has just exploded, especially in your field, in neuroscience, over the past 10 years. Like, 20 years ago, when you were - probably when you were doing your Ph. D. , this was still kind of, like, fringe stuff, right? JHA: Well, when I started this work, all of my mentors said this is ill-advised. . . RAZ: Don't do it. JHA: . . . This is career suicide. RAZ: You're never going to get tenure. JHA: Never going to get tenure, and nobody will ever care. So stop. And, frankly, my curiosity got the better of me, and I said, I can't not do it because I'm finding that it is an incredibly powerful tool that not only have I benefited from personally but we've seen over millennia that people have gone to this type of mental training to help themselves feel better. RAZ: So Amishi, if we're now, you know, really beginning to understand this link, this scientific link between meditation, mindfulness, and attention and focus, what is actually happening in our brain when we practice mindfulness? JHA: Right. That's what we're exploring right now. So just to begin by demystifying the whole thing. I mean, the work in my laboratories is taking a cognitive neuroscience brain-training approach to asking and answering that question. We look at brain networks that we know are responsible for things like focus. Salience detection, mind-wandering. And the evidence is now amassing that it's those same brain networks that are getting stronger. So we see that as the mental push up - focus, notice, reengage. And this is not some kind of spa vacation. We're not trying to mollify people. We're trying to wake them up to what's actually happening in the moment and in their lives. RAZ: Neuroscientist Amishi Jha. She's an associate professor at the University of Miami. You can see her entire talk at ted. com. GUY RAZ, HOST:  On the show today, ideas about how the things that compete for our attention might actually be hurting it. AMISHI JHA: This is this whole myth of multitasking, like, I'll keep all of my alerts on and sit down to do something important. It never works out. RAZ: This is neuroscientist Amishi Jha. She's a professor at the University of Miami, where she studies attention. JHA: Deciding to pick up your phone to see who just texted you, or looking at that website just because you wanted to see if you want to buy those new pair of shoes, whatever it is, it's pulling your attention away and now you have to expend even more capacity to get it back on track. RAZ: Can you break this down from a neuroscience perspective? Like, what actually is attention? JHA: One way we can think about attention, I see it as sort of like a flashlight. So just like a flashlight, you know, wherever we direct it in a darkened room, that part of our visual scene will be processed better. It allows us to willfully direct our brains' resources to particular things, whether it's the external environment, or we can even direct that flashlight internally to memories or emotions, if we'd like. RAZ: For the past 15 years, Amishi's been studying why it's harder to pay attention the longer we have to do it. JHA: And what is making people more likely to decline in their performance the longer they have to stay attentive? RAZ: And a lot of her experiments start with participants putting on these hats that kind of look like swimming caps. And those caps have electrodes embedded in them to measure attention. JHA: With things like functional MRI and brainwave recordings. RAZ: And then participants are asked to do a series of tasks. JHA: Something as simple as, you're going to see a number on the screen. Every time you see a number, press a button. Except, when that number's three, don't press the button. RAZ: Sounds pretty easy, right? JHA: Right. It's so simple. RAZ: You see the number three. You don't press the button. JHA: But what happens within - I would say within two minutes of having people do this task, they mess up. RAZ: So when the number three does pop up. . . JHA: Sure enough, they press the button. And you wonder, why? I mean, you didn't make the numbers hard to see. No. But what we did is make the task so boring that people willfully will start mind-wandering. RAZ: Amishi Jha picks up her idea from the TED stage. (SOUNDBITE OF TED TALK) JHA: So what do all of these studies tell us? They tell us that attention is very powerful in terms of affecting our perception, and things like stress and mind wandering diminish its power. But that's all in the context of these very controlled laboratory settings. What about in the real world? What about in our actual day-to-day life? What about now? Where is your attention right now? I'd like to make a prediction about your attention for the remainder of my talk. You will be unaware of what I'm saying four out of the next eight minutes. Now, why am I saying this? A growing body of literature suggests that we mind-wander. We take our mind away from the task at hand about 50 percent of our waking moments. And when this mind-wandering happens, it can be problematic. Now, I don't think there'll be any dire consequences with you all sitting here today, but imagine a military leader missing four minutes of a military briefing, or a judge missing four minutes of testimony. The consequences in those cases could be dire. So one question we might ask is, why do we do this? Why do we mind-wander so much? Well, part of the answer is that our mind is an exquisite, time-traveling master. We can rewind the mind to the past to reflect on events that have already happened. Right? Or we can go in fast-future to plan for the next thing that we want to do. And we land in this mental time-travel mode of the past or the future very frequently, most times without our awareness, even if we want to be paying attention. RAZ: I feel like we humans are in this grand experiment where increasingly we're all becoming like cats in front of a laser beam. JHA: (Laughter). RAZ: Like, we're just like squirrels in a park. You know, it's, like, every direction. We're just, like, looking around. JHA: Right. RAZ: And so that means we're not attentive. We're not focusing on a task or a person or a conversation or something. But then there's this whole group of people, like, you know, like, Manoush Zomorodi, who we heard from earlier, who say, well, you need to let your mind wander, right, because that's where creativity comes from. And I don't quite know how to reconcile those two different ideas. JHA: Yeah. Both are true. Both are tied to spontaneous thought. But when I use the term mind-wandering, I'm really referring to that happening when we don't want it to. We want to be focused on something, and another thought pops in. The other category would be something we more colloquially would call daydreaming. That's when you let this happen without cost. RAZ: Right. Yeah. JHA: That capacity to let the mind engage in spontaneous thought is so generative. Positive mood increases. Creativity increases. And the key is that we have the space to do that. So if every moment the attention system is being occupied by some other demand, there are fewer opportunities to let that spontaneous thought arise. And I think that's why we need to set ourselves up to have daily practices that help give us that space back. RAZ: Yeah. JHA: And you see this already. I mean, to have people actually sign up to go into a room and sit quietly with their eyes closed on purpose. RAZ: Like one of those meditation retreats. JHA: Yeah. But that wouldn't have happened before we were constantly bombarded with information. But people do this all the time. RAZ: Yeah. JHA: You know, I did it. You know, I decided to take a week trip to India, not have my phone, and sit with my eyes closed and not talk to anybody. RAZ: I mean, did you go there just as a researcher to think, you know, I'm a neuroscientist, and I study this stuff. I should go check it out? Or did you - were you like, I am losing my mind. I need to, like, get control over it. JHA: Well, the whole reason mindfulness training entered my laboratory is because of my direct personal interest in it. And the interest came grudgingly. I mean, I'm an Indian woman. I grew up in a family where my parents meditated probably since I was a child. RAZ: Wow. JHA: And growing up, I pretty much disregarded it. It's like, it's that thing they do. RAZ: You were like, it's that weird thing my parents do. JHA: Exactly. I'm a Western scientist. Like, unless there's evidence, I don't care about it or I don't trust it. RAZ: Right. JHA: And I was probably in my late 30s and realized that being in a highly demanding academic profession, having young children, a husband who was in grad school, it was really hard to manage my own stress. And that's what made me interested in engaging in some tools that I could implement daily to help my attention. Here I was studying it in a lab, but I felt like I had no access to it. RAZ: So what was it like? JHA: (Laughter). It was terrifying at first. It was kind of like a boot camp for attention. RAZ: (Laughter). JHA: And it was kind of funny because after I was done with the retreat, you know, I had to go back to the airport, return to regular life. And I'm like, I wonder if all this time did anything? And one of the first things that happened to me, I was in the aisle seat. And a woman came up to me, and she's like, I'm in the middle seat. So I got up to let her in. As she was sitting down, she poured the entire contents of a jumbo size beverage on me. (Laughter). And my response was just - surprised me. I was just like, that's OK. You didn't mean to do that. It's going to be all right. RAZ: Wow. JHA: And that was the test. Just, like, I didn't get angry. RAZ: You didn't freak out. JHA: I didn't freak out. It was such a sense of balance I had, even as I was experiencing something potentially unpleasant. RAZ: How long did that last? JHA: (Laughter). You know, it's funny because my children comment on that. They're like, we think the retreat effect is gone now. RAZ: (Laughter). JHA: You know? RAZ: Go back. JHA: Yeah. Go back. I guess, subjectively, it really depends on how my commitment to continuing to practice remains intact as I return to normal life. RAZ: So, I mean, it's interesting because this has been - like, the idea of mindfulness and meditation, this has just exploded, especially in your field, in neuroscience, over the past 10 years. Like, 20 years ago, when you were - probably when you were doing your Ph. D. , this was still kind of, like, fringe stuff, right? JHA: Well, when I started this work, all of my mentors said this is ill-advised. . . RAZ: Don't do it. JHA: . . . This is career suicide. RAZ: You're never going to get tenure. JHA: Never going to get tenure, and nobody will ever care. So stop. And, frankly, my curiosity got the better of me, and I said, I can't not do it because I'm finding that it is an incredibly powerful tool that not only have I benefited from personally but we've seen over millennia that people have gone to this type of mental training to help themselves feel better. RAZ: So Amishi, if we're now, you know, really beginning to understand this link, this scientific link between meditation, mindfulness, and attention and focus, what is actually happening in our brain when we practice mindfulness? JHA: Right. That's what we're exploring right now. So just to begin by demystifying the whole thing. I mean, the work in my laboratories is taking a cognitive neuroscience brain-training approach to asking and answering that question. We look at brain networks that we know are responsible for things like focus. Salience detection, mind-wandering. And the evidence is now amassing that it's those same brain networks that are getting stronger. So we see that as the mental push up - focus, notice, reengage. And this is not some kind of spa vacation. We're not trying to mollify people. We're trying to wake them up to what's actually happening in the moment and in their lives. RAZ: Neuroscientist Amishi Jha. She's an associate professor at the University of Miami. You can see her entire talk at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-25-614007959": {"title": "Zeynep Tufekci: How Is Our Attention Packaged And Sold As A Commodity? : NPR", "url": "https://www.npr.org/2018/05/25/614007959/zeynep-tufekci-how-is-our-attention-packaged-and-sold-as-a-commodity", "author": "No author found", "published_date": "2018-05-25", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. I'm Guy Raz. So how much advertising do you think you see every day? Think about all the TV commercials. . . (SOUNDBITE OF AD)UNIDENTIFIED ACTOR #1: (As character) Face the world. . . RAZ: . . . The ads on YouTube or Hulu. . . (SOUNDBITE OF AD)UNIDENTIFIED ACTOR #2: (As character) Go to geico. com to see how much you could save. . . RAZ: . . . Pop-up ads, banner ads, targeted ads, logos. . . (SOUNDBITE OF AD)UNIDENTIFIED ACTOR #3: (As character) I need a new phone. . . (SOUNDBITE OF AD)UNIDENTIFIED ACTOR #4: (As character) Olay Regenerist. . . RAZ: . . . And don't forget your commute. . . (SOUNDBITE OF AD)UNIDENTIFIED ACTOR #5: (As character) Love - it's what makes a Subaru a Subaru. RAZ: . . . Billboards, posters on buses, trains, buildings. . . (SOUNDBITE OF AD)UNIDENTIFIED ACTOR #6: (As character) Alert your doctor right away if there's difficulty swallowing, speaking, eating. . . RAZ: Now, whatever number you're thinking of, the number of ads you see every day, I'm going to guess it's too low. . . (SOUNDBITE OF AD)MORGAN FREEMAN: New Mountain Dew Ice - a clear, refreshing lemon-lime Dew. RAZ: . . . Because some estimates show that most Americans are exposed to between 4,000 and 10,000 ads every day. And all those ads are after one thing - your attention. ZEYNEP TUFEKCI: Attention is an absolutely significant resource, right? RAZ: This is sociologist Zeynep Tufekci. TUFEKCI: Everybody has 24 hours in the day. You sleep some, you work some, and what time you have free is one of the most important things you have. Getting your attention and putting in front of you can change your opinions. It changes what you prioritize. It affects politics. It affects your social interactions. I think that in an age where you have too much information, the crucial resource is that which information consumes, which is attention. Your attention is being battled over and being packaged and sold. RAZ: Attention is a commodity. TUFEKCI: Absolutely. And we have a digital economy that is essentially based on making sure that we are not in control of our attention. RAZ: On the show today - competing for your attention, ideas on the value of our awareness and why in an age of infinite distractions whoever can capture our attention holds a lot of power. Zeynep Tufekci explains from the TED stage. (SOUNDBITE OF TED TALK)TUFEKCI: Do you ever go on YouTube meaning to watch one video and an hour later you've watched 27? You know how YouTube has this column on the right that says up next, and it autoplays something? It's an algorithm picking what it thinks that you might be interested in and maybe not find on your own. It's not a human editor. It's what algorithms do. It picks up on what you have watched and what people like you have watched and infers that that must be what you're interested in, what you want more of and just shows you more. It sounds like a benign and useful feature, except when it isn't. So in 2016, I attended rallies of then-candidate Donald Trump to study as a scholar the movement supporting him. I studied social movements, so I was studying it, too. And then I wanted to write something about one of his rallies so I watched it a few times on YouTube. YouTube started recommending to me and autoplaying to me white supremacist videos in increasing order of extremism. If I watched one, it served up one even more extreme and autoplayed that one too. If you watch Hillary Clinton or Bernie Sanders content, YouTube recommends and autoplays conspiracy left, and it goes downhill from there. Well, you might be thinking, this is politics, but it's not. This isn't about politics. It's just the algorithm figuring out human behavior. I once watched a video about vegetarianism on YouTube, and YouTube recommended and autoplayed a video about being vegan. It's like you're never hardcore enough for YouTube. (SOUNDBITE OF MUSIC)TUFEKCI: So what's going on here isn't YouTube engineers are out to wreck the world, right? But they have set loose an algorithm that's optimized to grab your attention for as long as possible to keep you on the site under, their word for it, engagement while YouTube serves the ads. And the algorithm has sussed out that humans are particularly susceptible, especially young people are particularly susceptible, to the idea that they're discovering a secret, that they're being told something edgier - right? - something more extreme because it's kind of like, ooh, this is novel. I'm interested in this, right? It's sort of seducing you. It's sort of trying to play to your appetites. So the algorithm automatically plays more and more. So if you just watch some political stuff, you end up with Alex Jones, who has all these horrible conspiracy theories. You watch some, you know, science stuff and three recommended autoplays later you're in the moon landing never happened. You watch something about Trump, and a little bit later, the algorithm is playing the Holocaust never happened stuff. So by optimizing for grabbing your attention, we have in effect through YouTube's recommender algorithm created this engine of extremism that is deployed globally. RAZ: Zeynep, this is really bad. TUFEKCI: I agree. RAZ: Like, this is - this should never have been allowed to happen. TUFEKCI: Well, I absolutely agree it is a big problem, but I'm an optimist, right? Just like we can deal with the other things that come with the 21st century and just like, you know, industrial revolution has brought us a lot of good things, this is something we can deal with. We just have to say, look, let's make explicit what the problem is. Let's realize that our attention is a crucial resource. And it's an equal resource, right? Every human being on the planet only has so many hours. It doesn't matter if you're rich or not. Right? You still have so many hours. It's one of the most human of things. And we have to treat our attention and our time as the crucial resource it is and change. And we're being told a story by Silicon Valley that this is inevitable, this is good or that this stuff has to come in combination. You know, if you're going to use digital stuff, you're going to have your attention manipulated and sold. It's just not true. They package it this way, and we don't have to. We finally are paying attention to the question of attention. The next step is, we need tools and regulations and industry self-regulation and individual awareness and all those things together to say, how do we grab back control of this most precious resource? (SOUNDBITE OF MUSIC)RAZ: Zeynep Tufekci - she's a sociologist at the University of North Carolina and a columnist for The New York Times. You can find all of her talks at ted. com. GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. So how much advertising do you think you see every day? Think about all the TV commercials. . . (SOUNDBITE OF AD) UNIDENTIFIED ACTOR #1: (As character) Face the world. . . RAZ: . . . The ads on YouTube or Hulu. . . (SOUNDBITE OF AD) UNIDENTIFIED ACTOR #2: (As character) Go to geico. com to see how much you could save. . . RAZ: . . . Pop-up ads, banner ads, targeted ads, logos. . . (SOUNDBITE OF AD) UNIDENTIFIED ACTOR #3: (As character) I need a new phone. . . (SOUNDBITE OF AD) UNIDENTIFIED ACTOR #4: (As character) Olay Regenerist. . . RAZ: . . . And don't forget your commute. . . (SOUNDBITE OF AD) UNIDENTIFIED ACTOR #5: (As character) Love - it's what makes a Subaru a Subaru. RAZ: . . . Billboards, posters on buses, trains, buildings. . . (SOUNDBITE OF AD) UNIDENTIFIED ACTOR #6: (As character) Alert your doctor right away if there's difficulty swallowing, speaking, eating. . . RAZ: Now, whatever number you're thinking of, the number of ads you see every day, I'm going to guess it's too low. . . (SOUNDBITE OF AD) MORGAN FREEMAN: New Mountain Dew Ice - a clear, refreshing lemon-lime Dew. RAZ: . . . Because some estimates show that most Americans are exposed to between 4,000 and 10,000 ads every day. And all those ads are after one thing - your attention. ZEYNEP TUFEKCI: Attention is an absolutely significant resource, right? RAZ: This is sociologist Zeynep Tufekci. TUFEKCI: Everybody has 24 hours in the day. You sleep some, you work some, and what time you have free is one of the most important things you have. Getting your attention and putting in front of you can change your opinions. It changes what you prioritize. It affects politics. It affects your social interactions. I think that in an age where you have too much information, the crucial resource is that which information consumes, which is attention. Your attention is being battled over and being packaged and sold. RAZ: Attention is a commodity. TUFEKCI: Absolutely. And we have a digital economy that is essentially based on making sure that we are not in control of our attention. RAZ: On the show today - competing for your attention, ideas on the value of our awareness and why in an age of infinite distractions whoever can capture our attention holds a lot of power. Zeynep Tufekci explains from the TED stage. (SOUNDBITE OF TED TALK) TUFEKCI: Do you ever go on YouTube meaning to watch one video and an hour later you've watched 27? You know how YouTube has this column on the right that says up next, and it autoplays something? It's an algorithm picking what it thinks that you might be interested in and maybe not find on your own. It's not a human editor. It's what algorithms do. It picks up on what you have watched and what people like you have watched and infers that that must be what you're interested in, what you want more of and just shows you more. It sounds like a benign and useful feature, except when it isn't. So in 2016, I attended rallies of then-candidate Donald Trump to study as a scholar the movement supporting him. I studied social movements, so I was studying it, too. And then I wanted to write something about one of his rallies so I watched it a few times on YouTube. YouTube started recommending to me and autoplaying to me white supremacist videos in increasing order of extremism. If I watched one, it served up one even more extreme and autoplayed that one too. If you watch Hillary Clinton or Bernie Sanders content, YouTube recommends and autoplays conspiracy left, and it goes downhill from there. Well, you might be thinking, this is politics, but it's not. This isn't about politics. It's just the algorithm figuring out human behavior. I once watched a video about vegetarianism on YouTube, and YouTube recommended and autoplayed a video about being vegan. It's like you're never hardcore enough for YouTube. (SOUNDBITE OF MUSIC) TUFEKCI: So what's going on here isn't YouTube engineers are out to wreck the world, right? But they have set loose an algorithm that's optimized to grab your attention for as long as possible to keep you on the site under, their word for it, engagement while YouTube serves the ads. And the algorithm has sussed out that humans are particularly susceptible, especially young people are particularly susceptible, to the idea that they're discovering a secret, that they're being told something edgier - right? - something more extreme because it's kind of like, ooh, this is novel. I'm interested in this, right? It's sort of seducing you. It's sort of trying to play to your appetites. So the algorithm automatically plays more and more. So if you just watch some political stuff, you end up with Alex Jones, who has all these horrible conspiracy theories. You watch some, you know, science stuff and three recommended autoplays later you're in the moon landing never happened. You watch something about Trump, and a little bit later, the algorithm is playing the Holocaust never happened stuff. So by optimizing for grabbing your attention, we have in effect through YouTube's recommender algorithm created this engine of extremism that is deployed globally. RAZ: Zeynep, this is really bad. TUFEKCI: I agree. RAZ: Like, this is - this should never have been allowed to happen. TUFEKCI: Well, I absolutely agree it is a big problem, but I'm an optimist, right? Just like we can deal with the other things that come with the 21st century and just like, you know, industrial revolution has brought us a lot of good things, this is something we can deal with. We just have to say, look, let's make explicit what the problem is. Let's realize that our attention is a crucial resource. And it's an equal resource, right? Every human being on the planet only has so many hours. It doesn't matter if you're rich or not. Right? You still have so many hours. It's one of the most human of things. And we have to treat our attention and our time as the crucial resource it is and change. And we're being told a story by Silicon Valley that this is inevitable, this is good or that this stuff has to come in combination. You know, if you're going to use digital stuff, you're going to have your attention manipulated and sold. It's just not true. They package it this way, and we don't have to. We finally are paying attention to the question of attention. The next step is, we need tools and regulations and industry self-regulation and individual awareness and all those things together to say, how do we grab back control of this most precious resource? (SOUNDBITE OF MUSIC) RAZ: Zeynep Tufekci - she's a sociologist at the University of North Carolina and a columnist for The New York Times. You can find all of her talks at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-25-614315967": {"title": "Oregon Couple Unplugs Alexa After Private Conversation Is Recorded : NPR", "url": "https://www.npr.org/2018/05/25/614315967/oregon-couple-unplugs-alexa-after-it-records-private-conversation", "author": "No author found", "published_date": "2018-05-25", "content": "DAVID GREENE, HOST: Good morning. I'm David Greene. You wonder if your smart speaker is always listening? Well, an Oregon couple tells KIRO-TV they had a private conversation, their Amazon Echo recorded it and then sent the audio to someone on their contact list. Now, Amazon says the couple must have woken up their speaker by saying something sounding like Alexa, then they used words that sounded like send message. Amazon says this is extremely rare. Not enough for this couple. They've unplugged Alexa, and they say it's for good. DAVID GREENE, HOST:  Good morning. I'm David Greene. You wonder if your smart speaker is always listening? Well, an Oregon couple tells KIRO-TV they had a private conversation, their Amazon Echo recorded it and then sent the audio to someone on their contact list. Now, Amazon says the couple must have woken up their speaker by saying something sounding like Alexa, then they used words that sounded like send message. Amazon says this is extremely rare. Not enough for this couple. They've unplugged Alexa, and they say it's for good.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-26-614387170": {"title": "Newark Police Camera System Relies On Residents, Stirring Privacy Concerns : NPR", "url": "https://www.npr.org/2018/05/26/614387170/newark-police-camera-system-relies-on-residents-stirring-privacy-concerns", "author": "No author found", "published_date": "2018-05-26", "content": "SCOTT SIMON, HOST: Newark, N. J. , is taking the concept of a neighborhood watch to a whole new level. They've installed hundreds of cameras around the city to create a virtual block watch. But as New Jersey Public Radio's Karen Rouse tells us, some people don't want to be under surveillance. KAREN ROUSE, BYLINE: Newark Mayor Ras Baraka says the new cameras replace the old system that was wiped out during Hurricane Sandy. He says these cameras are different. RAS BARAKA: These cameras are going to be watched, they're going to be watched by our police department. But we're also recruiting our neighborhood and our residents to participate and engage with us to watch their neighborhoods as well. ROUSE: But that engagement goes well beyond the residents. Newark's virtual patrol allows literally anyone on Earth with an email address and Internet connection to watch whatever activities the cameras capture - cars driving through intersections, a young woman leaving a corner store, kids hanging out on a street. Sixty-two cameras are already in operation. By January, there will be 300 that anyone, anywhere can watch, like Adam Schwartz. He's a senior lawyer at the Electronic Frontier Foundation, a civil liberties nonprofit in California. I called and talked him through the Newark Police Department website. ADAM SCHWARTZ: All right, so I'm just creating my account. ROUSE: And within seconds, he sees a list of locations for the cameras. SCHWARTZ: So I can click on one of these cameras and then see what's going on. ROUSE: Schwartz says such easy access to the cameras could have a chilling effect on civil liberties. SCHWARTZ: There are people who if they are invited to go to a demonstration and they see that one of these blue cameras is up, they might decide they don't want to show up. ROUSE: And it could put black and brown people at risk for being targeted. SCHWARTZ: It's lower income and minority communities where these technologies are deployed, and they lead to more arrests. ROUSE: It's a concern for Newarkers like Linda Carter and Josie Gonsalves. I asked them to meet up with me at Raman's Boulevard and Broad Street. That's one of the 62 blocks being monitored by a surveillance camera. We gathered in front of a tax office and watched on my iPhone as the cameras panned back and forth. JOSIE GONSALVES: Those white things - those are the cameras? I can see his umbrella. UNIDENTIFIED PERSON: We're in H&R Block. GONSALVES: I can see you. UNIDENTIFIED PERSON: Yeah, that's us. LINDA CARTER: That's us. GONSALVES: I see you and his umbrella. CARTER: Here's my red coat. ROUSE: After the initial thrill of spotting themselves on my iPhone via the Newark Police Department website, Carter expressed dread. What do you think about that, being able to see yourself on a camera that anybody can log in? CARTER: I'm very, very concerned. People are collecting information. It's like we're taking our power away as citizens because we don't get to opt out, and everybody else can use it for whatever reason, whether it's good or bad. ROUSE: Gonsalves says it changes how Newarkers like her will carry themselves in public. GONSALVES: We as citizens, the whole way in which we engage with each other, we're constantly watching ourselves, watching our neighbors, and everyone becomes a suspect. ROUSE: Not everyone finds the cameras bewildering. Some residents say violent crimes like murder are still common, and the cameras add a layer of protection. STEVE SURFARO: This is taking see something, say something to a whole other level. ROUSE: Steve Surfaro is an official with the Security Industry Association, a trade group that represents companies that build security and surveillance systems. He said he doesn't think the resolution is clear enough to cause concern. SURFARO: What you're able to do is to see people. You're able to see vehicles. You can see weather. But you can't really read license plates, and you can't really recognize faces. ROUSE: He says the cameras can capture activities where the public can help, like spotting a fire or a crowd gathered around someone in distress. One downside, though, is that the public could misread what they're seeing, like mistaking someone handing off a book to someone else as a drug deal. Mayor Baraka says Newark is installing signs to let the public and the criminals know they're being watched. And he says that's not so unusual these days when cameras are in stores and on people's homes. BARAKA: What we're trying to do is really compile and organize all of this to give us the opportunity to stay ahead of some of the violence and crime that's happening in the city. ROUSE: So far, the city says more than 1,600 people have registered on the police department's website. That's 1,600 more sets of eyes watching the streets of Newark. For NPR News, I'm Karen Rouse. SCOTT SIMON, HOST:  Newark, N. J. , is taking the concept of a neighborhood watch to a whole new level. They've installed hundreds of cameras around the city to create a virtual block watch. But as New Jersey Public Radio's Karen Rouse tells us, some people don't want to be under surveillance. KAREN ROUSE, BYLINE: Newark Mayor Ras Baraka says the new cameras replace the old system that was wiped out during Hurricane Sandy. He says these cameras are different. RAS BARAKA: These cameras are going to be watched, they're going to be watched by our police department. But we're also recruiting our neighborhood and our residents to participate and engage with us to watch their neighborhoods as well. ROUSE: But that engagement goes well beyond the residents. Newark's virtual patrol allows literally anyone on Earth with an email address and Internet connection to watch whatever activities the cameras capture - cars driving through intersections, a young woman leaving a corner store, kids hanging out on a street. Sixty-two cameras are already in operation. By January, there will be 300 that anyone, anywhere can watch, like Adam Schwartz. He's a senior lawyer at the Electronic Frontier Foundation, a civil liberties nonprofit in California. I called and talked him through the Newark Police Department website. ADAM SCHWARTZ: All right, so I'm just creating my account. ROUSE: And within seconds, he sees a list of locations for the cameras. SCHWARTZ: So I can click on one of these cameras and then see what's going on. ROUSE: Schwartz says such easy access to the cameras could have a chilling effect on civil liberties. SCHWARTZ: There are people who if they are invited to go to a demonstration and they see that one of these blue cameras is up, they might decide they don't want to show up. ROUSE: And it could put black and brown people at risk for being targeted. SCHWARTZ: It's lower income and minority communities where these technologies are deployed, and they lead to more arrests. ROUSE: It's a concern for Newarkers like Linda Carter and Josie Gonsalves. I asked them to meet up with me at Raman's Boulevard and Broad Street. That's one of the 62 blocks being monitored by a surveillance camera. We gathered in front of a tax office and watched on my iPhone as the cameras panned back and forth. JOSIE GONSALVES: Those white things - those are the cameras? I can see his umbrella. UNIDENTIFIED PERSON: We're in H&R Block. GONSALVES: I can see you. UNIDENTIFIED PERSON: Yeah, that's us. LINDA CARTER: That's us. GONSALVES: I see you and his umbrella. CARTER: Here's my red coat. ROUSE: After the initial thrill of spotting themselves on my iPhone via the Newark Police Department website, Carter expressed dread. What do you think about that, being able to see yourself on a camera that anybody can log in? CARTER: I'm very, very concerned. People are collecting information. It's like we're taking our power away as citizens because we don't get to opt out, and everybody else can use it for whatever reason, whether it's good or bad. ROUSE: Gonsalves says it changes how Newarkers like her will carry themselves in public. GONSALVES: We as citizens, the whole way in which we engage with each other, we're constantly watching ourselves, watching our neighbors, and everyone becomes a suspect. ROUSE: Not everyone finds the cameras bewildering. Some residents say violent crimes like murder are still common, and the cameras add a layer of protection. STEVE SURFARO: This is taking see something, say something to a whole other level. ROUSE: Steve Surfaro is an official with the Security Industry Association, a trade group that represents companies that build security and surveillance systems. He said he doesn't think the resolution is clear enough to cause concern. SURFARO: What you're able to do is to see people. You're able to see vehicles. You can see weather. But you can't really read license plates, and you can't really recognize faces. ROUSE: He says the cameras can capture activities where the public can help, like spotting a fire or a crowd gathered around someone in distress. One downside, though, is that the public could misread what they're seeing, like mistaking someone handing off a book to someone else as a drug deal. Mayor Baraka says Newark is installing signs to let the public and the criminals know they're being watched. And he says that's not so unusual these days when cameras are in stores and on people's homes. BARAKA: What we're trying to do is really compile and organize all of this to give us the opportunity to stay ahead of some of the violence and crime that's happening in the city. ROUSE: So far, the city says more than 1,600 people have registered on the police department's website. That's 1,600 more sets of eyes watching the streets of Newark. For NPR News, I'm Karen Rouse.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-28-615010156": {"title": "How Europe's New Data Privacy Law Is Supposed To Give Users More Control : NPR", "url": "https://www.npr.org/2018/05/28/615010156/how-europes-new-data-privacy-law-is-supposed-to-give-users-more-control", "author": "No author found", "published_date": "2018-05-28", "content": "MARY LOUISE KELLY, HOST: In Europe, a sweeping digital privacy law kicked in this past Friday. Companies have been updating their terms of service and their data security rules. And even here in the U. S. , we are feeling the impact. And that is where we kick off this week's All Tech Considered. (SOUNDBITE OF MUSIC)KELLY: Well, if you are like me, your email inbox has been inundated with messages about the new European rules. And if you're like me, you may not have read every line of the fine print of these emails yet. But The Wall Street Journal's personal technology columnist Joanna Stern has. She has diligently been reading dozens of these messages. And she joins us now. Hey, there. JOANNA STERN: Hi. How are you? KELLY: We are well. Thank you. I wonder if you would start by giving us the real quick version of this European law and what it's designed to do. STERN: Yeah. So GDPR, which stands for the General Data Protection Regulation, went into effect in the European Union. And this is a new set of laws that basically tightens what companies can do with our data. Overall, what most people need to know is this is supposed to be very good for consumers. And it makes companies be more transparent about what they're doing with what they collect on us. KELLY: OK. And one of the ways in which they're being transparent is emailing all of us with these long policies and updates to the rules. You actually - you made a video, which I want to play. It's up on the Wall Street Journal website. And this is you. You printed out paper copies of some of these policies and then lined them up on a football field. (SOUNDBITE OF VIDEO)STERN: One-hundred and twenty yards, 360 feet. . . (SOUNDBITE OF MUSIC)KELLY: A not-very-subtle message there that these policies are really long. STERN: One thing I quickly noticed from this and one thing that GDPR does require is that they have to be written in plain English. So I actually was able to make sense of these policies a lot more. But I also noticed that when I compared between the old policies and the new policies, the post-GDPR policies - some of them have gotten twice as long. KELLY: You have taken the time to read all these. What is your trick to them? And were there common things you're looking for as you're skimming through? STERN: Yeah. So two things kind of stick out. One, after you've read pretty much only, like, five of these, you start to notice a pattern - one, the company tells you what data is collected, what they're taking on you; two, they'll tell you why they're using that data - and this is a big part of GDPR, that these companies have to tell you how they're using that data and for what reason; and then part three is - most of these companies have now updated with where they give you the ability to have some control over what they do with your data. That doesn't mean they're not going to collect it. But they'll point you to some settings where you can control, do you want this information to be used to target ads at you? Do you not? And I would say the biggest takeaway I also had here is that nobody's going to read through all those one, two, three parts. But what you can do is search for keywords. And one thing that I sort of - at around Policy No. 20, I sort of gave up and said - OK, I'm going to search for these keywords. KELLY: (Laughter) Started just skimming fast, yeah. So what keywords should we search for? STERN: Well, third parties is a big one. That's really been, you know, the brunt of a lot of the privacy issues we've seen here in the U. S. with Cambridge Analytica and some of the election stuff is that some of this data was being given to third parties, to other apps. And many people were not aware of that. And so you should search for the term third parties and see where your data could end up going. KELLY: It sounds like the other takeaway is we still need to be proactive about this. STERN: Absolutely. I mean, none of these companies are going to come up to us, tap us on the shoulder and say, oh, hey, guess what - we took that information that you put in, and we decided to serve you a bunch more ads about that. Plus, we sold it to another company, so we made more money this quarter. That's not going to happen. KELLY: That is Joanna Stern, personal technology columnist for The Wall Street Journal. Joanna, thanks so much. STERN: Thanks. MARY LOUISE KELLY, HOST:  In Europe, a sweeping digital privacy law kicked in this past Friday. Companies have been updating their terms of service and their data security rules. And even here in the U. S. , we are feeling the impact. And that is where we kick off this week's All Tech Considered. (SOUNDBITE OF MUSIC) KELLY: Well, if you are like me, your email inbox has been inundated with messages about the new European rules. And if you're like me, you may not have read every line of the fine print of these emails yet. But The Wall Street Journal's personal technology columnist Joanna Stern has. She has diligently been reading dozens of these messages. And she joins us now. Hey, there. JOANNA STERN: Hi. How are you? KELLY: We are well. Thank you. I wonder if you would start by giving us the real quick version of this European law and what it's designed to do. STERN: Yeah. So GDPR, which stands for the General Data Protection Regulation, went into effect in the European Union. And this is a new set of laws that basically tightens what companies can do with our data. Overall, what most people need to know is this is supposed to be very good for consumers. And it makes companies be more transparent about what they're doing with what they collect on us. KELLY: OK. And one of the ways in which they're being transparent is emailing all of us with these long policies and updates to the rules. You actually - you made a video, which I want to play. It's up on the Wall Street Journal website. And this is you. You printed out paper copies of some of these policies and then lined them up on a football field. (SOUNDBITE OF VIDEO) STERN: One-hundred and twenty yards, 360 feet. . . (SOUNDBITE OF MUSIC) KELLY: A not-very-subtle message there that these policies are really long. STERN: One thing I quickly noticed from this and one thing that GDPR does require is that they have to be written in plain English. So I actually was able to make sense of these policies a lot more. But I also noticed that when I compared between the old policies and the new policies, the post-GDPR policies - some of them have gotten twice as long. KELLY: You have taken the time to read all these. What is your trick to them? And were there common things you're looking for as you're skimming through? STERN: Yeah. So two things kind of stick out. One, after you've read pretty much only, like, five of these, you start to notice a pattern - one, the company tells you what data is collected, what they're taking on you; two, they'll tell you why they're using that data - and this is a big part of GDPR, that these companies have to tell you how they're using that data and for what reason; and then part three is - most of these companies have now updated with where they give you the ability to have some control over what they do with your data. That doesn't mean they're not going to collect it. But they'll point you to some settings where you can control, do you want this information to be used to target ads at you? Do you not? And I would say the biggest takeaway I also had here is that nobody's going to read through all those one, two, three parts. But what you can do is search for keywords. And one thing that I sort of - at around Policy No. 20, I sort of gave up and said - OK, I'm going to search for these keywords. KELLY: (Laughter) Started just skimming fast, yeah. So what keywords should we search for? STERN: Well, third parties is a big one. That's really been, you know, the brunt of a lot of the privacy issues we've seen here in the U. S. with Cambridge Analytica and some of the election stuff is that some of this data was being given to third parties, to other apps. And many people were not aware of that. And so you should search for the term third parties and see where your data could end up going. KELLY: It sounds like the other takeaway is we still need to be proactive about this. STERN: Absolutely. I mean, none of these companies are going to come up to us, tap us on the shoulder and say, oh, hey, guess what - we took that information that you put in, and we decided to serve you a bunch more ads about that. Plus, we sold it to another company, so we made more money this quarter. That's not going to happen. KELLY: That is Joanna Stern, personal technology columnist for The Wall Street Journal. Joanna, thanks so much. STERN: Thanks.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-28-609790069": {"title": "Cryptocurrency Miners Make Big Promises In Small Towns : NPR", "url": "https://www.npr.org/2018/05/28/609790069/cryptocurrency-miners-make-big-promises-in-small-towns", "author": "No author found", "published_date": "2018-05-28", "content": "RACHEL MARTIN, HOST:  Like many American manufacturing towns, Massena, N. Y. , has hemorrhaged jobs in the past decade. GM and Reynolds Metal closed. Aluminum company Alcoa downsized. But now Massena is banking on a new opportunity to boost its economy - cryptocurrency. North Country Public Radio's David Sommerstein explains. DAVID SOMMERSTEIN, BYLINE: Massena is the archetype of the company town that lost its companies. Main Street is a mix of stately old buildings and empty union hall, a couple banks and restaurants and a bunch of vacant storefronts. So back in January, Town Supervisor Steven O'Shaughnessy was over the moon when all of a sudden, companies that do something called bitcoin mining were knocking on Massena's door. The town has something they really need. STEVEN O'SHAUGHNESSY: They need lots of power. They need to be able to count on it. And we can provide that. SOMMERSTEIN: Massena has some of the cheapest electricity in the Northeast. It comes from a hydropower dam nearby on the St. Lawrence River. A quick primer on bitcoin mining - bitcoin is the best-known of the digital or cryptocurrencies, which operate completely independently of banks. Instead, they rely on powerful computers around the world to verify every transaction that uses a digital coin. People operating those computers earn new coins for doing that verification work. That's called mining. So now companies want to set up huge warehouses full of computers to mine digital currency. The thing is, all this mining gobbles up vast amounts of electricity, and that's why a Hong Kong-based company called Digital Skynet Limited came to Massena and set up hundreds of computers in an old factory. TINA BARKSDALE: Yeah, these are the miners, and as you can hear, they make a lot of noise. SOMMERSTEIN: Tina Barksdale says Digital Skynet employs 20 people here now, but she imagines something much bigger - way more computers, but also, an innovative hub for the cryptocurrency technology, generating hundreds of jobs - the transformation for the town of Massena. BARKSDALE: From being just Massena, N. Y. , to being a cryptocurrency sort of Silicon Valley. But, of course, it all depends on the power. SOMMERSTEIN: When the value of a traded bitcoin soared to almost $20,000 late last year, a bitcoin-mining rush hit lots of places with cheap power. But it only takes so many employees to keep an eye on a bunch of computers, and in some places, the power-hungry miners started driving up electric rates for everybody else. Near Massena, the city of Plattsburgh, N. Y. , put a moratorium on new mines because of rising electric bills. And so local leaders, like State Senator Joe Griffo, began to ask, will bitcoin mining really create enough jobs to justify using all that electricity? And will those jobs last? JOE GRIFFO: I would not dismiss it out of hand, but I think you also have to look at things realistically. Where is bitcoin going to go over time? SOMMERSTEIN: Bitcoin's value has been all over the place. It steadily dropped most of this year. So the big question is, if the price keeps dropping, will miners stick around? Lex Sokolin of Autonomous Research in London studies digital currencies, and he believes they have a promising future, but he thinks it's inevitable developers will come up with a more energy-efficient way to mine them. LEX SOKOLIN: It just seems somewhat preposterous to me because the amount of electricity that's being used is growing, and growing and growing, and I think the community will try to find a different way to do this. SOMMERSTEIN: Towns like Massena would lose their advantage. Digital Skynet's Tina Barksdale says that's why Massena should embrace bitcoin mining now to establish its position in an emerging digital currency economy. For NPR News, I'm David Sommerstein in Massena, N. Y. (SOUNDBITE OF THE ALBUM LEAF'S \"SYNTHESIS\") RACHEL MARTIN, HOST:   Like many American manufacturing towns, Massena, N. Y. , has hemorrhaged jobs in the past decade. GM and Reynolds Metal closed. Aluminum company Alcoa downsized. But now Massena is banking on a new opportunity to boost its economy - cryptocurrency. North Country Public Radio's David Sommerstein explains. DAVID SOMMERSTEIN, BYLINE: Massena is the archetype of the company town that lost its companies. Main Street is a mix of stately old buildings and empty union hall, a couple banks and restaurants and a bunch of vacant storefronts. So back in January, Town Supervisor Steven O'Shaughnessy was over the moon when all of a sudden, companies that do something called bitcoin mining were knocking on Massena's door. The town has something they really need. STEVEN O'SHAUGHNESSY: They need lots of power. They need to be able to count on it. And we can provide that. SOMMERSTEIN: Massena has some of the cheapest electricity in the Northeast. It comes from a hydropower dam nearby on the St. Lawrence River. A quick primer on bitcoin mining - bitcoin is the best-known of the digital or cryptocurrencies, which operate completely independently of banks. Instead, they rely on powerful computers around the world to verify every transaction that uses a digital coin. People operating those computers earn new coins for doing that verification work. That's called mining. So now companies want to set up huge warehouses full of computers to mine digital currency. The thing is, all this mining gobbles up vast amounts of electricity, and that's why a Hong Kong-based company called Digital Skynet Limited came to Massena and set up hundreds of computers in an old factory. TINA BARKSDALE: Yeah, these are the miners, and as you can hear, they make a lot of noise. SOMMERSTEIN: Tina Barksdale says Digital Skynet employs 20 people here now, but she imagines something much bigger - way more computers, but also, an innovative hub for the cryptocurrency technology, generating hundreds of jobs - the transformation for the town of Massena. BARKSDALE: From being just Massena, N. Y. , to being a cryptocurrency sort of Silicon Valley. But, of course, it all depends on the power. SOMMERSTEIN: When the value of a traded bitcoin soared to almost $20,000 late last year, a bitcoin-mining rush hit lots of places with cheap power. But it only takes so many employees to keep an eye on a bunch of computers, and in some places, the power-hungry miners started driving up electric rates for everybody else. Near Massena, the city of Plattsburgh, N. Y. , put a moratorium on new mines because of rising electric bills. And so local leaders, like State Senator Joe Griffo, began to ask, will bitcoin mining really create enough jobs to justify using all that electricity? And will those jobs last? JOE GRIFFO: I would not dismiss it out of hand, but I think you also have to look at things realistically. Where is bitcoin going to go over time? SOMMERSTEIN: Bitcoin's value has been all over the place. It steadily dropped most of this year. So the big question is, if the price keeps dropping, will miners stick around? Lex Sokolin of Autonomous Research in London studies digital currencies, and he believes they have a promising future, but he thinks it's inevitable developers will come up with a more energy-efficient way to mine them. LEX SOKOLIN: It just seems somewhat preposterous to me because the amount of electricity that's being used is growing, and growing and growing, and I think the community will try to find a different way to do this. SOMMERSTEIN: Towns like Massena would lose their advantage. Digital Skynet's Tina Barksdale says that's why Massena should embrace bitcoin mining now to establish its position in an emerging digital currency economy. For NPR News, I'm David Sommerstein in Massena, N. Y. (SOUNDBITE OF THE ALBUM LEAF'S \"SYNTHESIS\")", "section": "National", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-05-31-615718566": {"title": "CryptoKitties: Using The Blockchain For Cat Pictures : NPR", "url": "https://www.npr.org/2018/05/31/615718566/cryptokitties-using-the-blockchain-for-cat-pictures", "author": "No author found", "published_date": "2018-05-31", "content": "RACHEL MARTIN, HOST: OK. This is a story about cat pictures, digital cat pictures called CryptoKitties. DAVID GREENE, HOST: CryptoKitties - people collect them online. They're these little cartoon cats generated programmatically with billions of possible combinations. They've got big-eyed expressions and pouting faces. And users are paying big money for them, even though at first they didn't do very much. But things are starting to change. ROHAM GHAREGOZLOU: People are making all kinds of add-on games. So you can dress up your kitty. You can take it racing. You can train it in kitty battles. GREENE: That is Roham Gharegozlou. He's the CEO of CryptoKitties. GHAREGOZLOU: There's even a social network for kitties where you can give it a personality and start making friends. GREENE: Friends? Friends for a virtual cat? OK. MARTIN: The cats have gained traction in part because they're built on a trendy technology called the blockchain. It's a kind of public receipt that logs every cat and who it belongs to. It's the same tech that bitcoin and other cryptocurrencies are built on. It means the cats can't be taken away or copied or destroyed. GHAREGOZLOU: Cat pictures on the internet aren't anything new. But until blockchain, these weren't real. You can send copies back and forth, but no one really owns the real thing. There's no way to prove it. And that's what blockchain does. For the first time, a digital item can be real. It can be impossible to counterfeit. MARTIN: You can even breed them, with each new virtual cat passing along its unique attributes - cattributes (ph). GREENE: Nice. Humans have paid about $24 million for CryptoKitties since this company launched in November. GHAREGOZLOU: There was an auction in New York where one of our cats was actually displayed at Christie's New York. And there was a live auction. And the money went to charity, but it was about $140,000 that that specific kitty sold for. GREENE: Some have turned buying and selling CryptoKitties into a really profitable business. We just hope that if this turns out to be a fad that fades, these kitties aren't going to end up in the cryptopound. (SOUNDBITE OF ANAMANAGUCHI'S \"MEOW\") RACHEL MARTIN, HOST:  OK. This is a story about cat pictures, digital cat pictures called CryptoKitties. DAVID GREENE, HOST:  CryptoKitties - people collect them online. They're these little cartoon cats generated programmatically with billions of possible combinations. They've got big-eyed expressions and pouting faces. And users are paying big money for them, even though at first they didn't do very much. But things are starting to change. ROHAM GHAREGOZLOU: People are making all kinds of add-on games. So you can dress up your kitty. You can take it racing. You can train it in kitty battles. GREENE: That is Roham Gharegozlou. He's the CEO of CryptoKitties. GHAREGOZLOU: There's even a social network for kitties where you can give it a personality and start making friends. GREENE: Friends? Friends for a virtual cat? OK. MARTIN: The cats have gained traction in part because they're built on a trendy technology called the blockchain. It's a kind of public receipt that logs every cat and who it belongs to. It's the same tech that bitcoin and other cryptocurrencies are built on. It means the cats can't be taken away or copied or destroyed. GHAREGOZLOU: Cat pictures on the internet aren't anything new. But until blockchain, these weren't real. You can send copies back and forth, but no one really owns the real thing. There's no way to prove it. And that's what blockchain does. For the first time, a digital item can be real. It can be impossible to counterfeit. MARTIN: You can even breed them, with each new virtual cat passing along its unique attributes - cattributes (ph). GREENE: Nice. Humans have paid about $24 million for CryptoKitties since this company launched in November. GHAREGOZLOU: There was an auction in New York where one of our cats was actually displayed at Christie's New York. And there was a live auction. And the money went to charity, but it was about $140,000 that that specific kitty sold for. GREENE: Some have turned buying and selling CryptoKitties into a really profitable business. We just hope that if this turns out to be a fad that fades, these kitties aren't going to end up in the cryptopound. (SOUNDBITE OF ANAMANAGUCHI'S \"MEOW\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-04-616979834": {"title": "Apple CEO Tim Cook Says Company Hasn't Collected Data Available From Facebook : NPR", "url": "https://www.npr.org/2018/06/04/616979834/apple-ceo-tim-cook-says-company-hasnt-collected-data-available-from-facebook", "author": "No author found", "published_date": "2018-06-04", "content": "ARI SHAPIRO, HOST: Today NPR spoke directly with Apple CEO Tim Cook about the revelations that hardware makers had access to personal data in the Facebook app. NPR's Laura Sydell was there and joins us now. Hi, Laura. LAURA SYDELL, BYLINE: Hello. SHAPIRO: So what did Tim Cook say about this New York Times report that Apple may have had access to personal data collected by Facebook? SYDELL: Well, he doesn't deny that Apple may very well have had access to this information, but he says they would have no reason to use it. They wouldn't bother to collect it. And here he is responding specifically. He was talking with me and NPR Morning Edition host Steve Inskeep. TIM COOK: The things mentioned in the Times article about relationship statuses and all this kind of stuff - this is so foreign to us and not data that we have ever received at all or requested - zero. What we did was we integrated the ability to share in the operating system, make it simple to share a photo and that sort of thing. So it's a convenience for the user. We weren't in the data business. We've never been in the data business. SYDELL: You know, Apple makes hardware. It's the core of Apple's business. They have some services. They have Apple Pay. They have music. But it really isn't important to Apple to collect a lot of data on you so they can target average advertising to you. SHAPIRO: So is Tim Cook opposed to the Facebook model of a free service with online advertising? SYDELL: You know, he didn't say he was opposed to it. He said it's a question of degree. You shouldn't just be randomly collecting as much data from people as possible. You can still have fairly targeted ads without excessively collecting personal information. SHAPIRO: OK, so Apple is not in the data business. This all happened during Apple's Worldwide Developers Conference. Did they make any announcements related to this? SYDELL: Well, I should be clear that, you know, Tim Cook has been very critical of Facebook's model which relies on just collecting lots and lots of information. And today they actually took steps that could make it a little harder for Facebook to collect data on people who are using Macs or Apple products. So Apple has made it easier to keep your computer anonymous while you're searching the web if you're using their browser. So if you visit a particular site, often sites are able to see unique identifiers for your computer, and then they follow you around after you leave the site. So Apple found a way to mitigate that. So it now says your computer could look like any other Mac. In addition, Apple says when your browser is searching the web, they've made it more difficult for, say, a company like Facebook to track where you go, and that is definitely a way that Facebook collects information. Later when we spoke with Tim Cook, we asked, did they introduce this as a way to directly target Facebook? And he said, no, they're simply just trying to give their users more privacy, and that may affect Facebook. SHAPIRO: That's NPR's Laura Sydell speaking with us from Silicon Valley. Thanks, Laura. SYDELL: You're welcome. ARI SHAPIRO, HOST:  Today NPR spoke directly with Apple CEO Tim Cook about the revelations that hardware makers had access to personal data in the Facebook app. NPR's Laura Sydell was there and joins us now. Hi, Laura. LAURA SYDELL, BYLINE: Hello. SHAPIRO: So what did Tim Cook say about this New York Times report that Apple may have had access to personal data collected by Facebook? SYDELL: Well, he doesn't deny that Apple may very well have had access to this information, but he says they would have no reason to use it. They wouldn't bother to collect it. And here he is responding specifically. He was talking with me and NPR Morning Edition host Steve Inskeep. TIM COOK: The things mentioned in the Times article about relationship statuses and all this kind of stuff - this is so foreign to us and not data that we have ever received at all or requested - zero. What we did was we integrated the ability to share in the operating system, make it simple to share a photo and that sort of thing. So it's a convenience for the user. We weren't in the data business. We've never been in the data business. SYDELL: You know, Apple makes hardware. It's the core of Apple's business. They have some services. They have Apple Pay. They have music. But it really isn't important to Apple to collect a lot of data on you so they can target average advertising to you. SHAPIRO: So is Tim Cook opposed to the Facebook model of a free service with online advertising? SYDELL: You know, he didn't say he was opposed to it. He said it's a question of degree. You shouldn't just be randomly collecting as much data from people as possible. You can still have fairly targeted ads without excessively collecting personal information. SHAPIRO: OK, so Apple is not in the data business. This all happened during Apple's Worldwide Developers Conference. Did they make any announcements related to this? SYDELL: Well, I should be clear that, you know, Tim Cook has been very critical of Facebook's model which relies on just collecting lots and lots of information. And today they actually took steps that could make it a little harder for Facebook to collect data on people who are using Macs or Apple products. So Apple has made it easier to keep your computer anonymous while you're searching the web if you're using their browser. So if you visit a particular site, often sites are able to see unique identifiers for your computer, and then they follow you around after you leave the site. So Apple found a way to mitigate that. So it now says your computer could look like any other Mac. In addition, Apple says when your browser is searching the web, they've made it more difficult for, say, a company like Facebook to track where you go, and that is definitely a way that Facebook collects information. Later when we spoke with Tim Cook, we asked, did they introduce this as a way to directly target Facebook? And he said, no, they're simply just trying to give their users more privacy, and that may affect Facebook. SHAPIRO: That's NPR's Laura Sydell speaking with us from Silicon Valley. Thanks, Laura. SYDELL: You're welcome.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-04-616280585": {"title": "Apple Requested 'Zero' Personal Data In Deals With Facebook, CEO Tim Cook Says : NPR", "url": "https://www.npr.org/2018/06/04/616280585/apple-requested-zero-personal-data-in-deals-with-facebook-ceo-tim-cook-says", "author": "No author found", "published_date": "2018-06-04", "content": "STEVE INSKEEP, HOST:  And I'm Steve Inskeep in San Francisco. Just south of here yesterday, the CEO of Apple took the stage. (SOUNDBITE OF APPLE WORLDWIDE DEVELOPER CONFERENCE)TIM COOK: Good morning. (APPLAUSE)COOK: Good morning. INSKEEP: Tim Cook had reason to be cheerful. He runs the world's most valuable company worth close to a trillion dollars. He was addressing software developers, promoting new features to the operating systems of the iPhone and the iPad and the Apple Watch. (SOUNDBITE OF APPLE WORLDWIDE DEVELOPER CONFERENCE)COOK: Changing the world and making it a better place is what it's all about for us. INSKEEP: That's the motto for much of the tech industry, though it faces unprecedented questions about whether the industry really is making the world better. Apple's shareholders recently warned of growing societal unease about the mental health effects of too much phone use. So now Apple says a new feature will make it easier to track what you or your children have been doing on a device. COOK: So you know how much time you're spending, where you're spending it, how many times per hour you're picking up a device, how many notifications you get, who's sending those to you. INSKEEP: That's how Tim Cook described it when he sat with us after his speech. What, if anything, bothers you about the amount of time people are spending on your phones? COOK: Well, if you back up and think about what we're about, we've never been about maximizing usage of our devices. It's never been a focus of ours. There's clearly users out there that are worried about the amount of time they're spending, or the amount of distraction or interruptions they get. INSKEEP: I want to ask about your thinking, though, because you could say, I'm responding to demand and there's some shareholders that wanted this, but are you actually bothered? Are you actually concerned by the possibility that what you argue is a social good could be a social detriment? COOK: I think there are cases in life where anything good used to the extreme becomes not good. And so I think you can depend on your device so much and spend so much time on certain apps or pick up your phone so many times during the day that this is no longer good. INSKEEP: But this was literally your ambition as a company, right, to make sure that people use everything? COOK: No. That's the interesting thing. We're in a very unique position because we had never been about maximizing the number of times you pick it up, the number of hours that you use it. . . INSKEEP: But you maximize the number of things you can do with it. That's for sure. COOK: We do that, yes, because we want it to be an incredible device for you. And so, you know, we've provided a way for you to have health information on it, financial information, you can pay with it. But if you're getting bombarded by notifications all day long, that's probably a use of the system that might not be so good anymore. INSKEEP: Is there a little bit of conflict that you have to think about because you are offering people ways to limit their screen time while speaking to this giant conference of thousands of people, developers, who are in business to go sell products through your app store that would encourage people to use the phone more? COOK: No. I don't see it as that because we always put the user at the center. And so our question is always, what is in their best interest? INSKEEP: Do you agree with people who use the word addiction, talk about addiction to smartphones? COOK: I'm not a clinician, and so I don't know. What I do know is that you can use something too much. And I'm concerned about it. But what we want to do is provide you great tools so you can make a judgment for yourself. INSKEEP: You also made some news by announcing some changes to your Web browser and elsewhere, which you say will make it more difficult for outside parties to track what you do online, to track your data. Are you deliberately making your platforms more hostile for companies that do business like Facebook, which was mentioned in your presentation? COOK: We're not targeting any single company. We're targeting a practice of people collecting information without the vast majority of users knowing that it's being collected. We think that when a person leaves one website and goes to another and another and another, they do not have a reasonable expectation that that original website is still following their every move. And so we want to do what we can do there to try to prevent that. But we're not taking the ability of if somebody, you know, is on the NPR app, we think it's fine that, in rational, that NPR knows that person is there and knows something about them, assuming the user has elected to share that. What we don't think is, when you leave there, we don't think that the surveillance engine should stay. INSKEEP: I want people to know that you have emphasized that your company does not intend to make money off of data mining, does not intend to make money off of advertising. You make money by selling hardware. But you're addressing a developers conference here, where theres a lot of app developers who may well rely on advertising, who may well rely on data. COOK: Well, let me let me correct something you said there. We don't believe there's anything wrong with digital advertising. It's a key part of people's business models, and we think it's perfectly fine. It's the crafting of a detailed profile, and tracking you in places where you don't reasonably expect to be tracked and companies gathering information well beyond what you would have voluntarily shared if you knew what they were doing. INSKEEP: Is there a better business model than the business model that lots of companies are using? COOK: Again, on the digital - we have no issue with digital advertising. INSKEEP: Excessive data tracking, you do have an issue. COOK: It's the collection of information beyond what the person is fully and completely aware is taking place and the building of this detailed profile. I think that steps over a boundary. INSKEEP: That's some of our talk with Apple CEO Tim Cook here in California. We're hearing him throughout the morning. NPR digital culture correspondent Laura Sydell was with us for that talk and is with us now. Hey, Laura. LAURA SYDELL, BYLINE: Good morning. INSKEEP: So we heard Apple pushing back on Facebook there, but just a couple days ago there was this story suggesting that Apple in some cases has been in league with Facebook. SYDELL: That is right. As a matter of fact, what was reported was that Facebook made it possible for the hardware manufacturers like Apple or Samsung to essentially get all kinds of personal information off their app, including your friends and your friends' personal information. INSKEEP: OK. That's what the story said. What did Tim Cook say? SYDELL: Tim Cook says we could have done it, but we didn't do it. All we did was try to make things more convenient so that maybe you could post something to - you could post a photo on Facebook, and that's about it. They are not in the business, as he said, of getting users' data. INSKEEP: Although they are working pretty closely with Facebook, as you see from examples. . . SYDELL: Yes. And, in fact, a lot of these apps do it. So even though they are not doing it, the apps in their app store may very well be involved in it. INSKEEP: NPR's digital culture correspondent Laura Sydell this morning talking about her interview with Apple's Tim Cook. STEVE INSKEEP, HOST:   And I'm Steve Inskeep in San Francisco. Just south of here yesterday, the CEO of Apple took the stage. (SOUNDBITE OF APPLE WORLDWIDE DEVELOPER CONFERENCE) TIM COOK: Good morning. (APPLAUSE) COOK: Good morning. INSKEEP: Tim Cook had reason to be cheerful. He runs the world's most valuable company worth close to a trillion dollars. He was addressing software developers, promoting new features to the operating systems of the iPhone and the iPad and the Apple Watch. (SOUNDBITE OF APPLE WORLDWIDE DEVELOPER CONFERENCE) COOK: Changing the world and making it a better place is what it's all about for us. INSKEEP: That's the motto for much of the tech industry, though it faces unprecedented questions about whether the industry really is making the world better. Apple's shareholders recently warned of growing societal unease about the mental health effects of too much phone use. So now Apple says a new feature will make it easier to track what you or your children have been doing on a device. COOK: So you know how much time you're spending, where you're spending it, how many times per hour you're picking up a device, how many notifications you get, who's sending those to you. INSKEEP: That's how Tim Cook described it when he sat with us after his speech. What, if anything, bothers you about the amount of time people are spending on your phones? COOK: Well, if you back up and think about what we're about, we've never been about maximizing usage of our devices. It's never been a focus of ours. There's clearly users out there that are worried about the amount of time they're spending, or the amount of distraction or interruptions they get. INSKEEP: I want to ask about your thinking, though, because you could say, I'm responding to demand and there's some shareholders that wanted this, but are you actually bothered? Are you actually concerned by the possibility that what you argue is a social good could be a social detriment? COOK: I think there are cases in life where anything good used to the extreme becomes not good. And so I think you can depend on your device so much and spend so much time on certain apps or pick up your phone so many times during the day that this is no longer good. INSKEEP: But this was literally your ambition as a company, right, to make sure that people use everything? COOK: No. That's the interesting thing. We're in a very unique position because we had never been about maximizing the number of times you pick it up, the number of hours that you use it. . . INSKEEP: But you maximize the number of things you can do with it. That's for sure. COOK: We do that, yes, because we want it to be an incredible device for you. And so, you know, we've provided a way for you to have health information on it, financial information, you can pay with it. But if you're getting bombarded by notifications all day long, that's probably a use of the system that might not be so good anymore. INSKEEP: Is there a little bit of conflict that you have to think about because you are offering people ways to limit their screen time while speaking to this giant conference of thousands of people, developers, who are in business to go sell products through your app store that would encourage people to use the phone more? COOK: No. I don't see it as that because we always put the user at the center. And so our question is always, what is in their best interest? INSKEEP: Do you agree with people who use the word addiction, talk about addiction to smartphones? COOK: I'm not a clinician, and so I don't know. What I do know is that you can use something too much. And I'm concerned about it. But what we want to do is provide you great tools so you can make a judgment for yourself. INSKEEP: You also made some news by announcing some changes to your Web browser and elsewhere, which you say will make it more difficult for outside parties to track what you do online, to track your data. Are you deliberately making your platforms more hostile for companies that do business like Facebook, which was mentioned in your presentation? COOK: We're not targeting any single company. We're targeting a practice of people collecting information without the vast majority of users knowing that it's being collected. We think that when a person leaves one website and goes to another and another and another, they do not have a reasonable expectation that that original website is still following their every move. And so we want to do what we can do there to try to prevent that. But we're not taking the ability of if somebody, you know, is on the NPR app, we think it's fine that, in rational, that NPR knows that person is there and knows something about them, assuming the user has elected to share that. What we don't think is, when you leave there, we don't think that the surveillance engine should stay. INSKEEP: I want people to know that you have emphasized that your company does not intend to make money off of data mining, does not intend to make money off of advertising. You make money by selling hardware. But you're addressing a developers conference here, where theres a lot of app developers who may well rely on advertising, who may well rely on data. COOK: Well, let me let me correct something you said there. We don't believe there's anything wrong with digital advertising. It's a key part of people's business models, and we think it's perfectly fine. It's the crafting of a detailed profile, and tracking you in places where you don't reasonably expect to be tracked and companies gathering information well beyond what you would have voluntarily shared if you knew what they were doing. INSKEEP: Is there a better business model than the business model that lots of companies are using? COOK: Again, on the digital - we have no issue with digital advertising. INSKEEP: Excessive data tracking, you do have an issue. COOK: It's the collection of information beyond what the person is fully and completely aware is taking place and the building of this detailed profile. I think that steps over a boundary. INSKEEP: That's some of our talk with Apple CEO Tim Cook here in California. We're hearing him throughout the morning. NPR digital culture correspondent Laura Sydell was with us for that talk and is with us now. Hey, Laura. LAURA SYDELL, BYLINE: Good morning. INSKEEP: So we heard Apple pushing back on Facebook there, but just a couple days ago there was this story suggesting that Apple in some cases has been in league with Facebook. SYDELL: That is right. As a matter of fact, what was reported was that Facebook made it possible for the hardware manufacturers like Apple or Samsung to essentially get all kinds of personal information off their app, including your friends and your friends' personal information. INSKEEP: OK. That's what the story said. What did Tim Cook say? SYDELL: Tim Cook says we could have done it, but we didn't do it. All we did was try to make things more convenient so that maybe you could post something to - you could post a photo on Facebook, and that's about it. They are not in the business, as he said, of getting users' data. INSKEEP: Although they are working pretty closely with Facebook, as you see from examples. . . SYDELL: Yes. And, in fact, a lot of these apps do it. So even though they are not doing it, the apps in their app store may very well be involved in it. INSKEEP: NPR's digital culture correspondent Laura Sydell this morning talking about her interview with Apple's Tim Cook.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-04-616917807": {"title": "How Facebook's Data-Sharing Agreement With Device Makers Could Affect Users : NPR", "url": "https://www.npr.org/2018/06/04/616917807/how-facebooks-data-sharing-agreement-with-device-makers-could-affect-users", "author": "No author found", "published_date": "2018-06-04", "content": "MARY LOUISE KELLY, HOST: And I'm Mary Louise Kelly with more questions about how Facebook treats our personal data. It is All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")KELLY: The New York Times reports that Facebook has had data-sharing agreements with at least 60 other tech companies, including giants like Apple and Amazon and Microsoft. Basically, these companies could access information on Facebook users and their Facebook friends, all without their explicit consent. Well, let's bring in one of the Times reporters who worked on this story, investigative reporter Michael LaForgia. Welcome. MICHAEL LAFORGIA: Thank you. KELLY: Let me start by asking you to paint a picture of how you went about reporting this. I gather you actually tried this out yourself. You got a Blackberry phone. LAFORGIA: That's right. We plugged in my Facebook account information to this BlackBerry phone after deleting the app. And right away, it started sucking down all kinds of information. It got my email address and cellphone number, both sides of my private messages and the names and user IDs of the people I exchanged messages with. KELLY: You say you deleted the Facebook app. So you were not logged on to Facebook. This is something the phone was able to access through these other companies? LAFORGIA: Right. We got the names, birthdays, work and education histories of nearly all of my 550 friends. And then we were actually able to push it a little further and recover the names and pretty important user ID from their friends, which ended up taking us from about 550 people to about 295,000 people just from a single account. KELLY: Two-hundred-and-ninety-five-thousand people - what? - because of all of the friends of their Facebook friends? LAFORGIA: That's right. KELLY: Help me reconcile your story today with comments from Facebook. I mean, it was just a couple of months ago that Facebook CEO Mark Zuckerberg was testifying before Congress. And he said Facebook doesn't share data with third parties anymore and hasn't for a couple of years. So what gives here? LAFORGIA: This was something that we also spent a long time trying to understand. And Facebook has maintained, since we first brought these questions to them, that because these deals are governed by very strict contracts that there is much stricter oversight than there was over third-party app developers. KELLY: Bottom line - if I am a Facebook user and I read this headline, you know, from your story today that Facebook is sharing information with another company that I may or may not have been aware about, how worried should I be? Should I be worried? LAFORGIA: Well, I mean, first of all, there's obvious value in being able to open your photo album and post something directly to Facebook without opening the app. KELLY: You're saying there are practical ways this may be really helpful to Facebook users. LAFORGIA: Right - or to view your friends' birthdays right from your native contact book or calendar apps. And the reality is it's unlikely that these companies are doing anything untoward with this information according to experts that we've talked to. But the danger comes in when these companies either store this information on their own servers or expose it on the device to third-party apps that can sync up with it and then do who knows what with it. KELLY: What's Facebook's saying in response to your story? LAFORGIA: Facebook has put out a statement saying it disagrees with our story. But they haven't disputed any of the facts. They say that in the cases of these device partnerships, they consider the outside companies extensions of Facebook, not third parties. They also appear to be making the case that when my device collects the data of my friends, I then own that information. And it's up to me whether I want to risk sharing it or not by syncing it up with other apps. KELLY: Putting some of the onus on Facebook users to take responsibility for being aware of how their data may be used. LAFORGIA: That seems to be the case. Right. KELLY: And what's your takeaway? If I could ask you to take off your reporter hat for a second and put on your Facebook user hat, are you concerned by what your reporting has uncovered? LAFORGIA: It turns out that I did use a BlackBerry device to access personal information of about 550 of my closest friends, so I probably owe several hundred people. . . KELLY: (Laughter). LAFORGIA: . . . An apology. But I will not be issuing that apology over Facebook - maybe a postcard. Who knows? KELLY: That is New York Times investigative reporter Michael LaForgia. Thanks so much. LAFORGIA: Thanks for having me. MARY LOUISE KELLY, HOST:  And I'm Mary Louise Kelly with more questions about how Facebook treats our personal data. It is All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") KELLY: The New York Times reports that Facebook has had data-sharing agreements with at least 60 other tech companies, including giants like Apple and Amazon and Microsoft. Basically, these companies could access information on Facebook users and their Facebook friends, all without their explicit consent. Well, let's bring in one of the Times reporters who worked on this story, investigative reporter Michael LaForgia. Welcome. MICHAEL LAFORGIA: Thank you. KELLY: Let me start by asking you to paint a picture of how you went about reporting this. I gather you actually tried this out yourself. You got a Blackberry phone. LAFORGIA: That's right. We plugged in my Facebook account information to this BlackBerry phone after deleting the app. And right away, it started sucking down all kinds of information. It got my email address and cellphone number, both sides of my private messages and the names and user IDs of the people I exchanged messages with. KELLY: You say you deleted the Facebook app. So you were not logged on to Facebook. This is something the phone was able to access through these other companies? LAFORGIA: Right. We got the names, birthdays, work and education histories of nearly all of my 550 friends. And then we were actually able to push it a little further and recover the names and pretty important user ID from their friends, which ended up taking us from about 550 people to about 295,000 people just from a single account. KELLY: Two-hundred-and-ninety-five-thousand people - what? - because of all of the friends of their Facebook friends? LAFORGIA: That's right. KELLY: Help me reconcile your story today with comments from Facebook. I mean, it was just a couple of months ago that Facebook CEO Mark Zuckerberg was testifying before Congress. And he said Facebook doesn't share data with third parties anymore and hasn't for a couple of years. So what gives here? LAFORGIA: This was something that we also spent a long time trying to understand. And Facebook has maintained, since we first brought these questions to them, that because these deals are governed by very strict contracts that there is much stricter oversight than there was over third-party app developers. KELLY: Bottom line - if I am a Facebook user and I read this headline, you know, from your story today that Facebook is sharing information with another company that I may or may not have been aware about, how worried should I be? Should I be worried? LAFORGIA: Well, I mean, first of all, there's obvious value in being able to open your photo album and post something directly to Facebook without opening the app. KELLY: You're saying there are practical ways this may be really helpful to Facebook users. LAFORGIA: Right - or to view your friends' birthdays right from your native contact book or calendar apps. And the reality is it's unlikely that these companies are doing anything untoward with this information according to experts that we've talked to. But the danger comes in when these companies either store this information on their own servers or expose it on the device to third-party apps that can sync up with it and then do who knows what with it. KELLY: What's Facebook's saying in response to your story? LAFORGIA: Facebook has put out a statement saying it disagrees with our story. But they haven't disputed any of the facts. They say that in the cases of these device partnerships, they consider the outside companies extensions of Facebook, not third parties. They also appear to be making the case that when my device collects the data of my friends, I then own that information. And it's up to me whether I want to risk sharing it or not by syncing it up with other apps. KELLY: Putting some of the onus on Facebook users to take responsibility for being aware of how their data may be used. LAFORGIA: That seems to be the case. Right. KELLY: And what's your takeaway? If I could ask you to take off your reporter hat for a second and put on your Facebook user hat, are you concerned by what your reporting has uncovered? LAFORGIA: It turns out that I did use a BlackBerry device to access personal information of about 550 of my closest friends, so I probably owe several hundred people. . . KELLY: (Laughter). LAFORGIA: . . . An apology. But I will not be issuing that apology over Facebook - maybe a postcard. Who knows? KELLY: That is New York Times investigative reporter Michael LaForgia. Thanks so much. LAFORGIA: Thanks for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-04-616917855": {"title": "Toyota's V-2-V Technology Would Allow Cars To Talk To Each Other On The Highway : NPR", "url": "https://www.npr.org/2018/06/04/616917855/toyotas-v-2-v-technology-would-allow-cars-to-talk-to-each-other-on-the-highway", "author": "No author found", "published_date": "2018-06-04", "content": "MARY LOUISE KELLY, HOST: If only cars could talk to each other. Actually, they can. At least there's technology out there that could get them sharing information with other cars, which could potentially save thousands of lives every year. Michigan Radio's Tracy Samilton has been reporting on this and explains there is a high-stakes game of chicken going on when it comes to actually putting the technology in every car. TRACY SAMILTON, BYLINE: You and I, we're the good drivers, right? We've never cut someone off in our blind spot. We've never run a red light or not noticed a car in front slowing down and nearly hit them. But all those other drivers, they do that a lot. And it's dangerous. What if our cars had devices that could communicate with other cars and help us avoid accidents? Well, that technology, using GPS and Wi-Fi, exists. It's called vehicle-to-vehicle, or V2V. CEM SARAYDAR: It enhances your senses, in a way, to give you more time to react to potentially dangerous events. SAMILTON: That's Cem Saraydar with General Motors. GM was the first in the U. S. to put V2V in a car, namely the Cadillac CTS. Saraydar says the potential is huge. It could prevent up to 80 percent of accidents involving non-impaired drivers, potentially saving 15,000 lives a year. Saraydar has arranged a demonstration. And we get into a CTS with researcher at Vivek Vijaya Kumar behind the wheel. Another driver is in a different CTS. VIVEK VIJAYA KUMAR: So each vehicle transmits or broadcasts information such as its position, speed, heading and so on. SAMILTON: There's different warnings for different situations. If the CTS in front suddenly brakes, this lady tells you to watch out. (SOUNDBITE OF ARCHIVED RECORDING)COMPUTER-GENERATED VOICE: Hard braking ahead. SAMILTON: The seat can vibrate on the right or the left to call out dangers on either side of the car. In one scenario, we're creeping along, about to enter a road from the parking lot. Bushes are obscuring our view of the car that's about to be in our path. The seat vibrates, and there's this. (SOUNDBITE OF BEEPING)SAMILTON: Get your attention? That's the point. And if you're about to plow into the back of another car, you'll get a whole slew of warnings. . . (SOUNDBITE OF BEEPING)SAMILTON: . . . Plus a projection on the windshield to warn you. Automakers say the technology has proven itself and is ready for prime time. So what's holding it up? A final regulation mandating V2V has languished in both the Obama and Trump administrations. So now Toyota, in a challenge to competitors, says it will voluntarily put V2V in most of its cars - by the mid-2020s. Toyota's John Kenney says this technology is really V2E, vehicle-to-everything. Cars can communicate with traffic lights that are sending signals. If people carried little clip-on devices or put apps on their iPhones. . . JOHN KENNEY: We can also communicate with pedestrians and motorcyclists and bicyclists and road workers to make them safer. SAMILTON: Proponents are really frustrated by the delay. Jim Sayer heads the University of Michigan Transportation Research Institute. The group conducted one of the first pilots of V2V seven years ago. Sayer says while V2V or V2E isn't as sexy as autonomous vehicles, it's safe. And it works. JIM SAYER: Every year we wait to require these devices, thousands of lives are being lost that could otherwise be saved. SAMILTON: And there's yet another reason for urgency. The FCC has set aside a frequency for V2V, but other industries are eyeing it hungrily. For NPR News, I'm Tracy Samilton. (SOUNDBITE OF BLACK MILK'S \"WHEN THE SKY FALLS\") MARY LOUISE KELLY, HOST:  If only cars could talk to each other. Actually, they can. At least there's technology out there that could get them sharing information with other cars, which could potentially save thousands of lives every year. Michigan Radio's Tracy Samilton has been reporting on this and explains there is a high-stakes game of chicken going on when it comes to actually putting the technology in every car. TRACY SAMILTON, BYLINE: You and I, we're the good drivers, right? We've never cut someone off in our blind spot. We've never run a red light or not noticed a car in front slowing down and nearly hit them. But all those other drivers, they do that a lot. And it's dangerous. What if our cars had devices that could communicate with other cars and help us avoid accidents? Well, that technology, using GPS and Wi-Fi, exists. It's called vehicle-to-vehicle, or V2V. CEM SARAYDAR: It enhances your senses, in a way, to give you more time to react to potentially dangerous events. SAMILTON: That's Cem Saraydar with General Motors. GM was the first in the U. S. to put V2V in a car, namely the Cadillac CTS. Saraydar says the potential is huge. It could prevent up to 80 percent of accidents involving non-impaired drivers, potentially saving 15,000 lives a year. Saraydar has arranged a demonstration. And we get into a CTS with researcher at Vivek Vijaya Kumar behind the wheel. Another driver is in a different CTS. VIVEK VIJAYA KUMAR: So each vehicle transmits or broadcasts information such as its position, speed, heading and so on. SAMILTON: There's different warnings for different situations. If the CTS in front suddenly brakes, this lady tells you to watch out. (SOUNDBITE OF ARCHIVED RECORDING) COMPUTER-GENERATED VOICE: Hard braking ahead. SAMILTON: The seat can vibrate on the right or the left to call out dangers on either side of the car. In one scenario, we're creeping along, about to enter a road from the parking lot. Bushes are obscuring our view of the car that's about to be in our path. The seat vibrates, and there's this. (SOUNDBITE OF BEEPING) SAMILTON: Get your attention? That's the point. And if you're about to plow into the back of another car, you'll get a whole slew of warnings. . . (SOUNDBITE OF BEEPING) SAMILTON: . . . Plus a projection on the windshield to warn you. Automakers say the technology has proven itself and is ready for prime time. So what's holding it up? A final regulation mandating V2V has languished in both the Obama and Trump administrations. So now Toyota, in a challenge to competitors, says it will voluntarily put V2V in most of its cars - by the mid-2020s. Toyota's John Kenney says this technology is really V2E, vehicle-to-everything. Cars can communicate with traffic lights that are sending signals. If people carried little clip-on devices or put apps on their iPhones. . . JOHN KENNEY: We can also communicate with pedestrians and motorcyclists and bicyclists and road workers to make them safer. SAMILTON: Proponents are really frustrated by the delay. Jim Sayer heads the University of Michigan Transportation Research Institute. The group conducted one of the first pilots of V2V seven years ago. Sayer says while V2V or V2E isn't as sexy as autonomous vehicles, it's safe. And it works. JIM SAYER: Every year we wait to require these devices, thousands of lives are being lost that could otherwise be saved. SAMILTON: And there's yet another reason for urgency. The FCC has set aside a frequency for V2V, but other industries are eyeing it hungrily. For NPR News, I'm Tracy Samilton. (SOUNDBITE OF BLACK MILK'S \"WHEN THE SKY FALLS\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-05-616980803": {"title": "Apple CEO Tim Cook On Screen Time Controls, Working With China: Transcript : NPR", "url": "https://www.npr.org/2018/06/05/616980803/transcript-apple-ceo-tim-cook-on-screen-time-controls-working-with-china", "author": "No author found", "published_date": "2018-06-05", "content": "", "section": "Technology", "disclaimer": ""}, "2018-06-06-617659560": {"title": "Facebook's New Features Encourage Users To Telegraph Their Music Taste : NPR", "url": "https://www.npr.org/2018/06/06/617659560/facebooks-new-music-features-encourage-users-to-telegraph-their-taste", "author": "No author found", "published_date": "2018-06-06", "content": "", "section": "Music News", "disclaimer": ""}, "2018-06-06-617676327": {"title": "What The Controversy Over Facebook's Privacy Policy Reveals About The Company And Us : NPR", "url": "https://www.npr.org/2018/06/06/617676327/what-the-controversy-over-facebooks-privacy-policy-reveals-about-the-company-and", "author": "No author found", "published_date": "2018-06-06", "content": "MARY LOUISE KELLY, HOST: Revelations keep rolling in about how Facebook shares its users' data without their consent. The New York Times reported this week that at least 60 device makers had secret agreements with Facebook. Now Facebook has confirmed that among those 60 are four Chinese companies. The U. S. government considers one of them a national security threat. ARI SHAPIRO, HOST: These scandals and others do not seem to be hurting Facebook's bottom line much. In fact, last week, its stock hit its highest point of the year. And while it has fallen a bit since, there are no signs of a user mass exodus. So how much do we actually know or care about our privacy on Facebook and other platforms? NPR's social science correspondent Shankar Vedantam joins us to discuss that. Hi, Shankar. SHANKAR VEDANTAM, BYLINE: Hi, Ari. SHAPIRO: So lots of people say, I can't believe Facebook is using this with my data. Do people really care? - 'cause we're not seeing them unsubscribe en masse, as far as we can tell. VEDANTAM: Well, to be clear, Ari, there are two separate strands here. If Facebook has shared information in violation of its legal commitments to users, that's a legal and regulatory matter. There's a separate strand here that's psychological. When it comes to, you know, the garden variety exploitation of our information, Facebook and other companies in Silicon Valley often point out that they have asked for and obtain our permission to share our information. SHAPIRO: Right, at the end of an 8,000-page privacy policy that appears on our small iPhone screen. VEDANTAM: Exactly, Ari. In other words, we have given consent, but it actually has not been our informed consent. Studies show that vanishingly few people actually read the privacy policy and terms of service agreements when they sign up for online services. The researchers Jonathan Obar and Anne Oeldorf-Hirsch once ran a study where they asked volunteers to sign up for a fictitious new social networking site called NameDrop. It would have taken the volunteers about half an hour to read the privacy policy, Ari. The median time they spent on it was less than 14 seconds. The vast majority, Obar said, gave their consent to some really crazy things. JONATHAN OBAR: As a form of payment, you'd be giving up a first-born child, and 98 percent of participants that took the study didn't even notice this particular clause. SHAPIRO: OK, so those people are giving up their first-born child to this fictitious social network. Is the answer just to make privacy policies more digestible and readable or what? VEDANTAM: Certainly, I think that's an important first step. But it probably won't be enough, Ari. You know, the real problem is not just that we don't understand Facebook. The real problem is that we don't understand ourselves. If you put a camera up in someone's room and tell them that you're broadcasting what they do, people will be very mindful of the camera for the first hour, maybe even the first day or the first week. But in a very short period of time, people are going to forget that the camera is there. Here's the thing, if you ask the person a couple of weeks later, didn't you know the camera was there? People will tell you, yes, they did know. But knowing something and having it be front-of-mind are two completely different things. SHAPIRO: So one question is do people know? And the answer, as you said for the most part, is they don't. And the other question is do people care? And it sounds like not enough to actually do something. So is there a solution? VEDANTAM: Well, for the very disciplined amongst us, I suppose that education and information can help people act to protect their information. But given that many companies are exploiting not just our privacy but fundamental aspects about how our brains work, our laziness, our inattention, our distractibility, we might need policies that can help protect us not just from companies but protect us from ourselves. SHAPIRO: A provocative thought from Shankar Vedantam. Thank you so much. VEDANTAM: Thank you, Ari. SHAPIRO: And Shankar, of course, also hosts NPR's Hidden Brain podcast. MARY LOUISE KELLY, HOST:  Revelations keep rolling in about how Facebook shares its users' data without their consent. The New York Times reported this week that at least 60 device makers had secret agreements with Facebook. Now Facebook has confirmed that among those 60 are four Chinese companies. The U. S. government considers one of them a national security threat. ARI SHAPIRO, HOST:  These scandals and others do not seem to be hurting Facebook's bottom line much. In fact, last week, its stock hit its highest point of the year. And while it has fallen a bit since, there are no signs of a user mass exodus. So how much do we actually know or care about our privacy on Facebook and other platforms? NPR's social science correspondent Shankar Vedantam joins us to discuss that. Hi, Shankar. SHANKAR VEDANTAM, BYLINE: Hi, Ari. SHAPIRO: So lots of people say, I can't believe Facebook is using this with my data. Do people really care? - 'cause we're not seeing them unsubscribe en masse, as far as we can tell. VEDANTAM: Well, to be clear, Ari, there are two separate strands here. If Facebook has shared information in violation of its legal commitments to users, that's a legal and regulatory matter. There's a separate strand here that's psychological. When it comes to, you know, the garden variety exploitation of our information, Facebook and other companies in Silicon Valley often point out that they have asked for and obtain our permission to share our information. SHAPIRO: Right, at the end of an 8,000-page privacy policy that appears on our small iPhone screen. VEDANTAM: Exactly, Ari. In other words, we have given consent, but it actually has not been our informed consent. Studies show that vanishingly few people actually read the privacy policy and terms of service agreements when they sign up for online services. The researchers Jonathan Obar and Anne Oeldorf-Hirsch once ran a study where they asked volunteers to sign up for a fictitious new social networking site called NameDrop. It would have taken the volunteers about half an hour to read the privacy policy, Ari. The median time they spent on it was less than 14 seconds. The vast majority, Obar said, gave their consent to some really crazy things. JONATHAN OBAR: As a form of payment, you'd be giving up a first-born child, and 98 percent of participants that took the study didn't even notice this particular clause. SHAPIRO: OK, so those people are giving up their first-born child to this fictitious social network. Is the answer just to make privacy policies more digestible and readable or what? VEDANTAM: Certainly, I think that's an important first step. But it probably won't be enough, Ari. You know, the real problem is not just that we don't understand Facebook. The real problem is that we don't understand ourselves. If you put a camera up in someone's room and tell them that you're broadcasting what they do, people will be very mindful of the camera for the first hour, maybe even the first day or the first week. But in a very short period of time, people are going to forget that the camera is there. Here's the thing, if you ask the person a couple of weeks later, didn't you know the camera was there? People will tell you, yes, they did know. But knowing something and having it be front-of-mind are two completely different things. SHAPIRO: So one question is do people know? And the answer, as you said for the most part, is they don't. And the other question is do people care? And it sounds like not enough to actually do something. So is there a solution? VEDANTAM: Well, for the very disciplined amongst us, I suppose that education and information can help people act to protect their information. But given that many companies are exploiting not just our privacy but fundamental aspects about how our brains work, our laziness, our inattention, our distractibility, we might need policies that can help protect us not just from companies but protect us from ourselves. SHAPIRO: A provocative thought from Shankar Vedantam. Thank you so much. VEDANTAM: Thank you, Ari. SHAPIRO: And Shankar, of course, also hosts NPR's Hidden Brain podcast.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-06-617532884": {"title": "Aisle, Middle ... Or Video? Emirates President Predicts Windowless Planes Are Coming : NPR", "url": "https://www.npr.org/2018/06/06/617532884/aisle-middle-or-video-emirates-president-predicts-windowless-planes-are-coming", "author": "No author found", "published_date": "2018-06-06", "content": "", "section": "Technology", "disclaimer": ""}, "2018-06-06-617517990": {"title": "Former FBI Counter-Terrorism Agent Reflects On 'Messing With The Enemy' : NPR", "url": "https://www.npr.org/2018/06/06/617517990/former-fbi-counter-terrorism-agent-reflects-on-messing-with-the-enemy", "author": "No author found", "published_date": "2018-06-06", "content": "DAVE DAVIES, HOST: This is FRESH AIR. I'm Dave Davies in for Terry Gross, who's off this week. My guest Clint Watts has spent a fair chunk of his professional life battling terrorism - some of it using a home computer and a credit card. Watts is a former army officer and FBI counterterrorism agent who devoted some of his time as a civilian to tracking and engaging terrorists on social media - and more recently, researching Russian efforts to influence American elections. Watt's new book recounts some of those experiences and reflects on the power of social media to shape opinions and propagate false narratives when manipulated by actors with a political agenda. The Senate Intelligence Committee summoned Watts to testify about Russian meddling in the election, where he warned that the country needs a comprehensive program to combat the problem. Clint Watts is a Robert A. Fox fellow at the Foreign Policy Research Institute and a senior fellow at the Center for Cyber and Homeland Security at George Washington University. His new book is \"Messing With The Enemy: Surviving In A Social Media World Of Hackers Terrorists Russians And Fake News. \"Well, Clint Watts, welcome to FRESH AIR. You have an interesting resume. You went to West Point, served in the 101st Airborne and then left the military and went to the FBI in 2002, worked in counterterrorism, left the FBI, went and did graduate work in international studies, went back to teaching at the Combating Terrorism Center at West Point, returned to the FBI in 2006 - right? - again working on counterterrorism. What was it about the government's efforts at counterterrorism that just didn't work, at least for you? CLINT WATTS: Yeah, I've had an interesting career. And I can't figure out if I can't hold down a job, or it's just been an interesting pathway that sort of worked out. I think with counterterrorism - in the FBI in 2002 and '03, the organization was in tumult. If you remember, this is after 9/11. And the organization was moving quickly to try and get its hands around the counterterrorism mission. And counterterrorism was very different then. We were still in the reactive mode of the 1990s and '80s that you would see if we were chasing bank robberies. And what's been remarkable during the FBI Director Mueller's tenure, which was the entire time was there - both times - is the transition to intelligence-driven operations. And the second time I came back, that's what we are doing in counterterrorism. And I think that's why our country has been so well-protected since 9/11. And we've seen so little violence on our homeland. It's because of the great work and the great transition the FBI made. And so it was two different experiences. And the second one was more of what I expected than the first, I think. DAVIES: You left the FBI and worked as a cybersecurity consultant, right? And you started a blog. And you say you conducted social media experiments to engage terrorists. How did you connect with them? How did you get their attention? WATTS: It happened by accident for the most part. I first went out on to Twitter. And this is in the early days of Twitter when it was actually a nice place to be. And you met communities of people where you could discuss topics. In those early days around 2010 and '11, I was interfacing with people that were doing other counterterrorism work. And you'd have great dialogues on Twitter. And I would write about it on my blog. But what I started to figure out is there were actual terrorists or terrorist sympathizers out there in the world that would read these things too. It was very similar to what we saw with al-Qaida people - wanting to actually look at Combating Terrorism Center's website. They're very curious about themselves. And they have a certain kind of narcissism, which powers their movement in certain ways. And so I would write up these articles. And when I would write these articles or post - trying to analyze the currents of terrorist and where they were going, the future of al-Qaida - I started drawing in a few actual terrorist or terrorist sympathizers. And they would provide me feedback. And so I always looked to the crowd if you think about it. Crowdsourcing was big during the period after 2010. And the idea was you can ask any crowd a question. They'll give you the best answer. And I tried applying this in terrorism analysis. And I was failing. But I found these outliers in the mix, which were oftentimes people who are living overseas or knew a foreign language or were actually in these organizations. And they would provide me feedback. And that allowed me to sort of write a forecast over time of what eventually became the Islamic State overtaking al-Qaida. DAVIES: So this is amazing. So you've sort of created this intellectual community of private citizens, researchers, think tank people, people in the intelligence community, who want to study how to stop terrorism, as well as terrorist sympathizers and terrorists themselves all engaging in shoptalk. WATTS: Yes. And what was fascinating is it was the same formulas or techniques that I have been trained for intelligence analysis in the government. But you just have very few sources. You know, you just have very few people you could discuss this with. When you go to the open-source world of social media, you use the same sort of techniques and ideas. But you actually ask a very diverse community all across the world. You can get so much more in terms of rich answers that you really gain a lot of clarity on it. And so sometimes I would even put out what I thought was probably wrong - or an 80 percent solution - on my blog. And terrorists would be so quick to tell me I was wrong - or terror sympathizers would be - in what I got wrong. And so they would provide me feedback. And I would just change my estimate based on their feedback. And same with the people doing counterterrorism research. They would provide their input. If I had emailed them all and asked them to contribute, I would have not heard from them. But as soon as I put something out on social media, everyone's a critic. And they like to tell you wrong. And I would just deliberately do that and let them provide feedback to me. DAVIES: So you'd post a question - like, after the death of Osama bin Laden, what happens with al-Qaida? That would be kind of an example. And then what would you learn from the answers? WATTS: Yeah, what I started to learn - and this is what really played out over the next few years - is how social media was really powered by biases and that your community really shapes how you think about things. So I thought when I would put that question out there, which seemed like an inevitability - it actually came from one of my colleagues, a guy named Will McCants. What happens if, you know, bin Laden dies tomorrow? Does this just all end, or does it turn into something else? And across the board, the answer was always just kind of like it's not a big deal, or it doesn't matter. But I would get these fringe responses, which were from the Middle East or sometimes South Asia. Or sometimes it would even be Americans that just had studied abroad, and they had a different perspective. And I would aggregate them. And I essentially developed this system I called the wisdom of outliers. I'd look at these outlying possibilities and weigh them against each other and then try and look for indicators of when their prophecy - you know, their version of the future would come. And that really played out. That survey was fascinating because I put it out right before bin Laden was killed. And it was interesting to see how the world unfolded right after he was killed. And some of these outliers got it exactly right. DAVIES: Yeah, so what did the outliers tell you? And what did that help you conclude about the terrorist world? WATTS: What they were telling me - when I'd look at a lot of the terrorist sympathizers, they were looking for what was the next thing after al-Qaida. What was it going to be like when we don't have this head shed that's governing us? When will we pursue a caliphate? And it's fascinating. Within just a few months really after the death of bin Laden, you saw these emirates pop up, you know, in Yemen, Mali, you know, the Sahara and then later in the Islamic State. And they were trying to go ahead and pursue this dream because there was no real person stopping them. Bin Laden used to always say, we need to use caution. Be patient. Don't try and implement your will on the people. And if you just fast-forward three years, we saw the Islamic State do this in a devastating and aggressive way. They moved very quickly. And they did this because they had won a social media nation that overtook the establishment. They essentially had such popularity. They could mobilize people online or on the ground to pursue their vision. And they did this in such short order. And I think that's what came from the survey - was there were people hinting at the fact that after bin Laden dies there's going to be a social movement. There are people that are ready to move. And he's really just a lid on a pressure cooker that's out there. DAVIES: A lot of passion and violence that was ready to be unleashed. WATTS: Exactly. And it wasn't just the passion and violence. It was the barrier to entry. If you were a member of al-Qaida in the early days. You had to go to a training camp. You had to speak Arabic. You had to learn the ideology. You had to be indoctrinated. If you fast forward to about the time when Anwar al-Awlaki, the Yemeni-American cleric who went and joined al-Qaida in the Arabian Peninsula, you start to see him recruit people in a very different way. He was doing it through social media. He was doing it through short bursts, not these long diatribes that we would see bin Laden or Zawahiri give. And most importantly, he was doing it in such a way that he was recruiting people in the English language. And this was different. You didn't have to learn Arabic to access this violent ideology. You could just use your own language. If you look at what the Islamic State did, they lowered the barrier to entry. Everybody could participate. They did videos that were super short - almost like action videos or video games. And the other thing that they did was they translated into many different languages. They were speaking to everyone. The focus was on violence more than ideology. And it recruited so many more people into their ranks. And it really fueled the Islamic State - the building of an actual caliphate, something bin Laden always talked about but was always hesitant to pursue. DAVIES: Clint Watts' new book is \"Messing With The Enemy: Surviving In A Social Media World Of Hackers, Terrorists, Russians, And Fake News. \" We'll continue our conversation in just a moment. This is FRESH AIR. (SOUNDBITE OF PAQUITO D'RIVERA'S \"CONTRADANZA\")DAVIES: This is FRESH AIR, and we're speaking with Clint Watts. He's a former Army officer and FBI special agent who worked on counterterrorism operations. His new book about the use of social media by terrorist groups and Russian actors seeking to influence American politics is called \"Messing With The Enemy. \"You developed a special interest in a young American who'd gone to Somalia. You had something of an expertise in the Horn of Africa. And this guy had settled in with the terrorist group al-Shabab. Tell us about this man and what kind of relationship he developed. WATTS: Omar Hammami was from Daphne, Ala. And so he was a fascinating character. During the time when America was fighting in Iraq - this is after 9/11 - he became very adherent to the faith of Islam. And he also became more militant in his views. And he essentially would challenge other Americans in Alabama, in his high school, about why bin Laden was right or that he had the right to attack the United States and that America was wrong or essentially deserved the 9/11 attacks. And this is a very controversial stance, not only today but even more so back right after 9/11. And he continued to pursue it despite all the sort of pressure against him. He moved from there to Toronto and then Toronto to Cairo. And Cairo - instead of going to Iraq like lots of other foreign fighters of the time, he went to Somalia. And there was a group called the Islamic Courts Union. And he was known for being, really, one of the first people on social media to draw a following. He did it, as well, by being American. He was known as the American jihadi who could do raps. And he would record these on YouTube. And this is when we saw, the first time, a real clash of culture. It was the American culture merging with jihadi ideology to try and recruit people in America, which was very different. And he was doing this from Somalia. And he thought he was becoming quite notorious. And so from that, he became so emboldened and they promoted him so heavily as really a recruitment tool that he thought he was bigger than the people that were in charge. And so when the Islamic Courts Union became al-Shabab, he really became a player - or thought he was. And when he challenged the boss, a guy named Godane in Somalia, to try and lead and help guide Shabab, he was pushed out of the group and onto the run. And that's where I encountered him, was not in Somalia but on Twitter. He took to Twitter, YouTube and some other platforms to really get his story out there because he feared he was going to be killed by his own terrorist group. And that's when I started interacting with him because he saw myself and many other people on Twitter as a way to get his story out there. DAVIES: So you have this young man who has kind of become a little bit of a celebrity, at least in his own mind, as a terrorist leader but is suddenly on the outs. What kind of relationship did you develop with him? What strategy did you employ? What were you trying to do? WATTS: What was interesting about Hammami was I couldn't figure out what he was trying to do. Was he still supporting al-Qaida and jihadi ideology now that his terrorist group had turned on him? Or was he just trying to save his own skin, you know, by staying alive and keeping the public aware of his message? He would reach out to me and many others on Twitter. And a lot of people in the Twitter landscape then thought - oh, we can talk to him, and maybe we can talk him down from his violence. But I saw it as a much different way. Omar Hammami was a great vehicle to discredit other people, other potential foreign fighters that are in the social media space that might want to come and join - other future recruits who want to come and join. So I just wanted Omar to tell his story. And Omar wanted to tell his story because he wanted people to know that he was a big deal. Jihadists tend to be big narcissists. They have to. They have to spread word of their victories and their cause. And I knew every time he talked about his situation, he was eroding the brand of al-Qaida and al-Shabab. He was hurting jihadists because he's talking to an American, he's talking to an American in English and he's revealing that there are lots of cracks in the foundation of that terrorist group. They don't all get along. And so I wanted recruits not to go to Somalia, not to look up to Omar Hammami. But I wanted Hammami to tell us why they shouldn't do that. So I would just engage Omar on Twitter. But I also remembered, back from my FBI training days, that whether you're a car salesman or someone doing an interview or a journalist like you, Dave, you want to build rapport, you know, with whoever you're discussing. And so rather than talk to Omar about why he made such a bad choice, I would talk to Omar about what it's like to be an American and the things that he and I shared in common. I grew up in Missouri; he grew up in Alabama. He played soccer as a kid; so did I. He liked a lot of the same TV shows that I liked. And so I focused on that because what I wanted him to really do is tell me that al-Qaida, as an ideology, was bankrupt and that it wasn't what everyone thought it might be if they were looking to join. DAVIES: OK. This is what's odd about this. Right? You're a guy with a history in the intelligence community. You were an FBI agent. He is a guy who believes in death to America and jihad. So even if you're having some kind of friendly conversation, if he tells you how miserable his life has become since he made this choice to go to Somalia and embrace al-Shabab, you write about it in the blog to dissuade others from thinking, this is going to be a wonderful choice to make. Doesn't he see this? And doesn't he say - hey, that's not the message I want to get out? WATTS: He does. But his narcissism, I think, ultimately trumped his ideology. One of the posts that I wrote that really got his attention was called \"6 Reasons Not To Join Al-Shabab: Courtesy Of Omar Hammami\" (ph). And what I did was I read one of his bios. He would document in these long diatribes, you know, his experience in his bio as part of this jihadist movement. And I just picked out all the reasons why this is a silly escapade that he had gone on. But I used his own words. You know, I sort of laid it out. And he couldn't really argue with it in the end because I was talking to what he had actually said. This is what you said, Omar. This is my take on it. But then he would, you know, offer back other reasons why he thought he should pursue it. And so each one of those exchanges just gave me more insight into it. And probably over about a six-month period, what was fascinating was I could start to ask him questions in a very deliberate way. Do you see yourself as a member of al-Qaida? And what was fascinating, he was kind of a forerunner of what was to come. He said, I don't see myself as a member of al-Qaida, but I believe in what they're doing. He essentially was saying he wasn't part of it. And then when I challenged him a little bit further, he was saying after al-Qaida - and this was one of his famous tweets, to me, was post-al-Qaida equals broad-based jihad. He was signaling that after this is over, it will be the people's jihadist movement, a social media-powered populist movement. And it's eerie how similar that looks to what actually happened. The Islamic State, you know, which was ISIS before, really took over what al-Qaida started and took it into a very different direction. And they used social media to power it. DAVIES: You didn't hide who you were, and if they were potential terrorists reading this, reading your Twitter feed and can figure out where you are and may not appreciate your denigrating the choice of being a jihadist, were you looking over your shoulder? Did you worry? WATTS: Yeah. I think I've been looking over my shoulder for the last decade on social media. And that's definitely the case. You know, there were times that I worried about it, but I also felt like by being myself, at least to a certain point, maybe I'm resonating with some of those same people that are terrorist sympathizers. And yeah, I was very concerned about it. Around the Boston bomber time, I didn't live far from the Boston bombing, only about a mile or two away. And so I was concerned at that point because this was around the Omar discussions timeframe. And so, you know, I was well-trained in the military and law enforcement. I keep my eye open and, you know, I watch for things, but it was a concern. DAVIES: What became of Omar? WATTS: Omar was killed. Omar was killed by his group, al-Shabab, and it was coming. He would go intermittent on social media and vanish for some time. And I always assumed that he would either be detained or killed by al-Shabab. I was hoping that he would turn himself in or that somebody would turn him in to U. S. authorities because I thought it'd be great to have a law enforcement end, you know, to Omar without it always being about war and killing. But he was hunted down and killed in Somalia. I believe it was in 2013. DAVIES: And did this interaction with Omar or other conversations and writings that you engaged in, do you think they struck a blow against terrorism? Did they tell counterterrorists things they didn't know? WATTS: I don't know. You know, I don't know from inside government what was gained from it. I feel like at some point there must have been someone watching these conversations. And the more they became aware of Omar Hammami - and not just my conversation. There were plenty of other people out there talking about it, too. But the more they saw Omar's experience, the more they realized this prophecy of jihad that's oftentimes preached on the Internet or pushed on social media was bogus and that it wasn't something worth pursuing. So I'm hoping at some level - and I'm pretty confident that it was - there were people that saw those conversations and said, you know what? I don't know that I want to make that jump that Omar Hammami did. With that, though, I wouldn't - I would be remiss if I did not mention that this is the same time the conflict in Syria was picking up. Even in its best case, maybe I helped deter somebody from wanting to go to Somalia, but maybe they didn't go to Somalia and they just chose to go to Syria. One of the things that Omar eerily would talk about was he has a Syrian heritage. And so he would say it's messed up here with Shabab in Somalia, but you know where it is going - right? - and that's Syria. And he would point to what the Islamic State was building. It just wasn't quite there yet, but there were already movements of foreign fighters heading towards the Syria battlefield then. DAVIES: Clint Watts' book is \"Messing With The Enemy. \" After a break, he'll talk about his tracking of Russian efforts to influence American politics and why he says it's hard to combat fake news when our own government officials lie so much. Also, jazz critic Kevin Whitehead reviews a reissue of a classic 1991 album by Anthony Braxton and his quartet, and film critic Justin Chang reviews the new thriller \"Hereditary\" starring Toni Collette. I'm Dave Davies, and this is FRESH AIR. (SOUNDBITE OF MUSIC)DAVIES: This is FRESH AIR. I'm Dave Davies in for Terry Gross, who's off this week. We're speaking with former Army officer and former FBI counterterrorism agent Clint Watts. His new book is about his efforts as a civilian to engage with and undermine terrorist networks and to track Russian meddling in American politics. His book is called \"Messing With The Enemy. \"So in March of 2014 you write - and this is two years before the presidential campaign - a petition was posted to the whitehouse. gov website. And I didn't know this, but apparently, it's common to post petitions to that site. This one had the title Alaska Back To Russia, which is kind of an odd thing to be advocating. What did the activity around this particular petition reveal? WATTS: Yeah. That petition was odd because it was being promoted, sent around and amplified in the same network that was trolling me about Syria. And what was interesting is sometimes they would even tweet in the Russian language, and so that was curious. And then when you looked at the numbers, there were about roughly 40,000 signatures to this at the time. And we just couldn't figure out who the 40,000 Alaskans or Americans were that wanted this action to happen. And so when you pulled out the actual accounts that were sharing this link, they followed a very different pattern. If you went to most any petition that's at the White House website, it's basically a bell curve. Some people have some followers and a lot - are following a lot, and vice versa, some people are - you know, have lots of followers and are only following a few. And in the middle, it's about 50-50. But this was different. When you pulled out those accounts, they were an algorithm. They all had a certain increment of follower and following to where it almost looks like lines, like a set of blinds over a window. And that was one of the first social bots really that I had stumbled onto that was in an organized way, that with my colleagues, we could see this wasn't just some basic effort. This wasn't just recruiting and radicalizing extremists to go to Syria and Iraq. This was an organized effort to change people's perceptions, and it was aimed at the United States. DAVIES: Right. So if I'm - get what you're saying, when you looked at the accounts that signed on to endorse this petition - let's give Alaska back to Russia - what you saw were indications that they were mechanically controlled; they weren't real human beings; they were robotically controlled. WATTS: That's exactly right. With social bots, what you tend to find is they tweet at extremely high volumes at nearly all hours of the day, something a human could not do. And they also tend to have pictures that are innocuous, or as I always tell people, if you go and buy a picture frame at the store and you see that weird sort of picture of people, you know, in it that look too happy to be normal - it was those kind of pictures where you can instinctively look at it, and you're like, this is inauthentic; there's something not there to it. And that burst, that storm would change direction and share content almost instantaneously and in a very deliberate way. And so the next thing we sort of did was we observed them over time, and they would share links to the same news stories, the same news articles. So the content of those news articles almost always pushed back to a very pro-Russian foreign policy agenda. And what was also interesting is some of the top links that always surfaced were RT and Sputnik news, which were two state-sponsored news outlets of the Russian government. DAVIES: So - right. So what you were really seeing here is an early look - earlier than most of us were aware - of this Russian effort to influence American politics. Give us an example of one of these efforts that related to domestic issues or politics in the United States. WATTS: Right. So what was fascinating going from '14 into '15 is, they wanted to talk a lot less about foreign policy and much more about U. S. social issues. And this is when we really realized they were using the old Russian active measures playbook, which is, talk about generally four themes - social issues, financial issues, political issues or calamitous issues, which are inciting fear. And they would stay on these themes and sort of mix them in there. So one of those always interesting to me was the Bundy ranch. They would talk pretty heavily about the Bundy ranch. Black Lives Matter protests were a big issue they would discuss and amplify. And the one that really struck with me that I started to believe that this might be gaining traction was Jade Helm 2015. This was a military exercise that was happening in the Southwest of the United States, and the way it sort of came out onto the Internet was that this was a plan by the Obama administration to declare martial law and take everybody's weapons through use of the military. I don't think the Russians created that, but they amplified it to such a point that it really increased the level to which people both saw it and they believed it. And it was the first time I saw a physical reaction in the United States where I saw people engaging with the content. And when they showed up to protest, I oftentimes would wonder, did they think there were going to be hundreds of people at this rally because they saw such widespread support, which was actually false, in social media? And this was kind of the way to do it. The idea of Russian active measures, which is a Soviet reboot - you know, a reboot of the Soviet era - is to use the force of politics rather than the politics of force to win over audience and your adversary and amplify the divisions between them so that they're fighting against each other rather than against you. And this is that sort of social media judo that they were doing in 2015. DAVIES: You know, I saw an interview with you in Mother Jones, and you were talking about ways to - you were asked about ways to inoculate against disinformation campaigns. And it quoted you as saying, it's increasingly hard because our government officials lie so much. What - is that true? What do you mean? WATTS: Yeah. I think when I look since 2016 to now, my lesson is that in 2018, we don't need to worry about Russian disinformation. We need to worry about American disinformation. And it is in the social media space that we see everyone is essentially taking the same methods, the same techniques that are being - that were employed by the Russian disinformation system and using it for their own purposes. So are people creating their own information outlets, which spout their view on the world? Are they using social bots to amplify their message? Are they putting out forgeries or selectively leaked information to drive a political narrative? This is information warfare, and I see it duplicated now more by American politicians, political campaigns, public relations groups than I do by Russian disinformation. And I think the real scary thing over the horizon is, Russia doesn't need to make fake news anymore. In the United States, we have plenty of fake news that we make that they can repurpose. And right now, just between the government and the mainstream media, there is huge debate about who is right and whose versions of facts are the real facts that we should be listening to. That's a very dangerous place, not just for the United States, for democracies across the world. If everybody uses this information strategy, there will be no unions left at the end. It will leave us, instead of the United States of America, the divided states of America. DAVIES: That's a pretty sobering picture. Can you give us an example of one of these fake narratives that troubles us? WATTS: Yeah. The most recent one I think is a great example is spygate. We've heard a lot about spygate over the past couple days, and it's really been this narrative that the Obama administration put a spy in the Trump campaign. That is completely false. And it is a narrative - a false narrative - that has moved around in social media to such a degree that I'm confident many people believe that there was a spy from the Obama administration put into the Trump campaign. Yet we've seen representatives from Congress go, we've actually reviewed what the FBI did. And we saw from both sides the aisle now - Representative Trey Gowdy from the Republicans - say, there is nothing here; there was no spy, and this is actually good work on behalf of the FBI. But that refutation, again - it came so much later that that false conspiracy has already been spread around. And whether it's that - the notion that Trump Tower was wiretapped - or probably a half a dozen other large conspiracies, that false information moves around the ecosystem, and you see competing bubbles - partisan bubbles - take those false narratives and push them back and forth against each other. DAVIES: Right. So in a better world, how is that, you know, truth squaded (ph) effectively? WATTS: The only way we can beat disinformation is leadership. And it really takes our elected leaders, regardless of their position, to put country over party, to put truth over fictions and not use these political opportunities to push a conspiracy against a certain component because those conspiracies, they don't just hurt their opponent, it hurts Americans. It hurts American trust in democratic institutions, hurts American trust in elected officials. And it actually is to gain for our foreign adversaries who love to see us fighting amongst ourselves and spreading conspiracy. It opens the door for them to reuse a conspiracy or to create another one right behind it. DAVIES: What's the role of, you know, social media platforms like Twitter and Facebook? They obviously have a huge impact. Are there changes that would make them a more positive force? WATTS: Yeah. A year ago, I would have been more negative about the social media companies. But I have to say, if I look at all the actors involved in what has happened, social media companies have moved forward quite a bit in the last year. I think Facebook has devoted a lot of resources to quelling disinformation, trying to keep advanced manipulators off their platform. I even, you know, Twitter I've been hard on at times because I feel like they really missed the Russian disinformation effort. But they've made some real positive changes lately in times - trying to think about how you change the nature of conversations, you know, so that they're not so derisive on their platform. And they've instituted some terms of service and some controls to try and change that. But, you know, they can only do so much. What I see from the best manipulators on social media, the most nefarious of them, is they play within the terms of service. And they move from one platform to another based on shutdown. So if we think back to terrorists, we were closing them off Twitter and then they were moving to Telegram. Same kind of thing happens with disinformation is you can shut them out of one platform, but they tend to move or migrate to another. So this will be an enduring battle for the social media companies. And ultimately, what they need to do is restore trust with the users. And I think they're trying to institute those controls. Now, the question is, will it be too little too late? Or will social media just descend into partisans on both sides who yell back and forth at each other? And really, for the main consumer, it becomes a negative user experience. I think, you know, we're still another year or two from knowing that. But I do feel like they've' made some gains in recent months. DAVIES: OK. Clint Watts, thanks so much for speaking with us. WATTS: Thank you. DAVIES: I spoke to Clint Watts yesterday. His new book is \"Messing With The Enemy: Surviving In A Social Media World Of Hackers, Terrorists, Russians, And Fake News. \" Coming up, jazz critic Kevin Whitehead reviews a reissue of a classic 1991 album by Anthony Braxton and his quartet. This is FRESH AIR. (SOUNDBITE OF SLOWBERN'S \"WHEN WAR WAS KING\") DAVE DAVIES, HOST:  This is FRESH AIR. I'm Dave Davies in for Terry Gross, who's off this week. My guest Clint Watts has spent a fair chunk of his professional life battling terrorism - some of it using a home computer and a credit card. Watts is a former army officer and FBI counterterrorism agent who devoted some of his time as a civilian to tracking and engaging terrorists on social media - and more recently, researching Russian efforts to influence American elections. Watt's new book recounts some of those experiences and reflects on the power of social media to shape opinions and propagate false narratives when manipulated by actors with a political agenda. The Senate Intelligence Committee summoned Watts to testify about Russian meddling in the election, where he warned that the country needs a comprehensive program to combat the problem. Clint Watts is a Robert A. Fox fellow at the Foreign Policy Research Institute and a senior fellow at the Center for Cyber and Homeland Security at George Washington University. His new book is \"Messing With The Enemy: Surviving In A Social Media World Of Hackers Terrorists Russians And Fake News. \" Well, Clint Watts, welcome to FRESH AIR. You have an interesting resume. You went to West Point, served in the 101st Airborne and then left the military and went to the FBI in 2002, worked in counterterrorism, left the FBI, went and did graduate work in international studies, went back to teaching at the Combating Terrorism Center at West Point, returned to the FBI in 2006 - right? - again working on counterterrorism. What was it about the government's efforts at counterterrorism that just didn't work, at least for you? CLINT WATTS: Yeah, I've had an interesting career. And I can't figure out if I can't hold down a job, or it's just been an interesting pathway that sort of worked out. I think with counterterrorism - in the FBI in 2002 and '03, the organization was in tumult. If you remember, this is after 9/11. And the organization was moving quickly to try and get its hands around the counterterrorism mission. And counterterrorism was very different then. We were still in the reactive mode of the 1990s and '80s that you would see if we were chasing bank robberies. And what's been remarkable during the FBI Director Mueller's tenure, which was the entire time was there - both times - is the transition to intelligence-driven operations. And the second time I came back, that's what we are doing in counterterrorism. And I think that's why our country has been so well-protected since 9/11. And we've seen so little violence on our homeland. It's because of the great work and the great transition the FBI made. And so it was two different experiences. And the second one was more of what I expected than the first, I think. DAVIES: You left the FBI and worked as a cybersecurity consultant, right? And you started a blog. And you say you conducted social media experiments to engage terrorists. How did you connect with them? How did you get their attention? WATTS: It happened by accident for the most part. I first went out on to Twitter. And this is in the early days of Twitter when it was actually a nice place to be. And you met communities of people where you could discuss topics. In those early days around 2010 and '11, I was interfacing with people that were doing other counterterrorism work. And you'd have great dialogues on Twitter. And I would write about it on my blog. But what I started to figure out is there were actual terrorists or terrorist sympathizers out there in the world that would read these things too. It was very similar to what we saw with al-Qaida people - wanting to actually look at Combating Terrorism Center's website. They're very curious about themselves. And they have a certain kind of narcissism, which powers their movement in certain ways. And so I would write up these articles. And when I would write these articles or post - trying to analyze the currents of terrorist and where they were going, the future of al-Qaida - I started drawing in a few actual terrorist or terrorist sympathizers. And they would provide me feedback. And so I always looked to the crowd if you think about it. Crowdsourcing was big during the period after 2010. And the idea was you can ask any crowd a question. They'll give you the best answer. And I tried applying this in terrorism analysis. And I was failing. But I found these outliers in the mix, which were oftentimes people who are living overseas or knew a foreign language or were actually in these organizations. And they would provide me feedback. And that allowed me to sort of write a forecast over time of what eventually became the Islamic State overtaking al-Qaida. DAVIES: So this is amazing. So you've sort of created this intellectual community of private citizens, researchers, think tank people, people in the intelligence community, who want to study how to stop terrorism, as well as terrorist sympathizers and terrorists themselves all engaging in shoptalk. WATTS: Yes. And what was fascinating is it was the same formulas or techniques that I have been trained for intelligence analysis in the government. But you just have very few sources. You know, you just have very few people you could discuss this with. When you go to the open-source world of social media, you use the same sort of techniques and ideas. But you actually ask a very diverse community all across the world. You can get so much more in terms of rich answers that you really gain a lot of clarity on it. And so sometimes I would even put out what I thought was probably wrong - or an 80 percent solution - on my blog. And terrorists would be so quick to tell me I was wrong - or terror sympathizers would be - in what I got wrong. And so they would provide me feedback. And I would just change my estimate based on their feedback. And same with the people doing counterterrorism research. They would provide their input. If I had emailed them all and asked them to contribute, I would have not heard from them. But as soon as I put something out on social media, everyone's a critic. And they like to tell you wrong. And I would just deliberately do that and let them provide feedback to me. DAVIES: So you'd post a question - like, after the death of Osama bin Laden, what happens with al-Qaida? That would be kind of an example. And then what would you learn from the answers? WATTS: Yeah, what I started to learn - and this is what really played out over the next few years - is how social media was really powered by biases and that your community really shapes how you think about things. So I thought when I would put that question out there, which seemed like an inevitability - it actually came from one of my colleagues, a guy named Will McCants. What happens if, you know, bin Laden dies tomorrow? Does this just all end, or does it turn into something else? And across the board, the answer was always just kind of like it's not a big deal, or it doesn't matter. But I would get these fringe responses, which were from the Middle East or sometimes South Asia. Or sometimes it would even be Americans that just had studied abroad, and they had a different perspective. And I would aggregate them. And I essentially developed this system I called the wisdom of outliers. I'd look at these outlying possibilities and weigh them against each other and then try and look for indicators of when their prophecy - you know, their version of the future would come. And that really played out. That survey was fascinating because I put it out right before bin Laden was killed. And it was interesting to see how the world unfolded right after he was killed. And some of these outliers got it exactly right. DAVIES: Yeah, so what did the outliers tell you? And what did that help you conclude about the terrorist world? WATTS: What they were telling me - when I'd look at a lot of the terrorist sympathizers, they were looking for what was the next thing after al-Qaida. What was it going to be like when we don't have this head shed that's governing us? When will we pursue a caliphate? And it's fascinating. Within just a few months really after the death of bin Laden, you saw these emirates pop up, you know, in Yemen, Mali, you know, the Sahara and then later in the Islamic State. And they were trying to go ahead and pursue this dream because there was no real person stopping them. Bin Laden used to always say, we need to use caution. Be patient. Don't try and implement your will on the people. And if you just fast-forward three years, we saw the Islamic State do this in a devastating and aggressive way. They moved very quickly. And they did this because they had won a social media nation that overtook the establishment. They essentially had such popularity. They could mobilize people online or on the ground to pursue their vision. And they did this in such short order. And I think that's what came from the survey - was there were people hinting at the fact that after bin Laden dies there's going to be a social movement. There are people that are ready to move. And he's really just a lid on a pressure cooker that's out there. DAVIES: A lot of passion and violence that was ready to be unleashed. WATTS: Exactly. And it wasn't just the passion and violence. It was the barrier to entry. If you were a member of al-Qaida in the early days. You had to go to a training camp. You had to speak Arabic. You had to learn the ideology. You had to be indoctrinated. If you fast forward to about the time when Anwar al-Awlaki, the Yemeni-American cleric who went and joined al-Qaida in the Arabian Peninsula, you start to see him recruit people in a very different way. He was doing it through social media. He was doing it through short bursts, not these long diatribes that we would see bin Laden or Zawahiri give. And most importantly, he was doing it in such a way that he was recruiting people in the English language. And this was different. You didn't have to learn Arabic to access this violent ideology. You could just use your own language. If you look at what the Islamic State did, they lowered the barrier to entry. Everybody could participate. They did videos that were super short - almost like action videos or video games. And the other thing that they did was they translated into many different languages. They were speaking to everyone. The focus was on violence more than ideology. And it recruited so many more people into their ranks. And it really fueled the Islamic State - the building of an actual caliphate, something bin Laden always talked about but was always hesitant to pursue. DAVIES: Clint Watts' new book is \"Messing With The Enemy: Surviving In A Social Media World Of Hackers, Terrorists, Russians, And Fake News. \" We'll continue our conversation in just a moment. This is FRESH AIR. (SOUNDBITE OF PAQUITO D'RIVERA'S \"CONTRADANZA\") DAVIES: This is FRESH AIR, and we're speaking with Clint Watts. He's a former Army officer and FBI special agent who worked on counterterrorism operations. His new book about the use of social media by terrorist groups and Russian actors seeking to influence American politics is called \"Messing With The Enemy. \" You developed a special interest in a young American who'd gone to Somalia. You had something of an expertise in the Horn of Africa. And this guy had settled in with the terrorist group al-Shabab. Tell us about this man and what kind of relationship he developed. WATTS: Omar Hammami was from Daphne, Ala. And so he was a fascinating character. During the time when America was fighting in Iraq - this is after 9/11 - he became very adherent to the faith of Islam. And he also became more militant in his views. And he essentially would challenge other Americans in Alabama, in his high school, about why bin Laden was right or that he had the right to attack the United States and that America was wrong or essentially deserved the 9/11 attacks. And this is a very controversial stance, not only today but even more so back right after 9/11. And he continued to pursue it despite all the sort of pressure against him. He moved from there to Toronto and then Toronto to Cairo. And Cairo - instead of going to Iraq like lots of other foreign fighters of the time, he went to Somalia. And there was a group called the Islamic Courts Union. And he was known for being, really, one of the first people on social media to draw a following. He did it, as well, by being American. He was known as the American jihadi who could do raps. And he would record these on YouTube. And this is when we saw, the first time, a real clash of culture. It was the American culture merging with jihadi ideology to try and recruit people in America, which was very different. And he was doing this from Somalia. And he thought he was becoming quite notorious. And so from that, he became so emboldened and they promoted him so heavily as really a recruitment tool that he thought he was bigger than the people that were in charge. And so when the Islamic Courts Union became al-Shabab, he really became a player - or thought he was. And when he challenged the boss, a guy named Godane in Somalia, to try and lead and help guide Shabab, he was pushed out of the group and onto the run. And that's where I encountered him, was not in Somalia but on Twitter. He took to Twitter, YouTube and some other platforms to really get his story out there because he feared he was going to be killed by his own terrorist group. And that's when I started interacting with him because he saw myself and many other people on Twitter as a way to get his story out there. DAVIES: So you have this young man who has kind of become a little bit of a celebrity, at least in his own mind, as a terrorist leader but is suddenly on the outs. What kind of relationship did you develop with him? What strategy did you employ? What were you trying to do? WATTS: What was interesting about Hammami was I couldn't figure out what he was trying to do. Was he still supporting al-Qaida and jihadi ideology now that his terrorist group had turned on him? Or was he just trying to save his own skin, you know, by staying alive and keeping the public aware of his message? He would reach out to me and many others on Twitter. And a lot of people in the Twitter landscape then thought - oh, we can talk to him, and maybe we can talk him down from his violence. But I saw it as a much different way. Omar Hammami was a great vehicle to discredit other people, other potential foreign fighters that are in the social media space that might want to come and join - other future recruits who want to come and join. So I just wanted Omar to tell his story. And Omar wanted to tell his story because he wanted people to know that he was a big deal. Jihadists tend to be big narcissists. They have to. They have to spread word of their victories and their cause. And I knew every time he talked about his situation, he was eroding the brand of al-Qaida and al-Shabab. He was hurting jihadists because he's talking to an American, he's talking to an American in English and he's revealing that there are lots of cracks in the foundation of that terrorist group. They don't all get along. And so I wanted recruits not to go to Somalia, not to look up to Omar Hammami. But I wanted Hammami to tell us why they shouldn't do that. So I would just engage Omar on Twitter. But I also remembered, back from my FBI training days, that whether you're a car salesman or someone doing an interview or a journalist like you, Dave, you want to build rapport, you know, with whoever you're discussing. And so rather than talk to Omar about why he made such a bad choice, I would talk to Omar about what it's like to be an American and the things that he and I shared in common. I grew up in Missouri; he grew up in Alabama. He played soccer as a kid; so did I. He liked a lot of the same TV shows that I liked. And so I focused on that because what I wanted him to really do is tell me that al-Qaida, as an ideology, was bankrupt and that it wasn't what everyone thought it might be if they were looking to join. DAVIES: OK. This is what's odd about this. Right? You're a guy with a history in the intelligence community. You were an FBI agent. He is a guy who believes in death to America and jihad. So even if you're having some kind of friendly conversation, if he tells you how miserable his life has become since he made this choice to go to Somalia and embrace al-Shabab, you write about it in the blog to dissuade others from thinking, this is going to be a wonderful choice to make. Doesn't he see this? And doesn't he say - hey, that's not the message I want to get out? WATTS: He does. But his narcissism, I think, ultimately trumped his ideology. One of the posts that I wrote that really got his attention was called \"6 Reasons Not To Join Al-Shabab: Courtesy Of Omar Hammami\" (ph). And what I did was I read one of his bios. He would document in these long diatribes, you know, his experience in his bio as part of this jihadist movement. And I just picked out all the reasons why this is a silly escapade that he had gone on. But I used his own words. You know, I sort of laid it out. And he couldn't really argue with it in the end because I was talking to what he had actually said. This is what you said, Omar. This is my take on it. But then he would, you know, offer back other reasons why he thought he should pursue it. And so each one of those exchanges just gave me more insight into it. And probably over about a six-month period, what was fascinating was I could start to ask him questions in a very deliberate way. Do you see yourself as a member of al-Qaida? And what was fascinating, he was kind of a forerunner of what was to come. He said, I don't see myself as a member of al-Qaida, but I believe in what they're doing. He essentially was saying he wasn't part of it. And then when I challenged him a little bit further, he was saying after al-Qaida - and this was one of his famous tweets, to me, was post-al-Qaida equals broad-based jihad. He was signaling that after this is over, it will be the people's jihadist movement, a social media-powered populist movement. And it's eerie how similar that looks to what actually happened. The Islamic State, you know, which was ISIS before, really took over what al-Qaida started and took it into a very different direction. And they used social media to power it. DAVIES: You didn't hide who you were, and if they were potential terrorists reading this, reading your Twitter feed and can figure out where you are and may not appreciate your denigrating the choice of being a jihadist, were you looking over your shoulder? Did you worry? WATTS: Yeah. I think I've been looking over my shoulder for the last decade on social media. And that's definitely the case. You know, there were times that I worried about it, but I also felt like by being myself, at least to a certain point, maybe I'm resonating with some of those same people that are terrorist sympathizers. And yeah, I was very concerned about it. Around the Boston bomber time, I didn't live far from the Boston bombing, only about a mile or two away. And so I was concerned at that point because this was around the Omar discussions timeframe. And so, you know, I was well-trained in the military and law enforcement. I keep my eye open and, you know, I watch for things, but it was a concern. DAVIES: What became of Omar? WATTS: Omar was killed. Omar was killed by his group, al-Shabab, and it was coming. He would go intermittent on social media and vanish for some time. And I always assumed that he would either be detained or killed by al-Shabab. I was hoping that he would turn himself in or that somebody would turn him in to U. S. authorities because I thought it'd be great to have a law enforcement end, you know, to Omar without it always being about war and killing. But he was hunted down and killed in Somalia. I believe it was in 2013. DAVIES: And did this interaction with Omar or other conversations and writings that you engaged in, do you think they struck a blow against terrorism? Did they tell counterterrorists things they didn't know? WATTS: I don't know. You know, I don't know from inside government what was gained from it. I feel like at some point there must have been someone watching these conversations. And the more they became aware of Omar Hammami - and not just my conversation. There were plenty of other people out there talking about it, too. But the more they saw Omar's experience, the more they realized this prophecy of jihad that's oftentimes preached on the Internet or pushed on social media was bogus and that it wasn't something worth pursuing. So I'm hoping at some level - and I'm pretty confident that it was - there were people that saw those conversations and said, you know what? I don't know that I want to make that jump that Omar Hammami did. With that, though, I wouldn't - I would be remiss if I did not mention that this is the same time the conflict in Syria was picking up. Even in its best case, maybe I helped deter somebody from wanting to go to Somalia, but maybe they didn't go to Somalia and they just chose to go to Syria. One of the things that Omar eerily would talk about was he has a Syrian heritage. And so he would say it's messed up here with Shabab in Somalia, but you know where it is going - right? - and that's Syria. And he would point to what the Islamic State was building. It just wasn't quite there yet, but there were already movements of foreign fighters heading towards the Syria battlefield then. DAVIES: Clint Watts' book is \"Messing With The Enemy. \" After a break, he'll talk about his tracking of Russian efforts to influence American politics and why he says it's hard to combat fake news when our own government officials lie so much. Also, jazz critic Kevin Whitehead reviews a reissue of a classic 1991 album by Anthony Braxton and his quartet, and film critic Justin Chang reviews the new thriller \"Hereditary\" starring Toni Collette. I'm Dave Davies, and this is FRESH AIR. (SOUNDBITE OF MUSIC) DAVIES: This is FRESH AIR. I'm Dave Davies in for Terry Gross, who's off this week. We're speaking with former Army officer and former FBI counterterrorism agent Clint Watts. His new book is about his efforts as a civilian to engage with and undermine terrorist networks and to track Russian meddling in American politics. His book is called \"Messing With The Enemy. \" So in March of 2014 you write - and this is two years before the presidential campaign - a petition was posted to the whitehouse. gov website. And I didn't know this, but apparently, it's common to post petitions to that site. This one had the title Alaska Back To Russia, which is kind of an odd thing to be advocating. What did the activity around this particular petition reveal? WATTS: Yeah. That petition was odd because it was being promoted, sent around and amplified in the same network that was trolling me about Syria. And what was interesting is sometimes they would even tweet in the Russian language, and so that was curious. And then when you looked at the numbers, there were about roughly 40,000 signatures to this at the time. And we just couldn't figure out who the 40,000 Alaskans or Americans were that wanted this action to happen. And so when you pulled out the actual accounts that were sharing this link, they followed a very different pattern. If you went to most any petition that's at the White House website, it's basically a bell curve. Some people have some followers and a lot - are following a lot, and vice versa, some people are - you know, have lots of followers and are only following a few. And in the middle, it's about 50-50. But this was different. When you pulled out those accounts, they were an algorithm. They all had a certain increment of follower and following to where it almost looks like lines, like a set of blinds over a window. And that was one of the first social bots really that I had stumbled onto that was in an organized way, that with my colleagues, we could see this wasn't just some basic effort. This wasn't just recruiting and radicalizing extremists to go to Syria and Iraq. This was an organized effort to change people's perceptions, and it was aimed at the United States. DAVIES: Right. So if I'm - get what you're saying, when you looked at the accounts that signed on to endorse this petition - let's give Alaska back to Russia - what you saw were indications that they were mechanically controlled; they weren't real human beings; they were robotically controlled. WATTS: That's exactly right. With social bots, what you tend to find is they tweet at extremely high volumes at nearly all hours of the day, something a human could not do. And they also tend to have pictures that are innocuous, or as I always tell people, if you go and buy a picture frame at the store and you see that weird sort of picture of people, you know, in it that look too happy to be normal - it was those kind of pictures where you can instinctively look at it, and you're like, this is inauthentic; there's something not there to it. And that burst, that storm would change direction and share content almost instantaneously and in a very deliberate way. And so the next thing we sort of did was we observed them over time, and they would share links to the same news stories, the same news articles. So the content of those news articles almost always pushed back to a very pro-Russian foreign policy agenda. And what was also interesting is some of the top links that always surfaced were RT and Sputnik news, which were two state-sponsored news outlets of the Russian government. DAVIES: So - right. So what you were really seeing here is an early look - earlier than most of us were aware - of this Russian effort to influence American politics. Give us an example of one of these efforts that related to domestic issues or politics in the United States. WATTS: Right. So what was fascinating going from '14 into '15 is, they wanted to talk a lot less about foreign policy and much more about U. S. social issues. And this is when we really realized they were using the old Russian active measures playbook, which is, talk about generally four themes - social issues, financial issues, political issues or calamitous issues, which are inciting fear. And they would stay on these themes and sort of mix them in there. So one of those always interesting to me was the Bundy ranch. They would talk pretty heavily about the Bundy ranch. Black Lives Matter protests were a big issue they would discuss and amplify. And the one that really struck with me that I started to believe that this might be gaining traction was Jade Helm 2015. This was a military exercise that was happening in the Southwest of the United States, and the way it sort of came out onto the Internet was that this was a plan by the Obama administration to declare martial law and take everybody's weapons through use of the military. I don't think the Russians created that, but they amplified it to such a point that it really increased the level to which people both saw it and they believed it. And it was the first time I saw a physical reaction in the United States where I saw people engaging with the content. And when they showed up to protest, I oftentimes would wonder, did they think there were going to be hundreds of people at this rally because they saw such widespread support, which was actually false, in social media? And this was kind of the way to do it. The idea of Russian active measures, which is a Soviet reboot - you know, a reboot of the Soviet era - is to use the force of politics rather than the politics of force to win over audience and your adversary and amplify the divisions between them so that they're fighting against each other rather than against you. And this is that sort of social media judo that they were doing in 2015. DAVIES: You know, I saw an interview with you in Mother Jones, and you were talking about ways to - you were asked about ways to inoculate against disinformation campaigns. And it quoted you as saying, it's increasingly hard because our government officials lie so much. What - is that true? What do you mean? WATTS: Yeah. I think when I look since 2016 to now, my lesson is that in 2018, we don't need to worry about Russian disinformation. We need to worry about American disinformation. And it is in the social media space that we see everyone is essentially taking the same methods, the same techniques that are being - that were employed by the Russian disinformation system and using it for their own purposes. So are people creating their own information outlets, which spout their view on the world? Are they using social bots to amplify their message? Are they putting out forgeries or selectively leaked information to drive a political narrative? This is information warfare, and I see it duplicated now more by American politicians, political campaigns, public relations groups than I do by Russian disinformation. And I think the real scary thing over the horizon is, Russia doesn't need to make fake news anymore. In the United States, we have plenty of fake news that we make that they can repurpose. And right now, just between the government and the mainstream media, there is huge debate about who is right and whose versions of facts are the real facts that we should be listening to. That's a very dangerous place, not just for the United States, for democracies across the world. If everybody uses this information strategy, there will be no unions left at the end. It will leave us, instead of the United States of America, the divided states of America. DAVIES: That's a pretty sobering picture. Can you give us an example of one of these fake narratives that troubles us? WATTS: Yeah. The most recent one I think is a great example is spygate. We've heard a lot about spygate over the past couple days, and it's really been this narrative that the Obama administration put a spy in the Trump campaign. That is completely false. And it is a narrative - a false narrative - that has moved around in social media to such a degree that I'm confident many people believe that there was a spy from the Obama administration put into the Trump campaign. Yet we've seen representatives from Congress go, we've actually reviewed what the FBI did. And we saw from both sides the aisle now - Representative Trey Gowdy from the Republicans - say, there is nothing here; there was no spy, and this is actually good work on behalf of the FBI. But that refutation, again - it came so much later that that false conspiracy has already been spread around. And whether it's that - the notion that Trump Tower was wiretapped - or probably a half a dozen other large conspiracies, that false information moves around the ecosystem, and you see competing bubbles - partisan bubbles - take those false narratives and push them back and forth against each other. DAVIES: Right. So in a better world, how is that, you know, truth squaded (ph) effectively? WATTS: The only way we can beat disinformation is leadership. And it really takes our elected leaders, regardless of their position, to put country over party, to put truth over fictions and not use these political opportunities to push a conspiracy against a certain component because those conspiracies, they don't just hurt their opponent, it hurts Americans. It hurts American trust in democratic institutions, hurts American trust in elected officials. And it actually is to gain for our foreign adversaries who love to see us fighting amongst ourselves and spreading conspiracy. It opens the door for them to reuse a conspiracy or to create another one right behind it. DAVIES: What's the role of, you know, social media platforms like Twitter and Facebook? They obviously have a huge impact. Are there changes that would make them a more positive force? WATTS: Yeah. A year ago, I would have been more negative about the social media companies. But I have to say, if I look at all the actors involved in what has happened, social media companies have moved forward quite a bit in the last year. I think Facebook has devoted a lot of resources to quelling disinformation, trying to keep advanced manipulators off their platform. I even, you know, Twitter I've been hard on at times because I feel like they really missed the Russian disinformation effort. But they've made some real positive changes lately in times - trying to think about how you change the nature of conversations, you know, so that they're not so derisive on their platform. And they've instituted some terms of service and some controls to try and change that. But, you know, they can only do so much. What I see from the best manipulators on social media, the most nefarious of them, is they play within the terms of service. And they move from one platform to another based on shutdown. So if we think back to terrorists, we were closing them off Twitter and then they were moving to Telegram. Same kind of thing happens with disinformation is you can shut them out of one platform, but they tend to move or migrate to another. So this will be an enduring battle for the social media companies. And ultimately, what they need to do is restore trust with the users. And I think they're trying to institute those controls. Now, the question is, will it be too little too late? Or will social media just descend into partisans on both sides who yell back and forth at each other? And really, for the main consumer, it becomes a negative user experience. I think, you know, we're still another year or two from knowing that. But I do feel like they've' made some gains in recent months. DAVIES: OK. Clint Watts, thanks so much for speaking with us. WATTS: Thank you. DAVIES: I spoke to Clint Watts yesterday. His new book is \"Messing With The Enemy: Surviving In A Social Media World Of Hackers, Terrorists, Russians, And Fake News. \" Coming up, jazz critic Kevin Whitehead reviews a reissue of a classic 1991 album by Anthony Braxton and his quartet. This is FRESH AIR. (SOUNDBITE OF SLOWBERN'S \"WHEN WAR WAS KING\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-06-617530138": {"title": "Facebook Data-Sharing Deals Include China's Huawei \u2014 Under U.S. Suspicion Since 2012 : NPR", "url": "https://www.npr.org/2018/06/06/617530138/facebooks-data-sharing-deals-included-huawei-under-u-s-suspicion-since-2012", "author": "No author found", "published_date": "2018-06-06", "content": "", "section": "Technology", "disclaimer": ""}, "2018-06-06-615137239": {"title": "What Americans Told Us About Online Shopping Says A Lot About Amazon : NPR", "url": "https://www.npr.org/2018/06/06/615137239/what-americans-told-us-about-online-shopping-says-a-lot-about-amazon", "author": "No author found", "published_date": "2018-06-06", "content": "STEVE INSKEEP, HOST: A new survey indicates just how much Amazon, the giant online retailer, has reshaped America, this nation of shoppers. Here's NPR's Alina Selyukh. ALINA SELYUKH, BYLINE: OK. Here's a big number - 92 percent. The new NPR/Marist poll found that, in America, of all the people who have ever bought anything on the internet, almost all of them - 92 percent - have bought something on Amazon. And here's a question that's been bugging me as a radio reporter - what does that sound like? What's the sound of Amazon's insane popularity? (SOUNDBITE OF CONSTRUCTION SITE NOISE)SELYUKH: One unexpected answer was in the lobby of my own apartment building, construction of a new package room specifically to tame the piles of brown cardboard boxes spilling out of the lobby closet - the sound of America physically adjusting itself to Amazon's success. But of course, the more obvious showcase of Amazon's popularity is this. How much do you shop on Amazon specifically? MELANIE HINCHEY: Gosh, it's definitely over, like, $100 a month, probably over 200 a month. SELYUKH: Melanie Hinchey is a mother of two who works in tech in Milwaukee. And she is one of a hundred million people in the world who pay for the Amazon Prime membership. In the U. S. , that involves shelling out $119 a year. If you add in all the extra moochers who use other people's accounts, our poll found that, in America, nearly two-thirds of all online shoppers are living inside the Amazon retail universe. A quarter of Prime users buy something every week. HINCHEY: I buy things like soy milk and body wash, things that are shelf stable that I know I'm going to use every week via Prime. And they just come once a month. I don't even have to think about buying those things. SELYUKH: This kind of stocking up on the basics online - like toothpaste, garbage bags, cereal, canned foods - it's a powerful shift in behavior that we're just starting to see. According to our survey, most Americans have never done this. They've never bought basic household or nonperishable goods online. But the people who do, like Hinchey, tend to rely on Amazon. Convenience is habit-forming, and it's something founder and CEO Jeff Bezos knew early on. Here's what he told NPR in 1999. (SOUNDBITE OF ARCHIVED BROADCAST)JEFF BEZOS: Our No. 1 mission is to be Earth's most customer-centric company. And we mean that across any industry and across any time. SELYUKH: This obsession with customers has catapulted Amazon into a company worth $800 billion today. It's made Bezos the richest man in the world. Amazon now employs more than half a million workers. And we should note - the company is one of NPR's underwriters. Amazon is now making movies and TV shows, storing government data on the cloud, selling internet-connected door locks, making the popular Alexa smart speaker. When a company gets to be this far-reaching, critics emerge from all directions. And for Amazon, the most prominent one is President Trump. Bezos was asked about this by the head of a media company called Axel Springer in April. The question was, what if Trump decided that Amazon was too big and should be broken up? Here's Bezos. (SOUNDBITE OF ARCHIVED RECORDING)BEZOS: For me - again, this is one of those things where I focus on what we can control. And I expect, whether it's, you know, the current U. S. administration or any other government agency anywhere in the world, Amazon is now a large corporation. And I expect us to be scrutinized. SELYUKH: But what the scrutiny entails is unclear. Legal scholar Lina Khan has been studying the power and impact of Amazon. She says that the government and the courts apply the laws in a way that makes it difficult to check Amazon's dominance. LINA KHAN: Antitrust laws have become very focused on consumer welfare. SELYUKH: She's saying the laws focus on making sure that consumers have enough choices or don't get overcharged, for example. But with Amazon, Khan argues the issue is not the short-term interest of shoppers but the impact on the suppliers and the retailers and the competitors in all the areas where Amazon looms large. Many of them now depend on Amazon to reach their own customers. KHAN: And it shows how if you are amassing market power, antitrusts won't necessarily respond to that anymore unless it's obvious that you're also hurting consumers in some way. SELYUKH: Khan has her own critics. But that's the thing about Amazon. It reaches far and wide, but customers trust it - a lot. In our survey, a majority of online shoppers said they don't have confidence in most online retailers when it comes to protecting their privacy, but 2 out of 3 said they do trust Amazon. And that's a big reason why Americans are OK with Amazon's reign as the top online retailer and brown boxes keep piling up in my building and on doorsteps across the country. Alina Selyukh, NPR News. (SOUNDBITE OF J'SAN'S \"GROOVY LIFE\") STEVE INSKEEP, HOST:  A new survey indicates just how much Amazon, the giant online retailer, has reshaped America, this nation of shoppers. Here's NPR's Alina Selyukh. ALINA SELYUKH, BYLINE: OK. Here's a big number - 92 percent. The new NPR/Marist poll found that, in America, of all the people who have ever bought anything on the internet, almost all of them - 92 percent - have bought something on Amazon. And here's a question that's been bugging me as a radio reporter - what does that sound like? What's the sound of Amazon's insane popularity? (SOUNDBITE OF CONSTRUCTION SITE NOISE) SELYUKH: One unexpected answer was in the lobby of my own apartment building, construction of a new package room specifically to tame the piles of brown cardboard boxes spilling out of the lobby closet - the sound of America physically adjusting itself to Amazon's success. But of course, the more obvious showcase of Amazon's popularity is this. How much do you shop on Amazon specifically? MELANIE HINCHEY: Gosh, it's definitely over, like, $100 a month, probably over 200 a month. SELYUKH: Melanie Hinchey is a mother of two who works in tech in Milwaukee. And she is one of a hundred million people in the world who pay for the Amazon Prime membership. In the U. S. , that involves shelling out $119 a year. If you add in all the extra moochers who use other people's accounts, our poll found that, in America, nearly two-thirds of all online shoppers are living inside the Amazon retail universe. A quarter of Prime users buy something every week. HINCHEY: I buy things like soy milk and body wash, things that are shelf stable that I know I'm going to use every week via Prime. And they just come once a month. I don't even have to think about buying those things. SELYUKH: This kind of stocking up on the basics online - like toothpaste, garbage bags, cereal, canned foods - it's a powerful shift in behavior that we're just starting to see. According to our survey, most Americans have never done this. They've never bought basic household or nonperishable goods online. But the people who do, like Hinchey, tend to rely on Amazon. Convenience is habit-forming, and it's something founder and CEO Jeff Bezos knew early on. Here's what he told NPR in 1999. (SOUNDBITE OF ARCHIVED BROADCAST) JEFF BEZOS: Our No. 1 mission is to be Earth's most customer-centric company. And we mean that across any industry and across any time. SELYUKH: This obsession with customers has catapulted Amazon into a company worth $800 billion today. It's made Bezos the richest man in the world. Amazon now employs more than half a million workers. And we should note - the company is one of NPR's underwriters. Amazon is now making movies and TV shows, storing government data on the cloud, selling internet-connected door locks, making the popular Alexa smart speaker. When a company gets to be this far-reaching, critics emerge from all directions. And for Amazon, the most prominent one is President Trump. Bezos was asked about this by the head of a media company called Axel Springer in April. The question was, what if Trump decided that Amazon was too big and should be broken up? Here's Bezos. (SOUNDBITE OF ARCHIVED RECORDING) BEZOS: For me - again, this is one of those things where I focus on what we can control. And I expect, whether it's, you know, the current U. S. administration or any other government agency anywhere in the world, Amazon is now a large corporation. And I expect us to be scrutinized. SELYUKH: But what the scrutiny entails is unclear. Legal scholar Lina Khan has been studying the power and impact of Amazon. She says that the government and the courts apply the laws in a way that makes it difficult to check Amazon's dominance. LINA KHAN: Antitrust laws have become very focused on consumer welfare. SELYUKH: She's saying the laws focus on making sure that consumers have enough choices or don't get overcharged, for example. But with Amazon, Khan argues the issue is not the short-term interest of shoppers but the impact on the suppliers and the retailers and the competitors in all the areas where Amazon looms large. Many of them now depend on Amazon to reach their own customers. KHAN: And it shows how if you are amassing market power, antitrusts won't necessarily respond to that anymore unless it's obvious that you're also hurting consumers in some way. SELYUKH: Khan has her own critics. But that's the thing about Amazon. It reaches far and wide, but customers trust it - a lot. In our survey, a majority of online shoppers said they don't have confidence in most online retailers when it comes to protecting their privacy, but 2 out of 3 said they do trust Amazon. And that's a big reason why Americans are OK with Amazon's reign as the top online retailer and brown boxes keep piling up in my building and on doorsteps across the country. Alina Selyukh, NPR News. (SOUNDBITE OF J'SAN'S \"GROOVY LIFE\")", "section": "Online Shopping And The Power Of Amazon", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-07-618076844": {"title": "Facebook To Users: You May Want To Update Your Privacy Settings Again.  : NPR", "url": "https://www.npr.org/2018/06/07/618076844/facebook-to-users-you-may-want-to-update-your-privacy-settings-again", "author": "No author found", "published_date": "2018-06-07", "content": "", "section": "Technology", "disclaimer": ""}, "2018-06-07-617882254": {"title": "Dresses Flutter On Drones In Saudi Fashion Show, But Critics Aren't Buying It : NPR", "url": "https://www.npr.org/2018/06/07/617882254/dresses-flutter-on-drones-in-saudi-fashion-show-but-critics-arent-buying-it", "author": "No author found", "published_date": "2018-06-07", "content": "", "section": "News", "disclaimer": ""}, "2018-06-07-617849382": {"title": "China's ZTE To Pay $1 Billion Fine To Settle U.S. Trade Case : NPR", "url": "https://www.npr.org/2018/06/07/617849382/chinas-zte-to-pay-1-billion-fine-to-settle-u-s-trade-case", "author": "No author found", "published_date": "2018-06-07", "content": "STEVE INSKEEP, HOST:  We have some news this morning. The United States has struck a deal to save the Chinese telecom company ZTE. This deal was announced this morning by Commerce Secretary Wilbur Ross. ZTE, you may recall, had been pushed to the brink of insolvency by a ban that prevented it from buying American-made components for its phones. President Trump then in a surprising move promised to save Chinese jobs by somehow saving the company, which, the U. S. now thinks it's done. NPR's Uri Berliner is covering this story. He's with us. Hey there, Uri. URI BERLINER, BYLINE: Hey, Steve. INSKEEP: I want to begin by just playing a little bit of Wilbur Ross here - this is from CNBC - explaining what the United States has done. (SOUNDBITE OF ARCHIVED RECORDING)WILBUR ROSS: We think this settlement which brought the company, a $17 billion company, to its knees, more or less put them out of business. Now they're accepting having this compliance team in, whole new management, whole new board should serve as a very strong deterrent not only for them but for other potential bad actors. INSKEEP: OK. A lot to go through there, Uri. First, which brought the company, a $17 billion company to its knees. What was it that ZTE did that brought it to its knees, that caused the United States to punish it in the first place? BERLINER: Well, let's go back. First ZTE violated U. S. sanctions by doing business with Iran and North Korea. And then ZTE violated a 2017 settlement agreement that was related to those sanctions. U. S. officials said ZTE lied during those talks. So essentially it was a two-time offender. INSKEEP: OK. So that's what ZTE did. Then they got this extreme punishment. What would the reason be then that the United States would have any incentive to let them out of it? BERLINER: Well, I mean, there's a summit coming up with Kim Jong Un. President Trump would like Chinese cooperation as that settlement - as that summit gets underway. That's definitely one part. And there is a lot of horse trading over trade right now, and sort of ZTE is. . . INSKEEP: Trade disputes between the United States and China. Isn't there also a personal aspect to this? President Trump got a request from China's President Xi Jinping to please do something about this company? BERLINER: He did mention working with Xi Jinping in his tweet, yes. INSKEEP: OK. So then the news is that there is this deal. What are the terms exactly of the deal? What does ZTE get to do now that it was going to be prevented from doing? BERLINER: Well, first ZTE has to pay a $1 billion fine and then put $400 million in escrow, in an escrow account in case it commits future violations. And the U. S. gets to choose a team of compliance officers that will be embedded inside of ZTE to make sure they're sticking to the agreement. The company has 30 days to get rid of its board and top executive team and replace them, and then ZTE gets to trade with American companies again. INSKEEP: OK. So they're not banned from trading with the United States, but there are still some consequences. The whole board has to go away, and the top executives have to go away. BERLINER: Yeah. I mean, Secretary Ross really remarked about how strict this deal was, especially this thing of the U. S. getting to pick people to embed within a Chinese company to really see what they're doing. INSKEEP: And so the idea is, they lied before - hopefully, they can be caught if they're lying again? BERLINER: Yeah. I guess the idea is that there'll be strict compliance right now. INSKEEP: Very briefly, though, aren't there also national security concerns about this company and whether it was spying on people, including Americans, or could be? BERLINER: Well, there have been some national security concerns. Last month the Pentagon said that ZTE phones couldn't be sold on American military bases. INSKEEP: OK. Uri, thanks very much. Really appreciate it. BERLINER: You're welcome. INSKEEP: That's NPR's Uri Berliner giving us details of this deal announced with ZTE, the Chinese telecom company, and the United States government. STEVE INSKEEP, HOST:   We have some news this morning. The United States has struck a deal to save the Chinese telecom company ZTE. This deal was announced this morning by Commerce Secretary Wilbur Ross. ZTE, you may recall, had been pushed to the brink of insolvency by a ban that prevented it from buying American-made components for its phones. President Trump then in a surprising move promised to save Chinese jobs by somehow saving the company, which, the U. S. now thinks it's done. NPR's Uri Berliner is covering this story. He's with us. Hey there, Uri. URI BERLINER, BYLINE: Hey, Steve. INSKEEP: I want to begin by just playing a little bit of Wilbur Ross here - this is from CNBC - explaining what the United States has done. (SOUNDBITE OF ARCHIVED RECORDING) WILBUR ROSS: We think this settlement which brought the company, a $17 billion company, to its knees, more or less put them out of business. Now they're accepting having this compliance team in, whole new management, whole new board should serve as a very strong deterrent not only for them but for other potential bad actors. INSKEEP: OK. A lot to go through there, Uri. First, which brought the company, a $17 billion company to its knees. What was it that ZTE did that brought it to its knees, that caused the United States to punish it in the first place? BERLINER: Well, let's go back. First ZTE violated U. S. sanctions by doing business with Iran and North Korea. And then ZTE violated a 2017 settlement agreement that was related to those sanctions. U. S. officials said ZTE lied during those talks. So essentially it was a two-time offender. INSKEEP: OK. So that's what ZTE did. Then they got this extreme punishment. What would the reason be then that the United States would have any incentive to let them out of it? BERLINER: Well, I mean, there's a summit coming up with Kim Jong Un. President Trump would like Chinese cooperation as that settlement - as that summit gets underway. That's definitely one part. And there is a lot of horse trading over trade right now, and sort of ZTE is. . . INSKEEP: Trade disputes between the United States and China. Isn't there also a personal aspect to this? President Trump got a request from China's President Xi Jinping to please do something about this company? BERLINER: He did mention working with Xi Jinping in his tweet, yes. INSKEEP: OK. So then the news is that there is this deal. What are the terms exactly of the deal? What does ZTE get to do now that it was going to be prevented from doing? BERLINER: Well, first ZTE has to pay a $1 billion fine and then put $400 million in escrow, in an escrow account in case it commits future violations. And the U. S. gets to choose a team of compliance officers that will be embedded inside of ZTE to make sure they're sticking to the agreement. The company has 30 days to get rid of its board and top executive team and replace them, and then ZTE gets to trade with American companies again. INSKEEP: OK. So they're not banned from trading with the United States, but there are still some consequences. The whole board has to go away, and the top executives have to go away. BERLINER: Yeah. I mean, Secretary Ross really remarked about how strict this deal was, especially this thing of the U. S. getting to pick people to embed within a Chinese company to really see what they're doing. INSKEEP: And so the idea is, they lied before - hopefully, they can be caught if they're lying again? BERLINER: Yeah. I guess the idea is that there'll be strict compliance right now. INSKEEP: Very briefly, though, aren't there also national security concerns about this company and whether it was spying on people, including Americans, or could be? BERLINER: Well, there have been some national security concerns. Last month the Pentagon said that ZTE phones couldn't be sold on American military bases. INSKEEP: OK. Uri, thanks very much. Really appreciate it. BERLINER: You're welcome. INSKEEP: That's NPR's Uri Berliner giving us details of this deal announced with ZTE, the Chinese telecom company, and the United States government.", "section": "Business", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-08-617858833": {"title": "Online Returns: In New Poll, Shoppers Say They Hardly Ever Make Them. Um, What? : NPR", "url": "https://www.npr.org/2018/06/08/617858833/online-shoppers-say-they-rarely-return-purchases-why", "author": "No author found", "published_date": "2018-06-08", "content": "MARY LOUISE KELLY, HOST:  Now for a story most of us can probably relate to. You buy something online, and then you change your mind. So what to do - keep it, regift, send it back? Well, Americans had interesting things to say about their return habits in a new NPR/Marist Poll, so interesting that some of the responses surprised our retail reporter Alina Selyukh. ALINA SELYUKH, BYLINE: When you follow retail, you hear a lot about returns - that online shopping produces a ton of them. LARISA SUMMERS: It was $385 billion worth in one year last year. SELYUKH: That's 385. . . SUMMERS: Billion dollars, with a B, returned inventory. SELYUKH: Larisa Summers works for a company called up Optoro which is built around returns, helping retailers repurpose or resell the stuff people send back. SUMMERS: In some categories, 20 to 30 percent of goods get returned. SELYUKH: I went to Summers for a professional reality check because I'm trying to make sense of some fascinating results from the new NPR/Marist poll. Ninety-one percent of American online shoppers told us they rarely or never return things they buy online. And my immediate thought was, where are these people who never return things? PAT NOVAK: My name is Pat Novak, and I live north of Grand Rapids, Mich. SELYUKH: Novak rarely returns things for this straightforward reason. NOVAK: I don't enjoy shopping. It's not my favorite thing to do. SELYUKH: Not her favorite thing to do. Novak likes finding exactly the right thing before buying it so she doesn't have to shop again. Katie Burns from San Francisco has another reason why people don't return things. KATIE BURNS: I do have a basket in my apartment of things that I fully intend to return that have been there for more than a year, probably, that I have not gotten around to. SELYUKH: And now she's missed the return window for most of them. In our survey, a majority of online shoppers say they have indeed kept purchases they'd meant to return mainly to avoid the hassle. Now, another number in the polls surprised me the most. It has to do with how I personally shop, which is buy bunch of sweaters in different sizes, return what doesn't fit. Almost everyone - 94 percent of online shoppers - told us they rarely or never make a purchase expecting to return part of it. Am I really in such a tiny minority? STACEY STEINER: That surprises me. SELYUKH: Hey, there are two of us, me and Stacey Steiner from Jacksonville, Fla. STEINER: I'll go every once in a while, save up money, and then I'll just do a huge batch order. SELYUKH: Recently she wanted new dresses for her birthday. STEINER: So I think I bought, like, 13 dresses or something. SELYUKH: Oh, wow. STEINER: And I was able to try them on. And I picked three that I kept. SELYUKH: And guess who else shops like Steiner and I - director of the Marist Poll, Barbara Carvalho. BARBARA CARVALHO: I agree. The expectation was that we were going to see a very large proportion of people that return things. SELYUKH: But remember; we had 91 percent say they hardly ever returned things. My theory - asking about the frequency of returns was like calling people and saying, do you floss every day? But Carvalho says the poll accounts for this by suggesting a range of answers instead of a yes or no. And more importantly, I lost sight of the other side, the 9 percent of online shoppers who admit to making returns often or very often. CARVALHO: It actually translates into almost 16 million adults in the U. S. SELYUKH: A small percentage of shoppers giving retailers big headaches with costly returns. But sometimes the retailers have themselves to blame. Remember Novak from Michigan who doesn't like shopping? Last year, her husband ordered a small fuel tank for a camp stove but instead received a 5-foot-tall cooler. NOVAK: If I could return it, I would return the stupid cooler. They won't take it back. SELYUKH: She says the company only allows returns for warranty or defects, not wrong shipments. So now that's another reason why Novak does not return things. Alina Selyukh, NPR News. MARY LOUISE KELLY, HOST:   Now for a story most of us can probably relate to. You buy something online, and then you change your mind. So what to do - keep it, regift, send it back? Well, Americans had interesting things to say about their return habits in a new NPR/Marist Poll, so interesting that some of the responses surprised our retail reporter Alina Selyukh. ALINA SELYUKH, BYLINE: When you follow retail, you hear a lot about returns - that online shopping produces a ton of them. LARISA SUMMERS: It was $385 billion worth in one year last year. SELYUKH: That's 385. . . SUMMERS: Billion dollars, with a B, returned inventory. SELYUKH: Larisa Summers works for a company called up Optoro which is built around returns, helping retailers repurpose or resell the stuff people send back. SUMMERS: In some categories, 20 to 30 percent of goods get returned. SELYUKH: I went to Summers for a professional reality check because I'm trying to make sense of some fascinating results from the new NPR/Marist poll. Ninety-one percent of American online shoppers told us they rarely or never return things they buy online. And my immediate thought was, where are these people who never return things? PAT NOVAK: My name is Pat Novak, and I live north of Grand Rapids, Mich. SELYUKH: Novak rarely returns things for this straightforward reason. NOVAK: I don't enjoy shopping. It's not my favorite thing to do. SELYUKH: Not her favorite thing to do. Novak likes finding exactly the right thing before buying it so she doesn't have to shop again. Katie Burns from San Francisco has another reason why people don't return things. KATIE BURNS: I do have a basket in my apartment of things that I fully intend to return that have been there for more than a year, probably, that I have not gotten around to. SELYUKH: And now she's missed the return window for most of them. In our survey, a majority of online shoppers say they have indeed kept purchases they'd meant to return mainly to avoid the hassle. Now, another number in the polls surprised me the most. It has to do with how I personally shop, which is buy bunch of sweaters in different sizes, return what doesn't fit. Almost everyone - 94 percent of online shoppers - told us they rarely or never make a purchase expecting to return part of it. Am I really in such a tiny minority? STACEY STEINER: That surprises me. SELYUKH: Hey, there are two of us, me and Stacey Steiner from Jacksonville, Fla. STEINER: I'll go every once in a while, save up money, and then I'll just do a huge batch order. SELYUKH: Recently she wanted new dresses for her birthday. STEINER: So I think I bought, like, 13 dresses or something. SELYUKH: Oh, wow. STEINER: And I was able to try them on. And I picked three that I kept. SELYUKH: And guess who else shops like Steiner and I - director of the Marist Poll, Barbara Carvalho. BARBARA CARVALHO: I agree. The expectation was that we were going to see a very large proportion of people that return things. SELYUKH: But remember; we had 91 percent say they hardly ever returned things. My theory - asking about the frequency of returns was like calling people and saying, do you floss every day? But Carvalho says the poll accounts for this by suggesting a range of answers instead of a yes or no. And more importantly, I lost sight of the other side, the 9 percent of online shoppers who admit to making returns often or very often. CARVALHO: It actually translates into almost 16 million adults in the U. S. SELYUKH: A small percentage of shoppers giving retailers big headaches with costly returns. But sometimes the retailers have themselves to blame. Remember Novak from Michigan who doesn't like shopping? Last year, her husband ordered a small fuel tank for a camp stove but instead received a 5-foot-tall cooler. NOVAK: If I could return it, I would return the stupid cooler. They won't take it back. SELYUKH: She says the company only allows returns for warranty or defects, not wrong shipments. So now that's another reason why Novak does not return things. Alina Selyukh, NPR News.", "section": "Online Shopping And The Power Of Amazon", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-10-618648585": {"title": "U.S. Takes Supercomputer Crown From China : NPR", "url": "https://www.npr.org/2018/06/10/618648585/u-s-takes-supercomputer-crown-from-china", "author": "No author found", "published_date": "2018-06-10", "content": "LULU GARCIA-NAVARRO, HOST: And before the break, a quick aside on international relations and summits - China and the U. S. regularly spar over trade and foreign policy and now supercomputers. The U. S. Department of Energy's Oak Ridge National Laboratory this past week unveiled the aptly named Summit. Oak Ridge says it's the world's most powerful and smartest scientific supercomputer, taking the top spot from China. The last time a U. S. supercomputer held the crown was in 2012. And get this - Summit has a peak performance of two hundred thousand trillion calculations per second or 200 petaflops - peta what? OK. Here's an easier way to think about it. If every person on Earth completed one calculation per second, it would take the world population 305 days to do what Summit can do in one second. Whoa. But maybe the United States shouldn't gloat too much about having this computer. China might not have the speediest supercomputer in the world any longer, but according to an industry ranking of the 500 fastest supercomputers in the world, China has the world's most supercomputers overall, with the U. S. in second place. LULU GARCIA-NAVARRO, HOST:  And before the break, a quick aside on international relations and summits - China and the U. S. regularly spar over trade and foreign policy and now supercomputers. The U. S. Department of Energy's Oak Ridge National Laboratory this past week unveiled the aptly named Summit. Oak Ridge says it's the world's most powerful and smartest scientific supercomputer, taking the top spot from China. The last time a U. S. supercomputer held the crown was in 2012. And get this - Summit has a peak performance of two hundred thousand trillion calculations per second or 200 petaflops - peta what? OK. Here's an easier way to think about it. If every person on Earth completed one calculation per second, it would take the world population 305 days to do what Summit can do in one second. Whoa. But maybe the United States shouldn't gloat too much about having this computer. China might not have the speediest supercomputer in the world any longer, but according to an industry ranking of the 500 fastest supercomputers in the world, China has the world's most supercomputers overall, with the U. S. in second place.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-11-618928905": {"title": "Net Neutrality Has Been Rolled Back \u2014 But It's Not Dead Yet : NPR", "url": "https://www.npr.org/2018/06/11/618928905/net-neutrality-has-been-rolled-back-but-its-not-dead-yet", "author": "No author found", "published_date": "2018-06-11", "content": "", "section": "Politics", "disclaimer": ""}, "2018-06-11-618912309": {"title": "Cryptocurrencies Lose Billions In Value After An Exchange Is Hacked : NPR", "url": "https://www.npr.org/2018/06/11/618912309/cryptocurrencies-lose-billions-in-value-after-an-exchange-is-hacked", "author": "No author found", "published_date": "2018-06-11", "content": "", "section": "Economy", "disclaimer": ""}, "2018-06-11-618039318": {"title": "Online Big Spenders Tend To Be Men, NPR/Marist Poll Shows : NPR", "url": "https://www.npr.org/2018/06/11/618039318/online-big-spenders-tend-to-be-men-npr-marist-polls-shows", "author": "No author found", "published_date": "2018-06-11", "content": "DAVID GREENE, HOST: Americans are not just shopping online. They are buying pricey items on the Web. Nearly 3 out of 10 online shoppers in the United States have bought a product that costs a thousand dollars or more. That is according to an NPR/Marist poll. And as NPR's Uri Berliner reports, those big online spenders are much more likely to be men than women. URI BERLINER, BYLINE: The early days of the Web were pretty cumbersome. (SOUNDBITE OF AOL AD)UNIDENTIFIED ACTOR #1: (As character) How long have you had this? UNIDENTIFIED ACTOR #2: (As character) About a week. And it's so easy. All you do is point and click. UNIDENTIFIED ACTOR #1: (As character) But how does it work? UNIDENTIFIED ACTOR #2: (As character) All you need is a computer and a regular phone line. BERLINER: That's an AOL ad from the 1990s. Back then online shopping wasn't just awkward. . . (SOUNDBITE OF DIAL-UP MODEM)BERLINER: It felt risky. People were afraid to give out their credit card numbers. They worried, if products would arrive, would they be able to return them? Sucharita Kodali is an analyst with Forrester Research. She says gradually online shoppers dipped their toes in the water. SUCHARITA KODALI: And it would start with relatively low-ticket transactions, like a book, and that really was, I think, the beginning of it. BERLINER: The it she's talking about is the explosion in Internet shopping. Last year, U. S. shoppers spent $453 billion on retail purchases online. And many of them go big. Twenty-seven percent of online shoppers have bought an item that costs $1,000 or more. What do they buy? Just about anything. CARA VETOR: I purchased an infrared sauna online for about $1,100 in 2015. ANDY PENNELL: I bought a full-size movie prop of Han Solo in carbonite for $1,800. ALAN KWOK: I bought a lab-grown diamond engagement ring online for $11,000. KENNA MCKENZIE: Two harps. My latest harp cost $1,800. BERLINER: That's Cara Vetor of Indianapolis, Ind. , Andy Pennell of Kirkland, Wash. , Alan Kwok of Austin, Texas, and Kenna McKenzie of Tucumcari, N. M. If you dig a little deeper into the habits of online spenders, a striking fact emerges. Men are twice as likely as women to buy an item online that costs a thousand dollars or more. Guys have twitchier fingers. Kodali says she's not surprised. Men are usually the early adopters when it comes to technology. KODALI: If you were to consider, you know, high-end e-commerce purchases, and something that, you know, has historically had a lot of hold outs, the gender that I would expect to be the first to experiment with it would, of course, be men. And I think that's why you're seeing the breakout that you're seeing. BERLINER: Americus Reed, a professor of marketing at Wharton, believes there's another factor at work, too. AMERICUS REED: What the research has shown is that men tend to, if they are going to check something out, go to the store quickly, kind of gather as much information as they can quickly and just buy it online, particularly because it's just faster. BERLINER: Rick Hays of Tallahassee, Fla. , has no qualms about a big purchase he made last year at an online auction, even though it's just two sentences long. RICK HAYS: I purchased a note that Mark Twain had written regarding a missing package, and I paid about $1,500 for it. BERLINER: Hays says his wife has very different buying habits. HAYS: She struggled with buying a, you know, a hundred-dollar dress recently online. You know, she wants the experience of seeing it, the tactile experience, I think, of being around whatever item she wants to get. BERLINER: Hays says there may be another reason he's the more adventurous online shopper. His wife is still working. Hays is retired so he's got more time to browse. And just last week, Hays struck again, buying a signed picture of Muhammad Ali for $1,020. He says he was willing to go higher. Uri Berliner, NPR News. (SOUNDBITE OF SONG, \"EBAY SONG\")WEIRD AL YANKOVIC: (Singing) I bought it on eBay. Want to buy a \"Pac-Man Fever\" lunchbox? Want to buy a case of vintage tube socks? DAVID GREENE, HOST:  Americans are not just shopping online. They are buying pricey items on the Web. Nearly 3 out of 10 online shoppers in the United States have bought a product that costs a thousand dollars or more. That is according to an NPR/Marist poll. And as NPR's Uri Berliner reports, those big online spenders are much more likely to be men than women. URI BERLINER, BYLINE: The early days of the Web were pretty cumbersome. (SOUNDBITE OF AOL AD) UNIDENTIFIED ACTOR #1: (As character) How long have you had this? UNIDENTIFIED ACTOR #2: (As character) About a week. And it's so easy. All you do is point and click. UNIDENTIFIED ACTOR #1: (As character) But how does it work? UNIDENTIFIED ACTOR #2: (As character) All you need is a computer and a regular phone line. BERLINER: That's an AOL ad from the 1990s. Back then online shopping wasn't just awkward. . . (SOUNDBITE OF DIAL-UP MODEM) BERLINER: It felt risky. People were afraid to give out their credit card numbers. They worried, if products would arrive, would they be able to return them? Sucharita Kodali is an analyst with Forrester Research. She says gradually online shoppers dipped their toes in the water. SUCHARITA KODALI: And it would start with relatively low-ticket transactions, like a book, and that really was, I think, the beginning of it. BERLINER: The it she's talking about is the explosion in Internet shopping. Last year, U. S. shoppers spent $453 billion on retail purchases online. And many of them go big. Twenty-seven percent of online shoppers have bought an item that costs $1,000 or more. What do they buy? Just about anything. CARA VETOR: I purchased an infrared sauna online for about $1,100 in 2015. ANDY PENNELL: I bought a full-size movie prop of Han Solo in carbonite for $1,800. ALAN KWOK: I bought a lab-grown diamond engagement ring online for $11,000. KENNA MCKENZIE: Two harps. My latest harp cost $1,800. BERLINER: That's Cara Vetor of Indianapolis, Ind. , Andy Pennell of Kirkland, Wash. , Alan Kwok of Austin, Texas, and Kenna McKenzie of Tucumcari, N. M. If you dig a little deeper into the habits of online spenders, a striking fact emerges. Men are twice as likely as women to buy an item online that costs a thousand dollars or more. Guys have twitchier fingers. Kodali says she's not surprised. Men are usually the early adopters when it comes to technology. KODALI: If you were to consider, you know, high-end e-commerce purchases, and something that, you know, has historically had a lot of hold outs, the gender that I would expect to be the first to experiment with it would, of course, be men. And I think that's why you're seeing the breakout that you're seeing. BERLINER: Americus Reed, a professor of marketing at Wharton, believes there's another factor at work, too. AMERICUS REED: What the research has shown is that men tend to, if they are going to check something out, go to the store quickly, kind of gather as much information as they can quickly and just buy it online, particularly because it's just faster. BERLINER: Rick Hays of Tallahassee, Fla. , has no qualms about a big purchase he made last year at an online auction, even though it's just two sentences long. RICK HAYS: I purchased a note that Mark Twain had written regarding a missing package, and I paid about $1,500 for it. BERLINER: Hays says his wife has very different buying habits. HAYS: She struggled with buying a, you know, a hundred-dollar dress recently online. You know, she wants the experience of seeing it, the tactile experience, I think, of being around whatever item she wants to get. BERLINER: Hays says there may be another reason he's the more adventurous online shopper. His wife is still working. Hays is retired so he's got more time to browse. And just last week, Hays struck again, buying a signed picture of Muhammad Ali for $1,020. He says he was willing to go higher. Uri Berliner, NPR News. (SOUNDBITE OF SONG, \"EBAY SONG\") WEIRD AL YANKOVIC: (Singing) I bought it on eBay. Want to buy a \"Pac-Man Fever\" lunchbox? Want to buy a case of vintage tube socks?", "section": "Online Shopping And The Power Of Amazon", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-12-619072445": {"title": "Senators Move To Reverse Trump's Deal Lifting Sanctions On China's ZTE : NPR", "url": "https://www.npr.org/2018/06/12/619072445/senators-reverse-trumps-deal-to-keep-chinas-zte-alive", "author": "No author found", "published_date": "2018-06-12", "content": "", "section": "News", "disclaimer": ""}, "2018-06-13-619258265": {"title": "World Cup 2018: How To Watch The Matches Online And On TV : NPR", "url": "https://www.npr.org/2018/06/13/619258265/world-cup-2018-how-to-watch-the-matches-online-and-on-tv", "author": "No author found", "published_date": "2018-06-13", "content": "", "section": "World Cup", "disclaimer": ""}, "2018-06-13-619444956": {"title": "Seattle Repeals Tax On Big Business After Opposition From Amazon, Starbucks : NPR", "url": "https://www.npr.org/2018/06/13/619444956/seattle-repeals-tax-on-big-business-after-opposition-from-amazon-starbucks", "author": "No author found", "published_date": "2018-06-13", "content": "", "section": "National", "disclaimer": ""}, "2018-06-13-619426602": {"title": "Tesla Lays Off 9 Percent Of Workforce : NPR", "url": "https://www.npr.org/2018/06/13/619426602/tesla-lays-off-9-percent-of-workforce", "author": "No author found", "published_date": "2018-06-13", "content": "", "section": "Technology", "disclaimer": ""}, "2018-06-14-619488792": {"title": "Activists In Myanmar Say Facebook Needs To Do More To Quell Hate Speech : NPR", "url": "https://www.npr.org/2018/06/14/619488792/activists-in-myanmar-say-facebook-needs-to-do-more-to-quell-hate-speech", "author": "No author found", "published_date": "2018-06-14", "content": "", "section": "World", "disclaimer": ""}, "2018-06-14-619993903": {"title": "Elon Musk's Boring Company Will Build High-Speed Train To Chicago's O'Hare Airport : NPR", "url": "https://www.npr.org/2018/06/14/619993903/elon-musks-boring-company-will-build-high-speed-train-to-chicago-s-o-hare-airpor", "author": "No author found", "published_date": "2018-06-14", "content": "", "section": "Business", "disclaimer": ""}, "2018-06-15-620393860": {"title": "As Vote On ZTE Sanctions Looms, Some U.S. Lawmakers Focus On Huawei : NPR", "url": "https://www.npr.org/2018/06/15/620393860/as-vote-on-zte-sanctions-looms-some-u-s-lawmakers-focus-on-a-bigger-chinese-tele", "author": "No author found", "published_date": "2018-06-15", "content": "", "section": "Business", "disclaimer": ""}, "2018-06-15-620259820": {"title": "Trump Hits China With Tariffs On $50 Billion Of Goods; China Says It Will Retaliate : NPR", "url": "https://www.npr.org/2018/06/15/620259820/trump-levies-50-billion-in-tariffs-as-china-says-it-will-retaliate", "author": "No author found", "published_date": "2018-06-15", "content": "", "section": "Business", "disclaimer": ""}, "2018-06-18-621073544": {"title": "Apple iOS Update To Include Automatic Location Sharing With 911 Centers : NPR", "url": "https://www.npr.org/2018/06/18/621073544/this-apple-update-could-prove-to-be-a-true-lifesaver", "author": "No author found", "published_date": "2018-06-18", "content": "", "section": "Technology", "disclaimer": ""}, "2018-06-18-620953719": {"title": "Audi CEO Arrested In Connection With Volkswagen Emissions Scandal : NPR", "url": "https://www.npr.org/2018/06/18/620953719/audi-ceo-arrested-in-connection-with-volkswagen-emissions-scandal", "author": "No author found", "published_date": "2018-06-18", "content": "", "section": "Business", "disclaimer": ""}, "2018-06-18-617874348": {"title": "Bureaucracy And Politics Slow Election Security Funding To States : NPR", "url": "https://www.npr.org/2018/06/18/617874348/bureaucracy-and-politics-slow-election-security-funding-to-states", "author": "No author found", "published_date": "2018-06-18", "content": "", "section": "Politics", "disclaimer": ""}, "2018-06-18-620005246": {"title": "A Guide To Parental Controls For Kids' Tech Use : NPR", "url": "https://www.npr.org/2018/06/18/620005246/a-guide-to-parental-controls-for-kids-tech-use", "author": "No author found", "published_date": "2018-06-18", "content": "MARY LOUISE KELLY, HOST: The battle over screen time for families - that's what we're exploring this week in All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")KELLY: Apple recently became the latest company to announce they are arming parents with stronger weapons. But if you surf over to YouTube, you'll find kids trading tricks and tips for getting around these parental controls. Anya Kamenetz of the Ed team reports on a contest of wills in cyberspace. ANYA KAMENETZ, BYLINE: Apple CEO Tim Cook talked to NPR earlier this month about how to get kids using its products less with the company's new screen time controls. (SOUNDBITE OF ARCHIVED BROADCAST)TIM COOK: We've been doing things for parental control since the creation of the App Store, but this gives parents another huge tool to use. KAMENETZ: Apple's not alone. Google has Family Link. Disney has a product called Circle. And Amazon has the slightly Orwellian-sounding FreeTime. All these products promise to help busy parents enforce time limits and steer kids to pre-approved apps, games and videos. There's just one problem - the kids are fighting back. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: Today I'm going to show you how to hack parental controls. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: I'm going to show you how to take your parental controls off of a Kindle Fire 10 - I have a 10. (SOUNDBITE OF ARCHIVED RECORDING)BEN ZIMMERMAN: I found this glitch out on Family Link. If you. . . PHILIP ZIMMERMAN: You bypass the parent code. BEN: Yep. KAMENETZ: Those are all videos posted on YouTube about how to get around parental controls. And there are questions and further help in the comments and on other sites like Reddit. It's like tech support for beating your parents. Ben Zimmerman is 9 years old and lives in a suburb of Chicago. You can hear him on that last video sharing a bypass for Google's Family Link. BEN: All the YouTube videos that I watched about trying to get in Family Link never worked. KAMENETZ: When he couldn't find one, he figured out his own. Did you punish him? ZIMMERMAN: Absolutely not, absolutely not. I was actually very impressed. KAMENETZ: That's Ben's dad, Philip. Ben actually came to him with the workaround that he found for Family Link, and they posted a video together. Why? BEN: Well, I kind of want them to fix it so I can try to find another glitch for them to fix. KAMENETZ: Google says that the glitch has now been fixed, and it works with device manufacturers to address these hacks whenever they're found. Some experts say it can be a good learning experience for young people to start getting under the hood of technology in this way. But depending on the kid, breaking the rules and sneaking around to binge on screens can also be a sign of a real problem. And not every family has the time or the tech knowledge to oversee their kids' use so closely. So how much are these tools really designed to help families versus provide good PR for the companies? I asked Apple, Amazon, Google and Disney Circle, but none would say how many parents actually use the controls. KURT BEIDLER: Parents are involved, and 9 out of 10 parents actually want to be even more involved than they are today. So what that means to individual parents, I think, varies. KAMENETZ: Kurt Beidler is the Director of Kids and Family for Amazon devices. BEIDLER: I think software is useful as a tool to enforce the contract that you've already entered into with your child. KAMENETZ: He says no matter how good the software is, they'll never be able to replace parents. Anya Kamenetz, NPR News. MARY LOUISE KELLY, HOST:  The battle over screen time for families - that's what we're exploring this week in All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") KELLY: Apple recently became the latest company to announce they are arming parents with stronger weapons. But if you surf over to YouTube, you'll find kids trading tricks and tips for getting around these parental controls. Anya Kamenetz of the Ed team reports on a contest of wills in cyberspace. ANYA KAMENETZ, BYLINE: Apple CEO Tim Cook talked to NPR earlier this month about how to get kids using its products less with the company's new screen time controls. (SOUNDBITE OF ARCHIVED BROADCAST) TIM COOK: We've been doing things for parental control since the creation of the App Store, but this gives parents another huge tool to use. KAMENETZ: Apple's not alone. Google has Family Link. Disney has a product called Circle. And Amazon has the slightly Orwellian-sounding FreeTime. All these products promise to help busy parents enforce time limits and steer kids to pre-approved apps, games and videos. There's just one problem - the kids are fighting back. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: Today I'm going to show you how to hack parental controls. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #2: I'm going to show you how to take your parental controls off of a Kindle Fire 10 - I have a 10. (SOUNDBITE OF ARCHIVED RECORDING) BEN ZIMMERMAN: I found this glitch out on Family Link. If you. . . PHILIP ZIMMERMAN: You bypass the parent code. BEN: Yep. KAMENETZ: Those are all videos posted on YouTube about how to get around parental controls. And there are questions and further help in the comments and on other sites like Reddit. It's like tech support for beating your parents. Ben Zimmerman is 9 years old and lives in a suburb of Chicago. You can hear him on that last video sharing a bypass for Google's Family Link. BEN: All the YouTube videos that I watched about trying to get in Family Link never worked. KAMENETZ: When he couldn't find one, he figured out his own. Did you punish him? ZIMMERMAN: Absolutely not, absolutely not. I was actually very impressed. KAMENETZ: That's Ben's dad, Philip. Ben actually came to him with the workaround that he found for Family Link, and they posted a video together. Why? BEN: Well, I kind of want them to fix it so I can try to find another glitch for them to fix. KAMENETZ: Google says that the glitch has now been fixed, and it works with device manufacturers to address these hacks whenever they're found. Some experts say it can be a good learning experience for young people to start getting under the hood of technology in this way. But depending on the kid, breaking the rules and sneaking around to binge on screens can also be a sign of a real problem. And not every family has the time or the tech knowledge to oversee their kids' use so closely. So how much are these tools really designed to help families versus provide good PR for the companies? I asked Apple, Amazon, Google and Disney Circle, but none would say how many parents actually use the controls. KURT BEIDLER: Parents are involved, and 9 out of 10 parents actually want to be even more involved than they are today. So what that means to individual parents, I think, varies. KAMENETZ: Kurt Beidler is the Director of Kids and Family for Amazon devices. BEIDLER: I think software is useful as a tool to enforce the contract that you've already entered into with your child. KAMENETZ: He says no matter how good the software is, they'll never be able to replace parents. Anya Kamenetz, NPR News.", "section": "All Tech Considered", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-19-621331738": {"title": "Former CIA Employee Charged In Massive Leak Of Confidential Information : NPR", "url": "https://www.npr.org/2018/06/19/621331738/former-cia-employee-charged-in-massive-leak-of-confidential-information", "author": "No author found", "published_date": "2018-06-19", "content": "", "section": "Law", "disclaimer": ""}, "2018-06-19-619006003": {"title": "This Fashion Brand Grew On Instagram With Help Of Celebs, Relatable Models : NPR", "url": "https://www.npr.org/2018/06/19/619006003/this-fashion-brand-grew-on-instagram-with-help-of-celebs-relatable-models", "author": "No author found", "published_date": "2018-06-19", "content": "MARY LOUISE KELLY, HOST: If you've listened to Cardi B, you know she has expensive taste. (SOUNDBITE OF SONG, \"I LIKE IT\")CARDI B: (Rapping) I like those Balenciagas, the ones look like socks. I like going to the jeweler. I put rocks all in my watch. KELLY: But Cardi B is a spokesperson for a brand that is much more affordable than Balenciaga. A new NPR-Marist poll finds that online shoppers are roughly divided when it comes to shopping with brand loyalty in mind versus a good deal. Well, Fashion Nova does both. The company is known for sexy, affordable clothing worn by celebrities and Instagram models. Jasmine Garsd reports on how Fashion Nova came to become one of the top trending fashion brands on Google. JASMINE GARSD, BYLINE: In this commercial, Cardi B walks into an elevator looking like a sexy Inspector Gadget in a blue trench coat and whispers her secret to looking so good. (SOUNDBITE OF VIDEO)CARDI B: This drip right here - Fashion Nova. But don't tell nobody. Shh. GARSD: Cardi B recently published the video on her Instagram page for over 25 million followers to see. This is how Fashion Nova silently became a retail juggernaut. A spokesperson for the company says they've seen triple-digit growth every year since they were founded. Most of the outfits cost less than 40 bucks, but when you wear Fashion Nova, you're wearing the same thing your favorite star wore a few nights ago. GABRIELLA SANTANIELLO: Basically, they're doing nothing to promote their brand, aside from getting it into the hands of the hottest celebrities out there like the Kardashians, Cardi B. GARSD: Gabriella Santaniello is a fashion industry analyst. These days, every brand wants you to follow them on social media. Fashion Nova has just been super successful at tapping into millennial neurosis, the obsession with seeing and being seen. They understood that one selfie posted by Kylie Jenner wearing your dress translates into a legion of customers, and Kylie Jenner does that a lot. SANTANIELLO: Getting your product in the hands of an influencer is incredibly important. They have followers, and they're able to influence their buying decisions. GARSD: Fashion Nova has also used social media to fill this void left by the big brand names and high fashion magazines. It caters to curvy women and women of color. GABRIELLA LASCANO: I'm fat. Like, models aren't fat. GARSD: That's what Gabriella Lascano thought when she was a kid. Now, she's a 27-year-old plus-size Instagram model for Fashion Nova's Curve line. She has 220,000 followers. She's gorgeous but also relatable, living a normal life here in the Bronx. Lascano first gained some popularity on Instagram for her makeup tutorials. Then, in 2016, Fashion Nova started sending her clothing. LASCANO: This is another dress. Isn't it pretty? GARSD: The lace. . . At first, she got to keep the outfits she wore in the pictures. As her following on Instagram increased, Fashion Nova started paying. She won't tell me how much, and she looks carefree and glamorous, but the page is meticulously well-planned out. She watches her audience numbers intensely. She's like a marketing team rolled into one. LASCANO: I'm at a solid, like, 62, 63 percent women and their age - the age for women is mostly between 18 and 34, which is perfect, right? GARSD: She even times her posts to get maximum visibility. This is her full-time job. LASCANO: I do my own hair, my own makeup, the photography, the editing - all of that work I do myself. GARSD: Like a magazine ad, but unlike supermodels, she talks to her Fashion Nova followers, responds to their adoring comments. This is not Twitter. There's no nasty trolling. Instagram has a filter which allows users to block words from the comments section. Lascano's list of blocked words and phrases is long. LASCANO: Early death, elephant, everyone's lying to her, fat, Fiona, you know, from \"Shrek. \" People love that one. GARSD: Even though she can't see those comments anymore, Lascano says being an Instagram model and her own editor can trigger a lot of insecurities. It's that millennial neurosis, again. You aren't just watching. You're being watched in high def. But as Lascano scrolls through the comments section on her most recent post, her face lights up. LASCANO: Beautiful girl - (speaking Spanish) which is you're beautiful Spanish. Slay - you look amazing. Where did you buy that bathing suit (laughter)? GARSD: For NPR News, I'm Jasmine Garsd in the Bronx. (SOUNDBITE OF MONSTER RALLY'S \"SULTAN\") MARY LOUISE KELLY, HOST:  If you've listened to Cardi B, you know she has expensive taste. (SOUNDBITE OF SONG, \"I LIKE IT\") CARDI B: (Rapping) I like those Balenciagas, the ones look like socks. I like going to the jeweler. I put rocks all in my watch. KELLY: But Cardi B is a spokesperson for a brand that is much more affordable than Balenciaga. A new NPR-Marist poll finds that online shoppers are roughly divided when it comes to shopping with brand loyalty in mind versus a good deal. Well, Fashion Nova does both. The company is known for sexy, affordable clothing worn by celebrities and Instagram models. Jasmine Garsd reports on how Fashion Nova came to become one of the top trending fashion brands on Google. JASMINE GARSD, BYLINE: In this commercial, Cardi B walks into an elevator looking like a sexy Inspector Gadget in a blue trench coat and whispers her secret to looking so good. (SOUNDBITE OF VIDEO) CARDI B: This drip right here - Fashion Nova. But don't tell nobody. Shh. GARSD: Cardi B recently published the video on her Instagram page for over 25 million followers to see. This is how Fashion Nova silently became a retail juggernaut. A spokesperson for the company says they've seen triple-digit growth every year since they were founded. Most of the outfits cost less than 40 bucks, but when you wear Fashion Nova, you're wearing the same thing your favorite star wore a few nights ago. GABRIELLA SANTANIELLO: Basically, they're doing nothing to promote their brand, aside from getting it into the hands of the hottest celebrities out there like the Kardashians, Cardi B. GARSD: Gabriella Santaniello is a fashion industry analyst. These days, every brand wants you to follow them on social media. Fashion Nova has just been super successful at tapping into millennial neurosis, the obsession with seeing and being seen. They understood that one selfie posted by Kylie Jenner wearing your dress translates into a legion of customers, and Kylie Jenner does that a lot. SANTANIELLO: Getting your product in the hands of an influencer is incredibly important. They have followers, and they're able to influence their buying decisions. GARSD: Fashion Nova has also used social media to fill this void left by the big brand names and high fashion magazines. It caters to curvy women and women of color. GABRIELLA LASCANO: I'm fat. Like, models aren't fat. GARSD: That's what Gabriella Lascano thought when she was a kid. Now, she's a 27-year-old plus-size Instagram model for Fashion Nova's Curve line. She has 220,000 followers. She's gorgeous but also relatable, living a normal life here in the Bronx. Lascano first gained some popularity on Instagram for her makeup tutorials. Then, in 2016, Fashion Nova started sending her clothing. LASCANO: This is another dress. Isn't it pretty? GARSD: The lace. . . At first, she got to keep the outfits she wore in the pictures. As her following on Instagram increased, Fashion Nova started paying. She won't tell me how much, and she looks carefree and glamorous, but the page is meticulously well-planned out. She watches her audience numbers intensely. She's like a marketing team rolled into one. LASCANO: I'm at a solid, like, 62, 63 percent women and their age - the age for women is mostly between 18 and 34, which is perfect, right? GARSD: She even times her posts to get maximum visibility. This is her full-time job. LASCANO: I do my own hair, my own makeup, the photography, the editing - all of that work I do myself. GARSD: Like a magazine ad, but unlike supermodels, she talks to her Fashion Nova followers, responds to their adoring comments. This is not Twitter. There's no nasty trolling. Instagram has a filter which allows users to block words from the comments section. Lascano's list of blocked words and phrases is long. LASCANO: Early death, elephant, everyone's lying to her, fat, Fiona, you know, from \"Shrek. \" People love that one. GARSD: Even though she can't see those comments anymore, Lascano says being an Instagram model and her own editor can trigger a lot of insecurities. It's that millennial neurosis, again. You aren't just watching. You're being watched in high def. But as Lascano scrolls through the comments section on her most recent post, her face lights up. LASCANO: Beautiful girl - (speaking Spanish) which is you're beautiful Spanish. Slay - you look amazing. Where did you buy that bathing suit (laughter)? GARSD: For NPR News, I'm Jasmine Garsd in the Bronx. (SOUNDBITE OF MONSTER RALLY'S \"SULTAN\")", "section": "Online Shopping And The Power Of Amazon", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-19-621269712": {"title": "WHO Recognizes Gaming Disorder As A Mental Health Condition : NPR", "url": "https://www.npr.org/2018/06/19/621269712/who-recognizes-gaming-disorder-as-a-mental-health-condition", "author": "No author found", "published_date": "2018-06-19", "content": "DAVID GREENE, HOST: The World Health Organization has added gaming disorders, as in video gaming, to addictive disorders. But this idea of technology addiction is still controversial, as Anya Kamenetz reports. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: Oh, you can raid him now. UNIDENTIFIED PERSON #2: Yeah, I'm waiting. Don't worry. Don't worry. (SOUNDBITE OF SHOOTING SOUND EFFECTS IN VIDEO GAME)ANYA KAMENETZ, BYLINE: That's a clip giving an idea of the thrills of Fortnite, one of the most popular video games right now. And video games are really popular. All told, from casual mobile games to immersive multiplayer worlds, the industry brought in $36 billion last year with players from two-thirds of American households, according to the Entertainment Software Association. But a small percentage of people, particularly young men, seem to have a problem with gaming getting out of control. Sleep, school, work and relationships all fall by the wayside. The International Classification of Diseases is an official publication of the U. N. 's health agency. It is used by doctors around the world to identify health trends and statistics. The newest addition for the first time includes gaming disorder, which is classified similarly to a gambling addiction. The signs include playing video games obsessively and not being able to stop despite significant negative life consequences. Mental health professionals, like Nicholas Kardaras, the author of \"Glow Kids: How Screen Addiction Is Hijacking Our Kids,\" talk about patients, overwhelmingly young men, who get so wrapped up in a multi-hour gaming binge that they won't even get up to use the bathroom. Some clinicians and families hope that with more official recognition of gaming disorder will come easier access to help, like insurance coverage and treatment. And this all comes at a time of a broader concern that many people are using technology, including smartphones, more than is optimal for their health. But the move is still controversial. The psychiatric profession in the United States does not yet officially recognize internet or video game addiction as stand-alone disorders - listing them instead as conditions for further study. In a video about the new classification, Dr. Shekhar Saxena of the World Health Organization took pains to make clear that video games are a harmless pastime for most young people. (SOUNDBITE OF ARCHIVED RECORDING)SHEKHAR SAXENA: Everybody who indulges in gaming from time to time doesn't have this disorder. In fact, it's only a minority of people who game who will satisfy the strict criteria for gaming disorder in ICD-11. KAMENETZ: Complicating the picture further, clinicians say those who show problems with video games often have a co-occurring condition, such as depression, anxiety, ADHD or being on the autism spectrum. Twelve-step programs, like Alcoholics or Gamblers Anonymous, are undeveloped for video gamers. And the treatments on offer, like wilderness-based detox programs, can cost thousands of dollars and are unproven. Anya Kamenetz, NPR News, New York. DAVID GREENE, HOST:  The World Health Organization has added gaming disorders, as in video gaming, to addictive disorders. But this idea of technology addiction is still controversial, as Anya Kamenetz reports. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: Oh, you can raid him now. UNIDENTIFIED PERSON #2: Yeah, I'm waiting. Don't worry. Don't worry. (SOUNDBITE OF SHOOTING SOUND EFFECTS IN VIDEO GAME) ANYA KAMENETZ, BYLINE: That's a clip giving an idea of the thrills of Fortnite, one of the most popular video games right now. And video games are really popular. All told, from casual mobile games to immersive multiplayer worlds, the industry brought in $36 billion last year with players from two-thirds of American households, according to the Entertainment Software Association. But a small percentage of people, particularly young men, seem to have a problem with gaming getting out of control. Sleep, school, work and relationships all fall by the wayside. The International Classification of Diseases is an official publication of the U. N. 's health agency. It is used by doctors around the world to identify health trends and statistics. The newest addition for the first time includes gaming disorder, which is classified similarly to a gambling addiction. The signs include playing video games obsessively and not being able to stop despite significant negative life consequences. Mental health professionals, like Nicholas Kardaras, the author of \"Glow Kids: How Screen Addiction Is Hijacking Our Kids,\" talk about patients, overwhelmingly young men, who get so wrapped up in a multi-hour gaming binge that they won't even get up to use the bathroom. Some clinicians and families hope that with more official recognition of gaming disorder will come easier access to help, like insurance coverage and treatment. And this all comes at a time of a broader concern that many people are using technology, including smartphones, more than is optimal for their health. But the move is still controversial. The psychiatric profession in the United States does not yet officially recognize internet or video game addiction as stand-alone disorders - listing them instead as conditions for further study. In a video about the new classification, Dr. Shekhar Saxena of the World Health Organization took pains to make clear that video games are a harmless pastime for most young people. (SOUNDBITE OF ARCHIVED RECORDING) SHEKHAR SAXENA: Everybody who indulges in gaming from time to time doesn't have this disorder. In fact, it's only a minority of people who game who will satisfy the strict criteria for gaming disorder in ICD-11. KAMENETZ: Complicating the picture further, clinicians say those who show problems with video games often have a co-occurring condition, such as depression, anxiety, ADHD or being on the autism spectrum. Twelve-step programs, like Alcoholics or Gamblers Anonymous, are undeveloped for video gamers. And the treatments on offer, like wilderness-based detox programs, can cost thousands of dollars and are unproven. Anya Kamenetz, NPR News, New York.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-19-621269603": {"title": "IBM Touts Breakthrough Technology As Computer Debates A Person : NPR", "url": "https://www.npr.org/2018/06/19/621269603/ibm-touts-breakthrough-technology-as-computer-debates-a-person", "author": "No author found", "published_date": "2018-06-19", "content": "DAVID GREENE, HOST: There was an epic battle that took place last night at the IBM offices in San Francisco. A computer formally debated a person in what some experts are billing as a breakthrough in artificial intelligence. NPR's Laura Sydell was there, and she's with us. Hi, Laura. LAURA SYDELL, BYLINE: Hello. Good morning. GREENE: So this sounds like something out of a sci-fi movie. Was it as cool as it sounds? SYDELL: Well, you know, it was just this person standing next to this obelisk-shaped computer thing so it didn't look very interesting. GREENE: (Laughter) OK. SYDELL: You know, truthfully. GREENE: Well, then why did IBM want to do this? What was this about? SYDELL: Well, it was to show off some actually pretty cool new computer skills. You know, IBM has made news in the past for, you know, a computer that beat a world champion in chess, and it also beat champions at the game \"Jeopardy! \" This is a little different because in a debate, there aren't clear rules. There isn't a numerical score. So they built this computer. It's called Project Debater, and they put it up against two experienced debaters. GREENE: OK. So the computer was actually talking to two people? I mean going back and forth debating something. SYDELL: That is right. GREENE: OK. SYDELL: It actually spoke, you know, for four minutes and then for two minutes. It went through a formal debating process. GREENE: Can we say who won? Did the computer beat the humans? SYDELL: Well, you know, actually I'd say it was a draw in the sense that there were two debates and they pulled the audience, which was largely journalists, ahead of time to see how they felt about a particular topic, and then they pulled them after. And in one case, the computer actually did in fact beat the human. And the computer argued that telemedicine - that was the topic of the debate - was a good thing. And there was kind of a funny moment where the human debaters said that they didn't think that telemedicine was good - this is when doctors treat people from a distance - because you didn't have the physical hand of the doctor or the nurse, and the computer responded with some humor. It said, I am a true believer in the power of technology, as I should be. GREENE: As I should be. (Laughter) Wow. The computer making a joke, that's amazing. SYDELL: It's pretty good. I did speak with one of the debaters, Noa Ovadia, and she was relatively impressed with the computer. NOA OVADIA: I think the machine has reached an incredible degree of fluency, both in terms of sentence construction and in terms of argument construction. And I think it's at the level where it's comparable to some average debaters. Maybe not the best in the world, but definitely well-informed reasoning and logical construction. GREENE: So is this a big moment? I mean, is artificial intelligence, has it totally arrived? Are computers definitely getting as smart as us, or smarter? SYDELL: Well, I think it's significant in showing that a computer can create a coherent argument and pull together information and make a case. I mean, the computer didn't know what the debate would be ahead of time. OK? It had to actually sort through millions of documents and look for language that would help it make its case in real time. So mostly the language it used came directly from text. And ultimately what this means is that we could start to use computers - say, a lawyer could use it to make better arguments, or it might help doctors and patients decide on the best course of treatment or it could help you even pick stocks. GREENE: Wow. Can I get one of these computers anytime soon? SYDELL: Well, alas, it's not going to come out into the consumer market. You won't be fighting yet with Alexa and Siri (laughter). GREENE: (Laughter) OK. NPR's Laura Sydell on a computer debating a human. Laura, thanks. SYDELL: You're welcome. DAVID GREENE, HOST:  There was an epic battle that took place last night at the IBM offices in San Francisco. A computer formally debated a person in what some experts are billing as a breakthrough in artificial intelligence. NPR's Laura Sydell was there, and she's with us. Hi, Laura. LAURA SYDELL, BYLINE: Hello. Good morning. GREENE: So this sounds like something out of a sci-fi movie. Was it as cool as it sounds? SYDELL: Well, you know, it was just this person standing next to this obelisk-shaped computer thing so it didn't look very interesting. GREENE: (Laughter) OK. SYDELL: You know, truthfully. GREENE: Well, then why did IBM want to do this? What was this about? SYDELL: Well, it was to show off some actually pretty cool new computer skills. You know, IBM has made news in the past for, you know, a computer that beat a world champion in chess, and it also beat champions at the game \"Jeopardy! \" This is a little different because in a debate, there aren't clear rules. There isn't a numerical score. So they built this computer. It's called Project Debater, and they put it up against two experienced debaters. GREENE: OK. So the computer was actually talking to two people? I mean going back and forth debating something. SYDELL: That is right. GREENE: OK. SYDELL: It actually spoke, you know, for four minutes and then for two minutes. It went through a formal debating process. GREENE: Can we say who won? Did the computer beat the humans? SYDELL: Well, you know, actually I'd say it was a draw in the sense that there were two debates and they pulled the audience, which was largely journalists, ahead of time to see how they felt about a particular topic, and then they pulled them after. And in one case, the computer actually did in fact beat the human. And the computer argued that telemedicine - that was the topic of the debate - was a good thing. And there was kind of a funny moment where the human debaters said that they didn't think that telemedicine was good - this is when doctors treat people from a distance - because you didn't have the physical hand of the doctor or the nurse, and the computer responded with some humor. It said, I am a true believer in the power of technology, as I should be. GREENE: As I should be. (Laughter) Wow. The computer making a joke, that's amazing. SYDELL: It's pretty good. I did speak with one of the debaters, Noa Ovadia, and she was relatively impressed with the computer. NOA OVADIA: I think the machine has reached an incredible degree of fluency, both in terms of sentence construction and in terms of argument construction. And I think it's at the level where it's comparable to some average debaters. Maybe not the best in the world, but definitely well-informed reasoning and logical construction. GREENE: So is this a big moment? I mean, is artificial intelligence, has it totally arrived? Are computers definitely getting as smart as us, or smarter? SYDELL: Well, I think it's significant in showing that a computer can create a coherent argument and pull together information and make a case. I mean, the computer didn't know what the debate would be ahead of time. OK? It had to actually sort through millions of documents and look for language that would help it make its case in real time. So mostly the language it used came directly from text. And ultimately what this means is that we could start to use computers - say, a lawyer could use it to make better arguments, or it might help doctors and patients decide on the best course of treatment or it could help you even pick stocks. GREENE: Wow. Can I get one of these computers anytime soon? SYDELL: Well, alas, it's not going to come out into the consumer market. You won't be fighting yet with Alexa and Siri (laughter). GREENE: (Laughter) OK. NPR's Laura Sydell on a computer debating a human. Laura, thanks. SYDELL: You're welcome.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-20-620053144": {"title": "The Science Behind The World Cup Ball : NPR", "url": "https://www.npr.org/2018/06/20/620053144/the-science-behind-the-world-cup-ball", "author": "No author found", "published_date": "2018-06-20", "content": "DAVID GREENE, HOST: If you've been watching the World Cup, maybe you've noticed that there is a custom-designed soccer ball being used. NPR's Merrit Kennedy reports that scientists ran tests in a wind tunnel to understand how the new ball plays. MERRIT KENNEDY, BYLINE: It's called the Telstar 18. It has six panels and a slick black-and-white design inspired by Russian cityscapes. But is it actually a good soccer ball? To find out, scientists stuck it in a wind tunnel with a bunch of sensors. JOHN ERIC GOFF: It's actually recording the size of the forces on the ball. KENNEDY: That's John Eric Goff, a physics professor at Lynchburg College. Adidas has redesigned the World Cup ball for each tournament since 1970, and Goff says tiny changes to the design can make a big difference in how the ball responds during play. For example. . . GOFF: The dreaded 2010 Jabulani ball that was used in South Africa. KENNEDY: It was too smooth, he says, which caused it to behave in ways that players sometimes weren't expecting. The smoothness issue was basically fixed for the 2014 World Cup in Brazil. The ball had longer seams joining its panels, which roughened it up a bit. After crunching the numbers, Goff says this year's ball probably won't create big controversies. GOFF: I don't see that this ball is going to be so drastically different from the 2014 ball. KENNEDY: But there is one way the players might notice a difference - on high-speed kicks, the kind that a goalkeeper might do to send the ball way down the field. The scientists expect an 8 to 9 percent drop in range with the new ball. So why do they change the ball every time? GOFF: It's an interesting phenomena that the world's most popular sporting event for the world's most popular sport and the most important piece of equipment in that sport is changed every World Cup. KENNEDY: Goff says it's not about physics; it's about money and marketing. Adidas sells these balls to the public for a hundred and twenty-four bucks each. Merrit Kennedy, NPR News. DAVID GREENE, HOST:  If you've been watching the World Cup, maybe you've noticed that there is a custom-designed soccer ball being used. NPR's Merrit Kennedy reports that scientists ran tests in a wind tunnel to understand how the new ball plays. MERRIT KENNEDY, BYLINE: It's called the Telstar 18. It has six panels and a slick black-and-white design inspired by Russian cityscapes. But is it actually a good soccer ball? To find out, scientists stuck it in a wind tunnel with a bunch of sensors. JOHN ERIC GOFF: It's actually recording the size of the forces on the ball. KENNEDY: That's John Eric Goff, a physics professor at Lynchburg College. Adidas has redesigned the World Cup ball for each tournament since 1970, and Goff says tiny changes to the design can make a big difference in how the ball responds during play. For example. . . GOFF: The dreaded 2010 Jabulani ball that was used in South Africa. KENNEDY: It was too smooth, he says, which caused it to behave in ways that players sometimes weren't expecting. The smoothness issue was basically fixed for the 2014 World Cup in Brazil. The ball had longer seams joining its panels, which roughened it up a bit. After crunching the numbers, Goff says this year's ball probably won't create big controversies. GOFF: I don't see that this ball is going to be so drastically different from the 2014 ball. KENNEDY: But there is one way the players might notice a difference - on high-speed kicks, the kind that a goalkeeper might do to send the ball way down the field. The scientists expect an 8 to 9 percent drop in range with the new ball. So why do they change the ball every time? GOFF: It's an interesting phenomena that the world's most popular sporting event for the world's most popular sport and the most important piece of equipment in that sport is changed every World Cup. KENNEDY: Goff says it's not about physics; it's about money and marketing. Adidas sells these balls to the public for a hundred and twenty-four bucks each. Merrit Kennedy, NPR News.", "section": "Science", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-22-605007387": {"title": "In Major Privacy Win, Supreme Court Rules Police Need Warrant To Track Your Cellphone : NPR", "url": "https://www.npr.org/2018/06/22/605007387/supreme-court-rules-police-need-warrant-to-get-location-information-from-cell-to", "author": "No author found", "published_date": "2018-06-22", "content": "AUDIE CORNISH, HOST:  In a landmark decision, the U. S. Supreme Court ruled today that police must obtain a search warrant in order to gain access to an individual's cellphone location information. The 5 to 4 decision imposes new limits on law enforcement's ability to get at the increasing amount of data that private companies amass in the modern technological age. NPR legal affairs correspondent Nina Totenberg reports. NINA TOTENBERG, BYLINE: Customers' cellphone location information is routinely kept by cellphone providers to help them improve service. And until today, under the Supreme Court's prior rulings, the prevailing legal theory was that if an individual voluntarily shares his information with a third party by signing up for service, for instance, police do not need a search warrant to get that information from the service provider. Today, the Supreme Court blew a hole in that theory. Writing for the court majority, Chief Justice John Roberts said that cellphone location information is the perfect tool for government surveillance, analogous to an electronic monitoring bracelet. The writers of the Constitution, he said, would certainly have understood that an individual has a privacy interest in day-to-day, hour-to-hour and even minute-to-minute records of his whereabouts, a privacy interest that requires the government to get a search warrant before gaining access to that information. The case before the court was brought by Timothy Carpenter, prosecuted as a ringleader in a series of armed robberies in Michigan and Ohio. Cell tower location information showing he was at the robbery sites was used as damning evidence at his trial. Carpenter appealed his conviction, contending that police invaded his privacy without getting a search warrant first. Today, the Supreme Court agreed, declaring that the routine court order that police obtained in Carpenter's case only required a showing that police were seeking relevant information, whereas a search warrant requires that police meet a far higher standard. ORIN KERR: Big Brother is coming, and we need to stop it. That seems to be the big takeaway from the opinion. TOTENBERG: Fourth Amendment scholar Orin Kerr of the University of Southern California. KERR: It almost reflects an anxiety about technology thwarting privacy. If we don't stop the government here, what will they be able to do? TOTENBERG: Columbia law professor Jameel Jaffer. JAMEEL JAFFER: This is a landmark privacy case. But it's also a very significant case for First Amendment freedoms - that is, for the freedoms of speech and the press and association. A government that can track your every movement without a warrant is a government that can freely monitor activists' political associations or monitor government employees' contacts with the press. TOTENBERG: But Jaffer concedes that today's decision poses practical problems and leaves open important questions. Chief Justice Roberts cast the decision as a narrow one. It does not disturb the routine use of subpoenas to obtain financial, bank and other business records, he said, nor does it prevent police from obtaining cell location records without a warrant in emergency circumstances like a fleeing suspect, a kidnapping or threats of imminent danger. Moreover, he said, the decision does not call into question the use of security cameras and other techniques, and it does not consider other collection techniques involving foreign affairs and national security. What it does do, he said, is to ensure that the progress of science does not erode the Fourth Amendment guarantee of privacy. Roberts, a conservative, was joined by the court's four liberal justices. The court's other four conservatives dissented loudly, each writing separately to indicate his strong disagreement. While each had a different approach, they all said today's decision would lead to confusion, litigation and problems for law enforcement. Ed McAndrew, a former federal prosecutor, agrees. He notes that cell location information is often gathered at the early stages of an investigation when there isn't enough information for a search warrant. The same is true in terrorism and national security investigations. ED MCANDREW: And the national security context is only going to be different if we're dealing with foreign nationals. If we're dealing with American citizens, the Fourth Amendment principle's going to apply. TOTENBERG: Justice Stephen Breyer, who joined today's majority opinion, may have foreseen some of these problems at oral argument. (SOUNDBITE OF ARCHIVED RECORDING)STEPHEN BREYER: This is an open box. We know not where we go. TOTENBERG: Nina Totenberg, NPR News, Washington. AUDIE CORNISH, HOST:   In a landmark decision, the U. S. Supreme Court ruled today that police must obtain a search warrant in order to gain access to an individual's cellphone location information. The 5 to 4 decision imposes new limits on law enforcement's ability to get at the increasing amount of data that private companies amass in the modern technological age. NPR legal affairs correspondent Nina Totenberg reports. NINA TOTENBERG, BYLINE: Customers' cellphone location information is routinely kept by cellphone providers to help them improve service. And until today, under the Supreme Court's prior rulings, the prevailing legal theory was that if an individual voluntarily shares his information with a third party by signing up for service, for instance, police do not need a search warrant to get that information from the service provider. Today, the Supreme Court blew a hole in that theory. Writing for the court majority, Chief Justice John Roberts said that cellphone location information is the perfect tool for government surveillance, analogous to an electronic monitoring bracelet. The writers of the Constitution, he said, would certainly have understood that an individual has a privacy interest in day-to-day, hour-to-hour and even minute-to-minute records of his whereabouts, a privacy interest that requires the government to get a search warrant before gaining access to that information. The case before the court was brought by Timothy Carpenter, prosecuted as a ringleader in a series of armed robberies in Michigan and Ohio. Cell tower location information showing he was at the robbery sites was used as damning evidence at his trial. Carpenter appealed his conviction, contending that police invaded his privacy without getting a search warrant first. Today, the Supreme Court agreed, declaring that the routine court order that police obtained in Carpenter's case only required a showing that police were seeking relevant information, whereas a search warrant requires that police meet a far higher standard. ORIN KERR: Big Brother is coming, and we need to stop it. That seems to be the big takeaway from the opinion. TOTENBERG: Fourth Amendment scholar Orin Kerr of the University of Southern California. KERR: It almost reflects an anxiety about technology thwarting privacy. If we don't stop the government here, what will they be able to do? TOTENBERG: Columbia law professor Jameel Jaffer. JAMEEL JAFFER: This is a landmark privacy case. But it's also a very significant case for First Amendment freedoms - that is, for the freedoms of speech and the press and association. A government that can track your every movement without a warrant is a government that can freely monitor activists' political associations or monitor government employees' contacts with the press. TOTENBERG: But Jaffer concedes that today's decision poses practical problems and leaves open important questions. Chief Justice Roberts cast the decision as a narrow one. It does not disturb the routine use of subpoenas to obtain financial, bank and other business records, he said, nor does it prevent police from obtaining cell location records without a warrant in emergency circumstances like a fleeing suspect, a kidnapping or threats of imminent danger. Moreover, he said, the decision does not call into question the use of security cameras and other techniques, and it does not consider other collection techniques involving foreign affairs and national security. What it does do, he said, is to ensure that the progress of science does not erode the Fourth Amendment guarantee of privacy. Roberts, a conservative, was joined by the court's four liberal justices. The court's other four conservatives dissented loudly, each writing separately to indicate his strong disagreement. While each had a different approach, they all said today's decision would lead to confusion, litigation and problems for law enforcement. Ed McAndrew, a former federal prosecutor, agrees. He notes that cell location information is often gathered at the early stages of an investigation when there isn't enough information for a search warrant. The same is true in terrorism and national security investigations. ED MCANDREW: And the national security context is only going to be different if we're dealing with foreign nationals. If we're dealing with American citizens, the Fourth Amendment principle's going to apply. TOTENBERG: Justice Stephen Breyer, who joined today's majority opinion, may have foreseen some of these problems at oral argument. (SOUNDBITE OF ARCHIVED RECORDING) STEPHEN BREYER: This is an open box. We know not where we go. TOTENBERG: Nina Totenberg, NPR News, Washington.", "section": "Law", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-26-623545591": {"title": "Orlando Police End Test Of Amazon's Real-Time Facial 'Rekognition' System : NPR", "url": "https://www.npr.org/2018/06/26/623545591/orlando-police-end-test-of-amazons-real-time-facial-rekognition-system", "author": "No author found", "published_date": "2018-06-26", "content": "", "section": "Law", "disclaimer": ""}, "2018-06-28-624099686": {"title": "Wisconsin Hopes Foxconn Will Make It A Digital Hub, But Skepticism Abounds : NPR", "url": "https://www.npr.org/2018/06/28/624099686/wisconsin-hopes-foxconn-will-make-it-a-digital-hub-but-skepticism-abounds", "author": "No author found", "published_date": "2018-06-28", "content": "DAVID GREENE, HOST: President Trump is going to be in Wisconsin today to attend the groundbreaking for a giant new Foxconn factory. This is the Taiwan-based firm that is Apple's biggest supplier. They make screens for iPhones. Foxconn has said it will bring some 13,000 jobs to Wisconsin. Here's NPR's Jim Zarroli. JIM ZARROLI, BYLINE: Lots of cities and towns want to become the next Silicon Valley. Wisconsin hopes the $10 billion Foxconn plant will be its chance. Tim Sheehy heads the Metro Milwaukee Chamber of Commerce. TIMOTHY SHEEHY: This will be an investment by the fourth largest technology company - making the largest foreign direct investment in U. S. history. ZARROLI: To President Trump, the plant is a visible sign of the manufacturing revival he envisions. Here he was last August. (SOUNDBITE OF ARCHIVED RECORDING)PRESIDENT DONALD TRUMP: You saw last week - Foxconn. They make the Apple iPhones. They make all of the - desktop. They make - they're the biggest in the world. They're coming into Wisconsin with an unbelievable plant. ZARROLI: But it wasn't just rising business confidence that lured Foxconn to Wisconsin. State and local officials offered nearly $4 billion in incentives to the company. Gordon Hintz is the Democratic leader of the state Assembly. GORDON HINTZ: It's the largest taxpayer subsidy of a foreign corporation in U. S. history by a state, and I think a lot of people in Wisconsin are skeptical about this kind of economic development in general. ZARROLI: Hintz says it will take many years before the state's investment pays off, and serious questions exist about how many jobs will be created. Foxconn has made similar jobs promises in places such as Brazil that didn't pan out. A recent Marquette Law School poll indicated that most voters, even among Republicans, are skeptical about the plan. That's a perception that Governor Scott Walker is hoping to change. Walker is running for re-election and battling criticism that the state's growth has lagged. Last year, he went on Fox Business News to sell the Foxconn plant. (SOUNDBITE OF ARCHIVED RECORDING)SCOTT WALKER: Absolutely it's worth it. This is transformational. These LCD displays will be made in America for the very first time right here in the state of Wisconsin. ZARROLI: Today Walker will stand alongside President Trump at the groundbreaking. The meeting could be awkward. The state is home to the beloved company Harley-Davidson, and Walker even rode a Harley during his brief campaign for president in 2016. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER: Wisconsin Governor Scott Walker is touring New Hampshire today by motorcycle - on his Harley-Davidson right there. ZARROLI: This week Trump threatened Harley-Davidson. He did so after the company announced it was shifting some production overseas because of Trump's escalating trade battles with Europe. Again, Assembly leader Hintz. HINTZ: My hope is that Governor Walker will talk to, you know, President Trump and tell him that his, you know, overall economic policy is having a destructive effect on Wisconsin industries. ZARROLI: But on a day when both Walker and Trump are eager to celebrate an economic victory, whatever differences exist between them are unlikely to be made public. Jim Zarroli, NPR News. (SOUNDBITE OF MUSIC) DAVID GREENE, HOST:  President Trump is going to be in Wisconsin today to attend the groundbreaking for a giant new Foxconn factory. This is the Taiwan-based firm that is Apple's biggest supplier. They make screens for iPhones. Foxconn has said it will bring some 13,000 jobs to Wisconsin. Here's NPR's Jim Zarroli. JIM ZARROLI, BYLINE: Lots of cities and towns want to become the next Silicon Valley. Wisconsin hopes the $10 billion Foxconn plant will be its chance. Tim Sheehy heads the Metro Milwaukee Chamber of Commerce. TIMOTHY SHEEHY: This will be an investment by the fourth largest technology company - making the largest foreign direct investment in U. S. history. ZARROLI: To President Trump, the plant is a visible sign of the manufacturing revival he envisions. Here he was last August. (SOUNDBITE OF ARCHIVED RECORDING) PRESIDENT DONALD TRUMP: You saw last week - Foxconn. They make the Apple iPhones. They make all of the - desktop. They make - they're the biggest in the world. They're coming into Wisconsin with an unbelievable plant. ZARROLI: But it wasn't just rising business confidence that lured Foxconn to Wisconsin. State and local officials offered nearly $4 billion in incentives to the company. Gordon Hintz is the Democratic leader of the state Assembly. GORDON HINTZ: It's the largest taxpayer subsidy of a foreign corporation in U. S. history by a state, and I think a lot of people in Wisconsin are skeptical about this kind of economic development in general. ZARROLI: Hintz says it will take many years before the state's investment pays off, and serious questions exist about how many jobs will be created. Foxconn has made similar jobs promises in places such as Brazil that didn't pan out. A recent Marquette Law School poll indicated that most voters, even among Republicans, are skeptical about the plan. That's a perception that Governor Scott Walker is hoping to change. Walker is running for re-election and battling criticism that the state's growth has lagged. Last year, he went on Fox Business News to sell the Foxconn plant. (SOUNDBITE OF ARCHIVED RECORDING) SCOTT WALKER: Absolutely it's worth it. This is transformational. These LCD displays will be made in America for the very first time right here in the state of Wisconsin. ZARROLI: Today Walker will stand alongside President Trump at the groundbreaking. The meeting could be awkward. The state is home to the beloved company Harley-Davidson, and Walker even rode a Harley during his brief campaign for president in 2016. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER: Wisconsin Governor Scott Walker is touring New Hampshire today by motorcycle - on his Harley-Davidson right there. ZARROLI: This week Trump threatened Harley-Davidson. He did so after the company announced it was shifting some production overseas because of Trump's escalating trade battles with Europe. Again, Assembly leader Hintz. HINTZ: My hope is that Governor Walker will talk to, you know, President Trump and tell him that his, you know, overall economic policy is having a destructive effect on Wisconsin industries. ZARROLI: But on a day when both Walker and Trump are eager to celebrate an economic victory, whatever differences exist between them are unlikely to be made public. Jim Zarroli, NPR News. (SOUNDBITE OF MUSIC)", "section": "Business", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-29-624336039": {"title": "A Wealthy Californian Named Alastair Mactaggart Brought The American Tech Indust : NPR", "url": "https://www.npr.org/2018/06/29/624336039/california-passes-strict-internet-privacy-law-with-implications-for-the-country", "author": "No author found", "published_date": "2018-06-29", "content": "DAVID GREENE, HOST: All right, here in California, Governor Jerry Brown signed a law yesterday giving Californians sweeping, new data privacy rights. This could reverberate nationwide. And it's all because of a single voter who leveraged this state's ballot initiative process. Here's Capital Public Radio's Ben Adler. BEN ADLER, BYLINE: Alastair Mactaggart started worrying about data privacy after talking with a Google engineer. ALASTAIR MACTAGGART: These giant corporations know absolutely everything about you, and you have no rights. And I thought, oh, I'd like to find out about what these companies know about me. Then I thought, well, you know, someone should do something about that. And then one of these days, I'm like, maybe I'm someone. ADLER: Mactaggart is no billionaire like George Soros or the Koch brothers, but he did earn a fortune in Bay Area real estate. So he spent nearly 3 1/2 million dollars to place an initiative on California's November ballot, then negotiated a deal with the legislature instead. CHRISTIN MCMELEY: This is a really big deal. ADLER: Christin McMeley is a Washington, D. C. -based attorney who represents cable industry clients on privacy and information security matters. She says California's new law is similar, though not identical to newly enacted European Union standards. MCMELEY: It will absolutely change the way that companies do business in the state of California, if not the United States. ADLER: Starting in 2020, Californians will have the right to learn what companies like Facebook and Google know about them and stop the sharing or selling of their data. And they can sue over data breaches if companies fail to adequately protect their data. McMeley says the law won't only affect Internet companies. MCMELEY: This is much broader than online. It is all information. ADLER: The tech industry scorned the measure as unworkable. Google, Facebook, AT&T and Comcast joined the opposition campaign, along with the California Chamber of Commerce. And word surfaced that opponents would spend a hundred million dollars. But behind the scenes, that money never materialized. EVAN LOW: It's important to recognize that tech is not monolithic. ADLER: Democrat Evan Low represents Silicon Valley in the California Assembly. He says some companies were spoiling for a fight. Others wanted a compromise. And still others figured it wasn't their battle. So when Mactaggart and lawmakers gave the industry an ultimatum - either take the legislative deal or take their chances with an unpopular and expensive campaign, the industry blinked. LOW: I think it's a wake-up call for the tech community to recognize that we need to be engaged and proactive in being part of the solution. ADLER: Yesterday, facing a deadline for initiative proponents to withdraw their measures from the ballot, lawmakers rushed the deal through both houses. The bill's author promised to address concerns from business and consumer groups before it takes effect. At a news conference after Governor Brown signed the bill, Mactaggart called it a great stride forward. (SOUNDBITE OF ARCHIVED RECORDING)MACTAGGART: Because it's going to happen to the rest of the country. If it happened here, it will happen to the rest of the country. ADLER: And with that, he pulled his initiative off California's November ballot, his 3 1/2 million dollars having conquered a trillion-dollar Goliath. For NPR News, I'm Ben Adler in Sacramento. (SOUNDBITE OF TYCHO'S \"SLACK\") DAVID GREENE, HOST:  All right, here in California, Governor Jerry Brown signed a law yesterday giving Californians sweeping, new data privacy rights. This could reverberate nationwide. And it's all because of a single voter who leveraged this state's ballot initiative process. Here's Capital Public Radio's Ben Adler. BEN ADLER, BYLINE: Alastair Mactaggart started worrying about data privacy after talking with a Google engineer. ALASTAIR MACTAGGART: These giant corporations know absolutely everything about you, and you have no rights. And I thought, oh, I'd like to find out about what these companies know about me. Then I thought, well, you know, someone should do something about that. And then one of these days, I'm like, maybe I'm someone. ADLER: Mactaggart is no billionaire like George Soros or the Koch brothers, but he did earn a fortune in Bay Area real estate. So he spent nearly 3 1/2 million dollars to place an initiative on California's November ballot, then negotiated a deal with the legislature instead. CHRISTIN MCMELEY: This is a really big deal. ADLER: Christin McMeley is a Washington, D. C. -based attorney who represents cable industry clients on privacy and information security matters. She says California's new law is similar, though not identical to newly enacted European Union standards. MCMELEY: It will absolutely change the way that companies do business in the state of California, if not the United States. ADLER: Starting in 2020, Californians will have the right to learn what companies like Facebook and Google know about them and stop the sharing or selling of their data. And they can sue over data breaches if companies fail to adequately protect their data. McMeley says the law won't only affect Internet companies. MCMELEY: This is much broader than online. It is all information. ADLER: The tech industry scorned the measure as unworkable. Google, Facebook, AT&T and Comcast joined the opposition campaign, along with the California Chamber of Commerce. And word surfaced that opponents would spend a hundred million dollars. But behind the scenes, that money never materialized. EVAN LOW: It's important to recognize that tech is not monolithic. ADLER: Democrat Evan Low represents Silicon Valley in the California Assembly. He says some companies were spoiling for a fight. Others wanted a compromise. And still others figured it wasn't their battle. So when Mactaggart and lawmakers gave the industry an ultimatum - either take the legislative deal or take their chances with an unpopular and expensive campaign, the industry blinked. LOW: I think it's a wake-up call for the tech community to recognize that we need to be engaged and proactive in being part of the solution. ADLER: Yesterday, facing a deadline for initiative proponents to withdraw their measures from the ballot, lawmakers rushed the deal through both houses. The bill's author promised to address concerns from business and consumer groups before it takes effect. At a news conference after Governor Brown signed the bill, Mactaggart called it a great stride forward. (SOUNDBITE OF ARCHIVED RECORDING) MACTAGGART: Because it's going to happen to the rest of the country. If it happened here, it will happen to the rest of the country. ADLER: And with that, he pulled his initiative off California's November ballot, his 3 1/2 million dollars having conquered a trillion-dollar Goliath. For NPR News, I'm Ben Adler in Sacramento. (SOUNDBITE OF TYCHO'S \"SLACK\")", "section": "National", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-06-30-624373367": {"title": "More States Opting To 'Robo-Grade' Student Essays By Computer : NPR", "url": "https://www.npr.org/2018/06/30/624373367/more-states-opting-to-robo-grade-student-essays-by-computer", "author": "No author found", "published_date": "2018-06-30", "content": "SCOTT SIMON, HOST: Little pop quiz now. Who writes our theme music - A, Snoop Dogg, B, Dolly Parton, C, Philip Glass or, D, B. J. Leiderman? A computer would quickly know D is the correct answer. But now computers are also starting to grade students' essays. As NPR's Tovia Smith reports, many teachers see that as a mistake. TOVIA SMITH, BYLINE: Developers of the so-called robo-graders say they understand the skepticism. But they say if computers are already driving cars, detecting cancer and carrying on conversations, they can also handle grading a high school essay on, say, the fall of the Roman Empire. PETER FOLTZ: Yeah, I've been working on this now for about 25 years, so I feel that it's something that - the time is right. And it's really starting to be used now. SMITH: Peter Foltz is a professor at the University of Colorado and a researcher for Pearson, a company whose automated scoring program graded some 34 million student essays on state and national high-stakes tests last year. Foltz says computers learn what's considered good writing by analyzing essays graded by humans, and then they simply scan for those same features. FOLTZ: We have artificial intelligence techniques which can judge anywhere from about 50 to a hundred features - whether a student is on topic, the coherence or the flow of an argument, the complexity of word choice. And we've done a number of studies to show that the scoring can be highly accurate. SMITH: To demonstrate, he takes a not-so-stellar sample essay rife with spelling mistakes and sentence fragments, and he runs it by the robo-grader, which instantly spits back a not-so-stellar score. FOLTZ: So it gives an overall score of two out of four on these different writing traits. And it gets a one on spelling and grammar. It gives a two on task and focus, and. . . SMITH: Several states already use automated grading on their standardized tests. Utah, for example, started cautiously with human eyes backing up every computer score. But officials say the computers have proven spot-on, and now more states are considering it. (SOUNDBITE OF ARCHIVED RECORDING)JEFF WULFSON: I asked Alexa whether she thought we'd ever be able to use computers to reliably score student tests, and she said absolutely. (LAUGHTER)SMITH: Massachusetts Department of Education Deputy Commissioner Jeff Wulfson introduced the idea at a recent meeting. He's one of many now intrigued by the potential cost savings and the prospect of getting test results back in minutes rather than months. But many teachers are unconvinced. KELLY HENDERSON: The idea is bananas as far as I'm concerned. An art form, a form of expression being evaluated by an algorithm is patently ridiculous. ROBYN MARDER: Agreed. SMITH: Kelly Henderson and Robin Marder teach English at Newton South High School just outside Boston. HENDERSON: What about original ideas? Where's room for creativity of expression? A computer's going to miss all of that. SMITH: Even worse, Henderson worries robo-graders will encourage the worst kind of formulaic writing. HENDERSON: What is a computer program going to reward? Is it going to reward some vapid drivel that happens to be structurally sound? LES PERELMAN: That's a very easy question to answer. And that's what we'll see in the Babel Generator. SMITH: MIT researcher Les Perelman designed his Babel Generator to expose what he sees as the absurdity of robo-scoring. It works like a computerized Mad Libs, creating essays that makes zero sense but earn top scores from robo-graders. PERELMAN: OK, so we'll generate an essay. SMITH: To demonstrate, he gets an online practice question for the GRE exam that's graded with the same algorithms as actual tests. Then on his Babel Generator, he enters three words related to the essay prompt and presto, a 500-word wonder. PERELMAN: Motive is a scrutinization that has not and no doubt never will be disrupting yet somehow assimilated. SMITH: This is hilarious. PERELMAN: Yeah. SMITH: It makes no sense. PERELMAN: It makes absolutely no sense. SMITH: But Perelman promises that won't matter to the robo-grader and submits the essay for a score. PERELMAN: And. . . SMITH: Big moment of truth. PERELMAN: Six points - perfect score. It's so scary that it works. SMITH: It proves, Perelman says, that real ideas and facts don't matter to the algorithm and how easy it is to game the system. Even without a Babel Generator, he says, students can fool the computer by just using lots of big words, complex sentences and some key phrases like in conclusion. But Nitin Madnani, a researcher at ETS, the company that makes the GRE's robo-grader, says that's not exactly a hack. NITIN MADNANI: If somebody is smart enough to pay attention to all the things that a - you know, an automated system pays attention to, to incorporate them in their writing, that's no longer gaming. That's good writing. So you kind of do want to give them a good grade. SMITH: Madnani says actual GRE essays are always scored by a human reader as well as a computer, so pure babble would never pass a real test. And while other tests are graded only by machines, they're getting better at picking up student tricks and flagging them for human review. For example, some students have written one perfect paragraph and just repeated it four more times. Others have padded their essays with long quotes. David Shermis is a dean at the School of Education at the University of Houston, Clear Lake. DAVID SHERMIS: In this game of cat and mouse, the vendors have already identified that as a strategy. And so the essay will be scored with very low confidence, and it will say, please have a human rater take a look at this. SMITH: So in conclusion, robo-grading technology may indeed be demonstrating proficiency, but experts say it's also still got plenty of room for improvement. Tovia Smith, NPR News. SCOTT SIMON, HOST:  Little pop quiz now. Who writes our theme music - A, Snoop Dogg, B, Dolly Parton, C, Philip Glass or, D, B. J. Leiderman? A computer would quickly know D is the correct answer. But now computers are also starting to grade students' essays. As NPR's Tovia Smith reports, many teachers see that as a mistake. TOVIA SMITH, BYLINE: Developers of the so-called robo-graders say they understand the skepticism. But they say if computers are already driving cars, detecting cancer and carrying on conversations, they can also handle grading a high school essay on, say, the fall of the Roman Empire. PETER FOLTZ: Yeah, I've been working on this now for about 25 years, so I feel that it's something that - the time is right. And it's really starting to be used now. SMITH: Peter Foltz is a professor at the University of Colorado and a researcher for Pearson, a company whose automated scoring program graded some 34 million student essays on state and national high-stakes tests last year. Foltz says computers learn what's considered good writing by analyzing essays graded by humans, and then they simply scan for those same features. FOLTZ: We have artificial intelligence techniques which can judge anywhere from about 50 to a hundred features - whether a student is on topic, the coherence or the flow of an argument, the complexity of word choice. And we've done a number of studies to show that the scoring can be highly accurate. SMITH: To demonstrate, he takes a not-so-stellar sample essay rife with spelling mistakes and sentence fragments, and he runs it by the robo-grader, which instantly spits back a not-so-stellar score. FOLTZ: So it gives an overall score of two out of four on these different writing traits. And it gets a one on spelling and grammar. It gives a two on task and focus, and. . . SMITH: Several states already use automated grading on their standardized tests. Utah, for example, started cautiously with human eyes backing up every computer score. But officials say the computers have proven spot-on, and now more states are considering it. (SOUNDBITE OF ARCHIVED RECORDING) JEFF WULFSON: I asked Alexa whether she thought we'd ever be able to use computers to reliably score student tests, and she said absolutely. (LAUGHTER) SMITH: Massachusetts Department of Education Deputy Commissioner Jeff Wulfson introduced the idea at a recent meeting. He's one of many now intrigued by the potential cost savings and the prospect of getting test results back in minutes rather than months. But many teachers are unconvinced. KELLY HENDERSON: The idea is bananas as far as I'm concerned. An art form, a form of expression being evaluated by an algorithm is patently ridiculous. ROBYN MARDER: Agreed. SMITH: Kelly Henderson and Robin Marder teach English at Newton South High School just outside Boston. HENDERSON: What about original ideas? Where's room for creativity of expression? A computer's going to miss all of that. SMITH: Even worse, Henderson worries robo-graders will encourage the worst kind of formulaic writing. HENDERSON: What is a computer program going to reward? Is it going to reward some vapid drivel that happens to be structurally sound? LES PERELMAN: That's a very easy question to answer. And that's what we'll see in the Babel Generator. SMITH: MIT researcher Les Perelman designed his Babel Generator to expose what he sees as the absurdity of robo-scoring. It works like a computerized Mad Libs, creating essays that makes zero sense but earn top scores from robo-graders. PERELMAN: OK, so we'll generate an essay. SMITH: To demonstrate, he gets an online practice question for the GRE exam that's graded with the same algorithms as actual tests. Then on his Babel Generator, he enters three words related to the essay prompt and presto, a 500-word wonder. PERELMAN: Motive is a scrutinization that has not and no doubt never will be disrupting yet somehow assimilated. SMITH: This is hilarious. PERELMAN: Yeah. SMITH: It makes no sense. PERELMAN: It makes absolutely no sense. SMITH: But Perelman promises that won't matter to the robo-grader and submits the essay for a score. PERELMAN: And. . . SMITH: Big moment of truth. PERELMAN: Six points - perfect score. It's so scary that it works. SMITH: It proves, Perelman says, that real ideas and facts don't matter to the algorithm and how easy it is to game the system. Even without a Babel Generator, he says, students can fool the computer by just using lots of big words, complex sentences and some key phrases like in conclusion. But Nitin Madnani, a researcher at ETS, the company that makes the GRE's robo-grader, says that's not exactly a hack. NITIN MADNANI: If somebody is smart enough to pay attention to all the things that a - you know, an automated system pays attention to, to incorporate them in their writing, that's no longer gaming. That's good writing. So you kind of do want to give them a good grade. SMITH: Madnani says actual GRE essays are always scored by a human reader as well as a computer, so pure babble would never pass a real test. And while other tests are graded only by machines, they're getting better at picking up student tricks and flagging them for human review. For example, some students have written one perfect paragraph and just repeated it four more times. Others have padded their essays with long quotes. David Shermis is a dean at the School of Education at the University of Houston, Clear Lake. DAVID SHERMIS: In this game of cat and mouse, the vendors have already identified that as a strategy. And so the essay will be scored with very low confidence, and it will say, please have a human rater take a look at this. SMITH: So in conclusion, robo-grading technology may indeed be demonstrating proficiency, but experts say it's also still got plenty of room for improvement. Tovia Smith, NPR News.", "section": "National", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-02-625406645": {"title": "Move Over Uber: How The Internet Helps Domestic Workers Find Jobs : NPR", "url": "https://www.npr.org/2018/07/02/625406645/move-over-uber-how-the-internet-helps-domestic-workers-find-jobs", "author": "No author found", "published_date": "2018-07-02", "content": "(SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")MARY LOUISE KELLY, HOST: We have talked lots about Uber drivers and their work status. Our next guest says that is only part of the story of how apps are changing the gig economy. JULIA TICONA: What we know or what we knew about the gig economy and technology's influence in it has really been a men's story up until now. And so what we wanted to do is to really focus on women's experiences. KELLY: For researcher Julia Ticona, that meant surveying nannies and housekeepers and other domestic workers, jobs traditionally dominated by women who are increasingly turning to apps to find work, apps like care. com, TaskRabbit, Handy. I asked Ticona why she used pseudonyms for the workers profiled in her new report. TICONA: The decision to allow people to remain anonymous is a pretty standard one in qualitative research reports like this one. But it was especially important for us with this research because workers put so much information about themselves online. And so we wanted to be sensitive to the ways that workers may want to control the way that their experiences are portrayed in this report. KELLY: Another thing I want to ask about is what I gather is just the constant fear of deactivation, that if you have a less-than-fantastic experience with one employer and they complain about you, the site can take you offline while they investigate, and then you lose all the potential jobs that might be out there. TICONA: This depends on the platform. But some platforms will just, as you said, straight up deactivate you. We actually had an interviewee who had a really simple miscommunication with one of her clients about how to return a key. And she forgot to put it in the designated place. The client reported that key as stolen or missing, and she was deactivated from the app, which then made it so she couldn't actually communicate with the client to return her key. And it took her two months to reactivate her account in order to keep getting work in that way. For the nannies, however, it's something a little bit different. The ways that these platforms structure workers' reputations is still really important for the care workers on a site like care. com. The reviews of workers by clients only go one way. Clients can really only rate workers. Workers cannot rate clients. And what we found was that a bad rating on something like care. com has huge effects on somebody's ability to get a job. If you're a successful care worker, maybe you work for a few families relatively regularly. You don't have 120 reviews to sort of average out all of those different ratings. There's a hundred different ways that someone might give you a one-star review online. That - those have really huge consequences for these workers. KELLY: And I can imagine as a parent who's hired a lot of babysitters and nannies, one bad review is all it takes to give you pause. TICONA: Exactly. KELLY: There are also - beyond issues to your reputation or issues about being blocked from a site so you can't find work going forward, there are also real safety issues either in terms of going into a home that belongs to somebody else and the risks that come with that or just - I don't know - if you're trying to clean a house, you can slip and fall. We asked some of these online marketplaces to comment on worker safety. Care. com told us they have a dedicated safety team. TaskRabbit told us that they tell people, leave any situation that is not safe, and they also pointed out they have this live chat feature in the app. So if somebody feels in danger for whatever reason, they can reach out and communicate. What did you find the problem to be? Is it that workers aren't seeking help that they could be getting from these apps, or that the companies are not doing enough to protect people finding work through these platforms? TICONA: I think it's a little bit of both. One of the big problems that we saw with folks getting work through these apps was that these companies sell themselves as easy onboarding. All it takes is a smartphone. You know, if you have a mop and a vacuum or if you have experience caring for children, you throw up a profile, you can get work relatively quickly and easily. Unfortunately, what that means for workers is that a lot of times they're pretty unfamiliar with all the ins and outs working through an app like this. All they know is that their ratings and their ability to get future work is incredibly dependent on this client right now and how this client rates them. What if I tell this person that I feel uncomfortable and then they give me a one-star rating? KELLY: Yeah. TICONA: Will the app actually protect me in that case or not? KELLY: Thanks very much. TICONA: Thank you so much. I really appreciate the time to talk about this stuff. KELLY: That's Julia Ticona. She's one of the authors of \"Beyond Disruption: How Tech Shapes Labor Across Domestic Work And Ride Hailing,\" a new report put out by the research institute Data & Society. (SOUNDBITE OF GARRY HUGHES' \"TRANCE 7\") (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") MARY LOUISE KELLY, HOST:  We have talked lots about Uber drivers and their work status. Our next guest says that is only part of the story of how apps are changing the gig economy. JULIA TICONA: What we know or what we knew about the gig economy and technology's influence in it has really been a men's story up until now. And so what we wanted to do is to really focus on women's experiences. KELLY: For researcher Julia Ticona, that meant surveying nannies and housekeepers and other domestic workers, jobs traditionally dominated by women who are increasingly turning to apps to find work, apps like care. com, TaskRabbit, Handy. I asked Ticona why she used pseudonyms for the workers profiled in her new report. TICONA: The decision to allow people to remain anonymous is a pretty standard one in qualitative research reports like this one. But it was especially important for us with this research because workers put so much information about themselves online. And so we wanted to be sensitive to the ways that workers may want to control the way that their experiences are portrayed in this report. KELLY: Another thing I want to ask about is what I gather is just the constant fear of deactivation, that if you have a less-than-fantastic experience with one employer and they complain about you, the site can take you offline while they investigate, and then you lose all the potential jobs that might be out there. TICONA: This depends on the platform. But some platforms will just, as you said, straight up deactivate you. We actually had an interviewee who had a really simple miscommunication with one of her clients about how to return a key. And she forgot to put it in the designated place. The client reported that key as stolen or missing, and she was deactivated from the app, which then made it so she couldn't actually communicate with the client to return her key. And it took her two months to reactivate her account in order to keep getting work in that way. For the nannies, however, it's something a little bit different. The ways that these platforms structure workers' reputations is still really important for the care workers on a site like care. com. The reviews of workers by clients only go one way. Clients can really only rate workers. Workers cannot rate clients. And what we found was that a bad rating on something like care. com has huge effects on somebody's ability to get a job. If you're a successful care worker, maybe you work for a few families relatively regularly. You don't have 120 reviews to sort of average out all of those different ratings. There's a hundred different ways that someone might give you a one-star review online. That - those have really huge consequences for these workers. KELLY: And I can imagine as a parent who's hired a lot of babysitters and nannies, one bad review is all it takes to give you pause. TICONA: Exactly. KELLY: There are also - beyond issues to your reputation or issues about being blocked from a site so you can't find work going forward, there are also real safety issues either in terms of going into a home that belongs to somebody else and the risks that come with that or just - I don't know - if you're trying to clean a house, you can slip and fall. We asked some of these online marketplaces to comment on worker safety. Care. com told us they have a dedicated safety team. TaskRabbit told us that they tell people, leave any situation that is not safe, and they also pointed out they have this live chat feature in the app. So if somebody feels in danger for whatever reason, they can reach out and communicate. What did you find the problem to be? Is it that workers aren't seeking help that they could be getting from these apps, or that the companies are not doing enough to protect people finding work through these platforms? TICONA: I think it's a little bit of both. One of the big problems that we saw with folks getting work through these apps was that these companies sell themselves as easy onboarding. All it takes is a smartphone. You know, if you have a mop and a vacuum or if you have experience caring for children, you throw up a profile, you can get work relatively quickly and easily. Unfortunately, what that means for workers is that a lot of times they're pretty unfamiliar with all the ins and outs working through an app like this. All they know is that their ratings and their ability to get future work is incredibly dependent on this client right now and how this client rates them. What if I tell this person that I feel uncomfortable and then they give me a one-star rating? KELLY: Yeah. TICONA: Will the app actually protect me in that case or not? KELLY: Thanks very much. TICONA: Thank you so much. I really appreciate the time to talk about this stuff. KELLY: That's Julia Ticona. She's one of the authors of \"Beyond Disruption: How Tech Shapes Labor Across Domestic Work And Ride Hailing,\" a new report put out by the research institute Data & Society. (SOUNDBITE OF GARRY HUGHES' \"TRANCE 7\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-02-625250383": {"title": "Elon Musk Hails Tesla Reaching Goal Of 5,000 Model 3s A Week : NPR", "url": "https://www.npr.org/2018/07/02/625250383/elon-musk-hails-tesla-reaching-goal-of-5-000-model-3s-a-week", "author": "No author found", "published_date": "2018-07-02", "content": "", "section": "Technology", "disclaimer": ""}, "2018-07-07-625332469": {"title": "Many Look To Buddhism For Sanctuary From An Over-Connected World : NPR", "url": "https://www.npr.org/2018/07/07/625332469/many-look-to-buddhism-for-sanctuary-from-an-over-connected-world", "author": "No author found", "published_date": "2018-07-07", "content": "LINDA WERTHEIMER, HOST: The amount of time people spend on digital devices is soaring to the point that several countries are treating Internet addiction as a public health crisis. According to one survey, the average American adult spends nearly six hours a day on a smartphone. As people struggle to deal with their distracting devices, James Socolovsky reports on a group turning to Buddhism for more mindful approach. JEROME SOCOLOVSKY, BYLINE: About 15 people are seated on the floor of the All Beings Zen Sangha worship space in an apartment building in Washington, D. C. UNIDENTIFIED GROUP: (Chanting in Japanese). SOCOLOVKSY: They recite a Japanese chant known as the ten-phrase, life-prolonging Kannon Sutra and extol the teachings of the Buddhist sages. MARK STONE: (Singing) Heart of great and perfect wisdom sutra. (SOUNDBITE OF BELL)SOCOLOVKSY: And then they meditate for a full 30 minutes. (SOUNDBITE OF BELL)SOCOLOVKSY: It's completely silent - save for the air conditioning - until Mark Stone, one of the leaders, speaks. STONE: If you could take out your screens - stay on them for 12 minutes doing what you usually do. SOCOLOVKSY: During this screen-use workshop, participants stay in meditation pose while sending texts on their phones and checking in on social media. Stone, a retired economist, urges them to follow Buddhist principles, such as mindfulness and intentionality when they're online. He tells them to be aware of their posture and take deep breaths. What's been really helpful for him. . . STONE: . . . Is, when I pick up my screen, think about my intention. Why I'm enjoying this? SOCOLOVKSY: He also recommends setting aside devices for phone-free meals and longer digital fasts. At the end of the 12 minutes on their devices, Stone has a request. STONE: Anybody like to share how that was for them, to use the screen and then to sit, pause, take it all in? CARLOS MOURA: I did notice afterwards that it really wasn't - that I was focused, but I really wasn't aware of you all. You know, it's, like, you weren't there at all. LESLIE COHEN: I just physically noticed that my head really hurt. SOCOLOVKSY: Carlos Moura and Leslie Cohen are among the people taking part in the screen mindfulness workshop. Afterward, Cohen, a tourist from San Diego, says the chance to turn off is what brought her here. COHEN: We were in, like, Ocean City. And just - you know, the TV was on. The kids were on their screens. And I had a moment of, like, I've got to find a place to meditate as soon as I get to Washington, D. C. (SOUNDBITE OF BELL)SOCOLOVKSY: A meditation session begins at a different Buddhist center a few miles away. Bhante Dhammasiri, who was born in Sri Lanka, is the chief monk of the Theravadic Washington Buddhist Vihara or monastery. He's lived in this country for 32 years, long enough, he says, to watch a society become hooked on screens. BHANTE DHAMMASIRI: What we see today - they don't live the life. They forget to live the life because they are addicted to cellphone, especially cellphones. SOCOLOVKSY: He has a cellphone, which he says a devotee gave him but uses it mainly for calls and as a calendar. And he likes the convenience of the flashlight. But he won't go on Facebook or other social media platforms because. . . DHAMMASIRI: You are never getting satisfied. You will waste your whole precious time. SOCOLOVKSY: These devices may promise happiness and fulfillment but, the monk says, it's just an illusion. For NPR News, I'm Jerome Socolovsky in Washington. (SOUNDBITE OF MUSIC) LINDA WERTHEIMER, HOST:  The amount of time people spend on digital devices is soaring to the point that several countries are treating Internet addiction as a public health crisis. According to one survey, the average American adult spends nearly six hours a day on a smartphone. As people struggle to deal with their distracting devices, James Socolovsky reports on a group turning to Buddhism for more mindful approach. JEROME SOCOLOVSKY, BYLINE: About 15 people are seated on the floor of the All Beings Zen Sangha worship space in an apartment building in Washington, D. C. UNIDENTIFIED GROUP: (Chanting in Japanese). SOCOLOVKSY: They recite a Japanese chant known as the ten-phrase, life-prolonging Kannon Sutra and extol the teachings of the Buddhist sages. MARK STONE: (Singing) Heart of great and perfect wisdom sutra. (SOUNDBITE OF BELL) SOCOLOVKSY: And then they meditate for a full 30 minutes. (SOUNDBITE OF BELL) SOCOLOVKSY: It's completely silent - save for the air conditioning - until Mark Stone, one of the leaders, speaks. STONE: If you could take out your screens - stay on them for 12 minutes doing what you usually do. SOCOLOVKSY: During this screen-use workshop, participants stay in meditation pose while sending texts on their phones and checking in on social media. Stone, a retired economist, urges them to follow Buddhist principles, such as mindfulness and intentionality when they're online. He tells them to be aware of their posture and take deep breaths. What's been really helpful for him. . . STONE: . . . Is, when I pick up my screen, think about my intention. Why I'm enjoying this? SOCOLOVKSY: He also recommends setting aside devices for phone-free meals and longer digital fasts. At the end of the 12 minutes on their devices, Stone has a request. STONE: Anybody like to share how that was for them, to use the screen and then to sit, pause, take it all in? CARLOS MOURA: I did notice afterwards that it really wasn't - that I was focused, but I really wasn't aware of you all. You know, it's, like, you weren't there at all. LESLIE COHEN: I just physically noticed that my head really hurt. SOCOLOVKSY: Carlos Moura and Leslie Cohen are among the people taking part in the screen mindfulness workshop. Afterward, Cohen, a tourist from San Diego, says the chance to turn off is what brought her here. COHEN: We were in, like, Ocean City. And just - you know, the TV was on. The kids were on their screens. And I had a moment of, like, I've got to find a place to meditate as soon as I get to Washington, D. C. (SOUNDBITE OF BELL) SOCOLOVKSY: A meditation session begins at a different Buddhist center a few miles away. Bhante Dhammasiri, who was born in Sri Lanka, is the chief monk of the Theravadic Washington Buddhist Vihara or monastery. He's lived in this country for 32 years, long enough, he says, to watch a society become hooked on screens. BHANTE DHAMMASIRI: What we see today - they don't live the life. They forget to live the life because they are addicted to cellphone, especially cellphones. SOCOLOVKSY: He has a cellphone, which he says a devotee gave him but uses it mainly for calls and as a calendar. And he likes the convenience of the flashlight. But he won't go on Facebook or other social media platforms because. . . DHAMMASIRI: You are never getting satisfied. You will waste your whole precious time. SOCOLOVKSY: These devices may promise happiness and fulfillment but, the monk says, it's just an illusion. For NPR News, I'm Jerome Socolovsky in Washington. (SOUNDBITE OF MUSIC)", "section": "National", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-09-627266501": {"title": "The Push For A Gender-Neutral Siri : NPR", "url": "https://www.npr.org/2018/07/09/627266501/the-push-for-a-gender-neutral-siri", "author": "No author found", "published_date": "2018-07-09", "content": "AILSA CHANG, HOST: I'm Ailsa Chang with All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CHANG: Have you ever noticed something that most virtual assistants have in common? Just take a listen. SIRI: My name is Siri. CORTANA: Cortana here. How can I help? ALEXA: My name is Alexa. CHANG: Siri, Cortana, Alexa all start out mostly with female voices as their defaults. That's riled up a group of marketing and ad executives, tech experts and academics. They've gotten together to question the company's decisions and are campaigning for a change. NPR's Laura Sydell reports. (SOUNDBITE OF TV SHOW, \"STAR TREK: THE ORIGINAL SERIES\")LAURA SYDELL, BYLINE: There's a story about why Amazon's Alexa was given a female voice. (SOUNDBITE OF TV SHOW, \"STAR TREK: THE ORIGINAL SERIES\")WILLIAM SHATNER: (As Captain Kirk) Information on Anton Karidian. MAJEL BARRETT: (As Enterprise Computer) Director and star of traveling company of actors. SYDELL: Amazon's CEO Jeff Bezos is a \"Star Trek\" fan. Alex Spinelli used to run the team that created the software for Amazon's Alexa. ALEX SPINELLI: A big part of launching Alexa, the idea, was creating the \"Star Trek\" computer. The \"Star Trek\" computer was a woman. SYDELL: That's a story that bothers Robert LoCascio. ROBERT LOCASCIO: That's just not a good enough reason to make a technology a woman. SYDELL: LoCascio is the CEO of LivePerson, a company that builds chatbots and AI personas for brands. Chatbots are those little characters that pop up on your screen to offer help. LoCascio says the male-dominated AI industry brings its own unconscious bias to the decision of what gender to make a virtual assistant. LOCASCIO: That's why I believe it's, like, some guy is somewhere going, yeah, my mom - she's great at doing tasks. That's great, so I'll make it a woman's voice. SYDELL: LoCascio is one of the leaders of an effort called the Equal AI Initiative. Its members include Arianna Huffington and Wikipedia co-founder Jimmy Wales. LoCascio has a 2-year-old daughter. He was troubled that each of the major virtual assistants - Cortana, Alexa, Siri and the Google Assistant - had female voices and mostly female names. LOCASCIO: If you talk derogatory to an Alexa, children pick this up. They go back to school, and they think this is the way you talk to someone, and this may be the way you talk to women. SYDELL: LoCascio says it's important to act now because we're on the cusp of a major AI revolution. Over the next decade, these characters are likely to become ubiquitous at home, work, inside cars. The companies defend their choices. Many, including Amazon, say they tested different voices, and customers liked the women better. Deborah Harrison is manager of the Cortana editorial team at Microsoft. DEBORAH HARRISON: We had done some research about how people responded to different kinds of voices in different formats. At that time, people were responding better to the female voices. SYDELL: It's not surprising to Justine Cassell that American consumers prefer the female voice. Cassell is also a member of the Equal AI Initiative, and she is dean at the School of Computer Science at Carnegie Mellon University. She says cultures have unconscious biases. Take Apple's Siri. The company won't say why, but in Great Britain, it launched Siri with a male voice. Cassell thinks it's because the British have always had male servants. JUSTINE CASSELL: Apple may be wishing to evoke that stereotype of the always helpful, always present valet. SYDELL: In Germany, when BMW launched a GPS system with a female voice, the company got complaints from male customers who didn't want a woman telling them what to do according to the late Stanford University researcher Clifford Nass. Google and Apple now let customers choose a male voice, but Cassell and others are pushing a third option, the gender-ambiguous virtual assistant. Cassell says they do a little pitch adjusting to the voice, and it's harder to tell what gender it is. Here's the voice of a gender-neutral tutor for kids that Cassell created. COMPUTER-GENERATED VOICE: If I have three apples, and I take one apple away, how many apples do I have? CASSELL: Two. SYDELL: Cassell says most people project their own gender onto the character. Cassell and LivePerson's LoCascio say what's most important to consumers isn't gender. LoCascio says he wants to help companies understand what people want from virtual assistants. LivePerson works with GoDaddy, an Internet domain registration firm which is developing chatbots to help customers. Robert Ashby, GoDaddy senior director of digital care, says what he learned is that customers need to feel heard. ROBERT ASHBY: You could make it male or female, and as long as it's really hitting that core of empathy and support, that is the core to success, especially in a care engagement. SYDELL: GoDaddy is working on a gender-neutral chatbot. The home improvement chain Lowe's is, too. With help from LivePerson, it's creating a genderless character to help customers with questions about outdoor grills. It's just called Grill Master. Alexa might like all these efforts to empower women. Alexa, are you a feminist? ALEXA: Yes, I'm a feminist as defined by believing in gender equality. SYDELL: The question is whether her makers at Amazon are willing to take a risk and offer an alternative to Alexa, maybe an Alex. Laura Sydell, NPR News. AILSA CHANG, HOST:  I'm Ailsa Chang with All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CHANG: Have you ever noticed something that most virtual assistants have in common? Just take a listen. SIRI: My name is Siri. CORTANA: Cortana here. How can I help? ALEXA: My name is Alexa. CHANG: Siri, Cortana, Alexa all start out mostly with female voices as their defaults. That's riled up a group of marketing and ad executives, tech experts and academics. They've gotten together to question the company's decisions and are campaigning for a change. NPR's Laura Sydell reports. (SOUNDBITE OF TV SHOW, \"STAR TREK: THE ORIGINAL SERIES\") LAURA SYDELL, BYLINE: There's a story about why Amazon's Alexa was given a female voice. (SOUNDBITE OF TV SHOW, \"STAR TREK: THE ORIGINAL SERIES\") WILLIAM SHATNER: (As Captain Kirk) Information on Anton Karidian. MAJEL BARRETT: (As Enterprise Computer) Director and star of traveling company of actors. SYDELL: Amazon's CEO Jeff Bezos is a \"Star Trek\" fan. Alex Spinelli used to run the team that created the software for Amazon's Alexa. ALEX SPINELLI: A big part of launching Alexa, the idea, was creating the \"Star Trek\" computer. The \"Star Trek\" computer was a woman. SYDELL: That's a story that bothers Robert LoCascio. ROBERT LOCASCIO: That's just not a good enough reason to make a technology a woman. SYDELL: LoCascio is the CEO of LivePerson, a company that builds chatbots and AI personas for brands. Chatbots are those little characters that pop up on your screen to offer help. LoCascio says the male-dominated AI industry brings its own unconscious bias to the decision of what gender to make a virtual assistant. LOCASCIO: That's why I believe it's, like, some guy is somewhere going, yeah, my mom - she's great at doing tasks. That's great, so I'll make it a woman's voice. SYDELL: LoCascio is one of the leaders of an effort called the Equal AI Initiative. Its members include Arianna Huffington and Wikipedia co-founder Jimmy Wales. LoCascio has a 2-year-old daughter. He was troubled that each of the major virtual assistants - Cortana, Alexa, Siri and the Google Assistant - had female voices and mostly female names. LOCASCIO: If you talk derogatory to an Alexa, children pick this up. They go back to school, and they think this is the way you talk to someone, and this may be the way you talk to women. SYDELL: LoCascio says it's important to act now because we're on the cusp of a major AI revolution. Over the next decade, these characters are likely to become ubiquitous at home, work, inside cars. The companies defend their choices. Many, including Amazon, say they tested different voices, and customers liked the women better. Deborah Harrison is manager of the Cortana editorial team at Microsoft. DEBORAH HARRISON: We had done some research about how people responded to different kinds of voices in different formats. At that time, people were responding better to the female voices. SYDELL: It's not surprising to Justine Cassell that American consumers prefer the female voice. Cassell is also a member of the Equal AI Initiative, and she is dean at the School of Computer Science at Carnegie Mellon University. She says cultures have unconscious biases. Take Apple's Siri. The company won't say why, but in Great Britain, it launched Siri with a male voice. Cassell thinks it's because the British have always had male servants. JUSTINE CASSELL: Apple may be wishing to evoke that stereotype of the always helpful, always present valet. SYDELL: In Germany, when BMW launched a GPS system with a female voice, the company got complaints from male customers who didn't want a woman telling them what to do according to the late Stanford University researcher Clifford Nass. Google and Apple now let customers choose a male voice, but Cassell and others are pushing a third option, the gender-ambiguous virtual assistant. Cassell says they do a little pitch adjusting to the voice, and it's harder to tell what gender it is. Here's the voice of a gender-neutral tutor for kids that Cassell created. COMPUTER-GENERATED VOICE: If I have three apples, and I take one apple away, how many apples do I have? CASSELL: Two. SYDELL: Cassell says most people project their own gender onto the character. Cassell and LivePerson's LoCascio say what's most important to consumers isn't gender. LoCascio says he wants to help companies understand what people want from virtual assistants. LivePerson works with GoDaddy, an Internet domain registration firm which is developing chatbots to help customers. Robert Ashby, GoDaddy senior director of digital care, says what he learned is that customers need to feel heard. ROBERT ASHBY: You could make it male or female, and as long as it's really hitting that core of empathy and support, that is the core to success, especially in a care engagement. SYDELL: GoDaddy is working on a gender-neutral chatbot. The home improvement chain Lowe's is, too. With help from LivePerson, it's creating a genderless character to help customers with questions about outdoor grills. It's just called Grill Master. Alexa might like all these efforts to empower women. Alexa, are you a feminist? ALEXA: Yes, I'm a feminist as defined by believing in gender equality. SYDELL: The question is whether her makers at Amazon are willing to take a risk and offer an alternative to Alexa, maybe an Alex. Laura Sydell, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-11-627877965": {"title": "How Has Technology Affected Your Summer Vacations? Tell Us About It : NPR", "url": "https://www.npr.org/2018/07/11/627877965/how-has-technology-affected-your-summer-vacations-tell-us-about-it", "author": "No author found", "published_date": "2018-07-11", "content": "", "section": "Technology", "disclaimer": ""}, "2018-07-12-628550280": {"title": "PayPal Letter To Deceased Customer: 'You Should Read This Notice Carefully'  : NPR", "url": "https://www.npr.org/2018/07/12/628550280/paypal-letter-to-deceased-customer-you-should-read-this-notice-carefully", "author": "No author found", "published_date": "2018-07-12", "content": "", "section": "Business", "disclaimer": ""}, "2018-07-12-628522147": {"title": "Twitter Is Removing Millions Of Fake Followers From Users' Lists : NPR", "url": "https://www.npr.org/2018/07/12/628522147/twitter-is-removing-millions-of-fake-followers-from-users-lists", "author": "No author found", "published_date": "2018-07-12", "content": "", "section": "News", "disclaimer": ""}, "2018-07-12-628315199": {"title": "Michael Chertoff On Privacy In The Digital Age : NPR", "url": "https://www.npr.org/2018/07/12/628315199/michael-chertoff-on-privacy-in-the-digital-age", "author": "No author found", "published_date": "2018-07-12", "content": "NOEL KING, HOST:  Michael Chertoff was secretary of Homeland Security during President George W. Bush's second term. Chertoff also helped write the Patriot Act, which many privacy advocates think of as a vast government overreach into private lives, so it may seem surprising that Chertoff is now out with a new book about privacy and the appropriate collection of data. I talked to him about the book. It's called \"Exploding Data: Reclaiming Our Cyber Security In The Digital Age. \" And I asked him if he's changed his mind about the Patriot Act. MICHAEL CHERTOFF: No, it's not a question of changing my mind. I think, first of all, people sometimes really misunderstand the Patriot Act. Much of what it did had to do with sharing of information the government already held but was restricted in sharing among different agencies, and some of it dealt with treating what was generated as communication over the Internet under the same standard as telephony. But I will say this. I became aware of the enormous potential for good, but also of the risks involved in data collection. And as we went through various iterations of government collection of information, what we saw was, as people became concerned that maybe something was too generous to the government, it was tweaked, and it was maybe pulled back a little bit. KING: What does that look like? CHERTOFF: So a good example is this. There's a program that existed under the Patriot Act which allowed the government to basically collect what they call metadata. Metadata is who called who and how long. It does not involve the content of a conversation. People got concerned about that, so eventually, what happened under the Obama administration is, they changed the rule, and they said, OK, the government can't collect the metadata; it has to stay in the hands of the telephone or Internet companies until you get appropriate judicial permission to inspect it. KING: We know about the metadata because of Edward Snowden's leaks, not because the government came out and said, you know, we've been doing this terrible thing. Do you really trust the government to monitor and police itself? CHERTOFF: You know, actually, I do trust the government. First of all, the Congress knew about the collection of metadata, and the courts knew and approved of it. And although Snowden may not have liked it, it did comply with the law, and it was being regularly reviewed. KING: But nothing changed - nothing changed until the leaks and until the public found out about it. If we didn't know, wouldn't this still be going on? CHERTOFF: Well, I do - look; I think transparency - somewhat greater transparency would be a good thing because, frankly, I think if the government had made clear the scope of the program earlier and had released some of the judicial opinions, which were - eventually came out, people actually would've been, by and large, OK with it. KING: All right. So you're saying that the government does adapt to people's concerns. CHERTOFF: Remarkably, the government's much more adaptable than the private sector to issues about privacy. KING: Why is that, do you think? CHERTOFF: I think, frankly, it's the result of many, many years of battle scars. If you go back decades ago, you know, the government has been in - sometimes gotten in hot water over issues of collection. And so over time, the government has become sensitive. I think the private sector for a long time was viewed as the good guys. And I think what people have begun to recognize is that there is enormous value in personal data and that that is being harvested by the companies, and then that creates the risk of misuse. KING: There's a section in your book where you sort of take a glimpse into the future, and you imagine a U. S. society where people know that their data is being collected and where they begin to feel as if they're always being watched, and then they change their behavior. Is that the future that you think we are headed towards? CHERTOFF: I think it's a future we may be headed for if we don't take steps now. So look at all the data we generate now. If you have a personal exercise device, it generates data. Your credit card generates data about what you buy. You may be wearing a device that measures how you sleep. Someone could literally look at everything you do and make a judgment about whether you're living a healthy lifestyle or not-so-healthy lifestyle, and then your health insurer could decide they're going to raise your rates or lower your rates. And pretty soon, what would happen is you - every time you made a decision, you would say, hmm, am I being monitored? KING: So I really want a BLT, but I know that my health insurance company is going to know that I'm eating bacon, and so I'm not going to have the BLT. I'm going to have a salad. CHERTOFF: That may be one thing, but it may also mean you're not going to go certain places because you're worried a future employer may hold that against you, or you're not going to buy certain things because you're afraid a future employer may think that that shows your frame of mind is not good. KING: Michael Chertoff is author of the new book \"Exploding Data: Reclaiming Our Cyber Security In The Digital Age. \" Mr. Chertoff, thank you so much. CHERTOFF: Thank you. NOEL KING, HOST:   Michael Chertoff was secretary of Homeland Security during President George W. Bush's second term. Chertoff also helped write the Patriot Act, which many privacy advocates think of as a vast government overreach into private lives, so it may seem surprising that Chertoff is now out with a new book about privacy and the appropriate collection of data. I talked to him about the book. It's called \"Exploding Data: Reclaiming Our Cyber Security In The Digital Age. \" And I asked him if he's changed his mind about the Patriot Act. MICHAEL CHERTOFF: No, it's not a question of changing my mind. I think, first of all, people sometimes really misunderstand the Patriot Act. Much of what it did had to do with sharing of information the government already held but was restricted in sharing among different agencies, and some of it dealt with treating what was generated as communication over the Internet under the same standard as telephony. But I will say this. I became aware of the enormous potential for good, but also of the risks involved in data collection. And as we went through various iterations of government collection of information, what we saw was, as people became concerned that maybe something was too generous to the government, it was tweaked, and it was maybe pulled back a little bit. KING: What does that look like? CHERTOFF: So a good example is this. There's a program that existed under the Patriot Act which allowed the government to basically collect what they call metadata. Metadata is who called who and how long. It does not involve the content of a conversation. People got concerned about that, so eventually, what happened under the Obama administration is, they changed the rule, and they said, OK, the government can't collect the metadata; it has to stay in the hands of the telephone or Internet companies until you get appropriate judicial permission to inspect it. KING: We know about the metadata because of Edward Snowden's leaks, not because the government came out and said, you know, we've been doing this terrible thing. Do you really trust the government to monitor and police itself? CHERTOFF: You know, actually, I do trust the government. First of all, the Congress knew about the collection of metadata, and the courts knew and approved of it. And although Snowden may not have liked it, it did comply with the law, and it was being regularly reviewed. KING: But nothing changed - nothing changed until the leaks and until the public found out about it. If we didn't know, wouldn't this still be going on? CHERTOFF: Well, I do - look; I think transparency - somewhat greater transparency would be a good thing because, frankly, I think if the government had made clear the scope of the program earlier and had released some of the judicial opinions, which were - eventually came out, people actually would've been, by and large, OK with it. KING: All right. So you're saying that the government does adapt to people's concerns. CHERTOFF: Remarkably, the government's much more adaptable than the private sector to issues about privacy. KING: Why is that, do you think? CHERTOFF: I think, frankly, it's the result of many, many years of battle scars. If you go back decades ago, you know, the government has been in - sometimes gotten in hot water over issues of collection. And so over time, the government has become sensitive. I think the private sector for a long time was viewed as the good guys. And I think what people have begun to recognize is that there is enormous value in personal data and that that is being harvested by the companies, and then that creates the risk of misuse. KING: There's a section in your book where you sort of take a glimpse into the future, and you imagine a U. S. society where people know that their data is being collected and where they begin to feel as if they're always being watched, and then they change their behavior. Is that the future that you think we are headed towards? CHERTOFF: I think it's a future we may be headed for if we don't take steps now. So look at all the data we generate now. If you have a personal exercise device, it generates data. Your credit card generates data about what you buy. You may be wearing a device that measures how you sleep. Someone could literally look at everything you do and make a judgment about whether you're living a healthy lifestyle or not-so-healthy lifestyle, and then your health insurer could decide they're going to raise your rates or lower your rates. And pretty soon, what would happen is you - every time you made a decision, you would say, hmm, am I being monitored? KING: So I really want a BLT, but I know that my health insurance company is going to know that I'm eating bacon, and so I'm not going to have the BLT. I'm going to have a salad. CHERTOFF: That may be one thing, but it may also mean you're not going to go certain places because you're worried a future employer may hold that against you, or you're not going to buy certain things because you're afraid a future employer may think that that shows your frame of mind is not good. KING: Michael Chertoff is author of the new book \"Exploding Data: Reclaiming Our Cyber Security In The Digital Age. \" Mr. Chertoff, thank you so much. CHERTOFF: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-12-628085238": {"title": "Russian Influence Campaign Sought To Exploit Americans' Trust In Local News : NPR", "url": "https://www.npr.org/2018/07/12/628085238/russian-influence-campaign-sought-to-exploit-americans-trust-in-local-news", "author": "No author found", "published_date": "2018-07-12", "content": "AILSA CHANG, HOST: Americans have a tendency to trust local news sources. Maybe you're listening to your own local NPR station right now. The Russian government's misinformation effort during the 2016 election, an effort that continues today, sought to take advantage of that trust. NPR's Tim Mak has this story. TIM MAK, BYLINE: Much of what we now understand to be the Russian government's effort to interfere with the American elections took place in a building in St. Petersburg called the Internet Research Agency. Here's Deputy Attorney General Rod Rosenstein announcing special counsel Mueller's charges against the Internet Research Agency and others in February. (SOUNDBITE OF ARCHIVED RECORDING)ROD ROSENSTEIN: The defendants allegedly conducted what they called information warfare against the United States with the stated goal of spreading distrust towards the candidates and the political system in general. MAK: In 2017 and then again this past May, Twitter provided the House Intelligence Committee with lists of thousands of accounts that they identified as being linked to the Internet Research Agency. Hidden among these accounts was a group of Twitter accounts that appeared to be fake local news outlets. NPR has counted 48 of them. They have names like El Paso Top News, Milwaukee Voice, Camden City News and Seattle Post. BRET SCHAFER: A not insignificant amount of those had some sort of variation on what appeared to be a homegrown local news site - so Chicago Daily News, which actually was a newspaper but went bust in the '70s - but just things like Baltimore Breaking News. MAK: That's Bret Schafer, a social media analyst for the Alliance for Securing Democracy, which tracks Russian influence operations. Let's take one example. The Internet Research Agency created an account that looks like it is the Chicago Daily News. The account was created back in May 2014 and for years just tweeted local headlines, accumulating 19,000 followers by July 2016. Here's the thing. These accounts never spread misinformation. They just tweeted out real local news, serving as sleeper accounts building trust and readership for some future unforeseen effort. Here's Schafer again. SCHAFER: They set them up for a reason, and if at any given moment they wanted to operationalize this sort of network of what seemed to be local American news handles, they can significantly influence the narrative on a breaking news story. MAK: Though Twitter caught these Internet Research Agency accounts in the act and suspended them, the fact that the Russian government created them in the first place gives us insight into their strategy, firstly that the Russian misinformation campaign was a years-long effort, one that wasn't simply focused on the 2016 election but on destabilizing the United States over an extended period of time. Here's Congressman Adam Schiff, the top Democrat on the House Intelligence Committee. ADAM SCHIFF: The Russians are playing a long game that they've developed a presence on social media, that they've created these fictitious persons and fictitious organizations that have built up over a period of time a certain trustworthiness among people that follow them. MAK: The failed effort to make local news accounts also tells us something about how Americans trust local news more than national news and how the Russians knew of that vulnerability. A Pew survey from 2016 found that 82 percent of Americans have some or a lot of confidence in local news organizations. Here's Tom Rosenstiel, the executive director of the American Press Institute. TOM ROSENSTIEL: If you were trying to pass along information that is not true but you want people to believe it, creating or inventing fake local news sources is an effective way of doing it because people will convey some trust to the locality even if the publication is one they've never seen before. MAK: The Russian misinformation campaign continues now. Here's Senator Susan Collins, a Republican on the Senate Intelligence Committee, which is currently investigating Russian interference. SUSAN COLLINS: This effort is not over. It continues to this very day where the Russians are trying to sow the seeds of discontent in our society, take advantage of the polarization that exists. MAK: Tim Mak, NPR News, Washington. AILSA CHANG, HOST:  Americans have a tendency to trust local news sources. Maybe you're listening to your own local NPR station right now. The Russian government's misinformation effort during the 2016 election, an effort that continues today, sought to take advantage of that trust. NPR's Tim Mak has this story. TIM MAK, BYLINE: Much of what we now understand to be the Russian government's effort to interfere with the American elections took place in a building in St. Petersburg called the Internet Research Agency. Here's Deputy Attorney General Rod Rosenstein announcing special counsel Mueller's charges against the Internet Research Agency and others in February. (SOUNDBITE OF ARCHIVED RECORDING) ROD ROSENSTEIN: The defendants allegedly conducted what they called information warfare against the United States with the stated goal of spreading distrust towards the candidates and the political system in general. MAK: In 2017 and then again this past May, Twitter provided the House Intelligence Committee with lists of thousands of accounts that they identified as being linked to the Internet Research Agency. Hidden among these accounts was a group of Twitter accounts that appeared to be fake local news outlets. NPR has counted 48 of them. They have names like El Paso Top News, Milwaukee Voice, Camden City News and Seattle Post. BRET SCHAFER: A not insignificant amount of those had some sort of variation on what appeared to be a homegrown local news site - so Chicago Daily News, which actually was a newspaper but went bust in the '70s - but just things like Baltimore Breaking News. MAK: That's Bret Schafer, a social media analyst for the Alliance for Securing Democracy, which tracks Russian influence operations. Let's take one example. The Internet Research Agency created an account that looks like it is the Chicago Daily News. The account was created back in May 2014 and for years just tweeted local headlines, accumulating 19,000 followers by July 2016. Here's the thing. These accounts never spread misinformation. They just tweeted out real local news, serving as sleeper accounts building trust and readership for some future unforeseen effort. Here's Schafer again. SCHAFER: They set them up for a reason, and if at any given moment they wanted to operationalize this sort of network of what seemed to be local American news handles, they can significantly influence the narrative on a breaking news story. MAK: Though Twitter caught these Internet Research Agency accounts in the act and suspended them, the fact that the Russian government created them in the first place gives us insight into their strategy, firstly that the Russian misinformation campaign was a years-long effort, one that wasn't simply focused on the 2016 election but on destabilizing the United States over an extended period of time. Here's Congressman Adam Schiff, the top Democrat on the House Intelligence Committee. ADAM SCHIFF: The Russians are playing a long game that they've developed a presence on social media, that they've created these fictitious persons and fictitious organizations that have built up over a period of time a certain trustworthiness among people that follow them. MAK: The failed effort to make local news accounts also tells us something about how Americans trust local news more than national news and how the Russians knew of that vulnerability. A Pew survey from 2016 found that 82 percent of Americans have some or a lot of confidence in local news organizations. Here's Tom Rosenstiel, the executive director of the American Press Institute. TOM ROSENSTIEL: If you were trying to pass along information that is not true but you want people to believe it, creating or inventing fake local news sources is an effective way of doing it because people will convey some trust to the locality even if the publication is one they've never seen before. MAK: The Russian misinformation campaign continues now. Here's Senator Susan Collins, a Republican on the Senate Intelligence Committee, which is currently investigating Russian interference. SUSAN COLLINS: This effort is not over. It continues to this very day where the Russians are trying to sow the seeds of discontent in our society, take advantage of the polarization that exists. MAK: Tim Mak, NPR News, Washington.", "section": "Politics", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-13-628118868": {"title": "Has Video Refereeing Ruined The World Cup? : NPR", "url": "https://www.npr.org/2018/07/13/628118868/has-video-refereeing-ruined-the-world-cup", "author": "No author found", "published_date": "2018-07-13", "content": "AILSA CHANG, HOST: France faces Croatia in the World Cup final on Sunday. It marks the end of a month when soccer fans have been glued to their screens. And so have the referees. This is the first World Cup where they've used video replay. NPR's Jasmine Garsd reports on the controversy that's caused. UNIDENTIFIED PERSON #1: (Foreign language spoken). JASMINE GARSD, BYLINE: Tensions were high Tuesday at a crowded bar in New York packed with sweating Belgian and French fans. UNIDENTIFIED PERSON #2: (Chanting in foreign language). UNIDENTIFIED PERSON #3: (Chanting in foreign language). GARSD: As raucous as soccer gets, it's also a game of charades used to communicate with the referee. Players hold up invisible infraction cards to ask that an opponent be penalized. The ref points at his eyes to acknowledge a misbehaving player. And this year, a new gesture has been added. A box drawn in the air means let's check the video replay. UNIDENTIFIED PERSON #4: Let's go, Belgium. UNIDENTIFIED PEOPLE: Let's go. Let's go. GARSD: Fans of basketball and football are used to the referee stopping the game to consult with the video replay. But soccer purists say it's ruining everything. MARTIN ROGERS: I remember back in the day when if a game kicked off at 3 o'clock in the afternoon, you'd be all wrapped up by 4:45. GARSD: Martin Rogers is a sports columnist for USA Today. He says the overuse of video assistant referee, or VAR as it's called in soccer, is making the matches drag on. Rogers says video replay works for football and basketball. ROGERS: When you look at the calls that are used for replay in basketball, for example, it's normally factual. It's based on, did a player get a shot off before the clock expired? It's easy. You know. It's black and white. But soccer is such a subjective game. GARSD: He's referring to one of the most hated and beloved qualities of soccer - the endless drama. Take a player like Brazil's Neymar, the Meryl Streep of soccer. He dives spectacularly when a player merely brushes him, agonizes on the ground, hoping to get the ref to call a foul on the opposing team. Rogers says with performances like that, a human referee is more effective than video. ROGERS: You can see that there's been contact, but the video doesn't show how hard that contact is, especially in slow motion. So it's really, really difficult to tell. GARSD: But FIFA, soccer's governing body, says video reviews are close to perfection. That's a shift from some of the egregious wrong calls made in soccer games in the last decade or so, mistakes that went viral on social media. Chris Bowerbank hosts the soccer podcast Across the Pond. CHRIS BOWERBANK: I think it was just a matter of time before FIFA looked at other sports and how video technology has been used and trying to bring it into this game. GARSD: Bowerbank thinks video replay may have had a positive impact on this World Cup. Players are behaving better. BOWERBANK: There have been zero foul play or violent red cards in this World Cup. And part of it might be that, you know, there is a camera on people now at all times. GARSD: Back at the crowded pub in Manhattan, the end is near. France is winning 1-0. It's a cautious game, and the ref never consults with VAR. Outside, I run into Kenneth Coremans from Belgium smoking a cigarette with his friend. He's been crying. Did you feel like at any point they should have used video referees for this? KENNETH COREMANS: Yes. UNIDENTIFIED PERSON #5: Yes. COREMANS: Yes, definitely. GARSD: As they explain, two French fans walk by and taunt them. COREMANS: Good moments for us to take that free kick that's. . . UNIDENTIFIED PERSON #6: (Foreign language spoken). COREMANS: Yeah. And there. . . UNIDENTIFIED PERSON #7: (Foreign language spoken). COREMANS: There's the arrogance. GARSD: That's the thing about soccer. It can be really cruel. And that's the thing about using new technology. It can be annoying until you need it on your side. Jasmine Garsd, NPR News, New York. AILSA CHANG, HOST:  France faces Croatia in the World Cup final on Sunday. It marks the end of a month when soccer fans have been glued to their screens. And so have the referees. This is the first World Cup where they've used video replay. NPR's Jasmine Garsd reports on the controversy that's caused. UNIDENTIFIED PERSON #1: (Foreign language spoken). JASMINE GARSD, BYLINE: Tensions were high Tuesday at a crowded bar in New York packed with sweating Belgian and French fans. UNIDENTIFIED PERSON #2: (Chanting in foreign language). UNIDENTIFIED PERSON #3: (Chanting in foreign language). GARSD: As raucous as soccer gets, it's also a game of charades used to communicate with the referee. Players hold up invisible infraction cards to ask that an opponent be penalized. The ref points at his eyes to acknowledge a misbehaving player. And this year, a new gesture has been added. A box drawn in the air means let's check the video replay. UNIDENTIFIED PERSON #4: Let's go, Belgium. UNIDENTIFIED PEOPLE: Let's go. Let's go. GARSD: Fans of basketball and football are used to the referee stopping the game to consult with the video replay. But soccer purists say it's ruining everything. MARTIN ROGERS: I remember back in the day when if a game kicked off at 3 o'clock in the afternoon, you'd be all wrapped up by 4:45. GARSD: Martin Rogers is a sports columnist for USA Today. He says the overuse of video assistant referee, or VAR as it's called in soccer, is making the matches drag on. Rogers says video replay works for football and basketball. ROGERS: When you look at the calls that are used for replay in basketball, for example, it's normally factual. It's based on, did a player get a shot off before the clock expired? It's easy. You know. It's black and white. But soccer is such a subjective game. GARSD: He's referring to one of the most hated and beloved qualities of soccer - the endless drama. Take a player like Brazil's Neymar, the Meryl Streep of soccer. He dives spectacularly when a player merely brushes him, agonizes on the ground, hoping to get the ref to call a foul on the opposing team. Rogers says with performances like that, a human referee is more effective than video. ROGERS: You can see that there's been contact, but the video doesn't show how hard that contact is, especially in slow motion. So it's really, really difficult to tell. GARSD: But FIFA, soccer's governing body, says video reviews are close to perfection. That's a shift from some of the egregious wrong calls made in soccer games in the last decade or so, mistakes that went viral on social media. Chris Bowerbank hosts the soccer podcast Across the Pond. CHRIS BOWERBANK: I think it was just a matter of time before FIFA looked at other sports and how video technology has been used and trying to bring it into this game. GARSD: Bowerbank thinks video replay may have had a positive impact on this World Cup. Players are behaving better. BOWERBANK: There have been zero foul play or violent red cards in this World Cup. And part of it might be that, you know, there is a camera on people now at all times. GARSD: Back at the crowded pub in Manhattan, the end is near. France is winning 1-0. It's a cautious game, and the ref never consults with VAR. Outside, I run into Kenneth Coremans from Belgium smoking a cigarette with his friend. He's been crying. Did you feel like at any point they should have used video referees for this? KENNETH COREMANS: Yes. UNIDENTIFIED PERSON #5: Yes. COREMANS: Yes, definitely. GARSD: As they explain, two French fans walk by and taunt them. COREMANS: Good moments for us to take that free kick that's. . . UNIDENTIFIED PERSON #6: (Foreign language spoken). COREMANS: Yeah. And there. . . UNIDENTIFIED PERSON #7: (Foreign language spoken). COREMANS: There's the arrogance. GARSD: That's the thing about soccer. It can be really cruel. And that's the thing about using new technology. It can be annoying until you need it on your side. Jasmine Garsd, NPR News, New York.", "section": "World Cup", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-14-628765208": {"title": "Tech Workers Demand CEOs Stop Doing Business With ICE, Other U.S. Agencies : NPR", "url": "https://www.npr.org/2018/07/14/628765208/tech-workers-demand-ceos-stop-doing-business-with-ice-other-u-s-agencies", "author": "No author found", "published_date": "2018-07-14", "content": "RENEE MONTAGNE, HOST: Tech workers from Salesforce, Microsoft, Amazon and Google are putting pressure on their CEOs to cut ties and end contracts with some U. S. government agencies. It's rare for employees to tell their bosses to turn away business. But as NPR's Laura Sydell reports, there's a growing concern among tech workers that the cutting-edge tools they create can be used in immoral ways. LAURA SYDELL, BYLINE: There's always been a stream of idealism at the major tech companies. Google started with the motto don't be evil. Facebook's mission statement says it's committed to bringing the world closer together. The CEO of Salesforce, which builds software for businesses, loves to tout its values. Here's Marc Benioff at the company's annual conference. (SOUNDBITE OF ARCHIVED RECORDING)MARC BENIOFF: We've fought together - whether it's for LGBTQ equality across the country, that's been so important for us. We're fighting for women and gender equality and gender pay equity for women all over. . . SYDELL: Now the outspoken CEO is caught up in the nation's debate about immigration. Earlier this week, a handful of protesters outside of the Salesforce tower in downtown San Francisco called on the company to live up to its values. (SOUNDBITE OF PROTEST)UNIDENTIFIED PROTESTERS: (Chanting) Hey, hey, ho, ho, CBP has got to go. SYDELL: The protesters are here to support the more than 650 Salesforce employees who signed a petition asking CEO Benioff to end Salesforce's software contract with the U. S. Customs and Border Protection, or CBP. They're upset over its role in the treatment of migrant families and their children. So far, Salesforce is keeping the contract to supply software to manage human resources. In a tweet, CEO Benioff said the company doesn't work with CBP to separate families. He agrees separating families is wrong. But that's not enough for tech workers like Kevin Ortiz. KEVIN ORTIZ: Salesforce has the obligation both morally and ethically to actually stop all practice with CBP, regardless of what the relationship is, in order to cripple CBP. SYDELL: Whether Salesforce has a moral obligation may be debatable to some, but tech employees say they are proud of the values these companies promote. In fact, the demonstrators came from a variety of tech companies. And while they are speaking out, many like this programmer at a large company didn't want to use their names because their employers might retaliate. UNIDENTIFIED PERSON: This mantra of, oh, yeah, we actually deeply listen and care about our employees is finally starting to crumble because people are seeing the industry for what it really is, which is just like any other industry that's focused on making money and enriching their shareholders and CEOs. SYDELL: The Salesforce protest is part of a growing movement among tech workers. Employees at Google successfully pushed for an end to a contract with the Pentagon, which was going to use its technology for more targeted drone strikes. Amazon employees want the company to cancel contracts with law enforcement for facial recognition technology. At Microsoft, some programmers have threatened to quit rather than build software for ICE. Jennifer Chatman, a professor at Haas School of Business University of California Berkeley, says it's almost unheard of for workers to ask their employer to stop doing business with a particular client. But she thinks there's a reason this is happening at tech companies. JENNIFER CHATMAN: The organizations encourage responsibility by having generally flatter hierarchies. They encourage people to challenge and debate. They encourage people to test the status quo. SYDELL: Todd Gitlin, a professor of sociology and communications at Columbia University, says there is a precedent for scientists and engineers refusing to work with the government on technology. After the U. S. used the atomic bomb in Hiroshima and Nagasaki, many of the scientists who worked on it were horrified. TODD GITLIN: And one of them was Robert Oppenheimer, who actually was deprived of his security clearance because he didn't want to work on the hydrogen bomb. SYDELL: Oppenheimer is known as the father of the atomic bomb. And while most big tech companies aren't in the business of building bombs, many like this anonymous programmer do see how the technologies they build are a double-edged sword. Social media can connect people and it can spread fake news. Facial recognition can catch criminals or be used by authoritarians to track their citizens. UNIDENTIFIED PERSON: We have the power to stand up and say we won't do that. That's not the future that we want to see. We want to show that there's a different future possible where tech isn't used for human rights abuse. SYDELL: And these workers hope that this is the beginning of employees speaking up to their bosses about how the technology they build gets used. Laura Sydell, NPR News, San Francisco. MONTAGNE: And we should note Salesforce is a financial supporter of NPR. RENEE MONTAGNE, HOST:  Tech workers from Salesforce, Microsoft, Amazon and Google are putting pressure on their CEOs to cut ties and end contracts with some U. S. government agencies. It's rare for employees to tell their bosses to turn away business. But as NPR's Laura Sydell reports, there's a growing concern among tech workers that the cutting-edge tools they create can be used in immoral ways. LAURA SYDELL, BYLINE: There's always been a stream of idealism at the major tech companies. Google started with the motto don't be evil. Facebook's mission statement says it's committed to bringing the world closer together. The CEO of Salesforce, which builds software for businesses, loves to tout its values. Here's Marc Benioff at the company's annual conference. (SOUNDBITE OF ARCHIVED RECORDING) MARC BENIOFF: We've fought together - whether it's for LGBTQ equality across the country, that's been so important for us. We're fighting for women and gender equality and gender pay equity for women all over. . . SYDELL: Now the outspoken CEO is caught up in the nation's debate about immigration. Earlier this week, a handful of protesters outside of the Salesforce tower in downtown San Francisco called on the company to live up to its values. (SOUNDBITE OF PROTEST) UNIDENTIFIED PROTESTERS: (Chanting) Hey, hey, ho, ho, CBP has got to go. SYDELL: The protesters are here to support the more than 650 Salesforce employees who signed a petition asking CEO Benioff to end Salesforce's software contract with the U. S. Customs and Border Protection, or CBP. They're upset over its role in the treatment of migrant families and their children. So far, Salesforce is keeping the contract to supply software to manage human resources. In a tweet, CEO Benioff said the company doesn't work with CBP to separate families. He agrees separating families is wrong. But that's not enough for tech workers like Kevin Ortiz. KEVIN ORTIZ: Salesforce has the obligation both morally and ethically to actually stop all practice with CBP, regardless of what the relationship is, in order to cripple CBP. SYDELL: Whether Salesforce has a moral obligation may be debatable to some, but tech employees say they are proud of the values these companies promote. In fact, the demonstrators came from a variety of tech companies. And while they are speaking out, many like this programmer at a large company didn't want to use their names because their employers might retaliate. UNIDENTIFIED PERSON: This mantra of, oh, yeah, we actually deeply listen and care about our employees is finally starting to crumble because people are seeing the industry for what it really is, which is just like any other industry that's focused on making money and enriching their shareholders and CEOs. SYDELL: The Salesforce protest is part of a growing movement among tech workers. Employees at Google successfully pushed for an end to a contract with the Pentagon, which was going to use its technology for more targeted drone strikes. Amazon employees want the company to cancel contracts with law enforcement for facial recognition technology. At Microsoft, some programmers have threatened to quit rather than build software for ICE. Jennifer Chatman, a professor at Haas School of Business University of California Berkeley, says it's almost unheard of for workers to ask their employer to stop doing business with a particular client. But she thinks there's a reason this is happening at tech companies. JENNIFER CHATMAN: The organizations encourage responsibility by having generally flatter hierarchies. They encourage people to challenge and debate. They encourage people to test the status quo. SYDELL: Todd Gitlin, a professor of sociology and communications at Columbia University, says there is a precedent for scientists and engineers refusing to work with the government on technology. After the U. S. used the atomic bomb in Hiroshima and Nagasaki, many of the scientists who worked on it were horrified. TODD GITLIN: And one of them was Robert Oppenheimer, who actually was deprived of his security clearance because he didn't want to work on the hydrogen bomb. SYDELL: Oppenheimer is known as the father of the atomic bomb. And while most big tech companies aren't in the business of building bombs, many like this anonymous programmer do see how the technologies they build are a double-edged sword. Social media can connect people and it can spread fake news. Facial recognition can catch criminals or be used by authoritarians to track their citizens. UNIDENTIFIED PERSON: We have the power to stand up and say we won't do that. That's not the future that we want to see. We want to show that there's a different future possible where tech isn't used for human rights abuse. SYDELL: And these workers hope that this is the beginning of employees speaking up to their bosses about how the technology they build gets used. Laura Sydell, NPR News, San Francisco. MONTAGNE: And we should note Salesforce is a financial supporter of NPR.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-16-627254166": {"title": "Rising Seas Could Cause Problems For Internet Infrastructure : NPR", "url": "https://www.npr.org/2018/07/16/627254166/rising-seas-could-cause-problems-for-internet-infrastructure", "author": "No author found", "published_date": "2018-07-16", "content": "", "section": "Science", "disclaimer": ""}, "2018-07-16-629519367": {"title": "FCC Chairman Pai Cites 'Serious Concerns' About Sinclair-Tribune Deal : NPR", "url": "https://www.npr.org/2018/07/16/629519367/fcc-chairman-pai-cites-serious-concerns-about-sinclair-tribune-deal", "author": "No author found", "published_date": "2018-07-16", "content": "", "section": "Media", "disclaimer": ""}, "2018-07-17-629705445": {"title": "Netflix Falls Short On Subscriber Target, Scares Investors : NPR", "url": "https://www.npr.org/2018/07/17/629705445/netflix-falls-short-on-subscriber-target-spooks-investors", "author": "No author found", "published_date": "2018-07-17", "content": "", "section": "Movies", "disclaimer": ""}, "2018-07-18-630146884": {"title": "Google AI Chiefs, Elon Musk Sign Pledge Against Autonomous Weapons : NPR", "url": "https://www.npr.org/2018/07/18/630146884/ai-innovators-take-pledge-against-autonomous-killer-weapons", "author": "No author found", "published_date": "2018-07-18", "content": "", "section": "Technology", "disclaimer": ""}, "2018-07-18-629527928": {"title": "Glass Has Ancient Origins But A High-Tech Future : NPR", "url": "https://www.npr.org/2018/07/18/629527928/glass-has-ancient-origins-but-a-high-tech-future", "author": "No author found", "published_date": "2018-07-18", "content": "ARI SHAPIRO, HOST: The future of how you interact with computers depends on a technology that's more than 3,000 years old. You use it every day on your smartphone, your TV, throughout your home, car, most likely at work. It's in the wires that bring you Internet service at near light speed. It is glass. NPR's Dustin Dwyer reports on why some people believe the impact of glass on our lives is only getting started. DUSTIN DWYER, BYLINE: Karol Wight meets me in the lobby of the Corning Museum of Glass in Corning, N. Y. , and leads me to a gallery room full of light and sculptured glass. She has a Ph. D. in art history. She studied the ancient origins of glass. And she tells me how she first got interested in a material so many people overlook. KAROL WIGHT: And the more I did the research and the more I understood that glass is a very ancient material, I just fell in love with it and never turned back. DWYER: Now Wight leads the Corning Museum of Glass, which is about a lot more than art. The museum was started by Corning the company. You might know it from the cookware it developed years ago, CorningWare and Pyrex. But Corning has been at the center of a lot of important products. It made the glass for Thomas Edison's light bulbs. It made glass for the cathode ray tubes in TVs. It made the glass for mirrors in the Hubble telescope. And more recently, it's Corning's glass that made the iPhone touch screen possible. And for the past few years, Corning has been trying to push a new idea - that glass isn't just an important product; it's actually the defining material of our time. Corning calls it the glass age. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: Yes, this is the glass age. But it's only just begun. Its potential is barely tapped. DWYER: And what's next in this glass age? Touch screens everywhere - your walls, your car, the mirror, in the dressing room at the mall, windows that can be programmed to let in exactly the amount of light that you want and more fiber optic cables, which aren't wires. They're actually made up of extremely thin strands of glass. And it's not just folks in Corning, N. Y. , who believe glass has a lot of potential for the future. Chris Pickett is CEO of a Silicon Valley-based company called DigiLens. He wants to change the way information reaches your eyes. CHRIS PICKETT: So we make thin, transparent, holographic waveguide displays. DWYER: Like a computer screen but see-through, just like in a sci-fi movie. DigiLens already has displays in jets. It's working on a new kind of heads-up display for cars that would turn a driver's windshield into a screen. PICKETT: And they see that information. And it doesn't look like it's reflecting off the windshield. It actually looks like it's out at some distance. DWYER: Pickett says the idea is to make the images appear as if they're in the real world so there's less distraction to look away from the road. DigiLens is also one of a number of companies that's trying to use glass to do the same augmented reality approach for actual glasses that people can wear. Think Google Glass but hopefully better this time around. PICKETT: It's going to fundamentally change the way people see and interact with data and images. And that is going to be huge. DWYER: So what's standing in the way of glass? Well, for one, no one's really sure those weird-looking augmented reality glasses will take off. And those other futuristic ideas for glass - they need some work, too. And there's another rival on the scene, one with some admirable traits - plastic. Plastic bends. It doesn't break like glass. It also isn't as clear for displays, and it can be scratched. And glass has been around a lot longer. And it has some big supporters looking to the future. Dustin Dwyer, NPR News. (SOUNDBITE OF SIRIUSMO'S \"NIGHTS OFF\") ARI SHAPIRO, HOST:  The future of how you interact with computers depends on a technology that's more than 3,000 years old. You use it every day on your smartphone, your TV, throughout your home, car, most likely at work. It's in the wires that bring you Internet service at near light speed. It is glass. NPR's Dustin Dwyer reports on why some people believe the impact of glass on our lives is only getting started. DUSTIN DWYER, BYLINE: Karol Wight meets me in the lobby of the Corning Museum of Glass in Corning, N. Y. , and leads me to a gallery room full of light and sculptured glass. She has a Ph. D. in art history. She studied the ancient origins of glass. And she tells me how she first got interested in a material so many people overlook. KAROL WIGHT: And the more I did the research and the more I understood that glass is a very ancient material, I just fell in love with it and never turned back. DWYER: Now Wight leads the Corning Museum of Glass, which is about a lot more than art. The museum was started by Corning the company. You might know it from the cookware it developed years ago, CorningWare and Pyrex. But Corning has been at the center of a lot of important products. It made the glass for Thomas Edison's light bulbs. It made glass for the cathode ray tubes in TVs. It made the glass for mirrors in the Hubble telescope. And more recently, it's Corning's glass that made the iPhone touch screen possible. And for the past few years, Corning has been trying to push a new idea - that glass isn't just an important product; it's actually the defining material of our time. Corning calls it the glass age. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: Yes, this is the glass age. But it's only just begun. Its potential is barely tapped. DWYER: And what's next in this glass age? Touch screens everywhere - your walls, your car, the mirror, in the dressing room at the mall, windows that can be programmed to let in exactly the amount of light that you want and more fiber optic cables, which aren't wires. They're actually made up of extremely thin strands of glass. And it's not just folks in Corning, N. Y. , who believe glass has a lot of potential for the future. Chris Pickett is CEO of a Silicon Valley-based company called DigiLens. He wants to change the way information reaches your eyes. CHRIS PICKETT: So we make thin, transparent, holographic waveguide displays. DWYER: Like a computer screen but see-through, just like in a sci-fi movie. DigiLens already has displays in jets. It's working on a new kind of heads-up display for cars that would turn a driver's windshield into a screen. PICKETT: And they see that information. And it doesn't look like it's reflecting off the windshield. It actually looks like it's out at some distance. DWYER: Pickett says the idea is to make the images appear as if they're in the real world so there's less distraction to look away from the road. DigiLens is also one of a number of companies that's trying to use glass to do the same augmented reality approach for actual glasses that people can wear. Think Google Glass but hopefully better this time around. PICKETT: It's going to fundamentally change the way people see and interact with data and images. And that is going to be huge. DWYER: So what's standing in the way of glass? Well, for one, no one's really sure those weird-looking augmented reality glasses will take off. And those other futuristic ideas for glass - they need some work, too. And there's another rival on the scene, one with some admirable traits - plastic. Plastic bends. It doesn't break like glass. It also isn't as clear for displays, and it can be scratched. And glass has been around a lot longer. And it has some big supporters looking to the future. Dustin Dwyer, NPR News. (SOUNDBITE OF SIRIUSMO'S \"NIGHTS OFF\")", "section": "Business", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-18-630130125": {"title": "Elon Musk Apologizes To Diver In Cave Rescue For Words 'Spoken In Anger' : NPR", "url": "https://www.npr.org/2018/07/18/630130125/elon-musk-apologizes-to-diver-in-cave-rescue-for-words-spoken-in-anger", "author": "No author found", "published_date": "2018-07-18", "content": "", "section": "Technology", "disclaimer": ""}, "2018-07-18-629731693": {"title": "Viral WhatsApp Messages Are Triggering Mob Killings In India : NPR", "url": "https://www.npr.org/2018/07/18/629731693/fake-news-turns-deadly-in-india", "author": "No author found", "published_date": "2018-07-18", "content": "", "section": "World", "disclaimer": ""}, "2018-07-18-630030673": {"title": "EU Hits Google With $5 Billion Fine For Pushing Apps On Android Users : NPR", "url": "https://www.npr.org/2018/07/18/630030673/eu-hits-google-with-5-billion-fine-for-pushing-apps-on-android-users", "author": "No author found", "published_date": "2018-07-18", "content": "", "section": "Business", "disclaimer": ""}, "2018-07-18-629760717": {"title": "Put Your Face In It: How Gaming Helped Me Understand My Dog : NPR", "url": "https://www.npr.org/2018/07/18/629760717/put-your-face-in-it-how-gaming-helped-me-understand-my-dog", "author": "No author found", "published_date": "2018-07-18", "content": "", "section": "Pop Culture Happy Hour", "disclaimer": ""}, "2018-07-19-630589365": {"title": "Fresh Facebook Controversy: Zuckerberg Defends Rights Of Holocaust Deniers : NPR", "url": "https://www.npr.org/2018/07/19/630589365/fresh-facebook-controversy-zuckerberg-defends-rights-of-holocaust-deniers", "author": "No author found", "published_date": "2018-07-19", "content": "ARI SHAPIRO, HOST: There's a fresh controversy for Facebook. Yesterday in an interview with the podcast Recode, Facebook CEO Mark Zuckerberg said Holocaust deniers should be allowed to express their opinion on the social media platform. (SOUNDBITE OF PODCAST, \"RECODE\")MARK ZUCKERBERG: At the end of the day, I don't believe that our platform should take that down because I think that there are things that different people get wrong. Either - I don't think that they're intentionally getting it wrong. SHAPIRO: That statement comes amidst mounting concerns about Facebook's inability to weed out misinformation. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: Facebook has often stated its mission to create community and expose people to new ideas. The problem is some of those ideas can be deeply offensive. DAVID SHIH: You have to tolerate the hate speech if you're going to get past it. What if that doesn't happen? GARSD: Professor David Shih teaches literature at the University of Wisconsin-Eau Claire. He says it's a risky proposition. SHIH: And if that doesn't happen, what you have is this preponderance of hate speech put out into the public that causes injury and harm to targeted groups. GARSD: Take the case of Myanmar. Last year during the Rohingya crisis, there were rampant insults against Muslims on social media. The United Nations then accused Facebook of contributing to the violence. Facebook has since pledged to remove posts intended to promote physical harm. Intention is a key word for Zuckerberg - the difference between what offends us and what is intended to cause violence. ROBERT SHIBLEY: The social media companies have largely taken on the characteristics of a public square. And one of the responsibilities that we expect of those who guard the public square is to allow a wide diversity of views. GARSD: Robert Shibley is the president of the Foundation for Individual Rights in Education, a nonprofit for free speech. SHIBLEY: Well, I think it's human nature for people to feel uncomfortable when they hear things that they find objectionable or offensive or repugnant. You know, the normal reaction to that throughout all of history is to try to get that person to stop doing it. But unfortunately, it's not one that is particularly compatible with having a free, liberal democracy. GARSD: Sticks and stones can break your bones, but Facebook posts can never hurt you. Professor David Shih, who teaches Asian literature, says for minorities that have been historically targeted, that's far from true. SHIH: Who gets to decide that violence may or may not follow that speech? Those of us who have minority identities all know that we might find ourselves in a situation where somebody uses language directed at us, and we deeply fear that violence will follow. GARSD: Since his comments aired, Zuckerberg has clarified that he does not defend Holocaust deniers and Facebook remains committed to stopping misinformation. Jasmine Garsd, NPR News, New York. (SOUNDBITE OF MUSIC) ARI SHAPIRO, HOST:  There's a fresh controversy for Facebook. Yesterday in an interview with the podcast Recode, Facebook CEO Mark Zuckerberg said Holocaust deniers should be allowed to express their opinion on the social media platform. (SOUNDBITE OF PODCAST, \"RECODE\") MARK ZUCKERBERG: At the end of the day, I don't believe that our platform should take that down because I think that there are things that different people get wrong. Either - I don't think that they're intentionally getting it wrong. SHAPIRO: That statement comes amidst mounting concerns about Facebook's inability to weed out misinformation. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: Facebook has often stated its mission to create community and expose people to new ideas. The problem is some of those ideas can be deeply offensive. DAVID SHIH: You have to tolerate the hate speech if you're going to get past it. What if that doesn't happen? GARSD: Professor David Shih teaches literature at the University of Wisconsin-Eau Claire. He says it's a risky proposition. SHIH: And if that doesn't happen, what you have is this preponderance of hate speech put out into the public that causes injury and harm to targeted groups. GARSD: Take the case of Myanmar. Last year during the Rohingya crisis, there were rampant insults against Muslims on social media. The United Nations then accused Facebook of contributing to the violence. Facebook has since pledged to remove posts intended to promote physical harm. Intention is a key word for Zuckerberg - the difference between what offends us and what is intended to cause violence. ROBERT SHIBLEY: The social media companies have largely taken on the characteristics of a public square. And one of the responsibilities that we expect of those who guard the public square is to allow a wide diversity of views. GARSD: Robert Shibley is the president of the Foundation for Individual Rights in Education, a nonprofit for free speech. SHIBLEY: Well, I think it's human nature for people to feel uncomfortable when they hear things that they find objectionable or offensive or repugnant. You know, the normal reaction to that throughout all of history is to try to get that person to stop doing it. But unfortunately, it's not one that is particularly compatible with having a free, liberal democracy. GARSD: Sticks and stones can break your bones, but Facebook posts can never hurt you. Professor David Shih, who teaches Asian literature, says for minorities that have been historically targeted, that's far from true. SHIH: Who gets to decide that violence may or may not follow that speech? Those of us who have minority identities all know that we might find ourselves in a situation where somebody uses language directed at us, and we deeply fear that violence will follow. GARSD: Since his comments aired, Zuckerberg has clarified that he does not defend Holocaust deniers and Facebook remains committed to stopping misinformation. Jasmine Garsd, NPR News, New York. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-19-630486140": {"title": "Zuckerberg Looks To Clear Up Stance On Facebook, Fake News And The Holocaust : NPR", "url": "https://www.npr.org/2018/07/19/630486140/zuckerberg-looks-to-clear-up-stance-on-facebook-fake-news-and-the-holocaust", "author": "No author found", "published_date": "2018-07-19", "content": "", "section": "Media", "disclaimer": ""}, "2018-07-19-630358800": {"title": "Immigrant Rights Group Reject $250,000 From Salesforce Over Border Patrol Ties : NPR", "url": "https://www.npr.org/2018/07/19/630358800/immigrant-rights-group-turns-down-250-000-from-tech-firm-over-ties-to-border-pat", "author": "No author found", "published_date": "2018-07-19", "content": "", "section": "Business", "disclaimer": ""}, "2018-07-20-630588925": {"title": "Week Of Trump Reversals Puts 2018 Election Security In The Spotlight : NPR", "url": "https://www.npr.org/2018/07/20/630588925/week-of-trump-reversals-puts-2018-election-security-in-the-spotlight", "author": "No author found", "published_date": "2018-07-20", "content": "NOEL KING, HOST: The midterm elections are four months away. And there are real concerns about election interference. Yesterday, at the Aspen Security Forum, Director of National Intelligence Dan Coats talked about the threat that Russia poses. (SOUNDBITE OF ARCHIVED RECORDING)DAN COATS: I think we have to be relentless in terms of calling out the Russians for what they've done. We have to be vigilant in terms of putting steps in place to make sure it doesn't happen again. KING: I talked with NPR reporter Miles Parks about whether there's more security around these upcoming elections than there was around the presidential election in 2016. MILES PARKS, BYLINE: When you think about how Russians operated this cyberattack, basically, a lot of it was what's called spear phishing - targeted emails to try and get passwords from people. Election officials were not thinking like targets before the summer of 2016. And there was a lot of clicking on emails like that. Now they're thinking like targets. And that kind of changes the game in a lot of different ways. But now that caveat - the technology that we're actually using to vote - that has not really changed in the past two years. One study found that 41 states will use equipment to vote in this upcoming midterm election that's more than a decade old. KING: Ten years old. PARKS: Yeah, exactly. Earlier this week, I asked Senator Marco Rubio how confident he is in America's voting system. And here's what he told me. MARCO RUBIO: I'm confident about America's election system. But I'm equally confident about the determination and the capability of Russian intelligence to interfere in ways that most people don't think about. It's not about changing votes, necessarily. KING: When Rubio says it's not about changing votes, what does he mean? What is he saying that he is worried about? PARKS: What he's talking about is voter confidence. Basically, this scenario does not involve actually affecting vote tallies. What it involves is going into voter registration systems, changing where people are supposed to cast their ballots, breaking into election websites that are supposed to show the winners and then showing losers instead. And basically, that sews chaos within the voting public without ever affecting or changing a vote. KING: What is the government doing to fix this or to at least improve it? PARKS: Right, so Congress did allocate $380 million this year to election security, which is a big deal. But it's important to realize that money in context. The state of California got more money than that for the 2000 elections to overhaul just their voting infrastructure. I don't want to say it's a drop in the bucket, but it's not enough to actually affect the hardware that people are voting on. It's going to go towards trainings and software improvements and things like that. What's unclear is whether there's more money coming down the road. House Democrats released a report earlier this month that said it would cost about $1. 4 billion over the next 10 years to actually get America's voting infrastructure up to where we need it to be. KING: And so when President Trump sort of veers back and forth on the extent to which Russia interfered in the 2016 election, as we've seen him do this week, what effect does that have on the government's ability to get the money out there and to get the job done? PARKS: Right. I think it definitely doesn't help, so the National Association of Secretaries of State actually released a statement after that remarkable press conference in Helsinki earlier this week. And they asked the White House to, quote, \"provide clear and accurate statements going forward. \" And then when you look at the fact that it's been academic groups and private sector groups who've actually made a lot of strides in training election workers and thinking about this issue in - on the big picture, it kind of shows that the government has had trouble making this a priority. KING: NPR's Miles Parks. Thank you so much, Miles. PARKS: Thank you. NOEL KING, HOST:  The midterm elections are four months away. And there are real concerns about election interference. Yesterday, at the Aspen Security Forum, Director of National Intelligence Dan Coats talked about the threat that Russia poses. (SOUNDBITE OF ARCHIVED RECORDING) DAN COATS: I think we have to be relentless in terms of calling out the Russians for what they've done. We have to be vigilant in terms of putting steps in place to make sure it doesn't happen again. KING: I talked with NPR reporter Miles Parks about whether there's more security around these upcoming elections than there was around the presidential election in 2016. MILES PARKS, BYLINE: When you think about how Russians operated this cyberattack, basically, a lot of it was what's called spear phishing - targeted emails to try and get passwords from people. Election officials were not thinking like targets before the summer of 2016. And there was a lot of clicking on emails like that. Now they're thinking like targets. And that kind of changes the game in a lot of different ways. But now that caveat - the technology that we're actually using to vote - that has not really changed in the past two years. One study found that 41 states will use equipment to vote in this upcoming midterm election that's more than a decade old. KING: Ten years old. PARKS: Yeah, exactly. Earlier this week, I asked Senator Marco Rubio how confident he is in America's voting system. And here's what he told me. MARCO RUBIO: I'm confident about America's election system. But I'm equally confident about the determination and the capability of Russian intelligence to interfere in ways that most people don't think about. It's not about changing votes, necessarily. KING: When Rubio says it's not about changing votes, what does he mean? What is he saying that he is worried about? PARKS: What he's talking about is voter confidence. Basically, this scenario does not involve actually affecting vote tallies. What it involves is going into voter registration systems, changing where people are supposed to cast their ballots, breaking into election websites that are supposed to show the winners and then showing losers instead. And basically, that sews chaos within the voting public without ever affecting or changing a vote. KING: What is the government doing to fix this or to at least improve it? PARKS: Right, so Congress did allocate $380 million this year to election security, which is a big deal. But it's important to realize that money in context. The state of California got more money than that for the 2000 elections to overhaul just their voting infrastructure. I don't want to say it's a drop in the bucket, but it's not enough to actually affect the hardware that people are voting on. It's going to go towards trainings and software improvements and things like that. What's unclear is whether there's more money coming down the road. House Democrats released a report earlier this month that said it would cost about $1. 4 billion over the next 10 years to actually get America's voting infrastructure up to where we need it to be. KING: And so when President Trump sort of veers back and forth on the extent to which Russia interfered in the 2016 election, as we've seen him do this week, what effect does that have on the government's ability to get the money out there and to get the job done? PARKS: Right. I think it definitely doesn't help, so the National Association of Secretaries of State actually released a statement after that remarkable press conference in Helsinki earlier this week. And they asked the White House to, quote, \"provide clear and accurate statements going forward. \" And then when you look at the fact that it's been academic groups and private sector groups who've actually made a lot of strides in training election workers and thinking about this issue in - on the big picture, it kind of shows that the government has had trouble making this a priority. KING: NPR's Miles Parks. Thank you so much, Miles. PARKS: Thank you.", "section": "Politics", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-22-631258411": {"title": "How A Drone Helped Rescue A Climber : NPR", "url": "https://www.npr.org/2018/07/22/631258411/how-a-drone-helped-rescue-a-climber", "author": "No author found", "published_date": "2018-07-22", "content": "KORVA COLEMAN, HOST: High-altitude searches are a risky business. When a climber goes missing in places like the Himalayas, the thin air, avalanches and hidden crevasses can mean danger for rescue climbers. So let us introduce the hero of our story, the DJI Mavic Pro drone. It was used earlier this month to help find Scottish mountaineer Rick Allen. He went missing after a solo attempt to set a new route on Pakistan's Broad Peak, sitting at over 26,000 feet. Other climbers thought he had died. It turns out Allen had survived a 100-foot fall and spent the next 36 hours hanging on. (SOUNDBITE OF MUSIC)COLEMAN: Luckily, another team on a nearby peak was using a drone to film their exploits. Soon, its cameras were switched from a vanity project to a search mission, and Rick Allen was spotted and rescued. It was quite the feat for the Mavic Pro. It's designed to work at a maximum altitude of a bit over 16,000 feet. And here it was around 10,000 feet higher. In the cold, its batteries could have conked out. Instead, because of the little drone that could, Rick Allen was rescued, suffering nothing more than a few cuts and bit of frostbite. KORVA COLEMAN, HOST:  High-altitude searches are a risky business. When a climber goes missing in places like the Himalayas, the thin air, avalanches and hidden crevasses can mean danger for rescue climbers. So let us introduce the hero of our story, the DJI Mavic Pro drone. It was used earlier this month to help find Scottish mountaineer Rick Allen. He went missing after a solo attempt to set a new route on Pakistan's Broad Peak, sitting at over 26,000 feet. Other climbers thought he had died. It turns out Allen had survived a 100-foot fall and spent the next 36 hours hanging on. (SOUNDBITE OF MUSIC) COLEMAN: Luckily, another team on a nearby peak was using a drone to film their exploits. Soon, its cameras were switched from a vanity project to a search mission, and Rick Allen was spotted and rescued. It was quite the feat for the Mavic Pro. It's designed to work at a maximum altitude of a bit over 16,000 feet. And here it was around 10,000 feet higher. In the cold, its batteries could have conked out. Instead, because of the little drone that could, Rick Allen was rescued, suffering nothing more than a few cuts and bit of frostbite.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-25-632257414": {"title": "Walmart To Test Waymo Self-Driving Cars For Grocery Pickup Service : NPR", "url": "https://www.npr.org/2018/07/25/632257414/walmart-to-test-self-driving-cars-for-grocery-pickup-service", "author": "No author found", "published_date": "2018-07-25", "content": "", "section": "Business", "disclaimer": ""}, "2018-07-26-632653239": {"title": "Facebook's Big Growth Is Slowing, Sending Its Stock Tumbling : NPR", "url": "https://www.npr.org/2018/07/26/632653239/facebooks-big-growth-is-slowing-sending-its-stock-tumbling", "author": "No author found", "published_date": "2018-07-26", "content": "", "section": "Business", "disclaimer": ""}, "2018-07-27-633090205": {"title": "Twitter Shares Fall, Ending A Hard Week For Social Media Stocks : NPR", "url": "https://www.npr.org/2018/07/27/633090205/twitter-shares-fall-ending-a-hard-week-for-social-media-stock", "author": "No author found", "published_date": "2018-07-27", "content": "", "section": "Technology", "disclaimer": ""}, "2018-07-27-632945197": {"title": "Virgin Galactic Space Plane Reaches New Heights In Test Flight : NPR", "url": "https://www.npr.org/2018/07/27/632945197/virgin-galactic-space-plane-reaches-new-heights-in-test-flight", "author": "No author found", "published_date": "2018-07-27", "content": "", "section": "Space", "disclaimer": ""}, "2018-07-28-631812255": {"title": "Scooters: Sidewalk Nuisances, Or The Future Of Local Transportation? : NPR", "url": "https://www.npr.org/2018/07/28/631812255/scooters-sidewalk-nuisances-or-the-future-of-public-transportation", "author": "No author found", "published_date": "2018-07-28", "content": "SCOTT SIMON, HOST: Electric scooters are suddenly popping up on sidewalks in major cities around the country. They've become just about as popular as BJ Leiderman, who writes our theme music. Companies like Uber and Google have recently invested millions, hoping that scooters could be an answer to spotty public transit and increasingly congested traffic. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: When Deputy City Attorney Adam Stephens walked into his office in Milwaukee one morning in late June, he found messages complaining about the Birds. He was not amused. He went for a walk. ADAM STEPHENS: Within a couple of minutes, I found one parked on the sidewalk and was able to visually examine it and kind of figure out what it was. GARSD: Bird is the name of an electric scooter company. Completely unannounced, it dropped off somewhere between 70 and a hundred rental scooters throughout Milwaukee, where it's illegal to ride them in public. Here's how it works. You download an app and locate a scooter near you. It's about a dollar to unlock and then 15 cents a minute. You can pretty much drop them off anywhere, and that's part of the problem. People have been leaving them all over the city sidewalks. STEPHENS: Well, that causes problems because you have elderly people, you have people with disabilities, you have the visually impaired who rely on seeing eye dogs. GARSD: In recent months, the #ScootersBehavingBadly popped up, featuring scooters around the country parked in pedestrian walkways, riders speeding through while wearing headphones. Milwaukee issued a cease-and-desist order, but Bird refused. The case is now in federal court. Things have gone sour in several cities, like San Francisco and St. Paul, where scooter companies have been kicked out. But in some cities, they've flourished, like in Washington, D. C. , where I took a scooter from the brand Skip out for a spin. It's kind of fun and a little terrifying. You get to weave through rush-hour traffic with a cool breeze blowing in your face. You don't arrive drenched in sweat. I bumped into a fellow rider Octavion Carter. He uses these to get around Howard University and gives me some advice. OCTAVION CARTER: Watch the ground because if you go over a crack or a pothole, you might fall. GARSD: It's happened to you? CARTER: Yes. It happens to everybody (laughter). GARSD: There are about 1,200 electric scooters for rent in Washington, D. C. These companies have a few months to prove their worth. Hi. hey. LUZ LAZO: How are you? GARSD: Nice to meet you. I meet Luz Lazo outside The Washington Post offices, where she reports on transportation. She says some people are annoyed at the trend. But also, in a city where public transportation is notoriously unreliable. . . LAZO: I see a lot of people on the scooters now. And really, a lot of people who are quite frustrated with other modes of transportation might say I want to give this a try. Whether you know, it's something that is going to last or be a success, I mean, we still have to wait and see. GARSD: Will the scooter, skateboard's goofy-looking cousin, be another fad, just like Segways or hoverboards? Big tech doesn't think so. Silicon Valley is betting on the future of microtransportation. Uber recently invested in the scooter company Lime, and Lyft has announced it will soon be offering scooters on its app. As I ride back through D. C. on my rental with the wind in my face, one thing becomes clear. It's fun, but there's no way I'm doing this in winter. Jasmine Garsd, NPR News, Washington. SCOTT SIMON, HOST:  Electric scooters are suddenly popping up on sidewalks in major cities around the country. They've become just about as popular as BJ Leiderman, who writes our theme music. Companies like Uber and Google have recently invested millions, hoping that scooters could be an answer to spotty public transit and increasingly congested traffic. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: When Deputy City Attorney Adam Stephens walked into his office in Milwaukee one morning in late June, he found messages complaining about the Birds. He was not amused. He went for a walk. ADAM STEPHENS: Within a couple of minutes, I found one parked on the sidewalk and was able to visually examine it and kind of figure out what it was. GARSD: Bird is the name of an electric scooter company. Completely unannounced, it dropped off somewhere between 70 and a hundred rental scooters throughout Milwaukee, where it's illegal to ride them in public. Here's how it works. You download an app and locate a scooter near you. It's about a dollar to unlock and then 15 cents a minute. You can pretty much drop them off anywhere, and that's part of the problem. People have been leaving them all over the city sidewalks. STEPHENS: Well, that causes problems because you have elderly people, you have people with disabilities, you have the visually impaired who rely on seeing eye dogs. GARSD: In recent months, the #ScootersBehavingBadly popped up, featuring scooters around the country parked in pedestrian walkways, riders speeding through while wearing headphones. Milwaukee issued a cease-and-desist order, but Bird refused. The case is now in federal court. Things have gone sour in several cities, like San Francisco and St. Paul, where scooter companies have been kicked out. But in some cities, they've flourished, like in Washington, D. C. , where I took a scooter from the brand Skip out for a spin. It's kind of fun and a little terrifying. You get to weave through rush-hour traffic with a cool breeze blowing in your face. You don't arrive drenched in sweat. I bumped into a fellow rider Octavion Carter. He uses these to get around Howard University and gives me some advice. OCTAVION CARTER: Watch the ground because if you go over a crack or a pothole, you might fall. GARSD: It's happened to you? CARTER: Yes. It happens to everybody (laughter). GARSD: There are about 1,200 electric scooters for rent in Washington, D. C. These companies have a few months to prove their worth. Hi. hey. LUZ LAZO: How are you? GARSD: Nice to meet you. I meet Luz Lazo outside The Washington Post offices, where she reports on transportation. She says some people are annoyed at the trend. But also, in a city where public transportation is notoriously unreliable. . . LAZO: I see a lot of people on the scooters now. And really, a lot of people who are quite frustrated with other modes of transportation might say I want to give this a try. Whether you know, it's something that is going to last or be a success, I mean, we still have to wait and see. GARSD: Will the scooter, skateboard's goofy-looking cousin, be another fad, just like Segways or hoverboards? Big tech doesn't think so. Silicon Valley is betting on the future of microtransportation. Uber recently invested in the scooter company Lime, and Lyft has announced it will soon be offering scooters on its app. As I ride back through D. C. on my rental with the wind in my face, one thing becomes clear. It's fun, but there's no way I'm doing this in winter. Jasmine Garsd, NPR News, Washington.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-28-633056819": {"title": "Russian Hackers Targeted The Most Vulnerable Part Of U.S. Elections. Again : NPR", "url": "https://www.npr.org/2018/07/28/633056819/russian-hackers-targeted-the-most-vulnerable-part-of-u-s-elections-again", "author": "No author found", "published_date": "2018-07-28", "content": "SCOTT SIMON, HOST: Russian hackers are going after U. S. elections again. Democratic Senator Claire McCaskill said this week her campaign was targeted by a phishing attack from Russia after that had been reported in The Daily Beast. The senator is up for re-election in November, in Missouri, a state that voted overwhelmingly for Donald Trump in 2016. And there's reason to believe other campaigns could be under attack as well. NPR's Miles Parks has more. MILES PARKS, BYLINE: Matt Rhoades ran Mitt Romney's 2012 presidential campaign. He says, campaigns are an easy target for Russia because despite what popular culture may say, they often aren't that organized. MATT RHOADES: The only thing that is actually consistent with the movies when it comes to campaigns is people eat a lot of pizza. And they're not that sophisticated. That's what makes our campaigns so thrilling and exciting, but it also makes them soft targets. PARKS: Rhoades helps lead a project at Harvard University that aims to help election officials and campaigns grapple with the new reality - that they now have a target on their backs. I sat down with him and Robby Mook, Hillary Clinton's 2016 campaign manager, at a conference Harvard hosted in the spring. Clinton was the target of a successful attack similar to the one McCaskill faced. Her campaign chairman, John Podesta, had his email account accessed by Russian operatives, who proceeded to publish reams of emails. Here's Mook. ROBBY MOOK: The irony of campaigns is they are the grittiest and least resourced startups that are out there, but they're incredibly valuable targets. PARKS: Campaigns often don't have the time or money to develop long-term security plans. And they're bringing in new staff all the time without training. Those staffers and sometimes volunteers may also be using their own equipment. In the case of the McCaskill attack, Russian operatives sent fake emails that were made to look like official notices to change a password. There's no indication the attack was successful. Eric Rosenbach leads the project at Harvard. He serves as the chief of staff for the Department of Defense and used to lead all aspects of the department's cyber activity. He says there's also no reason to believe McCaskill staff is alone, even if they're the first this election cycle to publicly state they've been targeted. ERIC ROSENBACH: The fact that you find one part of a Russian cyber intrusion or attack usually means that you've only found a very small part of it. They're just very sophisticated. So you always have to operate as if you've only found the beginning of what is probably a much more complex problem and situation. PARKS: Until the U. S. institutes an effective foreign policy to deter these sorts of attacks, Rosenbach says they'll continue. Miles Parks, NPR News, Washington. SCOTT SIMON, HOST:  Russian hackers are going after U. S. elections again. Democratic Senator Claire McCaskill said this week her campaign was targeted by a phishing attack from Russia after that had been reported in The Daily Beast. The senator is up for re-election in November, in Missouri, a state that voted overwhelmingly for Donald Trump in 2016. And there's reason to believe other campaigns could be under attack as well. NPR's Miles Parks has more. MILES PARKS, BYLINE: Matt Rhoades ran Mitt Romney's 2012 presidential campaign. He says, campaigns are an easy target for Russia because despite what popular culture may say, they often aren't that organized. MATT RHOADES: The only thing that is actually consistent with the movies when it comes to campaigns is people eat a lot of pizza. And they're not that sophisticated. That's what makes our campaigns so thrilling and exciting, but it also makes them soft targets. PARKS: Rhoades helps lead a project at Harvard University that aims to help election officials and campaigns grapple with the new reality - that they now have a target on their backs. I sat down with him and Robby Mook, Hillary Clinton's 2016 campaign manager, at a conference Harvard hosted in the spring. Clinton was the target of a successful attack similar to the one McCaskill faced. Her campaign chairman, John Podesta, had his email account accessed by Russian operatives, who proceeded to publish reams of emails. Here's Mook. ROBBY MOOK: The irony of campaigns is they are the grittiest and least resourced startups that are out there, but they're incredibly valuable targets. PARKS: Campaigns often don't have the time or money to develop long-term security plans. And they're bringing in new staff all the time without training. Those staffers and sometimes volunteers may also be using their own equipment. In the case of the McCaskill attack, Russian operatives sent fake emails that were made to look like official notices to change a password. There's no indication the attack was successful. Eric Rosenbach leads the project at Harvard. He serves as the chief of staff for the Department of Defense and used to lead all aspects of the department's cyber activity. He says there's also no reason to believe McCaskill staff is alone, even if they're the first this election cycle to publicly state they've been targeted. ERIC ROSENBACH: The fact that you find one part of a Russian cyber intrusion or attack usually means that you've only found a very small part of it. They're just very sophisticated. So you always have to operate as if you've only found the beginning of what is probably a much more complex problem and situation. PARKS: Until the U. S. institutes an effective foreign policy to deter these sorts of attacks, Rosenbach says they'll continue. Miles Parks, NPR News, Washington.", "section": "Politics", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-30-634177862": {"title": "Attorneys General Sue Trump Administration To Block 3D-Printed Guns  : NPR", "url": "https://www.npr.org/2018/07/30/634177862/attorneys-general-sue-trump-administration-to-block-3d-printed-guns", "author": "No author found", "published_date": "2018-07-30", "content": "", "section": "Law", "disclaimer": ""}, "2018-07-30-633933951": {"title": "Offensive Tweets Remind Major Leaguers That On Social Media, The Past Is Never Past : NPR", "url": "https://www.npr.org/2018/07/30/633933951/offensive-tweets-remind-major-leaguers-that-on-social-media-the-past-is-never-pa", "author": "No author found", "published_date": "2018-07-30", "content": "", "section": "Sports", "disclaimer": ""}, "2018-07-30-629800775": {"title": "Some Amazon Reviews Are Too Good To Be Believed. They're Paid For : NPR", "url": "https://www.npr.org/2018/07/30/629800775/some-amazon-reviews-are-too-good-to-be-believed-theyre-paid-for", "author": "No author found", "published_date": "2018-07-30", "content": "AILSA CHANG, HOST: Time now for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CHANG: Maybe this sounds familiar. You buy something online, and when it arrives, it's not quite what you expected. But the product had dozens of glowing reviews, like 5 out of 5 stars. Well, outside auditors say be careful. Unreliable reviews are all over e-commerce sites. They come from an underground economy where sellers pay for shining reviews. Reporter Ryan Kailath explains. RYAN KAILATH, BYLINE: A brand new iPhone charger, like, from the Apple store costs almost 40 bucks. But Amazon has hundreds of cheaper options. That's where I got a knockoff charger recently for a third the price. Hey, UPS. Plus free two-day shipping. You know what this is? I've been waiting out for it. It's one of those cheap iPhone chargers, but for some reason. . . PRINCE PATTERSON: Oh, the Amazon Prime ones. KAILATH: Uh-huh. PATTERSON: I just went through, like, three of those already, bro. KAILATH: Prince Patterson drives for UPS. Yours were good, or they were not good. PATTERSON: They're so-so. They're a so-so charger. KAILATH: That's the thing. These cheap chargers can range from just fine to terrible. The worst ones can fry your phone. So how do you find the good ones? In theory, user reviews help with this. If a hundred other people say the thing works great, it must work great, right? Unless. . . Where you can get sellers, like the guys who make these cheap products. PATTERSON: Mmm hmm. KAILATH: I explain that some sellers will actually pay a few bucks for positive reviews. PATTERSON: Now you've got me, like, wondering - that's what I look for. Before I buy anything off Amazon, I read the reviews. So now I'm wondering, like, damn, are these legit reviews? KAILATH: Outside sites like ReviewMeta and Fakespot estimate that for certain popular products on Amazon like phone chargers, Bluetooth speakers, more than half the user reviews are suspect. Amazon disputes those estimates. SHARON CHIARELLA: Our approximation is that less than 1 percent of reviews are inauthentic. KAILATH: Sharon Chiarella is Amazon's vice president of community shopping. We should note here the Amazon is an NPR sponsor. CHIARELLA: We have built a lot of technology to assess whether or not we think a review is authentic. And there's a lot that goes into that. KAILATH: Amazon looks for suspicious patterns and scores incoming reviews for trustworthiness. Penalties for cheating can be harsh. In the past three years, Amazon sued more than a thousand sellers for buying reviews. CHIARELLA: And when we do those lawsuits, we subpoena the bad actors to get data from them. That allows us to identify more bad actors and spider out from there and train our algorithms. KAILATH: But as Amazon gets better at hunting them down, paid reviewers employ their own evasive maneuvers. I spoke with several online. One agreed to call me. TRAVIS: Hey there. KAILATH: Hey, Travis. What's up? TRAVIS: Not much, just got out of school. KAILATH: Travis is a teenager living in the Northeast. He asked that we use only his first name to avoid the attention and possible legal action from Amazon. Here's how it works. There are these sort of shadow marketplaces online set up just to buy and sell reviews - private Facebook groups, Slack channels, subreddits. Sellers congregate there, hawking all kinds of stuff. TRAVIS: Ethernet cables, flashlights, protein powder, fanny packs. KAILATH: If something catches Travis' attention, he approaches the seller, and they negotiate terms. Generally after he buys the product and leaves a five-star review, the seller will refund his purchase plus a few bucks for the trouble. Travis says he earns a couple of hundred a month this way, and the sellers provide detailed instructions to avoid detection. TRAVIS: Order here at the Amazon link. Don't click any coupons. Review within four to five days after receiving. This one's pretty important because if you review too soon after receiving, it'll look pretty suspicious. RENEE DIRESTA: It's sort of a whack-a-mole problem for Amazon in that the efforts to game the system persist. KAILATH: Renee DiResta researches disinformation online. She explains that in this black market, Travis is like the low-level street dealer. The people who commission him are often companies selling products on Alibaba, the Chinese e-commerce giant. They want to penetrate the U. S. market using Amazon's reputation and the cachet of its user reviews to do so. DIRESTA: If you order from Alibaba, it's going to take six to eight weeks to arrive. It's not a great experience. KAILATH: As Amazon keeps cracking down, paid reviewers will keep finding ways to evade them. Customers can turn to outside review sites like CNET or Wirecutter to find transparent information. But as long as there's a business incentive to game them, online user reviews will remain muddy waters. For NPR News, I'm Ryan Kailath. AILSA CHANG, HOST:  Time now for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CHANG: Maybe this sounds familiar. You buy something online, and when it arrives, it's not quite what you expected. But the product had dozens of glowing reviews, like 5 out of 5 stars. Well, outside auditors say be careful. Unreliable reviews are all over e-commerce sites. They come from an underground economy where sellers pay for shining reviews. Reporter Ryan Kailath explains. RYAN KAILATH, BYLINE: A brand new iPhone charger, like, from the Apple store costs almost 40 bucks. But Amazon has hundreds of cheaper options. That's where I got a knockoff charger recently for a third the price. Hey, UPS. Plus free two-day shipping. You know what this is? I've been waiting out for it. It's one of those cheap iPhone chargers, but for some reason. . . PRINCE PATTERSON: Oh, the Amazon Prime ones. KAILATH: Uh-huh. PATTERSON: I just went through, like, three of those already, bro. KAILATH: Prince Patterson drives for UPS. Yours were good, or they were not good. PATTERSON: They're so-so. They're a so-so charger. KAILATH: That's the thing. These cheap chargers can range from just fine to terrible. The worst ones can fry your phone. So how do you find the good ones? In theory, user reviews help with this. If a hundred other people say the thing works great, it must work great, right? Unless. . . Where you can get sellers, like the guys who make these cheap products. PATTERSON: Mmm hmm. KAILATH: I explain that some sellers will actually pay a few bucks for positive reviews. PATTERSON: Now you've got me, like, wondering - that's what I look for. Before I buy anything off Amazon, I read the reviews. So now I'm wondering, like, damn, are these legit reviews? KAILATH: Outside sites like ReviewMeta and Fakespot estimate that for certain popular products on Amazon like phone chargers, Bluetooth speakers, more than half the user reviews are suspect. Amazon disputes those estimates. SHARON CHIARELLA: Our approximation is that less than 1 percent of reviews are inauthentic. KAILATH: Sharon Chiarella is Amazon's vice president of community shopping. We should note here the Amazon is an NPR sponsor. CHIARELLA: We have built a lot of technology to assess whether or not we think a review is authentic. And there's a lot that goes into that. KAILATH: Amazon looks for suspicious patterns and scores incoming reviews for trustworthiness. Penalties for cheating can be harsh. In the past three years, Amazon sued more than a thousand sellers for buying reviews. CHIARELLA: And when we do those lawsuits, we subpoena the bad actors to get data from them. That allows us to identify more bad actors and spider out from there and train our algorithms. KAILATH: But as Amazon gets better at hunting them down, paid reviewers employ their own evasive maneuvers. I spoke with several online. One agreed to call me. TRAVIS: Hey there. KAILATH: Hey, Travis. What's up? TRAVIS: Not much, just got out of school. KAILATH: Travis is a teenager living in the Northeast. He asked that we use only his first name to avoid the attention and possible legal action from Amazon. Here's how it works. There are these sort of shadow marketplaces online set up just to buy and sell reviews - private Facebook groups, Slack channels, subreddits. Sellers congregate there, hawking all kinds of stuff. TRAVIS: Ethernet cables, flashlights, protein powder, fanny packs. KAILATH: If something catches Travis' attention, he approaches the seller, and they negotiate terms. Generally after he buys the product and leaves a five-star review, the seller will refund his purchase plus a few bucks for the trouble. Travis says he earns a couple of hundred a month this way, and the sellers provide detailed instructions to avoid detection. TRAVIS: Order here at the Amazon link. Don't click any coupons. Review within four to five days after receiving. This one's pretty important because if you review too soon after receiving, it'll look pretty suspicious. RENEE DIRESTA: It's sort of a whack-a-mole problem for Amazon in that the efforts to game the system persist. KAILATH: Renee DiResta researches disinformation online. She explains that in this black market, Travis is like the low-level street dealer. The people who commission him are often companies selling products on Alibaba, the Chinese e-commerce giant. They want to penetrate the U. S. market using Amazon's reputation and the cachet of its user reviews to do so. DIRESTA: If you order from Alibaba, it's going to take six to eight weeks to arrive. It's not a great experience. KAILATH: As Amazon keeps cracking down, paid reviewers will keep finding ways to evade them. Customers can turn to outside review sites like CNET or Wirecutter to find transparent information. But as long as there's a business incentive to game them, online user reviews will remain muddy waters. For NPR News, I'm Ryan Kailath.", "section": "Business", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-31-634331593": {"title": "Uber Parks Its Self-Driving Truck Project, Saying It Will Push For Autonomous Cars : NPR", "url": "https://www.npr.org/2018/07/31/634331593/uber-parks-its-self-driving-truck-project-saying-it-will-push-for-autonomous-car", "author": "No author found", "published_date": "2018-07-31", "content": "", "section": "Business", "disclaimer": ""}, "2018-07-31-634217975": {"title": "Homeland Security Officials Strategize To Thwart Cyberattacks : NPR", "url": "https://www.npr.org/2018/07/31/634217975/homeland-security-officials-strategize-to-thwart-cyberattacks", "author": "No author found", "published_date": "2018-07-31", "content": "RACHEL MARTIN, HOST: Russian hackers have penetrated electric utilities across the U. S. This is according to the Department of Homeland Security. So elections, power grids - what's next? How vulnerable is the United States to cyberattacks, and what's at stake? We're going to ask Christopher Krebs this question. He is the top cybersecurity official at the Department of Homeland Security. He's in New York for a DHS conference on this very topic today. Mr. Krebs, thanks for being here. CHRISTOPHER KREBS: Hey, thanks for having me. Good morning. MARTIN: Do you believe the federal government has a grip on the cyber threat particularly from Russia? KREBS: I certainly think that we're working with industry and the private sector to - a little bit better than we used to, that's for sure. That's the point of what we're doing up in New York today hosting the DHS National Cybersecurity Summit. We've got a number of CEOs and other senior executives across the industry, along with senior officials from the FBI, NSA, DHS and Department of Energy, coming together and say, hey, how can we tackle this problem together? This is truly one of those issues where, united, we stand, divided, we fall. MARTIN: So, I mean, the DHS said last week, made this kind of startling announcement, that hackers had infiltrated hundreds of control rooms in power utilities. How can that happen? KREBS: So, you know, as everybody looks at the Internet, we are seeing increasing connection for efficiencies, for management. But the problem is the more you connect, the more you raise your risk profile. So when we think about control systems, there are some - certainly some use cases where you would want to have something connect to the Internet so you could remote update. What we actually saw in that incident that we talked about, that ongoing Russian campaign, it wasn't that there were dozens or hundreds, even, of control system. There's actually just one or two that they actually successfully accessed. And it was not a baseload generation asset. But nonetheless, it is worrisome, and we need industry and government to work together to identify the risks, identify the threat and work together on how to mitigate that. And really what I mean is just stop it from happening going forward. And if it does happen, how do we limit the consequences in the event that there is - that something does happen? MARTIN: Right. But it is troubling that you don't have the answers to those very important questions at this point. I want to switch gears and talk about the election. Three months out from midterms, the director of National Intelligence, Dan Coats, says, hey, the warning lights are blinking right now, everyone. And we saw hackers - Russian hackers already tried to disrupt the campaign of Senator Claire McCaskill. Can we assume that this vote is going to be free and fair? KREBS: That's certainly what we're shooting for. Look; we learned a lot from 2016 and what the Russian actors did to try to interfere with our election. And we're assuming as if they're going to come back in 2018 and 2020 beyond that. We are working with every single state, all 50 states, a number of the biggest counties out there, and DHS is providing them with information, strategic threat intelligence but also technical assistance, helping them understand where the risk is in their system and giving them the tools to increase the security of those systems. We've got - we've got about $380 million from the Congress to do some patching and to do some cybersecurity investments. So we are aiming for a resilient election. I think we've made a lot of progress since 2016. But this is not a sprint. This is a marathon that we're going to keep working at it every single day. MARTIN: Christopher Krebs, under secretary for cybersecurity at the Department of Homeland Security, thanks so much for your time this morning. We appreciate it. KREBS: Thank you. Have a good day. RACHEL MARTIN, HOST:  Russian hackers have penetrated electric utilities across the U. S. This is according to the Department of Homeland Security. So elections, power grids - what's next? How vulnerable is the United States to cyberattacks, and what's at stake? We're going to ask Christopher Krebs this question. He is the top cybersecurity official at the Department of Homeland Security. He's in New York for a DHS conference on this very topic today. Mr. Krebs, thanks for being here. CHRISTOPHER KREBS: Hey, thanks for having me. Good morning. MARTIN: Do you believe the federal government has a grip on the cyber threat particularly from Russia? KREBS: I certainly think that we're working with industry and the private sector to - a little bit better than we used to, that's for sure. That's the point of what we're doing up in New York today hosting the DHS National Cybersecurity Summit. We've got a number of CEOs and other senior executives across the industry, along with senior officials from the FBI, NSA, DHS and Department of Energy, coming together and say, hey, how can we tackle this problem together? This is truly one of those issues where, united, we stand, divided, we fall. MARTIN: So, I mean, the DHS said last week, made this kind of startling announcement, that hackers had infiltrated hundreds of control rooms in power utilities. How can that happen? KREBS: So, you know, as everybody looks at the Internet, we are seeing increasing connection for efficiencies, for management. But the problem is the more you connect, the more you raise your risk profile. So when we think about control systems, there are some - certainly some use cases where you would want to have something connect to the Internet so you could remote update. What we actually saw in that incident that we talked about, that ongoing Russian campaign, it wasn't that there were dozens or hundreds, even, of control system. There's actually just one or two that they actually successfully accessed. And it was not a baseload generation asset. But nonetheless, it is worrisome, and we need industry and government to work together to identify the risks, identify the threat and work together on how to mitigate that. And really what I mean is just stop it from happening going forward. And if it does happen, how do we limit the consequences in the event that there is - that something does happen? MARTIN: Right. But it is troubling that you don't have the answers to those very important questions at this point. I want to switch gears and talk about the election. Three months out from midterms, the director of National Intelligence, Dan Coats, says, hey, the warning lights are blinking right now, everyone. And we saw hackers - Russian hackers already tried to disrupt the campaign of Senator Claire McCaskill. Can we assume that this vote is going to be free and fair? KREBS: That's certainly what we're shooting for. Look; we learned a lot from 2016 and what the Russian actors did to try to interfere with our election. And we're assuming as if they're going to come back in 2018 and 2020 beyond that. We are working with every single state, all 50 states, a number of the biggest counties out there, and DHS is providing them with information, strategic threat intelligence but also technical assistance, helping them understand where the risk is in their system and giving them the tools to increase the security of those systems. We've got - we've got about $380 million from the Congress to do some patching and to do some cybersecurity investments. So we are aiming for a resilient election. I think we've made a lot of progress since 2016. But this is not a sprint. This is a marathon that we're going to keep working at it every single day. MARTIN: Christopher Krebs, under secretary for cybersecurity at the Department of Homeland Security, thanks so much for your time this morning. We appreciate it. KREBS: Thank you. Have a good day.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-07-31-630866664": {"title": "WATCH: Building A Probe That Will Survive A Trip To The Sun : NPR", "url": "https://www.npr.org/2018/07/31/630866664/nasa-s-mission-to-touch-the-sun-launches-this-summer", "author": "No author found", "published_date": "2018-07-31", "content": "", "section": "VIDEOS: Maddie About Science", "disclaimer": ""}, "2018-08-02-634998447": {"title": "Spotify Pulls Some Alex Jones Podcast Episodes : NPR", "url": "https://www.npr.org/2018/08/02/634998447/spotify-pulls-some-alex-jones-podcast-episodes", "author": "No author found", "published_date": "2018-08-02", "content": "", "section": "Music News", "disclaimer": ""}, "2018-08-02-632697978": {"title": "Apple Becomes World's 1st Private-Sector Company Worth $1 Trillion : NPR", "url": "https://www.npr.org/2018/08/02/632697978/apple-becomes-worlds-1st-private-sector-company-worth-1-trillion", "author": "No author found", "published_date": "2018-08-02", "content": "AILSA CHANG, HOST: OK, Apple has become the first private sector company to be worth a trillion dollars. That's after its share price hit an all-time high today. Reporter Ryan Kailath has the story now that he's worked out exactly how many zeroes are in 1 trillion. RYAN KAILATH, BYLINE: Twelve zeros, as it happens. You need four commas for a trillion dollars. Apple hit the figure when its share price jumped more than 8 percent this week. That's after the company reported strong quarterly earnings driven mostly by iPhone sales. The trillion and change is actually Apple's market capitalization. That's what you get when you multiply the number of shares by the share price. It's a historic moment, and the talking heads are talking. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: But yes, we officially hit it. We officially made history today. Apple did. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: Apple just hit a trillion dollars in market value. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #3: For the first time in history, a company is worth $1 trillion. KAILATH: So big deal - super meaningful, right? KIM FORREST: Kind of unimportant. KAILATH: Kim Forrest is a portfolio manager at Fort Pitt Capital, and she says the number's largely symbolic. Apple doesn't unlock any new capitalist superpowers now that it's crossed the threshold. It just sounds cool. If anything, it means the company can borrow more money if it wants. Forrest likens it to, well, being rich. FORREST: The bigger your house, the more you can borrow against it because the lender thinks that your house is going to be worth more tomorrow than it is today. KAILATH: Some investors think this is actually a bad thing, a sign that Apple's stock is overvalued. Forrest says, maybe, but she adds Apple's got revenue on the books to back up its high valuation unlike some of its fancy tech sector peers. Some of those peers are likely to join Apple in the four-comma club. Amazon's close. But in the history books, you rarely learn who came in second. For NPR News, I'm Ryan Kailath. AILSA CHANG, HOST:  OK, Apple has become the first private sector company to be worth a trillion dollars. That's after its share price hit an all-time high today. Reporter Ryan Kailath has the story now that he's worked out exactly how many zeroes are in 1 trillion. RYAN KAILATH, BYLINE: Twelve zeros, as it happens. You need four commas for a trillion dollars. Apple hit the figure when its share price jumped more than 8 percent this week. That's after the company reported strong quarterly earnings driven mostly by iPhone sales. The trillion and change is actually Apple's market capitalization. That's what you get when you multiply the number of shares by the share price. It's a historic moment, and the talking heads are talking. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: But yes, we officially hit it. We officially made history today. Apple did. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #2: Apple just hit a trillion dollars in market value. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #3: For the first time in history, a company is worth $1 trillion. KAILATH: So big deal - super meaningful, right? KIM FORREST: Kind of unimportant. KAILATH: Kim Forrest is a portfolio manager at Fort Pitt Capital, and she says the number's largely symbolic. Apple doesn't unlock any new capitalist superpowers now that it's crossed the threshold. It just sounds cool. If anything, it means the company can borrow more money if it wants. Forrest likens it to, well, being rich. FORREST: The bigger your house, the more you can borrow against it because the lender thinks that your house is going to be worth more tomorrow than it is today. KAILATH: Some investors think this is actually a bad thing, a sign that Apple's stock is overvalued. Forrest says, maybe, but she adds Apple's got revenue on the books to back up its high valuation unlike some of its fancy tech sector peers. Some of those peers are likely to join Apple in the four-comma club. Amazon's close. But in the history books, you rarely learn who came in second. For NPR News, I'm Ryan Kailath.", "section": "Business", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-02-634827587": {"title": "Google Testing A Censored Search Engine Just For China : NPR", "url": "https://www.npr.org/2018/08/02/634827587/google-testing-a-censored-search-engine-just-for-china", "author": "No author found", "published_date": "2018-08-02", "content": "", "section": "Technology", "disclaimer": ""}, "2018-08-02-634749387": {"title": "What Is QAnon? The Conspiracy Theory Tiptoeing Into Trump World : NPR", "url": "https://www.npr.org/2018/08/02/634749387/what-is-qanon-the-conspiracy-theory-tiptoeing-into-trump-world", "author": "No author found", "published_date": "2018-08-02", "content": "", "section": "Politics", "disclaimer": ""}, "2018-08-02-634682228": {"title": "This Is 'Not Fine': New Evidence Of Russian Interference Meets Inaction, Frustration : NPR", "url": "https://www.npr.org/2018/08/02/634682228/this-is-not-fine-new-evidence-of-russian-interference-meets-inaction-frustration", "author": "No author found", "published_date": "2018-08-02", "content": "", "section": "Politics", "disclaimer": ""}, "2018-08-03-635221620": {"title": "Amazon Pulls Some Nazi-Themed, Offensive Items After Criticism, But Many Remain : NPR", "url": "https://www.npr.org/2018/08/03/635221620/amazon-removes-some-racist-items-from-its-site-after-criticism-but-many-remain", "author": "No author found", "published_date": "2018-08-03", "content": "", "section": "Business", "disclaimer": ""}, "2018-08-03-635226270": {"title": "Solutions To Facebook's Privacy And Security Concerns Come At A Cost  : NPR", "url": "https://www.npr.org/2018/08/03/635226270/solutions-to-facebooks-privacy-and-security-concerns-come-at-a-cost", "author": "No author found", "published_date": "2018-08-03", "content": "RACHEL MARTIN, HOST: Earlier this week, Facebook said it took down 32 accounts involved in deceptive political influence campaigns. This comes after over a year of scandals that have plagued the company and just months ahead of the midterm elections. There is growing concern over what kind of security a social media platform that is free for users can actually provide. NPR's Jasmine Garsd reports that privacy and security come at a cost. JASMINE GARSD, BYLINE: Despite what you may have heard, Facebook is in an enviable position. About half of the Internet-using world is on it. And the company made over $13 billion in revenue in the last three months. And yet if you tuned into the company's most recent earnings call, you'd think you were crashing someone's funeral. Facebook announced it hasn't been growing as fast as usual. Here's one investor. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: What's driving this fairly dramatic deceleration of revenue growth? GARSD: CFO David Wehner could only muster an awkward response. (SOUNDBITE OF ARCHIVED RECORDING)DAVID WEHNER: In terms of what is - you know, what is driving. . . GARSD: Immediately after the opinion pieces reined in - warning that, after a string of privacy and security problems, the social media platform is in deep trouble. Jeffrey Chester is the executive director of the Center for Digital Democracy. He says what's happening to Facebook. . . JEFFREY CHESTER: Was a message being sent by both the investor class and the global Internet public - I mean, the reason that there's a decline in revenues is that people are leaving Facebook. GARSD: But Antonio Garcia-Martinez, a former Facebook product manager, says something different is happening. ANTONIO GARCIA-MARTINEZ: Facebook has had to hire a lot more people and have a lot more teams in various parts of the world dealing with, you know, privacy issues. GARSD: During that really tense earnings call, the company warned that next year its expenses will keep rising faster than revenues. A Facebook spokesperson told NPR that it intends to add 5,000 more content moderators by the end of 2018. All this hiring by Facebook CEO Mark Zuckerberg is not convincing everyone. Here's Jeffrey Chester again, of the Center for Digital Democracy. CHESTER: One of the strategies here that Facebook is embarking on is to tell Congress it's doing everything it can. And don't think about regulating. We're taking care of it. But I think we're deceiving ourselves. GARSD: Chester says Facebook needs to accept U. S. government regulation. Whatever you make of Facebook's recent efforts and motives, this is a pivotal moment. Antonio Garcia-Martinez says the fact that Facebook is spending so much on privacy will be good for users. GARCIA-MARTINEZ: Historically, most social media strategies have been - look - just go for growth. Ignore niceties, like user security and privacy and content moderation, because they're too expensive. I don't think that's going to be a viable strategy going forward. GARSD: He says we're moving into an era in which social media companies must take privacy and security seriously. And that means paying a price. Jasmine Garsd, NPR News, New York. RACHEL MARTIN, HOST:  Earlier this week, Facebook said it took down 32 accounts involved in deceptive political influence campaigns. This comes after over a year of scandals that have plagued the company and just months ahead of the midterm elections. There is growing concern over what kind of security a social media platform that is free for users can actually provide. NPR's Jasmine Garsd reports that privacy and security come at a cost. JASMINE GARSD, BYLINE: Despite what you may have heard, Facebook is in an enviable position. About half of the Internet-using world is on it. And the company made over $13 billion in revenue in the last three months. And yet if you tuned into the company's most recent earnings call, you'd think you were crashing someone's funeral. Facebook announced it hasn't been growing as fast as usual. Here's one investor. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: What's driving this fairly dramatic deceleration of revenue growth? GARSD: CFO David Wehner could only muster an awkward response. (SOUNDBITE OF ARCHIVED RECORDING) DAVID WEHNER: In terms of what is - you know, what is driving. . . GARSD: Immediately after the opinion pieces reined in - warning that, after a string of privacy and security problems, the social media platform is in deep trouble. Jeffrey Chester is the executive director of the Center for Digital Democracy. He says what's happening to Facebook. . . JEFFREY CHESTER: Was a message being sent by both the investor class and the global Internet public - I mean, the reason that there's a decline in revenues is that people are leaving Facebook. GARSD: But Antonio Garcia-Martinez, a former Facebook product manager, says something different is happening. ANTONIO GARCIA-MARTINEZ: Facebook has had to hire a lot more people and have a lot more teams in various parts of the world dealing with, you know, privacy issues. GARSD: During that really tense earnings call, the company warned that next year its expenses will keep rising faster than revenues. A Facebook spokesperson told NPR that it intends to add 5,000 more content moderators by the end of 2018. All this hiring by Facebook CEO Mark Zuckerberg is not convincing everyone. Here's Jeffrey Chester again, of the Center for Digital Democracy. CHESTER: One of the strategies here that Facebook is embarking on is to tell Congress it's doing everything it can. And don't think about regulating. We're taking care of it. But I think we're deceiving ourselves. GARSD: Chester says Facebook needs to accept U. S. government regulation. Whatever you make of Facebook's recent efforts and motives, this is a pivotal moment. Antonio Garcia-Martinez says the fact that Facebook is spending so much on privacy will be good for users. GARCIA-MARTINEZ: Historically, most social media strategies have been - look - just go for growth. Ignore niceties, like user security and privacy and content moderation, because they're too expensive. I don't think that's going to be a viable strategy going forward. GARSD: He says we're moving into an era in which social media companies must take privacy and security seriously. And that means paying a price. Jasmine Garsd, NPR News, New York.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-05-635748542": {"title": "Russian Operatives, Facebook And Social Movements : NPR", "url": "https://www.npr.org/2018/08/05/635748542/russian-operatives-facebook-and-social-movements", "author": "No author found", "published_date": "2018-08-05", "content": "LULU GARCIA-NAVARRO, HOST: This is a story about the real-life consequences of disinformation campaigns online. A Facebook page called Resisters billed itself as a space for online and offline feminist activism against fascism - turns out it was fake - just one of 32 pages and accounts Facebook took down this past week for engaging in, quote, \"coordinated, inauthentic behavior\" - behavior Facebook said was similar to that of a Russia-based group accused of interfering in the 2016 election, although, in its announcement, Facebook declined to directly link these pages to Russia. Well, Russia or not, whoever was behind the fake Resisters Facebook page reached out to real people, including activists like Lourdes Ashley Hunter of the Trans Women of Color Collective here in D. C. LOURDES ASHLEY HUNTER: And so I was contacted from someone named Mary Smith (ph) about a rally that was happening in front of the White House. GARCIA-NAVARRO: This was July 2017. Smith said Resisters was protesting President Trump's ban of transgender people in the military and was looking for someone local to emcee. Hunter agreed. HUNTER: They sent me all of the details, the speakers list, the speakers' bios. You know, there was nothing suspect about it. GARCIA-NAVARRO: They communicated over Facebook Messenger. But as the big day approached, Hunter wanted to talk with Mary Smith over the phone. HUNTER: She just had surgery. She could not talk. And I was like, OK. And then on the day of the event, I never met her. She said that she was on her way. And then there was another message about - she left early. And we were supposed to get together for dinner. And that didn't happen. GARCIA-NAVARRO: And it wasn't until very recently that she found out why. When did you find out that this person wasn't who she said she was? HUNTER: When I got a phone call from The Hill and then a call from CNN earlier this week. GARCIA-NAVARRO: The Hill, the newspaper, and CNN, the broadcast network. HUNTER: Correct. And so I happened to go back through my Facebook Messenger from then. And I noticed that the Facebook user Mary Smith is no longer, like, there. But the messages were. GARCIA-NAVARRO: Right - so you could see your interaction. And you didn't have any contact with Mary Smith after this event. HUNTER: No, I didn't. GARCIA-NAVARRO: When you found this out, what did you think? HUNTER: I was like, is this really happening? I also was quite bothered by the fact that folk would actually take advantage of community organizers working, like, to shift the landscape of violence and discrimination that is happening in our country. GARCIA-NAVARRO: As an activist, are you concerned that people possibly outside of this country are using social issues, which are divisive, and trying to sort of pit people against each other? HUNTER: That is the worry. It's a huge concern. And it's unfortunate. But this is the age of technology that we're living in. And with any business practice, you want to do your due diligence with the folks that you're partnering with. It's just better for our work that this has happened because then we get to elevate our work and be more intentional with the communities that we serve. GARCIA-NAVARRO: So when you say due diligence, what does that mean, practically speaking? What are you doing differently? HUNTER: Meeting with people, doing background work around, who are these people that we're working with? Who have they worked with before in the past? What were some of the outcomes of that? And also not rushing to plan events without making sure that we have people on the team who are totally committed to the work that we're doing. GARCIA-NAVARRO: This reminds me of the fact that, you know, Facebook and Twitter and so many other social media platforms have been such a powerful tool for activism if you think of the Arab Spring and how integral it was for that. Are we at a moment, though, when the role of social media has to be re-evaluated? HUNTER: I don't think so. It's critical that social media is accessible to folk, especially on the ground, especially folk who don't have access to resources and multimillion-dollar news networks. So I don't think that it needs to be re-evaluated. I think that we just need to be more cognizant of the power that social media has and be able to use it intentionally. GARCIA-NAVARRO: What would you like to see groups like Facebook do? Are you happy that these pages were taken down? HUNTER: I think that Facebook needs to be more intentional with its users. Like, I never got a message from Facebook. I never got a notification. And so I think while Facebook has made this sweeping judgment to take down these pages, I think that being more intentional with its users, especially those suspected of being involved. . . GARCIA-NAVARRO: More transparent, you mean. HUNTER: Yes, absolutely. GARCIA-NAVARRO: Have you heard from Facebook since then? HUNTER: I have not. But they haven't taken down any of my pages. They haven't taken down Trans Women of Color Collective's page. So I think that they know that I'm a real person and. . . GARCIA-NAVARRO: Just to confirm, you are a real person, yes? HUNTER: (Laughter) Yes, I am. GARCIA-NAVARRO: Lourdes Ashley Hunter is the executive director of the Trans Women of Color Collective. Thank you so much. HUNTER: Thank you for having me. LULU GARCIA-NAVARRO, HOST:  This is a story about the real-life consequences of disinformation campaigns online. A Facebook page called Resisters billed itself as a space for online and offline feminist activism against fascism - turns out it was fake - just one of 32 pages and accounts Facebook took down this past week for engaging in, quote, \"coordinated, inauthentic behavior\" - behavior Facebook said was similar to that of a Russia-based group accused of interfering in the 2016 election, although, in its announcement, Facebook declined to directly link these pages to Russia. Well, Russia or not, whoever was behind the fake Resisters Facebook page reached out to real people, including activists like Lourdes Ashley Hunter of the Trans Women of Color Collective here in D. C. LOURDES ASHLEY HUNTER: And so I was contacted from someone named Mary Smith (ph) about a rally that was happening in front of the White House. GARCIA-NAVARRO: This was July 2017. Smith said Resisters was protesting President Trump's ban of transgender people in the military and was looking for someone local to emcee. Hunter agreed. HUNTER: They sent me all of the details, the speakers list, the speakers' bios. You know, there was nothing suspect about it. GARCIA-NAVARRO: They communicated over Facebook Messenger. But as the big day approached, Hunter wanted to talk with Mary Smith over the phone. HUNTER: She just had surgery. She could not talk. And I was like, OK. And then on the day of the event, I never met her. She said that she was on her way. And then there was another message about - she left early. And we were supposed to get together for dinner. And that didn't happen. GARCIA-NAVARRO: And it wasn't until very recently that she found out why. When did you find out that this person wasn't who she said she was? HUNTER: When I got a phone call from The Hill and then a call from CNN earlier this week. GARCIA-NAVARRO: The Hill, the newspaper, and CNN, the broadcast network. HUNTER: Correct. And so I happened to go back through my Facebook Messenger from then. And I noticed that the Facebook user Mary Smith is no longer, like, there. But the messages were. GARCIA-NAVARRO: Right - so you could see your interaction. And you didn't have any contact with Mary Smith after this event. HUNTER: No, I didn't. GARCIA-NAVARRO: When you found this out, what did you think? HUNTER: I was like, is this really happening? I also was quite bothered by the fact that folk would actually take advantage of community organizers working, like, to shift the landscape of violence and discrimination that is happening in our country. GARCIA-NAVARRO: As an activist, are you concerned that people possibly outside of this country are using social issues, which are divisive, and trying to sort of pit people against each other? HUNTER: That is the worry. It's a huge concern. And it's unfortunate. But this is the age of technology that we're living in. And with any business practice, you want to do your due diligence with the folks that you're partnering with. It's just better for our work that this has happened because then we get to elevate our work and be more intentional with the communities that we serve. GARCIA-NAVARRO: So when you say due diligence, what does that mean, practically speaking? What are you doing differently? HUNTER: Meeting with people, doing background work around, who are these people that we're working with? Who have they worked with before in the past? What were some of the outcomes of that? And also not rushing to plan events without making sure that we have people on the team who are totally committed to the work that we're doing. GARCIA-NAVARRO: This reminds me of the fact that, you know, Facebook and Twitter and so many other social media platforms have been such a powerful tool for activism if you think of the Arab Spring and how integral it was for that. Are we at a moment, though, when the role of social media has to be re-evaluated? HUNTER: I don't think so. It's critical that social media is accessible to folk, especially on the ground, especially folk who don't have access to resources and multimillion-dollar news networks. So I don't think that it needs to be re-evaluated. I think that we just need to be more cognizant of the power that social media has and be able to use it intentionally. GARCIA-NAVARRO: What would you like to see groups like Facebook do? Are you happy that these pages were taken down? HUNTER: I think that Facebook needs to be more intentional with its users. Like, I never got a message from Facebook. I never got a notification. And so I think while Facebook has made this sweeping judgment to take down these pages, I think that being more intentional with its users, especially those suspected of being involved. . . GARCIA-NAVARRO: More transparent, you mean. HUNTER: Yes, absolutely. GARCIA-NAVARRO: Have you heard from Facebook since then? HUNTER: I have not. But they haven't taken down any of my pages. They haven't taken down Trans Women of Color Collective's page. So I think that they know that I'm a real person and. . . GARCIA-NAVARRO: Just to confirm, you are a real person, yes? HUNTER: (Laughter) Yes, I am. GARCIA-NAVARRO: Lourdes Ashley Hunter is the executive director of the Trans Women of Color Collective. Thank you so much. HUNTER: Thank you for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-05-635127389": {"title": "Majority Of Black Americans Value Social Media For Amplifying Lesser-Known Issues : NPR", "url": "https://www.npr.org/2018/08/05/635127389/majority-of-black-americans-value-social-media-for-amplifying-lesser-known-issue", "author": "No author found", "published_date": "2018-08-05", "content": "", "section": "National", "disclaimer": ""}, "2018-08-06-636016812": {"title": "#Blessed: Is Everyone Happier Than You On Social Media? : NPR", "url": "https://www.npr.org/2018/08/06/636016812/-blessed-is-everyone-happier-than-you-on-social-media", "author": "No author found", "published_date": "2018-08-06", "content": "ARI SHAPIRO, HOST: This month on All Tech Considered, we look at the gap between how we portray ourselves online and who we really are. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")SHAPIRO: Let's start with the story of a Twitter thread that recently went viral. BuzzFeed host Tracy Clayton asked people to share photos of themselves that they had uploaded to social media. Specifically, she wanted to see photos in which they looked perfectly fine but inside felt anything but all right. NPR's Jasmine Garsd introduces us to one woman who contributed to the thread. JASMINE GARSD, BYLINE: Mankaprr Conteh remembers when this picture was taken. She's in the Caribbean. MANKAPRR CONTEH: And my hair is in my hands, and the water is shallow. And I'm smiling. GARSD: She looks stunning. Had you been browsing Instagram, you wouldn't have known that this was one of the worst times of her life. CONTEH: I was initially diagnosed with depression and then later diagnosed with bipolar two. It's like hating waking up, like, dreading the morning. GARSD: Concerned about her daughter, her mother took her on a vacation, which is when the picture was taken. If you've spent any time on social media, you've seen these pictures of people having a better time than you - hashtag blessed, hashtag grateful. Dr. Brian Primack is the director of the Center for Research on Media, Tech and Health at the University of Pittsburgh. BRIAN PRIMACK: People who feel socially isolated may be reaching out on social media to self-medicate. GARSD: Primack suspects people who are depressed often post in this way to reach out, feel they are part of the fun. He's co-authored several studies about how social media affects mental health. And he found that people who checked social media the most frequently had almost three times the risk of depression compared with people who checked less often. He says part of the problem is, on one level, we know these are filtered, curated photos. But. . . PRIMACK: You know, these are real people, so you feel like this is very much real life. You know it's not a Jose Cuervo ad where the people are getting paid to put on smiles. You know, these are people that you actually know. GARSD: So what comes first? Do you reach out more on social media because you're depressed, or do you get more depressed because you spend more time there? Dr. Primack doesn't know, but he suspects it's a cycle. PRIMACK: That reaching out might then only serve to increase the perception of social isolation, which then leads to more social media use, et cetera. GARSD: I asked Conteh why she posted that picture of herself smiling on the beach. And she gives me a different reason. CONTEH: It was like a celebration of the not-crappiness of that moment. And it's almost like trying to get back into the groove of, like, normalcy in the midst of depression. GARSD: It captured the way she wished things could actually be. But she gets the irony of going on this platform in which. . . CONTEH: Everybody seems happy and productive and accomplished and beautiful. And it's, like, I am none of these things right now. GARSD: And back then, she didn't feel any of those things. Now she says things are different. She talks openly about her depression on and offline. CONTEH: Hey, this is what I'm going through. This is how I'm trying to work on it and live with it and cope with it. This is how I think it's affecting you. And this is how we're going to try to, like, move forward with all that in mind. GARSD: Breaking a cycle, really, truly getting back into the groove. Jasmine Garsd, NPR News, New York. ARI SHAPIRO, HOST:  This month on All Tech Considered, we look at the gap between how we portray ourselves online and who we really are. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") SHAPIRO: Let's start with the story of a Twitter thread that recently went viral. BuzzFeed host Tracy Clayton asked people to share photos of themselves that they had uploaded to social media. Specifically, she wanted to see photos in which they looked perfectly fine but inside felt anything but all right. NPR's Jasmine Garsd introduces us to one woman who contributed to the thread. JASMINE GARSD, BYLINE: Mankaprr Conteh remembers when this picture was taken. She's in the Caribbean. MANKAPRR CONTEH: And my hair is in my hands, and the water is shallow. And I'm smiling. GARSD: She looks stunning. Had you been browsing Instagram, you wouldn't have known that this was one of the worst times of her life. CONTEH: I was initially diagnosed with depression and then later diagnosed with bipolar two. It's like hating waking up, like, dreading the morning. GARSD: Concerned about her daughter, her mother took her on a vacation, which is when the picture was taken. If you've spent any time on social media, you've seen these pictures of people having a better time than you - hashtag blessed, hashtag grateful. Dr. Brian Primack is the director of the Center for Research on Media, Tech and Health at the University of Pittsburgh. BRIAN PRIMACK: People who feel socially isolated may be reaching out on social media to self-medicate. GARSD: Primack suspects people who are depressed often post in this way to reach out, feel they are part of the fun. He's co-authored several studies about how social media affects mental health. And he found that people who checked social media the most frequently had almost three times the risk of depression compared with people who checked less often. He says part of the problem is, on one level, we know these are filtered, curated photos. But. . . PRIMACK: You know, these are real people, so you feel like this is very much real life. You know it's not a Jose Cuervo ad where the people are getting paid to put on smiles. You know, these are people that you actually know. GARSD: So what comes first? Do you reach out more on social media because you're depressed, or do you get more depressed because you spend more time there? Dr. Primack doesn't know, but he suspects it's a cycle. PRIMACK: That reaching out might then only serve to increase the perception of social isolation, which then leads to more social media use, et cetera. GARSD: I asked Conteh why she posted that picture of herself smiling on the beach. And she gives me a different reason. CONTEH: It was like a celebration of the not-crappiness of that moment. And it's almost like trying to get back into the groove of, like, normalcy in the midst of depression. GARSD: It captured the way she wished things could actually be. But she gets the irony of going on this platform in which. . . CONTEH: Everybody seems happy and productive and accomplished and beautiful. And it's, like, I am none of these things right now. GARSD: And back then, she didn't feel any of those things. Now she says things are different. She talks openly about her depression on and offline. CONTEH: Hey, this is what I'm going through. This is how I'm trying to work on it and live with it and cope with it. This is how I think it's affecting you. And this is how we're going to try to, like, move forward with all that in mind. GARSD: Breaking a cycle, really, truly getting back into the groove. Jasmine Garsd, NPR News, New York.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-06-636030043": {"title": "Alex Jones, Infowars Content Banned By YouTube, Apple and Facebook : NPR", "url": "https://www.npr.org/2018/08/06/636030043/youtube-apple-and-facebook-ban-infowars-which-decries-mega-purge", "author": "No author found", "published_date": "2018-08-06", "content": "", "section": "Media", "disclaimer": ""}, "2018-08-06-634881408": {"title": "An Asylum-Seeker Wrote A Book By Phone Texts From Manus Island Detention : NPR", "url": "https://www.npr.org/2018/08/06/634881408/an-asylum-seeker-wrote-a-book-by-phone-texts-from-manus-island-detention", "author": "No author found", "published_date": "2018-08-06", "content": "NOEL KING, HOST: Kurdish-Iranian journalist Behrouz Boochani fled Iran fearing for his safety. He wrote for a pro-Kurdish magazine that was raided by the Iranian military. So he went to Indonesia, and from there, he got on a boat to Australia. He wanted to seek political asylum. But the Australian Navy intercepted the boat, and he and others were detained. Boochani was sent to remote Manus Island, which is a part of Papua New Guinea. That's where we reached him by phone. BEHROUZ BOOCHANI: The Australian government calls this place a camp or offshore processing center, but for us it's a prison. KING: Australia's controversial policy of picking up refugees at sea and sending them to remote islands has left hundreds of people like Boochani detained indefinitely. He's documented his five years on Manus Island through articles and even a documentary released last year using only his phone. Now he's written a book, titled, \"No Friend But The Mountains. \" He wrote the whole book using the encrypted messaging service WhatsApp. BOOCHANI: Because I was scared from the authorities. Any time, they could come and take my paper. So that's why I wrote this book on the phone and sent it out bit by bit. KING: Your book is 400 pages. Were you texting a page at a time, a paragraph at a time? BOOCHANI: So WhatsApp was like my notebook. Some nights, I could write two pages or one page and send it out to my translator. So my translator put them together in PDF and sent it back to me. So it was a long process, and a very hard process. KING: So I understand that you don't have your book there in front of you so I'm going to read a bit of your book, an English language version. You write about Manus Island after it rains. You write about the flowers that look like chamomile, (reading) dancing incessantly, breathing heavily, gasping as though in love with the cool ocean breeze. I love those flowers. A zeal for resistance. A tremendous will for life bursting out from the coils and curves of the stems. Behrouz, you make Manus Island sound beautiful. And, I'm wondering, in a book that is about being trapped on this island, what went into the thinking to make it sound so lovely? BOOCHANI: I was able to survive in this harsh prison because of nature. Manus Island is a very beautiful island. And you know, I came from Kurdistan. Kurdistan land is very beautiful land. And I grew up on nature, and I think I could survive because of nature. KING: Boochani doesn't know who will read his book, and he says he doesn't care how much money it makes. He has no use for a lot of money on the island. And so he is stuck, unable to move forward, unable to go home. BOOCHANI: If I didn't have a strong reason, I wouldn't stay here for five years. KING: No. Has there been any movement in your case to get off of Manus Island? Do you have any hope that you will soon leave? BOOCHANI: Yeah. Actually, in the past few months, American government made a deal with Australia so they could accept some of the refugees from here. About one hundred people left Manus and they went to America. We all hope that finally after five years we get freedom in a place like America or other countries. KING: He is effectively pinning his hopes on the United States. Behrouz Boochani is a Kurdish-Iranian journalist. He's been detained on Manus Island and is the author of \"No Friend But The Mountains. \" NOEL KING, HOST:  Kurdish-Iranian journalist Behrouz Boochani fled Iran fearing for his safety. He wrote for a pro-Kurdish magazine that was raided by the Iranian military. So he went to Indonesia, and from there, he got on a boat to Australia. He wanted to seek political asylum. But the Australian Navy intercepted the boat, and he and others were detained. Boochani was sent to remote Manus Island, which is a part of Papua New Guinea. That's where we reached him by phone. BEHROUZ BOOCHANI: The Australian government calls this place a camp or offshore processing center, but for us it's a prison. KING: Australia's controversial policy of picking up refugees at sea and sending them to remote islands has left hundreds of people like Boochani detained indefinitely. He's documented his five years on Manus Island through articles and even a documentary released last year using only his phone. Now he's written a book, titled, \"No Friend But The Mountains. \" He wrote the whole book using the encrypted messaging service WhatsApp. BOOCHANI: Because I was scared from the authorities. Any time, they could come and take my paper. So that's why I wrote this book on the phone and sent it out bit by bit. KING: Your book is 400 pages. Were you texting a page at a time, a paragraph at a time? BOOCHANI: So WhatsApp was like my notebook. Some nights, I could write two pages or one page and send it out to my translator. So my translator put them together in PDF and sent it back to me. So it was a long process, and a very hard process. KING: So I understand that you don't have your book there in front of you so I'm going to read a bit of your book, an English language version. You write about Manus Island after it rains. You write about the flowers that look like chamomile, (reading) dancing incessantly, breathing heavily, gasping as though in love with the cool ocean breeze. I love those flowers. A zeal for resistance. A tremendous will for life bursting out from the coils and curves of the stems. Behrouz, you make Manus Island sound beautiful. And, I'm wondering, in a book that is about being trapped on this island, what went into the thinking to make it sound so lovely? BOOCHANI: I was able to survive in this harsh prison because of nature. Manus Island is a very beautiful island. And you know, I came from Kurdistan. Kurdistan land is very beautiful land. And I grew up on nature, and I think I could survive because of nature. KING: Boochani doesn't know who will read his book, and he says he doesn't care how much money it makes. He has no use for a lot of money on the island. And so he is stuck, unable to move forward, unable to go home. BOOCHANI: If I didn't have a strong reason, I wouldn't stay here for five years. KING: No. Has there been any movement in your case to get off of Manus Island? Do you have any hope that you will soon leave? BOOCHANI: Yeah. Actually, in the past few months, American government made a deal with Australia so they could accept some of the refugees from here. About one hundred people left Manus and they went to America. We all hope that finally after five years we get freedom in a place like America or other countries. KING: He is effectively pinning his hopes on the United States. Behrouz Boochani is a Kurdish-Iranian journalist. He's been detained on Manus Island and is the author of \"No Friend But The Mountains. \"", "section": "World", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-07-636274330": {"title": "Pentagon Restricts Fitness And GPS Trackers For Deployed Personnel : NPR", "url": "https://www.npr.org/2018/08/07/636274330/pentagon-restricts-fitness-and-gps-trackers-for-deployed-personnel", "author": "No author found", "published_date": "2018-08-07", "content": "", "section": "National", "disclaimer": ""}, "2018-08-08-636644539": {"title": "Twitter's CEO Explains Why He Didn't Block Alex Jones, Bucking Big Tech Trend  : NPR", "url": "https://www.npr.org/2018/08/08/636644539/jack-dorsey-explains-why-twitter-didnt-block-alex-jones-bucking-big-tech-trend", "author": "No author found", "published_date": "2018-08-08", "content": "", "section": "Technology", "disclaimer": ""}, "2018-08-09-637180286": {"title": "Julian Assange 'Seriously Considering' Testifying Before Senate Panel : NPR", "url": "https://www.npr.org/2018/08/09/637180286/julian-assange-seriously-considering-testifying-before-senate-panel", "author": "No author found", "published_date": "2018-08-09", "content": "", "section": "National Security", "disclaimer": ""}, "2018-08-09-637008474": {"title": "New York City Temporarily Halts More Uber And Lyft Cars On The Road : NPR", "url": "https://www.npr.org/2018/08/09/637008474/new-york-city-temporarily-halts-more-uber-and-lyft-cars-on-the-road", "author": "No author found", "published_date": "2018-08-09", "content": "", "section": "Business", "disclaimer": ""}, "2018-08-10-637253043": {"title": "For Abortion Activists In Argentina, A Campaign Waged Online Faces A Disconnect : NPR", "url": "https://www.npr.org/2018/08/10/637253043/for-abortion-activists-in-argentina-a-campaign-waged-online-faces-a-disconnect", "author": "No author found", "published_date": "2018-08-10", "content": "NOEL KING, HOST: All right. In Argentina, the Senate has rejected a bill that would have legalized abortion. If it had passed, Argentina would have become one of a few countries in Latin America where abortion is legal. This came as a big blow for feminist groups who've been protesting on the streets of Argentina for months. Social media also played a big role in this campaign. NPR's Jasmine Garsd has the story. JASMINE GARSD, BYLINE: It's a sunny day, and a woman walks past a young man on the street. He mutters an obscene cat call. (SOUNDBITE OF YOUTUBE VIDEO, \"PIROPOS-CUALCA\")UNIDENTIFIED PERSON #1: (Speaking Spanish). GARSD: In the clip, the woman smiles and says, thank you. But then the camera pans to her fantasy, what she really wishes she could do. (SOUNDBITE OF YOUTUBE VIDEO, \"PIROPOS-CUALCA\")GARSD: The video goes on to show her, in her imagination, pulling out a knife and stabbing him. It's a comedy skit, a dark one. Since it published on YouTube in 2014, it's gotten over 1. 3 million views and catapulted Argentine comedian Malena Pichot to Internet fame. It's part of a wave of young Latin American feminists who have very skillfully used social media to get the message out and take down long-held sexist tradition. MALENA PICHOT: (Speaking Spanish). GARSD: \"Every day, I have girls on the street approaching me,\" says Pichot, \"telling me that they are feminists because they saw some skit of mine. I mean, clearly, outside of television, online, in life, something is undeniably happening with the feminist movement. \" The digital feminist movement first tackled harassment, kind of Argentina's #MeToo moment, but it quickly pivoted to something more divisive - legalizing abortion. An estimated 500,000 women abort illegally in the country every year. The hashtag #legalabortionnow took over Argentine Twitter and Facebook. Over the last few months, women showed up in droves in downtown Buenos Aires and around the country wearing green handkerchiefs symbolizing the cause, demanding a change. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PROTESTERS: (Chanting in Spanish). UNIDENTIFIED PROTESTER: (Speaking Spanish). UNIDENTIFIED PROTESTERS: (Chanting in Spanish). GARSD: Conservatives fought back. They had their own symbol - a light-blue handkerchief, the color of the Argentine flag. They, too, went on social media with the anti-abortion hashtag #savebothlives. And then the holiest of all Argentines, Pope Francis himself, weighed in with an Instagram post in which he's holding a baby, captioned - the divine gift of life must be promoted, guarded and protected from conception to its natural end. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: (Speaking Spanish). GARSD: And that's the final vote, announced in the Argentine Senate on Thursday morning. Abortion remains illegal. I have never felt prouder of my country, tweeted one woman, an opponent of abortion. Another activist tweeted bitterly, go home and rest, senators. Tomorrow, you can send your lovers to abort at a private clinic. That was retweeted several thousand times, but as the Argentine feminist movement found out yesterday, online support doesn't always translate into political victory. Jasmine Garsd, NPR News, New York. NOEL KING, HOST:  All right. In Argentina, the Senate has rejected a bill that would have legalized abortion. If it had passed, Argentina would have become one of a few countries in Latin America where abortion is legal. This came as a big blow for feminist groups who've been protesting on the streets of Argentina for months. Social media also played a big role in this campaign. NPR's Jasmine Garsd has the story. JASMINE GARSD, BYLINE: It's a sunny day, and a woman walks past a young man on the street. He mutters an obscene cat call. (SOUNDBITE OF YOUTUBE VIDEO, \"PIROPOS-CUALCA\") UNIDENTIFIED PERSON #1: (Speaking Spanish). GARSD: In the clip, the woman smiles and says, thank you. But then the camera pans to her fantasy, what she really wishes she could do. (SOUNDBITE OF YOUTUBE VIDEO, \"PIROPOS-CUALCA\") GARSD: The video goes on to show her, in her imagination, pulling out a knife and stabbing him. It's a comedy skit, a dark one. Since it published on YouTube in 2014, it's gotten over 1. 3 million views and catapulted Argentine comedian Malena Pichot to Internet fame. It's part of a wave of young Latin American feminists who have very skillfully used social media to get the message out and take down long-held sexist tradition. MALENA PICHOT: (Speaking Spanish). GARSD: \"Every day, I have girls on the street approaching me,\" says Pichot, \"telling me that they are feminists because they saw some skit of mine. I mean, clearly, outside of television, online, in life, something is undeniably happening with the feminist movement. \" The digital feminist movement first tackled harassment, kind of Argentina's #MeToo moment, but it quickly pivoted to something more divisive - legalizing abortion. An estimated 500,000 women abort illegally in the country every year. The hashtag #legalabortionnow took over Argentine Twitter and Facebook. Over the last few months, women showed up in droves in downtown Buenos Aires and around the country wearing green handkerchiefs symbolizing the cause, demanding a change. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PROTESTERS: (Chanting in Spanish). UNIDENTIFIED PROTESTER: (Speaking Spanish). UNIDENTIFIED PROTESTERS: (Chanting in Spanish). GARSD: Conservatives fought back. They had their own symbol - a light-blue handkerchief, the color of the Argentine flag. They, too, went on social media with the anti-abortion hashtag #savebothlives. And then the holiest of all Argentines, Pope Francis himself, weighed in with an Instagram post in which he's holding a baby, captioned - the divine gift of life must be promoted, guarded and protected from conception to its natural end. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #2: (Speaking Spanish). GARSD: And that's the final vote, announced in the Argentine Senate on Thursday morning. Abortion remains illegal. I have never felt prouder of my country, tweeted one woman, an opponent of abortion. Another activist tweeted bitterly, go home and rest, senators. Tomorrow, you can send your lovers to abort at a private clinic. That was retweeted several thousand times, but as the Argentine feminist movement found out yesterday, online support doesn't always translate into political victory. Jasmine Garsd, NPR News, New York.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-12-637817855": {"title": "For Some Facebook Employees, Free Food Is Coming To An End : NPR", "url": "https://www.npr.org/2018/08/12/637817855/for-some-facebook-employees-free-food-is-coming-to-an-end", "author": "No author found", "published_date": "2018-08-12", "content": "LAKSHMI SINGH, HOST: There's a certain stereotype of Silicon Valley tech offices - that they provide endless snacks and meals around the clock. Well, now there's an effort underway to do away with that legendary perk. Menlo Park in California is banning Facebook from offering free food at its newest campus, and San Francisco is considering something similar. KQED's Tonya Mosley has more. TONYA MOSLEY, BYLINE: The inside of Facebook in Menlo Park, Calif. , is the stuff of lore. This 430,000-square-foot campus offers perks like an onsite cleaners, a dentist and free food, basically a smorgasbord of anything your heart desires - custom omelets, braised beef, handmade sushi. You really never have to leave the office. It's what lured Ben Werner here. He traveled all the way from France to get a tour of Facebook from a friend who works there. He wanted to see for himself all of the perks he has read so much about. BEN WERNER: I'd like to have those things taken care of - probably also would mean that I'd spend more time at work. But I guess it's a two-way street then that benefits us both. MOSLEY: But about 8 miles away in Mountain View, Calif. , the home of Google - free food, at least at the new Facebook campus, won't be on the menu. LENNY SIEGEL: We believe these companies are part of our community. A growing number of their employees live in our community. And we want them to be a part of our community. MOSLEY: Mountain View Mayor Lenny Siegel says, for years, restaurant owners have complained that employees of Google never come out to eat or shop. So when the city learned that Facebook would be opening a new office here in September, the council passed a measure that bars the company from serving free food in its corporate cafeteria. Facebook said, no problem. Under the agreement, the social media company can subsidize meals from restaurants open to the public. But Mayor Siegel acknowledges there are still a few kinks that need to be smoothed out. SIEGEL: Facebook is a global company, and some of their people work in the middle of the night. And if all the restaurants are closed, I would be open to considering food service in the middle of the night. MOSLEY: Erika Rasmussen manages an open-air grocery store called the Milk Pail Market next to the new Facebook office. To her, the thought of feeding 2,000 employees who are hungry around the clock is a bit nerve-wracking. ERIKA RASMUSSEN: We don't want Facebook to overwhelm this area, but we do want Facebook to support this area because we will need their patronage to survive. MOSLEY: Facebook says it's still working out the details, but some other ideas also include turning the ground floor of this new building into a food court with local restaurants open to the public. Deepak Rao, a tech employee at a startup in Silicon Valley, says perks aren't the defining reason he and his colleagues do the work they do. But sometimes, he says, when you're working long hours, perks like free food feel like a necessity. DEEPAK RAO: To go out, drive, or whatever, go eat, come - that could take an hour and a half, which you might not have. MOSLEY: So for tech companies, it's been worth it to keep employees like Rao at work for as long as they can stay by providing food in-house. These new laws will change what's become a given in Silicon Valley work culture. The city of San Francisco is also considering a similar measure that would ban cafeterias in all new office buildings, forcing tech employees to venture out and share a bit of the wealth outside of their walls. For NPR News, I'm Tonya Mosley. LAKSHMI SINGH, HOST:  There's a certain stereotype of Silicon Valley tech offices - that they provide endless snacks and meals around the clock. Well, now there's an effort underway to do away with that legendary perk. Menlo Park in California is banning Facebook from offering free food at its newest campus, and San Francisco is considering something similar. KQED's Tonya Mosley has more. TONYA MOSLEY, BYLINE: The inside of Facebook in Menlo Park, Calif. , is the stuff of lore. This 430,000-square-foot campus offers perks like an onsite cleaners, a dentist and free food, basically a smorgasbord of anything your heart desires - custom omelets, braised beef, handmade sushi. You really never have to leave the office. It's what lured Ben Werner here. He traveled all the way from France to get a tour of Facebook from a friend who works there. He wanted to see for himself all of the perks he has read so much about. BEN WERNER: I'd like to have those things taken care of - probably also would mean that I'd spend more time at work. But I guess it's a two-way street then that benefits us both. MOSLEY: But about 8 miles away in Mountain View, Calif. , the home of Google - free food, at least at the new Facebook campus, won't be on the menu. LENNY SIEGEL: We believe these companies are part of our community. A growing number of their employees live in our community. And we want them to be a part of our community. MOSLEY: Mountain View Mayor Lenny Siegel says, for years, restaurant owners have complained that employees of Google never come out to eat or shop. So when the city learned that Facebook would be opening a new office here in September, the council passed a measure that bars the company from serving free food in its corporate cafeteria. Facebook said, no problem. Under the agreement, the social media company can subsidize meals from restaurants open to the public. But Mayor Siegel acknowledges there are still a few kinks that need to be smoothed out. SIEGEL: Facebook is a global company, and some of their people work in the middle of the night. And if all the restaurants are closed, I would be open to considering food service in the middle of the night. MOSLEY: Erika Rasmussen manages an open-air grocery store called the Milk Pail Market next to the new Facebook office. To her, the thought of feeding 2,000 employees who are hungry around the clock is a bit nerve-wracking. ERIKA RASMUSSEN: We don't want Facebook to overwhelm this area, but we do want Facebook to support this area because we will need their patronage to survive. MOSLEY: Facebook says it's still working out the details, but some other ideas also include turning the ground floor of this new building into a food court with local restaurants open to the public. Deepak Rao, a tech employee at a startup in Silicon Valley, says perks aren't the defining reason he and his colleagues do the work they do. But sometimes, he says, when you're working long hours, perks like free food feel like a necessity. DEEPAK RAO: To go out, drive, or whatever, go eat, come - that could take an hour and a half, which you might not have. MOSLEY: So for tech companies, it's been worth it to keep employees like Rao at work for as long as they can stay by providing food in-house. These new laws will change what's become a given in Silicon Valley work culture. The city of San Francisco is also considering a similar measure that would ban cafeterias in all new office buildings, forcing tech employees to venture out and share a bit of the wealth outside of their walls. For NPR News, I'm Tonya Mosley.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-12-637163104": {"title": "Election Security Becomes A Political Issue In Georgia Governor's Race : NPR", "url": "https://www.npr.org/2018/08/12/637163104/election-security-becomes-a-political-issue-in-georgia-governors-race", "author": "No author found", "published_date": "2018-08-12", "content": "LULU GARCIA-NAVARRO, HOST:  And now onto election security, which is a huge campaign issue ahead of this year's midterms. In Georgia, the Republican candidate for governor is also secretary of state. The top election official, Brian Kemp, insists Georgia's elections are secure. But as Johnny Kauffman of member station WABE reports, Democrats want immediate changes. (APPLAUSE)CAROLINE STOVER: Thank you. And again, we're here at Manuel's Tavern in Atlanta. And we do have people from all over Georgia tuning in. So welcome, everyone. JOHNNY KAUFFMAN, BYLINE: Caroline Stover, a Democratic activist, speaks to about 100 people in the back room of a wood-paneled bar as they sip beers and eat tater tots. They came to learn about Georgia's election system. And what they hear from Stover and others is that the system is not secure. STOVER: Here we are three months before an important election, living with the reality that, once again, Georgia voters might be going to the polls and not know if your vote actually counts. KAUFFMAN: One big topic at Manuel's Tavern is paper ballots. A handful of voters filed an ongoing federal lawsuit pushing the state to use them in the midterms. Democrats like Stover want Republican secretary of state and candidate for governor Brian Kemp to order a switch to paper ballots himself. STOVER: Enough is enough. And we simply can't wait any longer for, for example, our secretary of state. KAUFFMAN: Georgia is one of 14 states that uses electronic voting machines without a paper trail or paper ballots. Cybersecurity experts agree this leaves elections more vulnerable. Worst-case scenario - hackers manipulate Georgia's vote totals. And there's no paper backup to do a manual recount. That could mean chaos. Kemp isn't opposed to paper ballots. But he says a switch to them shouldn't happen before the 2018 election. BRIAN KEMP: And that would be an absolute disaster - changing from the current system that we have now to paper ballots. KAUFFMAN: Kemp has faced questions about the security of Georgia's elections. Ahead of the 2016 contest, an outside researcher found voter information and passwords unsecured on a state contractor's website. After Russian hacking attempts were revealed, Kemp turned down assistance from the Department of Homeland Security. A few months later, he accused DHS itself of hacking the state's network. That proved to be false. Kemp insists Georgia's elections are secure. KEMP: I think, if anything, my record is a strength for me. I mean, I'm glad to talk about that record all day long. KAUFFMAN: Kemp notes added firewalls and regular work with cybersecurity vendors. Before he won the Republican nomination, some in Kemp's party questioned his record. They're more quiet now. Kemp's opponent, Democrat Stacey Abrams, hasn't criticized him much. But other Democrats have. ELIZABETH STARLING: I think he's not only failing in his duty, but he shouldn't have it. He should not have that responsibility anymore. KAUFFMAN: Back at the bar, Democrat Elizabeth Starling says Kemp has no business overseeing any election as secretary of state when he's on the ballot himself. STARLING: I'm infuriated. KAUFFMAN: The Georgia Democratic Party this week called for Kemp to resign. While some previous secretaries of state running for higher office here have stepped down, Kemp says he has no plans to. Marian Schneider says election administration in the U. S. is often run by partisans. But today, she worries it will hurt the country. Schneider is president of Verified Voting, a nonprofit that advocates for paper ballots. MARIAN SCHNEIDER: This issue of election administration and election security is not a political issue. It's a national security issue. These are national security concerns. KAUFFMAN: Schneider says everyone should still vote. As candidates in Georgia fight to win those votes, they'll also be arguing about whether the technology used to cast and count them can be trusted. For NPR News, I'm Johnny Kauffman in Atlanta. LULU GARCIA-NAVARRO, HOST:   And now onto election security, which is a huge campaign issue ahead of this year's midterms. In Georgia, the Republican candidate for governor is also secretary of state. The top election official, Brian Kemp, insists Georgia's elections are secure. But as Johnny Kauffman of member station WABE reports, Democrats want immediate changes. (APPLAUSE) CAROLINE STOVER: Thank you. And again, we're here at Manuel's Tavern in Atlanta. And we do have people from all over Georgia tuning in. So welcome, everyone. JOHNNY KAUFFMAN, BYLINE: Caroline Stover, a Democratic activist, speaks to about 100 people in the back room of a wood-paneled bar as they sip beers and eat tater tots. They came to learn about Georgia's election system. And what they hear from Stover and others is that the system is not secure. STOVER: Here we are three months before an important election, living with the reality that, once again, Georgia voters might be going to the polls and not know if your vote actually counts. KAUFFMAN: One big topic at Manuel's Tavern is paper ballots. A handful of voters filed an ongoing federal lawsuit pushing the state to use them in the midterms. Democrats like Stover want Republican secretary of state and candidate for governor Brian Kemp to order a switch to paper ballots himself. STOVER: Enough is enough. And we simply can't wait any longer for, for example, our secretary of state. KAUFFMAN: Georgia is one of 14 states that uses electronic voting machines without a paper trail or paper ballots. Cybersecurity experts agree this leaves elections more vulnerable. Worst-case scenario - hackers manipulate Georgia's vote totals. And there's no paper backup to do a manual recount. That could mean chaos. Kemp isn't opposed to paper ballots. But he says a switch to them shouldn't happen before the 2018 election. BRIAN KEMP: And that would be an absolute disaster - changing from the current system that we have now to paper ballots. KAUFFMAN: Kemp has faced questions about the security of Georgia's elections. Ahead of the 2016 contest, an outside researcher found voter information and passwords unsecured on a state contractor's website. After Russian hacking attempts were revealed, Kemp turned down assistance from the Department of Homeland Security. A few months later, he accused DHS itself of hacking the state's network. That proved to be false. Kemp insists Georgia's elections are secure. KEMP: I think, if anything, my record is a strength for me. I mean, I'm glad to talk about that record all day long. KAUFFMAN: Kemp notes added firewalls and regular work with cybersecurity vendors. Before he won the Republican nomination, some in Kemp's party questioned his record. They're more quiet now. Kemp's opponent, Democrat Stacey Abrams, hasn't criticized him much. But other Democrats have. ELIZABETH STARLING: I think he's not only failing in his duty, but he shouldn't have it. He should not have that responsibility anymore. KAUFFMAN: Back at the bar, Democrat Elizabeth Starling says Kemp has no business overseeing any election as secretary of state when he's on the ballot himself. STARLING: I'm infuriated. KAUFFMAN: The Georgia Democratic Party this week called for Kemp to resign. While some previous secretaries of state running for higher office here have stepped down, Kemp says he has no plans to. Marian Schneider says election administration in the U. S. is often run by partisans. But today, she worries it will hurt the country. Schneider is president of Verified Voting, a nonprofit that advocates for paper ballots. MARIAN SCHNEIDER: This issue of election administration and election security is not a political issue. It's a national security issue. These are national security concerns. KAUFFMAN: Schneider says everyone should still vote. As candidates in Georgia fight to win those votes, they'll also be arguing about whether the technology used to cast and count them can be trusted. For NPR News, I'm Johnny Kauffman in Atlanta.", "section": "Elections", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-13-637312256": {"title": "Impact Of Facebook's False Posts Is Difficult To Measure : NPR", "url": "https://www.npr.org/2018/08/13/637312256/impact-of-facebooks-false-posts-is-difficult-to-measure", "author": "No author found", "published_date": "2018-08-13", "content": "", "section": "News", "disclaimer": ""}, "2018-08-13-638176030": {"title": "Elon Musk Says Saudi Investment Fund Could Help Him Take Tesla Private : NPR", "url": "https://www.npr.org/2018/08/13/638176030/elon-musk-says-saudi-investment-fund-could-help-him-take-tesla-private", "author": "No author found", "published_date": "2018-08-13", "content": "", "section": "Business", "disclaimer": ""}, "2018-08-13-633997148": {"title": "The Relentless Pace Of Satisfying Fans Is Burning Out Some YouTube Stars : NPR", "url": "https://www.npr.org/2018/08/13/633997148/the-relentless-pace-of-satisfying-fans-is-burning-out-some-youtube-stars", "author": "No author found", "published_date": "2018-08-13", "content": "MARY LOUISE KELLY, HOST: This month on All Tech Considered, we examine the gap between how we portray ourselves online and who we really are. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")KELLY: The story of Hollywood is rife with stories of celebrities who broke down under pressure to perform and please their fans. Well, that is happening to some YouTube stars these days. As NPR's Laura Sydell reports, the pressure has reached new heights in the age of social media algorithms. LAURA SYDELL, BYLINE: Being a YouTube star sounds glamorous to a lot of young people - fame, money, fun. ELLE MILLS: Being a YouTuber is, like, my childhood dream. SYDELL: And at 20 years old, Elle Mills has achieved her dream. She has more than 1. 3 million subscribers to her YouTube channel. Her 2 1/2- to 6-minute videos are characterized by a self-deprecating humor somewhere between Lena Dunham in \"Girls\" and a Woody Allen film. Here she is after an attempt to levitate her childhood home. (SOUNDBITE OF YOUTUBE VIDEO, \"I TURNED MY HOUSE INTO 'UP'\"MILLS: Getting a group of teenagers to blow up 4,000 balloons and put them on my roof might have been a bit too ambitious. Let's just say the end result was a bit underwhelming. SYDELL: Mills makes a living from selling her branded merchandise - hoodies and T-shirts with her signature logos, Canada's maple leaf - she's Canadian - a soda pop bottle and a camera. She had deals with Samsung and Wendy's, and she tours to meet fans. That's on top of producing the videos which can take 60-plus hours to shoot, produce and edit. While TV and film stars can take a break when the filming is over, Mills feels the pressure to post every week so she can keep her fans engaged. MILLS: I definitely was feeling drained because I was trying to pump out an amazing video once every two weeks or, like, once every week. And that's just not attainable, especially with the hours that goes into each video. SYDELL: It was not just the 60 to 70 hours a week that burnt out Issa Tweimeh, known as issa twaimz to his viewers. He dropped out of college at 19 to pursue YouTube full-time. He enjoyed making up silly songs like this one. (SOUNDBITE OF YOUTUBE VIDEO, \"THE LLAMA SONG\")ISSA TWEIMEH: (Singing) Happy llama, sad llama, mentally-disturbed llama, super llama, drama llama, big fat mama llama. . . SYDELL: That video got over 33 million views. Twaimz is now 23. He's changed over four years. He's been grappling with his sexuality. He's more interested in putting a positive message into the world than just getting clicks. TWEIMEH: I felt like people wanted me to do this one thing, and I was growing up and getting older. And I was like, I think I'm getting too old to be doing the same thing over and over again. SYDELL: Twaimz fell into a depression. He still wanted to make videos. But he wanted to change his style, and the relentless pace of YouTube didn't give him time to do that. Taking a break from your own YouTube channel can be risky. Jon Brence, the director of talent at Fullscreen, helps manage careers for YouTube stars. Brence says he's noticed that YouTube's algorithm favors those who post regularly. JON BRENCE: So if you're not actively creating or if you're going on a trip and you haven't actively created content to publish during said trip, you will go effectively back to the back of the line. SYDELL: In a statement, YouTube says it does not program to favor people who post more often. However, viewers on YouTube may prefer channels that post more often, and that does impact what the algorithm favors. Still, the company knows there's a problem. It's even got a whole section on YouTube where creators can get information about burnout and watch videos like this one about noticing the signs of it. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: Do you ever feel like you just can't create anymore? Well, that would be one of the signs of burnout. SYDELL: YouTube also offers suggestions on how to avoid burnout and live a balanced life. Despite the risks of losing some of her audience, Elle Mills says she had to take a break. MILLS: To be honest, at that point, I needed that break so bad that it didn't really matter. SYDELL: In a moment of desperation, she turned to her fans to explain it to them. (SOUNDBITE OF YOUTUBE VIDEO, \"BURNT OUT AT 19\")MILLS: Now, I don't want to worry anyone. I am getting the help I need, and I have a bunch of people looking after me. And I will be putting my mental health first for a bit. SYDELL: Talent manager Brence says for certain stars like Mills, turning to her fans helps. BRENCE: She has a fan base that cares about her first and foremost. And when she's saying, I'm going to take a break, they're willing to take the break with her. SYDELL: But Brence says there are other stars who are much more dependent on the algorithm. Brence says some stars just burn bright and then burn out after two years, and you never hear from them again. Elle Mills says after a month off traveling and talking with friends and family, she's back and feeling better. On her return, Mills once again opened up to her fans. (SOUNDBITE OF ARCHIVED RECORDING)MILLS: As long as I'm having fun and I'm passionate about what I'm making, everything's going to be all right. And now I finally feel content again. SYDELL: And Mills says her fans have come back along with her. Laura Sydell, NPR News. (SOUNDBITE OF BOUND'S \"DO MAKE SAY THINK\") MARY LOUISE KELLY, HOST:  This month on All Tech Considered, we examine the gap between how we portray ourselves online and who we really are. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") KELLY: The story of Hollywood is rife with stories of celebrities who broke down under pressure to perform and please their fans. Well, that is happening to some YouTube stars these days. As NPR's Laura Sydell reports, the pressure has reached new heights in the age of social media algorithms. LAURA SYDELL, BYLINE: Being a YouTube star sounds glamorous to a lot of young people - fame, money, fun. ELLE MILLS: Being a YouTuber is, like, my childhood dream. SYDELL: And at 20 years old, Elle Mills has achieved her dream. She has more than 1. 3 million subscribers to her YouTube channel. Her 2 1/2- to 6-minute videos are characterized by a self-deprecating humor somewhere between Lena Dunham in \"Girls\" and a Woody Allen film. Here she is after an attempt to levitate her childhood home. (SOUNDBITE OF YOUTUBE VIDEO, \"I TURNED MY HOUSE INTO 'UP'\" MILLS: Getting a group of teenagers to blow up 4,000 balloons and put them on my roof might have been a bit too ambitious. Let's just say the end result was a bit underwhelming. SYDELL: Mills makes a living from selling her branded merchandise - hoodies and T-shirts with her signature logos, Canada's maple leaf - she's Canadian - a soda pop bottle and a camera. She had deals with Samsung and Wendy's, and she tours to meet fans. That's on top of producing the videos which can take 60-plus hours to shoot, produce and edit. While TV and film stars can take a break when the filming is over, Mills feels the pressure to post every week so she can keep her fans engaged. MILLS: I definitely was feeling drained because I was trying to pump out an amazing video once every two weeks or, like, once every week. And that's just not attainable, especially with the hours that goes into each video. SYDELL: It was not just the 60 to 70 hours a week that burnt out Issa Tweimeh, known as issa twaimz to his viewers. He dropped out of college at 19 to pursue YouTube full-time. He enjoyed making up silly songs like this one. (SOUNDBITE OF YOUTUBE VIDEO, \"THE LLAMA SONG\") ISSA TWEIMEH: (Singing) Happy llama, sad llama, mentally-disturbed llama, super llama, drama llama, big fat mama llama. . . SYDELL: That video got over 33 million views. Twaimz is now 23. He's changed over four years. He's been grappling with his sexuality. He's more interested in putting a positive message into the world than just getting clicks. TWEIMEH: I felt like people wanted me to do this one thing, and I was growing up and getting older. And I was like, I think I'm getting too old to be doing the same thing over and over again. SYDELL: Twaimz fell into a depression. He still wanted to make videos. But he wanted to change his style, and the relentless pace of YouTube didn't give him time to do that. Taking a break from your own YouTube channel can be risky. Jon Brence, the director of talent at Fullscreen, helps manage careers for YouTube stars. Brence says he's noticed that YouTube's algorithm favors those who post regularly. JON BRENCE: So if you're not actively creating or if you're going on a trip and you haven't actively created content to publish during said trip, you will go effectively back to the back of the line. SYDELL: In a statement, YouTube says it does not program to favor people who post more often. However, viewers on YouTube may prefer channels that post more often, and that does impact what the algorithm favors. Still, the company knows there's a problem. It's even got a whole section on YouTube where creators can get information about burnout and watch videos like this one about noticing the signs of it. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: Do you ever feel like you just can't create anymore? Well, that would be one of the signs of burnout. SYDELL: YouTube also offers suggestions on how to avoid burnout and live a balanced life. Despite the risks of losing some of her audience, Elle Mills says she had to take a break. MILLS: To be honest, at that point, I needed that break so bad that it didn't really matter. SYDELL: In a moment of desperation, she turned to her fans to explain it to them. (SOUNDBITE OF YOUTUBE VIDEO, \"BURNT OUT AT 19\") MILLS: Now, I don't want to worry anyone. I am getting the help I need, and I have a bunch of people looking after me. And I will be putting my mental health first for a bit. SYDELL: Talent manager Brence says for certain stars like Mills, turning to her fans helps. BRENCE: She has a fan base that cares about her first and foremost. And when she's saying, I'm going to take a break, they're willing to take the break with her. SYDELL: But Brence says there are other stars who are much more dependent on the algorithm. Brence says some stars just burn bright and then burn out after two years, and you never hear from them again. Elle Mills says after a month off traveling and talking with friends and family, she's back and feeling better. On her return, Mills once again opened up to her fans. (SOUNDBITE OF ARCHIVED RECORDING) MILLS: As long as I'm having fun and I'm passionate about what I'm making, everything's going to be all right. And now I finally feel content again. SYDELL: And Mills says her fans have come back along with her. Laura Sydell, NPR News. (SOUNDBITE OF BOUND'S \"DO MAKE SAY THINK\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-14-638591076": {"title": "Tinder Co-Founders Sue App's Owners For At Least $2B, Saying They Were 'Cheated' : NPR", "url": "https://www.npr.org/2018/08/14/638591076/tinder-co-founders-sue-apps-owners-for-at-least-2b-saying-they-were-cheated", "author": "No author found", "published_date": "2018-08-14", "content": "", "section": "Technology", "disclaimer": ""}, "2018-08-14-638512558": {"title": "Nope, Those Aren't Mailboxes: Paris Rolls Out Sidewalk Urinals : NPR", "url": "https://www.npr.org/2018/08/14/638512558/nope-those-arent-mailboxes-paris-rolls-out-sidewalk-urinals", "author": "No author found", "published_date": "2018-08-14", "content": "", "section": "Strange News", "disclaimer": ""}, "2018-08-15-638849245": {"title": "Alex Jones Penalized By Twitter : NPR", "url": "https://www.npr.org/2018/08/15/638849245/alex-jones-penalized-by-twitter", "author": "No author found", "published_date": "2018-08-15", "content": "", "section": "Technology", "disclaimer": ""}, "2018-08-18-639822885": {"title": "Hunting For Russian Trolls (Online) : NPR", "url": "https://www.npr.org/2018/08/18/639822885/hunting-for-russian-trolls-online", "author": "No author found", "published_date": "2018-08-18", "content": "JENNIFER LUDDEN, HOST: As we approach the midterm elections, the U. S. government warns there is potential for Russian interference. Intelligence agencies and technology companies are on alert for social media trolls - and so is Josh Russell. By day, he's a systems analyst. But when he goes home at night, he tracks malicious Russian social media accounts. For Russell, the fight is personal. He knows he was influenced by these accounts during the 2016 campaign. Russell says he didn't know much about Donald Trump then. But when he scrolled through Facebook and read it, there were a lot of disturbing stories about Hillary Clinton. JOSH RUSSELL: A lot of them were - had to do with people in her orbit having died or the Clinton Foundation being corrupt or things that she had done in Haiti or something like that. LUDDEN: It was the claim that Clinton had been personally responsible for people's deaths that set off alarm bells for Russell. So he decided to do some fact-checking. RUSSELL: I came to the realization pretty quickly that someone may have been probably lying to me on social media. LUDDEN: It turned out there were many false stories circulating that year. But Russell was skeptical of claims that Russia had anything to do with them until lists of Russian troll accounts began to appear online. He was shocked. RUSSELL: My god, they're right. They're exactly right. What they've been telling me about this is literally happening. I can go look this stuff up. It's right there. LUDDEN: Russell became obsessed. He spent the last two years uncovering other Russian bots. How? RUSSELL: It takes quite a bit of work, so. . . LUDDEN: At night, after his kids are in bed, Russell investigates whether known Russian accounts have shared content on other platforms. He posts the results of his sleuthing online, and he's often one step ahead of tech journalists and social media companies themselves. He hopes to help others be skeptical about what they see online. RUSSELL: You're never going to be able to crack down on all of the propaganda or disinformation or misinformation on all these social media platforms. So the more people that you can get to know what it looks like, the easier it is to fight back with actual real information. LUDDEN: That's Josh Russell, systems analyst and amateur Russian troll hunter. (SOUNDBITE OF JASSANOVA'S \"FADE OUT\") JENNIFER LUDDEN, HOST:  As we approach the midterm elections, the U. S. government warns there is potential for Russian interference. Intelligence agencies and technology companies are on alert for social media trolls - and so is Josh Russell. By day, he's a systems analyst. But when he goes home at night, he tracks malicious Russian social media accounts. For Russell, the fight is personal. He knows he was influenced by these accounts during the 2016 campaign. Russell says he didn't know much about Donald Trump then. But when he scrolled through Facebook and read it, there were a lot of disturbing stories about Hillary Clinton. JOSH RUSSELL: A lot of them were - had to do with people in her orbit having died or the Clinton Foundation being corrupt or things that she had done in Haiti or something like that. LUDDEN: It was the claim that Clinton had been personally responsible for people's deaths that set off alarm bells for Russell. So he decided to do some fact-checking. RUSSELL: I came to the realization pretty quickly that someone may have been probably lying to me on social media. LUDDEN: It turned out there were many false stories circulating that year. But Russell was skeptical of claims that Russia had anything to do with them until lists of Russian troll accounts began to appear online. He was shocked. RUSSELL: My god, they're right. They're exactly right. What they've been telling me about this is literally happening. I can go look this stuff up. It's right there. LUDDEN: Russell became obsessed. He spent the last two years uncovering other Russian bots. How? RUSSELL: It takes quite a bit of work, so. . . LUDDEN: At night, after his kids are in bed, Russell investigates whether known Russian accounts have shared content on other platforms. He posts the results of his sleuthing online, and he's often one step ahead of tech journalists and social media companies themselves. He hopes to help others be skeptical about what they see online. RUSSELL: You're never going to be able to crack down on all of the propaganda or disinformation or misinformation on all these social media platforms. So the more people that you can get to know what it looks like, the easier it is to fight back with actual real information. LUDDEN: That's Josh Russell, systems analyst and amateur Russian troll hunter. (SOUNDBITE OF JASSANOVA'S \"FADE OUT\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-19-640002304": {"title": "HUD Hits Facebook For Allowing Housing Discrimination  : NPR", "url": "https://www.npr.org/2018/08/19/640002304/hud-hits-facebook-for-allowing-housing-discrimination", "author": "No author found", "published_date": "2018-08-19", "content": "", "section": "National", "disclaimer": ""}, "2018-08-20-640205975": {"title": "iPad Battery Malfunction Leads To Apple Store Evacuation In Amsterdam : NPR", "url": "https://www.npr.org/2018/08/20/640205975/ipad-battery-malfunction-leads-to-apple-store-evacuation-in-amsterdam", "author": "No author found", "published_date": "2018-08-20", "content": "", "section": "World", "disclaimer": ""}, "2018-08-21-640696857": {"title": "Facebook Shuts 652 Iran-Backed Accounts Linked In Global Disinformation Campaign : NPR", "url": "https://www.npr.org/2018/08/21/640696857/facebook-shuts-down-652-iranian-backed-and-some-russian-backed-accounts", "author": "No author found", "published_date": "2018-08-21", "content": "AILSA CHANG, HOST: We're learning more today about ongoing cyberattacks against political targets in the United States. We've been talking a lot about how Russia has been involved in disinformation campaigns, but now they're not the only state actor that has been accused by a major U. S. tech company of these types of efforts. Facebook said tonight that it has discovered and disrupted an Iranian disinformation campaign on its network. That is in addition to a new Russian disinformation campaign. NPR political reporter Tim Mak is on this story, and he joins us now. Welcome. TIM MAK, BYLINE: Thank you. CHANG: So can you just start us off by telling us, exactly what did Facebook discover tonight? MAK: So Facebook says it has taken down a coordinated network of pages originating in Iran and targeting users in the Middle East, in Latin America, the U. K. and the United States. And so this campaign involves some 652 pages with close to a million Facebook followers and thousands of dollars of advertising buys. So these Facebook and Instagram accounts - they did a number of things. They pretended to be an independent Iranian media organization while actually linking back to Iranian state media. In other parts of the network, they falsely posed as news and civil society organizations to spread political disinformation. And this campaign also involved more traditional kind of cyberattacks. This is like attempted hacking and efforts to spread what we call malware. The network of accounts also pushed narratives that include anti-Saudi, anti-Israeli, pro-Palestinian themes as well as support for specific American policies favorable to Iran. CHANG: So how did Facebook find out that these campaigns were even happening? MAK: It's interesting. They were originally tipped off not by kind of their internal security processes but by FireEye, a third-party cybersecurity firm. And so Facebook launched an investigation, and they revealed hundreds of these Iranian-linked accounts. Facebook has also been emphasizing its work with American law enforcement, and it shared the information about this disinformation campaign that it has discovered with the U. K. and American governments. Also because Iran is currently subject to some pretty strict sanctions, Facebook has briefed the Treasury and State Departments on what they found through the investigation that they announced this evening. CHANG: Now, hasn't much of this kind of activity been connected to Russia usually? MAK: Yeah, we've talked a lot about. . . CHANG: Yes. MAK: . . . Russian disinformation. CHANG: Over and over again. MAK: And what it looks like now is that the Iranian campaign took a page out of Russia's playbook, that, you know - and we know that not - that Facebook tonight announced not just an Iranian campaign because that - I mean, that is what is new about. . . CHANG: Right. MAK: . . . What we're learning today. But they also announced that it's - that Facebook has disrupted new Russian accounts. I mean, we talked three weeks ago when Facebook shut down more than 30 Russian accounts. It had a massive scope and following that seems similar to what Iranians have done in this case. Separately, Microsoft in a separate announcement - they said that they had shut down an attempted attack on conservative think tanks and the U. S. government by a Russian hacking group called Fancy Bear. So here's Senator Lindsey Graham at a cybersecurity hearing this afternoon. (SOUNDBITE OF ARCHIVED RECORDING)LINDSEY GRAHAM: America is under cyberattack. We're beginning to act but not quick enough and not forcefully enough. CHANG: Does it seem like there will be more cyberattacks going forward this year? I mean, are you getting any sense of that in your reporting? MAK: Yeah. Senator Mark Warner said that there's a lot of tech companies lining up essentially to disrupt some of these campaigns by state actors. We'll see that over and over again probably in the coming months as we approach the midterm elections. We're - what we're worried about is not only Russian cyber operations, not only Iranian cyber operations but possibly from North Korea and China as well. CHANG: That's NPR's Tim Mak. Thank you. MAK: Thanks a lot. AILSA CHANG, HOST:  We're learning more today about ongoing cyberattacks against political targets in the United States. We've been talking a lot about how Russia has been involved in disinformation campaigns, but now they're not the only state actor that has been accused by a major U. S. tech company of these types of efforts. Facebook said tonight that it has discovered and disrupted an Iranian disinformation campaign on its network. That is in addition to a new Russian disinformation campaign. NPR political reporter Tim Mak is on this story, and he joins us now. Welcome. TIM MAK, BYLINE: Thank you. CHANG: So can you just start us off by telling us, exactly what did Facebook discover tonight? MAK: So Facebook says it has taken down a coordinated network of pages originating in Iran and targeting users in the Middle East, in Latin America, the U. K. and the United States. And so this campaign involves some 652 pages with close to a million Facebook followers and thousands of dollars of advertising buys. So these Facebook and Instagram accounts - they did a number of things. They pretended to be an independent Iranian media organization while actually linking back to Iranian state media. In other parts of the network, they falsely posed as news and civil society organizations to spread political disinformation. And this campaign also involved more traditional kind of cyberattacks. This is like attempted hacking and efforts to spread what we call malware. The network of accounts also pushed narratives that include anti-Saudi, anti-Israeli, pro-Palestinian themes as well as support for specific American policies favorable to Iran. CHANG: So how did Facebook find out that these campaigns were even happening? MAK: It's interesting. They were originally tipped off not by kind of their internal security processes but by FireEye, a third-party cybersecurity firm. And so Facebook launched an investigation, and they revealed hundreds of these Iranian-linked accounts. Facebook has also been emphasizing its work with American law enforcement, and it shared the information about this disinformation campaign that it has discovered with the U. K. and American governments. Also because Iran is currently subject to some pretty strict sanctions, Facebook has briefed the Treasury and State Departments on what they found through the investigation that they announced this evening. CHANG: Now, hasn't much of this kind of activity been connected to Russia usually? MAK: Yeah, we've talked a lot about. . . CHANG: Yes. MAK: . . . Russian disinformation. CHANG: Over and over again. MAK: And what it looks like now is that the Iranian campaign took a page out of Russia's playbook, that, you know - and we know that not - that Facebook tonight announced not just an Iranian campaign because that - I mean, that is what is new about. . . CHANG: Right. MAK: . . . What we're learning today. But they also announced that it's - that Facebook has disrupted new Russian accounts. I mean, we talked three weeks ago when Facebook shut down more than 30 Russian accounts. It had a massive scope and following that seems similar to what Iranians have done in this case. Separately, Microsoft in a separate announcement - they said that they had shut down an attempted attack on conservative think tanks and the U. S. government by a Russian hacking group called Fancy Bear. So here's Senator Lindsey Graham at a cybersecurity hearing this afternoon. (SOUNDBITE OF ARCHIVED RECORDING) LINDSEY GRAHAM: America is under cyberattack. We're beginning to act but not quick enough and not forcefully enough. CHANG: Does it seem like there will be more cyberattacks going forward this year? I mean, are you getting any sense of that in your reporting? MAK: Yeah. Senator Mark Warner said that there's a lot of tech companies lining up essentially to disrupt some of these campaigns by state actors. We'll see that over and over again probably in the coming months as we approach the midterm elections. We're - what we're worried about is not only Russian cyber operations, not only Iranian cyber operations but possibly from North Korea and China as well. CHANG: That's NPR's Tim Mak. Thank you. MAK: Thanks a lot.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-21-637122361": {"title": "How Smartphone Apps Could Change The Way Sexual Assault Is Reported : NPR", "url": "https://www.npr.org/2018/08/21/637122361/how-smartphone-apps-could-change-the-way-sexual-assault-is-reported", "author": "No author found", "published_date": "2018-08-21", "content": "AUDIE CORNISH, HOST: By most accounts, the majority of people who experience sexual harassment or assault never report what happened to them. High tech may have a solution - apps that can help survivors come forward. NPR's Tovia Smith reports. TOVIA SMITH, BYLINE: It's easy to understand the many reasons why alleged victims don't want to report. There's the worry they won't be believed, fear of reprisals and embarrassment. UNIDENTIFIED PERSON: That was really hard for me to think about - telling another person, especially someone you've never really had contact with before. SMITH: This woman, who asked that her name not be used to protect her privacy, says she was sexually harassed at college by another student. And even though it was making her depressed and anxious, she couldn't bring herself to tell authorities at school. UNIDENTIFIED PERSON: Because I was afraid of being blamed for it. SMITH: It wasn't until spring break when she was back in the comfort of her own home, literally under her covers, that she mustered up the resolve to finally get it out not face to face but through an app on her laptop. UNIDENTIFIED PERSON: It was almost like building courage behind a computer screen because nobody is judging you at that moment. So you can get it off your chest without questioning if somebody's going to believe you. SMITH: The app she used called Callisto lets users send an encrypted report directly to school officials or to keep it on hold until they're ready. Users can also choose to hold a report unless and until someone else accuses the same person. That's big for survivors who fear going it alone and for those who may question whether what happened was just a misunderstanding or a one-time misstep, as Ryan Soscia once did. Years after he says he was molested as a kid, Soscia says he reported it just recently on a brand-new app he developed himself called JDoe. RYAN SOSCIA: You can either pretend it didn't happen, or you could say, I'm going to make sure you pay for this. SMITH: Soscia says the real inspiration for his app came a few years ago at a high school graduation party when a friend revealed he'd been molested. Within hours, Soscia says nearly 10 others disclosed the same person did the same thing to them. And only after realizing their strength in numbers did his friends go to the police. That got Soscia thinking about how technology could help ensure those kinds of discoveries are no longer left to serendipity. SOSCIA: We can find those connections exponentially faster. So the hope is we're going to be able to prevent these types of crimes from happening. And the idea that that could have stopped this from happening to 10 other people - that's really powerful. SMITH: While the Callisto app can only be used by people whose colleges or companies buy in for what could be $10,000 to $30,000 a year, the JDoe model allows anyone to use the app for free. Funding comes instead from a stable of attorneys who pay anywhere from $1,000 to $20,000 a year for access to potentially lucrative civil cases. The lawyers all agree to take the cases on contingency. JDoe also gets a kicker if they win. But clients pay nothing upfront. JAMES HALL: It's a win-win-win. SMITH: Attorney James Hall was among the first to sign up. He says survivors end up with stronger cases. Instead of he said, she said, they're he said, they said. Lawyers get better odds of winning and bigger payouts. And defendants, Hall says, get reassurance that bogus claims will be screened out. HALL: If it's a frivolous case, most lawyers are going to recognize that there will not be any money at the end of the day, that it's not going to be profitable. It's going to waste your time and should not be brought. SMITH: But others worry that getting everyone to lawyer up from the get-go might hurt more than it helps. Peter Cappelli, a professor of management at The Wharton School, also questions the basic concept of survivors deciding whether to report based on what others have done. PETER CAPPELLI: I think what you want is to tell people the criteria are policy related. They're not personally related. And you should bring forward anything that fits the criteria and not, you know, whether you feel enough other people have made the complaint or not. SMITH: Cappelli and others also raise concerns about due process. CYNTHIA GARRETT: You know, it's a permanent registry of unsubstantiated #MeToo-style accusations. And the reports will remain even if a student was found not responsible. SMITH: Cynthia Garrett is with a group that represents accused students called Families Advocating for Campus Equality. She worries that complaints can be filed without the accused even knowing. Then it would take only one new claim to match with the old unfounded or untried ones, she says, to ruin a career. GARRETT: And when somebody down the road makes this complaint for, oh, God, he made a sexual joke that was inappropriate and the app shows up, oh, there's been previous reports, an employer will interpret that as more than it may have been. But the problem is that decades later, they have no way to defend themselves. SMITH: Developers dismiss those concerns, saying reporting apps are just a kind of tip line for allegations that will ultimately be vetted by humans. As the student who reported her harassment by app says, the apps are neither judge nor jury. UNIDENTIFIED PERSON: It's not a verdict. That's where a campus comes in. They can do a full-blown investigation. And if the investigation comes out as false, then it comes out as false. And if it comes out as true, then we take further action. SMITH: In her case, school officials issued a mutual no-contact order, and the harassment stopped. If her only reporting option was walking into a stranger's office in some campus administration building, this student says, she probably would still be suffering in silence. Tovia Smith, NPR News. (SOUNDBITE OF RACHEL BOYD'S \"BACK IN YOUR BOX\") AUDIE CORNISH, HOST:  By most accounts, the majority of people who experience sexual harassment or assault never report what happened to them. High tech may have a solution - apps that can help survivors come forward. NPR's Tovia Smith reports. TOVIA SMITH, BYLINE: It's easy to understand the many reasons why alleged victims don't want to report. There's the worry they won't be believed, fear of reprisals and embarrassment. UNIDENTIFIED PERSON: That was really hard for me to think about - telling another person, especially someone you've never really had contact with before. SMITH: This woman, who asked that her name not be used to protect her privacy, says she was sexually harassed at college by another student. And even though it was making her depressed and anxious, she couldn't bring herself to tell authorities at school. UNIDENTIFIED PERSON: Because I was afraid of being blamed for it. SMITH: It wasn't until spring break when she was back in the comfort of her own home, literally under her covers, that she mustered up the resolve to finally get it out not face to face but through an app on her laptop. UNIDENTIFIED PERSON: It was almost like building courage behind a computer screen because nobody is judging you at that moment. So you can get it off your chest without questioning if somebody's going to believe you. SMITH: The app she used called Callisto lets users send an encrypted report directly to school officials or to keep it on hold until they're ready. Users can also choose to hold a report unless and until someone else accuses the same person. That's big for survivors who fear going it alone and for those who may question whether what happened was just a misunderstanding or a one-time misstep, as Ryan Soscia once did. Years after he says he was molested as a kid, Soscia says he reported it just recently on a brand-new app he developed himself called JDoe. RYAN SOSCIA: You can either pretend it didn't happen, or you could say, I'm going to make sure you pay for this. SMITH: Soscia says the real inspiration for his app came a few years ago at a high school graduation party when a friend revealed he'd been molested. Within hours, Soscia says nearly 10 others disclosed the same person did the same thing to them. And only after realizing their strength in numbers did his friends go to the police. That got Soscia thinking about how technology could help ensure those kinds of discoveries are no longer left to serendipity. SOSCIA: We can find those connections exponentially faster. So the hope is we're going to be able to prevent these types of crimes from happening. And the idea that that could have stopped this from happening to 10 other people - that's really powerful. SMITH: While the Callisto app can only be used by people whose colleges or companies buy in for what could be $10,000 to $30,000 a year, the JDoe model allows anyone to use the app for free. Funding comes instead from a stable of attorneys who pay anywhere from $1,000 to $20,000 a year for access to potentially lucrative civil cases. The lawyers all agree to take the cases on contingency. JDoe also gets a kicker if they win. But clients pay nothing upfront. JAMES HALL: It's a win-win-win. SMITH: Attorney James Hall was among the first to sign up. He says survivors end up with stronger cases. Instead of he said, she said, they're he said, they said. Lawyers get better odds of winning and bigger payouts. And defendants, Hall says, get reassurance that bogus claims will be screened out. HALL: If it's a frivolous case, most lawyers are going to recognize that there will not be any money at the end of the day, that it's not going to be profitable. It's going to waste your time and should not be brought. SMITH: But others worry that getting everyone to lawyer up from the get-go might hurt more than it helps. Peter Cappelli, a professor of management at The Wharton School, also questions the basic concept of survivors deciding whether to report based on what others have done. PETER CAPPELLI: I think what you want is to tell people the criteria are policy related. They're not personally related. And you should bring forward anything that fits the criteria and not, you know, whether you feel enough other people have made the complaint or not. SMITH: Cappelli and others also raise concerns about due process. CYNTHIA GARRETT: You know, it's a permanent registry of unsubstantiated #MeToo-style accusations. And the reports will remain even if a student was found not responsible. SMITH: Cynthia Garrett is with a group that represents accused students called Families Advocating for Campus Equality. She worries that complaints can be filed without the accused even knowing. Then it would take only one new claim to match with the old unfounded or untried ones, she says, to ruin a career. GARRETT: And when somebody down the road makes this complaint for, oh, God, he made a sexual joke that was inappropriate and the app shows up, oh, there's been previous reports, an employer will interpret that as more than it may have been. But the problem is that decades later, they have no way to defend themselves. SMITH: Developers dismiss those concerns, saying reporting apps are just a kind of tip line for allegations that will ultimately be vetted by humans. As the student who reported her harassment by app says, the apps are neither judge nor jury. UNIDENTIFIED PERSON: It's not a verdict. That's where a campus comes in. They can do a full-blown investigation. And if the investigation comes out as false, then it comes out as false. And if it comes out as true, then we take further action. SMITH: In her case, school officials issued a mutual no-contact order, and the harassment stopped. If her only reporting option was walking into a stranger's office in some campus administration building, this student says, she probably would still be suffering in silence. Tovia Smith, NPR News. (SOUNDBITE OF RACHEL BOYD'S \"BACK IN YOUR BOX\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-21-640493486": {"title": "Microsoft Says It Thwarted Attack By Russian Hackers : NPR", "url": "https://www.npr.org/2018/08/21/640493486/microsoft-says-it-thwarted-attack-by-russian-hackers", "author": "No author found", "published_date": "2018-08-21", "content": "DAVID GREENE, HOST: Well, this morning we can put aside any lingering doubts we had about whether Russia is still trying to meddle in U. S. politics. A group with Kremlin ties that tried to influence the 2016 election is at it again. This time, their targets include conservative think tanks. The group is APT28, also known as Fancy Bear. In 2016, they were blamed for, among other things, targeting Hillary Clinton's campaign. This latest attack was announced by Microsoft, which said early this morning it had discovered and disabled six misleading websites set up by Fancy Bear. And we are joined right now by Microsoft's president and chief legal officer, Brad Smith. Mr. Smith, thanks for coming on the program. I know it's probably been a busy morning for you. BRAD SMITH: No, thank you. GREENE: Can you just start by telling me about one of these fake sites? What did it look like, and what do you think this Russian group was trying to do with it? SMITH: Well, what this group does is it registers for a fake domain name, but it chooses a name that is designed to look like a real site. It then makes it look like an Internet support site for an organization like a conservative think tank, in this instance. It would then send emails to board members or employees of, say, this think tank telling them that there is a problem with their email account, and they need to go to this site to address it. When they get to the site, they see, typically, a page that looks just like a page of their employer where they work. They're asked to enter their password. And then their credentials are harvested, so to speak. This group has their password. It's able to access their email account. It's able to get into the entity's network and start finding other documents. And that is, in essence, what was done repeatedly in 2016 in the United States. It's what was attempted in the French presidential election last year, as well. GREENE: And so it's the same model they seem to be using. So does it look like this is somehow politically motivated? I mean, if this group was trying to go after people who were logging onto conservative websites, does that mean they're - this group is now trying to focus on Republicans or conservatives here? SMITH: Well, we - it clearly suggests they're focusing on conservative groups. One of the groups is the International Republican Institute. It has six Republican senators among the board members. So clearly, there is an effort here to focus on and target these Republican groups. Obviously, we can't speak for what exactly was intended, but I think we can conclude that it wasn't for the advancement of American democracy. GREENE: That looks like it's clear. Well, I - how closely did you work with U. S. intelligence on this? SMITH: We cooperate closely with the law enforcement and government agencies here in the United States and a number of other countries all the time. This particular effort that we undertook was undertaken by Microsoft. We have a world-leading threat intelligence service ourselves. We identified these sites. We went to court. It's the 12th time we've been able to get - go to court, get an order. And then that enables us to transfer control of these kinds of sites to our own digital crimes unit. GREENE: And you did that in time, right? I mean, there's. . . SMITH: So we took action ourselves. GREENE: There's no evidence that they were successful in getting any logging credentials or actually successfully phishing, right? SMITH: And that's an important point. In this particular instance, we believe we were able to act quickly enough that these specific sites were not used successfully. And both the International Republican Institute, the Hudson Institute have responded, you know, very quickly, very strongly. And we're now working with a new effort to - across the board, to help secure candidates and campaigns with an eye to this November's elections. GREENE: You mentioned that you have shut down some of these sites before. You've never come out so publicly with a real public relations push to talk about what you're doing. Talk to me about the timing of that. I mean, the first thing I wondered was whether you regret maybe not doing enough to protect users in the past, and you really want to be out there in front of this saying, you know, hey, we're doing a lot here. SMITH: Well, I think we're all coming to the conclusion that the kinds of Russian-sponsored attacks that we started to see in 2016 have been even broader than we first thought. That's across the tech sector. That's across this country. And if you're going to stand up successfully and defend a democracy against these kinds of foreign attacks, we need to bring people together. And we can only bring people together if everyone is in the know about what's going on. And this is an important moment across political parties, across the political community and with the tech sector, where, with more knowledge, we can start to take broader and more systemic steps to respond to these kinds of problems. GREENE: They sound like very lofty goals for a company to have, to play such a sizable role in, you know, as you say, protecting democracy. SMITH: Well, I would say two things. We are in the business every day of protecting our customers. That's what we do for consumers. That's what we do for large enterprise customers. With an account guard initiative we're launching today, we're providing politicians and campaigns and political parties and think tanks with the kind of customer protection that our largest customers would get. And we're providing it at no extra cost. And I think that does reflect the second aspect. It's really the point you allude to. When we live in a democracy, we can't take it for granted. We all have an individual responsibility to step up and do what it takes. GREENE: And we'll have to, sadly, leave it there, Brad Smith. I apologize. Microsoft's president and chief legal officer. Thanks. SMITH: OK. Thank you. DAVID GREENE, HOST:  Well, this morning we can put aside any lingering doubts we had about whether Russia is still trying to meddle in U. S. politics. A group with Kremlin ties that tried to influence the 2016 election is at it again. This time, their targets include conservative think tanks. The group is APT28, also known as Fancy Bear. In 2016, they were blamed for, among other things, targeting Hillary Clinton's campaign. This latest attack was announced by Microsoft, which said early this morning it had discovered and disabled six misleading websites set up by Fancy Bear. And we are joined right now by Microsoft's president and chief legal officer, Brad Smith. Mr. Smith, thanks for coming on the program. I know it's probably been a busy morning for you. BRAD SMITH: No, thank you. GREENE: Can you just start by telling me about one of these fake sites? What did it look like, and what do you think this Russian group was trying to do with it? SMITH: Well, what this group does is it registers for a fake domain name, but it chooses a name that is designed to look like a real site. It then makes it look like an Internet support site for an organization like a conservative think tank, in this instance. It would then send emails to board members or employees of, say, this think tank telling them that there is a problem with their email account, and they need to go to this site to address it. When they get to the site, they see, typically, a page that looks just like a page of their employer where they work. They're asked to enter their password. And then their credentials are harvested, so to speak. This group has their password. It's able to access their email account. It's able to get into the entity's network and start finding other documents. And that is, in essence, what was done repeatedly in 2016 in the United States. It's what was attempted in the French presidential election last year, as well. GREENE: And so it's the same model they seem to be using. So does it look like this is somehow politically motivated? I mean, if this group was trying to go after people who were logging onto conservative websites, does that mean they're - this group is now trying to focus on Republicans or conservatives here? SMITH: Well, we - it clearly suggests they're focusing on conservative groups. One of the groups is the International Republican Institute. It has six Republican senators among the board members. So clearly, there is an effort here to focus on and target these Republican groups. Obviously, we can't speak for what exactly was intended, but I think we can conclude that it wasn't for the advancement of American democracy. GREENE: That looks like it's clear. Well, I - how closely did you work with U. S. intelligence on this? SMITH: We cooperate closely with the law enforcement and government agencies here in the United States and a number of other countries all the time. This particular effort that we undertook was undertaken by Microsoft. We have a world-leading threat intelligence service ourselves. We identified these sites. We went to court. It's the 12th time we've been able to get - go to court, get an order. And then that enables us to transfer control of these kinds of sites to our own digital crimes unit. GREENE: And you did that in time, right? I mean, there's. . . SMITH: So we took action ourselves. GREENE: There's no evidence that they were successful in getting any logging credentials or actually successfully phishing, right? SMITH: And that's an important point. In this particular instance, we believe we were able to act quickly enough that these specific sites were not used successfully. And both the International Republican Institute, the Hudson Institute have responded, you know, very quickly, very strongly. And we're now working with a new effort to - across the board, to help secure candidates and campaigns with an eye to this November's elections. GREENE: You mentioned that you have shut down some of these sites before. You've never come out so publicly with a real public relations push to talk about what you're doing. Talk to me about the timing of that. I mean, the first thing I wondered was whether you regret maybe not doing enough to protect users in the past, and you really want to be out there in front of this saying, you know, hey, we're doing a lot here. SMITH: Well, I think we're all coming to the conclusion that the kinds of Russian-sponsored attacks that we started to see in 2016 have been even broader than we first thought. That's across the tech sector. That's across this country. And if you're going to stand up successfully and defend a democracy against these kinds of foreign attacks, we need to bring people together. And we can only bring people together if everyone is in the know about what's going on. And this is an important moment across political parties, across the political community and with the tech sector, where, with more knowledge, we can start to take broader and more systemic steps to respond to these kinds of problems. GREENE: They sound like very lofty goals for a company to have, to play such a sizable role in, you know, as you say, protecting democracy. SMITH: Well, I would say two things. We are in the business every day of protecting our customers. That's what we do for consumers. That's what we do for large enterprise customers. With an account guard initiative we're launching today, we're providing politicians and campaigns and political parties and think tanks with the kind of customer protection that our largest customers would get. And we're providing it at no extra cost. And I think that does reflect the second aspect. It's really the point you allude to. When we live in a democracy, we can't take it for granted. We all have an individual responsibility to step up and do what it takes. GREENE: And we'll have to, sadly, leave it there, Brad Smith. I apologize. Microsoft's president and chief legal officer. Thanks. SMITH: OK. Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-21-640453576": {"title": "Microsoft Says Russian Operation Targeted U.S. Conservative Groups As Midterms Loom : NPR", "url": "https://www.npr.org/2018/08/21/640453576/microsoft-says-russian-operation-targeted-u-s-political-groups-as-midterms-loom", "author": "No author found", "published_date": "2018-08-21", "content": "", "section": "Technology", "disclaimer": ""}, "2018-08-21-639646651": {"title": "WATCH: Self-Driving Cars Need To Learn How Humans Drive : NPR", "url": "https://www.npr.org/2018/08/21/639646651/watch-self-driving-cars-need-to-learn-how-humans-drive", "author": "No author found", "published_date": "2018-08-21", "content": "", "section": "VIDEOS: Maddie About Science", "disclaimer": ""}, "2018-08-22-640815074": {"title": "Verizon Throttled Firefighters' Data As Mendocino Wildfire Raged, Fire Chief Says : NPR", "url": "https://www.npr.org/2018/08/22/640815074/verizon-throttled-firefighters-data-as-mendocino-wildfire-raged-fire-chief-says", "author": "No author found", "published_date": "2018-08-22", "content": "", "section": "National", "disclaimer": ""}, "2018-08-22-640900988": {"title": "Details Of Uber Harassment Settlement Released : NPR", "url": "https://www.npr.org/2018/08/22/640900988/dozens-sued-uber-for-harassment-heres-what-they-re-set-to-receive", "author": "No author found", "published_date": "2018-08-22", "content": "", "section": "Technology", "disclaimer": ""}, "2018-08-22-640793647": {"title": "Facebook Shuts Down Hundreds Of Accounts Backed By Iran, Russia : NPR", "url": "https://www.npr.org/2018/08/22/640793647/facebook-shuts-down-hundreds-of-accounts-backed-by-iran-russia", "author": "No author found", "published_date": "2018-08-22", "content": "DAVID GREENE, HOST: So in the past year or so, we have been talking about disinformation campaigns waged on social media that were associated with Russia. Well, now we are learning that other countries might be taking a page out of that playbook. Facebook shared details of a sprawling campaign out of Iran to spread misinformation all over the world. And NPR's Alina Selyukh has been following the latest here. Hi, Alina. ALINA SELYUKH, BYLINE: Hi. Good morning. GREENE: Good morning. God, the focus has really been on Russia for so long and the U. S. midterms in American politics. Now we're talking about Iran. What exactly is Facebook announcing here? SELYUKH: This is new, right? So Facebook says it has shut down a massive disinformation campaign that originated from Iran and targeted users not just in the U. K. or the U. S. , where Facebook is particularly widely used, but also in the Middle East and Latin America. Altogether, Facebook says it took down 652 accounts, pages and groups - and that's on Facebook and the sister site Instagram - that were all coordinated, falsely posing as news or civil society organizations, spreading political disinformation, going back almost a decade, running thousands of dollars' worth of ads boasting in English, Arabic and Farsi. And altogether, they had almost a million followers. GREENE: Wow. So this is huge, global reach going on for, like, 10 years. I mean, how is Facebook just learning about it now? SELYUKH: So this is definitely the first announcement of this kind of magnitude about a campaign out of Iran from Facebook. To add to that, actually, Twitter also followed with its own suspension of almost 300 accounts - also coordinated manipulation, also allegedly from Iran. But to your question, Facebook says it got tipped off on a key part of this by cybersecurity firm FireEye. And FireEye, in its own report on this, says that this was a network of accounts essentially, pushing policies that are favorable to Iran - you know, narratives that include anti-Saudi themes, anti-Israeli and pro-Palestinian themes, support for specific U. S. policies favorable to Iran like the U. S. -Iran nuclear deal. And FireEye points out that what all this demonstrates is that as much as our discourse here in the U. S. has focused on Russia and how its influence campaigns may have factored into the 2016 election, we're now seeing new actors - nation-states - getting into this world of Internet-driven, social-media-driven influence operations to shape political discourse. GREENE: But you can't forget about Russia. I mean, even this week - I mean, we had Brad Smith, the president of Microsoft, on the program yesterday talking about his company finding new fraudulent websites coming from Russia. SELYUKH: Right. So actually, in this new announcement from Facebook, the company is still trying to sort out what Russia is doing on the network. In the latest batch, Facebook said it actually did also shut down more accounts linked to the Russian military intelligence services. We don't know how many. Facebook said that the campaigns originating from Iran and Russia did not seem to be linked or coordinated with each other but did use similar tactics with sort of networks of misleading accounts. GREENE: But this is interesting. This feels like it could be a new phase when you have to focus not just on American politics but on just broader disinformation campaigns. Like, did this Iran stuff have anything to do with the midterm elections here? SELYUKH: Not directly, no. The tech companies are certainly mindful of the timing with the elections coming up in November. They are eager to show that they're getting aggressive on cybercrime. But to your point, FireEye says that the massive Iranian disinformation campaign did feature some Trump-themed messaging and some liberal-leaning messaging but in general didn't seem designed to influence the 2018 elections in the U. S. And on Russia, similarly, an executive at another cybersecurity firm, CrowdStrike, told me that he, too, has not seen any significant activity targeted at the midterms in terms of disinformation. All that said, all this goes to shows that essentially in the matter of a decade, we went from social media being something fun that you do with your friends or. . . GREENE: Right. SELYUKH: . . . Follow the news to now having a hearing in the Senate Intelligence Committee about social media and disinformation globally. And that hearing is on September 5. GREENE: Yeah. Amazing to think how - where we have come with social media. NPR's Alina Selyukh. Thanks a lot. SELYUKH: Thank you. DAVID GREENE, HOST:  So in the past year or so, we have been talking about disinformation campaigns waged on social media that were associated with Russia. Well, now we are learning that other countries might be taking a page out of that playbook. Facebook shared details of a sprawling campaign out of Iran to spread misinformation all over the world. And NPR's Alina Selyukh has been following the latest here. Hi, Alina. ALINA SELYUKH, BYLINE: Hi. Good morning. GREENE: Good morning. God, the focus has really been on Russia for so long and the U. S. midterms in American politics. Now we're talking about Iran. What exactly is Facebook announcing here? SELYUKH: This is new, right? So Facebook says it has shut down a massive disinformation campaign that originated from Iran and targeted users not just in the U. K. or the U. S. , where Facebook is particularly widely used, but also in the Middle East and Latin America. Altogether, Facebook says it took down 652 accounts, pages and groups - and that's on Facebook and the sister site Instagram - that were all coordinated, falsely posing as news or civil society organizations, spreading political disinformation, going back almost a decade, running thousands of dollars' worth of ads boasting in English, Arabic and Farsi. And altogether, they had almost a million followers. GREENE: Wow. So this is huge, global reach going on for, like, 10 years. I mean, how is Facebook just learning about it now? SELYUKH: So this is definitely the first announcement of this kind of magnitude about a campaign out of Iran from Facebook. To add to that, actually, Twitter also followed with its own suspension of almost 300 accounts - also coordinated manipulation, also allegedly from Iran. But to your question, Facebook says it got tipped off on a key part of this by cybersecurity firm FireEye. And FireEye, in its own report on this, says that this was a network of accounts essentially, pushing policies that are favorable to Iran - you know, narratives that include anti-Saudi themes, anti-Israeli and pro-Palestinian themes, support for specific U. S. policies favorable to Iran like the U. S. -Iran nuclear deal. And FireEye points out that what all this demonstrates is that as much as our discourse here in the U. S. has focused on Russia and how its influence campaigns may have factored into the 2016 election, we're now seeing new actors - nation-states - getting into this world of Internet-driven, social-media-driven influence operations to shape political discourse. GREENE: But you can't forget about Russia. I mean, even this week - I mean, we had Brad Smith, the president of Microsoft, on the program yesterday talking about his company finding new fraudulent websites coming from Russia. SELYUKH: Right. So actually, in this new announcement from Facebook, the company is still trying to sort out what Russia is doing on the network. In the latest batch, Facebook said it actually did also shut down more accounts linked to the Russian military intelligence services. We don't know how many. Facebook said that the campaigns originating from Iran and Russia did not seem to be linked or coordinated with each other but did use similar tactics with sort of networks of misleading accounts. GREENE: But this is interesting. This feels like it could be a new phase when you have to focus not just on American politics but on just broader disinformation campaigns. Like, did this Iran stuff have anything to do with the midterm elections here? SELYUKH: Not directly, no. The tech companies are certainly mindful of the timing with the elections coming up in November. They are eager to show that they're getting aggressive on cybercrime. But to your point, FireEye says that the massive Iranian disinformation campaign did feature some Trump-themed messaging and some liberal-leaning messaging but in general didn't seem designed to influence the 2018 elections in the U. S. And on Russia, similarly, an executive at another cybersecurity firm, CrowdStrike, told me that he, too, has not seen any significant activity targeted at the midterms in terms of disinformation. All that said, all this goes to shows that essentially in the matter of a decade, we went from social media being something fun that you do with your friends or. . . GREENE: Right. SELYUKH: . . . Follow the news to now having a hearing in the Senate Intelligence Committee about social media and disinformation globally. And that hearing is on September 5. GREENE: Yeah. Amazing to think how - where we have come with social media. NPR's Alina Selyukh. Thanks a lot. SELYUKH: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-23-641359735": {"title": "FireEye Manager Discusses How Cybersecurity Firm Revealed Propaganda Campaign : NPR", "url": "https://www.npr.org/2018/08/23/641359735/fireeye-manager-discusses-how-cybersecurity-firm-revealed-propaganda-campaign", "author": "No author found", "published_date": "2018-08-23", "content": "AILSA CHANG, HOST: We're about to hear from one of the people responsible for revealing an alleged Iranian propaganda campaign on social media sites, including Facebook, Twitter and Google. Tech companies removed hundreds of accounts this week linked to that operation, some with ties to state-owned media in Iran. The cybersecurity company FireEye exposed all of this. Lee Foster manages the information operations analysis team at the company, and he joins us in the studio. Welcome. LEE FOSTER: Thank you for having me. CHANG: So tell us how you discovered this Iranian campaign. FOSTER: So we run a dedicated information operations analysis team which is focused on trying to uncover these kinds of influence operations. We saw a number of social media personas purporting to be American left-leaning liberals but kind of pushing commentary that, you know, was related to in particular Israel and Palestine and so on. CHANG: Yeah. FOSTER: And so when we see kind of unusual conversations like that, we like to look further at the kind of source of the content. So from there we were able to actually look at this Liberty Front Press site which claims to be kind of an independent media entity but doesn't have any real detail about who it is, who operates it and so on - so very suspicious. And then from there we just kind of unraveled the broader network of activity. CHANG: That's fascinating. I understand that FireEye does not believe the Iranian campaign was actually targeting the midterm elections here in the U. S. , but are you seeing an uptick in cyberattacks in these months leading up to November? FOSTER: So in the context of the Iranian operation we uncovered, we don't assess it was designed specifically for the purposes of targeting the midterm elections for the reasons that this activity started up at least since last summer. And Facebook's announcement kind of talked about how in fact they identified accounts perhaps going back even as far as 2011. CHANG: Wow. FOSTER: Coupled with the fact that there was a very broad geographic diversity in terms of the focus of some of these accounts and sites. We found sites and social media accounts seemingly dedicated to pushing content to audiences not only in the U. S. but the U. K. , the Middle East and Latin America. CHANG: Now, Facebook has also shut down hundreds of accounts connected to Russian intelligence operations. You guys flagged this Iranian campaign. Does it look like the Iranian campaign took a cue from the Russians or there was any coordination? What did you guys find? FOSTER: We don't see any evidence of coordination. But the findings that we revealed are significant because it does demonstrate there are other actors beyond Russia. . . CHANG: Yeah. FOSTER: . . . That appear to see value in pursuing these kinds of online influence campaigns. CHANG: Now, in the past couple days we've seen so many announcements by big tech firms saying they've discovered these various plots. How much more is out there do you think that hasn't been discovered? FOSTER: I mean, that's very difficult to quantify, right? CHANG: (Laughter) We don't know what we don't know. FOSTER: Who knows, right? When we discovered this activity, it's not like we were out there looking for an Iranian operation. CHANG: Right. FOSTER: It's something we stumbled into. The thing about this activity is the barriers to entry are relatively low, right? It doesn't cost, you know, much money if any money to set up a handful of false personas on social media, set up an inauthentic news site. The new sites that we uncovered were often just appropriating content from legitimate news sources, including Western media organizations, interspersing with that some seemingly original content. But really it doesn't take a nation-state to be able to engage in this kind of activity. CHANG: Do discoveries of these campaigns like this particular discovery your team just made - do they help inform you to be even more vigilant and to be able to detect more easily future campaigns? FOSTER: So the interesting thing about the campaign we've identified here is how similar it is to other campaigns in terms of the methods and tactics used. But the really kind of big, significant thing about our finding here is it demonstrates that there are actors beyond Russia that are engaging in this type of activity. And so it really highlights just how kind of broad this problem really can be. CHANG: Lee Foster is the manager of the information operations analysis team for the cybersecurity company FireEye. Thank you so much for coming into the studio today. FOSTER: Thank you for having me. AILSA CHANG, HOST:  We're about to hear from one of the people responsible for revealing an alleged Iranian propaganda campaign on social media sites, including Facebook, Twitter and Google. Tech companies removed hundreds of accounts this week linked to that operation, some with ties to state-owned media in Iran. The cybersecurity company FireEye exposed all of this. Lee Foster manages the information operations analysis team at the company, and he joins us in the studio. Welcome. LEE FOSTER: Thank you for having me. CHANG: So tell us how you discovered this Iranian campaign. FOSTER: So we run a dedicated information operations analysis team which is focused on trying to uncover these kinds of influence operations. We saw a number of social media personas purporting to be American left-leaning liberals but kind of pushing commentary that, you know, was related to in particular Israel and Palestine and so on. CHANG: Yeah. FOSTER: And so when we see kind of unusual conversations like that, we like to look further at the kind of source of the content. So from there we were able to actually look at this Liberty Front Press site which claims to be kind of an independent media entity but doesn't have any real detail about who it is, who operates it and so on - so very suspicious. And then from there we just kind of unraveled the broader network of activity. CHANG: That's fascinating. I understand that FireEye does not believe the Iranian campaign was actually targeting the midterm elections here in the U. S. , but are you seeing an uptick in cyberattacks in these months leading up to November? FOSTER: So in the context of the Iranian operation we uncovered, we don't assess it was designed specifically for the purposes of targeting the midterm elections for the reasons that this activity started up at least since last summer. And Facebook's announcement kind of talked about how in fact they identified accounts perhaps going back even as far as 2011. CHANG: Wow. FOSTER: Coupled with the fact that there was a very broad geographic diversity in terms of the focus of some of these accounts and sites. We found sites and social media accounts seemingly dedicated to pushing content to audiences not only in the U. S. but the U. K. , the Middle East and Latin America. CHANG: Now, Facebook has also shut down hundreds of accounts connected to Russian intelligence operations. You guys flagged this Iranian campaign. Does it look like the Iranian campaign took a cue from the Russians or there was any coordination? What did you guys find? FOSTER: We don't see any evidence of coordination. But the findings that we revealed are significant because it does demonstrate there are other actors beyond Russia. . . CHANG: Yeah. FOSTER: . . . That appear to see value in pursuing these kinds of online influence campaigns. CHANG: Now, in the past couple days we've seen so many announcements by big tech firms saying they've discovered these various plots. How much more is out there do you think that hasn't been discovered? FOSTER: I mean, that's very difficult to quantify, right? CHANG: (Laughter) We don't know what we don't know. FOSTER: Who knows, right? When we discovered this activity, it's not like we were out there looking for an Iranian operation. CHANG: Right. FOSTER: It's something we stumbled into. The thing about this activity is the barriers to entry are relatively low, right? It doesn't cost, you know, much money if any money to set up a handful of false personas on social media, set up an inauthentic news site. The new sites that we uncovered were often just appropriating content from legitimate news sources, including Western media organizations, interspersing with that some seemingly original content. But really it doesn't take a nation-state to be able to engage in this kind of activity. CHANG: Do discoveries of these campaigns like this particular discovery your team just made - do they help inform you to be even more vigilant and to be able to detect more easily future campaigns? FOSTER: So the interesting thing about the campaign we've identified here is how similar it is to other campaigns in terms of the methods and tactics used. But the really kind of big, significant thing about our finding here is it demonstrates that there are actors beyond Russia that are engaging in this type of activity. And so it really highlights just how kind of broad this problem really can be. CHANG: Lee Foster is the manager of the information operations analysis team for the cybersecurity company FireEye. Thank you so much for coming into the studio today. FOSTER: Thank you for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-25-641835302": {"title": "What Closing A Government Radio Station Would Mean For Your Clocks : NPR", "url": "https://www.npr.org/2018/08/25/641835302/what-closing-a-government-radio-station-would-mean-for-your-clocks", "author": "No author found", "published_date": "2018-08-25", "content": "SCOTT SIMON, HOST: Shortwave listeners might recognize this signature ID. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: National Institute of Standards and Technology time - this is radio station WWV, Fort Collins, Colo. SIMON: WWV is the oldest continuously operating radio station in the United States. It's been on the air since 1920. It's signal provides a frequency standard for receivers. The time stamp is regulated by an atomic clock. But a 2019 budget proposal for NIST would close WWV, WWVH in Hawaii and WWVB, which syncs up the time for about 50 million radio-controlled clocks, wristwatches and appliances. Thomas Witherspoon wrote about this on his shortwave listener website, SWLing. com He joins us from the studios of the CBC in Quebec City - that's a lot of alphabet soup to get through in this intro. Thanks for being with us, Mr. Witherspoon. THOMAS WITHERSPOON: Oh, thank you very much. It's my pleasure. SIMON: So what would the effect of the closing of WWV be? WITHERSPOON: Well, these little WWVB receivers are embedded in lots of devices that look for accurate timing - clocks, watches, weather stations, even irrigation systems. So if the WWVB signal goes away, these devices will have to be changed manually. They're not going to update themselves. SIMON: Isn't that all taken care of on the Internet these days? I mean, we set the time according to what we see on our iPhones. I venture most Americans do. WITHERSPOON: Yeah, a lot of them do. But a lot of people think that devices are actually connecting to the Internet to get their time signal. But I've got an alarm clock next to my bed, for example. The little embedded receiver, they don't require a lot of resources. This kind of runs in the background and doesn't need the internet - doesn't need anything else. They kind of hum along. SIMON: Now, one of our producers spoke with the president of La Crosse Technology. They make a lot of these radio-controlled clocks that are found in schools and factory floors and homes. He says that he thinks Congress would never approve this cut because there are so many millions of devices. Does that reassure you? WITHERSPOON: That's nice to hear someone in industry saying that. But right now as the budget sits, it does cut out all of the WWV time stations. So if it goes through as proposed right now, it will be cut in 2019. SIMON: I think I know the answer because I used to listen to shortwave and haven't in years. It's all on the web now. What's the state of that as a hobby these days? WITHERSPOON: That's a really good question. So I am absolutely in love with the shortwaves. I'm an amateur radio operator, so I actually communicate over the shortwaves. I've been listening to shortwave radio since I was 8 years old. In fact, one of the very first things I heard on shortwave radio was WWV, when my father would set his watch manually to it from a little console radio in our living room. The state of shortwave radio right now - a lot of the large international broadcasters are dropping out of the scene. It's expensive to run shortwave radio stations. But there are surprisingly a lot of stations that are still out there that you can hear. The BBC World Service still broadcasts on shortwave - the Voice of America. You know, one of the reasons I love it so much is someone could be in a country under a repressive regime and listen to a shortwave radio, and there's no way the powers that be could actually track them. SIMON: Are you just being nostalgic about WWV. WITHERSPOON: (Laughter) I'm a nostalgic guy. So I'm always nostalgic about WWV. But I use them all the time. I mean, the thing is they're sort of the heartbeat of shortwave radio. When something goes wrong, you check the WWV to see if you're picking up their signal. And you know then everything's OK. So you know, maritime operators, military operators, amateur radio operators, we all listen to and use WWV stations regularly. SIMON: Thomas Witherspoon, from the radio blog SWLing. com, thanks so much for being with us. WITHERSPOON: Thank you very much, Scott. It was a pleasure. SCOTT SIMON, HOST:  Shortwave listeners might recognize this signature ID. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: National Institute of Standards and Technology time - this is radio station WWV, Fort Collins, Colo. SIMON: WWV is the oldest continuously operating radio station in the United States. It's been on the air since 1920. It's signal provides a frequency standard for receivers. The time stamp is regulated by an atomic clock. But a 2019 budget proposal for NIST would close WWV, WWVH in Hawaii and WWVB, which syncs up the time for about 50 million radio-controlled clocks, wristwatches and appliances. Thomas Witherspoon wrote about this on his shortwave listener website, SWLing. com He joins us from the studios of the CBC in Quebec City - that's a lot of alphabet soup to get through in this intro. Thanks for being with us, Mr. Witherspoon. THOMAS WITHERSPOON: Oh, thank you very much. It's my pleasure. SIMON: So what would the effect of the closing of WWV be? WITHERSPOON: Well, these little WWVB receivers are embedded in lots of devices that look for accurate timing - clocks, watches, weather stations, even irrigation systems. So if the WWVB signal goes away, these devices will have to be changed manually. They're not going to update themselves. SIMON: Isn't that all taken care of on the Internet these days? I mean, we set the time according to what we see on our iPhones. I venture most Americans do. WITHERSPOON: Yeah, a lot of them do. But a lot of people think that devices are actually connecting to the Internet to get their time signal. But I've got an alarm clock next to my bed, for example. The little embedded receiver, they don't require a lot of resources. This kind of runs in the background and doesn't need the internet - doesn't need anything else. They kind of hum along. SIMON: Now, one of our producers spoke with the president of La Crosse Technology. They make a lot of these radio-controlled clocks that are found in schools and factory floors and homes. He says that he thinks Congress would never approve this cut because there are so many millions of devices. Does that reassure you? WITHERSPOON: That's nice to hear someone in industry saying that. But right now as the budget sits, it does cut out all of the WWV time stations. So if it goes through as proposed right now, it will be cut in 2019. SIMON: I think I know the answer because I used to listen to shortwave and haven't in years. It's all on the web now. What's the state of that as a hobby these days? WITHERSPOON: That's a really good question. So I am absolutely in love with the shortwaves. I'm an amateur radio operator, so I actually communicate over the shortwaves. I've been listening to shortwave radio since I was 8 years old. In fact, one of the very first things I heard on shortwave radio was WWV, when my father would set his watch manually to it from a little console radio in our living room. The state of shortwave radio right now - a lot of the large international broadcasters are dropping out of the scene. It's expensive to run shortwave radio stations. But there are surprisingly a lot of stations that are still out there that you can hear. The BBC World Service still broadcasts on shortwave - the Voice of America. You know, one of the reasons I love it so much is someone could be in a country under a repressive regime and listen to a shortwave radio, and there's no way the powers that be could actually track them. SIMON: Are you just being nostalgic about WWV. WITHERSPOON: (Laughter) I'm a nostalgic guy. So I'm always nostalgic about WWV. But I use them all the time. I mean, the thing is they're sort of the heartbeat of shortwave radio. When something goes wrong, you check the WWV to see if you're picking up their signal. And you know then everything's OK. So you know, maritime operators, military operators, amateur radio operators, we all listen to and use WWV stations regularly. SIMON: Thomas Witherspoon, from the radio blog SWLing. com, thanks so much for being with us. WITHERSPOON: Thank you very much, Scott. It was a pleasure.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-26-642007977": {"title": "How Hurricane Harvey Harmed The Clocks : NPR", "url": "https://www.npr.org/2018/08/26/642007977/how-hurricane-harvey-harmed-the-clocks", "author": "No author found", "published_date": "2018-08-26", "content": "MELISSA BLOCK, HOST: At this moment last year, Hurricane Harvey was over Texas. The storm's rainfall inundated areas from Port Aransas to beyond Houston. The damage was intense, both to lives and property. Houston Public Media's Allison Lee has this look at one part of the recovery, one that's intricate and slow-going. (SOUNDBITE OF CLOCK CUCKOOING)ALLISON LEE, BYLINE: Old clocks that tick-tock and chime. Turns out a lot of people flooded by Harvey owned old clocks. And now many of them have ended up at Chappell Jordan Clock Galleries in Houston. (SOUNDBITE OF CLOCK CHIMING)RALPH POKLUDA: There's always something chiming here. People ask me, do you ever get used to this? I said, I don't even hear it. LEE: Ralph Pokluda has worked at the shop for 49 years. He says they're still getting what he calls Harvey clocks, and they're currently at a six-month turnaround time. As a native Houstonian, Pokluda says he's seen lots of storms. But he says Harvey was different. POKLUDA: This time, we quickly became overwhelmed. We ended up taking in not quite 200 grandfather clocks. And we took in wall and mantel clocks, too. So you can imagine a mantel clock sitting on the mantel - that that was inundated. . . LEE: He said some of those clocks were some of the few items people were able to save. And they're working longer hours to keep up with demand. POKLUDA: This is how we get next door. This - watch your step. LEE: Pokluda brings me to the workshop out back, where workers like Kevin Killian (ph) are winding gears. . . (SOUNDBITE OF WINDING GEARS)LEE: . . . And polishing brass. (SOUNDBITE OF POLISHING BRASS)KEVIN KILLIAN: Still seeing Harvey clocks. . . POKLUDA: Yeah. Is the thrill going and picking up Harvey clocks? KILLIAN: Yeah. But we're not picking too many of them up anymore. . . POKLUDA: Jeff (ph) just picked one up today. KILLIAN: Oh, he did? POKLUDA: Yeah. LEE: The shop's warehouse is packed with 150 broken grandfather clocks. Some will cost thousands to repair. But Pokluda says the businesses they rely on were also affected. POKLUDA: Dial painters are backlogged. The cabinet-makers are overwhelmed. LEE: One of the grandfather clocks in for repair came from Jerry Arnold's (ph) home that was flooded with 4 feet of water. He's still living in an apartment today. JERRY ARNOLD: I mean, everything was so soggy. LEE: His clock repair will cost around $3,500, about a thousand dollars more than it's worth. But Arnold says, for him, it's not about the money. ARNOLD: It means a lot just surviving this long. Maybe even if it wasn't a family clock, I would still take care of it because of where it's been, you know? I respect it. (SOUNDBITE OF CLOCK CHIMING)LEE: Back at the shop, Pokluda says bye to a customer who came in for an estimate. UNIDENTIFIED PERSON #1: All right. . . POKLUDA: Thank you, Ted (ph). UNIDENTIFIED PERSON #1: Thank you. UNIDENTIFIED PERSON #2: Thank you. LEE: The customer decided the cost wasn't worth the repair. UNIDENTIFIED PERSON #1: Keep those fingers nimble. POKLUDA: I will. LEE: Pokluda says he expects to be caught up on their workload by the end of the year. For NPR News, I'm Allison Lee in Houston. MELISSA BLOCK, HOST:  At this moment last year, Hurricane Harvey was over Texas. The storm's rainfall inundated areas from Port Aransas to beyond Houston. The damage was intense, both to lives and property. Houston Public Media's Allison Lee has this look at one part of the recovery, one that's intricate and slow-going. (SOUNDBITE OF CLOCK CUCKOOING) ALLISON LEE, BYLINE: Old clocks that tick-tock and chime. Turns out a lot of people flooded by Harvey owned old clocks. And now many of them have ended up at Chappell Jordan Clock Galleries in Houston. (SOUNDBITE OF CLOCK CHIMING) RALPH POKLUDA: There's always something chiming here. People ask me, do you ever get used to this? I said, I don't even hear it. LEE: Ralph Pokluda has worked at the shop for 49 years. He says they're still getting what he calls Harvey clocks, and they're currently at a six-month turnaround time. As a native Houstonian, Pokluda says he's seen lots of storms. But he says Harvey was different. POKLUDA: This time, we quickly became overwhelmed. We ended up taking in not quite 200 grandfather clocks. And we took in wall and mantel clocks, too. So you can imagine a mantel clock sitting on the mantel - that that was inundated. . . LEE: He said some of those clocks were some of the few items people were able to save. And they're working longer hours to keep up with demand. POKLUDA: This is how we get next door. This - watch your step. LEE: Pokluda brings me to the workshop out back, where workers like Kevin Killian (ph) are winding gears. . . (SOUNDBITE OF WINDING GEARS) LEE: . . . And polishing brass. (SOUNDBITE OF POLISHING BRASS) KEVIN KILLIAN: Still seeing Harvey clocks. . . POKLUDA: Yeah. Is the thrill going and picking up Harvey clocks? KILLIAN: Yeah. But we're not picking too many of them up anymore. . . POKLUDA: Jeff (ph) just picked one up today. KILLIAN: Oh, he did? POKLUDA: Yeah. LEE: The shop's warehouse is packed with 150 broken grandfather clocks. Some will cost thousands to repair. But Pokluda says the businesses they rely on were also affected. POKLUDA: Dial painters are backlogged. The cabinet-makers are overwhelmed. LEE: One of the grandfather clocks in for repair came from Jerry Arnold's (ph) home that was flooded with 4 feet of water. He's still living in an apartment today. JERRY ARNOLD: I mean, everything was so soggy. LEE: His clock repair will cost around $3,500, about a thousand dollars more than it's worth. But Arnold says, for him, it's not about the money. ARNOLD: It means a lot just surviving this long. Maybe even if it wasn't a family clock, I would still take care of it because of where it's been, you know? I respect it. (SOUNDBITE OF CLOCK CHIMING) LEE: Back at the shop, Pokluda says bye to a customer who came in for an estimate. UNIDENTIFIED PERSON #1: All right. . . POKLUDA: Thank you, Ted (ph). UNIDENTIFIED PERSON #1: Thank you. UNIDENTIFIED PERSON #2: Thank you. LEE: The customer decided the cost wasn't worth the repair. UNIDENTIFIED PERSON #1: Keep those fingers nimble. POKLUDA: I will. LEE: Pokluda says he expects to be caught up on their workload by the end of the year. For NPR News, I'm Allison Lee in Houston.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-27-642156809": {"title": "Jacksonville Shooting Witness Recalls 'People Crying' And 'Running For Their Lives'  : NPR", "url": "https://www.npr.org/2018/08/27/642156809/jacksonville-shooting-witness-recalls-people-crying-and-running-for-their-lives", "author": "No author found", "published_date": "2018-08-27", "content": "DAVID GREENE, HOST: In Florida, a man opened fire during a video game tournament yesterday. Two people were killed, and at least nine others were wounded in this shooting before authorities say the suspect shot himself. The suspect we're told was participating in the tournament, which featured the video game Madden Football. One of the gamers who ran for safety was Ryen Aleman, who traveled to Florida to play in the tournament. He is now back home in Texas. I spoke with him this morning and began by asking him to walk me through what happened yesterday. RYEN ALEMAN: I was playing the game, the Madden game, with my opponent. And this happened behind us, like, when the shooting was occurring. So I heard a pop. And we have headphones on. And I take off my headphones and look to the left, and everybody started running. So what I did was I dropped down and crawled to the restroom. And when I crawled to the restroom, like, the shooting was still occurring - like, multiple shots. And I went inside the restroom. And I just stayed in there. But like, I felt trapped. Like, if he comes in here and opens this door, then I'm just - I'm probably going to die 'cause there was no way of me running anywhere. So I guess, like, after, like, 25 or 30 shots, like, it had stopped. And I started hearing everybody, like, still running and crying and just saying, like, I'm shot; help me. At that time, I didn't know what to do. Like, I was still panicking. Like, I was - like, at that time I didn't know if the killer was still alive or if he was - like, if he probably and was going to come back and start shooting everybody again. So I just started panicking and panicking. And in my head, I was just saying, just run. Don't look anywhere else. Just try to get a taxi or something and just go straight to the airport. So that's what I did. I just opened the door, and I saw everybody running around. And I just started running to the closest taxi I seen. And I just went straight to the airport. GREENE: Now, Ryen Aleman told me that he knew the two people who were killed. ALEMAN: When you go to these tournaments, it's people from all around the, like, United States. And it's the top Madden competitors. Like, all we want to do is play against each other and, like, see where we're at against, like, the top Madden competitors. And, I mean, just that morning we're all just goofing around, seeing, like, who's going to win it all, who's going to take it today. I mean, just to see him - just to see both of them pass away is - I mean, I don't understand. GREENE: And are you doing OK? Are you getting help and support from people? ALEMAN: Yeah, from family, friends. GREENE: Well, we'll be thinking of you. I can't imagine going through something like this. And I know it takes a very long time to recover. So. . . ALEMAN: Yeah. GREENE: . . . We'll be thinking of you. And thank you for talking to us. ALEMAN: Thank you. GREENE: That was Ryen Aleman, one of the gamers who ran for safety yesterday during that shooting in Jacksonville, Fla. DAVID GREENE, HOST:  In Florida, a man opened fire during a video game tournament yesterday. Two people were killed, and at least nine others were wounded in this shooting before authorities say the suspect shot himself. The suspect we're told was participating in the tournament, which featured the video game Madden Football. One of the gamers who ran for safety was Ryen Aleman, who traveled to Florida to play in the tournament. He is now back home in Texas. I spoke with him this morning and began by asking him to walk me through what happened yesterday. RYEN ALEMAN: I was playing the game, the Madden game, with my opponent. And this happened behind us, like, when the shooting was occurring. So I heard a pop. And we have headphones on. And I take off my headphones and look to the left, and everybody started running. So what I did was I dropped down and crawled to the restroom. And when I crawled to the restroom, like, the shooting was still occurring - like, multiple shots. And I went inside the restroom. And I just stayed in there. But like, I felt trapped. Like, if he comes in here and opens this door, then I'm just - I'm probably going to die 'cause there was no way of me running anywhere. So I guess, like, after, like, 25 or 30 shots, like, it had stopped. And I started hearing everybody, like, still running and crying and just saying, like, I'm shot; help me. At that time, I didn't know what to do. Like, I was still panicking. Like, I was - like, at that time I didn't know if the killer was still alive or if he was - like, if he probably and was going to come back and start shooting everybody again. So I just started panicking and panicking. And in my head, I was just saying, just run. Don't look anywhere else. Just try to get a taxi or something and just go straight to the airport. So that's what I did. I just opened the door, and I saw everybody running around. And I just started running to the closest taxi I seen. And I just went straight to the airport. GREENE: Now, Ryen Aleman told me that he knew the two people who were killed. ALEMAN: When you go to these tournaments, it's people from all around the, like, United States. And it's the top Madden competitors. Like, all we want to do is play against each other and, like, see where we're at against, like, the top Madden competitors. And, I mean, just that morning we're all just goofing around, seeing, like, who's going to win it all, who's going to take it today. I mean, just to see him - just to see both of them pass away is - I mean, I don't understand. GREENE: And are you doing OK? Are you getting help and support from people? ALEMAN: Yeah, from family, friends. GREENE: Well, we'll be thinking of you. I can't imagine going through something like this. And I know it takes a very long time to recover. So. . . ALEMAN: Yeah. GREENE: . . . We'll be thinking of you. And thank you for talking to us. ALEMAN: Thank you. GREENE: That was Ryen Aleman, one of the gamers who ran for safety yesterday during that shooting in Jacksonville, Fla.", "section": "National", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-28-642730338": {"title": "Texas Company Will Send 3D-Printable Gun Files Directly To Customers : NPR", "url": "https://www.npr.org/2018/08/28/642730338/texas-company-will-send-3d-printable-gun-files-directly-to-customers", "author": "No author found", "published_date": "2018-08-28", "content": "", "section": "National", "disclaimer": ""}, "2018-08-28-641408278": {"title": "Scientists Turn To Harbor Seals To Help Improve Tracking Software : NPR", "url": "https://www.npr.org/2018/08/28/641408278/need-to-track-a-submarine-a-harbor-seal-can-show-you-how", "author": "No author found", "published_date": "2018-08-28", "content": "ARI SHAPIRO, HOST: Engineers in California have a new idea for how to track enemy submarines. Their idea was inspired by lessons learned from research on artificial intelligence and harbor seals, as NPR science correspondent Joe Palca reports. JOE PALCA, BYLINE: Just about anything moving through the water creates a flow pattern called a vortex. Ask Eva Kanso what a vortex is, and you get a pretty simple answer. EVA KANSO: It's a fluid that's moving around. That's what I mean by a vortex. PALCA: Kanso is a professor of aerospace and mechanical engineering at the University of Southern California. So let's say you see a vortex swirling in a murky pool of water. KANSO: How would you know what is it that created this vortex? PALCA: That's an academic question for Kanso, and she's spent some time working on the problem. But she says it's a very real, very practical question for a harbor seal. KANSO: The animal wants to understand. Is it a prey that's created this vortex, or is it the predator that created this flow pattern? PALCA: Turns out seals can figure that out. They can even track a tasty fish by the vortex it generates. A seal doesn't use its eyes to see the vortex. Instead, there's evidence it uses its whiskers as sensors that can measure the shape and intensity of the vortex. Now, Kanso is an engineer, not a marine biologist, but she figured there were lessons to be learned from the seals. KANSO: Going from local measurement, from what you can measure where you are with your senses, to extract information globally about the whole flow field - that's the question that's interesting to us. PALCA: In other words, she and her colleagues wanted to know how much they could deduce about what's moving around in a broad swath of water just from the flow patterns. Her approach was to use artificial intelligence. Just like a computer can be trained to recognize an apple from a collection of pixels in an image, she and her colleagues have trained a computer to recognize various objects based on the flow patterns they make in the water. For now the computer program isn't all that smart. It can only make sense of simple patterns. But she expects it will get better, and she sees a day when it could be useful for submarine hunters since submarines can leave distinctive flow patterns. KANSO: So they would be able to say, OK, I think there was a submarine that was passing through this location at this speed. PALCA: And maybe someday the computer could provide even more information. KANSO: OK, now that I sense this flow pattern and I can kind of guess who created it - and I can track that flow to its source. PALCA: Being able to track submarines this way is years off, but there's evidence someone believes it might be possible. The Navy helps pay for Eva Kanso's research. Joe Palca, NPR News. ARI SHAPIRO, HOST:  Engineers in California have a new idea for how to track enemy submarines. Their idea was inspired by lessons learned from research on artificial intelligence and harbor seals, as NPR science correspondent Joe Palca reports. JOE PALCA, BYLINE: Just about anything moving through the water creates a flow pattern called a vortex. Ask Eva Kanso what a vortex is, and you get a pretty simple answer. EVA KANSO: It's a fluid that's moving around. That's what I mean by a vortex. PALCA: Kanso is a professor of aerospace and mechanical engineering at the University of Southern California. So let's say you see a vortex swirling in a murky pool of water. KANSO: How would you know what is it that created this vortex? PALCA: That's an academic question for Kanso, and she's spent some time working on the problem. But she says it's a very real, very practical question for a harbor seal. KANSO: The animal wants to understand. Is it a prey that's created this vortex, or is it the predator that created this flow pattern? PALCA: Turns out seals can figure that out. They can even track a tasty fish by the vortex it generates. A seal doesn't use its eyes to see the vortex. Instead, there's evidence it uses its whiskers as sensors that can measure the shape and intensity of the vortex. Now, Kanso is an engineer, not a marine biologist, but she figured there were lessons to be learned from the seals. KANSO: Going from local measurement, from what you can measure where you are with your senses, to extract information globally about the whole flow field - that's the question that's interesting to us. PALCA: In other words, she and her colleagues wanted to know how much they could deduce about what's moving around in a broad swath of water just from the flow patterns. Her approach was to use artificial intelligence. Just like a computer can be trained to recognize an apple from a collection of pixels in an image, she and her colleagues have trained a computer to recognize various objects based on the flow patterns they make in the water. For now the computer program isn't all that smart. It can only make sense of simple patterns. But she expects it will get better, and she sees a day when it could be useful for submarine hunters since submarines can leave distinctive flow patterns. KANSO: So they would be able to say, OK, I think there was a submarine that was passing through this location at this speed. PALCA: And maybe someday the computer could provide even more information. KANSO: OK, now that I sense this flow pattern and I can kind of guess who created it - and I can track that flow to its source. PALCA: Being able to track submarines this way is years off, but there's evidence someone believes it might be possible. The Navy helps pay for Eva Kanso's research. Joe Palca, NPR News.", "section": "Joe's Big Idea", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-29-641744076": {"title": "Bugged At Work: How Secret Recordings Are Changing The Workplace : NPR", "url": "https://www.npr.org/2018/08/29/641744076/bugged-at-work-how-secret-recordings-are-changing-the-workplace", "author": "No author found", "published_date": "2018-08-29", "content": "", "section": "National", "disclaimer": ""}, "2018-08-29-643058414": {"title": "Dockless Scooters Gain Popularity And Scorn Across The U.S.  : NPR", "url": "https://www.npr.org/2018/08/29/643058414/dockless-scooters-gain-popularity-and-scorn-across-the-u-s", "author": "No author found", "published_date": "2018-08-29", "content": "", "section": "Here & Now Compass", "disclaimer": ""}, "2018-08-29-642937977": {"title": "Texas Nurse Loses Job After Apparently Posting About Patient In Anti-Vaxxer Group : NPR", "url": "https://www.npr.org/2018/08/29/642937977/texas-nurse-loses-job-after-apparently-posting-about-patient-in-anti-vaxxer-grou", "author": "No author found", "published_date": "2018-08-29", "content": "", "section": "Health Care", "disclaimer": ""}, "2018-08-30-642980896": {"title": "As Warming Climate Brings More Flash Floods, Austin Tries To Help Drivers  : NPR", "url": "https://www.npr.org/2018/08/30/642980896/cant-tell-where-it-s-flooded-look-at-your-phone-stay-safe", "author": "No author found", "published_date": "2018-08-30", "content": "AILSA CHANG, HOST:  The warming climate means more intense rain, and that means more flash floods. In Texas, officials hope that letting people see the rising waters on their smartphones can help keep them safe. From member station KUT in Austin, Mose Buchele reports. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: Fire department - what's the address of the emergency? MOSE BUCHELE, BYLINE: This is tape of a 911 call during one of Austin's many recent flash floods. This is known as the Halloween flood of 2013. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: I was driving and the water - the water was too fast. It's too much. I'm in the middle of the field. BUCHELE: The woman said she was driving to work and hit water. Her truck was washed into a field. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: I'm holding myself on a tree. UNIDENTIFIED PERSON #1: Ma'am. BUCHELE: It sounds like she says she's holding onto a tree. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: Ma'am, do you have anything that floats? BUCHELE: The call cuts off, but this woman appears to have survived. Five others died in that flood. In Texas, 75 percent of flash flood deaths happen on roads, often at low water crossings where cars are swept away by flooding creeks. So officials here are always trying to keep people off of dangerous roads in heavy rains. In Austin, they think they may have found a new way. MATT PORCHER: All right. So we are at Joe Tanner near 290. BUCHELE: This is Matt Porcher. He's with the city's flood early warning team. PORCHER: And this is kind of one of our frequent flyers for low water crossings. Every time we have heavy rainfall in south Austin, Joe Tanner will overtop from Williamson Creek. BUCHELE: He'd come here to put in a flood camera. As we talk, the truck arrives. The team breaks out a ladder, gets to work installing it about nine feet up a utility pole. From there, it will post images online so people can see creek conditions. The city already has flood gauges to measure rising water and a website to tell people about road closures. But Porcher and his team are betting on the power of the image to keep people from putting themselves at risk. PORCHER: Rather than someone having to drive up to this low water crossing and try to make a decision there - well, can I make it? It's only a couple inches of water - they can just pull up the creek camera, say, oh, it's flooded. So I need to find another alternate route. BUCHELE: They created a mobile-friendly website and are working on a way to alert people about specific crossings. Other flood-prone cities like Miami are also developing similar ways of warning people. But will sharing images really keep more people off the streets? NICHOLAS KMAN: Yeah, I think so. BUCHELE: This is Dr. Nicholas Kman. He's a medical manager for FEMA's Urban Search & Rescue team in Ohio. He's studied the rise of what are sometimes called disaster apps. He thinks pictures can be a powerful online tool. But he says this type of tech often relies on cellular service or Wi-Fi to get the information to the public. KMAN: A lot of times in a disaster, those things will go down. So if there's no cell service and you're relying on your cellphone to power the app or there's no Wi-Fi, then you're not going to be able to use it. BUCHELE: And in fact, spotty cellular coverage has been a challenge for the team at some low water crossings here in Austin but not on Joe Tanner Lane. After the camera's up, the team crowds around a laptop on the back of a pickup truck to see how the image uploads to the website. UNIDENTIFIED PERSON #3: That looks pretty good right there. PORCHER: Yeah, yeah. I'm happy. UNIDENTIFIED PERSON #4: Yeah. PORCHER: That's just perfect placement, so I don't even think we really need to tweak it. BUCHELE: There are now seven cameras posting photos online in Austin, and the city hopes to put up around 20 more by the end of the year. Porcher says they've also gotten calls from neighboring communities about installing them. He hopes one day there will be a statewide system of cameras trained on Texas creeks and rivers helping to keep people out of harm's way. For NPR News, I'm Mose Buchele in Austin. (SOUNDBITE OF SONG, \"BURNT BACK\") AILSA CHANG, HOST:   The warming climate means more intense rain, and that means more flash floods. In Texas, officials hope that letting people see the rising waters on their smartphones can help keep them safe. From member station KUT in Austin, Mose Buchele reports. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: Fire department - what's the address of the emergency? MOSE BUCHELE, BYLINE: This is tape of a 911 call during one of Austin's many recent flash floods. This is known as the Halloween flood of 2013. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #2: I was driving and the water - the water was too fast. It's too much. I'm in the middle of the field. BUCHELE: The woman said she was driving to work and hit water. Her truck was washed into a field. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #2: I'm holding myself on a tree. UNIDENTIFIED PERSON #1: Ma'am. BUCHELE: It sounds like she says she's holding onto a tree. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: Ma'am, do you have anything that floats? BUCHELE: The call cuts off, but this woman appears to have survived. Five others died in that flood. In Texas, 75 percent of flash flood deaths happen on roads, often at low water crossings where cars are swept away by flooding creeks. So officials here are always trying to keep people off of dangerous roads in heavy rains. In Austin, they think they may have found a new way. MATT PORCHER: All right. So we are at Joe Tanner near 290. BUCHELE: This is Matt Porcher. He's with the city's flood early warning team. PORCHER: And this is kind of one of our frequent flyers for low water crossings. Every time we have heavy rainfall in south Austin, Joe Tanner will overtop from Williamson Creek. BUCHELE: He'd come here to put in a flood camera. As we talk, the truck arrives. The team breaks out a ladder, gets to work installing it about nine feet up a utility pole. From there, it will post images online so people can see creek conditions. The city already has flood gauges to measure rising water and a website to tell people about road closures. But Porcher and his team are betting on the power of the image to keep people from putting themselves at risk. PORCHER: Rather than someone having to drive up to this low water crossing and try to make a decision there - well, can I make it? It's only a couple inches of water - they can just pull up the creek camera, say, oh, it's flooded. So I need to find another alternate route. BUCHELE: They created a mobile-friendly website and are working on a way to alert people about specific crossings. Other flood-prone cities like Miami are also developing similar ways of warning people. But will sharing images really keep more people off the streets? NICHOLAS KMAN: Yeah, I think so. BUCHELE: This is Dr. Nicholas Kman. He's a medical manager for FEMA's Urban Search & Rescue team in Ohio. He's studied the rise of what are sometimes called disaster apps. He thinks pictures can be a powerful online tool. But he says this type of tech often relies on cellular service or Wi-Fi to get the information to the public. KMAN: A lot of times in a disaster, those things will go down. So if there's no cell service and you're relying on your cellphone to power the app or there's no Wi-Fi, then you're not going to be able to use it. BUCHELE: And in fact, spotty cellular coverage has been a challenge for the team at some low water crossings here in Austin but not on Joe Tanner Lane. After the camera's up, the team crowds around a laptop on the back of a pickup truck to see how the image uploads to the website. UNIDENTIFIED PERSON #3: That looks pretty good right there. PORCHER: Yeah, yeah. I'm happy. UNIDENTIFIED PERSON #4: Yeah. PORCHER: That's just perfect placement, so I don't even think we really need to tweak it. BUCHELE: There are now seven cameras posting photos online in Austin, and the city hopes to put up around 20 more by the end of the year. Porcher says they've also gotten calls from neighboring communities about installing them. He hopes one day there will be a statewide system of cameras trained on Texas creeks and rivers helping to keep people out of harm's way. For NPR News, I'm Mose Buchele in Austin. (SOUNDBITE OF SONG, \"BURNT BACK\")", "section": "Environment And Energy Collaborative", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-30-643362185": {"title": "Emailing On Your Commute? That's Work, A New Study Says : NPR", "url": "https://www.npr.org/2018/08/30/643362185/emailing-on-your-commute-thats-work-a-new-study-says", "author": "No author found", "published_date": "2018-08-30", "content": "", "section": "Your Money", "disclaimer": ""}, "2018-08-31-643582392": {"title": "A Memoir Of An '80s California Childhood \u2014 And Being Steve Jobs' Daughter : NPR", "url": "https://www.npr.org/2018/08/31/643582392/being-steve-jobs-daughter-in-small-fry", "author": "No author found", "published_date": "2018-08-31", "content": "RACHEL MARTIN, HOST: The book \"Small Fry\" is a memoir of a girl growing up in 1980s California. She's raised by her single mom, a struggling artist. And the two are always moving from place to place, never able to make ends meet. The author says it's a universal coming-of-age story amid the scent of eucalyptus and West Coast sunlight, except the writer's father is Steve Jobs, the founder of Apple. Lisa Brennan-Jobs was born when Steve Jobs was really young, focused on his work. And he didn't want to be a father. In her book, though, Lisa Brennan-Jobs is trying not to present herself as a victim. LISA BRENNAN-JOBS: When I started writing it, there was a lot of self-pity. I would try to get the reader to feel bad for me, and it turned out that it doesn't really work on the page. MARTIN: Why did you want to do that? BRENNAN-JOBS: 'Cause I felt bad for myself, and I felt very ashamed, I think. So what I had to do was go back into the stories that weren't working and understand where my part in them was. I mean, a friend said to me, hey, Lisa, I knew you then. You kind of got your way. MARTIN: (Laughter). BRENNAN-JOBS: I don't really believe this victim stuff. MARTIN: Interesting. BRENNAN-JOBS: For example, I had a lot of pity around the fact that we didn't have working heat in the downstairs part of the house when I lived with my dad. MARTIN: Right. BRENNAN-JOBS: And I would sort of use this story to get pity from other people, and it works. MARTIN: I mean, yeah, even in the book. I read that, and I'm like, he was depriving you of heat in that room? BRENNAN-JOBS: No, but the truth is Northern California doesn't really get that cold. I could've changed rooms if I'd needed to. It was a stand-in for something else. And I'm not saying that it's not an element of the story. But when I could see underneath my own stories and sort of was on to myself a bit, it opened the book up for me. MARTIN: How did you think about Steve Jobs before he admitted that he was your father? How did you think about this guy who would show up and take you roller-skating in your neighborhood? BRENNAN-JOBS: My mother had kept the fact of my father alive for me in the sense that she knew him well. She knew his better nature. And she knew he was my father (laughter). MARTIN: Yeah. BRENNAN-JOBS: So she had, I think, quite skillfully kept him alive for me. So that when he did decide to become more a part of my life, there was a place for him in my heart. MARTIN: How did you walk through the world knowing that you were the daughter of Steve Jobs? How did you navigate that part of your identity? BRENNAN-JOBS: I would use anything I had. I think there's something about being a kid where you don't really have much yet. So maybe you use your tennis shoes 'cause they're nice, or you use your - I don't know - maybe your parents have a pool or something (laughter). And so this is what I had. And I describe this in the book. Like, it was like an itch sometimes. If I felt badly about myself, I could slyly pull out that I have this famous father. And of course it didn't really work because. . . MARTIN: Right, it didn't scratch the itch. BRENNAN-JOBS: I didn't have a nice - well, also it didn't seem real, right? Like, I didn't have the clothes that a kid with a famous, rich dad would have. I didn't have the house. I didn't have the mannerisms. I didn't have the sense of entitlement. And I don't mean that in a bad way. I just - we didn't have the stuff, so I'd pull it out to try to make myself feel better or feel special. You know, these are things that I might have wanted to hide when I was writing the book. And I kind of just decided to jump into the points that I was ashamed of and see if it would resonate with other people. MARTIN: I think it's fair to say that when it came to sharing his financial wealth, Steve Jobs was not necessarily that generous towards you when you were younger. Examples, he rushed to sign a child support agreement just four days before Apple went public, making him worth more than $200 million. You and your mom asked him for help to buy a house in Palo Alto, and he bought it for himself instead. Why do you think he did those things? BRENNAN-JOBS: I think he knew that he hadn't done right by me, and he was also very young. But then there was something that he did later in my life, which is - a lot of people might have then shoved money at me to try to compensate. But of course money later doesn't compensate for money earlier. Money can be detrimental for kids. And I guess I would say about myself, like, I might've settled for it. I probably shouldn't say that. I might have - I just think, like, if he bought me a pony and bought everything I wanted, I think I might have been falsely soothed. MARTIN: It is clear throughout the book that despite all of that, not just the financial deprivation but also how he kept himself away from you - I mean, there was a distance, a coldness during your childhood. But you loved him throughout. You revered him even throughout, but did you ever allow yourself to be angry at him? BRENNAN-JOBS: I think both of us went off and on. But there was a time when I was living in London after college. We weren't really in touch. And I - missing him, some misguided attempt to connect with him, I checked his website. And he said he had three children and not four. Oh, and I was so upset. Just this feeling of always wanting to be on the inside but being on the outside. And I guess the thing is if you're in the comfy inner circle, you don't need to write a book to understand your life. But it was very painful to feel this kind of whiplash of being his daughter and then not being his daughter. And I actually called him at work and left a message and asked - and told him about the work bio. And I called my aunts, and I called my mom. And I tried to get him to change it, which of course is a little sad because if someone changes something 'cause you've asked them to. . . MARTIN: Right. BRENNAN-JOBS: . . . It's a little differentMARTIN: It's not the same. (LAUGHTER)BRENNAN-JOBS: And he changed it for about a week, and then it was changed back. I don't know what happened. MARTIN: What was your last conversation like with him? BRENNAN-JOBS: He was apologizing. And he was - we had a kind of Hollywood ending, which I didn't think would happen - so strange. You see these movies where people apologize in the end, and you think there's no way that it will ever happen in life. But it did happen. And he was saying this phrase, I owe you one, I owe you one, which was so confusing to me. But it was soothing. But I think, still, I had to go over it in my mind again. And the real resolution for me came more in writing the book, even more than our last conversation. MARTIN: Lisa Brennan-Jobs. Her new memoir is titled \"Small Fry. \" Thanks so much for talking with us. BRENNAN-JOBS: Thank you so much. (SOUNDBITE OF MUSIC)MARTIN: \"Small Fry\" did not sit well with the rest of the Jobs family. Steve's wife, Laurene Powell Jobs, their kids and his sister Mona Simpson put out this statement, saying the book, quote, \"differs dramatically from our memories of those times. The portrayal of Steve is not the husband and father we knew. Steve loved Lisa, and he regretted that he was not the father he should have been during her early childhood. \" RACHEL MARTIN, HOST:  The book \"Small Fry\" is a memoir of a girl growing up in 1980s California. She's raised by her single mom, a struggling artist. And the two are always moving from place to place, never able to make ends meet. The author says it's a universal coming-of-age story amid the scent of eucalyptus and West Coast sunlight, except the writer's father is Steve Jobs, the founder of Apple. Lisa Brennan-Jobs was born when Steve Jobs was really young, focused on his work. And he didn't want to be a father. In her book, though, Lisa Brennan-Jobs is trying not to present herself as a victim. LISA BRENNAN-JOBS: When I started writing it, there was a lot of self-pity. I would try to get the reader to feel bad for me, and it turned out that it doesn't really work on the page. MARTIN: Why did you want to do that? BRENNAN-JOBS: 'Cause I felt bad for myself, and I felt very ashamed, I think. So what I had to do was go back into the stories that weren't working and understand where my part in them was. I mean, a friend said to me, hey, Lisa, I knew you then. You kind of got your way. MARTIN: (Laughter). BRENNAN-JOBS: I don't really believe this victim stuff. MARTIN: Interesting. BRENNAN-JOBS: For example, I had a lot of pity around the fact that we didn't have working heat in the downstairs part of the house when I lived with my dad. MARTIN: Right. BRENNAN-JOBS: And I would sort of use this story to get pity from other people, and it works. MARTIN: I mean, yeah, even in the book. I read that, and I'm like, he was depriving you of heat in that room? BRENNAN-JOBS: No, but the truth is Northern California doesn't really get that cold. I could've changed rooms if I'd needed to. It was a stand-in for something else. And I'm not saying that it's not an element of the story. But when I could see underneath my own stories and sort of was on to myself a bit, it opened the book up for me. MARTIN: How did you think about Steve Jobs before he admitted that he was your father? How did you think about this guy who would show up and take you roller-skating in your neighborhood? BRENNAN-JOBS: My mother had kept the fact of my father alive for me in the sense that she knew him well. She knew his better nature. And she knew he was my father (laughter). MARTIN: Yeah. BRENNAN-JOBS: So she had, I think, quite skillfully kept him alive for me. So that when he did decide to become more a part of my life, there was a place for him in my heart. MARTIN: How did you walk through the world knowing that you were the daughter of Steve Jobs? How did you navigate that part of your identity? BRENNAN-JOBS: I would use anything I had. I think there's something about being a kid where you don't really have much yet. So maybe you use your tennis shoes 'cause they're nice, or you use your - I don't know - maybe your parents have a pool or something (laughter). And so this is what I had. And I describe this in the book. Like, it was like an itch sometimes. If I felt badly about myself, I could slyly pull out that I have this famous father. And of course it didn't really work because. . . MARTIN: Right, it didn't scratch the itch. BRENNAN-JOBS: I didn't have a nice - well, also it didn't seem real, right? Like, I didn't have the clothes that a kid with a famous, rich dad would have. I didn't have the house. I didn't have the mannerisms. I didn't have the sense of entitlement. And I don't mean that in a bad way. I just - we didn't have the stuff, so I'd pull it out to try to make myself feel better or feel special. You know, these are things that I might have wanted to hide when I was writing the book. And I kind of just decided to jump into the points that I was ashamed of and see if it would resonate with other people. MARTIN: I think it's fair to say that when it came to sharing his financial wealth, Steve Jobs was not necessarily that generous towards you when you were younger. Examples, he rushed to sign a child support agreement just four days before Apple went public, making him worth more than $200 million. You and your mom asked him for help to buy a house in Palo Alto, and he bought it for himself instead. Why do you think he did those things? BRENNAN-JOBS: I think he knew that he hadn't done right by me, and he was also very young. But then there was something that he did later in my life, which is - a lot of people might have then shoved money at me to try to compensate. But of course money later doesn't compensate for money earlier. Money can be detrimental for kids. And I guess I would say about myself, like, I might've settled for it. I probably shouldn't say that. I might have - I just think, like, if he bought me a pony and bought everything I wanted, I think I might have been falsely soothed. MARTIN: It is clear throughout the book that despite all of that, not just the financial deprivation but also how he kept himself away from you - I mean, there was a distance, a coldness during your childhood. But you loved him throughout. You revered him even throughout, but did you ever allow yourself to be angry at him? BRENNAN-JOBS: I think both of us went off and on. But there was a time when I was living in London after college. We weren't really in touch. And I - missing him, some misguided attempt to connect with him, I checked his website. And he said he had three children and not four. Oh, and I was so upset. Just this feeling of always wanting to be on the inside but being on the outside. And I guess the thing is if you're in the comfy inner circle, you don't need to write a book to understand your life. But it was very painful to feel this kind of whiplash of being his daughter and then not being his daughter. And I actually called him at work and left a message and asked - and told him about the work bio. And I called my aunts, and I called my mom. And I tried to get him to change it, which of course is a little sad because if someone changes something 'cause you've asked them to. . . MARTIN: Right. BRENNAN-JOBS: . . . It's a little different MARTIN: It's not the same. (LAUGHTER) BRENNAN-JOBS: And he changed it for about a week, and then it was changed back. I don't know what happened. MARTIN: What was your last conversation like with him? BRENNAN-JOBS: He was apologizing. And he was - we had a kind of Hollywood ending, which I didn't think would happen - so strange. You see these movies where people apologize in the end, and you think there's no way that it will ever happen in life. But it did happen. And he was saying this phrase, I owe you one, I owe you one, which was so confusing to me. But it was soothing. But I think, still, I had to go over it in my mind again. And the real resolution for me came more in writing the book, even more than our last conversation. MARTIN: Lisa Brennan-Jobs. Her new memoir is titled \"Small Fry. \" Thanks so much for talking with us. BRENNAN-JOBS: Thank you so much. (SOUNDBITE OF MUSIC) MARTIN: \"Small Fry\" did not sit well with the rest of the Jobs family. Steve's wife, Laurene Powell Jobs, their kids and his sister Mona Simpson put out this statement, saying the book, quote, \"differs dramatically from our memories of those times. The portrayal of Steve is not the husband and father we knew. Steve loved Lisa, and he regretted that he was not the father he should have been during her early childhood. \"", "section": "Author Interviews", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-08-31-643582365": {"title": "Twitter Public Policy Director On How Company Monitors Content : NPR", "url": "https://www.npr.org/2018/08/31/643582365/twitter-public-policy-director-on-how-company-monitors-content", "author": "No author found", "published_date": "2018-08-31", "content": "RACHEL MARTIN, HOST: Twitter says it's cleaning up the conversation. The social media platform recently eliminated millions of fake accounts. STEVE INSKEEP, HOST: And Carlos Monje, Twitter's head of public policy, says Twitter also blocked accounts linked to Iran. CARLOS MONJE: They were trying to manipulate the conversation on Twitter, and that's why we suspended them. It was about 700 accounts, about a hundred of those pretending to be American. They were small relative to what we saw in 2016, but they were growing and they were building for the longer term. INSKEEP: Twitter says it's targeting deceptive actors as the 2018 election nears. It's under scrutiny. Next week, a Senate committee questions CEO Jack Dorsey. MARTIN: Twitter is at the heart of the debate over our public debate. It's where the president routinely makes false claims, including saying true stories are fake. It's where his critics sometimes overreact to the latest news. Millions of bots, automated accounts, have spread extreme statements. INSKEEP: When Twitter's Carlos Monje came by to take our questions, he insisted the company is getting better at spotting fake identities. What's trickier, for a platform devoted to free speech, is what to do about false ideas. Why was Twitter slower than other social media platforms to move against the noted conspiracy theorist Alex Jones? MONJE: We have a terms of service that we work on and think about every day. We think it's important for all voices to be heard as long as those voices aren't trying to silence those of others. We were able to action his account after he broke our rules. INSKEEP: You gave him a time-out for a week. MONJE: We gave him a time-out because there was a threat of violence, right? And we try very hard to implement our rules in a dispassionate way. When people do break our rules, we will action them. INSKEEP: He's back on. MONJE: He is. INSKEEP: Is he doing any better? MONJE: We're watching all the accounts, and we get tons of reports across the board. So we'll keep an eye on his activity. INSKEEP: I went, before this interview, and had a look at the Alex Jones Twitter account, and something in the last number of hours that he's put out there is, quote, \"genocide against white farmers taking place in South Africa. \" Now, that's false, and there are fact-checking organizations that have looked into that and found that there is no evidence for that. There is an underlying issue that can be debated about land in South Africa. There's something going on. But he makes a false charge about genocide, and he amplifies it on Twitter, and it's a charge that is commonly made by white supremacists to whip up racial anxiety. Is that appropriate for Twitter? MONJE: We want to make sure that we allow a venue for all voices to come out. If somebody breaks the law, if somebody engages in hateful conduct, if somebody calls for violence or uses racial slurs, those are things that we think are beyond the pale, and we'll take you off the platform. INSKEEP: But what about that particular example? It's not exactly a racial slur. He didn't use the N-word. He probably did something that was worse. MONJE: You know what we've found is that when something like that happens, when there's a falsehood that comes out on the platform, the rest of the platform and folks who actually know what's actually going on swamp the falsehood with truth. And we've seen that over and over again. It happened after the Boston Marathon bombing when there was a rumor out that they had caught the shooter. When Gabby Giffords was shot, it was actually the hospital that said no, in fact, she was still alive. The platform can be a tremendous force for getting the actual truth out there. And deleting a tweet doesn't delete the ideology behind it. And we think it's important to have a space where these sometimes terrible ideas can come forward and be challenged in the public and in the open. INSKEEP: People sometimes come on this program and express a racist idea. And it's important to talk about it because it's out there. I would agree with that. But if I'm in an interview and you say something that is false or racist, it's on me. It's my responsibility in presenting that to the audience to give people enough information that they can understand what it is. Do you have any responsibility when Alex Jones or any number of people puts out false information, or is that purely on other users on the platform to point that out? MONJE: We take our responsibility extremely seriously, and that's to have a place that supports the public conversation - right? - and to set up a place where people can feel safe sharing their opinions and people can find out what's happening and talk about it. If we get to a position where we're putting the thumb on the scales on any side of any issue, it doesn't serve that public conversation. It isn't that all speech is OK. It's that the rollicking, small-D democratic debate is something that we treasure and spend a lot of time thinking about - how do we enable that healthy debate to continue on our platform? INSKEEP: What do you think about when you go to work at this company - and I'm sure you're proud to work for Twitter - and you must hear the comments of people who use Twitter, who rely on Twitter, who like certain aspects of Twitter but nevertheless describe Twitter as a cesspool? What do you think about when you hear that? MONJE: I think we have the most incredible engineers anywhere, who, if we tell them to aim at a challenge, they'll get there. We know we're doing better than we were. We can see it in the numbers that we see internally, the reduced number of abuse reports. Our work's never going to be done because the bad guys always shift their tactics. INSKEEP: So you've told me how you respond, how you're trying to make the conversation better. But I want to come back to that word, cesspool, because people use it. Why do you think it is that people who use your service, who know your service, commonly describe it as a cesspool? MONJE: You know, we have made a bunch of changes to our platform - 20 30 policy changes just in the last year. INSKEEP: Well, you're talking about changes. I get you want to talk about changes. . . MONJE: Yeah. Yeah. INSKEEP: But why do you think it is that people view Twitter that way, people who use it? MONJE: I think we need to do a better job explaining all the different ways that we give our users more control over their experience. If you don't want to see a conversation, if you don't want to hear from a user, you can mute them. You can block them. You can report them. Twitter really is working to take that bad stuff that you're talking about. . . INSKEEP: But what is it that causes the actual problem? Are you saying it's purely human nature, that's how humanity is? Is there something about social media that is bringing out the worst in people? What makes certain aspects of this otherwise very useful conversation so very bad? MONJE: You know, I go back to the example of what was a hard day, I think, for me and for a lot of people here in the U. S. , which was the Charlottesville rallies, right? INSKEEP: Sure. MONJE: We saw a lot of bad people. . . INSKEEP: Around the Robert E. Lee statue. A woman was killed. Right. MONJE: A really, really, really, you know, difficult day, seeing Tiki torches on the streets, seeing people spew hate on the streets of America, right? And we saw that reflected on Twitter in that a very, very small minority of users came out and said terrible things. But what we also saw, in the country and online and on our platform, is the rest of civil society coming out and rejecting it firmly and flatly and saying, this is not who we are. INSKEEP: Is there something special about social media that brings out the worst in people? MONJE: I don't think so. INSKEEP: Do you think who we are on social media is actually just pretty much who we are? MONJE: I think people come to Twitter to find out what's happening in the world and to talk about it, right? It is a rollicking debate. INSKEEP: Carlos Monje of Twitter. Thanks for coming by. MONJE: Thanks so much for having me. RACHEL MARTIN, HOST:  Twitter says it's cleaning up the conversation. The social media platform recently eliminated millions of fake accounts. STEVE INSKEEP, HOST:  And Carlos Monje, Twitter's head of public policy, says Twitter also blocked accounts linked to Iran. CARLOS MONJE: They were trying to manipulate the conversation on Twitter, and that's why we suspended them. It was about 700 accounts, about a hundred of those pretending to be American. They were small relative to what we saw in 2016, but they were growing and they were building for the longer term. INSKEEP: Twitter says it's targeting deceptive actors as the 2018 election nears. It's under scrutiny. Next week, a Senate committee questions CEO Jack Dorsey. MARTIN: Twitter is at the heart of the debate over our public debate. It's where the president routinely makes false claims, including saying true stories are fake. It's where his critics sometimes overreact to the latest news. Millions of bots, automated accounts, have spread extreme statements. INSKEEP: When Twitter's Carlos Monje came by to take our questions, he insisted the company is getting better at spotting fake identities. What's trickier, for a platform devoted to free speech, is what to do about false ideas. Why was Twitter slower than other social media platforms to move against the noted conspiracy theorist Alex Jones? MONJE: We have a terms of service that we work on and think about every day. We think it's important for all voices to be heard as long as those voices aren't trying to silence those of others. We were able to action his account after he broke our rules. INSKEEP: You gave him a time-out for a week. MONJE: We gave him a time-out because there was a threat of violence, right? And we try very hard to implement our rules in a dispassionate way. When people do break our rules, we will action them. INSKEEP: He's back on. MONJE: He is. INSKEEP: Is he doing any better? MONJE: We're watching all the accounts, and we get tons of reports across the board. So we'll keep an eye on his activity. INSKEEP: I went, before this interview, and had a look at the Alex Jones Twitter account, and something in the last number of hours that he's put out there is, quote, \"genocide against white farmers taking place in South Africa. \" Now, that's false, and there are fact-checking organizations that have looked into that and found that there is no evidence for that. There is an underlying issue that can be debated about land in South Africa. There's something going on. But he makes a false charge about genocide, and he amplifies it on Twitter, and it's a charge that is commonly made by white supremacists to whip up racial anxiety. Is that appropriate for Twitter? MONJE: We want to make sure that we allow a venue for all voices to come out. If somebody breaks the law, if somebody engages in hateful conduct, if somebody calls for violence or uses racial slurs, those are things that we think are beyond the pale, and we'll take you off the platform. INSKEEP: But what about that particular example? It's not exactly a racial slur. He didn't use the N-word. He probably did something that was worse. MONJE: You know what we've found is that when something like that happens, when there's a falsehood that comes out on the platform, the rest of the platform and folks who actually know what's actually going on swamp the falsehood with truth. And we've seen that over and over again. It happened after the Boston Marathon bombing when there was a rumor out that they had caught the shooter. When Gabby Giffords was shot, it was actually the hospital that said no, in fact, she was still alive. The platform can be a tremendous force for getting the actual truth out there. And deleting a tweet doesn't delete the ideology behind it. And we think it's important to have a space where these sometimes terrible ideas can come forward and be challenged in the public and in the open. INSKEEP: People sometimes come on this program and express a racist idea. And it's important to talk about it because it's out there. I would agree with that. But if I'm in an interview and you say something that is false or racist, it's on me. It's my responsibility in presenting that to the audience to give people enough information that they can understand what it is. Do you have any responsibility when Alex Jones or any number of people puts out false information, or is that purely on other users on the platform to point that out? MONJE: We take our responsibility extremely seriously, and that's to have a place that supports the public conversation - right? - and to set up a place where people can feel safe sharing their opinions and people can find out what's happening and talk about it. If we get to a position where we're putting the thumb on the scales on any side of any issue, it doesn't serve that public conversation. It isn't that all speech is OK. It's that the rollicking, small-D democratic debate is something that we treasure and spend a lot of time thinking about - how do we enable that healthy debate to continue on our platform? INSKEEP: What do you think about when you go to work at this company - and I'm sure you're proud to work for Twitter - and you must hear the comments of people who use Twitter, who rely on Twitter, who like certain aspects of Twitter but nevertheless describe Twitter as a cesspool? What do you think about when you hear that? MONJE: I think we have the most incredible engineers anywhere, who, if we tell them to aim at a challenge, they'll get there. We know we're doing better than we were. We can see it in the numbers that we see internally, the reduced number of abuse reports. Our work's never going to be done because the bad guys always shift their tactics. INSKEEP: So you've told me how you respond, how you're trying to make the conversation better. But I want to come back to that word, cesspool, because people use it. Why do you think it is that people who use your service, who know your service, commonly describe it as a cesspool? MONJE: You know, we have made a bunch of changes to our platform - 20 30 policy changes just in the last year. INSKEEP: Well, you're talking about changes. I get you want to talk about changes. . . MONJE: Yeah. Yeah. INSKEEP: But why do you think it is that people view Twitter that way, people who use it? MONJE: I think we need to do a better job explaining all the different ways that we give our users more control over their experience. If you don't want to see a conversation, if you don't want to hear from a user, you can mute them. You can block them. You can report them. Twitter really is working to take that bad stuff that you're talking about. . . INSKEEP: But what is it that causes the actual problem? Are you saying it's purely human nature, that's how humanity is? Is there something about social media that is bringing out the worst in people? What makes certain aspects of this otherwise very useful conversation so very bad? MONJE: You know, I go back to the example of what was a hard day, I think, for me and for a lot of people here in the U. S. , which was the Charlottesville rallies, right? INSKEEP: Sure. MONJE: We saw a lot of bad people. . . INSKEEP: Around the Robert E. Lee statue. A woman was killed. Right. MONJE: A really, really, really, you know, difficult day, seeing Tiki torches on the streets, seeing people spew hate on the streets of America, right? And we saw that reflected on Twitter in that a very, very small minority of users came out and said terrible things. But what we also saw, in the country and online and on our platform, is the rest of civil society coming out and rejecting it firmly and flatly and saying, this is not who we are. INSKEEP: Is there something special about social media that brings out the worst in people? MONJE: I don't think so. INSKEEP: Do you think who we are on social media is actually just pretty much who we are? MONJE: I think people come to Twitter to find out what's happening in the world and to talk about it, right? It is a rollicking debate. INSKEEP: Carlos Monje of Twitter. Thanks for coming by. MONJE: Thanks so much for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-01-643909884": {"title": "California Lawmakers Pass Net Neutrality Bill : NPR", "url": "https://www.npr.org/2018/09/01/643909884/california-lawmakers-pass-net-neutrality-bill", "author": "No author found", "published_date": "2018-09-01", "content": "", "section": "National", "disclaimer": ""}, "2018-09-03-644355847": {"title": "Are Tech Giants Doing Enough To Fight Against Foreign Powers Trying To Influence Elections? : NPR", "url": "https://www.npr.org/2018/09/03/644355847/are-tech-giants-doing-enough-to-fight-against-foreign-powers-trying-to-influence", "author": "No author found", "published_date": "2018-09-03", "content": "AUDIE CORNISH, HOST: Time now for ALL TECH CONSIDERED. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CORNISH: All this month, we're looking at how technology can be used to influence or undermine the core tenets of democracy. Top executives from Facebook and Twitter will testify before the Senate Intelligence Committee this Wednesday. Google has also been invited. Facebook has led the way on public disclosures of propaganda operations from Russia. As NPR's Alina Selyukh reports, the company is trying to show how much it has changed since the last election. ALINA SELYUKH, BYLINE: Every year, Facebook makes billions of dollars on ads. And some of them really blend in. They can look like just another video in your news feed, except a tiny tag says sponsored. This became a particular sticking point in the investigation over what happened in 2016, how Facebook exposed tens of millions of American users to misinformation and divisive propaganda. Here's Louisiana senator John Kennedy questioning Facebook's lawyer in October. (SOUNDBITE OF ARCHIVED RECORDING)JOHN KENNEDY: The truth of the matter is you have 5 million advertisers that change every month, every minute, probably every second. You don't have the ability to know who every one of those advertisers is, do you? SELYUKH: Facebook's lawyer acknowledged the company has to rely on advertisers to identify themselves. This is tricky with elections at stake. Facebook now says 11 million people saw ads purchased by Russia-linked accounts during the 2016 cycle. As campaigns geared up for this year's midterms, Facebook decided to get more transparent. (SOUNDBITE OF ARCHIVED RECORDING)ROB LEATHERN: Starting today, all election and issue ads on Facebook and Instagram in the U. S. must be clearly labeled. SELYUKH: This was Rob Leathern, a Facebook ads executive, on a call with reporters in May. All political ads on Facebook now have to show not just the Facebook group behind it but the person or entity paying for it. And whoever is buying the ad has to submit a copy of their U. S. ID and get a special code in the mail. That applies to ads about people running for office but also politically sensitive issues - think immigration, guns or civil rights. And on top of it all, Facebook is now collecting these ads into a searchable archive. ADAV NOTI: Facebook has done a very good job in the last few months. SELYUKH: Adav Noti is with the watchdog group Campaign Legal Center. NOTI: It's a real step forward for Facebook, given that, until a few months ago, Facebook was the most recalcitrant of all the major online platforms in terms of fighting against that sort of disclosure and transparency. SELYUKH: So far, the most common complaint against the new rules is how broadly Facebook applies them. If you spend enough time on the ad archive, you'll find news stories and even random events like a comedy show - but also, of course, the never-ending flood of political ads. University of Wisconsin-Madison professor Young Mie Kim studied divisive advertising in 2016. And she says Facebook's new archive still does not address one common tactic - multiple groups coordinating to push the same agenda. YOUNG MIE KIM: A large number of small groups were targeting similar types of people. So then, at the aggregate level, it adds up. SELYUKH: There's still no way for the public to connect the dots between multiple Facebook pages that may be funded by the same people and advertising to the same users. Kim also has a more obvious complaint about Facebook's new transparency. KIM: They should have done this way before 2016 election. SELYUKH: Now the midterms are weeks away, and the 2020 campaigning is starting up. Raffi Krikorian is the chief technology officer of the Democratic National Committee. And he says transparency on social media remains a huge concern. RAFFI KRIKORIAN: This technology landscape is changing so quickly that I'm worried that voters don't fully grok how to tell truths apart from advertising from misinformation. SELYUKH: Krikorian wants Facebook and other tech companies to start being more proactive about correcting misinformation on their platforms. The tech platforms say that's not really their role. Meanwhile, campaign finance experts point out whatever tech companies do for transparency is completely voluntary. The laws on the books were not written for the age of social media. Alina Selyukh, NPR News. AUDIE CORNISH, HOST:  Time now for ALL TECH CONSIDERED. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CORNISH: All this month, we're looking at how technology can be used to influence or undermine the core tenets of democracy. Top executives from Facebook and Twitter will testify before the Senate Intelligence Committee this Wednesday. Google has also been invited. Facebook has led the way on public disclosures of propaganda operations from Russia. As NPR's Alina Selyukh reports, the company is trying to show how much it has changed since the last election. ALINA SELYUKH, BYLINE: Every year, Facebook makes billions of dollars on ads. And some of them really blend in. They can look like just another video in your news feed, except a tiny tag says sponsored. This became a particular sticking point in the investigation over what happened in 2016, how Facebook exposed tens of millions of American users to misinformation and divisive propaganda. Here's Louisiana senator John Kennedy questioning Facebook's lawyer in October. (SOUNDBITE OF ARCHIVED RECORDING) JOHN KENNEDY: The truth of the matter is you have 5 million advertisers that change every month, every minute, probably every second. You don't have the ability to know who every one of those advertisers is, do you? SELYUKH: Facebook's lawyer acknowledged the company has to rely on advertisers to identify themselves. This is tricky with elections at stake. Facebook now says 11 million people saw ads purchased by Russia-linked accounts during the 2016 cycle. As campaigns geared up for this year's midterms, Facebook decided to get more transparent. (SOUNDBITE OF ARCHIVED RECORDING) ROB LEATHERN: Starting today, all election and issue ads on Facebook and Instagram in the U. S. must be clearly labeled. SELYUKH: This was Rob Leathern, a Facebook ads executive, on a call with reporters in May. All political ads on Facebook now have to show not just the Facebook group behind it but the person or entity paying for it. And whoever is buying the ad has to submit a copy of their U. S. ID and get a special code in the mail. That applies to ads about people running for office but also politically sensitive issues - think immigration, guns or civil rights. And on top of it all, Facebook is now collecting these ads into a searchable archive. ADAV NOTI: Facebook has done a very good job in the last few months. SELYUKH: Adav Noti is with the watchdog group Campaign Legal Center. NOTI: It's a real step forward for Facebook, given that, until a few months ago, Facebook was the most recalcitrant of all the major online platforms in terms of fighting against that sort of disclosure and transparency. SELYUKH: So far, the most common complaint against the new rules is how broadly Facebook applies them. If you spend enough time on the ad archive, you'll find news stories and even random events like a comedy show - but also, of course, the never-ending flood of political ads. University of Wisconsin-Madison professor Young Mie Kim studied divisive advertising in 2016. And she says Facebook's new archive still does not address one common tactic - multiple groups coordinating to push the same agenda. YOUNG MIE KIM: A large number of small groups were targeting similar types of people. So then, at the aggregate level, it adds up. SELYUKH: There's still no way for the public to connect the dots between multiple Facebook pages that may be funded by the same people and advertising to the same users. Kim also has a more obvious complaint about Facebook's new transparency. KIM: They should have done this way before 2016 election. SELYUKH: Now the midterms are weeks away, and the 2020 campaigning is starting up. Raffi Krikorian is the chief technology officer of the Democratic National Committee. And he says transparency on social media remains a huge concern. RAFFI KRIKORIAN: This technology landscape is changing so quickly that I'm worried that voters don't fully grok how to tell truths apart from advertising from misinformation. SELYUKH: Krikorian wants Facebook and other tech companies to start being more proactive about correcting misinformation on their platforms. The tech platforms say that's not really their role. Meanwhile, campaign finance experts point out whatever tech companies do for transparency is completely voluntary. The laws on the books were not written for the age of social media. Alina Selyukh, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-05-644629018": {"title": "Marco Rubio Clashes With Alex Jones In Capitol: 'I'll Take Care Of You Myself' : NPR", "url": "https://www.npr.org/2018/09/05/644629018/marco-rubio-clashes-in-capitol-with-alex-jones-i-ll-take-care-of-you-myself", "author": "No author found", "published_date": "2018-09-05", "content": "", "section": "Politics", "disclaimer": ""}, "2018-09-05-644844174": {"title": "Theranos, Blood-Testing Company Plagued By Scandal, Says It Will Dissolve : NPR", "url": "https://www.npr.org/2018/09/05/644844174/theranos-blood-testing-company-accused-of-massive-fraud-says-it-will-dissolve", "author": "No author found", "published_date": "2018-09-05", "content": "", "section": "Business", "disclaimer": ""}, "2018-09-05-644607908": {"title": "Senate Committee Vents About Hijacking Of Big Tech For Information War : NPR", "url": "https://www.npr.org/2018/09/05/644607908/facebook-twitter-heavies-set-to-appear-at-senate-hearing-google-may-be-mia", "author": "No author found", "published_date": "2018-09-05", "content": "RACHEL MARTIN, HOST: Top executives from Facebook and Twitter are here in town today with some important meetings. They're going before House and Senate committees on Capitol Hill, and they will be grilled again about what social media companies could have done to stop Russian interference in the 2016 U. S. election and what they're doing now about Russian propaganda ahead of the midterms. NPR's Tim Mak will be covering the hearing, and he's with us in the studio. Hey, Tim. TIM MAK, BYLINE: Hey there. MARTIN: So as I mentioned, these companies have been in this situation before answering these kinds of questions. In the past, they've been reluctant to say they could have done more to prevent their platforms from being abused. Are they - do you expect them to say something different today? MAK: Yeah. I think there's an increasing sense amongst big tech firms that this is a serious problem, that an open society and social media platforms can be actually used against American democracy. Facebook, for example, is expected to emphasize how they're doubling the number of people working on safety and security issues to more than 20,000. And they're going to talk about how they're collaborating with law enforcement increasingly in the months ahead. But there's one firm that's not going to be there at all, and that's Google, which has declined to send founder Larry Page or its CEO to appear before the Senate Intelligence Committee today. And in his place, there's going be a little chair there, a little sign - a bit of Washington theater - as there is bipartisan consensus from Republicans and Democrats on this committee that their offer, Google's offer, to send its chief legal officer is insufficient. MARTIN: I mean, this is really what the central tension has been about, how these companies define themselves, right? Are they just a platform for people to express their views, or do they have a responsibility to curate them? How did they come down on this? MAK: Yeah. I mean, Facebook in particular is trying to make a good faith effort towards transparency and addressing some concerns from lawmakers and the public. And a lot of this is driven by the changing nature of the threat and how serious it now appears and has become. There's no sign that this threat is diminishing. In fact, just in the past month, we had Microsoft, Facebook and Twitter all announce that they had discovered and disrupted foreign operations. Iran was revealed to be a new player in worldwide disinformation campaigns. And on top of this, we found that there was a new Russian disinformation campaign and apparent hacking attempts. Big tech firms are really on red alert. MARTIN: So these threats keep popping up and these companies say that they're equipped to manage them. MAK: Well, that's the thing about it. Some of these issues can be addressed by the companies themselves. Some of them can be addressed by regulations that Congress might propose, such as online ad disclosures or things like new sanctions against foreign actors. But a lot of this has to do with people themselves. It's a multidimensional threat. It involves everything from hacking emails to bot networks online to disinformation campaigns to election infrastructure security. And a lot of this can be dealt with folks who are just listening to the radio right now, and that's doing things like securing their emails with multifactor authentication and being super judicious about the kinds of information they view online, whether or not they double check their sources, whether or not they're looking at news sources that have credibility and a long track record of being established. It's a really complicated problem. It involves state governments, big tech firms, Congress and people being responsible themselves. MARTIN: And us. You're saying it's on us. All right. NPR's Tim Mak, thanks so much. We appreciate it. MAK: Thanks a lot. RACHEL MARTIN, HOST:  Top executives from Facebook and Twitter are here in town today with some important meetings. They're going before House and Senate committees on Capitol Hill, and they will be grilled again about what social media companies could have done to stop Russian interference in the 2016 U. S. election and what they're doing now about Russian propaganda ahead of the midterms. NPR's Tim Mak will be covering the hearing, and he's with us in the studio. Hey, Tim. TIM MAK, BYLINE: Hey there. MARTIN: So as I mentioned, these companies have been in this situation before answering these kinds of questions. In the past, they've been reluctant to say they could have done more to prevent their platforms from being abused. Are they - do you expect them to say something different today? MAK: Yeah. I think there's an increasing sense amongst big tech firms that this is a serious problem, that an open society and social media platforms can be actually used against American democracy. Facebook, for example, is expected to emphasize how they're doubling the number of people working on safety and security issues to more than 20,000. And they're going to talk about how they're collaborating with law enforcement increasingly in the months ahead. But there's one firm that's not going to be there at all, and that's Google, which has declined to send founder Larry Page or its CEO to appear before the Senate Intelligence Committee today. And in his place, there's going be a little chair there, a little sign - a bit of Washington theater - as there is bipartisan consensus from Republicans and Democrats on this committee that their offer, Google's offer, to send its chief legal officer is insufficient. MARTIN: I mean, this is really what the central tension has been about, how these companies define themselves, right? Are they just a platform for people to express their views, or do they have a responsibility to curate them? How did they come down on this? MAK: Yeah. I mean, Facebook in particular is trying to make a good faith effort towards transparency and addressing some concerns from lawmakers and the public. And a lot of this is driven by the changing nature of the threat and how serious it now appears and has become. There's no sign that this threat is diminishing. In fact, just in the past month, we had Microsoft, Facebook and Twitter all announce that they had discovered and disrupted foreign operations. Iran was revealed to be a new player in worldwide disinformation campaigns. And on top of this, we found that there was a new Russian disinformation campaign and apparent hacking attempts. Big tech firms are really on red alert. MARTIN: So these threats keep popping up and these companies say that they're equipped to manage them. MAK: Well, that's the thing about it. Some of these issues can be addressed by the companies themselves. Some of them can be addressed by regulations that Congress might propose, such as online ad disclosures or things like new sanctions against foreign actors. But a lot of this has to do with people themselves. It's a multidimensional threat. It involves everything from hacking emails to bot networks online to disinformation campaigns to election infrastructure security. And a lot of this can be dealt with folks who are just listening to the radio right now, and that's doing things like securing their emails with multifactor authentication and being super judicious about the kinds of information they view online, whether or not they double check their sources, whether or not they're looking at news sources that have credibility and a long track record of being established. It's a really complicated problem. It involves state governments, big tech firms, Congress and people being responsible themselves. MARTIN: And us. You're saying it's on us. All right. NPR's Tim Mak, thanks so much. We appreciate it. MAK: Thanks a lot.", "section": "National Security", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-06-645352618": {"title": "Twitter Bans Alex Jones And InfoWars; Cites Abusive Behavior : NPR", "url": "https://www.npr.org/2018/09/06/645352618/twitter-bans-alex-jones-and-infowars-cites-abusive-behavior", "author": "No author found", "published_date": "2018-09-06", "content": "", "section": "Media", "disclaimer": ""}, "2018-09-06-645247376": {"title": "Feds Charge North Korean Cyber-Operative In Sony Hack, Ransomware Attack : NPR", "url": "https://www.npr.org/2018/09/06/645247376/feds-charge-north-korean-cyber-operative-in-sony-hack-ransomware-attack", "author": "No author found", "published_date": "2018-09-06", "content": "ARI SHAPIRO, HOST: The Justice Department announced charges today against a North Korean man for his alleged role in a string of cyberattacks in the last few years. Perhaps the most notable of those attacks is the one on Sony Pictures back in 2014. NPR justice reporter Ryan Lucas is following this and joins us now with more. Hi, Ryan. RYAN LUCAS, BYLINE: Hi there. SHAPIRO: First tell us about the suspect. LUCAS: Well, he's a 34-year-old North Korean computer programmer named Park Jin Hyok. The Justice Department unsealed a criminal complaint against him today. It accuses him of being part of this hacking conspiracy that had a global scale. And it was carried out, according to the Justice Department, on behalf of the North Korean government. It says he worked for a front company but was in fact part of a team of hackers known as the Lazarus Group. And that group carried out cyberattacks against computer networks across the world. SHAPIRO: What specific attacks does the Justice Department allege he was involved in? LUCAS: Well, it's an extensive list. It involves attacks against financial institutions, entertainment companies and others. But there are three really big attacks worth talking about. One is the Sony Pictures attack in 2014 that you mentioned earlier. That was in retaliation for a movie that the studio was putting out called \"The Interview. \" You may remember it. SHAPIRO: Right. LUCAS: Comedy about a CIA plot to assassinate North Korean leader Kim Jong Un. Hackers stole emails from the studio, released those to the public, to the great embarrassment of Sony executives, and they also damaged Sony's computer systems. SHAPIRO: And the Obama administration blamed North Korea for that but never actually charged anyone. What else did this hacker allegedly do? LUCAS: Well, the second is the attempted cybertheft of around $1 billion from Bangladesh Bank. The hackers ultimately were able to make off with a mere $81 million. And then lastly there's something called the WannaCry ransomware attack that was in 2017. That was malware that basically hijacked computers, encrypted all the data on them, demanded money from the victims to unlock the computers, to give them access back to them. This was an indiscriminate attack. This was a really big deal. It infected hundreds of thousands of computers in about 150 countries. And the British National Health Service in particular and hospitals there were hit particularly hard. SHAPIRO: Yeah, that's a really striking track record for one hacker. This is not the first time the Justice Department has brought charges against a state-backed foreign hacker. Is there any indication that it's actually preventing more people from doing this? LUCAS: That's a really good question. Park's last known location is North Korea. He's unlikely to face trial in the U. S. at any point in time. And that's been the case with really most of the other charges that the U. S. has brought against alleged state-sponsored hackers. And U. S. officials will acknowledge that, but they say that the government has a long memory. The long arm of U. S. law can sometimes pluck these people from other countries if they travel to the wrong place. But beyond that, experts say that there's also this sense kind of inside the U. S. government that there have to be consequences for these sorts of cyberattacks. There have to be consequences to try to deter them from happening in the future. And also, experts and U. S. officials say that, for example, China hated it when the U. S. charged Chinese military officials back in 2014 with hacking and say that the Chinese still complain about that. But it's an open question what exactly consequences are going to look like. But again, this is just one tool in kind of the government's toolbox. SHAPIRO: And briefly, do you expect this to have any impact on the effort to negotiate a nuclear deal with North Korea? LUCAS: Well, there's been no comment so far from North Korea about the charges. But North Korea's very strategic about what they respond to. They may ignore this if they view that that is in their interests, or they may pounce on it and take a swipe at the U. S. But we've had kind of warm words in the past day or so from Kim Jong Un and President Trump, so maybe there will be momentum back in those talks. SHAPIRO: NPR's Ryan Lucas, thanks. LUCAS: Thank you. ARI SHAPIRO, HOST:  The Justice Department announced charges today against a North Korean man for his alleged role in a string of cyberattacks in the last few years. Perhaps the most notable of those attacks is the one on Sony Pictures back in 2014. NPR justice reporter Ryan Lucas is following this and joins us now with more. Hi, Ryan. RYAN LUCAS, BYLINE: Hi there. SHAPIRO: First tell us about the suspect. LUCAS: Well, he's a 34-year-old North Korean computer programmer named Park Jin Hyok. The Justice Department unsealed a criminal complaint against him today. It accuses him of being part of this hacking conspiracy that had a global scale. And it was carried out, according to the Justice Department, on behalf of the North Korean government. It says he worked for a front company but was in fact part of a team of hackers known as the Lazarus Group. And that group carried out cyberattacks against computer networks across the world. SHAPIRO: What specific attacks does the Justice Department allege he was involved in? LUCAS: Well, it's an extensive list. It involves attacks against financial institutions, entertainment companies and others. But there are three really big attacks worth talking about. One is the Sony Pictures attack in 2014 that you mentioned earlier. That was in retaliation for a movie that the studio was putting out called \"The Interview. \" You may remember it. SHAPIRO: Right. LUCAS: Comedy about a CIA plot to assassinate North Korean leader Kim Jong Un. Hackers stole emails from the studio, released those to the public, to the great embarrassment of Sony executives, and they also damaged Sony's computer systems. SHAPIRO: And the Obama administration blamed North Korea for that but never actually charged anyone. What else did this hacker allegedly do? LUCAS: Well, the second is the attempted cybertheft of around $1 billion from Bangladesh Bank. The hackers ultimately were able to make off with a mere $81 million. And then lastly there's something called the WannaCry ransomware attack that was in 2017. That was malware that basically hijacked computers, encrypted all the data on them, demanded money from the victims to unlock the computers, to give them access back to them. This was an indiscriminate attack. This was a really big deal. It infected hundreds of thousands of computers in about 150 countries. And the British National Health Service in particular and hospitals there were hit particularly hard. SHAPIRO: Yeah, that's a really striking track record for one hacker. This is not the first time the Justice Department has brought charges against a state-backed foreign hacker. Is there any indication that it's actually preventing more people from doing this? LUCAS: That's a really good question. Park's last known location is North Korea. He's unlikely to face trial in the U. S. at any point in time. And that's been the case with really most of the other charges that the U. S. has brought against alleged state-sponsored hackers. And U. S. officials will acknowledge that, but they say that the government has a long memory. The long arm of U. S. law can sometimes pluck these people from other countries if they travel to the wrong place. But beyond that, experts say that there's also this sense kind of inside the U. S. government that there have to be consequences for these sorts of cyberattacks. There have to be consequences to try to deter them from happening in the future. And also, experts and U. S. officials say that, for example, China hated it when the U. S. charged Chinese military officials back in 2014 with hacking and say that the Chinese still complain about that. But it's an open question what exactly consequences are going to look like. But again, this is just one tool in kind of the government's toolbox. SHAPIRO: And briefly, do you expect this to have any impact on the effort to negotiate a nuclear deal with North Korea? LUCAS: Well, there's been no comment so far from North Korea about the charges. But North Korea's very strategic about what they respond to. They may ignore this if they view that that is in their interests, or they may pounce on it and take a swipe at the U. S. But we've had kind of warm words in the past day or so from Kim Jong Un and President Trump, so maybe there will be momentum back in those talks. SHAPIRO: NPR's Ryan Lucas, thanks. LUCAS: Thank you.", "section": "National Security", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-06-642565632": {"title": "What Happens When A.I. Takes The Wheel? : NPR", "url": "https://www.npr.org/2018/09/06/642565632/what-happens-when-a-i-takes-the-wheel", "author": "No author found", "published_date": "2018-09-06", "content": "", "section": "Author Interviews", "disclaimer": ""}, "2018-09-06-645140933": {"title": "Facebook Use Changing Among Young Users : NPR", "url": "https://www.npr.org/2018/09/06/645140933/facebook-use-changing-among-young-users", "author": "No author found", "published_date": "2018-09-06", "content": "STEVE INSKEEP, HOST: Facebook has a problem with younger users. This is not entirely new. The common story goes that young people drifted away from Facebook as soon as they found out their grandparents were on it. But a survey by the Pew Research Center offers some details about young people, Facebook and privacy. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: There's this constant handwringing about kids on social media these days. But American teens might just be generation privacy. Aaron Smith is an associate director at the Pew Research Center. AARON SMITH: Despite the common perception that young adults are not privacy conscious at all, they are actually very active in managing their online presences in a pretty substantial way. GARSD: In a new survey, Pew found that nearly two-thirds of young users have adjusted their privacy settings in the last year. Users over 65 - only a third have done that. The survey also found that nearly half of young users between the ages of 18 and 29 have even deleted the Facebook app from their phone. But how much of that deleting is due to privacy concerns or just the fact that Facebook might not be cool enough for them? DEBRA AHO WILLIAMSON: They think Facebook is for old people. GARSD: Debra Aho Williamson is a principal analyst at research firm eMarketer. She says the jury is still out over whether this will affect Facebook's bottom line. After all, Facebook owns Instagram. And the younger generations. . . WILLIAMSON: They would prefer to use a more visual type of communication like Instagram or Snapchat. GARSD: Sure, Snapchat, Instagram, they're prettier and more colorful. And they can also be more private than Facebook. ALISA TRUZALINO: You know, you can send something to someone and it goes away after like a few minutes, so. . . GARSD: Alisa Truzalino (ph) is 19 years old. Lately, she doesn't use Facebook much at all. But it's not because she's worried about Russian or Iranian interference campaigns, data mining or fake news. She just doesn't feel comfortable. TRUZALINO: You know, like, anyone can look you up on social media and see what you're doing. And then like, you can lose out on job opportunities. People won't take you seriously and stuff like that, so. . . GARSD: Kids today, they might be onto something. A snap and an Instagram story lasts a little bit. That Facebook album featuring your weekend in Vegas back in 2016? It's there forever. Jasmine Garsd, NPR News, New York. STEVE INSKEEP, HOST:  Facebook has a problem with younger users. This is not entirely new. The common story goes that young people drifted away from Facebook as soon as they found out their grandparents were on it. But a survey by the Pew Research Center offers some details about young people, Facebook and privacy. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: There's this constant handwringing about kids on social media these days. But American teens might just be generation privacy. Aaron Smith is an associate director at the Pew Research Center. AARON SMITH: Despite the common perception that young adults are not privacy conscious at all, they are actually very active in managing their online presences in a pretty substantial way. GARSD: In a new survey, Pew found that nearly two-thirds of young users have adjusted their privacy settings in the last year. Users over 65 - only a third have done that. The survey also found that nearly half of young users between the ages of 18 and 29 have even deleted the Facebook app from their phone. But how much of that deleting is due to privacy concerns or just the fact that Facebook might not be cool enough for them? DEBRA AHO WILLIAMSON: They think Facebook is for old people. GARSD: Debra Aho Williamson is a principal analyst at research firm eMarketer. She says the jury is still out over whether this will affect Facebook's bottom line. After all, Facebook owns Instagram. And the younger generations. . . WILLIAMSON: They would prefer to use a more visual type of communication like Instagram or Snapchat. GARSD: Sure, Snapchat, Instagram, they're prettier and more colorful. And they can also be more private than Facebook. ALISA TRUZALINO: You know, you can send something to someone and it goes away after like a few minutes, so. . . GARSD: Alisa Truzalino (ph) is 19 years old. Lately, she doesn't use Facebook much at all. But it's not because she's worried about Russian or Iranian interference campaigns, data mining or fake news. She just doesn't feel comfortable. TRUZALINO: You know, like, anyone can look you up on social media and see what you're doing. And then like, you can lose out on job opportunities. People won't take you seriously and stuff like that, so. . . GARSD: Kids today, they might be onto something. A snap and an Instagram story lasts a little bit. That Facebook album featuring your weekend in Vegas back in 2016? It's there forever. Jasmine Garsd, NPR News, New York.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-07-645565601": {"title": "A 55-Square-Mile Wind Farm Is Now Operating Off England's Shore : NPR", "url": "https://www.npr.org/2018/09/07/645565601/a-55-square-mile-wind-farm-is-now-operating-off-englands-shore", "author": "No author found", "published_date": "2018-09-07", "content": "", "section": "Energy", "disclaimer": ""}, "2018-09-07-645524670": {"title": "British Airways Says Customers' Financial Data Was Hacked In 380,000 Transactions : NPR", "url": "https://www.npr.org/2018/09/07/645524670/british-airways-says-customers-financial-data-was-hacked-in-380-00-transactions", "author": "No author found", "published_date": "2018-09-07", "content": "", "section": "Business", "disclaimer": ""}, "2018-09-09-645704776": {"title": "'Swiped': A Documentary Swipes Left On Dating Apps : NPR", "url": "https://www.npr.org/2018/09/09/645704776/a-documentary-swipes-left-on-dating-apps", "author": "No author found", "published_date": "2018-09-09", "content": "MICHEL MARTIN, HOST: Now, this is one of those conversations that's going to be so obvious to some people that they might wonder why we're having it. And here, I'm talking about the 40 million Americans who currently participate in some form of online dating, using apps like Tinder or Bumble or Hinge. But for others, well, there are revelations aplenty in the new HBO documentary \"Swiped: Hooking Up In The Digital Age,\" including how these apps and this method of meeting people could be changing how we think about relationships. Here's a clip. (SOUNDBITE OF DOCUMENTARY, \"SWIPED: HOOKING UP IN THE DIGITAL AGE\")UNIDENTIFIED PERSON #1: Just in, like, economic terms, if you have a surplus of options, then the value goes down. I could see a Tinder profile that I'm, like, excited about. If I met that person in real life, I would have this, like, sense of urgency. But I think on Tinder, if I see that profile, well, then I just swipe one way or another. I'm just swiping. And then, there's somebody else, immediately. MARTIN: Nancy Jo Sales is an award-winning journalist and best-selling author, but \"Swiped\" is her first film. And she's with us now from our studios in New York. Nancy Jo, thanks so much for joining us. NANCY JO SALES: Thank you for having me. MARTIN: So I'm just going to go right to the point. It's a pretty bleak picture that you paint, and this will come as no surprise to people who date. And some of this might be a gendered response. I'm just going to admit that upfront. I know very few women who find this experience awesome. . . SALES: (Laughter). MARTIN: I'm - I mean, I don't know. If - and. . . SALES: Sorry to laugh. I don't mean to laugh. . . MARTIN: Yeah. SALES: It was a laugh of recognition. MARTIN: Yeah. And part of it is because it's that these apps are very visually oriented. They're very focused on appearance, on looks, on superficial looks, on how you look in the five seconds that somebody's going to look at your profile picture but also the fact that women feel like they've been commodified, you know? SALES: Yeah. MARTIN: They're just - they're a commodity now. And, presumably, men feel that way, too. But they seem to feel that way less. Did you suspect that going in, or is that something that emerged from your reporting? SALES: I understand what you mean about a bleak picture, but I think the bleakness comes from the technology itself. I think that what the film is trying to do is to get us to look at the technology and what it means and what it's doing to us, how it's changing our culture, how it's changing the way we treat each other, how we interact. And I think that some of these results and ramifications are pretty bleak. But what I wanted to do and what I tried to do in the film was - Number 1 - to get people think about that and examine that but also to bring to life and humanize the people in these stacks of pictures. MARTIN: Well, to that end, you have some very - I don't know - heartbreaking encounters with people talking about their experiences on online dating. And there's a scene where a group of African-Americans are talking about their experiences with online dating. I'm just going to play a short clip. And yes, I'm going to bleep some of the language. (SOUNDBITE OF DOCUMENTARY, \"SWIPED: HOOKING UP IN THE DIGITAL AGE\")UNIDENTIFIED PERSON #2: Here's how you get treated as a black woman when you're at a dating site. Either they don't want to [expletive] with you because you're black - I don't know why that freaks so many people out - or you're so exotic because you're black. I've never [expletive] a black girl before. MARTIN: Why is that? SALES: I think that dating apps normalize things that are unacceptable. And - one of the things we just talked about, objectification. And another thing I think has - is we heard about racism because it's somehow considered, on these apps, OK to choose what you want in a romantic partner. And, sometimes, that veers towards what some of our African-American characters are experiencing as racism. And that's not OK, you know? Imagine being a woman age 22, 23, 24 and going on a dating app and seeing - you know, swiping on people and seeing a profile, which they said they saw pretty regularly, that actually said, and this is a quote, \"no blacks. \"MARTIN: One of the things that was - I think many people will find fascinating is you got to interview the makers of a number of these apps, including Tinder, Bumble and Hinge executives. What struck you from those conversations? SALES: I would say my favorite part in the film, in a way, is - and just in terms of revelations - as talking to Jonathan Badeen, who is the CSO of Tinder. And he is the person who invented the swipe. Now, the swipe is - you know, the swipe mechanic, it's called, where you swipe on someone's face or picture, right or left, hot or, you know, hot or not. But I was so struck by him talking about inventing the swipe and how he was quite open in discussing how he had based it in part on studies, psychological studies about controlling behavior and causing people to become addicted to things. MARTIN: You know, you confronted them about whether they thought about the deeper implications of what they have created. And I just want to play a short clip from an interview you had with the sociologist at Tinder. Her name is Jessica Carbino, and this is what she had to say. (SOUNDBITE OF DOCUMENTARY, \"SWIPED: HOOKING UP IN THE DIGITAL AGE\")JESSICA CARBINO: It's incredible, the number of people who've met via Tinder. SALES: Some people do use it to have more casual relationships. I mean, it is used that way as well. CARBINO: Certainly. People meet people at church or meet people at their schools, and they have casual relationships with them as well. MARTIN: So what's going on? Is that this - what is that? I mean, you're making a specific point, which is that you're changing people's behavior. And you're changing - what? - thousands of years of social history - right? - with these apps. And what do they. . . SALES: Tens of thousands. MARTIN: Yeah. And what do they say about that? SALES: I think that some of the things that they say about the apps are ridiculous, not just in this film but in interviews and elsewhere. And I think that it's marketing because I think that what they really are are businesses, and their real goal, overall, is to make money, you know? But they don't want us to think about that, you know? When I asked Jonathan Badeen - again, CSO of Tinder - you know, why did you guys make this app, you know, he didn't say so that people can fall in love and get married. What he said was, well, we were looking for disruption in the marketplace. They certainly have created disruption in the realm of love, sex and dating. MARTIN: How do you want people to - what do you want them to take from the film? I know that you do report this detail, that, according to the dating app Hinge, according to their research, 81 percent of Hinge users have never found a long-term relationship on any of these online dating, you know, apps. Is that the takeaway here? What do you think the takeaway is? SALES: I think that I would love for the film to raise a discussion around dating app culture and online dating and sexual violence. I was really not aware of this, I would say, relationship between dating apps and rape culture before I started interviewing young women for the film. There's a real problem with it, you know? And I took it to the heads of these companies in the film, and I did not find their responses satisfying. So I'm hoping that this conversation will begin in a real way, especially in the #MeToo moment (ph). We have, you know, women speaking up about sexual harassment, sexual assault. And yet the place where I would say it's likely that they're experiencing a lot of this the most in their dating lives, on dating apps, it's not being talked about. MARTIN: That's Nancy Jo Sales. She's the director of \"Swiped: Hooking Up In The Digital Age. \" It comes out tomorrow on HBO. Nancy Jo, thanks so much for talking to us. SALES: Thank you. MICHEL MARTIN, HOST:  Now, this is one of those conversations that's going to be so obvious to some people that they might wonder why we're having it. And here, I'm talking about the 40 million Americans who currently participate in some form of online dating, using apps like Tinder or Bumble or Hinge. But for others, well, there are revelations aplenty in the new HBO documentary \"Swiped: Hooking Up In The Digital Age,\" including how these apps and this method of meeting people could be changing how we think about relationships. Here's a clip. (SOUNDBITE OF DOCUMENTARY, \"SWIPED: HOOKING UP IN THE DIGITAL AGE\") UNIDENTIFIED PERSON #1: Just in, like, economic terms, if you have a surplus of options, then the value goes down. I could see a Tinder profile that I'm, like, excited about. If I met that person in real life, I would have this, like, sense of urgency. But I think on Tinder, if I see that profile, well, then I just swipe one way or another. I'm just swiping. And then, there's somebody else, immediately. MARTIN: Nancy Jo Sales is an award-winning journalist and best-selling author, but \"Swiped\" is her first film. And she's with us now from our studios in New York. Nancy Jo, thanks so much for joining us. NANCY JO SALES: Thank you for having me. MARTIN: So I'm just going to go right to the point. It's a pretty bleak picture that you paint, and this will come as no surprise to people who date. And some of this might be a gendered response. I'm just going to admit that upfront. I know very few women who find this experience awesome. . . SALES: (Laughter). MARTIN: I'm - I mean, I don't know. If - and. . . SALES: Sorry to laugh. I don't mean to laugh. . . MARTIN: Yeah. SALES: It was a laugh of recognition. MARTIN: Yeah. And part of it is because it's that these apps are very visually oriented. They're very focused on appearance, on looks, on superficial looks, on how you look in the five seconds that somebody's going to look at your profile picture but also the fact that women feel like they've been commodified, you know? SALES: Yeah. MARTIN: They're just - they're a commodity now. And, presumably, men feel that way, too. But they seem to feel that way less. Did you suspect that going in, or is that something that emerged from your reporting? SALES: I understand what you mean about a bleak picture, but I think the bleakness comes from the technology itself. I think that what the film is trying to do is to get us to look at the technology and what it means and what it's doing to us, how it's changing our culture, how it's changing the way we treat each other, how we interact. And I think that some of these results and ramifications are pretty bleak. But what I wanted to do and what I tried to do in the film was - Number 1 - to get people think about that and examine that but also to bring to life and humanize the people in these stacks of pictures. MARTIN: Well, to that end, you have some very - I don't know - heartbreaking encounters with people talking about their experiences on online dating. And there's a scene where a group of African-Americans are talking about their experiences with online dating. I'm just going to play a short clip. And yes, I'm going to bleep some of the language. (SOUNDBITE OF DOCUMENTARY, \"SWIPED: HOOKING UP IN THE DIGITAL AGE\") UNIDENTIFIED PERSON #2: Here's how you get treated as a black woman when you're at a dating site. Either they don't want to [expletive] with you because you're black - I don't know why that freaks so many people out - or you're so exotic because you're black. I've never [expletive] a black girl before. MARTIN: Why is that? SALES: I think that dating apps normalize things that are unacceptable. And - one of the things we just talked about, objectification. And another thing I think has - is we heard about racism because it's somehow considered, on these apps, OK to choose what you want in a romantic partner. And, sometimes, that veers towards what some of our African-American characters are experiencing as racism. And that's not OK, you know? Imagine being a woman age 22, 23, 24 and going on a dating app and seeing - you know, swiping on people and seeing a profile, which they said they saw pretty regularly, that actually said, and this is a quote, \"no blacks. \" MARTIN: One of the things that was - I think many people will find fascinating is you got to interview the makers of a number of these apps, including Tinder, Bumble and Hinge executives. What struck you from those conversations? SALES: I would say my favorite part in the film, in a way, is - and just in terms of revelations - as talking to Jonathan Badeen, who is the CSO of Tinder. And he is the person who invented the swipe. Now, the swipe is - you know, the swipe mechanic, it's called, where you swipe on someone's face or picture, right or left, hot or, you know, hot or not. But I was so struck by him talking about inventing the swipe and how he was quite open in discussing how he had based it in part on studies, psychological studies about controlling behavior and causing people to become addicted to things. MARTIN: You know, you confronted them about whether they thought about the deeper implications of what they have created. And I just want to play a short clip from an interview you had with the sociologist at Tinder. Her name is Jessica Carbino, and this is what she had to say. (SOUNDBITE OF DOCUMENTARY, \"SWIPED: HOOKING UP IN THE DIGITAL AGE\") JESSICA CARBINO: It's incredible, the number of people who've met via Tinder. SALES: Some people do use it to have more casual relationships. I mean, it is used that way as well. CARBINO: Certainly. People meet people at church or meet people at their schools, and they have casual relationships with them as well. MARTIN: So what's going on? Is that this - what is that? I mean, you're making a specific point, which is that you're changing people's behavior. And you're changing - what? - thousands of years of social history - right? - with these apps. And what do they. . . SALES: Tens of thousands. MARTIN: Yeah. And what do they say about that? SALES: I think that some of the things that they say about the apps are ridiculous, not just in this film but in interviews and elsewhere. And I think that it's marketing because I think that what they really are are businesses, and their real goal, overall, is to make money, you know? But they don't want us to think about that, you know? When I asked Jonathan Badeen - again, CSO of Tinder - you know, why did you guys make this app, you know, he didn't say so that people can fall in love and get married. What he said was, well, we were looking for disruption in the marketplace. They certainly have created disruption in the realm of love, sex and dating. MARTIN: How do you want people to - what do you want them to take from the film? I know that you do report this detail, that, according to the dating app Hinge, according to their research, 81 percent of Hinge users have never found a long-term relationship on any of these online dating, you know, apps. Is that the takeaway here? What do you think the takeaway is? SALES: I think that I would love for the film to raise a discussion around dating app culture and online dating and sexual violence. I was really not aware of this, I would say, relationship between dating apps and rape culture before I started interviewing young women for the film. There's a real problem with it, you know? And I took it to the heads of these companies in the film, and I did not find their responses satisfying. So I'm hoping that this conversation will begin in a real way, especially in the #MeToo moment (ph). We have, you know, women speaking up about sexual harassment, sexual assault. And yet the place where I would say it's likely that they're experiencing a lot of this the most in their dating lives, on dating apps, it's not being talked about. MARTIN: That's Nancy Jo Sales. She's the director of \"Swiped: Hooking Up In The Digital Age. \" It comes out tomorrow on HBO. Nancy Jo, thanks so much for talking to us. SALES: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-10-646422422": {"title": "How Facebook Has Shaped Democracy : NPR", "url": "https://www.npr.org/2018/09/10/646422422/how-facebook-has-shaped-democracy", "author": "No author found", "published_date": "2018-09-10", "content": "AUDIE CORNISH, HOST: And I'm Audie Cornish with All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CORNISH: This month, we're looking at how technology shapes our democracy. And there's no way to talk about the power tech has over our choices without looking at Facebook. The company has had a rocky year as it has had to face up to questions about how Facebook was used to influence the 2016 election. (SOUNDBITE OF ARCHIVED RECORDING)CHUCK GRASSLEY: We welcome everyone to today's hearing on Facebook's social media privacy. CORNISH: And that forced a rare appearance by founder and CEO Mark Zuckerberg before lawmakers on Capitol Hill in April. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: We didn't take a broad enough view of our responsibility, and that was a big mistake. And it was my mistake. And I'm sorry. CORNISH: New Yorker writer Evan Osnos spent the summer interviewing Zuckerberg and dozens of people inside and out of the company. He's here to talk more about it. Welcome, Evan. EVAN OSNOS: Thanks, Audie. CORNISH: I want to talk about some of the scandals the company has faced over the last year. Let's start with Cambridge Analytica and this conversation about how Facebook was used in the Russian influence campaign in the 2016 U. S. election. At the end of the day, does Zuckerberg get why people look at them as a problem and a problem in the context of this 2016 meddling? OSNOS: I think it's beginning to sink in, but it hasn't sunk in probably enough. He's coming to grips with the fact that people no longer view Facebook as this novel, new addition to their lives. And that has been whiplash for him because as he said, you know, for the first 10 years of Facebook, basically people gave him positive press all the time, and he never really thought much more about it. And now there is this very serious national conversation about what role technology and specifically social media plays in our democracy. . . CORNISH: In the spread of misinformation. OSNOS: Exactly, the spread of misinformation and addiction and the effects on our health, on our happiness, on our sense of alienation or connection. And he has been slower than he should have been to embrace that. And he's kind of creeping up to the point of acknowledging that. CORNISH: You write that as Facebook expanded, so did its blind spots in regards to global communities. And you give the example of Sri Lanka, where after a Buddhist mob attacked Muslims this spring over a false rumor, a presidential adviser was quoted as saying, the germs are ours, but Facebook is the wind. What are some of those blind spots? OSNOS: Well, for a long time I think Facebook was able to explain to itself that even when there were negative side effects of the growth of social media, things like violence in Sri Lanka or in Myanmar or in India, that those were the necessary costs of historical change. I think that the internal narrative was our goal, our central purpose is to connect people. And that language almost took on a kind of talismanic quality. And what they're recognizing and coming to terms with is that there are a lot of negative consequences of that. CORNISH: Now, I want to play a clip from the congressional hearing back in April. Utah Senator Orrin Hatch asked Zuckerberg a question. (SOUNDBITE OF ARCHIVED RECORDING)ORRIN HATCH: If so, how do you sustain a business model in which users don't pay for your service? ZUCKERBERG: Senator, we run ads. HATCH: I see. CORNISH: An exchange like this does not give one confidence that Congress is ready to regulate Facebook in. . . OSNOS: Right. CORNISH: . . . A serious way. How was it received inside Facebook? OSNOS: Facebook was fundamentally relieved when Mark Zuckerberg went in front of the Senate because there really weren't a whole lot of hard questions he couldn't answer. But it was also a sign that there is change that is going to happen from Washington. This period when Washington was not going to regulate them at all is coming to an end. And I think what they're trying to do is steer that process as much as they can. CORNISH: In the meantime, there's been a vote, so to speak, from Wall Street because in July, Facebook's stock. . . OSNOS: Yeah. CORNISH: . . . Experienced the largest one-day drop on Wall Street history. Is there a sense that there's a kind of reckoning? OSNOS: As big as that drop was, actually, inside Facebook they said, look; this is the routine part of business. We're investing - as they put it into security - and we're going to take a hit on our profits. But it is a clear sign that the way the world thinks about Facebook is entering a new phase. The downside consequences matter. And they're going to be evaluated on the basis of that. And I think Wall Street's momentary loss of confidence was a sign of something bigger to come. CORNISH: In the meantime, you found a CEO who is fairly isolated - isolated from. . . OSNOS: Right. CORNISH: . . . Criticism, isolated from feedback within his own company. Am I reading that right? OSNOS: Absolutely. One of the most interesting things that I sensed about him is that he is aware of his own seclusion. As he put it, you know, being in the Silicon Valley bubble is a problem. He went on a listening tour to 30 states. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: I'm here with my friend Pete Buttigieg, who's the mayor here. OSNOS: And the truth was it felt forced. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: So I've been going around to different states to see how, you know, different communities are working across the country. CORNISH: Another example of how he's tried to do this - he once posted a Facebook Live video in 2016 just kind of, like, at home. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: Hi, I'm live here in my backyard smoking some meats. So I'm here, and I figured it would just be fun to see what everyone is doing and hanging out on the Internet. So what's up? What are you guys all doing this afternoon? OSNOS: What he's trying to do is figure out a way where he can actually not find himself as out of touch with where the American public is as the company has been over the last two years. And they've been sort of racing to try to rebuild confidence. But you can't do that by going out on a listening tour. You have to do that by building it into the leadership of your company. Who are the people you're listening to? Are they giving you bad news? Are they giving you unwelcome opinions? Because if everybody around you just says more and better and bigger, then you're not going to be able to make those hard choices. CORNISH: What surprised you about the Mark Zuckerberg of today? OSNOS: I think he has come to grips with the fact that the first wave of public fascination with social media is ending. But he doesn't yet have an answer for what's going to follow. He doesn't know what it means to be more than just a tool that is put into the hands of the public, and then they decide what to do with it. He doesn't have answers to the questions of, how do we define the boundaries of free speech; how do we control the risks of contributing to violence? These are really hard problems. And they're not technical problems. They're problems of human affairs. And that is new territory for him. CORNISH: How does Mark Zuckerberg see Facebook's role when it comes to elections, politics, democracy? Does he have a grasp of that? OSNOS: He does now. It's been a very painful process to learn it. I think if you'd asked him this in October of 2016, right before the election, he would have said, look; it's up to national governments to protect their own institutions and protect their democracy. He's come to recognize that there is a unique responsibility that comes with having a platform that is larger than any country on Earth. And that means - and it's a slightly daunting prospect that the integrity and sanctity of elections around the world do now rest to some degree on the shoulders of Mark Zuckerberg and his employees at Facebook. And he does take that seriously. CORNISH: Are they ready? OSNOS: We won't know, to be blunt, until those elections happen. There have been a few elections this year in which they had been able to prevent the kind of rampant disinformation that was such a problem in 2016. They've taken down a couple of big information operations in the last few months. But they don't have a measurement system that helps them know before the fact. So there will be a lot of eyes on Facebook during the midterm elections. CORNISH: New Yorker writer Evan Osnos - his story \"Ghost In The Machine: Can Mark Zuckerberg Fix Facebook Before It Breaks Democracy? \" is out now. Thanks for speaking with us. OSNOS: Thanks for having me. AUDIE CORNISH, HOST:  And I'm Audie Cornish with All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CORNISH: This month, we're looking at how technology shapes our democracy. And there's no way to talk about the power tech has over our choices without looking at Facebook. The company has had a rocky year as it has had to face up to questions about how Facebook was used to influence the 2016 election. (SOUNDBITE OF ARCHIVED RECORDING) CHUCK GRASSLEY: We welcome everyone to today's hearing on Facebook's social media privacy. CORNISH: And that forced a rare appearance by founder and CEO Mark Zuckerberg before lawmakers on Capitol Hill in April. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: We didn't take a broad enough view of our responsibility, and that was a big mistake. And it was my mistake. And I'm sorry. CORNISH: New Yorker writer Evan Osnos spent the summer interviewing Zuckerberg and dozens of people inside and out of the company. He's here to talk more about it. Welcome, Evan. EVAN OSNOS: Thanks, Audie. CORNISH: I want to talk about some of the scandals the company has faced over the last year. Let's start with Cambridge Analytica and this conversation about how Facebook was used in the Russian influence campaign in the 2016 U. S. election. At the end of the day, does Zuckerberg get why people look at them as a problem and a problem in the context of this 2016 meddling? OSNOS: I think it's beginning to sink in, but it hasn't sunk in probably enough. He's coming to grips with the fact that people no longer view Facebook as this novel, new addition to their lives. And that has been whiplash for him because as he said, you know, for the first 10 years of Facebook, basically people gave him positive press all the time, and he never really thought much more about it. And now there is this very serious national conversation about what role technology and specifically social media plays in our democracy. . . CORNISH: In the spread of misinformation. OSNOS: Exactly, the spread of misinformation and addiction and the effects on our health, on our happiness, on our sense of alienation or connection. And he has been slower than he should have been to embrace that. And he's kind of creeping up to the point of acknowledging that. CORNISH: You write that as Facebook expanded, so did its blind spots in regards to global communities. And you give the example of Sri Lanka, where after a Buddhist mob attacked Muslims this spring over a false rumor, a presidential adviser was quoted as saying, the germs are ours, but Facebook is the wind. What are some of those blind spots? OSNOS: Well, for a long time I think Facebook was able to explain to itself that even when there were negative side effects of the growth of social media, things like violence in Sri Lanka or in Myanmar or in India, that those were the necessary costs of historical change. I think that the internal narrative was our goal, our central purpose is to connect people. And that language almost took on a kind of talismanic quality. And what they're recognizing and coming to terms with is that there are a lot of negative consequences of that. CORNISH: Now, I want to play a clip from the congressional hearing back in April. Utah Senator Orrin Hatch asked Zuckerberg a question. (SOUNDBITE OF ARCHIVED RECORDING) ORRIN HATCH: If so, how do you sustain a business model in which users don't pay for your service? ZUCKERBERG: Senator, we run ads. HATCH: I see. CORNISH: An exchange like this does not give one confidence that Congress is ready to regulate Facebook in. . . OSNOS: Right. CORNISH: . . . A serious way. How was it received inside Facebook? OSNOS: Facebook was fundamentally relieved when Mark Zuckerberg went in front of the Senate because there really weren't a whole lot of hard questions he couldn't answer. But it was also a sign that there is change that is going to happen from Washington. This period when Washington was not going to regulate them at all is coming to an end. And I think what they're trying to do is steer that process as much as they can. CORNISH: In the meantime, there's been a vote, so to speak, from Wall Street because in July, Facebook's stock. . . OSNOS: Yeah. CORNISH: . . . Experienced the largest one-day drop on Wall Street history. Is there a sense that there's a kind of reckoning? OSNOS: As big as that drop was, actually, inside Facebook they said, look; this is the routine part of business. We're investing - as they put it into security - and we're going to take a hit on our profits. But it is a clear sign that the way the world thinks about Facebook is entering a new phase. The downside consequences matter. And they're going to be evaluated on the basis of that. And I think Wall Street's momentary loss of confidence was a sign of something bigger to come. CORNISH: In the meantime, you found a CEO who is fairly isolated - isolated from. . . OSNOS: Right. CORNISH: . . . Criticism, isolated from feedback within his own company. Am I reading that right? OSNOS: Absolutely. One of the most interesting things that I sensed about him is that he is aware of his own seclusion. As he put it, you know, being in the Silicon Valley bubble is a problem. He went on a listening tour to 30 states. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: I'm here with my friend Pete Buttigieg, who's the mayor here. OSNOS: And the truth was it felt forced. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: So I've been going around to different states to see how, you know, different communities are working across the country. CORNISH: Another example of how he's tried to do this - he once posted a Facebook Live video in 2016 just kind of, like, at home. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: Hi, I'm live here in my backyard smoking some meats. So I'm here, and I figured it would just be fun to see what everyone is doing and hanging out on the Internet. So what's up? What are you guys all doing this afternoon? OSNOS: What he's trying to do is figure out a way where he can actually not find himself as out of touch with where the American public is as the company has been over the last two years. And they've been sort of racing to try to rebuild confidence. But you can't do that by going out on a listening tour. You have to do that by building it into the leadership of your company. Who are the people you're listening to? Are they giving you bad news? Are they giving you unwelcome opinions? Because if everybody around you just says more and better and bigger, then you're not going to be able to make those hard choices. CORNISH: What surprised you about the Mark Zuckerberg of today? OSNOS: I think he has come to grips with the fact that the first wave of public fascination with social media is ending. But he doesn't yet have an answer for what's going to follow. He doesn't know what it means to be more than just a tool that is put into the hands of the public, and then they decide what to do with it. He doesn't have answers to the questions of, how do we define the boundaries of free speech; how do we control the risks of contributing to violence? These are really hard problems. And they're not technical problems. They're problems of human affairs. And that is new territory for him. CORNISH: How does Mark Zuckerberg see Facebook's role when it comes to elections, politics, democracy? Does he have a grasp of that? OSNOS: He does now. It's been a very painful process to learn it. I think if you'd asked him this in October of 2016, right before the election, he would have said, look; it's up to national governments to protect their own institutions and protect their democracy. He's come to recognize that there is a unique responsibility that comes with having a platform that is larger than any country on Earth. And that means - and it's a slightly daunting prospect that the integrity and sanctity of elections around the world do now rest to some degree on the shoulders of Mark Zuckerberg and his employees at Facebook. And he does take that seriously. CORNISH: Are they ready? OSNOS: We won't know, to be blunt, until those elections happen. There have been a few elections this year in which they had been able to prevent the kind of rampant disinformation that was such a problem in 2016. They've taken down a couple of big information operations in the last few months. But they don't have a measurement system that helps them know before the fact. So there will be a lot of eyes on Facebook during the midterm elections. CORNISH: New Yorker writer Evan Osnos - his story \"Ghost In The Machine: Can Mark Zuckerberg Fix Facebook Before It Breaks Democracy? \" is out now. Thanks for speaking with us. OSNOS: Thanks for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-11-646724291": {"title": "A Massive Floating Boom Is Supposed To Clean Up The Pacific. Can It Work? : NPR", "url": "https://www.npr.org/2018/09/11/646724291/a-massive-floating-boom-is-supposed-to-clean-up-the-pacific-can-it-work", "author": "No author found", "published_date": "2018-09-11", "content": "", "section": "Technology", "disclaimer": ""}, "2018-09-11-646333029": {"title": "DOJ Probe Into Bias At Tech Companies Should Include Democrats, California AG Says : NPR", "url": "https://www.npr.org/2018/09/11/646333029/justice-probe-into-bias-at-tech-companies-should-include-democrats-california-ag", "author": "No author found", "published_date": "2018-09-11", "content": "", "section": "Technology", "disclaimer": ""}, "2018-09-11-643722787": {"title": "For Many In Venezuela, Social Media Is A Matter Of Life And Death : NPR", "url": "https://www.npr.org/2018/09/11/643722787/for-many-in-venezuela-social-media-is-a-matter-of-life-and-death", "author": "No author found", "published_date": "2018-09-11", "content": "RACHEL MARTIN, HOST: President Trump uses the phrase fake news a lot to criticize media stories he doesn't like. There's something similar happening in Venezuela. Last week, the government said social media is exacerbating the spread of fake news against the regime. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: Guillermo does not exist - on social media at least. He has a Facebook account, but he doesn't publicly use his real name. He doesn't have a profile picture, no location, and he never posts a single thing. He mostly logs in to read about sports. GUILLERMO: (Speaking Spanish). GARSD: Guillermo asks that his last name be withheld. He worries about his family. They're still in Venezuela. Over a million Venezuelans have left the country in the last two years, and he worries that if he posts anything indicating he might have money. . . GUILLERMO: (Speaking Spanish). GARSD: He says, \"someone I know or who knows my family could kidnap them just because of a picture because they might think that I can pay a ransom of thousands of dollars. \" According to the U. S. Department of State, Venezuela is one of the most violent countries in the world. Guillermo says his friends living abroad are also very careful with social media. For him, social media is more danger than it's worth. But for a man named Javier Rojo in Caracas, it's a lifeline - literally. He's a pharmacist in a country with severe medicine shortages. JAVIER ROJO: (Speaking Spanish). GARSD: \"There are shortages of everything,\" he says. \"Insulin is especially needed. \" Rojo's pharmacy has a Twitter account, which he uses to announce what they don't have, medicines people desperately need. For him, social media is truly a matter of life and death. Like in one recent post, he retweets a diabetic who is urgently seeking insulin. We haven't had insulin for over a month, Rojo tweets out. In another, he retweets someone looking for methotrexate for arthritis. We don't have this, he announces. Please, retweet. On occasion, a medicine is found. A lot of times, the tweets are like messages in a bottle - no response, no follow-ups. Since he took over the account in February, the pharmacy has gone from a few dozen followers to nearly 30,000. One of the things that motivated him to get on Twitter was a phone call back in March. ROJO: (Speaking Spanish). GARSD: \"A person called looking for an antidepressant, which they couldn't afford. \"ROJO: (Speaking Spanish). GARSD: \"And the person responded, OK, I'm going to have to kill myself. And they hung up. \" Rojo says it devastated him and motivated him to double down on the pharmacy's Twitter account that he had recently started. Eric Farnsworth is a Venezuela expert. He says social media communication is critical in the country. ERIC FARNSWORTH: In some ways, social media has been a lifeline for common Venezuelan citizens who don't have access to the normal newspapers or radio or television that they had been accustomed to when Venezuela was a democracy. GARSD: Farnsworth is the vice president of the Council of the Americas, a think tank and business organization. He says social media can also be dangerous for many Venezuelans still living there. FARNSWORTH: There is a real self-censorship now in Venezuela, which is to say people are very careful about what they say because you don't know who is listening, and you don't know who's hearing what you say to whom. GARSD: In Venezuela but also here in the U. S. , people like Guillermo, the guy who stays anonymous on Facebook for his family's safety. But back in Caracas, Javier Rojo says things are so bad. . . ROJO: (Speaking Spanish). GARSD: \"What else could happen to us? What else? \" Rojo suspects the government is already monitoring social media. He says you have to be careful, be subtle. Last month in between his posts about insulin and antibiotics, he tweeted about how sad it makes him to see people leaving Venezuela. It was written in all lower case. It would have been easy to miss the random letters he capitalized throughout the tweet - L-I-B-E-R-T-A-D - libertad - freedom. Jasmine Garsd, NPR News, New York. RACHEL MARTIN, HOST:  President Trump uses the phrase fake news a lot to criticize media stories he doesn't like. There's something similar happening in Venezuela. Last week, the government said social media is exacerbating the spread of fake news against the regime. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: Guillermo does not exist - on social media at least. He has a Facebook account, but he doesn't publicly use his real name. He doesn't have a profile picture, no location, and he never posts a single thing. He mostly logs in to read about sports. GUILLERMO: (Speaking Spanish). GARSD: Guillermo asks that his last name be withheld. He worries about his family. They're still in Venezuela. Over a million Venezuelans have left the country in the last two years, and he worries that if he posts anything indicating he might have money. . . GUILLERMO: (Speaking Spanish). GARSD: He says, \"someone I know or who knows my family could kidnap them just because of a picture because they might think that I can pay a ransom of thousands of dollars. \" According to the U. S. Department of State, Venezuela is one of the most violent countries in the world. Guillermo says his friends living abroad are also very careful with social media. For him, social media is more danger than it's worth. But for a man named Javier Rojo in Caracas, it's a lifeline - literally. He's a pharmacist in a country with severe medicine shortages. JAVIER ROJO: (Speaking Spanish). GARSD: \"There are shortages of everything,\" he says. \"Insulin is especially needed. \" Rojo's pharmacy has a Twitter account, which he uses to announce what they don't have, medicines people desperately need. For him, social media is truly a matter of life and death. Like in one recent post, he retweets a diabetic who is urgently seeking insulin. We haven't had insulin for over a month, Rojo tweets out. In another, he retweets someone looking for methotrexate for arthritis. We don't have this, he announces. Please, retweet. On occasion, a medicine is found. A lot of times, the tweets are like messages in a bottle - no response, no follow-ups. Since he took over the account in February, the pharmacy has gone from a few dozen followers to nearly 30,000. One of the things that motivated him to get on Twitter was a phone call back in March. ROJO: (Speaking Spanish). GARSD: \"A person called looking for an antidepressant, which they couldn't afford. \" ROJO: (Speaking Spanish). GARSD: \"And the person responded, OK, I'm going to have to kill myself. And they hung up. \" Rojo says it devastated him and motivated him to double down on the pharmacy's Twitter account that he had recently started. Eric Farnsworth is a Venezuela expert. He says social media communication is critical in the country. ERIC FARNSWORTH: In some ways, social media has been a lifeline for common Venezuelan citizens who don't have access to the normal newspapers or radio or television that they had been accustomed to when Venezuela was a democracy. GARSD: Farnsworth is the vice president of the Council of the Americas, a think tank and business organization. He says social media can also be dangerous for many Venezuelans still living there. FARNSWORTH: There is a real self-censorship now in Venezuela, which is to say people are very careful about what they say because you don't know who is listening, and you don't know who's hearing what you say to whom. GARSD: In Venezuela but also here in the U. S. , people like Guillermo, the guy who stays anonymous on Facebook for his family's safety. But back in Caracas, Javier Rojo says things are so bad. . . ROJO: (Speaking Spanish). GARSD: \"What else could happen to us? What else? \" Rojo suspects the government is already monitoring social media. He says you have to be careful, be subtle. Last month in between his posts about insulin and antibiotics, he tweeted about how sad it makes him to see people leaving Venezuela. It was written in all lower case. It would have been easy to miss the random letters he capitalized throughout the tweet - L-I-B-E-R-T-A-D - libertad - freedom. Jasmine Garsd, NPR News, New York.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-12-647040758": {"title": "Does Facebook Really Work? People Question Effectiveness Of Ads : NPR", "url": "https://www.npr.org/2018/09/12/647040758/advertising-on-facebook-is-it-worth-it", "author": "No author found", "published_date": "2018-09-12", "content": "AUDIE CORNISH, HOST: Facebook gets most of its revenue from advertising - more than $13 billion between April and June of this year. It sells itself as a simple and effective way to reach a target audience. That targeting has gotten Facebook into hot water with users over privacy concerns. And for businesses, the larger looming question is, is it worth it? NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: Some version of this might have happened to you. You hit like on a healthy eating Facebook page, and soon enough you start getting advertisements for fitness groups and weight loss products which you're not interested in. A new class-action lawsuit claims that Facebook isn't delivering on its advertising promises. Here's Seth Lesser, the lawyer who is suing Facebook. SETH LESSER: Facebook's advertising pitch is that you can put into the program exactly your target audience. GARSD: Where people live, how much money they earn. Do they have a college education? LESSER: And Facebook says, we can get you those such people at 89 percent accuracy. GARSD: Facebook says it can't guarantee every ad will be this effective. Lesser says it's misleading. His client is investorvillage. com, a site that offers online discussion forums on investing. It recently spent around $1,600 on two Facebook ad campaigns. They were targeted at people with an income of at least $250,000 and a college education, people who would be interested in the stock market. Their ads got a lot of likes, but. . . LESSER: These people were not those it had asked to be targeted. GARSD: Investor Village says at least 40 percent of them were from outside the target audience, people who just hit like on anything. The advertising industry has obsessed over reaching its target audience for decades. In the 1960s-based show \"Mad Men,\" ad exec Don Draper racks his brains about how to sell cereal. (SOUNDBITE OF TV SHOW, \"MAD MEN\")JON HAMM: (As Don Draper) Kids see the giant bowl of cereal, and they smile because, you know, they'd eat a box of it if they could. And moms see it, and they get this twinge of how little their kid still is even though they have to deal with life. Get those two together in a market, and I think we're going to sell some cereal. GARSD: OK, but he couldn't be sure how many people would see a billboard or read a magazine ad. And then Facebook happened, over 2 billion people offering information about what they love and what they hate, where they've been and where they'd like to go. It's Don Draper's wildest dream come true. But it turns out that even with all this information, Don Draper's job is not that much easier. SALEEM ALHABASH: So there's a depth of data that is immeasurable now that is not matched with the types of insights that we know about consumers to make advertising more effective. GARSD: Saleem Alhabash is an associate professor at Michigan State University's department of advertising. He says advertisers today have so much detailed information. It can actually make targeting the right audience difficult. Marcus Collins is an executive at Doner, an advertising agency. MARCUS COLLINS: We put an undue pressure on these technology platforms that we don't put on traditional media. And it's not fair. GARSD: He says Facebook is just a tool. Millions of advertisers are drawn to it. But sometimes, like in the case of Investor Village, it may not work. COLLINS: Facebook isn't the magic potion to make presto change-o (ph), you reach everybody, and everyone will start buying your products. GARSD: No matter how much data we have, Collins says convincing people to buy what you're selling is always going to be more than just a numbers game. Jasmine Garsd, NPR News, New York. AUDIE CORNISH, HOST:  Facebook gets most of its revenue from advertising - more than $13 billion between April and June of this year. It sells itself as a simple and effective way to reach a target audience. That targeting has gotten Facebook into hot water with users over privacy concerns. And for businesses, the larger looming question is, is it worth it? NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: Some version of this might have happened to you. You hit like on a healthy eating Facebook page, and soon enough you start getting advertisements for fitness groups and weight loss products which you're not interested in. A new class-action lawsuit claims that Facebook isn't delivering on its advertising promises. Here's Seth Lesser, the lawyer who is suing Facebook. SETH LESSER: Facebook's advertising pitch is that you can put into the program exactly your target audience. GARSD: Where people live, how much money they earn. Do they have a college education? LESSER: And Facebook says, we can get you those such people at 89 percent accuracy. GARSD: Facebook says it can't guarantee every ad will be this effective. Lesser says it's misleading. His client is investorvillage. com, a site that offers online discussion forums on investing. It recently spent around $1,600 on two Facebook ad campaigns. They were targeted at people with an income of at least $250,000 and a college education, people who would be interested in the stock market. Their ads got a lot of likes, but. . . LESSER: These people were not those it had asked to be targeted. GARSD: Investor Village says at least 40 percent of them were from outside the target audience, people who just hit like on anything. The advertising industry has obsessed over reaching its target audience for decades. In the 1960s-based show \"Mad Men,\" ad exec Don Draper racks his brains about how to sell cereal. (SOUNDBITE OF TV SHOW, \"MAD MEN\") JON HAMM: (As Don Draper) Kids see the giant bowl of cereal, and they smile because, you know, they'd eat a box of it if they could. And moms see it, and they get this twinge of how little their kid still is even though they have to deal with life. Get those two together in a market, and I think we're going to sell some cereal. GARSD: OK, but he couldn't be sure how many people would see a billboard or read a magazine ad. And then Facebook happened, over 2 billion people offering information about what they love and what they hate, where they've been and where they'd like to go. It's Don Draper's wildest dream come true. But it turns out that even with all this information, Don Draper's job is not that much easier. SALEEM ALHABASH: So there's a depth of data that is immeasurable now that is not matched with the types of insights that we know about consumers to make advertising more effective. GARSD: Saleem Alhabash is an associate professor at Michigan State University's department of advertising. He says advertisers today have so much detailed information. It can actually make targeting the right audience difficult. Marcus Collins is an executive at Doner, an advertising agency. MARCUS COLLINS: We put an undue pressure on these technology platforms that we don't put on traditional media. And it's not fair. GARSD: He says Facebook is just a tool. Millions of advertisers are drawn to it. But sometimes, like in the case of Investor Village, it may not work. COLLINS: Facebook isn't the magic potion to make presto change-o (ph), you reach everybody, and everyone will start buying your products. GARSD: No matter how much data we have, Collins says convincing people to buy what you're selling is always going to be more than just a numbers game. Jasmine Garsd, NPR News, New York.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-12-647180302": {"title": "New Apple Watch To Detect Abnormal Heartbeats : NPR", "url": "https://www.npr.org/2018/09/12/647180302/new-apple-watch-to-detect-abnormal-heartbeats", "author": "No author found", "published_date": "2018-09-12", "content": "", "section": "Technology", "disclaimer": ""}, "2018-09-12-646441153": {"title": "6-Figure Price Tag Expected For Rare Apple-1 Computer At Auction : NPR", "url": "https://www.npr.org/2018/09/12/646441153/6-figure-price-tag-expected-for-rare-apple-1-computer-at-auction", "author": "No author found", "published_date": "2018-09-12", "content": "DAVID GREENE, HOST: All right. Apple has its biggest hardware event of the year later today. This is when they unveil all these new devices - always a whole lot of anticipation. Of course, before Apple was a trillion-dollar company, before its phones and laptops came to dominate the tech industry, it was just another California startup. And now one of the first products the company ever made is about to hit the auction block. New Hampshire Public Radio's Todd Bookman reports on a very valuable Apple-1 computer. TODD BOOKMAN, BYLINE: Looking at an Apple-1 is kind of like looking at the Rosetta Stone. You don't totally understand what you're seeing, but you sense its significance. COREY COHEN: The Apple-1 didn't come with a keyboard. It didn't come with a monitor or anything like that. BOOKMAN: This is Corey Cohen. He's known nationally as the guy when it comes to Apple-1 restorations. COHEN: It really was just the board, but the board itself was really the first you could only buy assembled computer. BOOKMAN: In front of him is one of the 200 original Apple-1 circuit boards. It was built in 1976 and retailed for $666. 66. Don't picture a normal computer. This is just the guts, the brain, a thin green rectangle with chips and capacitors and microprocessors soldered on in neat rows. With this powerful technology, you could, well. . . COHEN: Write small programs, play a couple of video games that were kind of text-based, you know, interactive fiction. And that's pretty much it. BOOKMAN: Home computers back then weren't for the masses. They were for hobbyists and folks who liked to tinker. This Apple-1 is unique because it hasn't been modified and the thing still actually works. Cohen Has it rigged up to a vintage keyboard and small black-and-white monitor. COHEN: So this is your boot-up screen. There's not much to it. It's just kind of flashing all the empty characters. So we will clear the screen. We'll reset. It's like watching grass grow. BOOKMAN: The auction on September 25 will likely be more exciting. Bobby Livingston, who helps run New Hampshire-based RR Auction, says they can trace this Apple-1's origins. BOBBY LIVINGSTON: This particular board was purchased from the original owner for $300 in 1976. I think the original guy only had it for about three months. He was a mainframe guy, he said. He didn't really want to mess with it, so he sold it to our client, who's had it since 1976. BOOKMAN: The seller is remaining anonymous. He or she stands to make a lot of money off that purchase from 40 years ago. The estimated auction price tag is $300,000. With that kind of cash, you are buying a piece of Apple history, a piece of the Steve Wozniak legend. (SOUNDBITE OF ARCHIVED RECORDING)STEVE WOZNIAK: The summer that I built the Apple-1 computer, I was totally aware that a revolution was close to a starting. BOOKMAN: This is Wozniak, or Woz, who designed the Apple-1, speaking in an old Bloomberg News story. (SOUNDBITE OF ARCHIVED RECORDING)WOZNIAK: Steve Jobs came into town. He'd pop into town, see what I was up to, the latest thing I designed for fun, and then he'd somehow turn them into some money for both of us. BOOKMAN: These guys would make a lot of money but not off the Apple-1. It was the Apple II released a year later in 1977 that would forever change home computing. Dag Spicer is with the Computer History Museum, which has an Apple-1 in its collection. He says it's one of their most popular pieces. DAG SPICER: The Apple-1 is so iconic of that era, of the garage era of Silicon Valley, that I think there's almost no other object that really encapsulates what it does culturally and technologically. BOOKMAN: Corey Cohen, the Apple-1 expert, says anyone willing to spend this kind of money is likely to keep the computer behind glass. COHEN: From a layout perspective, it's considered a piece of art. Many people hang these on the wall. BOOKMAN: Even if you don't understand how it works, it's a fascinating thing to look at. For NPR News, I'm Todd Bookman. (SOUNDBITE OF 7 MINUTES DEAD'S \"THE PASSING\") DAVID GREENE, HOST:  All right. Apple has its biggest hardware event of the year later today. This is when they unveil all these new devices - always a whole lot of anticipation. Of course, before Apple was a trillion-dollar company, before its phones and laptops came to dominate the tech industry, it was just another California startup. And now one of the first products the company ever made is about to hit the auction block. New Hampshire Public Radio's Todd Bookman reports on a very valuable Apple-1 computer. TODD BOOKMAN, BYLINE: Looking at an Apple-1 is kind of like looking at the Rosetta Stone. You don't totally understand what you're seeing, but you sense its significance. COREY COHEN: The Apple-1 didn't come with a keyboard. It didn't come with a monitor or anything like that. BOOKMAN: This is Corey Cohen. He's known nationally as the guy when it comes to Apple-1 restorations. COHEN: It really was just the board, but the board itself was really the first you could only buy assembled computer. BOOKMAN: In front of him is one of the 200 original Apple-1 circuit boards. It was built in 1976 and retailed for $666. 66. Don't picture a normal computer. This is just the guts, the brain, a thin green rectangle with chips and capacitors and microprocessors soldered on in neat rows. With this powerful technology, you could, well. . . COHEN: Write small programs, play a couple of video games that were kind of text-based, you know, interactive fiction. And that's pretty much it. BOOKMAN: Home computers back then weren't for the masses. They were for hobbyists and folks who liked to tinker. This Apple-1 is unique because it hasn't been modified and the thing still actually works. Cohen Has it rigged up to a vintage keyboard and small black-and-white monitor. COHEN: So this is your boot-up screen. There's not much to it. It's just kind of flashing all the empty characters. So we will clear the screen. We'll reset. It's like watching grass grow. BOOKMAN: The auction on September 25 will likely be more exciting. Bobby Livingston, who helps run New Hampshire-based RR Auction, says they can trace this Apple-1's origins. BOBBY LIVINGSTON: This particular board was purchased from the original owner for $300 in 1976. I think the original guy only had it for about three months. He was a mainframe guy, he said. He didn't really want to mess with it, so he sold it to our client, who's had it since 1976. BOOKMAN: The seller is remaining anonymous. He or she stands to make a lot of money off that purchase from 40 years ago. The estimated auction price tag is $300,000. With that kind of cash, you are buying a piece of Apple history, a piece of the Steve Wozniak legend. (SOUNDBITE OF ARCHIVED RECORDING) STEVE WOZNIAK: The summer that I built the Apple-1 computer, I was totally aware that a revolution was close to a starting. BOOKMAN: This is Wozniak, or Woz, who designed the Apple-1, speaking in an old Bloomberg News story. (SOUNDBITE OF ARCHIVED RECORDING) WOZNIAK: Steve Jobs came into town. He'd pop into town, see what I was up to, the latest thing I designed for fun, and then he'd somehow turn them into some money for both of us. BOOKMAN: These guys would make a lot of money but not off the Apple-1. It was the Apple II released a year later in 1977 that would forever change home computing. Dag Spicer is with the Computer History Museum, which has an Apple-1 in its collection. He says it's one of their most popular pieces. DAG SPICER: The Apple-1 is so iconic of that era, of the garage era of Silicon Valley, that I think there's almost no other object that really encapsulates what it does culturally and technologically. BOOKMAN: Corey Cohen, the Apple-1 expert, says anyone willing to spend this kind of money is likely to keep the computer behind glass. COHEN: From a layout perspective, it's considered a piece of art. Many people hang these on the wall. BOOKMAN: Even if you don't understand how it works, it's a fascinating thing to look at. For NPR News, I'm Todd Bookman. (SOUNDBITE OF 7 MINUTES DEAD'S \"THE PASSING\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-12-646808300": {"title": "Federal Court Asked To Scrap Georgia's 27,000 Electronic Voting Machines  : NPR", "url": "https://www.npr.org/2018/09/12/646808300/federal-court-asked-to-scrap-georgias-27-000-electronic-voting-machines", "author": "No author found", "published_date": "2018-09-12", "content": "RACHEL MARTIN, HOST: Paperless voting machines are having their day in court today in Georgia. A federal judge is weighing whether the state should use paper ballots instead of the electronic voting machines in November or whether all that would just create unnecessary chaos. Johnny Kauffman of member station WABE is following this and joins us now from Atlanta. So Johnny, who filed the case, and what are they worried about? JOHNNY KAUFFMAN, BYLINE: This case comes from a few Georgia voters and election security advocates here. And they brought the case because Georgia is one of 14 states that uses these touch-screen voting machines. They don't produce a paper trail. And so if hackers manipulate vote totals or something else goes wrong, you can't do a recount or audit. And the plaintiffs say this undermines the state's interest in preventing voter fraud. It makes people less confident in the election. And they're like, we have these competitive midterms coming up. This is really urgent. There are some big races in Georgia, too. I think also there's the broader context - right? - of, you know, this news that Russian hackers targeted the election systems in 2016. And that brought a lot of attention and sort of energy behind this lawsuit. MARTIN: Right. So what our state election officials saying? I mean, do they think paper's a good idea? KAUFFMAN: Well, the top election official in Georgia is Secretary of State Brian Kemp. He also happens to be a Republican running for governor. He will be on the ballot whether it's paper or electronic, right? And for years, Kemp has faced a lot of questions about how he's handled election security. The most relevant example comes from - right around the 2016 election, there was this website overseen by a state contractor that was unsecured. And basically, open to the public were passwords from poll workers, voter registration records. Kemp blames the contractor for that. But whatever exactly happened, the incident did not reassure people about Georgia's election security. Also Kemp rejected help from the U. S. Department of Homeland Security twice that was helped to bolster, you know, Georgia's defenses. Other states were offered this stuff, too. Despite all this - right? - Kemp insists that Georgia's elections are secure. But when it comes to the lawsuit, he says switching to paper ballots right now - and he told me this - he said it would be, quote, \"an absolute disaster. \"MARTIN: I mean, would it be? I mean, it does seem, like, kind of short notice to switch two months before an election. KAUFFMAN: It's really short notice. And, you know, Georgians can already vote on paper by requesting an absentee ballot in the mail. So what the plaintiffs in this case want to do - these voters and advocates - is expand that absentee ballot system, this paper system, as an alternative to the electronic machines. Some ideas they have would be sending out absentee ballots to the entire state, supplying them at polling places on Election Day. Whatever - if the judge decides to throw out the electronic machines, it would probably be up to them to decide. But election officials here like Lynn Bailey, who's from Richmond County, east of Atlanta - they're worried about this. And she says - Bailey does - that switching to paper ballots so close to Election Day would lead to long lines, confusion, lower turnout and would be expensive. LYNN BAILEY: We are in the throes of full steam ahead with this election. It would be a big distraction to make such a change this close in. KAUFFMAN: One thing to note about Georgia is that we have a lot of counties here - 159 counties. A lot of people would say that's too many counties. And most are even smaller than Richmond County where Bailey is. So it's these smaller counties Bailey is worried about. They don't have as many resources. Making this switch would be a lot harder for them. So if the judge does throw out the voting machines, it's going to create a lot of scrambling and questions about how exactly to do it before November 6. MARTIN: All right, Johnny Kauffman with member station WABE in Atlanta - thanks so much, Johnny. We appreciate it. KAUFFMAN: You're welcome. (SOUNDBITE OF TESK'S \"GREEN STAMPS\") RACHEL MARTIN, HOST:  Paperless voting machines are having their day in court today in Georgia. A federal judge is weighing whether the state should use paper ballots instead of the electronic voting machines in November or whether all that would just create unnecessary chaos. Johnny Kauffman of member station WABE is following this and joins us now from Atlanta. So Johnny, who filed the case, and what are they worried about? JOHNNY KAUFFMAN, BYLINE: This case comes from a few Georgia voters and election security advocates here. And they brought the case because Georgia is one of 14 states that uses these touch-screen voting machines. They don't produce a paper trail. And so if hackers manipulate vote totals or something else goes wrong, you can't do a recount or audit. And the plaintiffs say this undermines the state's interest in preventing voter fraud. It makes people less confident in the election. And they're like, we have these competitive midterms coming up. This is really urgent. There are some big races in Georgia, too. I think also there's the broader context - right? - of, you know, this news that Russian hackers targeted the election systems in 2016. And that brought a lot of attention and sort of energy behind this lawsuit. MARTIN: Right. So what our state election officials saying? I mean, do they think paper's a good idea? KAUFFMAN: Well, the top election official in Georgia is Secretary of State Brian Kemp. He also happens to be a Republican running for governor. He will be on the ballot whether it's paper or electronic, right? And for years, Kemp has faced a lot of questions about how he's handled election security. The most relevant example comes from - right around the 2016 election, there was this website overseen by a state contractor that was unsecured. And basically, open to the public were passwords from poll workers, voter registration records. Kemp blames the contractor for that. But whatever exactly happened, the incident did not reassure people about Georgia's election security. Also Kemp rejected help from the U. S. Department of Homeland Security twice that was helped to bolster, you know, Georgia's defenses. Other states were offered this stuff, too. Despite all this - right? - Kemp insists that Georgia's elections are secure. But when it comes to the lawsuit, he says switching to paper ballots right now - and he told me this - he said it would be, quote, \"an absolute disaster. \" MARTIN: I mean, would it be? I mean, it does seem, like, kind of short notice to switch two months before an election. KAUFFMAN: It's really short notice. And, you know, Georgians can already vote on paper by requesting an absentee ballot in the mail. So what the plaintiffs in this case want to do - these voters and advocates - is expand that absentee ballot system, this paper system, as an alternative to the electronic machines. Some ideas they have would be sending out absentee ballots to the entire state, supplying them at polling places on Election Day. Whatever - if the judge decides to throw out the electronic machines, it would probably be up to them to decide. But election officials here like Lynn Bailey, who's from Richmond County, east of Atlanta - they're worried about this. And she says - Bailey does - that switching to paper ballots so close to Election Day would lead to long lines, confusion, lower turnout and would be expensive. LYNN BAILEY: We are in the throes of full steam ahead with this election. It would be a big distraction to make such a change this close in. KAUFFMAN: One thing to note about Georgia is that we have a lot of counties here - 159 counties. A lot of people would say that's too many counties. And most are even smaller than Richmond County where Bailey is. So it's these smaller counties Bailey is worried about. They don't have as many resources. Making this switch would be a lot harder for them. So if the judge does throw out the voting machines, it's going to create a lot of scrambling and questions about how exactly to do it before November 6. MARTIN: All right, Johnny Kauffman with member station WABE in Atlanta - thanks so much, Johnny. We appreciate it. KAUFFMAN: You're welcome. (SOUNDBITE OF TESK'S \"GREEN STAMPS\")", "section": "Elections", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-13-647225815": {"title": "Gov. Brown's Biggest Climate Foe Isn't Trump. It's Car-Loving Californians : NPR", "url": "https://www.npr.org/2018/09/13/647225815/gov-browns-biggest-climate-foe-isn-t-trump-it-s-car-loving-californians", "author": "No author found", "published_date": "2018-09-13", "content": "DAVID GREENE, HOST: California's governor, Jerry Brown, is hosting an international climate summit this week. For years, California has really struck its own path when it comes to cutting carbon pollution. But in spite of California's ambitious goals, car emissions are actually increasing. Here's Lauren Sommer from member station KQED. LAUREN SOMMER, BYLINE: Broadly speaking, there are two kinds of customers who walk into Concord Chevrolet in the East Bay area. People like Pablo Chang-Castillo. PABLO CHANG-CASTILLO: I'm getting a green vehicle, an electric vehicle. SOMMER: A brand-new, black Chevy Bolt. It goes about 230 miles on a charge. He says using electricity instead of gas will save him money. CHANG-CASTILLO: Like, I think gas is in the past, and I think that this is the future. SOMMER: But the majority of customers are like Mark Bauhs. MARK BAUHS: My wife just told me her car is too small, so I need a bigger car. SOMMER: He's here to test drive a mid-size SUV. BAUHS: I haven't quite moved over to electric cars yet. It definitely has never crossed my mind for a family car. SOMMER: But here's the thing - to reach California's ambitious climate goals, every new car sold in the state will have to be plug-in by 2040. And right now, electric and plug-in hybrid cars are only 6 percent of sales. GIL TAL: The main issue is most of the Californians are not aware of the benefit and opportunity of buying plug-in electric cars. SOMMER: Gil Tal directs the Plug-In Hybrid & Electric Vehicle Research Center at UC Davis. He says, in California, electric cars are cleaner than gas cars because the electricity comes from growing amounts of solar and wind power. So to get them on roads, California passed a mandate that requires automakers to offer zero-emission vehicles in the state. The goal is 5 million by 2030. But that's only half the battle. People actually have to buy them. And Tal says surveys show that most people don't know much about electric cars, and they generally don't learn much at car dealerships. TAL: A lot of the purchase process happens ahead of showing up. SOMMER: They've already made up their mind. So the electric car conversion has to happen earlier. One good way could be advertising. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: With instant acceleration, electric cars are more fun to drive and more affordable than ever. SOMMER: But that's not an ad. It's actually a public service announcement that Volkswagen has to run as penance for their emissions cheating scandal. Chances are you've never seen an ad for an electric car from a major car company. STEVEN MAJOROS: This is a very difficult segment. It's a difficult product proposition. SOMMER: Steven Majoros is marketing director for Chevrolet cars and crossovers. He says so far they haven't run a national TV spot for the Chevy Bolt, just regional ads. MAJOROS: Let's just be realistic. How big is the EV market? In the United States - right? - it's about 1, 1. 5 percent of the market. So we have to always balance market demand, market size with how much we, quote, unquote, you know, \"advertise. \"SOMMER: That's a trend across automakers. According to one analysis, major car companies only spend a fraction on advertising electric cars as compared with their best-selling models. Chevy is counting on word of mouth. MAJOROS: This is a way better experience for most people than a normal internal combustion engine. SOMMER: But not many people are getting that experience, and trucks and SUVs dominate the market. So the switch from gas to electric might take a push of some kind, like high gas prices. Whatever it takes, California is hoping every driver comes around - and soon. For NPR News, I'm Lauren Sommer in San Francisco. (SOUNDBITE OF ROBOT ORCHESTRA'S \"DUSTYRHODES\") DAVID GREENE, HOST:  California's governor, Jerry Brown, is hosting an international climate summit this week. For years, California has really struck its own path when it comes to cutting carbon pollution. But in spite of California's ambitious goals, car emissions are actually increasing. Here's Lauren Sommer from member station KQED. LAUREN SOMMER, BYLINE: Broadly speaking, there are two kinds of customers who walk into Concord Chevrolet in the East Bay area. People like Pablo Chang-Castillo. PABLO CHANG-CASTILLO: I'm getting a green vehicle, an electric vehicle. SOMMER: A brand-new, black Chevy Bolt. It goes about 230 miles on a charge. He says using electricity instead of gas will save him money. CHANG-CASTILLO: Like, I think gas is in the past, and I think that this is the future. SOMMER: But the majority of customers are like Mark Bauhs. MARK BAUHS: My wife just told me her car is too small, so I need a bigger car. SOMMER: He's here to test drive a mid-size SUV. BAUHS: I haven't quite moved over to electric cars yet. It definitely has never crossed my mind for a family car. SOMMER: But here's the thing - to reach California's ambitious climate goals, every new car sold in the state will have to be plug-in by 2040. And right now, electric and plug-in hybrid cars are only 6 percent of sales. GIL TAL: The main issue is most of the Californians are not aware of the benefit and opportunity of buying plug-in electric cars. SOMMER: Gil Tal directs the Plug-In Hybrid & Electric Vehicle Research Center at UC Davis. He says, in California, electric cars are cleaner than gas cars because the electricity comes from growing amounts of solar and wind power. So to get them on roads, California passed a mandate that requires automakers to offer zero-emission vehicles in the state. The goal is 5 million by 2030. But that's only half the battle. People actually have to buy them. And Tal says surveys show that most people don't know much about electric cars, and they generally don't learn much at car dealerships. TAL: A lot of the purchase process happens ahead of showing up. SOMMER: They've already made up their mind. So the electric car conversion has to happen earlier. One good way could be advertising. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: With instant acceleration, electric cars are more fun to drive and more affordable than ever. SOMMER: But that's not an ad. It's actually a public service announcement that Volkswagen has to run as penance for their emissions cheating scandal. Chances are you've never seen an ad for an electric car from a major car company. STEVEN MAJOROS: This is a very difficult segment. It's a difficult product proposition. SOMMER: Steven Majoros is marketing director for Chevrolet cars and crossovers. He says so far they haven't run a national TV spot for the Chevy Bolt, just regional ads. MAJOROS: Let's just be realistic. How big is the EV market? In the United States - right? - it's about 1, 1. 5 percent of the market. So we have to always balance market demand, market size with how much we, quote, unquote, you know, \"advertise. \" SOMMER: That's a trend across automakers. According to one analysis, major car companies only spend a fraction on advertising electric cars as compared with their best-selling models. Chevy is counting on word of mouth. MAJOROS: This is a way better experience for most people than a normal internal combustion engine. SOMMER: But not many people are getting that experience, and trucks and SUVs dominate the market. So the switch from gas to electric might take a push of some kind, like high gas prices. Whatever it takes, California is hoping every driver comes around - and soon. For NPR News, I'm Lauren Sommer in San Francisco. (SOUNDBITE OF ROBOT ORCHESTRA'S \"DUSTYRHODES\")", "section": "Environment And Energy Collaborative", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-18-649158898": {"title": "Facebook Allowed Employers To Exclude Women From Job Ads, ACLU Says : NPR", "url": "https://www.npr.org/2018/09/18/649158898/facebook-allowed-employers-to-exclude-women-from-job-ads-aclu-says", "author": "No author found", "published_date": "2018-09-18", "content": "", "section": "Business", "disclaimer": ""}, "2018-09-18-649121322": {"title": "Georgia Will Use Electronic Voting Machines This Fall As Paper Ballot Case Falters : NPR", "url": "https://www.npr.org/2018/09/18/649121322/georgia-will-use-electronic-voting-machines-this-fall-as-paper-ballot-case-falte", "author": "No author found", "published_date": "2018-09-18", "content": "", "section": "Elections", "disclaimer": ""}, "2018-09-18-649060315": {"title": "Technology Helps Motorists Maneuver In A Natural Disaster : NPR", "url": "https://www.npr.org/2018/09/18/649060315/technology-helps-motorists-maneuver-in-a-natural-disaster", "author": "No author found", "published_date": "2018-09-18", "content": "RACHEL MARTIN, HOST: So it used to be that when a storm hit, the only way to evacuate was to hop in a car with a map and hope for the best. Now, though, there is so much technology, it's almost overwhelming. NPR's Jasmine Garsd has been looking into which options actually work. JASMINE GARSD, BYLINE: It was Twitter that showed a woman named Emily Streets (ph) the way - literally. She was in Los Angeles last year, and one of the worst wildfires in California's history was raging. She opened Waze, a navigation app owned by Google, to figure out how to get to work. Waze instructed her that the fastest route was the 405. The problem is, it only looked fast. The 405 was actually closed. EMILY STREETS: The 405 goes through, like, a hilly area and with a lot of brush and bushes, and it was all on fire. GARSD: Streets is one of several Californians who had this problem. And like a good millennial, she took to Twitter to find out what was going on. Following instructions on Twitter, she was finally able to get to work. Oddly enough, last week, Streets found herself in a somewhat similar conundrum. She was working in South Carolina as Hurricane Florence approached. Streets hopped into her colleague's car. He pulled up Waze, and Streets warned him. STREETS: Yes, I actually had that conversation of saying, like, I've stopped using Waze. It never - it always wants you to take crazy side routes and impossible left turns. GARSD: This is a question for millions of Americans in the path of natural disasters. Of all the technology available, what should you use to help you get through? DANIELLE SOSKEL: Working in something like a fire, which is super dynamic and constantly changing, is obviously a lot harder to keep up to date. GARSD: Danielle Soskel is a program manager for Waze. Soskel walked me through all the things Waze was doing to make their navigation system as accurate as possible ahead of Florence. Users can press the help button in the app to find nearby shelters. Soskel says Waze has worked closely with officials to update what areas are flooded and off-limits. And they also have over 300 volunteer editors. SOSKEL: These are entirely volunteer people who come and work and make sure that their local communities are updated with accurate information and that their communities are safe. GARSD: Other tech firms also stepped it up for Florence. Airbnb offered free housing for evacuees. And then there's CrowdSource Rescue, which started during Hurricane Harvey and pairs up volunteer people in need of rescue. Spokesperson Leah Halbina says that as the storm approached. . . LEAH HALBINA: We were working closely with the New Bern police department and fire department. And then things are starting to move closer to Wilmington and Myrtle Beach. GARSD: The technology isn't infallible, but it helps. As for Emily Streets, who was wary of using Waze again during Florence evacuations, she says her colleague insisted. STREETS: He said - you know what? - he had no problems with it. He trusted it. And so I was like, OK. GARSD: They all got out safely. She says she's ready to give the technology a second chance. Jasmine Garsd, NPR News, New York. (SOUNDBITE OF ANATOLE'S \"OUTGROWN\") RACHEL MARTIN, HOST:  So it used to be that when a storm hit, the only way to evacuate was to hop in a car with a map and hope for the best. Now, though, there is so much technology, it's almost overwhelming. NPR's Jasmine Garsd has been looking into which options actually work. JASMINE GARSD, BYLINE: It was Twitter that showed a woman named Emily Streets (ph) the way - literally. She was in Los Angeles last year, and one of the worst wildfires in California's history was raging. She opened Waze, a navigation app owned by Google, to figure out how to get to work. Waze instructed her that the fastest route was the 405. The problem is, it only looked fast. The 405 was actually closed. EMILY STREETS: The 405 goes through, like, a hilly area and with a lot of brush and bushes, and it was all on fire. GARSD: Streets is one of several Californians who had this problem. And like a good millennial, she took to Twitter to find out what was going on. Following instructions on Twitter, she was finally able to get to work. Oddly enough, last week, Streets found herself in a somewhat similar conundrum. She was working in South Carolina as Hurricane Florence approached. Streets hopped into her colleague's car. He pulled up Waze, and Streets warned him. STREETS: Yes, I actually had that conversation of saying, like, I've stopped using Waze. It never - it always wants you to take crazy side routes and impossible left turns. GARSD: This is a question for millions of Americans in the path of natural disasters. Of all the technology available, what should you use to help you get through? DANIELLE SOSKEL: Working in something like a fire, which is super dynamic and constantly changing, is obviously a lot harder to keep up to date. GARSD: Danielle Soskel is a program manager for Waze. Soskel walked me through all the things Waze was doing to make their navigation system as accurate as possible ahead of Florence. Users can press the help button in the app to find nearby shelters. Soskel says Waze has worked closely with officials to update what areas are flooded and off-limits. And they also have over 300 volunteer editors. SOSKEL: These are entirely volunteer people who come and work and make sure that their local communities are updated with accurate information and that their communities are safe. GARSD: Other tech firms also stepped it up for Florence. Airbnb offered free housing for evacuees. And then there's CrowdSource Rescue, which started during Hurricane Harvey and pairs up volunteer people in need of rescue. Spokesperson Leah Halbina says that as the storm approached. . . LEAH HALBINA: We were working closely with the New Bern police department and fire department. And then things are starting to move closer to Wilmington and Myrtle Beach. GARSD: The technology isn't infallible, but it helps. As for Emily Streets, who was wary of using Waze again during Florence evacuations, she says her colleague insisted. STREETS: He said - you know what? - he had no problems with it. He trusted it. And so I was like, OK. GARSD: They all got out safely. She says she's ready to give the technology a second chance. Jasmine Garsd, NPR News, New York. (SOUNDBITE OF ANATOLE'S \"OUTGROWN\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-20-650102426": {"title": "Google Warned Senators, Aides Of Hacking Attempts On Personal Emails : NPR", "url": "https://www.npr.org/2018/09/20/650102426/google-warned-senators-aides-of-hacking-attempts-on-personal-emails", "author": "No author found", "published_date": "2018-09-20", "content": "", "section": "Technology", "disclaimer": ""}, "2018-09-20-650079273": {"title": "Hackers Steal $59 Million In Cryptocurrency From Japanese Exchange  : NPR", "url": "https://www.npr.org/2018/09/20/650079273/hackers-steal-59-million-in-cryptocurrency-from-japanese-exchange", "author": "No author found", "published_date": "2018-09-20", "content": "", "section": "Business", "disclaimer": ""}, "2018-09-21-649535367": {"title": "Hacks, Security Gaps And Oligarchs: The Business Of Voting Comes Under Scrutiny : NPR", "url": "https://www.npr.org/2018/09/21/649535367/hacks-security-gaps-and-oligarchs-the-business-of-voting-comes-under-scrutiny", "author": "No author found", "published_date": "2018-09-21", "content": "", "section": "Politics", "disclaimer": ""}, "2018-09-22-650137845": {"title": "As West Grows, Water Use Declines Thanks To Better Toilets : NPR", "url": "https://www.npr.org/2018/09/22/650137845/as-west-grows-water-use-declines-thanks-to-better-toilets", "author": "No author found", "published_date": "2018-09-22", "content": "SCOTT SIMON, HOST: Something unexpected is happening in the West's biggest cities. Population is booming, but water use is staying the same or in some cases, actually going down. From member station KUNC, Luke Runyon reports on what to credit for all that savings. LUKE RUNYON, BYLINE: Theresa MacFarland, her husband and two kids live in an historic two-story home in Longmont, Colo. , just outside Boulder. It has all the vintage touches - hardwood floors, big windows, wood detailing, and one really old toilet. MacFarland points it out to her four-year-old daughter Althea. THERESA MACFARLAND: That toilet has been there longer than Daddy and I have been alive. NEKA SUNLIN: Yeah. MACFARLAND: Probably longer than Grandma and Grandpa have been alive. SUNLIN: (Laughter). RUNYON: A little stamp on the bowl says it was built in the 1950s. Lately, it's had some trouble getting the job done. And that's why MacFarland contacted a local conservation group called Resource Central to find a more water-friendly model. Neka Sunlin oversees the group's Flush for the Future program and helped install McFarland's new toilet. SUNLIN: We guesstimate this one's using about five gallons a flush. The new one uses less than one. MACFARLAND: It's amazing. SUNLIN: It's 0. 8 gallons per flush. So you're going to be saving probably four gallons of water per flush. RUNYON: In an average month, a toilet uses more water than any other fixture in a home - more than your washing machine, your occasional long shower and way more than your dishwasher. People flush between five and eight times a day. Since the 1990s, people have been replacing a lot of toilets throughout the country. Back then, Congress created national standards for water use. They mandated toilets use only about one and a half gallons for each flush. For the plumbing industry, it was a huge deal. PETE DEMARCO: Watershed moment - no pun intended. RUNYON: One of the people who helped write the low-flush rules was Pete DeMarco. He's with the International Association of Plumbing and Mechanical Officials. DEMARCO: And it wasn't just toilets where flows were being reduced, but showerheads, kitchen faucets, lavatory faucets and urinals were also being regulated. RUNYON: DeMarco says this is a big reason why cities have been able to grow and still keep their water use in check. Indoor use dropped 22 percent nationwide between 1999 and 2016 - much of that due to just swapping out old fixtures. And recently, states with water scarcity problems have passed even tighter regulations. DREW BECKWITH: And so you, basically, have these high-efficiency toilets now, as a matter of course. You cannot go out in the store in Colorado or California and buy an old toilet. RUNYON: Drew Beckwith is a water policy expert who works in suburban Denver. He says conservationists have been a victim of their own success. With national standards in place, there's not much more people can do to limit water use inside homes. BECKWITH: We've sort of done our business with respect to toilets. And it's time to, you know, maybe get off the pot and move on to outdoor water use, which is more - I see - the focus of urban water efficiency today. RUNYON: Back at the MacFarland home outside Boulder, the brand new toilet is hooked up. (SOUNDBITE OF TOILET RESERVOIR FILLING)RUNYON: And water is filling the tank. The honor of first flush goes to Theresa MacFarland's daughter Althea. MACFARLAND: Check it out - there's this blue button. (Unintelligible). Give it a shot. (SOUNDBITE OF TOILET FLUSHING)RUNYON: Didn't think we'd get through this whole story without hearing at least one flush, right? For NPR News, I'm Luke Runyon in Longmont, Colo. SCOTT SIMON, HOST:  Something unexpected is happening in the West's biggest cities. Population is booming, but water use is staying the same or in some cases, actually going down. From member station KUNC, Luke Runyon reports on what to credit for all that savings. LUKE RUNYON, BYLINE: Theresa MacFarland, her husband and two kids live in an historic two-story home in Longmont, Colo. , just outside Boulder. It has all the vintage touches - hardwood floors, big windows, wood detailing, and one really old toilet. MacFarland points it out to her four-year-old daughter Althea. THERESA MACFARLAND: That toilet has been there longer than Daddy and I have been alive. NEKA SUNLIN: Yeah. MACFARLAND: Probably longer than Grandma and Grandpa have been alive. SUNLIN: (Laughter). RUNYON: A little stamp on the bowl says it was built in the 1950s. Lately, it's had some trouble getting the job done. And that's why MacFarland contacted a local conservation group called Resource Central to find a more water-friendly model. Neka Sunlin oversees the group's Flush for the Future program and helped install McFarland's new toilet. SUNLIN: We guesstimate this one's using about five gallons a flush. The new one uses less than one. MACFARLAND: It's amazing. SUNLIN: It's 0. 8 gallons per flush. So you're going to be saving probably four gallons of water per flush. RUNYON: In an average month, a toilet uses more water than any other fixture in a home - more than your washing machine, your occasional long shower and way more than your dishwasher. People flush between five and eight times a day. Since the 1990s, people have been replacing a lot of toilets throughout the country. Back then, Congress created national standards for water use. They mandated toilets use only about one and a half gallons for each flush. For the plumbing industry, it was a huge deal. PETE DEMARCO: Watershed moment - no pun intended. RUNYON: One of the people who helped write the low-flush rules was Pete DeMarco. He's with the International Association of Plumbing and Mechanical Officials. DEMARCO: And it wasn't just toilets where flows were being reduced, but showerheads, kitchen faucets, lavatory faucets and urinals were also being regulated. RUNYON: DeMarco says this is a big reason why cities have been able to grow and still keep their water use in check. Indoor use dropped 22 percent nationwide between 1999 and 2016 - much of that due to just swapping out old fixtures. And recently, states with water scarcity problems have passed even tighter regulations. DREW BECKWITH: And so you, basically, have these high-efficiency toilets now, as a matter of course. You cannot go out in the store in Colorado or California and buy an old toilet. RUNYON: Drew Beckwith is a water policy expert who works in suburban Denver. He says conservationists have been a victim of their own success. With national standards in place, there's not much more people can do to limit water use inside homes. BECKWITH: We've sort of done our business with respect to toilets. And it's time to, you know, maybe get off the pot and move on to outdoor water use, which is more - I see - the focus of urban water efficiency today. RUNYON: Back at the MacFarland home outside Boulder, the brand new toilet is hooked up. (SOUNDBITE OF TOILET RESERVOIR FILLING) RUNYON: And water is filling the tank. The honor of first flush goes to Theresa MacFarland's daughter Althea. MACFARLAND: Check it out - there's this blue button. (Unintelligible). Give it a shot. (SOUNDBITE OF TOILET FLUSHING) RUNYON: Didn't think we'd get through this whole story without hearing at least one flush, right? For NPR News, I'm Luke Runyon in Longmont, Colo.", "section": "Environment", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-23-649524683": {"title": "California Launches New Effort To Fight Election Disinformation  : NPR", "url": "https://www.npr.org/2018/09/23/649524683/california-launches-new-effort-to-fight-election-disinformation", "author": "No author found", "published_date": "2018-09-23", "content": "", "section": "Politics", "disclaimer": ""}, "2018-09-24-651056648": {"title": "SiriusXM To Buy Internet Radio Pioneer Pandora In $3.5 Billion Deal : NPR", "url": "https://www.npr.org/2018/09/24/651056648/siriusxm-to-buy-internet-radio-pioneer-pandora-in-3-5-billion-deal", "author": "No author found", "published_date": "2018-09-24", "content": "", "section": "Media", "disclaimer": ""}, "2018-09-25-651472693": {"title": "Attorneys General Zoom In On Tech Privacy And Power : NPR", "url": "https://www.npr.org/2018/09/25/651472693/attorneys-general-zoom-in-on-tech-privacy-and-power", "author": "No author found", "published_date": "2018-09-25", "content": "MARY LOUISE KELLY, HOST: Top U. S. Justice Department officials met this morning with a group of state attorneys general to talk about the tech industry. The Justice Department had originally pitched the meeting as a focus on allegations that the social media companies suppress conservative viewpoints. But the conversation went far beyond that to issues that raised the stakes for tech giants. NPR's Alina Selyukh is here to tell us more. Hey, Alina. ALINA SELYUKH, BYLINE: Hello. KELLY: Hi. So set the scene for me a little bit more. This was a big meeting at the Justice Department. And what exactly was the agenda? SELYUKH: Right. The gathering was a bipartisan meeting. It was dubbed a listening session by the Department of Justice. There were 14 states represented whether by attorneys general or their deputies. U. S. Attorney General Jeff Sessions was there along with his own deputies. And originally they did, as you mentioned, set this meeting up to address accusations from conservative politicians that social media companies allegedly suppress conservative views. But this meeting ended up being a much broader conversation, potentially more significant. I spoke with the Nebraska attorney general, Doug Peterson, who ran through a list of his concerns focused on consumer protection. DOUG PETERSON: Transparency, whether or not data security is adequate, what type of representations are being made by Internet companies to their users. Do they get true consent? SELYUKH: Peterson is a Republican. And from what he and others told me about this meeting, the AGs talked about a lot of things - how tech companies like Facebook and Google collect user data, how they protect it, how antitrust laws might be used to set the right standards for consumer privacy and whether simply these companies are too big. KELLY: OK, so they met. They talked about all kinds of stuff. Did they come up with any concrete plans to act on all this stuff? SELYUKH: Right. So Peterson told me that next step is many more state AGs are going to be interested in this topic. They are looking at ways to pursue a multistate action of some sort. We don't know what exactly that might mean. It is very early in the process. KELLY: OK. SELYUKH: For example, Louisiana Attorney General Jeff Landry has previously spoken about breaking up social media behemoths. Landry did not comment today. But I did speak with California Attorney General Xavier Becerra, who's a Democrat. And he says the attorney generals today did talk about historic cases when the government moved to break up companies such as Standard Oil and Microsoft. Here's Becerra. XAVIER BECERRA: There's is a recognition that privacy has a different definition for everyone these days. And what does matter is how the law treats privacy. But clearly, rarely do you have a discussion of privacy without ultimately having a conversation about antitrust. SELYUKH: And of course antitrust is exactly that question of whether companies are too big. But Becerra said the AGs weren't exactly sure that simply breaking up the companies into smaller pieces would resolve all the concerns - for example, those concerns about privacy. KELLY: And meanwhile, I'm imagining tech companies might have some thoughts on all this. SELYUKH: Indeed. Well, I'll tell you a few weeks ago, people in the tech industry were pretty quick to dismiss this whole meeting as political theater. But now we're in a different reality. We've got state AGs from both parties cooperating on much bigger questions. I spoke to Dipayan Ghosh, who is a former policy adviser in Facebook. And he's now a fellow at the Harvard Kennedy School. And we spoke before this meeting. And he said that just having these prosecutors from all over the country digging around the company's algorithmic secret sauce was going to be tough for them. DIPAYAN GHOSH: If the state AGs band together with the attorney general and start to ask really difficult, incisive questions of these companies and threaten action if the industry doesn't come to the table and discuss it, then I think that the industry is going to be in a pretty difficult position. SELYUKH: And this bipartisan meeting today definitely puts them on notice. KELLY: All right, NPR's Alina Selyukh filling us in there on ALL THINGS CONSIDERED from NPR News. MARY LOUISE KELLY, HOST:  Top U. S. Justice Department officials met this morning with a group of state attorneys general to talk about the tech industry. The Justice Department had originally pitched the meeting as a focus on allegations that the social media companies suppress conservative viewpoints. But the conversation went far beyond that to issues that raised the stakes for tech giants. NPR's Alina Selyukh is here to tell us more. Hey, Alina. ALINA SELYUKH, BYLINE: Hello. KELLY: Hi. So set the scene for me a little bit more. This was a big meeting at the Justice Department. And what exactly was the agenda? SELYUKH: Right. The gathering was a bipartisan meeting. It was dubbed a listening session by the Department of Justice. There were 14 states represented whether by attorneys general or their deputies. U. S. Attorney General Jeff Sessions was there along with his own deputies. And originally they did, as you mentioned, set this meeting up to address accusations from conservative politicians that social media companies allegedly suppress conservative views. But this meeting ended up being a much broader conversation, potentially more significant. I spoke with the Nebraska attorney general, Doug Peterson, who ran through a list of his concerns focused on consumer protection. DOUG PETERSON: Transparency, whether or not data security is adequate, what type of representations are being made by Internet companies to their users. Do they get true consent? SELYUKH: Peterson is a Republican. And from what he and others told me about this meeting, the AGs talked about a lot of things - how tech companies like Facebook and Google collect user data, how they protect it, how antitrust laws might be used to set the right standards for consumer privacy and whether simply these companies are too big. KELLY: OK, so they met. They talked about all kinds of stuff. Did they come up with any concrete plans to act on all this stuff? SELYUKH: Right. So Peterson told me that next step is many more state AGs are going to be interested in this topic. They are looking at ways to pursue a multistate action of some sort. We don't know what exactly that might mean. It is very early in the process. KELLY: OK. SELYUKH: For example, Louisiana Attorney General Jeff Landry has previously spoken about breaking up social media behemoths. Landry did not comment today. But I did speak with California Attorney General Xavier Becerra, who's a Democrat. And he says the attorney generals today did talk about historic cases when the government moved to break up companies such as Standard Oil and Microsoft. Here's Becerra. XAVIER BECERRA: There's is a recognition that privacy has a different definition for everyone these days. And what does matter is how the law treats privacy. But clearly, rarely do you have a discussion of privacy without ultimately having a conversation about antitrust. SELYUKH: And of course antitrust is exactly that question of whether companies are too big. But Becerra said the AGs weren't exactly sure that simply breaking up the companies into smaller pieces would resolve all the concerns - for example, those concerns about privacy. KELLY: And meanwhile, I'm imagining tech companies might have some thoughts on all this. SELYUKH: Indeed. Well, I'll tell you a few weeks ago, people in the tech industry were pretty quick to dismiss this whole meeting as political theater. But now we're in a different reality. We've got state AGs from both parties cooperating on much bigger questions. I spoke to Dipayan Ghosh, who is a former policy adviser in Facebook. And he's now a fellow at the Harvard Kennedy School. And we spoke before this meeting. And he said that just having these prosecutors from all over the country digging around the company's algorithmic secret sauce was going to be tough for them. DIPAYAN GHOSH: If the state AGs band together with the attorney general and start to ask really difficult, incisive questions of these companies and threaten action if the industry doesn't come to the table and discuss it, then I think that the industry is going to be in a pretty difficult position. SELYUKH: And this bipartisan meeting today definitely puts them on notice. KELLY: All right, NPR's Alina Selyukh filling us in there on ALL THINGS CONSIDERED from NPR News.", "section": "Business", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-25-651383143": {"title": "Instagram Co-Founders To Step Down : NPR", "url": "https://www.npr.org/2018/09/25/651383143/instagram-co-founders-to-step-down", "author": "No author found", "published_date": "2018-09-25", "content": "", "section": "Business", "disclaimer": ""}, "2018-09-25-651373756": {"title": "Sessions To Meet With State Attorneys General About Social Media : NPR", "url": "https://www.npr.org/2018/09/25/651373756/sessions-to-meet-with-state-attorneys-general-about-social-media", "author": "No author found", "published_date": "2018-09-25", "content": "STEVE INSKEEP, HOST:  Attorney General Jeff Sessions is expected to meet this morning with a group of state attorneys general. Republicans originally wanted a meeting to discuss claims for which there is no evidence of anti-conservative bias on social media. Instead the meeting is likely to broaden into a talk about the role that social media platforms do play in the public square. NPR's Alina Selyukh came by to tell us more. ALINA SELYUKH, BYLINE: The original announcement did talk about the allegations of bias on social media. Twitter, Facebook and Google have been accused of intentionally suppressing conservative viewpoints, which. . . INSKEEP: Like, putting them lower in the searches and so forth. They're lower in people's social media feeds, right? SELYUKH: That was one of the concerns that I believe President Trump has expressed. And at first it did seem like this meeting was a plan hatched among a handful Republican state AGs. Since then the Democratic AGs complained that this was politically motivated. They made the case that there were many other things to discuss about tech companies. Either way, today this meeting will have both Republicans and Democrats. INSKEEP: So are they still going to focus on the conspiracy theories about social media companies and conservative media? SELYUKH: I'm sure the anti-conservative bias will be a big topic. The way the Justice Department originally described this conversation was that it was going to be about whether social platforms may be, quote, \"hurting competition and intentionally stifling the free exchange of ideas. \" Facebook, Google and Twitter have all pushed back against this and argued that their algorithms are not political. But to break it down, there are two different things going on here. The culture and politics of Silicon Valley is really liberal. There's no way around that. Political contributions from workers there overwhelmingly go to Democrats. But what Trump and others have argued is that technologically the companies have allegedly rigged their software, somehow otherwise are suppressing consumer views, and for that they have not offered any evidence. And another thing to remember is companies like Google, Facebook, Twitter, they are private platforms. So theoretically, they can set whatever standards they want for what they allow to be said on their platforms. Though, of course, the companies themselves have long fought to be viewed as those neutral public squares. INSKEEP: Sure. Well, you've just hit on the dilemma there, right? Because they are private companies, they can do what they want. They have freedom of speech as a company. They can encourage other people to have freedom of speech. And yet, they have this gigantic public presence and public purpose. So now the state attorneys general are going to talk about that with Jeff Sessions. What can the states actually do? SELYUKH: State AGs can have a fair amount of oversight of social media. They are empowered by pretty broad consumer protection laws. And so actually in the announcement of the meeting, now that is the topic that's designated, consumer protection and the tech industry. INSKEEP: Meaning the state attorney general of Idaho - just hypothetically - could go after Google? Is that what could happen here? SELYUKH: Ultimately, even having federal and state attorneys digging under the hood and sort of asking very specific questions about how their algorithms operate can be the kind of attention that the companies do not want. And here's something that's even more interesting to me. The fact that it is now a bipartisan meeting, we might have a conversation beyond the political allegations of bias and into sort of antitrust, how big these companies are and how exactly they operate. INSKEEP: OK. Alina, thanks very much. Really appreciate it. SELYUKH: Thanks. INSKEEP: That's NPR's Alina Selyukh. STEVE INSKEEP, HOST:   Attorney General Jeff Sessions is expected to meet this morning with a group of state attorneys general. Republicans originally wanted a meeting to discuss claims for which there is no evidence of anti-conservative bias on social media. Instead the meeting is likely to broaden into a talk about the role that social media platforms do play in the public square. NPR's Alina Selyukh came by to tell us more. ALINA SELYUKH, BYLINE: The original announcement did talk about the allegations of bias on social media. Twitter, Facebook and Google have been accused of intentionally suppressing conservative viewpoints, which. . . INSKEEP: Like, putting them lower in the searches and so forth. They're lower in people's social media feeds, right? SELYUKH: That was one of the concerns that I believe President Trump has expressed. And at first it did seem like this meeting was a plan hatched among a handful Republican state AGs. Since then the Democratic AGs complained that this was politically motivated. They made the case that there were many other things to discuss about tech companies. Either way, today this meeting will have both Republicans and Democrats. INSKEEP: So are they still going to focus on the conspiracy theories about social media companies and conservative media? SELYUKH: I'm sure the anti-conservative bias will be a big topic. The way the Justice Department originally described this conversation was that it was going to be about whether social platforms may be, quote, \"hurting competition and intentionally stifling the free exchange of ideas. \" Facebook, Google and Twitter have all pushed back against this and argued that their algorithms are not political. But to break it down, there are two different things going on here. The culture and politics of Silicon Valley is really liberal. There's no way around that. Political contributions from workers there overwhelmingly go to Democrats. But what Trump and others have argued is that technologically the companies have allegedly rigged their software, somehow otherwise are suppressing consumer views, and for that they have not offered any evidence. And another thing to remember is companies like Google, Facebook, Twitter, they are private platforms. So theoretically, they can set whatever standards they want for what they allow to be said on their platforms. Though, of course, the companies themselves have long fought to be viewed as those neutral public squares. INSKEEP: Sure. Well, you've just hit on the dilemma there, right? Because they are private companies, they can do what they want. They have freedom of speech as a company. They can encourage other people to have freedom of speech. And yet, they have this gigantic public presence and public purpose. So now the state attorneys general are going to talk about that with Jeff Sessions. What can the states actually do? SELYUKH: State AGs can have a fair amount of oversight of social media. They are empowered by pretty broad consumer protection laws. And so actually in the announcement of the meeting, now that is the topic that's designated, consumer protection and the tech industry. INSKEEP: Meaning the state attorney general of Idaho - just hypothetically - could go after Google? Is that what could happen here? SELYUKH: Ultimately, even having federal and state attorneys digging under the hood and sort of asking very specific questions about how their algorithms operate can be the kind of attention that the companies do not want. And here's something that's even more interesting to me. The fact that it is now a bipartisan meeting, we might have a conversation beyond the political allegations of bias and into sort of antitrust, how big these companies are and how exactly they operate. INSKEEP: OK. Alina, thanks very much. Really appreciate it. SELYUKH: Thanks. INSKEEP: That's NPR's Alina Selyukh.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-27-652195624": {"title": "You've Been To Mars And A Comet; Japan's Space Agency Invites You To An Asteroid : NPR", "url": "https://www.npr.org/2018/09/27/652195624/-enjoy-standing-on-the-surface-of-this-asteroid-via-new-video", "author": "No author found", "published_date": "2018-09-27", "content": "", "section": "Space", "disclaimer": ""}, "2018-09-27-652315858": {"title": "SEC Sues Tesla CEO Elon Musk  : NPR", "url": "https://www.npr.org/2018/09/27/652315858/sec-sues-tesla-ceo-elon-musk", "author": "No author found", "published_date": "2018-09-27", "content": "", "section": "Business", "disclaimer": ""}, "2018-09-27-652139784": {"title": "Netherlands Proposes Legislation To Ban Use Of Phones On Bicycles : NPR", "url": "https://www.npr.org/2018/09/27/652139784/netherlands-proposes-legislation-to-ban-use-of-phones-on-bicycles", "author": "No author found", "published_date": "2018-09-27", "content": "", "section": "World", "disclaimer": ""}, "2018-09-27-651868493": {"title": "Europe's Copyright Reforms Are More Than (Just) A Boring Policy Change : NPR", "url": "https://www.npr.org/2018/09/27/651868493/europes-copyright-reforms-are-more-than-just-a-boring-policy-change", "author": "No author found", "published_date": "2018-09-27", "content": "", "section": "Music News", "disclaimer": ""}, "2018-09-27-652119109": {"title": "Uber Pays $148 Million Over Yearlong Cover-Up Of Data Breach : NPR", "url": "https://www.npr.org/2018/09/27/652119109/uber-pays-148-million-over-year-long-cover-up-of-data-breach", "author": "No author found", "published_date": "2018-09-27", "content": "", "section": "Business", "disclaimer": ""}, "2018-09-28-652748290": {"title": "Facebook Says Hackers Accessed Information Of 50 Million Users In Latest Data Breach : NPR", "url": "https://www.npr.org/2018/09/28/652748290/facebook-says-hackers-accessed-information-of-50-million-users-in-latest-data-br", "author": "No author found", "published_date": "2018-09-28", "content": "AUDIE CORNISH, HOST: We learned today that Facebook has had a new security breach, and the company says it affects almost 50 million accounts. As a precaution, Facebook is logging off those accounts and about 40 million more. The company says no passwords were stolen, but NPR's Alina Selyukh reports the full scope of the attack is unclear. ALINA SELYUKH, BYLINE: Facebook says hackers exploited three separate security gaps to gain access to the code that allowed them to take over millions of user accounts. The security gaps came together in the feature called View As which allows users to see how their profile page looks to someone else. The hackers were able to get what's called access tokens. These are digital keys that, for example, let you stay logged in on the Facebook app without having to re-enter your password. The most important thing that we don't know is to what extent the hackers actually used their access to the accounts. Here's Facebook CEO Mark Zuckerberg. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: The investigation is still very early. So we do not yet know if any of the accounts were actually misused. SELYUKH: Zuckerberg said so far, the company has not found evidence that hackers had access to any private messages or posted to any accounts, though he added that this could change as the investigation continues. Here's Guy Rosen, Facebook's executive who oversees safety and security. (SOUNDBITE OF ARCHIVED RECORDING)GUY ROSEN: We haven't yet been able to determine if there's specific targeting. It does seem broad. And we don't yet know who is behind these attacks or where they might be based. SELYUKH: Zuckerberg pointed out several times how quickly his team acted given frequent accusations that Facebook moved too slowly on the Cambridge Analytica security scandal. Facebook says with this data breach, engineers discovered it on Tuesday, notified the FBI on Wednesday, made fixes on Thursday and notified the public today. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: It definitely is an issue that this happened in the first place. SELYUKH: On the call, reporters posed one question to Zuckerberg several times in different ways. Why should people keep trusting Facebook? Zuckerberg seemed to search for an answer before resorting to one of his regular phrases. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: Security is a bit of - it's an arms race. SELYUKH: He said the breach underscored how constant the hack attacks are and, without addressing the trust issue directly, said Facebook's security teams were working very hard. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: This is going to be an ongoing effort. And we're going to need to keep on focusing on this over time. SELYUKH: The same can be said about Facebook's ongoing challenge of convincing federal and state officials that it's not too big to secure the personal data of millions and millions of users. Alina Selyukh, NPR News. AUDIE CORNISH, HOST:  We learned today that Facebook has had a new security breach, and the company says it affects almost 50 million accounts. As a precaution, Facebook is logging off those accounts and about 40 million more. The company says no passwords were stolen, but NPR's Alina Selyukh reports the full scope of the attack is unclear. ALINA SELYUKH, BYLINE: Facebook says hackers exploited three separate security gaps to gain access to the code that allowed them to take over millions of user accounts. The security gaps came together in the feature called View As which allows users to see how their profile page looks to someone else. The hackers were able to get what's called access tokens. These are digital keys that, for example, let you stay logged in on the Facebook app without having to re-enter your password. The most important thing that we don't know is to what extent the hackers actually used their access to the accounts. Here's Facebook CEO Mark Zuckerberg. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: The investigation is still very early. So we do not yet know if any of the accounts were actually misused. SELYUKH: Zuckerberg said so far, the company has not found evidence that hackers had access to any private messages or posted to any accounts, though he added that this could change as the investigation continues. Here's Guy Rosen, Facebook's executive who oversees safety and security. (SOUNDBITE OF ARCHIVED RECORDING) GUY ROSEN: We haven't yet been able to determine if there's specific targeting. It does seem broad. And we don't yet know who is behind these attacks or where they might be based. SELYUKH: Zuckerberg pointed out several times how quickly his team acted given frequent accusations that Facebook moved too slowly on the Cambridge Analytica security scandal. Facebook says with this data breach, engineers discovered it on Tuesday, notified the FBI on Wednesday, made fixes on Thursday and notified the public today. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: It definitely is an issue that this happened in the first place. SELYUKH: On the call, reporters posed one question to Zuckerberg several times in different ways. Why should people keep trusting Facebook? Zuckerberg seemed to search for an answer before resorting to one of his regular phrases. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: Security is a bit of - it's an arms race. SELYUKH: He said the breach underscored how constant the hack attacks are and, without addressing the trust issue directly, said Facebook's security teams were working very hard. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: This is going to be an ongoing effort. And we're going to need to keep on focusing on this over time. SELYUKH: The same can be said about Facebook's ongoing challenge of convincing federal and state officials that it's not too big to secure the personal data of millions and millions of users. Alina Selyukh, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-09-28-652631686": {"title": "Facebook Says Security Breach Affected Almost 50 Million Accounts : NPR", "url": "https://www.npr.org/2018/09/28/652631686/facebook-says-security-breach-affected-almost-50-million-accounts", "author": "No author found", "published_date": "2018-09-28", "content": "", "section": "Technology", "disclaimer": ""}, "2018-09-28-652265556": {"title": "Vishaan Chakrabarti: How Can We Design More Welcoming Cities? : NPR", "url": "https://www.npr.org/2018/09/28/652265556/vishaan-chakrabarti-how-can-we-design-more-welcoming-cities", "author": "No author found", "published_date": "2018-09-28", "content": "GUY RAZ, HOST: So, Vishaan, please introduce yourself. VISHAAN CHAKRABARTI: OK. My name is Vishaan Chakrabarti. I'm an architect and author. I'm also a professor at Columbia University. RAZ: Vishaan was born in Kolkata, India. But as a kid, he moved to Massachusetts, where he often felt like an outsider. CHAKRABARTI: I grew up in a suburb outside of Boston, kind of hated every minute of it. It was a very sort of racist, violent upbringing. And for immigrants especially - I'm a first-generation immigrant - you speak differently than other people. You look different though than other people there. And so there's this tendency to feel a little homeless or lost. RAZ: When Vishaan became an adult, he ended up relocating to New York City. And for the first time in his life, he didn't really feel different. CHAKRABARTI: You know, what I immediately encountered when I came to New York was this feeling like I had kind of discovered a long-lost twin brother. No one really questioned whether you belonged here. You belonged automatically by virtue of being here. And I think one of the great things that a truly great city does is provide this sense that you belong by virtue of who you are, not, you know, what your background is. RAZ: And that got Vishaan thinking about what it was that made the city so welcoming. And after becoming an architect, he noticed that a city's vibrancy and inclusivity has a lot to do with how it's designed. CHAKRABARTI: If you think about architecture as an act of writing in the city, you have to read the city first. And all of the basic components are human components. They need to invite diversity across race, gender, sexual orientation and especially class and socioeconomic structure. RAZ: And he says the key difference between a welcoming city and a less welcoming one is scale because so many of our modern cities were designed around big, anonymous buildings with streets that aren't walkable and almost no places for people to just gather. But Vishaan says older cities around the world were built on a much more human scale. CHAKRABARTI: It's not a one-size-fits-all thing. But I think pre-20th century cities all around the world tend to be much more dynamic, lyrical places. So Venice - let's just take Venice. (SOUNDBITE OF UNIDENTIFIED SONG)UNIDENTIFIED MUSICAL ARTIST: (Singing) Bella, bella notte. CHAKRABARTI: You look at those arched pedestrian bridges. You're going over a canal. The clearance is actually quite low, so the person who's operating the gondola is standing. And they know they have to duck to get under the bridge, so they don't call a lawyer when their forehead hits the bridge because they made a mistake. They know it's their responsibility to duck. And so I think to get to a more humane city, it entails maybe more risk but more reward. You know, you can go to Israel, to India, to Rome, and you'll find - there was a building block for building density in the 19th century, which was one elevator, kind of this charming cable elevator. . . RAZ: Yeah. CHAKRABARTI: . . . Right? - that has light coming through it. And then there's one staircase that. . . RAZ: Yeah. CHAKRABARTI: . . . Wraps around it. RAZ: Yeah. CHAKRABARTI: And usually, that apartment building had, like, 20 units. And you see that each building is maybe 50 feet long, 60 feet long. And so you get this tremendous variation. One is brick. One is stone. Doorways are different. And so you have this kind of rhythm along the street. You run into your neighbor. Everyone knows each other. It's very communal. That form of building isn't legal today because of fire codes and wheelchair regulations and a whole bunch of things. So now you need, you know, a few elevators. And you need two sets of fire stairs. And that's all connected by a long, anonymous hallway. And suddenly, a developer is really incentivized to build a 200-unit building instead of a 20-unit building in order to sort of amortize all of that cost. And as a consequence, it's very hard to know your neighbors in a 200-unit building. And so the scale at which we build is different and more mass-produced. And so you see the same building landing in Shanghai, in Mumbai and Dubai. And so I don't think one has to be an architect or an urban planner to understand these issues. I think people are feeling this viscerally. I hear this everywhere I go from everyday people who aren't trained in this at all but feel is kind of creeping homogenization of their world. RAZ: Vishaan Chakrabarti picks up his idea from the TED stage. (SOUNDBITE OF TED TALK)CHAKRABARTI: Now, maybe you think I'm just being nostalgic. Why does it matter? Who cares if there is this creeping sameness besetting our planet? Well, it matters because most people around the world are gravitating to urban areas globally. And how we design those urban areas could well determine whether we thrive or not as a species. And this is not just an aesthetic issue, mind you. This is an issue of international consequence because today, every day, literally, hundreds of thousands of people are moving into a city somewhere. And when you think about that, ask yourself, are they condemned to live in the same bland cities we built in the 20th century, or can we offer them something better? RAZ: It seems to me that one of the things that makes many cities, especially cities in wealthy countries, somewhat inhumane is their inaccessibility to so many people. It's like, you know, the city has become this inhumane place because it keeps people away because it is so expensive to live in the city, like New York City. . . CHAKRABARTI: Yeah. RAZ: . . . Or San Francisco or Boston or London or Paris. CHAKRABARTI: Yes. And, in fact, one of the things I worry about with the U. S. is, you know, there's a lot to compliment about European cities. But one of the things we need to be really careful of is most European cities have almost all of their poor people living on the outskirts, on the periphery. RAZ: Yeah. CHAKRABARTI: And only, really, the well-to-do live in the heart of these beautiful cities that we talk about. And the problem often is market rate housing doesn't feel like it's helping to address the problem. . . RAZ: No. CHAKRABARTI: . . . Because, you know, it's out of reach. Now, there are ways in which government subsidizes affordable housing. But as an architect, one of the things we're looking at is, how do you lower the cost of housing so that construction costs can come down? And that allows people to charge less rent than they otherwise would. And this is where mass transit infrastructure and infrastructure more generally is so important because we don't have to think about the city as just the city limits. There's a region around every major city. And if we have better mass transportation to get people to work from 20 or 30 or 40 miles outside of the city, then they can live a transit-oriented lifestyle without living in the heart of the city. And a lot of people will choose that if we give them good transit options. And we're actually seeing those transformations happening all over the world in big ways and in small ways. RAZ: In just a moment, Vishaan imagines how technology could help us redesign cities and allow us to bring the humanity of older cities into the 21st century. Stay with us. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC)RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about cities and how we can build more humane ones. And just before the break, we were hearing from architect Vishaan Chakrabarti. And Vishaan says design could be part of the solution, but it largely depends on how we use technology. (SOUNDBITE OF TED TALK)CHAKRABARTI: Technology was a big part of the problem in the 20th century. When we invented the automobile, what happened is the world all bent towards the invention, and we recreated our landscape around it. In the 21st century, technology can be part of the solution if it bends to the needs of the world. And so what do I mean by that? Take the autonomous vehicle. I don't think the autonomous vehicle is exciting because it's a driverless car. I think what's exciting about the autonomous vehicle is the promise that we could have these small urban vehicles that could safely commingle with pedestrians and bicycles. That would enable us to design humane streets again, streets without curbs (ph), maybe streets like the wooden walkways on Fire Island. Or maybe we could design streets with the cobblestone of the 21st century, something that captures kinetic energy, melts snow, helps you with your fitness when you walk. Or remember those big-ladder fire trucks? What if we could replace them and all the asphalt that comes with them with drones and robots that could rescue people from burning buildings? And if you think that's outlandish, you'd be amazed to know how much of that technology is already being used today in rescue activity. But now I'd like you to really imagine with me. Imagine if we could design the hovercraft wheelchair - right? - an invention that would not only allow equal access but would enable us to build the Italian hill town of the 21st century. I think you'd be amazed to know that if just a few of these inventions responsive to human need - would completely transform the way we could build our cities. RAZ: I mean, the thing is - right? - let's say we do transform cities into these more humane places, like, with the help of technology or whatever. I mean, there will still be some level of inhumanity in them, right? CHAKRABARTI: Right. RAZ: I mean, it's not - is it possible to have no social discord at all? CHAKRABARTI: You know, part of why I gave the talk I gave at TED was being cognizant of the fact that there were a lot of technologists in the room. And technologists can be wonderful people, but they can also be a bit ahistorical. And what I was hoping to convey to a roomful of technologists is we need them to invent stuff that helps us build more humane cities, right? Firefighting drones and, you know, different kind of a wheelchair, and so forth. So we can look at all those codes and standards that we have to build with and change them, so we can maybe have a kind of \"Back To The Future\" moment and think about what was great about cities before the 20th century that could maybe inhabit our cities in the 21st and 22nd century because technology's allowed us to change those standards. But that's not the goal. The goal is those standards are changing, so we can design this infrastructure, so we create more social friction. Cities, at their best, are engines of social friction. So if you look at the Arab Spring, and you saw what was happening in Tahrir Square, or you saw what was happening in Tunisia, it's because cities tend to be the platform for social change in nations. And social friction requires a kind of - in my book, I call it an infrastructure of opportunity. This idea that you're building public spaces, subway systems, the farmer's market - places where people who are from different walks of life meet eyeball to eyeball - because social friction is fundamentally what advances civilization, and I think that's really the heart of it. (SOUNDBITE OF MUSIC)RAZ: Architect Vishaan Chakrabarti. He's a professor at Columbia, and his latest book is called \"A Country Of Cities: A Manifesto For An Urban America. \" You can see his full talk at ted. com. GUY RAZ, HOST:  So, Vishaan, please introduce yourself. VISHAAN CHAKRABARTI: OK. My name is Vishaan Chakrabarti. I'm an architect and author. I'm also a professor at Columbia University. RAZ: Vishaan was born in Kolkata, India. But as a kid, he moved to Massachusetts, where he often felt like an outsider. CHAKRABARTI: I grew up in a suburb outside of Boston, kind of hated every minute of it. It was a very sort of racist, violent upbringing. And for immigrants especially - I'm a first-generation immigrant - you speak differently than other people. You look different though than other people there. And so there's this tendency to feel a little homeless or lost. RAZ: When Vishaan became an adult, he ended up relocating to New York City. And for the first time in his life, he didn't really feel different. CHAKRABARTI: You know, what I immediately encountered when I came to New York was this feeling like I had kind of discovered a long-lost twin brother. No one really questioned whether you belonged here. You belonged automatically by virtue of being here. And I think one of the great things that a truly great city does is provide this sense that you belong by virtue of who you are, not, you know, what your background is. RAZ: And that got Vishaan thinking about what it was that made the city so welcoming. And after becoming an architect, he noticed that a city's vibrancy and inclusivity has a lot to do with how it's designed. CHAKRABARTI: If you think about architecture as an act of writing in the city, you have to read the city first. And all of the basic components are human components. They need to invite diversity across race, gender, sexual orientation and especially class and socioeconomic structure. RAZ: And he says the key difference between a welcoming city and a less welcoming one is scale because so many of our modern cities were designed around big, anonymous buildings with streets that aren't walkable and almost no places for people to just gather. But Vishaan says older cities around the world were built on a much more human scale. CHAKRABARTI: It's not a one-size-fits-all thing. But I think pre-20th century cities all around the world tend to be much more dynamic, lyrical places. So Venice - let's just take Venice. (SOUNDBITE OF UNIDENTIFIED SONG) UNIDENTIFIED MUSICAL ARTIST: (Singing) Bella, bella notte. CHAKRABARTI: You look at those arched pedestrian bridges. You're going over a canal. The clearance is actually quite low, so the person who's operating the gondola is standing. And they know they have to duck to get under the bridge, so they don't call a lawyer when their forehead hits the bridge because they made a mistake. They know it's their responsibility to duck. And so I think to get to a more humane city, it entails maybe more risk but more reward. You know, you can go to Israel, to India, to Rome, and you'll find - there was a building block for building density in the 19th century, which was one elevator, kind of this charming cable elevator. . . RAZ: Yeah. CHAKRABARTI: . . . Right? - that has light coming through it. And then there's one staircase that. . . RAZ: Yeah. CHAKRABARTI: . . . Wraps around it. RAZ: Yeah. CHAKRABARTI: And usually, that apartment building had, like, 20 units. And you see that each building is maybe 50 feet long, 60 feet long. And so you get this tremendous variation. One is brick. One is stone. Doorways are different. And so you have this kind of rhythm along the street. You run into your neighbor. Everyone knows each other. It's very communal. That form of building isn't legal today because of fire codes and wheelchair regulations and a whole bunch of things. So now you need, you know, a few elevators. And you need two sets of fire stairs. And that's all connected by a long, anonymous hallway. And suddenly, a developer is really incentivized to build a 200-unit building instead of a 20-unit building in order to sort of amortize all of that cost. And as a consequence, it's very hard to know your neighbors in a 200-unit building. And so the scale at which we build is different and more mass-produced. And so you see the same building landing in Shanghai, in Mumbai and Dubai. And so I don't think one has to be an architect or an urban planner to understand these issues. I think people are feeling this viscerally. I hear this everywhere I go from everyday people who aren't trained in this at all but feel is kind of creeping homogenization of their world. RAZ: Vishaan Chakrabarti picks up his idea from the TED stage. (SOUNDBITE OF TED TALK) CHAKRABARTI: Now, maybe you think I'm just being nostalgic. Why does it matter? Who cares if there is this creeping sameness besetting our planet? Well, it matters because most people around the world are gravitating to urban areas globally. And how we design those urban areas could well determine whether we thrive or not as a species. And this is not just an aesthetic issue, mind you. This is an issue of international consequence because today, every day, literally, hundreds of thousands of people are moving into a city somewhere. And when you think about that, ask yourself, are they condemned to live in the same bland cities we built in the 20th century, or can we offer them something better? RAZ: It seems to me that one of the things that makes many cities, especially cities in wealthy countries, somewhat inhumane is their inaccessibility to so many people. It's like, you know, the city has become this inhumane place because it keeps people away because it is so expensive to live in the city, like New York City. . . CHAKRABARTI: Yeah. RAZ: . . . Or San Francisco or Boston or London or Paris. CHAKRABARTI: Yes. And, in fact, one of the things I worry about with the U. S. is, you know, there's a lot to compliment about European cities. But one of the things we need to be really careful of is most European cities have almost all of their poor people living on the outskirts, on the periphery. RAZ: Yeah. CHAKRABARTI: And only, really, the well-to-do live in the heart of these beautiful cities that we talk about. And the problem often is market rate housing doesn't feel like it's helping to address the problem. . . RAZ: No. CHAKRABARTI: . . . Because, you know, it's out of reach. Now, there are ways in which government subsidizes affordable housing. But as an architect, one of the things we're looking at is, how do you lower the cost of housing so that construction costs can come down? And that allows people to charge less rent than they otherwise would. And this is where mass transit infrastructure and infrastructure more generally is so important because we don't have to think about the city as just the city limits. There's a region around every major city. And if we have better mass transportation to get people to work from 20 or 30 or 40 miles outside of the city, then they can live a transit-oriented lifestyle without living in the heart of the city. And a lot of people will choose that if we give them good transit options. And we're actually seeing those transformations happening all over the world in big ways and in small ways. RAZ: In just a moment, Vishaan imagines how technology could help us redesign cities and allow us to bring the humanity of older cities into the 21st century. Stay with us. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC) RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about cities and how we can build more humane ones. And just before the break, we were hearing from architect Vishaan Chakrabarti. And Vishaan says design could be part of the solution, but it largely depends on how we use technology. (SOUNDBITE OF TED TALK) CHAKRABARTI: Technology was a big part of the problem in the 20th century. When we invented the automobile, what happened is the world all bent towards the invention, and we recreated our landscape around it. In the 21st century, technology can be part of the solution if it bends to the needs of the world. And so what do I mean by that? Take the autonomous vehicle. I don't think the autonomous vehicle is exciting because it's a driverless car. I think what's exciting about the autonomous vehicle is the promise that we could have these small urban vehicles that could safely commingle with pedestrians and bicycles. That would enable us to design humane streets again, streets without curbs (ph), maybe streets like the wooden walkways on Fire Island. Or maybe we could design streets with the cobblestone of the 21st century, something that captures kinetic energy, melts snow, helps you with your fitness when you walk. Or remember those big-ladder fire trucks? What if we could replace them and all the asphalt that comes with them with drones and robots that could rescue people from burning buildings? And if you think that's outlandish, you'd be amazed to know how much of that technology is already being used today in rescue activity. But now I'd like you to really imagine with me. Imagine if we could design the hovercraft wheelchair - right? - an invention that would not only allow equal access but would enable us to build the Italian hill town of the 21st century. I think you'd be amazed to know that if just a few of these inventions responsive to human need - would completely transform the way we could build our cities. RAZ: I mean, the thing is - right? - let's say we do transform cities into these more humane places, like, with the help of technology or whatever. I mean, there will still be some level of inhumanity in them, right? CHAKRABARTI: Right. RAZ: I mean, it's not - is it possible to have no social discord at all? CHAKRABARTI: You know, part of why I gave the talk I gave at TED was being cognizant of the fact that there were a lot of technologists in the room. And technologists can be wonderful people, but they can also be a bit ahistorical. And what I was hoping to convey to a roomful of technologists is we need them to invent stuff that helps us build more humane cities, right? Firefighting drones and, you know, different kind of a wheelchair, and so forth. So we can look at all those codes and standards that we have to build with and change them, so we can maybe have a kind of \"Back To The Future\" moment and think about what was great about cities before the 20th century that could maybe inhabit our cities in the 21st and 22nd century because technology's allowed us to change those standards. But that's not the goal. The goal is those standards are changing, so we can design this infrastructure, so we create more social friction. Cities, at their best, are engines of social friction. So if you look at the Arab Spring, and you saw what was happening in Tahrir Square, or you saw what was happening in Tunisia, it's because cities tend to be the platform for social change in nations. And social friction requires a kind of - in my book, I call it an infrastructure of opportunity. This idea that you're building public spaces, subway systems, the farmer's market - places where people who are from different walks of life meet eyeball to eyeball - because social friction is fundamentally what advances civilization, and I think that's really the heart of it. (SOUNDBITE OF MUSIC) RAZ: Architect Vishaan Chakrabarti. He's a professor at Columbia, and his latest book is called \"A Country Of Cities: A Manifesto For An Urban America. \" You can see his full talk at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-01-653430456": {"title": "Justice Department Sues California To Block State's Net Neutrality Law : NPR", "url": "https://www.npr.org/2018/10/01/653430456/justice-department-sues-california-to-block-states-net-neutrality-law", "author": "No author found", "published_date": "2018-10-01", "content": "ARI SHAPIRO, HOST: California's governor, Jerry Brown, signed a law last night barring Internet service providers from blocking or slowing down traffic or charging for faster loading. The law basically restores the net neutrality rules created under the Obama administration and repealed under President Trump. These are seen as the strongest net neutrality protections in the country. And within hours, the Justice Department sued to block the California law. NPR's Alina Selyukh is here to walk us through what this means for Internet users like us. Hey, Alina. ALINA SELYUKH, BYLINE: Hi. SHAPIRO: Remind us what net neutrality is and what impact this California law, if implemented, would have on ordinary people going online. SELYUKH: Right. The overarching question of net neutrality is how much power your Internet providers should have over your Internet experience. So typically when people talk about net neutrality, they mean regulations like no blocking of whatever website you want to visit, that Internet providers should not be able to slow down an app that you're visiting. Other elements sometimes include things like zero rating, which is a deal where Verizon might give you streaming of Hulu that doesn't count toward your data restrictions but then count Netflix toward data restrictions. And these are all rules that California put into place. They banned all these things. SHAPIRO: Before the Trump administration repealed the net neutrality rules, there was a really vocal campaign by activists trying to get the FCC not to take this step. Now that California has gone this route, what has the reaction been? SELYUKH: Well, California was indeed sort of reacting to the massive liberal wave of activism and online activism that prompted the writing of these rules. You know, the battle lines have been drawn for a while on the net neutrality debate, and they're still the same. On the one hand, you've got Internet companies, especially smaller ones like Etsy or video streaming company Vimeo saying that these rules are critical for them to be able to compete against the bigger companies. On the other side, you've got the telecom providers - your AT&T, your Verizon, your Comcast - that have been pushing California to not put net neutrality rules into place and are expected to sue California as well. SHAPIRO: And the Justice Department has already sued California. Explain why they're doing that. SELYUKH: On so many things actually. So it's - on a political level, California is an interesting state. It is the largest economy in the United States. It has the power to sort of set the bar for a lot of regulations. And they have used that power. And they're currently in legal dispute with the Department of Justice over a variety of policies, including immigration, emissions standards. And often when California sets a standard, many companies look around and think to themselves that it is a lot easier to just abide by the California standard and sort of how goes California, so goes the rest of the nation. SHAPIRO: Kind of the role that Texas played during the Obama administration, a economic powerhouse that counters the politics of who's in the Oval Office. SELYUKH: And the Department of Justice in this case did respond. You know, Attorney General Jeff Sessions said exactly that essentially he's accusing California Legislature of attempting to frustrate federal policy, as he put it. SHAPIRO: So what does this mean for consumers and companies while this fight plays out? SELYUKH: Not much yet. The rules were not slated to go into effect until January. At this point, the Justice Department is seeking an injunction to not have those rules go into effect at all. In the meantime, on a federal level, the tech companies and the consumer groups and 20 attorneys general, including the California attorney general, are suing the Federal Communications Commission, trying to unroll the repeal of the Obama-era rules. So all of this is kind of in the hands of the courts. And we're waiting to see how that plays out. SHAPIRO: NPR's Alina Selyukh, thanks. SELYUKH: Thank you. ARI SHAPIRO, HOST:  California's governor, Jerry Brown, signed a law last night barring Internet service providers from blocking or slowing down traffic or charging for faster loading. The law basically restores the net neutrality rules created under the Obama administration and repealed under President Trump. These are seen as the strongest net neutrality protections in the country. And within hours, the Justice Department sued to block the California law. NPR's Alina Selyukh is here to walk us through what this means for Internet users like us. Hey, Alina. ALINA SELYUKH, BYLINE: Hi. SHAPIRO: Remind us what net neutrality is and what impact this California law, if implemented, would have on ordinary people going online. SELYUKH: Right. The overarching question of net neutrality is how much power your Internet providers should have over your Internet experience. So typically when people talk about net neutrality, they mean regulations like no blocking of whatever website you want to visit, that Internet providers should not be able to slow down an app that you're visiting. Other elements sometimes include things like zero rating, which is a deal where Verizon might give you streaming of Hulu that doesn't count toward your data restrictions but then count Netflix toward data restrictions. And these are all rules that California put into place. They banned all these things. SHAPIRO: Before the Trump administration repealed the net neutrality rules, there was a really vocal campaign by activists trying to get the FCC not to take this step. Now that California has gone this route, what has the reaction been? SELYUKH: Well, California was indeed sort of reacting to the massive liberal wave of activism and online activism that prompted the writing of these rules. You know, the battle lines have been drawn for a while on the net neutrality debate, and they're still the same. On the one hand, you've got Internet companies, especially smaller ones like Etsy or video streaming company Vimeo saying that these rules are critical for them to be able to compete against the bigger companies. On the other side, you've got the telecom providers - your AT&T, your Verizon, your Comcast - that have been pushing California to not put net neutrality rules into place and are expected to sue California as well. SHAPIRO: And the Justice Department has already sued California. Explain why they're doing that. SELYUKH: On so many things actually. So it's - on a political level, California is an interesting state. It is the largest economy in the United States. It has the power to sort of set the bar for a lot of regulations. And they have used that power. And they're currently in legal dispute with the Department of Justice over a variety of policies, including immigration, emissions standards. And often when California sets a standard, many companies look around and think to themselves that it is a lot easier to just abide by the California standard and sort of how goes California, so goes the rest of the nation. SHAPIRO: Kind of the role that Texas played during the Obama administration, a economic powerhouse that counters the politics of who's in the Oval Office. SELYUKH: And the Department of Justice in this case did respond. You know, Attorney General Jeff Sessions said exactly that essentially he's accusing California Legislature of attempting to frustrate federal policy, as he put it. SHAPIRO: So what does this mean for consumers and companies while this fight plays out? SELYUKH: Not much yet. The rules were not slated to go into effect until January. At this point, the Justice Department is seeking an injunction to not have those rules go into effect at all. In the meantime, on a federal level, the tech companies and the consumer groups and 20 attorneys general, including the California attorney general, are suing the Federal Communications Commission, trying to unroll the repeal of the Obama-era rules. So all of this is kind of in the hands of the courts. And we're waiting to see how that plays out. SHAPIRO: NPR's Alina Selyukh, thanks. SELYUKH: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-01-652513097": {"title": "India's Biometric ID System Has Led To Starvation For Some Poor, Advocates Say : NPR", "url": "https://www.npr.org/2018/10/01/652513097/indias-biometric-id-system-has-led-to-starvation-for-some-poor-advocates-say", "author": "No author found", "published_date": "2018-10-01", "content": "AILSA CHANG, HOST: It's October, and this month, we're looking at our bodies the way technology sees them in All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CHANG: Smartwatches that detect heart problems, airport security systems that match our faces to our passports - as this type of technology becomes more common, it's forcing us to make some tough decisions. ARI SHAPIRO, HOST: We start in India. It has 1. 3 billion people but no equivalent of the Social Security number, so the government has struggled to deliver benefits to people who might be illiterate or live in remote, rural areas. It created the biggest biometric ID system in the world, and NPR's new Mumbai correspondent Lauren Frayer recently joined it. She has this report. LAUREN FRAYER, BYLINE: Do I look straight ahead? UNIDENTIFIED PERSON #1: Yes. FRAYER: So I look into this machine, and it's scanning the inside of my eye, my irises. Moving to India means joining the world's biggest biometric database called Aadhaar. I've been assigned a unique 12-digit number linked to my fingerprints, photo and iris scans. The data is stored on government servers. Aadhaar, which means foundation in Hindi, started eight years ago with a big, patriotic PR campaign. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: (Foreign language spoken). UNIDENTIFIED PERSON #3: (Foreign language spoken). UNIDENTIFIED PERSON #4: (Foreign language spoken). UNIDENTIFIED PERSON #5: (Foreign language spoken). FRAYER: This commercial shows elderly people smiling as Aadhaar helps them collect state pensions. The system is voluntary, but in just eight years, the government has managed to enroll more than 1. 2 billion people, the vast majority of the country, even those hardest to reach. (SOUNDBITE OF CAR HORN HONKING)FRAYER: Downtown Mumbai is packed with soaring glass skyscrapers, headquarters for some of India's biggest banks. But just across the street, there are 25 street children asleep in a circle under a highway underpass. MANISHA KAMBLE: My name is Manisha. I'm 17 years old (laughter). FRAYER: Manisha Kamble grew up on the street. She has no address, no birth certificate. She was basically invisible to the state until the charity Save the Children helped her enroll in Aadhaar. KAMBLE: (Foreign language spoken). FRAYER: \"In India, you're nothing without Aadhaar,\" Manisha says. She's proud to be counted, to become official. She's used Aadhaar to enroll in school. She studies at night under street lamps and got the highest marks in her class last spring. Aadhaar can be used to verify your identity when you do anything with the government - get married, pay taxes or draw welfare and also when you open a bank account, sign up for a cellphone contract or set up an e-wallet online. The system is designed to cut fraud. It's hard to counterfeit your irises. But it requires electricity to scan people's biometrics and Internet to check them against government databases. In downtown Mumbai, you might have those. In poorer places, you often don't. ASHOK KUMAR: (Foreign language spoken). FRAYER: Ashok Kumar scoops and measures out rice rations in rural Jharkhand, one of India's poorest states. More than half of Indians are eligible for free or subsidized food. The government says Aadhaar has helped purge hundreds of thousands of fake names from ration lists and from voter rolls. (SOUNDBITE OF SCANNER BEEPING)FRAYER: People line up outside Mr. Kumar's tiny stucco shop. He scans their fingerprints with something that looks like a credit card machine. It runs on batteries and a cellphone signal. KUMAR: No Internet. FRAYER: But he says the network is shaky. He walks across the street, lifting his machine up overhead until he finally gets a signal. He sets up shop instead on the steps of a Hindu temple. (SOUNDBITE OF SCANNER BEEPING)FRAYER: So you're putting in the Aadhaar number. COMPUTER-GENERATED VOICE: (Foreign language spoken). FRAYER: And now this lady will put her finger on the scanner. It checks her Aadhaar number against her fingerprints in a government database and prints out a receipt for her ration, a bag of rice. COMPUTER-GENERATED VOICE: (Foreign language spoken). FRAYER: The next customer, Karu Bhuiya, is not as lucky. His fingerprints are worn from manual labor. Mr. Kumar tries to scan them five times, but he gets an error message. KUMAR: Have problem. FRAYER: Problem with the machine. KUMAR: Problem. COMPUTER-GENERATED VOICE: (Foreign language spoken). FRAYER: Most machines in rural India only scan fingerprints, not irises. So Mr. Bhuiya goes home without food. Technical difficulties like this are blamed for pushing some of India's poorest into starvation, says economist Jean Dreze, who lives in Jharkhand, where Aadhaar is mandatory for food rations. He says he's counted a dozen such deaths in recent months. JEAN DREZE: I would actually prefer to call these destitution deaths because they are all cases of people who went hungry for days, who would have survived if they had had some resources. See; this is the unfortunate thing - that the most vulnerable people are those who are also more likely to be excluded by the system. FRAYER: When Aadhaar scanners break down, there's supposed to be a backup system on paper. But at the ration shop we visited, the paper log was blank, unused. NANDAN NILEKANI: There could be some implementation issues. Nobody should be denied benefits, either for lack of Aadhaar or for lack of authentication. FRAYER: Nandan Nilekani is a tech billionaire who left the private sector to create Aadhaar for the Indian government. He told NPR this past May that the benefits far outweigh any glitches. Last January, a data breach prompted many Indians to question that, though. Investigative journalist Rachna Khaira discovered that the laptops of some Aadhaar enrollment workers had been hacked. Khaira managed to buy access to up to a billion people's Aadhaar data for less than $7. RACHNA KHAIRA: My only concern was this - that if we should implement this project, it should be foolproof. We should not be scared. FRAYER: Scared that the government may not be able to keep people's data secure. And it's not just a task for the government. One of the ways India managed to enroll so many people was by partnering with banks, utilities and cellphone providers, many of which require Aadhaar. So now your data resides with all of those companies, too. Privacy activist Nikhil Pahwa says it's impossible to know how many data breaches have occurred. There are reports almost every day. NIKHIL PAHWA: Look; when it comes to Aadhaar, it's the Wild West out there in India. Millions and millions of people have been compromised by the process. I see this as a major national security risk. FRAYER: Concerned privacy activists took their case all the way to India's Supreme Court. And last week, the court ruled that private companies can no longer ask for your Aadhaar data. It also said schools can no longer require biometrics for admission. But the data is already out there and being used by marketing companies, and possibly by political parties. In India, though, these are mostly concerns for the educated urban class. (SOUNDBITE OF RAIN FALLING)FRAYER: Not far from that ration shop in Jharkhand, migrant workers take refuge from the monsoon in sagging thatch huts covered with blue tarps. Among them is Nisha Devi, who believes it was hunger that killed her uncle who recently died before he could get an Aadhaar card. NISHA DEVI: (Foreign language spoken). FRAYER: She says the rest of the family rushed to enroll after his death. Local officials told them it could help them get welfare. Devi has not been able to get benefits yet, but she hopes this sophisticated biometric system might one day help her. Lauren Frayer, NPR News, in rural Jharkhand, India. (SOUNDBITE OF DAN ROMER'S \"AN OLD FASHIONED MAN\") AILSA CHANG, HOST:  It's October, and this month, we're looking at our bodies the way technology sees them in All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CHANG: Smartwatches that detect heart problems, airport security systems that match our faces to our passports - as this type of technology becomes more common, it's forcing us to make some tough decisions. ARI SHAPIRO, HOST:  We start in India. It has 1. 3 billion people but no equivalent of the Social Security number, so the government has struggled to deliver benefits to people who might be illiterate or live in remote, rural areas. It created the biggest biometric ID system in the world, and NPR's new Mumbai correspondent Lauren Frayer recently joined it. She has this report. LAUREN FRAYER, BYLINE: Do I look straight ahead? UNIDENTIFIED PERSON #1: Yes. FRAYER: So I look into this machine, and it's scanning the inside of my eye, my irises. Moving to India means joining the world's biggest biometric database called Aadhaar. I've been assigned a unique 12-digit number linked to my fingerprints, photo and iris scans. The data is stored on government servers. Aadhaar, which means foundation in Hindi, started eight years ago with a big, patriotic PR campaign. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #2: (Foreign language spoken). UNIDENTIFIED PERSON #3: (Foreign language spoken). UNIDENTIFIED PERSON #4: (Foreign language spoken). UNIDENTIFIED PERSON #5: (Foreign language spoken). FRAYER: This commercial shows elderly people smiling as Aadhaar helps them collect state pensions. The system is voluntary, but in just eight years, the government has managed to enroll more than 1. 2 billion people, the vast majority of the country, even those hardest to reach. (SOUNDBITE OF CAR HORN HONKING) FRAYER: Downtown Mumbai is packed with soaring glass skyscrapers, headquarters for some of India's biggest banks. But just across the street, there are 25 street children asleep in a circle under a highway underpass. MANISHA KAMBLE: My name is Manisha. I'm 17 years old (laughter). FRAYER: Manisha Kamble grew up on the street. She has no address, no birth certificate. She was basically invisible to the state until the charity Save the Children helped her enroll in Aadhaar. KAMBLE: (Foreign language spoken). FRAYER: \"In India, you're nothing without Aadhaar,\" Manisha says. She's proud to be counted, to become official. She's used Aadhaar to enroll in school. She studies at night under street lamps and got the highest marks in her class last spring. Aadhaar can be used to verify your identity when you do anything with the government - get married, pay taxes or draw welfare and also when you open a bank account, sign up for a cellphone contract or set up an e-wallet online. The system is designed to cut fraud. It's hard to counterfeit your irises. But it requires electricity to scan people's biometrics and Internet to check them against government databases. In downtown Mumbai, you might have those. In poorer places, you often don't. ASHOK KUMAR: (Foreign language spoken). FRAYER: Ashok Kumar scoops and measures out rice rations in rural Jharkhand, one of India's poorest states. More than half of Indians are eligible for free or subsidized food. The government says Aadhaar has helped purge hundreds of thousands of fake names from ration lists and from voter rolls. (SOUNDBITE OF SCANNER BEEPING) FRAYER: People line up outside Mr. Kumar's tiny stucco shop. He scans their fingerprints with something that looks like a credit card machine. It runs on batteries and a cellphone signal. KUMAR: No Internet. FRAYER: But he says the network is shaky. He walks across the street, lifting his machine up overhead until he finally gets a signal. He sets up shop instead on the steps of a Hindu temple. (SOUNDBITE OF SCANNER BEEPING) FRAYER: So you're putting in the Aadhaar number. COMPUTER-GENERATED VOICE: (Foreign language spoken). FRAYER: And now this lady will put her finger on the scanner. It checks her Aadhaar number against her fingerprints in a government database and prints out a receipt for her ration, a bag of rice. COMPUTER-GENERATED VOICE: (Foreign language spoken). FRAYER: The next customer, Karu Bhuiya, is not as lucky. His fingerprints are worn from manual labor. Mr. Kumar tries to scan them five times, but he gets an error message. KUMAR: Have problem. FRAYER: Problem with the machine. KUMAR: Problem. COMPUTER-GENERATED VOICE: (Foreign language spoken). FRAYER: Most machines in rural India only scan fingerprints, not irises. So Mr. Bhuiya goes home without food. Technical difficulties like this are blamed for pushing some of India's poorest into starvation, says economist Jean Dreze, who lives in Jharkhand, where Aadhaar is mandatory for food rations. He says he's counted a dozen such deaths in recent months. JEAN DREZE: I would actually prefer to call these destitution deaths because they are all cases of people who went hungry for days, who would have survived if they had had some resources. See; this is the unfortunate thing - that the most vulnerable people are those who are also more likely to be excluded by the system. FRAYER: When Aadhaar scanners break down, there's supposed to be a backup system on paper. But at the ration shop we visited, the paper log was blank, unused. NANDAN NILEKANI: There could be some implementation issues. Nobody should be denied benefits, either for lack of Aadhaar or for lack of authentication. FRAYER: Nandan Nilekani is a tech billionaire who left the private sector to create Aadhaar for the Indian government. He told NPR this past May that the benefits far outweigh any glitches. Last January, a data breach prompted many Indians to question that, though. Investigative journalist Rachna Khaira discovered that the laptops of some Aadhaar enrollment workers had been hacked. Khaira managed to buy access to up to a billion people's Aadhaar data for less than $7. RACHNA KHAIRA: My only concern was this - that if we should implement this project, it should be foolproof. We should not be scared. FRAYER: Scared that the government may not be able to keep people's data secure. And it's not just a task for the government. One of the ways India managed to enroll so many people was by partnering with banks, utilities and cellphone providers, many of which require Aadhaar. So now your data resides with all of those companies, too. Privacy activist Nikhil Pahwa says it's impossible to know how many data breaches have occurred. There are reports almost every day. NIKHIL PAHWA: Look; when it comes to Aadhaar, it's the Wild West out there in India. Millions and millions of people have been compromised by the process. I see this as a major national security risk. FRAYER: Concerned privacy activists took their case all the way to India's Supreme Court. And last week, the court ruled that private companies can no longer ask for your Aadhaar data. It also said schools can no longer require biometrics for admission. But the data is already out there and being used by marketing companies, and possibly by political parties. In India, though, these are mostly concerns for the educated urban class. (SOUNDBITE OF RAIN FALLING) FRAYER: Not far from that ration shop in Jharkhand, migrant workers take refuge from the monsoon in sagging thatch huts covered with blue tarps. Among them is Nisha Devi, who believes it was hunger that killed her uncle who recently died before he could get an Aadhaar card. NISHA DEVI: (Foreign language spoken). FRAYER: She says the rest of the family rushed to enroll after his death. Local officials told them it could help them get welfare. Devi has not been able to get benefits yet, but she hopes this sophisticated biometric system might one day help her. Lauren Frayer, NPR News, in rural Jharkhand, India. (SOUNDBITE OF DAN ROMER'S \"AN OLD FASHIONED MAN\")", "section": "Asia", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-02-653736464": {"title": "'Presidential Alert' Message Sent Wednesday: It Was Only A Test : NPR", "url": "https://www.npr.org/2018/10/02/653736464/this-is-only-a-test-why-your-cellphone-will-buzz-wednesday-afternoon", "author": "No author found", "published_date": "2018-10-02", "content": "RACHEL MARTIN, HOST: This afternoon at exactly 2:18 Eastern time, you can expect to get a presidential alert on your cellphone. It's not exactly from President Trump. Rather, it's a test of a new nationwide warning system that a president could use in case of an attack by another country or a cyberattack or a widespread natural disaster. Still, some worry President Trump could abuse the system. NPR's Brian Naylor reports. BRIAN NAYLOR, BYLINE: Back in the days of the Cold War, it was pretty common to hear this announcement on TV or radio. You know the one - this station is conducting a test of the Emergency Broadcasting System. Things have gotten a little more sophisticated now. But basically, the same minute-long Emergency Alert System test is conducted every month or so on broadcast, cable and satellite TV and radio. There will be one of those this afternoon, too. But today, you'll also be getting an alert on your cellphone. Now, some of us receive alerts already - Amber Alerts for missing children or flash flood or tornado warnings. But this is the first time that you'll be getting a national presidential alert from FEMA. Irwin Redlener is director of the National Center for Disaster Preparedness at Columbia University. He says testing the system makes sense. IRWIN REDLENER: I think having the testing of this new wireless Emergency Alert System is a good idea. And you can think about a variety of scenarios where it would be good for the president to be able to speak directly to the public. NAYLOR: But this is where Dr. Redlener has some qualms, especially, he says, when it comes to this president. REDLENER: We shudder to think that the president might be using such a system for political purposes or to create a diversion if he felt the presidency was under threat for whatever reason. So I think there are real concerns here. I hate to be so blunt about it, but these are not powers that many Americans would want to give to Donald Trump. NAYLOR: Redlener is not alone. Last week, three people filed suit in New York to block the testing of the system known as Wireless Emergency Alerts, or WEA. The suit states the plaintiffs are Americans who do not wish to receive text messages of any kind on any topic or subject from President Trump. They say the government is violating their privacy and the sanctity of their homes and that it wants to turn people's cellphones into government loudspeakers that compel listening. FEMA says that accusation is baseless. In a background briefing for reporters, a senior FEMA official conceded, while there is no opting out of the alert, the system is very well-governed, and you would not have a situation where the president would just wake up one morning, as the official put it, and attempt to send a personal message. And besides, he has Twitter for that. Brian Naylor, NPR News, Washington. (SOUNDBITE OF KOETT'S \"LAST NIGHT ON RIVER\") RACHEL MARTIN, HOST:  This afternoon at exactly 2:18 Eastern time, you can expect to get a presidential alert on your cellphone. It's not exactly from President Trump. Rather, it's a test of a new nationwide warning system that a president could use in case of an attack by another country or a cyberattack or a widespread natural disaster. Still, some worry President Trump could abuse the system. NPR's Brian Naylor reports. BRIAN NAYLOR, BYLINE: Back in the days of the Cold War, it was pretty common to hear this announcement on TV or radio. You know the one - this station is conducting a test of the Emergency Broadcasting System. Things have gotten a little more sophisticated now. But basically, the same minute-long Emergency Alert System test is conducted every month or so on broadcast, cable and satellite TV and radio. There will be one of those this afternoon, too. But today, you'll also be getting an alert on your cellphone. Now, some of us receive alerts already - Amber Alerts for missing children or flash flood or tornado warnings. But this is the first time that you'll be getting a national presidential alert from FEMA. Irwin Redlener is director of the National Center for Disaster Preparedness at Columbia University. He says testing the system makes sense. IRWIN REDLENER: I think having the testing of this new wireless Emergency Alert System is a good idea. And you can think about a variety of scenarios where it would be good for the president to be able to speak directly to the public. NAYLOR: But this is where Dr. Redlener has some qualms, especially, he says, when it comes to this president. REDLENER: We shudder to think that the president might be using such a system for political purposes or to create a diversion if he felt the presidency was under threat for whatever reason. So I think there are real concerns here. I hate to be so blunt about it, but these are not powers that many Americans would want to give to Donald Trump. NAYLOR: Redlener is not alone. Last week, three people filed suit in New York to block the testing of the system known as Wireless Emergency Alerts, or WEA. The suit states the plaintiffs are Americans who do not wish to receive text messages of any kind on any topic or subject from President Trump. They say the government is violating their privacy and the sanctity of their homes and that it wants to turn people's cellphones into government loudspeakers that compel listening. FEMA says that accusation is baseless. In a background briefing for reporters, a senior FEMA official conceded, while there is no opting out of the alert, the system is very well-governed, and you would not have a situation where the president would just wake up one morning, as the official put it, and attempt to send a personal message. And besides, he has Twitter for that. Brian Naylor, NPR News, Washington. (SOUNDBITE OF KOETT'S \"LAST NIGHT ON RIVER\")", "section": "Politics", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-03-654011402": {"title": "Don't Want To Unlock Your Device For Customs? New Zealand May Fine You $3,300 : NPR", "url": "https://www.npr.org/2018/10/03/654011402/dont-want-to-unlock-your-device-for-customs-new-zealand-may-fine-you-3-300", "author": "No author found", "published_date": "2018-10-03", "content": "", "section": "World", "disclaimer": ""}, "2018-10-03-653961505": {"title": "Probe Lands On Ryugu Asteroid In Latest Success For International Group : NPR", "url": "https://www.npr.org/2018/10/03/653961505/probe-lands-on-ryugu-asteroid-in-latest-success-for-international-group", "author": "No author found", "published_date": "2018-10-03", "content": "", "section": "Space", "disclaimer": ""}, "2018-10-04-654333466": {"title": "The Central Question Behind Facebook: 'What Does Mark Zuckerberg Believe In?'  : NPR", "url": "https://www.npr.org/2018/10/04/654333466/the-central-question-behind-facebook-what-does-mark-zuckerberg-believe-in", "author": "No author found", "published_date": "2018-10-04", "content": "TERRY GROSS, HOST: This is FRESH AIR. I'm Terry Gross. Last week, Facebook announced the most serious security breach in the company's history in which an unknown hacker was able to log onto the accounts of at least 50 million Facebook users. But that's just one element of the crisis facing the world's largest social media platform. Facebook was the conduit for a Russian-backed disinformation campaign in the 2016 election that reached tens of millions of its users. And the political consulting firm Cambridge Analytica got access to the personal information of 87 million Facebook users, which they used to target messages for the Trump campaign. Facebook is now under investigation by the FBI, the Securities and Exchange Commission, the Federal Trade Commission and authorities in Europe and Australia. Our guest, New Yorker staff writer Evan Osnos, explores Facebook's history and profiles its 34-year-old founder and chief executive Mark Zuckerberg in a recent issue of the magazine. He says Zuckerberg's relentless drive to expand Facebook's reach has jeopardized the privacy of its users and made it vulnerable to political manipulation. His story is titled, \"Ghosts In The Machine: Can Mark Zuckerberg Fix Facebook Before It Breaks Democracy? \" He spoke with FRESH AIR'S Dave Davies. DAVE DAVIES, BYLINE: Well, Evan Osnos, welcome back to FRESH AIR. You know, we all know Facebook is a very big deal. Give us a sense of its size and reach. EVAN OSNOS: There's really nothing quite like it in the history of American business. It has now 2. 2 billion monthly active users, meaning it's larger than any country in the world. It's really got no natural precedent when you look at the history of enterprise. It's really closer, in terms of scale and reach, to a political ideology or a religious faith. I mean, just in literal terms, it now has as many adherents as Christianity. And that has all been built in the last 14 years, since it was founded in 2004. DAVIES: For this piece, you visited Mark Zuckerberg at his home, at his office. You had numerous meetings. A lot of access from a guy who is pretty careful about, you know, journalists getting connected to him. How did you convince him to share so much time with you? OSNOS: Well, he did it, I should say, reluctantly. You know, this was a long process that started a year ago when I first approached them about the idea of this kind of story. I said, look, we do in The New Yorker these very long, detailed profiles. And I think initially Facebook's view was, we don't need to do this. We're a big, powerful company. And I continued to work on it, basically started interviewing a lot of other people around Facebook, people who worked there in the past, people who work there in the present. And as that accumulated, I continued to say, look; this fundamentally should be about how Mark Zuckerberg sees the world. And over time, they came to, I think, sort of grudgingly - but I'm grateful for it - accept the idea that, to an extraordinary degree, Mark Zuckerberg is Facebook. I mean, that is the reality. He's the chairman. He's the CEO. He owns, he controls 60 percent of the voting shares. So if you're going to understand Facebook in any meaningful way, the conversation really has to start with him and end with him. And for that, I think they ultimately recognized he had to be a part of this project. The company has come up against a growing and really serious decline of public trust, both among politicians and among the general public. And I think they recognize that at the core of that - and I think he recognizes that - there is this profound question mark around what does Mark Zuckerberg believe in? What does he stand for? What does he care about? How does he see his role in the country? What does he see for the role of technology? And this project was my attempt to try to answer some of those questions for myself and for readers. DAVIES: Zuckerberg started Facebook when he was at Harvard. It takes off, and it grows and it grows because they're determined, you know, to get more users, to connect more people, as Zuckerberg likes to say. And you're right that in about 2007, it plateaued at about 50 million users, which a lot of other kind of similar platforms had done. What did the company do to break through that ceiling? OSNOS: Well, they first panicked, really. There was a sense internally that they wondered whether they'd sort of hit the wall and whether this thing that had grown so fast was now over. And what they did was they discovered something important, which is that in order to punch through that wall, they had to come up with a whole range of new ways of accessing new populations, people who wouldn't otherwise have been on Facebook. The first thing they did was they discovered they simply had to translate the site into other languages, make it available to other countries around the world and allow people to be able to post in Spanish and so on. And by doing that, they crossed this barrier, this moat, which had prevented other social media sites from growing. But it did something even more important, which was that it established the sacred importance - really, sacred is the word - of growth internally. They created a team called the Growth Team, which became - as a former Facebook executive said to me, it was really the cool crowd. This was the team everyone wanted to be on, was Growth. It was about really sanctifying the idea of growth as an end in itself, that if Facebook stopped growing then its whole reason for being would cease to exist. And that idea ended up becoming the dominant fact of the next decade, from 2007 until today, when Facebook became extraordinarily focused on whatever it could do to continue growing year after year. DAVIES: And one of the things they did was that they made it a platform for outside developers so that - maybe you want to explain what this means. So that other software developers could hitch into Facebook and use it. OSNOS: They made a big choice, which was that, instead of saying we're just going to be our own site, we're actually going to try to become a platform, like, almost like an operating system, the way that Windows used to be the, you know, the way that everybody went onto their personal computer, and then you would build applications on top of that. And that decision to open themselves up as a platform meant that they were then sharing data to a much larger group of developers, of programmers, than they otherwise would. And this turned out to be a fateful decision because if you fast-forward many years later to, you know, what we all now know as the Cambridge Analytica scandal, that was - it was directly a result of this decision to open themselves up as a platform. Because what they'd done was they'd allowed an academic researcher to use some of the data on the platform to build a personality quiz. But then he sold that data to the political consultancy known as Cambridge Analytica. And that sort of back door, the way in which that data went out the door, has created much of the crisis that now engulfs the company today. DAVIES: And you write that an executive within Facebook, Sandy Parakilas, was put in charge of looking into what these outside, you know, app developers were doing with the data that they got from the Facebook users who, you know, plugged into their games. What did he find? OSNOS: What he found unnerved him. Sandy Parakilas had joined Facebook in 2011 and was one of the people responsible for going out and figuring out whether the data that they were giving to developers was being misused in any way. You know, were people violating privacy? Were they taking more data than they were supposed to? And what he found was that they were. In some cases, programmers, for instance, were siphoning off people's pictures and their private messages. In other cases, he found a developer that was building essentially shadow profiles, profiles for people who've never given their consent, including for children. And it was - this was a case in which there was just this feeling of it being the Wild West. This data was out there, and nobody was paying attention. And he raised alarms internally. As he tells the story, what he did was he said, look, we need to do a major audit to go out and figure out where is our data, who has it and how are they using it. And as he says he was told, that's not going to happen because if you do it, you may not want to know what you're going to find. Meaning that they may have already lost control of so much of that data that they didn't really want to discover the full reach. And Sandy Parakilas left, but in many ways his warnings turned out to be prophetic because exactly the kind of undisciplined use of data which he had warned about and tried to raise greater alarms about internally turned out to be the origins of the Cambridge Analytica scandal, which became so consuming for the company this year. DAVIES: So a lot of resources within the company devoted to pushing growth, not so much into doing it responsibly. OSNOS: Yes. Exactly. As he said, you know, I needed people, I needed engineers to help me try to police where this data was going, but all of the resources were going to growth. He said at some point, he told me, that the growth team had engineers coming out of their ears. They had everything they wanted. Everything was devoted to growing, and it was not devoted to making sure that what they were doing at the time was safe. DAVIES: What are some of the other things the company did to keep growing that kind of pushed the boundaries of privacy? OSNOS: Well, they began to build in lots of ingenious details into the design of the site, things like autoplay videos. This is something as simple as changing the nature of Facebook so that when you scroll down your page, that the videos would begin to play without you having to click on them. What that was was essentially taking advantage of some of our own psychological wiring. That means that just eliminating that small obstacle makes you much more likely to stay on the site, to watch that ad and ultimately to consume whatever advertising is around it. And they did other things. They, for instance, you know, you remember in the very beginning Facebook used to have pages - single pages - you'd have to click onto the next page. They got rid of that, so it's just a continuous scroll. And all of these little tiny details were, in a way, making Facebook into a new generation of behavioral experts. They figured out how do you tweak people's vanities and their passions and their susceptibilities and their desires in order to keep them on the site. The most important thing Facebook could do - and this is how they measured it - was make sure that people were signing on and staying on. Whenever you joined the company, in your orientation, you were taught about something very important, a metric known as L6 of 7, which means the number of people who logged in six of the last seven days. And whatever you could do to try to raise that L6 of 7, that was the priority. But as, you know, somebody put it to me, the problem with that is that eventually you exhaust the positive ways of boosting that engagement, and eventually you start to look at what this person described as the dark patterns, the ways that you can use anxiety or vanity to try to get people to sign on. One of the things they discovered, for instance, was that if you send somebody an email that says that a Facebook friend has uploaded a picture of them to Facebook, that people are almost incapable of resisting the temptation to look. And that sort of tweak, that just minor behavioral nudge, turned out to be really hugely important to the growth of Facebook. DAVIES: Evan Osnos is a staff writer for The New Yorker. We'll talk some more after a short break. This is FRESH AIR. (SOUNDBITE OF MICHAEL BELLAR AND THE AS-IS ENSEMBLE'S \"HOT BOX MAGIC\")DAVIES: This is FRESH AIR, and we're speaking with New Yorker staff writer Evan Osnos. He has a piece in the magazine called \"Ghost In The Machine\" about Mark Zuckerberg and Facebook and its current controversies. It's in a recent issue. Now Facebook is in a lot of trouble these days because of data breaches - its role in elections. But you're right that some former Facebook executives are voicing doubts about the company's role for other things - exacerbating isolation, outrage, addictive behaviors. You want to give us an example? OSNOS: This is a big change for Facebook. Traditionally, its former executives have been silent. They leave the company and very often don't say much about it. Starting last year, Sean Parker who was Facebook's first president - he's a sort of well-known Silicon Valley figure - gave an interview in which he said that, as he put it, he's become a conscientious objector to social media. He said God knows what it's doing to our children's brains. And then a few days later, there was another very prominent former Facebook executive named Chamath Palihapitiya, who had been the head of growth, a vice president of user growth, which was an absolutely central position at the company for a number of years. And he came out and said, you know, in the back of our minds we all had a sense that something might be going wrong, that our product, the thing that we were building - I'm paraphrasing here - was, as he put it, contributing to the breakdown of social discourse, the breakdown of society. And he said he would never let his children use this kind of product. I think because the public face of the company has been so on message about how they contribute, in their minds, to doing good in the world, to have some very senior former executives come out and talk about what the public has come to believe, which is that there are these very serious side effects to the company's growth, was a real wakeup call, externally and I think somewhat internally. DAVIES: Yes, side effects in terms of increasing social tensions and, in some cases, violent political activity. What about isolation? Addictive behaviors? OSNOS: There has been a growing body of research that shows - and it's been published in sort of the major scientific and academic journals - that there is a correlation, in some cases, between heavy Facebook use and decreased sense of well-being, a sense of connection. People feel lonely. There is a lot that they're trying to figure out about this, but it's gotten to the point where the data is unimpeachable, and the company has begun to acknowledge it. You heard for the first time early this year that Facebook said, look, we recognize that there are different ways that you can use this, and if you're just using it passively, if you're just sitting there scrolling with glazed eyes, hour after hour, we recognize this is not good for you. And so what they did is they said we're going to try to change it a bit, so that people are more actively engaged, so that they're, you know, talking to friends, actively communicating with their family members and stuff like that. But that was a key difference from how they used to talk about it, which was that if you didn't agree with the fundamental premise that Facebook was basically good, then it was a kind of heresy, and they wouldn't want to have the conversation. They are now slowly acclimating to the idea that people just don't buy that anymore. DAVIES: Yeah, you talked to Zuckerberg a fair amount about this, you know, pushing of boundaries of privacy to grow. How does he regard this kind of experimentation? OSNOS: Well, he absorbed a central belief early on in his career, and I think it's become just key to understanding why he's made choices that he has and some of the mistakes that he has, which is that he decided early on that he was very often going to be criticized. He said, look, this is just a fact of what we do. As he said, look, we're not selling dog food here, we're doing something intensely, inherently controversial. It's at the intersection of psychology and technology. And so when people criticize Facebook for being too casual about their privacy, for allowing data to be - to get out into the world, very often what he came to believe was that they would criticize him at the time, but over time, eventually they would accept it, they would get used to it, and they would keep signing on. They would keep growing. And that idea about criticism really hardened and became a central governing principle at Facebook. The sense that they were leaders, they were pioneers, they were forging ahead. They had to push the public beyond its comfort zone when it came to being less private. Because if they didn't do it, then people wouldn't go there. But they were convinced that even in the cases of controversy, when you had civil libertarians or regulators or politicians or ordinary members of the public complaining about Facebook, that that was simply a sign that they were being bold, and that idea continued, really, until the present day. DAVIES: Is it true that in 2010, he said privacy is no longer a social norm? OSNOS: He did. And it caused a big uproar at the time. He said, look; this is a generational difference. We don't feel the same way about privacy that our parents and grandparents did. And people said that's wild. That's not right. Privacy is built into the very nature of the United States. It's really embedded in the Bill of Rights. And his belief was that it was, as it was often described, an antique, and that we needed to push people further. There was - in the early days of Facebook, there was a theme, a phrase that was bandied about called radical transparency, the idea that you had to be aggressively transparent in order to be modern. The sense was, as one person had put it, you know, that in the future, because of Facebook and other things like it that were exploding the boundaries of privacy, that extramarital affairs would become impossible. People couldn't hide things like that. They could no longer hide their lives outside of work from their lives in work. And they believed that to be a virtue, this sense that there would be this fusion, this union of our private selves and our public selves. But that put them at odds with the public. And the key fact I think was that over and over again, Mark Zuckerberg believed that being at odds with the public was not a sign you were doing something wrong; it was a sign that you were doing something innovative. And their mantra, their motto of course became move fast and break things. And that motto really captured the way that they see the world. DAVIES: Yeah, almost, like, redefining what it is to be human. OSNOS: Yeah, they believed that this tool, Facebook, had that kind of power. And they came to being at a time in Silicon Valley where you had this almost messianic sense of ambition - this belief that you weren't just building computer applications. You were actually building tools that were fundamentally reshaping society. And they embraced that wholeheartedly. DAVIES: There was a recent data breach at Facebook where 50 million users' information was taken by somebody. How serious a problem is this? OSNOS: This is a serious one. This is the largest security breach in Facebook's history. And what was unusual about this and what sets it apart from other cases, like Cambridge Analytica, was that this was outright theft. This was a case of hackers or hacker - we still don't know who it was - finding essentially an under-protected door and walking through it and taking control of at least 50 million Facebook user accounts. Facebook also, to be safe, took another 40 million users and kicked them off, forced them to log back in. So it may be as many as 90 million or more that were affected by this. And in this case, the hackers were able to get total control of the accounts. So they were able to get control of your privacy settings. They could go into your messages. They could post things on your behalf. At this point, Facebook says they haven't found any evidence of these hackers doing that. So that only heightens the mystery. They don't know why they did it. They don't know if this was a foreign government or if this were individuals - if this was a criminal act. But what's interesting about this particular case and why it really leaps out to people who study the history of Facebook is that a few years ago, Facebook might not have gone public with this as fast as they did. They would have - they would have probably investigated it more internally. But under the new rules that have been imposed by the European Union, they were required to announce this very fast. And as a result, they had to talk about this breach really before they know very much about it. So it's raised as many questions as it's answered at this point. GROSS: We're listening to the interview FRESH AIR's Dave Davies recorded with Evan Osnos, a staff writer for The New Yorker. Osnos' article about Facebook is titled \"Ghost In The Machine: Can Mark Zuckerberg Fix Facebook Before It Breaks Democracy? \" We'll hear more of the interview after a break. I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF MOLE'S \"STONES\")GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to the interview FRESH AIR's Dave Davies recorded with Evan Osnos about his recent New Yorker article, \"Ghost In The Machine: Can Mark Zuckerberg Fix Facebook Before It Breaks Democracy? \" Osnos profiles Zuckerberg and writes about how Zuckerberg's relentless drive to expand Facebook's reach has jeopardized the privacy of its users and made it vulnerable to political manipulation. DAVIES: There's been lots of discussion about Facebook being used to exacerbate social and political tensions in the United States. There are also serious questions about it being a catalyst for political violence in other parts of the world - Myanmar, for example. What are we seeing? OSNOS: Once Facebook in effect saturated the Western world, it began to move more aggressively into developing countries. And as it did - as it moved into places like India and Sri Lanka and Myanmar, it really became a powerful new ingredient and a dangerous new ingredient, in some cases, into these longstanding ethnic and religious rivalries. In Myanmar, for instance, when Facebook arrived, it really became in effect the Internet. There was only about 1 percent of the population that was online before Facebook was there. And then, over the course of the next few years, it grew rapidly. And people who were interested in fomenting this kind of ethnic hatred between Buddhists and Muslims figured out how to use Facebook to great effect. And they would spread hoaxes or rumors. And in - there were cases where a rumor or a hoax that was spread on Facebook directly contributed to the outbreak of a riot. And what you heard over that period was that people in Myanmar began to talk to the company, warned the company that this was a problem. As early as 2013, people began visiting Facebook's headquarters in Menlo Park, giving slide presentations, talking about the problem of ethnic violence being trafficked on Facebook. And the company would listen. And in some cases, they would give a hearing to that view. Over the years, they continued to visit activists and technology entrepreneurs from Myanmar. But they didn't see any fundamental change. And they were - every time they went, they would they would hear very often the same message, which was, we're going to hire dozens of Burmese language speakers to be able to police this kind of thing more effectively. But still fundamentally it didn't change. And eventually it became so pronounced. The role of Facebook as a catalyst in this violence became so serious that earlier this year, the U. N. investigator in charge of probing the persecution of the Rohingya Muslim minority in Myanmar described the role of Facebook as, in her words, a beast. She said it has become something that it was never intended. And it is actively contributing to this - what the U. N. now considers a genocide. DAVIES: Did you talk to Mark Zuckerberg about this? OSNOS: I did. And I asked him about it. And at first he was frankly a little glib about it. He said, look; this is a problem that is similar in a lot of places, and it's one that we're dealing with. I'm paraphrasing there. And I pushed him on it. I said, look; I talked to people in Myanmar just yesterday in fact, and they're baffled about why a company as big and as rich and as innovative as Facebook has been unable to deal with this problem. And what he said was, we take this seriously. We really do understand this is a problem, but it's not something in which you can just snap your fingers and solve it overnight. It takes a process. You have to build out these systems in order to allow artificial intelligence to detect hate speech and then hire the kinds of people who can solve the problem. He said that they're going to have over a hundred people - Burmese speakers who are going to be policing the Facebook in Myanmar. But I think it speaks to a broader dynamic at work here, which is that for a long time, as one of Mark Zuckerberg's friends said to me, when there were complaints about the company, he either thought that these were just Luddites. These were people who were slow to embrace technology or in other cases, they were exaggerating or overstating the role that Facebook was playing. He looked at a place like Myanmar and said, well, they were probably going to be fighting anyway. I'm putting words in his mouth there. Those are not words he said to me. But I think over time, he has come to this realization - and by his own description, a belated realization - that Facebook is not just a tool on the table. It is not just a new implement. It is in fact a fundamental and very - by its own nature, it has to be responsible for the forces that it unleashes. But then building the systems to try to get control of this is hard, and they have moved much more slowly than they should have, in some cases by their own admission. DAVIES: Let's talk about the 2016 elections. Did Facebook when the election was approaching see this as a big revenue opportunity? OSNOS: They did. They saw this as a important moment. They - Sheryl Sandberg in a call with investors and analysts compared it to the World Cup or the Super Bowl, which was. . . DAVIES: She's the chief operating officer, yeah. OSNOS: She is, yeah. She's sort of arguably the second most powerful person at Facebook. And what she said was this was going to be a major opportunity for them to sell ads - political ads. Projections at the time were that as candidates and political organizations became more aware of the importance of the Internet, that they were going to shift a lot of their spending from television into the Internet and that you were going to see a nine or tenfold increase in how much spending was going to be available. And Facebook wanted to be a big recipient of that. DAVIES: Right. It had gotten a special exemption to prevent ads which didn't disclose who paid for them, right? So it was kind of a wide-open opportunity. OSNOS: Yeah. Facebook had used its lobbying power. It had argued to the Federal Election Commission that it should be exempted from rules that require television advertising to be identified by the source of the funding - you know, that point at the end where. . . DAVIES: Right. OSNOS: . . . They always say who paid for the ad. They said, we shouldn't have to follow those rules because we're a new technology. And in their filings, they said, you don't want to stifle the growth of new innovation. But as a result, that meant that it was in a sense a very dark terrain, that things that were being posted on Facebook that were ads around politics were in many cases of mysterious origin. It was very hard to know who was posting them and why. DAVIES: And Facebook offered to embed a Facebook employee with both the Clinton and Trump campaigns to help the campaigns use the platform effectively. How did they respond? OSNOS: Well, the Clinton campaign rejected the offer. They thought they had more or less enough of their own technical capability to do it. But the Trump campaign embraced it eagerly. They were a much smaller, almost sort of shoestring operation. They had very little of the seasoned political expertise that was rallying around other presidential candidates. And so Facebook moved employees into the Trump campaign headquarters. And they helped them craft their messages. They helped them figure out how to reach the largest possible audience, how to test different messages - many, many messages a day to figure out just what - small differences, changing the background color or changing the text or the font - how that would impact the number of people that would click on it and ultimately might give money and support the candidate. So later, in the end after Donald Trump won the election, the senior campaign strategists were very clear. As one of them, Theresa Hong, said to an interviewer, without Facebook, we would not have won. They played an absolutely essential role in the process. DAVIES: How did the Trump campaign itself use the platform to affect things like turnout? OSNOS: Well, one of the things they did was the Trump campaign bought an ad campaign on Facebook that was designed to suppress turnout among constituencies that they expected to be important to the Democrats, including African-Americans and young liberals and white women. And by targeting that population using these incredibly powerful tools of persuasion that Facebook has, which have been engineered to optimize, to get people to respond - in the view of the Trump campaign, that was an important piece of their success. And they've talked about it ever since. DAVIES: Evan Osnos is a staff writer for The New Yorker. We'll talk some more after a short break. This is FRESH AIR. (SOUNDBITE OF DANILO PEREZ'S \"THE SAGA OF RITA JOE\")DAVIES: We're speaking with Evan Osnos. He's a staff writer for The New Yorker. His piece about Mark Zuckerberg and Facebook, \"Ghost In The Machine,\" is in a recent issue of the magazine. So when word began to emerge about the spread of false information on Facebook, some of it by Russian actors - we now know about the Internet Research Agency in Russia - how did Zuckerberg respond to this? OSNOS: Initially he rejected it. He said it just seems, as he put it, pretty crazy that the presence of fake news might have affected the outcome of the 2016 election. He said that just a few days after the results were in. Since then initially Facebook was really reluctant to embrace this idea that they played a meaningful role in the election. Mark Warner, the senator from Virginia who's the ranking Democrat on the Senate Intelligence Committee, contacted Facebook shortly after the election and said he really wanted to talk about the role of Russian interference on Facebook. And as he put it to me, they were completely dismissive. They just didn't believe that they had a serious role to play here. Over time, they have come to understand that that's simply not the case. Initially they'd estimated that about fewer than 10 million Facebook users might have been affected by Russian disinformation, and they later had to revise that in preparation for testimony in Congress. And they said, actually, as many as maybe 150 million Facebook users were affected by Russian disinformation. And what's remarkable about that is how efficient it was, actually, as a conduit for disinformation because the Russian Internet Research Agency, which was reporting to the Kremlin, had fewer than a hundred members of its staff on this project, and yet they were able to reach a size, 150 million Facebook users, that is extraordinary. And it was - I think to this day Facebook is struggling with that fundamental paradox, which is that on the one hand their business and their success depends on their ability to tout their powers of persuasion. They are telling advertisers, we can encourage users to listen to you, to believe in you and to act on what you're telling them. And yet at the same time, they're trying to say that they have not had this dispositive effect on our politics. And that is a contradiction. DAVIES: Right. And then there was the Cambridge Analytica scandal in which, you know, it emerged that a firm working for the Trump campaign had acquired the personal data of, what - 87 million Facebook users? OSNOS: That's right. DAVIES: So the company was in big trouble. Zuckerberg went before Congress, carefully prepped, of course. How did he do? OSNOS: Well, there was a lot riding on that appearance. You know, in many ways it kind of felt like a trial. Here he was on behalf of the company, going in front of Congress, and there was growing calls for regulation. And he, in some ways, vastly exceeded expectations, and that was because - largely because Congress showed itself to be really extraordinarily unprepared to deal with the complexity of Facebook. They just simply didn't ask the kinds of questions that would have really gotten to the heart of Facebook's operations and how it makes choices. So much so that at one point, Orrin Hatch, senator from Utah, said to Mark Zuckerberg, if you don't charge customers then how do you make any money? And Zuckerberg kind of gave a little smile and said, Senator, we run ads. It was such an obvious fact to anybody who's paid attention to technology that it really, I think, underscored the mismatch between the scale and investment and sophistication of these companies and Congress's inability to come up with the laws and the rules that can respond to them in real time. DAVIES: Maybe you could just explain that a bit. I mean, Facebook makes a fortune by digital ads. How does it work? OSNOS: Yeah. In some ways, Facebook is actually a little bit like a newspaper in the sense that the way that it pays for itself is by running ads alongside the content that people post and look for on there. So on any given moment when you go on Facebook, you will find these highly targeted ads. These are things that are chosen just for you based on your browsing behavior around the Internet, based on the posts that you've clicked on, the things that you look for. They choose ads that you are much more likely to click on than you would if they were just sending the same ad to everybody else. And that formula, that ability to micro-target, as it's known, ads to specific users has been this extraordinary geyser of business success. They just stumbled on something that was able to generate returns for companies that kept them coming back over and over again and advertising on Facebook. DAVIES: And the company is taking steps - certainly says taking steps - to monitor the content that political players are using the platform for and that, you know, is arguably critical as we approach the midterms. It's also pretty tricky, right? I mean, how do you distinguish spin from fakery or, you know, dangerous content from distasteful? How's the company doing this? OSNOS: It's become this consuming effort. You know, Facebook over the last year, as the controversy has grown, they've undertaken one initiative after another. So when it comes to political election advertising, for instance, what they said was, OK, even though for years we argued that we shouldn't have to disclose the funding sources for political ads, now we are going to not only do that, but we're going to go farther than TV does. We're going to let users be able to click on an ad and then know not only who paid for it, but what other ads do those people pay for and who are they targeting? They're also saying, we're going to do more to defend against the kind of disinformation campaigns. What they call coordinated inauthentic behavior. Essentially misinformation that's distributed to try to shape elections, not only in the United States, but in other countries. And they've had to - in many ways, Dave, it's almost like they've begun to take on some of the qualities of a government. You know, they've had to hire people who actually worked in the U. S. government on things like misinformation in order to try to ferret out efforts by Russian officials - or in one case, there was an Iranian campaign - to try to spread misinformation. But what we don't know and really won't know until after the midterm elections in the U. S. and the elections that come to follow are whether or not that's working. And I think what's interesting is Facebook has become much more public these days in terms of talking about when it finds examples of disinformation. It'll announce it. It will say, we took down a group of Russian impostors who were seeking to affect American voting behavior, for instance. But that's either a sign that they're winning the battle, or it's a sign that the battle has grown so much that they're going to continue to face this. And what I'm struck by is how much the integrity and the credibility of elections now rests on the shoulders of individual employees inside a private company in California. That's a very unusual situation for our democracy to be in. DAVIES: 'Cause they can actually make distinctions about what the public sees and what it doesn't. OSNOS: Yes, yeah. I mean, they have to make very subtle choices. Take, for example, the, you know, misinformation. What is the definition of misinformation? When is somebody being wrong by accident, and when is somebody being wrong on purpose? When are they trying to deceive large numbers of the public? Those kinds of very subtle things which are usually the province of the Supreme Court or of lawmakers are now being handled in conference rooms at Facebook, and that's very complicated. DAVIES: Evan Osnos is a staff writer for The New Yorker. His piece \"The Ghost In The Machine\" about Mark Zuckerberg and Facebook appeared in a recent issue. We'll talk some more after a short break. This is FRESH AIR. (SOUNDBITE OF THE ADAM PRICE GROUP'S \"STORYVILLE\")DAVIES: This is FRESH AIR, and we're speaking with Evan Osnos. He's a staff writer for The New Yorker. His story \"Ghost In The Machine\" about Mark Zuckerberg and Facebook, appears in a recent issue of the magazine. They recently took down posts from Alex Jones of Infowars. You want to tell us that - tell us about that? OSNOS: Yeah. This was really an important case study in how Facebook's going to deal with its most complicated problem in some sense, which is content. What do you simply do with the fact that people are posting a billion items of content to Facebook every day? That's the actual number. And what they've tried to do is to say, OK, we're going to punish hate speech; we're going to prevent hate speech from being on here. But for things that are less than hate speech - if it's just misinformation or things that appear to be wrong by accident, well, then we're not going to ban that person from Facebook. We're going to try to use other tools. We may make their posts less visible. We might share them less. But the case of Infowars, which, as we all know, is a conspiracy website led by Alex Jones - and for years, it has promoted in particular the falsehood - it's a false conspiracy theory that the massacre at Sandy Hook Elementary School was staged; it was a hoax, and it was designed to try to advance an anti-gun agenda. That's the theory. And for years, people have complained to Facebook about it. They said, this really has risen to the level of harassment of the parents who are - whose children were killed at Sandy Hook. And this summer, people started to criticize the company more publicly and said, look; you need to remove Infowars. This is no longer normal content. They've disqualified themselves from being a part of civil discourse here by directing all of this harassment at the parents. And then there was - the parents of one of the children at Sandy Hook wrote an open letter, quite candid and very tough, directed at Mark Zuckerberg and said that they have been driven into hiding because of their, as they put it, inexplicable battle with Facebook to try to get this kind of material taken down so that they don't have to deal with trolls and people who are, you know, discovering their address and then threatening them online. And finally this summer, Apple took down Infowars' podcasts. And then very soon thereafter, Facebook and other companies followed suit. And I talked to Mark Zuckerberg about it. I said, why did you wait this - why did you wait so long? Why did you wait so long to take down this thing that people had so clearly complained about? And he said, well, we don't want to punish people who are just wrong. We don't want to ban them from Facebook. What we're trying to do is figure out how to shape this thing. And he acknowledged in a fact that because Apple had moved on this, that - he said, at that point, we realized we had to make a decision. We had to get rid of this, and so we did it. But from my perspective, what was interesting about this was that this is the very beginning of an issue for Facebook. This is not the end. I mean, this is just the front edge of an unbelievably complex problem, which is, what are the bounds of free speech? What do we actually want to be able to have, and what do we consider to be out of bounds? What is, in effect, shouting fire in a crowded theater, and what is legitimate provocative, unsavory speech? And these are some of the hardest problems that we face, and they're now in the hands of - let's face it. They're in the hands of the engineers, the people who created this incredibly powerful application. DAVIES: Yeah, you know, there's lots of First Amendment law that says the government can't, you know, distinguish types of speech and prohibit it. Is it clear that a private company like Facebook can when it's this big? OSNOS: Well, actually, Facebook, as a private company, can do whatever it wants on speech. If they decided tomorrow that you couldn't talk about golden retrievers on Facebook, they could put that rule in place. And I think for some reason, that - you know, we find ourselves really torn. Even if you're not a fan of Infowars - and God knows I'm not - it has to make a person uneasy to know that there is now a company which is capable of deciding not only what kind of information it's going to suppress but also which kind of information it's going to promote. And on any given day, there are people who are going to be offended by those choices. But the tools by which Facebook is held accountable are not the tools that we use in politics. It's not like you vote the bums out. It's not like people are appointed to Facebook's board as if they were Supreme Court justices. This is a case in which a private company is making profound choices about the contours and the boundaries of political expression. And we don't have obvious tools with which to regulate them. DAVIES: You know, when you look at the arc of the story, I mean, this company founded by Mark Zuckerberg has this astonishing growth, is deeply committed to growth and, in doing so, you know, compromises privacy and ends up, you know, sharing data it shouldn't about its users and gets into some trouble. And a question arises about whether the company has the ability - has the capacity for self-reflection, whether it can take, you know, adverse information and re-examine its assumptions and practices. And in many respects, this really comes down to Zuckerberg. What did you find about that? OSNOS: I found that he is insulated to I think an unhealthy degree from this kind of criticism. And if he was sitting with me right now, I would say this directly. The reality is he's built the company in his own image. He's had the luxury of sculpting an organization to his like - I mean, quite literally, the blue color that Facebook has as its signature blue is chosen because he is red-green colorblind, and he prefers to look at the color blue. He can see it very distinctly. So in every way, both physical and spiritual, this company reflects his sensibilities. But in order to be able to continue to grow and evolve and respond to the problems that it's encountered, he needs people sitting in the room with him who will tell him, Mark, I think you're not seeing this the right way; you're not seeing this clearly; you're wrong. And I was struck that in our interviews, I got the sense from him that he knows that on some level. He's tried over the years to make these choices, to get outside what he described as the bubble. He's got five people who report directly to him. And they are all people who he has in effect chosen and installed in those positions. And there is very few - there are very few people at Facebook who are willing to stick their neck out and say, I fundamentally disagree; we need to do things differently. DAVIES: You know, at the end of this piece, you write that some people think of Mark Zuckerberg as an automaton with little regard for the human dimensions of his work. And you say, not exactly. The truth is something else. What's the truth? OSNOS: The truth is that he is at peace with what he has done, with the choices that he has made. I came to really understand that Mark Zuckerberg, in his own conception of his place in history, believes that no change happens painlessly and that change is difficult. And in many ways, it's like his inspiration - Augustus Caesar. He believes that he's made tradeoffs, that he has - in order to grow, he had to give up perfection. If he wanted to be vastly influential, then he couldn't always be quite as safe as people wanted him to be. And in his mind and in the mind of the people around him, they are vindicated by their sheer scale and success. And for that reason, it's very hard for them to accept that the public is howling, in many cases, for real change because they believe if we had given in to the critics at every step along the way and made changes, then we wouldn't be as big as we are today. DAVIES: Evan Osnos, thanks so much for speaking with us. OSNOS: Thanks very much for having me, Dave. GROSS: Evan Osnos is a staff writer for The New Yorker. His article about Facebook is titled \"Ghost In The Machine: Can Mark Zuckerberg Fix Facebook Before It Breaks Democracy? \" If you'd like to catch up on FRESH AIR interviews you missed, like our interview with Washington Post national security correspondent Greg Miller, author of the new book \"The Apprentice: Trump, Russia And The Subversion Of American Democracy,\" check out our podcast. You'll find lots of FRESH AIR interviews, including my recent interview with pianist, composer and singer Jon Batiste, who leads the house band on \"Late Night With Stephen Colbert\" and was at the piano for our interview. (SOUNDBITE OF THE ROB DIXON TRIO'S \"SAN LEANDRO\")GROSS: FRESH AIR's executive producer is Danny Miller. Our interviews and reviews are produced and edited by Amy Salit, Phyllis Myers, Sam Briger, Lauren Krenzel, Heidi Saman, Therese Madden, Mooj Zadie, Thea Chaloner and Seth Kelley. I'm Terry Gross. (SOUNDBITE OF THE ROB DIXON TRIO'S \"SAN LEANDRO\") TERRY GROSS, HOST:  This is FRESH AIR. I'm Terry Gross. Last week, Facebook announced the most serious security breach in the company's history in which an unknown hacker was able to log onto the accounts of at least 50 million Facebook users. But that's just one element of the crisis facing the world's largest social media platform. Facebook was the conduit for a Russian-backed disinformation campaign in the 2016 election that reached tens of millions of its users. And the political consulting firm Cambridge Analytica got access to the personal information of 87 million Facebook users, which they used to target messages for the Trump campaign. Facebook is now under investigation by the FBI, the Securities and Exchange Commission, the Federal Trade Commission and authorities in Europe and Australia. Our guest, New Yorker staff writer Evan Osnos, explores Facebook's history and profiles its 34-year-old founder and chief executive Mark Zuckerberg in a recent issue of the magazine. He says Zuckerberg's relentless drive to expand Facebook's reach has jeopardized the privacy of its users and made it vulnerable to political manipulation. His story is titled, \"Ghosts In The Machine: Can Mark Zuckerberg Fix Facebook Before It Breaks Democracy? \" He spoke with FRESH AIR'S Dave Davies. DAVE DAVIES, BYLINE: Well, Evan Osnos, welcome back to FRESH AIR. You know, we all know Facebook is a very big deal. Give us a sense of its size and reach. EVAN OSNOS: There's really nothing quite like it in the history of American business. It has now 2. 2 billion monthly active users, meaning it's larger than any country in the world. It's really got no natural precedent when you look at the history of enterprise. It's really closer, in terms of scale and reach, to a political ideology or a religious faith. I mean, just in literal terms, it now has as many adherents as Christianity. And that has all been built in the last 14 years, since it was founded in 2004. DAVIES: For this piece, you visited Mark Zuckerberg at his home, at his office. You had numerous meetings. A lot of access from a guy who is pretty careful about, you know, journalists getting connected to him. How did you convince him to share so much time with you? OSNOS: Well, he did it, I should say, reluctantly. You know, this was a long process that started a year ago when I first approached them about the idea of this kind of story. I said, look, we do in The New Yorker these very long, detailed profiles. And I think initially Facebook's view was, we don't need to do this. We're a big, powerful company. And I continued to work on it, basically started interviewing a lot of other people around Facebook, people who worked there in the past, people who work there in the present. And as that accumulated, I continued to say, look; this fundamentally should be about how Mark Zuckerberg sees the world. And over time, they came to, I think, sort of grudgingly - but I'm grateful for it - accept the idea that, to an extraordinary degree, Mark Zuckerberg is Facebook. I mean, that is the reality. He's the chairman. He's the CEO. He owns, he controls 60 percent of the voting shares. So if you're going to understand Facebook in any meaningful way, the conversation really has to start with him and end with him. And for that, I think they ultimately recognized he had to be a part of this project. The company has come up against a growing and really serious decline of public trust, both among politicians and among the general public. And I think they recognize that at the core of that - and I think he recognizes that - there is this profound question mark around what does Mark Zuckerberg believe in? What does he stand for? What does he care about? How does he see his role in the country? What does he see for the role of technology? And this project was my attempt to try to answer some of those questions for myself and for readers. DAVIES: Zuckerberg started Facebook when he was at Harvard. It takes off, and it grows and it grows because they're determined, you know, to get more users, to connect more people, as Zuckerberg likes to say. And you're right that in about 2007, it plateaued at about 50 million users, which a lot of other kind of similar platforms had done. What did the company do to break through that ceiling? OSNOS: Well, they first panicked, really. There was a sense internally that they wondered whether they'd sort of hit the wall and whether this thing that had grown so fast was now over. And what they did was they discovered something important, which is that in order to punch through that wall, they had to come up with a whole range of new ways of accessing new populations, people who wouldn't otherwise have been on Facebook. The first thing they did was they discovered they simply had to translate the site into other languages, make it available to other countries around the world and allow people to be able to post in Spanish and so on. And by doing that, they crossed this barrier, this moat, which had prevented other social media sites from growing. But it did something even more important, which was that it established the sacred importance - really, sacred is the word - of growth internally. They created a team called the Growth Team, which became - as a former Facebook executive said to me, it was really the cool crowd. This was the team everyone wanted to be on, was Growth. It was about really sanctifying the idea of growth as an end in itself, that if Facebook stopped growing then its whole reason for being would cease to exist. And that idea ended up becoming the dominant fact of the next decade, from 2007 until today, when Facebook became extraordinarily focused on whatever it could do to continue growing year after year. DAVIES: And one of the things they did was that they made it a platform for outside developers so that - maybe you want to explain what this means. So that other software developers could hitch into Facebook and use it. OSNOS: They made a big choice, which was that, instead of saying we're just going to be our own site, we're actually going to try to become a platform, like, almost like an operating system, the way that Windows used to be the, you know, the way that everybody went onto their personal computer, and then you would build applications on top of that. And that decision to open themselves up as a platform meant that they were then sharing data to a much larger group of developers, of programmers, than they otherwise would. And this turned out to be a fateful decision because if you fast-forward many years later to, you know, what we all now know as the Cambridge Analytica scandal, that was - it was directly a result of this decision to open themselves up as a platform. Because what they'd done was they'd allowed an academic researcher to use some of the data on the platform to build a personality quiz. But then he sold that data to the political consultancy known as Cambridge Analytica. And that sort of back door, the way in which that data went out the door, has created much of the crisis that now engulfs the company today. DAVIES: And you write that an executive within Facebook, Sandy Parakilas, was put in charge of looking into what these outside, you know, app developers were doing with the data that they got from the Facebook users who, you know, plugged into their games. What did he find? OSNOS: What he found unnerved him. Sandy Parakilas had joined Facebook in 2011 and was one of the people responsible for going out and figuring out whether the data that they were giving to developers was being misused in any way. You know, were people violating privacy? Were they taking more data than they were supposed to? And what he found was that they were. In some cases, programmers, for instance, were siphoning off people's pictures and their private messages. In other cases, he found a developer that was building essentially shadow profiles, profiles for people who've never given their consent, including for children. And it was - this was a case in which there was just this feeling of it being the Wild West. This data was out there, and nobody was paying attention. And he raised alarms internally. As he tells the story, what he did was he said, look, we need to do a major audit to go out and figure out where is our data, who has it and how are they using it. And as he says he was told, that's not going to happen because if you do it, you may not want to know what you're going to find. Meaning that they may have already lost control of so much of that data that they didn't really want to discover the full reach. And Sandy Parakilas left, but in many ways his warnings turned out to be prophetic because exactly the kind of undisciplined use of data which he had warned about and tried to raise greater alarms about internally turned out to be the origins of the Cambridge Analytica scandal, which became so consuming for the company this year. DAVIES: So a lot of resources within the company devoted to pushing growth, not so much into doing it responsibly. OSNOS: Yes. Exactly. As he said, you know, I needed people, I needed engineers to help me try to police where this data was going, but all of the resources were going to growth. He said at some point, he told me, that the growth team had engineers coming out of their ears. They had everything they wanted. Everything was devoted to growing, and it was not devoted to making sure that what they were doing at the time was safe. DAVIES: What are some of the other things the company did to keep growing that kind of pushed the boundaries of privacy? OSNOS: Well, they began to build in lots of ingenious details into the design of the site, things like autoplay videos. This is something as simple as changing the nature of Facebook so that when you scroll down your page, that the videos would begin to play without you having to click on them. What that was was essentially taking advantage of some of our own psychological wiring. That means that just eliminating that small obstacle makes you much more likely to stay on the site, to watch that ad and ultimately to consume whatever advertising is around it. And they did other things. They, for instance, you know, you remember in the very beginning Facebook used to have pages - single pages - you'd have to click onto the next page. They got rid of that, so it's just a continuous scroll. And all of these little tiny details were, in a way, making Facebook into a new generation of behavioral experts. They figured out how do you tweak people's vanities and their passions and their susceptibilities and their desires in order to keep them on the site. The most important thing Facebook could do - and this is how they measured it - was make sure that people were signing on and staying on. Whenever you joined the company, in your orientation, you were taught about something very important, a metric known as L6 of 7, which means the number of people who logged in six of the last seven days. And whatever you could do to try to raise that L6 of 7, that was the priority. But as, you know, somebody put it to me, the problem with that is that eventually you exhaust the positive ways of boosting that engagement, and eventually you start to look at what this person described as the dark patterns, the ways that you can use anxiety or vanity to try to get people to sign on. One of the things they discovered, for instance, was that if you send somebody an email that says that a Facebook friend has uploaded a picture of them to Facebook, that people are almost incapable of resisting the temptation to look. And that sort of tweak, that just minor behavioral nudge, turned out to be really hugely important to the growth of Facebook. DAVIES: Evan Osnos is a staff writer for The New Yorker. We'll talk some more after a short break. This is FRESH AIR. (SOUNDBITE OF MICHAEL BELLAR AND THE AS-IS ENSEMBLE'S \"HOT BOX MAGIC\") DAVIES: This is FRESH AIR, and we're speaking with New Yorker staff writer Evan Osnos. He has a piece in the magazine called \"Ghost In The Machine\" about Mark Zuckerberg and Facebook and its current controversies. It's in a recent issue. Now Facebook is in a lot of trouble these days because of data breaches - its role in elections. But you're right that some former Facebook executives are voicing doubts about the company's role for other things - exacerbating isolation, outrage, addictive behaviors. You want to give us an example? OSNOS: This is a big change for Facebook. Traditionally, its former executives have been silent. They leave the company and very often don't say much about it. Starting last year, Sean Parker who was Facebook's first president - he's a sort of well-known Silicon Valley figure - gave an interview in which he said that, as he put it, he's become a conscientious objector to social media. He said God knows what it's doing to our children's brains. And then a few days later, there was another very prominent former Facebook executive named Chamath Palihapitiya, who had been the head of growth, a vice president of user growth, which was an absolutely central position at the company for a number of years. And he came out and said, you know, in the back of our minds we all had a sense that something might be going wrong, that our product, the thing that we were building - I'm paraphrasing here - was, as he put it, contributing to the breakdown of social discourse, the breakdown of society. And he said he would never let his children use this kind of product. I think because the public face of the company has been so on message about how they contribute, in their minds, to doing good in the world, to have some very senior former executives come out and talk about what the public has come to believe, which is that there are these very serious side effects to the company's growth, was a real wakeup call, externally and I think somewhat internally. DAVIES: Yes, side effects in terms of increasing social tensions and, in some cases, violent political activity. What about isolation? Addictive behaviors? OSNOS: There has been a growing body of research that shows - and it's been published in sort of the major scientific and academic journals - that there is a correlation, in some cases, between heavy Facebook use and decreased sense of well-being, a sense of connection. People feel lonely. There is a lot that they're trying to figure out about this, but it's gotten to the point where the data is unimpeachable, and the company has begun to acknowledge it. You heard for the first time early this year that Facebook said, look, we recognize that there are different ways that you can use this, and if you're just using it passively, if you're just sitting there scrolling with glazed eyes, hour after hour, we recognize this is not good for you. And so what they did is they said we're going to try to change it a bit, so that people are more actively engaged, so that they're, you know, talking to friends, actively communicating with their family members and stuff like that. But that was a key difference from how they used to talk about it, which was that if you didn't agree with the fundamental premise that Facebook was basically good, then it was a kind of heresy, and they wouldn't want to have the conversation. They are now slowly acclimating to the idea that people just don't buy that anymore. DAVIES: Yeah, you talked to Zuckerberg a fair amount about this, you know, pushing of boundaries of privacy to grow. How does he regard this kind of experimentation? OSNOS: Well, he absorbed a central belief early on in his career, and I think it's become just key to understanding why he's made choices that he has and some of the mistakes that he has, which is that he decided early on that he was very often going to be criticized. He said, look, this is just a fact of what we do. As he said, look, we're not selling dog food here, we're doing something intensely, inherently controversial. It's at the intersection of psychology and technology. And so when people criticize Facebook for being too casual about their privacy, for allowing data to be - to get out into the world, very often what he came to believe was that they would criticize him at the time, but over time, eventually they would accept it, they would get used to it, and they would keep signing on. They would keep growing. And that idea about criticism really hardened and became a central governing principle at Facebook. The sense that they were leaders, they were pioneers, they were forging ahead. They had to push the public beyond its comfort zone when it came to being less private. Because if they didn't do it, then people wouldn't go there. But they were convinced that even in the cases of controversy, when you had civil libertarians or regulators or politicians or ordinary members of the public complaining about Facebook, that that was simply a sign that they were being bold, and that idea continued, really, until the present day. DAVIES: Is it true that in 2010, he said privacy is no longer a social norm? OSNOS: He did. And it caused a big uproar at the time. He said, look; this is a generational difference. We don't feel the same way about privacy that our parents and grandparents did. And people said that's wild. That's not right. Privacy is built into the very nature of the United States. It's really embedded in the Bill of Rights. And his belief was that it was, as it was often described, an antique, and that we needed to push people further. There was - in the early days of Facebook, there was a theme, a phrase that was bandied about called radical transparency, the idea that you had to be aggressively transparent in order to be modern. The sense was, as one person had put it, you know, that in the future, because of Facebook and other things like it that were exploding the boundaries of privacy, that extramarital affairs would become impossible. People couldn't hide things like that. They could no longer hide their lives outside of work from their lives in work. And they believed that to be a virtue, this sense that there would be this fusion, this union of our private selves and our public selves. But that put them at odds with the public. And the key fact I think was that over and over again, Mark Zuckerberg believed that being at odds with the public was not a sign you were doing something wrong; it was a sign that you were doing something innovative. And their mantra, their motto of course became move fast and break things. And that motto really captured the way that they see the world. DAVIES: Yeah, almost, like, redefining what it is to be human. OSNOS: Yeah, they believed that this tool, Facebook, had that kind of power. And they came to being at a time in Silicon Valley where you had this almost messianic sense of ambition - this belief that you weren't just building computer applications. You were actually building tools that were fundamentally reshaping society. And they embraced that wholeheartedly. DAVIES: There was a recent data breach at Facebook where 50 million users' information was taken by somebody. How serious a problem is this? OSNOS: This is a serious one. This is the largest security breach in Facebook's history. And what was unusual about this and what sets it apart from other cases, like Cambridge Analytica, was that this was outright theft. This was a case of hackers or hacker - we still don't know who it was - finding essentially an under-protected door and walking through it and taking control of at least 50 million Facebook user accounts. Facebook also, to be safe, took another 40 million users and kicked them off, forced them to log back in. So it may be as many as 90 million or more that were affected by this. And in this case, the hackers were able to get total control of the accounts. So they were able to get control of your privacy settings. They could go into your messages. They could post things on your behalf. At this point, Facebook says they haven't found any evidence of these hackers doing that. So that only heightens the mystery. They don't know why they did it. They don't know if this was a foreign government or if this were individuals - if this was a criminal act. But what's interesting about this particular case and why it really leaps out to people who study the history of Facebook is that a few years ago, Facebook might not have gone public with this as fast as they did. They would have - they would have probably investigated it more internally. But under the new rules that have been imposed by the European Union, they were required to announce this very fast. And as a result, they had to talk about this breach really before they know very much about it. So it's raised as many questions as it's answered at this point. GROSS: We're listening to the interview FRESH AIR's Dave Davies recorded with Evan Osnos, a staff writer for The New Yorker. Osnos' article about Facebook is titled \"Ghost In The Machine: Can Mark Zuckerberg Fix Facebook Before It Breaks Democracy? \" We'll hear more of the interview after a break. I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF MOLE'S \"STONES\") GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to the interview FRESH AIR's Dave Davies recorded with Evan Osnos about his recent New Yorker article, \"Ghost In The Machine: Can Mark Zuckerberg Fix Facebook Before It Breaks Democracy? \" Osnos profiles Zuckerberg and writes about how Zuckerberg's relentless drive to expand Facebook's reach has jeopardized the privacy of its users and made it vulnerable to political manipulation. DAVIES: There's been lots of discussion about Facebook being used to exacerbate social and political tensions in the United States. There are also serious questions about it being a catalyst for political violence in other parts of the world - Myanmar, for example. What are we seeing? OSNOS: Once Facebook in effect saturated the Western world, it began to move more aggressively into developing countries. And as it did - as it moved into places like India and Sri Lanka and Myanmar, it really became a powerful new ingredient and a dangerous new ingredient, in some cases, into these longstanding ethnic and religious rivalries. In Myanmar, for instance, when Facebook arrived, it really became in effect the Internet. There was only about 1 percent of the population that was online before Facebook was there. And then, over the course of the next few years, it grew rapidly. And people who were interested in fomenting this kind of ethnic hatred between Buddhists and Muslims figured out how to use Facebook to great effect. And they would spread hoaxes or rumors. And in - there were cases where a rumor or a hoax that was spread on Facebook directly contributed to the outbreak of a riot. And what you heard over that period was that people in Myanmar began to talk to the company, warned the company that this was a problem. As early as 2013, people began visiting Facebook's headquarters in Menlo Park, giving slide presentations, talking about the problem of ethnic violence being trafficked on Facebook. And the company would listen. And in some cases, they would give a hearing to that view. Over the years, they continued to visit activists and technology entrepreneurs from Myanmar. But they didn't see any fundamental change. And they were - every time they went, they would they would hear very often the same message, which was, we're going to hire dozens of Burmese language speakers to be able to police this kind of thing more effectively. But still fundamentally it didn't change. And eventually it became so pronounced. The role of Facebook as a catalyst in this violence became so serious that earlier this year, the U. N. investigator in charge of probing the persecution of the Rohingya Muslim minority in Myanmar described the role of Facebook as, in her words, a beast. She said it has become something that it was never intended. And it is actively contributing to this - what the U. N. now considers a genocide. DAVIES: Did you talk to Mark Zuckerberg about this? OSNOS: I did. And I asked him about it. And at first he was frankly a little glib about it. He said, look; this is a problem that is similar in a lot of places, and it's one that we're dealing with. I'm paraphrasing there. And I pushed him on it. I said, look; I talked to people in Myanmar just yesterday in fact, and they're baffled about why a company as big and as rich and as innovative as Facebook has been unable to deal with this problem. And what he said was, we take this seriously. We really do understand this is a problem, but it's not something in which you can just snap your fingers and solve it overnight. It takes a process. You have to build out these systems in order to allow artificial intelligence to detect hate speech and then hire the kinds of people who can solve the problem. He said that they're going to have over a hundred people - Burmese speakers who are going to be policing the Facebook in Myanmar. But I think it speaks to a broader dynamic at work here, which is that for a long time, as one of Mark Zuckerberg's friends said to me, when there were complaints about the company, he either thought that these were just Luddites. These were people who were slow to embrace technology or in other cases, they were exaggerating or overstating the role that Facebook was playing. He looked at a place like Myanmar and said, well, they were probably going to be fighting anyway. I'm putting words in his mouth there. Those are not words he said to me. But I think over time, he has come to this realization - and by his own description, a belated realization - that Facebook is not just a tool on the table. It is not just a new implement. It is in fact a fundamental and very - by its own nature, it has to be responsible for the forces that it unleashes. But then building the systems to try to get control of this is hard, and they have moved much more slowly than they should have, in some cases by their own admission. DAVIES: Let's talk about the 2016 elections. Did Facebook when the election was approaching see this as a big revenue opportunity? OSNOS: They did. They saw this as a important moment. They - Sheryl Sandberg in a call with investors and analysts compared it to the World Cup or the Super Bowl, which was. . . DAVIES: She's the chief operating officer, yeah. OSNOS: She is, yeah. She's sort of arguably the second most powerful person at Facebook. And what she said was this was going to be a major opportunity for them to sell ads - political ads. Projections at the time were that as candidates and political organizations became more aware of the importance of the Internet, that they were going to shift a lot of their spending from television into the Internet and that you were going to see a nine or tenfold increase in how much spending was going to be available. And Facebook wanted to be a big recipient of that. DAVIES: Right. It had gotten a special exemption to prevent ads which didn't disclose who paid for them, right? So it was kind of a wide-open opportunity. OSNOS: Yeah. Facebook had used its lobbying power. It had argued to the Federal Election Commission that it should be exempted from rules that require television advertising to be identified by the source of the funding - you know, that point at the end where. . . DAVIES: Right. OSNOS: . . . They always say who paid for the ad. They said, we shouldn't have to follow those rules because we're a new technology. And in their filings, they said, you don't want to stifle the growth of new innovation. But as a result, that meant that it was in a sense a very dark terrain, that things that were being posted on Facebook that were ads around politics were in many cases of mysterious origin. It was very hard to know who was posting them and why. DAVIES: And Facebook offered to embed a Facebook employee with both the Clinton and Trump campaigns to help the campaigns use the platform effectively. How did they respond? OSNOS: Well, the Clinton campaign rejected the offer. They thought they had more or less enough of their own technical capability to do it. But the Trump campaign embraced it eagerly. They were a much smaller, almost sort of shoestring operation. They had very little of the seasoned political expertise that was rallying around other presidential candidates. And so Facebook moved employees into the Trump campaign headquarters. And they helped them craft their messages. They helped them figure out how to reach the largest possible audience, how to test different messages - many, many messages a day to figure out just what - small differences, changing the background color or changing the text or the font - how that would impact the number of people that would click on it and ultimately might give money and support the candidate. So later, in the end after Donald Trump won the election, the senior campaign strategists were very clear. As one of them, Theresa Hong, said to an interviewer, without Facebook, we would not have won. They played an absolutely essential role in the process. DAVIES: How did the Trump campaign itself use the platform to affect things like turnout? OSNOS: Well, one of the things they did was the Trump campaign bought an ad campaign on Facebook that was designed to suppress turnout among constituencies that they expected to be important to the Democrats, including African-Americans and young liberals and white women. And by targeting that population using these incredibly powerful tools of persuasion that Facebook has, which have been engineered to optimize, to get people to respond - in the view of the Trump campaign, that was an important piece of their success. And they've talked about it ever since. DAVIES: Evan Osnos is a staff writer for The New Yorker. We'll talk some more after a short break. This is FRESH AIR. (SOUNDBITE OF DANILO PEREZ'S \"THE SAGA OF RITA JOE\") DAVIES: We're speaking with Evan Osnos. He's a staff writer for The New Yorker. His piece about Mark Zuckerberg and Facebook, \"Ghost In The Machine,\" is in a recent issue of the magazine. So when word began to emerge about the spread of false information on Facebook, some of it by Russian actors - we now know about the Internet Research Agency in Russia - how did Zuckerberg respond to this? OSNOS: Initially he rejected it. He said it just seems, as he put it, pretty crazy that the presence of fake news might have affected the outcome of the 2016 election. He said that just a few days after the results were in. Since then initially Facebook was really reluctant to embrace this idea that they played a meaningful role in the election. Mark Warner, the senator from Virginia who's the ranking Democrat on the Senate Intelligence Committee, contacted Facebook shortly after the election and said he really wanted to talk about the role of Russian interference on Facebook. And as he put it to me, they were completely dismissive. They just didn't believe that they had a serious role to play here. Over time, they have come to understand that that's simply not the case. Initially they'd estimated that about fewer than 10 million Facebook users might have been affected by Russian disinformation, and they later had to revise that in preparation for testimony in Congress. And they said, actually, as many as maybe 150 million Facebook users were affected by Russian disinformation. And what's remarkable about that is how efficient it was, actually, as a conduit for disinformation because the Russian Internet Research Agency, which was reporting to the Kremlin, had fewer than a hundred members of its staff on this project, and yet they were able to reach a size, 150 million Facebook users, that is extraordinary. And it was - I think to this day Facebook is struggling with that fundamental paradox, which is that on the one hand their business and their success depends on their ability to tout their powers of persuasion. They are telling advertisers, we can encourage users to listen to you, to believe in you and to act on what you're telling them. And yet at the same time, they're trying to say that they have not had this dispositive effect on our politics. And that is a contradiction. DAVIES: Right. And then there was the Cambridge Analytica scandal in which, you know, it emerged that a firm working for the Trump campaign had acquired the personal data of, what - 87 million Facebook users? OSNOS: That's right. DAVIES: So the company was in big trouble. Zuckerberg went before Congress, carefully prepped, of course. How did he do? OSNOS: Well, there was a lot riding on that appearance. You know, in many ways it kind of felt like a trial. Here he was on behalf of the company, going in front of Congress, and there was growing calls for regulation. And he, in some ways, vastly exceeded expectations, and that was because - largely because Congress showed itself to be really extraordinarily unprepared to deal with the complexity of Facebook. They just simply didn't ask the kinds of questions that would have really gotten to the heart of Facebook's operations and how it makes choices. So much so that at one point, Orrin Hatch, senator from Utah, said to Mark Zuckerberg, if you don't charge customers then how do you make any money? And Zuckerberg kind of gave a little smile and said, Senator, we run ads. It was such an obvious fact to anybody who's paid attention to technology that it really, I think, underscored the mismatch between the scale and investment and sophistication of these companies and Congress's inability to come up with the laws and the rules that can respond to them in real time. DAVIES: Maybe you could just explain that a bit. I mean, Facebook makes a fortune by digital ads. How does it work? OSNOS: Yeah. In some ways, Facebook is actually a little bit like a newspaper in the sense that the way that it pays for itself is by running ads alongside the content that people post and look for on there. So on any given moment when you go on Facebook, you will find these highly targeted ads. These are things that are chosen just for you based on your browsing behavior around the Internet, based on the posts that you've clicked on, the things that you look for. They choose ads that you are much more likely to click on than you would if they were just sending the same ad to everybody else. And that formula, that ability to micro-target, as it's known, ads to specific users has been this extraordinary geyser of business success. They just stumbled on something that was able to generate returns for companies that kept them coming back over and over again and advertising on Facebook. DAVIES: And the company is taking steps - certainly says taking steps - to monitor the content that political players are using the platform for and that, you know, is arguably critical as we approach the midterms. It's also pretty tricky, right? I mean, how do you distinguish spin from fakery or, you know, dangerous content from distasteful? How's the company doing this? OSNOS: It's become this consuming effort. You know, Facebook over the last year, as the controversy has grown, they've undertaken one initiative after another. So when it comes to political election advertising, for instance, what they said was, OK, even though for years we argued that we shouldn't have to disclose the funding sources for political ads, now we are going to not only do that, but we're going to go farther than TV does. We're going to let users be able to click on an ad and then know not only who paid for it, but what other ads do those people pay for and who are they targeting? They're also saying, we're going to do more to defend against the kind of disinformation campaigns. What they call coordinated inauthentic behavior. Essentially misinformation that's distributed to try to shape elections, not only in the United States, but in other countries. And they've had to - in many ways, Dave, it's almost like they've begun to take on some of the qualities of a government. You know, they've had to hire people who actually worked in the U. S. government on things like misinformation in order to try to ferret out efforts by Russian officials - or in one case, there was an Iranian campaign - to try to spread misinformation. But what we don't know and really won't know until after the midterm elections in the U. S. and the elections that come to follow are whether or not that's working. And I think what's interesting is Facebook has become much more public these days in terms of talking about when it finds examples of disinformation. It'll announce it. It will say, we took down a group of Russian impostors who were seeking to affect American voting behavior, for instance. But that's either a sign that they're winning the battle, or it's a sign that the battle has grown so much that they're going to continue to face this. And what I'm struck by is how much the integrity and the credibility of elections now rests on the shoulders of individual employees inside a private company in California. That's a very unusual situation for our democracy to be in. DAVIES: 'Cause they can actually make distinctions about what the public sees and what it doesn't. OSNOS: Yes, yeah. I mean, they have to make very subtle choices. Take, for example, the, you know, misinformation. What is the definition of misinformation? When is somebody being wrong by accident, and when is somebody being wrong on purpose? When are they trying to deceive large numbers of the public? Those kinds of very subtle things which are usually the province of the Supreme Court or of lawmakers are now being handled in conference rooms at Facebook, and that's very complicated. DAVIES: Evan Osnos is a staff writer for The New Yorker. His piece \"The Ghost In The Machine\" about Mark Zuckerberg and Facebook appeared in a recent issue. We'll talk some more after a short break. This is FRESH AIR. (SOUNDBITE OF THE ADAM PRICE GROUP'S \"STORYVILLE\") DAVIES: This is FRESH AIR, and we're speaking with Evan Osnos. He's a staff writer for The New Yorker. His story \"Ghost In The Machine\" about Mark Zuckerberg and Facebook, appears in a recent issue of the magazine. They recently took down posts from Alex Jones of Infowars. You want to tell us that - tell us about that? OSNOS: Yeah. This was really an important case study in how Facebook's going to deal with its most complicated problem in some sense, which is content. What do you simply do with the fact that people are posting a billion items of content to Facebook every day? That's the actual number. And what they've tried to do is to say, OK, we're going to punish hate speech; we're going to prevent hate speech from being on here. But for things that are less than hate speech - if it's just misinformation or things that appear to be wrong by accident, well, then we're not going to ban that person from Facebook. We're going to try to use other tools. We may make their posts less visible. We might share them less. But the case of Infowars, which, as we all know, is a conspiracy website led by Alex Jones - and for years, it has promoted in particular the falsehood - it's a false conspiracy theory that the massacre at Sandy Hook Elementary School was staged; it was a hoax, and it was designed to try to advance an anti-gun agenda. That's the theory. And for years, people have complained to Facebook about it. They said, this really has risen to the level of harassment of the parents who are - whose children were killed at Sandy Hook. And this summer, people started to criticize the company more publicly and said, look; you need to remove Infowars. This is no longer normal content. They've disqualified themselves from being a part of civil discourse here by directing all of this harassment at the parents. And then there was - the parents of one of the children at Sandy Hook wrote an open letter, quite candid and very tough, directed at Mark Zuckerberg and said that they have been driven into hiding because of their, as they put it, inexplicable battle with Facebook to try to get this kind of material taken down so that they don't have to deal with trolls and people who are, you know, discovering their address and then threatening them online. And finally this summer, Apple took down Infowars' podcasts. And then very soon thereafter, Facebook and other companies followed suit. And I talked to Mark Zuckerberg about it. I said, why did you wait this - why did you wait so long? Why did you wait so long to take down this thing that people had so clearly complained about? And he said, well, we don't want to punish people who are just wrong. We don't want to ban them from Facebook. What we're trying to do is figure out how to shape this thing. And he acknowledged in a fact that because Apple had moved on this, that - he said, at that point, we realized we had to make a decision. We had to get rid of this, and so we did it. But from my perspective, what was interesting about this was that this is the very beginning of an issue for Facebook. This is not the end. I mean, this is just the front edge of an unbelievably complex problem, which is, what are the bounds of free speech? What do we actually want to be able to have, and what do we consider to be out of bounds? What is, in effect, shouting fire in a crowded theater, and what is legitimate provocative, unsavory speech? And these are some of the hardest problems that we face, and they're now in the hands of - let's face it. They're in the hands of the engineers, the people who created this incredibly powerful application. DAVIES: Yeah, you know, there's lots of First Amendment law that says the government can't, you know, distinguish types of speech and prohibit it. Is it clear that a private company like Facebook can when it's this big? OSNOS: Well, actually, Facebook, as a private company, can do whatever it wants on speech. If they decided tomorrow that you couldn't talk about golden retrievers on Facebook, they could put that rule in place. And I think for some reason, that - you know, we find ourselves really torn. Even if you're not a fan of Infowars - and God knows I'm not - it has to make a person uneasy to know that there is now a company which is capable of deciding not only what kind of information it's going to suppress but also which kind of information it's going to promote. And on any given day, there are people who are going to be offended by those choices. But the tools by which Facebook is held accountable are not the tools that we use in politics. It's not like you vote the bums out. It's not like people are appointed to Facebook's board as if they were Supreme Court justices. This is a case in which a private company is making profound choices about the contours and the boundaries of political expression. And we don't have obvious tools with which to regulate them. DAVIES: You know, when you look at the arc of the story, I mean, this company founded by Mark Zuckerberg has this astonishing growth, is deeply committed to growth and, in doing so, you know, compromises privacy and ends up, you know, sharing data it shouldn't about its users and gets into some trouble. And a question arises about whether the company has the ability - has the capacity for self-reflection, whether it can take, you know, adverse information and re-examine its assumptions and practices. And in many respects, this really comes down to Zuckerberg. What did you find about that? OSNOS: I found that he is insulated to I think an unhealthy degree from this kind of criticism. And if he was sitting with me right now, I would say this directly. The reality is he's built the company in his own image. He's had the luxury of sculpting an organization to his like - I mean, quite literally, the blue color that Facebook has as its signature blue is chosen because he is red-green colorblind, and he prefers to look at the color blue. He can see it very distinctly. So in every way, both physical and spiritual, this company reflects his sensibilities. But in order to be able to continue to grow and evolve and respond to the problems that it's encountered, he needs people sitting in the room with him who will tell him, Mark, I think you're not seeing this the right way; you're not seeing this clearly; you're wrong. And I was struck that in our interviews, I got the sense from him that he knows that on some level. He's tried over the years to make these choices, to get outside what he described as the bubble. He's got five people who report directly to him. And they are all people who he has in effect chosen and installed in those positions. And there is very few - there are very few people at Facebook who are willing to stick their neck out and say, I fundamentally disagree; we need to do things differently. DAVIES: You know, at the end of this piece, you write that some people think of Mark Zuckerberg as an automaton with little regard for the human dimensions of his work. And you say, not exactly. The truth is something else. What's the truth? OSNOS: The truth is that he is at peace with what he has done, with the choices that he has made. I came to really understand that Mark Zuckerberg, in his own conception of his place in history, believes that no change happens painlessly and that change is difficult. And in many ways, it's like his inspiration - Augustus Caesar. He believes that he's made tradeoffs, that he has - in order to grow, he had to give up perfection. If he wanted to be vastly influential, then he couldn't always be quite as safe as people wanted him to be. And in his mind and in the mind of the people around him, they are vindicated by their sheer scale and success. And for that reason, it's very hard for them to accept that the public is howling, in many cases, for real change because they believe if we had given in to the critics at every step along the way and made changes, then we wouldn't be as big as we are today. DAVIES: Evan Osnos, thanks so much for speaking with us. OSNOS: Thanks very much for having me, Dave. GROSS: Evan Osnos is a staff writer for The New Yorker. His article about Facebook is titled \"Ghost In The Machine: Can Mark Zuckerberg Fix Facebook Before It Breaks Democracy? \" If you'd like to catch up on FRESH AIR interviews you missed, like our interview with Washington Post national security correspondent Greg Miller, author of the new book \"The Apprentice: Trump, Russia And The Subversion Of American Democracy,\" check out our podcast. You'll find lots of FRESH AIR interviews, including my recent interview with pianist, composer and singer Jon Batiste, who leads the house band on \"Late Night With Stephen Colbert\" and was at the piano for our interview. (SOUNDBITE OF THE ROB DIXON TRIO'S \"SAN LEANDRO\") GROSS: FRESH AIR's executive producer is Danny Miller. Our interviews and reviews are produced and edited by Amy Salit, Phyllis Myers, Sam Briger, Lauren Krenzel, Heidi Saman, Therese Madden, Mooj Zadie, Thea Chaloner and Seth Kelley. I'm Terry Gross. (SOUNDBITE OF THE ROB DIXON TRIO'S \"SAN LEANDRO\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-04-654306774": {"title": "U.S. Charges 7 Russian Intelligence Officers With Hacking 40 Sports And Doping Groups : NPR", "url": "https://www.npr.org/2018/10/04/654306774/russian-cyber-unit-accused-of-attacking-opcw-chemical-weapons-watchdog", "author": "No author found", "published_date": "2018-10-04", "content": "MARY LOUISE KELLY, HOST: The U. S. joined European governments today in accusing a group of Russian military intelligence officers of more cyber mischief. The indictment from the U. S. Justice Department describes hacks against sports stars and against anti-doping agencies in the U. S. and in Canada. It also says Russians targeted a Dutch group that was studying the poison used to try and kill a former Russian spy. NPR national justice correspondent Carrie Johnson was at the Justice Department today, as she is most days, and she is back here in the studio now with details. Hey, Carrie. CARRIE JOHNSON, BYLINE: Hi, Mary Louise. KELLY: OK, so tell me more about the Russian intelligence officers being charged and why now. What's the timing? JOHNSON: Yeah, the defendants are seven current Russian military officers. Today's charges include conspiracy and money laundering. A few of the defendants are also charged with wire fraud or identity theft. Court papers say some of the cyberactivity here started in 2014 when allegations about Russia cheating to avoid drug tests for the Olympics first came to light. But some of the behavior extended up till this summer in 2018. KELLY: Oh, so it's - this is very current. JOHNSON: Yeah, when the hackers were in touch with reporters who wanted access to documents these hackers got their hands on. KELLY: Now, I gather all of these seven Russian officers are in Russia. And there is no U. S. extradition treaty in place. So why bother charging them? JOHNSON: Well, FBI officials point out sometimes they get lucky. These guys travel to European countries where there are extradition treaties in place. But even if that doesn't happen, the DOJ says its practice of naming and shaming does make a difference because it shows the hackers America knows how to find them and describe what they did. And sometimes other parts of the government impose sanctions on individual hackers or the people who fund them. The assistant attorney general for national security, John Demers, told reporters that Russia launched this cyber effort because it was embarrassed by allegations that its athletes were evading drug tests. (SOUNDBITE OF ARCHIVED RECORDING)JOHN DEMERS: Embarrassed by that truth, Russia fought back by retaliating against the truth-tellers and against the truth itself. JOHNSON: Now, Mary Louise, the Justice Department says it's exposing those activities and that this indictment tells the real story. KELLY: I was asking you earlier about the timing of these charges, Carrie, and I have another question along those lines. The U. S. announced these charges just hours after Britain and the Netherlands announced their own accusations against Russians. Is that coincidence or coordinated? JOHNSON: Very much coordinated. The Justice Department actually thanked its international law enforcement partners for their help at the press conference here in D. C. today. They highlighted how the Dutch were actually able to disrupt a hacking plot in April where four Russian men carrying diplomatic passports traveled to the Hague and rented a car and then filled it with electronic equipment. They parked that car next to the Organization for the Prohibition of Chemical Weapons. The goal was to penetrate the Wi-Fi networks there. But the Dutch were on to them, so the men abandoned that car and left the country. And this was at the same time this organization, the OPCW, was examining a substance used to poison a former Russian military intelligence officer and his daughter in the U. K. this year. KELLY: This is the Sergei Skripal case. JOHNSON: Exactly. KELLY: OK. I mean, here's my big question, which is, what do these charges tell us, if anything, about the ongoing effort to protect U. S. elections? I mean, this is all, you know, something we're talking about still because of 2016 and hacks against Democratic servers and institutions. Are we any closer to knowing about efforts to protect the upcoming midterms from further Russian interference? JOHNSON: The DOJ wouldn't touch that question today. But there is something worth pointing out here. The new indictment does include three of the same people accused earlier this year of hacking the 2016 presidential election, which begs the question, are they at it again? If so, the U. S. government has just singled them out all over again. And one unusual thing stood out this morning. The DOJ actually had a warning for news media both here and around the world. Be careful, they said, about using material that comes from these hacks. It can be false or misleading, or the hackers can have a sinister motive, as the Russian government allegedly did in this case. KELLY: NPR's Carrie Johnson, thank you. JOHNSON: My pleasure. MARY LOUISE KELLY, HOST:  The U. S. joined European governments today in accusing a group of Russian military intelligence officers of more cyber mischief. The indictment from the U. S. Justice Department describes hacks against sports stars and against anti-doping agencies in the U. S. and in Canada. It also says Russians targeted a Dutch group that was studying the poison used to try and kill a former Russian spy. NPR national justice correspondent Carrie Johnson was at the Justice Department today, as she is most days, and she is back here in the studio now with details. Hey, Carrie. CARRIE JOHNSON, BYLINE: Hi, Mary Louise. KELLY: OK, so tell me more about the Russian intelligence officers being charged and why now. What's the timing? JOHNSON: Yeah, the defendants are seven current Russian military officers. Today's charges include conspiracy and money laundering. A few of the defendants are also charged with wire fraud or identity theft. Court papers say some of the cyberactivity here started in 2014 when allegations about Russia cheating to avoid drug tests for the Olympics first came to light. But some of the behavior extended up till this summer in 2018. KELLY: Oh, so it's - this is very current. JOHNSON: Yeah, when the hackers were in touch with reporters who wanted access to documents these hackers got their hands on. KELLY: Now, I gather all of these seven Russian officers are in Russia. And there is no U. S. extradition treaty in place. So why bother charging them? JOHNSON: Well, FBI officials point out sometimes they get lucky. These guys travel to European countries where there are extradition treaties in place. But even if that doesn't happen, the DOJ says its practice of naming and shaming does make a difference because it shows the hackers America knows how to find them and describe what they did. And sometimes other parts of the government impose sanctions on individual hackers or the people who fund them. The assistant attorney general for national security, John Demers, told reporters that Russia launched this cyber effort because it was embarrassed by allegations that its athletes were evading drug tests. (SOUNDBITE OF ARCHIVED RECORDING) JOHN DEMERS: Embarrassed by that truth, Russia fought back by retaliating against the truth-tellers and against the truth itself. JOHNSON: Now, Mary Louise, the Justice Department says it's exposing those activities and that this indictment tells the real story. KELLY: I was asking you earlier about the timing of these charges, Carrie, and I have another question along those lines. The U. S. announced these charges just hours after Britain and the Netherlands announced their own accusations against Russians. Is that coincidence or coordinated? JOHNSON: Very much coordinated. The Justice Department actually thanked its international law enforcement partners for their help at the press conference here in D. C. today. They highlighted how the Dutch were actually able to disrupt a hacking plot in April where four Russian men carrying diplomatic passports traveled to the Hague and rented a car and then filled it with electronic equipment. They parked that car next to the Organization for the Prohibition of Chemical Weapons. The goal was to penetrate the Wi-Fi networks there. But the Dutch were on to them, so the men abandoned that car and left the country. And this was at the same time this organization, the OPCW, was examining a substance used to poison a former Russian military intelligence officer and his daughter in the U. K. this year. KELLY: This is the Sergei Skripal case. JOHNSON: Exactly. KELLY: OK. I mean, here's my big question, which is, what do these charges tell us, if anything, about the ongoing effort to protect U. S. elections? I mean, this is all, you know, something we're talking about still because of 2016 and hacks against Democratic servers and institutions. Are we any closer to knowing about efforts to protect the upcoming midterms from further Russian interference? JOHNSON: The DOJ wouldn't touch that question today. But there is something worth pointing out here. The new indictment does include three of the same people accused earlier this year of hacking the 2016 presidential election, which begs the question, are they at it again? If so, the U. S. government has just singled them out all over again. And one unusual thing stood out this morning. The DOJ actually had a warning for news media both here and around the world. Be careful, they said, about using material that comes from these hacks. It can be false or misleading, or the hackers can have a sinister motive, as the Russian government allegedly did in this case. KELLY: NPR's Carrie Johnson, thank you. JOHNSON: My pleasure.", "section": "Europe", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-04-653454568": {"title": "Most Twitter Accounts Linked To 2016 Disinformation Are Still Active, Report Finds : NPR", "url": "https://www.npr.org/2018/10/04/653454568/most-twitter-accounts-linked-to-2016-disinformation-are-still-active-report-find", "author": "No author found", "published_date": "2018-10-04", "content": "", "section": "National Security", "disclaimer": ""}, "2018-10-04-654264122": {"title": "House Intern Arrested, Charged With Doxing Senator During Kavanaugh Hearing : NPR", "url": "https://www.npr.org/2018/10/04/654264122/house-intern-arrested-for-reportedly-doxing-senator-during-kavanaugh-hearing", "author": "No author found", "published_date": "2018-10-04", "content": "", "section": "Law", "disclaimer": ""}, "2018-10-05-654690893": {"title": "Transportation Department Looks To Clear The Road For Cars Without Steering Wheels : NPR", "url": "https://www.npr.org/2018/10/05/654690893/transportation-department-looks-to-clear-the-road-for-cars-without-steering-whee", "author": "No author found", "published_date": "2018-10-05", "content": "", "section": "Technology", "disclaimer": ""}, "2018-10-06-655121342": {"title": "The Implications Of Bloomberg's China Hacking Report : NPR", "url": "https://www.npr.org/2018/10/06/655121342/the-implications-of-bloombergs-china-hacking-report", "author": "No author found", "published_date": "2018-10-06", "content": "SCOTT SIMON, HOST: This week, a report by Bloomberg said that Chinese government operatives planted microchips, especially micro microchips, into servers created by an American company called Supermicro. Big names use Supermicro's products, including Apple and NPR funder Amazon. The Chinese government strenuously denies the allegations. NPR's Jasmine Garsd tells us how worried American companies and users ought to be. JASMINE GARSD, BYLINE: One thing is for sure - it was tiny as a pencil tip. But it sent fear through big tech's spine and national security communities. Adam Segal is a director of the Digital and Cyberspace Policy Program at the Council on Foreign Relations, a foreign policy think tank. He says if the Bloomberg story is true. . . ADAM SEGAL: . . . Then it's a major deal. GARSD: It's not the first time Chinese tech companies with suspected ties to the Chinese government have been called into question. Earlier this year, the Pentagon prohibited phones manufactured by Chinese companies Huawei and ZTE on military bases. And Congress has been scrutinizing Google and Facebook over their data-sharing relationship with Huawei. In both cases, national security was the concern. SEGAL: There's been reports from the Defense Science Board and others from at least the beginning of the 2000s that placing some type of chip would be a useful way for the Chinese intelligence services to gain access to information. GARSD: What's worrisome about the allegations in the Bloomberg story is that an overwhelming amount of our technology uses components manufactured in China. The recent alleged attack didn't just compromise Apple and Amazon but also a major bank and U. S. government contractors. It's scary because, if it's true, we're stuck. The U. S. supply chain is linked to China inextricably. KATIE MOUSSOURIS: I think we've already kind of passed the point where we can, for national security, just insource everything. I think we've passed that point at least 30 years ago. GARSD: Katie Moussouris is the founder and CEO of Luta Security. It helps governments and large organizations deal with cyberattacks. Moving manufacturing from China to the U. S. is a centerpiece of the Trump administration's trade policy. But she says it's just not feasible. MOUSSOURIS: The alternative of, let's say, insourcing instead of outsourcing and trying to manufacture every single part for all of our electronics domestically here in the United States - we simply would lack the capacity. GARSD: After the story published earlier this week, investors fled Supermicro, the manufacturer accused of having the implanted Chinese microchips. Shares took a nosedive. Amazon and Apple have called the reports untrue. Moussouris is wary of taking the Bloomberg report at its word. She says these types of attacks - hardware attacks - are very hard to pull off. Even the Bloomberg report compares them to black magic - so unusual, companies are not very good at detecting or even combating it. MOUSSOURIS: Companies like Apple and Microsoft and Amazon and Google, especially, have all done a lot of work to harden their software. So it makes it actually harder to exploit software vulnerabilities today than it is to exploit some hardware or hardware design flaws. GARSD: Moussouris says hardware hacking is difficult to pull off. But. . . MOUSSOURIS: Actually, when hardware hacking does happen, it's incredibly effective because you can bypass all of the software security in layers above. GARSD: Like black magic, if it does exist, it's pretty scary. Jasmine Garsd, NPR News, New York. SCOTT SIMON, HOST:  This week, a report by Bloomberg said that Chinese government operatives planted microchips, especially micro microchips, into servers created by an American company called Supermicro. Big names use Supermicro's products, including Apple and NPR funder Amazon. The Chinese government strenuously denies the allegations. NPR's Jasmine Garsd tells us how worried American companies and users ought to be. JASMINE GARSD, BYLINE: One thing is for sure - it was tiny as a pencil tip. But it sent fear through big tech's spine and national security communities. Adam Segal is a director of the Digital and Cyberspace Policy Program at the Council on Foreign Relations, a foreign policy think tank. He says if the Bloomberg story is true. . . ADAM SEGAL: . . . Then it's a major deal. GARSD: It's not the first time Chinese tech companies with suspected ties to the Chinese government have been called into question. Earlier this year, the Pentagon prohibited phones manufactured by Chinese companies Huawei and ZTE on military bases. And Congress has been scrutinizing Google and Facebook over their data-sharing relationship with Huawei. In both cases, national security was the concern. SEGAL: There's been reports from the Defense Science Board and others from at least the beginning of the 2000s that placing some type of chip would be a useful way for the Chinese intelligence services to gain access to information. GARSD: What's worrisome about the allegations in the Bloomberg story is that an overwhelming amount of our technology uses components manufactured in China. The recent alleged attack didn't just compromise Apple and Amazon but also a major bank and U. S. government contractors. It's scary because, if it's true, we're stuck. The U. S. supply chain is linked to China inextricably. KATIE MOUSSOURIS: I think we've already kind of passed the point where we can, for national security, just insource everything. I think we've passed that point at least 30 years ago. GARSD: Katie Moussouris is the founder and CEO of Luta Security. It helps governments and large organizations deal with cyberattacks. Moving manufacturing from China to the U. S. is a centerpiece of the Trump administration's trade policy. But she says it's just not feasible. MOUSSOURIS: The alternative of, let's say, insourcing instead of outsourcing and trying to manufacture every single part for all of our electronics domestically here in the United States - we simply would lack the capacity. GARSD: After the story published earlier this week, investors fled Supermicro, the manufacturer accused of having the implanted Chinese microchips. Shares took a nosedive. Amazon and Apple have called the reports untrue. Moussouris is wary of taking the Bloomberg report at its word. She says these types of attacks - hardware attacks - are very hard to pull off. Even the Bloomberg report compares them to black magic - so unusual, companies are not very good at detecting or even combating it. MOUSSOURIS: Companies like Apple and Microsoft and Amazon and Google, especially, have all done a lot of work to harden their software. So it makes it actually harder to exploit software vulnerabilities today than it is to exploit some hardware or hardware design flaws. GARSD: Moussouris says hardware hacking is difficult to pull off. But. . . MOUSSOURIS: Actually, when hardware hacking does happen, it's incredibly effective because you can bypass all of the software security in layers above. GARSD: Like black magic, if it does exist, it's pretty scary. Jasmine Garsd, NPR News, New York.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-06-654759573": {"title": "The Russia Investigations: The New Era Of Foreign Threats : NPR", "url": "https://www.npr.org/2018/10/06/654759573/the-russia-investigations-the-new-era-of-foreign-threats", "author": "No author found", "published_date": "2018-10-06", "content": "", "section": "National Security", "disclaimer": ""}, "2018-10-07-652363255": {"title": "The Robots Are Coming To Las Vegas : NPR", "url": "https://www.npr.org/2018/10/07/652363255/the-robots-are-coming-to-las-vegas", "author": "No author found", "published_date": "2018-10-07", "content": "LULU GARCIA-NAVARRO, HOST: We were in Las Vegas a couple of weeks ago, where the hospitality and gaming industries drive most of the economy, employing some 300,000 people, which is why a 2017 study predicting a massive disruption - the loss of up to two-thirds of all jobs in Las Vegas - has garnered, as you can imagine, a lot of attention. Now, what could cause that disruption? Meet Jett. (SOUNDBITE OF ROBOT BEEPING)GARCIA-NAVARRO: Jett is a delivery robot, the outside designed to resemble a dog - kind of. And along with its robot sibling Fetch, they're a new addition to the workforce at the Vdara Hotel in Las Vegas. So the robot is going through the lobby of the hotel with its delivery. It's about 3 feet high. And we have the Dalmatian one running for us. And it's causing quite a stir. Guests stop, turn and look as Jett navigates all by itself, making a small delivery from the lobby cafe to a room on the fifth floor. But first, Jett remotely calls for an elevator for a quick trip up. (SOUNDBITE OF ELEVATOR BEEPING)RECORDED VOICE: Fifth floor. GARCIA-NAVARRO: Kerryn Abel, director of hotel operations for the Vdara, joins me. (SOUNDBITE OF ROBOT BEEPING)RECORDED VOICE: Going down. KERRYN ABEL: So right now it's telling - it's getting out to let the guests know it's ready to leave the elevator. GARCIA-NAVARRO: And it's very polite. It says, excuse me, please. Those words scroll by on its front screen. We reach the room, and it alerts the guest by an automated phone call. And then it makes its delivery. And there it is. And it says, hello. Please remove your items. (SOUNDBITE OF ROBOT BEEPING)GARCIA-NAVARRO: And it says, yay. And it's shaking back and forth. Thanks. And then it says, bye. I'm heading home. And off it goes. Jett the delivery robot is just one example of how automation is gaining traction in Las Vegas. But Kerryn Abel says, don't worry. Little Jett doesn't portend the employment apocalypse just yet. Do you envision this as more than sort of a gimmick? It's something that people like. But do you see it becoming something other than that? ABEL: It's been really fun for the guests and fun for the employees. But it really isn't anything more than that. It's a convenience, but it still needs to be handled by employees. It still needs an employee interaction. So it isn't going to be anything that's going to replace something. But it is a fun interaction for our guests. GARCIA-NAVARRO: Cliff Atkinson is senior vice president for hospitality at MGM Resorts. And the Vdara is part of its family of hotels. While we were in Las Vegas, he was in Japan for work. So we got him on the phone. CLIFF ATKINSON: For us, we like to be innovative. And we like to try new, fun things. GARCIA-NAVARRO: Beyond the fun, Atkinson says automation is helping transform the industry as guests demand more of a customized experience. They're looking for options. Take, for instance, checking in. The front desk is already changing. ATKINSON: I see it as we eliminate front desks altogether and that you're able to check in over your phone. But the people that were at those front desk are still there. They're there to customize your journey, there to greet you at the car. And they're there to escort you to your room, to make sure you have everything you need. You can't replace that and that engagement. GARCIA-NAVARRO: For Atkinson, technology is an opportunity to retrain employees. Goodbye, front desk agent. Hello, lobby ambassador. But automation will apparently hit Las Vegas most in the spaces that we don't see. That's according to the 2017 study by the aptly named Institute for Spatial Economic Analysis. Administrative back offices, restaurant kitchens, jobs often done by people with low education levels, jobs done mostly by women and people of color will be affected first. GEOCONDA ARGUELLO KLINE: We know we can't (ph) always stop technology. Technology's here. GARCIA-NAVARRO: Geoconda Arguello Kline is secretary-treasurer for the Culinary Union, which represents about 57,000 hospitality workers in Las Vegas, the majority of them women and Hispanic. This year, she helped negotiate a new contract between the union and many of the big casinos in Las Vegas. It included what they called ground-breaking language. For workers whose jobs are going to be affected by new technology, they'll get a chance to retrain. ARGUELLO KLINE: Now the workers - they have the opportunity to get the training, to learn something new. And at the same time, the workers - they will say, you know, I don't want this. This is not for me. But I have a choice. GARCIA-NAVARRO: What she's saying is that for those who move on and decide not to accept the training, they'll get severance pay and benefits for six months. A key provision within this five-year contract - the union gets a heads-up of 180 days before a new technology is implemented. This is already happening in some hotels. Workers who chop salad are being replaced by machines in some places. ARGUELLO KLINE: Right now we feel like we protect this five years. But in these five years, we have to prepare for the next five years - what's going to happen. GARCIA-NAVARRO: The struggle between man and machine is as old as the dawn of the mechanized age. Remember men used to race on horses against the steam train? Chess players would match their wits against computers? Automation has already changed the way we work for generations now. But I still contend that a real bartender makes a better margarita. And so to test that theory, I head to my final stop in Las Vegas, The Tipsy Robot bar. AYANNA MARTIN: What is this? - 'cause this actually seems kind of cool. GARCIA-NAVARRO: Twenty-three-year-old Ayanna Martin (ph) of Maryland is selecting a drink off a computer tablet. MARTIN: Oh, there's cherry syrup in it. I'm going to go with this, I think. GARCIA-NAVARRO: Drink ordered, a big robotic arm, a cocktail shaker for a hand, gets to work. Liquor bottles hanging from the ceiling dispense exact amounts of booze. Then a quick shake. And finally, the robot arm pours the concoction into a plastic cup. (SOUNDBITE OF POURING LIQUID)MARTIN: My God, this is so good. (Laughter) This is actually really good. GARCIA-NAVARRO: So do you think can imagine drinking from a robot all the time instead of a human? MARTIN: Yes. It's probably going to be a lot more cost-effective, but it's not going to be as fun for the people who go to bars 'cause the fun part of going to a bar is making friends with the bartender and having them pour just a little bit extra, more than they're supposed to, in your drink. Robots can't do that (laughter). GARCIA-NAVARRO: So true, so true. But Victor Reza Valanejad, the general manager of The Tipsy Robot, says his bartending robots will mean more jobs, not fewer. VICTOR REZA VALANEJAD: Our business is growing. And we are not buying more robots. But we are hiring more humans. So just imagine if there was Tipsy Robots popping up everywhere and hiring 10 people, 10 people, 10 people, even more. That would be just great. GARCIA-NAVARRO: So you think that, actually, the nature of work is going to change. But it doesn't mean that humans are going to be replaced by robots. That's kind of optimistic. REZA VALANEJAD: No. GARCIA-NAVARRO: I like it. REZA VALANEJAD: Yeah. And, you know. . . GARCIA-NAVARRO: We talk some more about robots and the future of bartending. And then it's time for my taste test. I order their signature drink, a Pineapple Planet. Wow. That is not my thing (laughter). As I suspected, nothing tops a margarita made by a human bartender - for now. (SOUNDBITE OF KRAFTWERK SONG, \"THE ROBOTS\") LULU GARCIA-NAVARRO, HOST:  We were in Las Vegas a couple of weeks ago, where the hospitality and gaming industries drive most of the economy, employing some 300,000 people, which is why a 2017 study predicting a massive disruption - the loss of up to two-thirds of all jobs in Las Vegas - has garnered, as you can imagine, a lot of attention. Now, what could cause that disruption? Meet Jett. (SOUNDBITE OF ROBOT BEEPING) GARCIA-NAVARRO: Jett is a delivery robot, the outside designed to resemble a dog - kind of. And along with its robot sibling Fetch, they're a new addition to the workforce at the Vdara Hotel in Las Vegas. So the robot is going through the lobby of the hotel with its delivery. It's about 3 feet high. And we have the Dalmatian one running for us. And it's causing quite a stir. Guests stop, turn and look as Jett navigates all by itself, making a small delivery from the lobby cafe to a room on the fifth floor. But first, Jett remotely calls for an elevator for a quick trip up. (SOUNDBITE OF ELEVATOR BEEPING) RECORDED VOICE: Fifth floor. GARCIA-NAVARRO: Kerryn Abel, director of hotel operations for the Vdara, joins me. (SOUNDBITE OF ROBOT BEEPING) RECORDED VOICE: Going down. KERRYN ABEL: So right now it's telling - it's getting out to let the guests know it's ready to leave the elevator. GARCIA-NAVARRO: And it's very polite. It says, excuse me, please. Those words scroll by on its front screen. We reach the room, and it alerts the guest by an automated phone call. And then it makes its delivery. And there it is. And it says, hello. Please remove your items. (SOUNDBITE OF ROBOT BEEPING) GARCIA-NAVARRO: And it says, yay. And it's shaking back and forth. Thanks. And then it says, bye. I'm heading home. And off it goes. Jett the delivery robot is just one example of how automation is gaining traction in Las Vegas. But Kerryn Abel says, don't worry. Little Jett doesn't portend the employment apocalypse just yet. Do you envision this as more than sort of a gimmick? It's something that people like. But do you see it becoming something other than that? ABEL: It's been really fun for the guests and fun for the employees. But it really isn't anything more than that. It's a convenience, but it still needs to be handled by employees. It still needs an employee interaction. So it isn't going to be anything that's going to replace something. But it is a fun interaction for our guests. GARCIA-NAVARRO: Cliff Atkinson is senior vice president for hospitality at MGM Resorts. And the Vdara is part of its family of hotels. While we were in Las Vegas, he was in Japan for work. So we got him on the phone. CLIFF ATKINSON: For us, we like to be innovative. And we like to try new, fun things. GARCIA-NAVARRO: Beyond the fun, Atkinson says automation is helping transform the industry as guests demand more of a customized experience. They're looking for options. Take, for instance, checking in. The front desk is already changing. ATKINSON: I see it as we eliminate front desks altogether and that you're able to check in over your phone. But the people that were at those front desk are still there. They're there to customize your journey, there to greet you at the car. And they're there to escort you to your room, to make sure you have everything you need. You can't replace that and that engagement. GARCIA-NAVARRO: For Atkinson, technology is an opportunity to retrain employees. Goodbye, front desk agent. Hello, lobby ambassador. But automation will apparently hit Las Vegas most in the spaces that we don't see. That's according to the 2017 study by the aptly named Institute for Spatial Economic Analysis. Administrative back offices, restaurant kitchens, jobs often done by people with low education levels, jobs done mostly by women and people of color will be affected first. GEOCONDA ARGUELLO KLINE: We know we can't (ph) always stop technology. Technology's here. GARCIA-NAVARRO: Geoconda Arguello Kline is secretary-treasurer for the Culinary Union, which represents about 57,000 hospitality workers in Las Vegas, the majority of them women and Hispanic. This year, she helped negotiate a new contract between the union and many of the big casinos in Las Vegas. It included what they called ground-breaking language. For workers whose jobs are going to be affected by new technology, they'll get a chance to retrain. ARGUELLO KLINE: Now the workers - they have the opportunity to get the training, to learn something new. And at the same time, the workers - they will say, you know, I don't want this. This is not for me. But I have a choice. GARCIA-NAVARRO: What she's saying is that for those who move on and decide not to accept the training, they'll get severance pay and benefits for six months. A key provision within this five-year contract - the union gets a heads-up of 180 days before a new technology is implemented. This is already happening in some hotels. Workers who chop salad are being replaced by machines in some places. ARGUELLO KLINE: Right now we feel like we protect this five years. But in these five years, we have to prepare for the next five years - what's going to happen. GARCIA-NAVARRO: The struggle between man and machine is as old as the dawn of the mechanized age. Remember men used to race on horses against the steam train? Chess players would match their wits against computers? Automation has already changed the way we work for generations now. But I still contend that a real bartender makes a better margarita. And so to test that theory, I head to my final stop in Las Vegas, The Tipsy Robot bar. AYANNA MARTIN: What is this? - 'cause this actually seems kind of cool. GARCIA-NAVARRO: Twenty-three-year-old Ayanna Martin (ph) of Maryland is selecting a drink off a computer tablet. MARTIN: Oh, there's cherry syrup in it. I'm going to go with this, I think. GARCIA-NAVARRO: Drink ordered, a big robotic arm, a cocktail shaker for a hand, gets to work. Liquor bottles hanging from the ceiling dispense exact amounts of booze. Then a quick shake. And finally, the robot arm pours the concoction into a plastic cup. (SOUNDBITE OF POURING LIQUID) MARTIN: My God, this is so good. (Laughter) This is actually really good. GARCIA-NAVARRO: So do you think can imagine drinking from a robot all the time instead of a human? MARTIN: Yes. It's probably going to be a lot more cost-effective, but it's not going to be as fun for the people who go to bars 'cause the fun part of going to a bar is making friends with the bartender and having them pour just a little bit extra, more than they're supposed to, in your drink. Robots can't do that (laughter). GARCIA-NAVARRO: So true, so true. But Victor Reza Valanejad, the general manager of The Tipsy Robot, says his bartending robots will mean more jobs, not fewer. VICTOR REZA VALANEJAD: Our business is growing. And we are not buying more robots. But we are hiring more humans. So just imagine if there was Tipsy Robots popping up everywhere and hiring 10 people, 10 people, 10 people, even more. That would be just great. GARCIA-NAVARRO: So you think that, actually, the nature of work is going to change. But it doesn't mean that humans are going to be replaced by robots. That's kind of optimistic. REZA VALANEJAD: No. GARCIA-NAVARRO: I like it. REZA VALANEJAD: Yeah. And, you know. . . GARCIA-NAVARRO: We talk some more about robots and the future of bartending. And then it's time for my taste test. I order their signature drink, a Pineapple Planet. Wow. That is not my thing (laughter). As I suspected, nothing tops a margarita made by a human bartender - for now. (SOUNDBITE OF KRAFTWERK SONG, \"THE ROBOTS\")", "section": "Business", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-07-654339389": {"title": "China Makes A Big Play In Silicon Valley : NPR", "url": "https://www.npr.org/2018/10/07/654339389/china-makes-a-big-play-in-silicon-valley", "author": "No author found", "published_date": "2018-10-07", "content": "LULU GARCIA-NAVARRO, HOST: There is a trade war underway between the U. S. and China, and it is a big concern to the global economy. But there's another deeper rift brewing between Washington and Beijing. And that stems from China's effort to unseat the U. S. as the global leader when it comes to technology and innovation. In the next installment of our series on China's growing role in the world, we turn to the U. S. , where NPR's Jackie Northam reports Washington is pushing back. JACKIE NORTHAM, BYLINE: The U. S. is unquestionably the leader in advanced technology - think artificial intelligence, robotics, sophisticated weaponry. It's what's helped give the country a national security edge. That's something China has recognized and wants in on, says James Lewis, a specialist in China and technology at the Center for Strategic and International Studies. JAMES LEWIS: The Chinese figured out that technology is the key to wealth and power. And the source of technology is still the West for China. So how do they get their hands on that Western technology? NORTHAM: Three years ago, Beijing unveiled its Made in China 2025 strategy, which calls for the country to become a world leader in advanced technologies, including artificial intelligence, AI. As part of that, China is forming global partnerships, pumping massive resources into tech firms at home, in Europe and here in the U. S. MICHAEL BROWN: I'd say they're very systematic, very long term in their approach. And it's very well-funded. NORTHAM: Michael Brown is with the Pentagon's Defense Innovation Unit in Silicon Valley. He says there's serious concern in Washington that China is acquiring too much sensitive U. S. technology and transferring it back home. BROWN: They don't play by the same rules that we do. So cybertheft is on the table. Industrial espionage is on the table - which could include recruiting key talent at networking events that are sponsored by the Chinese government, working with U. S. universities. NORTHAM: Just last week, Bloomberg reported the U. S. was investigating whether China allegedly infiltrated the Pentagon and major companies, such as Apple and Amazon, by building spy chips into server motherboards. The motherboards were manufactured in China. The companies deny the allegations. But there are other ways China is making inroads to capture American innovation. There's a quiet leafy part of Silicon Valley called Sunnyvale, and a lot of the tech giants have a presence here. There's Amazon Labs, LinkedIn, Yahoo. And I'm coming up now to a Google complex. And I can actually see some of the workers sitting at picnic tables and enjoying the fine weather. Just behind the Google complex is a Chinese company called Baidu. And it's Google's rival. Baidu is China's largest Internet search provider. It opened this innovation center here in Silicon Valley with a focus on self-driving vehicles. And other Chinese tech powerhouses - Alibaba, Tencent, Huawei - also have their own research and development centers here. Instead of buying an existing U. S. business, the Chinese tech giants come in and build a new company from the ground up. These moves are called green field investments. And they hire away a lot of U. S. workers, says the CSIS' Lewis. LEWIS: People change jobs frequently in the tech industry. And you say, I'll pay you a little bit more than the market rate. You'll get a lot of talent. This is a way to acquire knowhow. NORTHAM: There's also been a surge of investments in tech startups through Chinese venture capital. Adam Lysenko, a senior analyst at Rhodium Group, an economic research firm, says there were more than 1,300 rounds of funding for U. S. startups with at least one Chinese investor over the past eight years. ADAM LYSENKO: It is very common for Chinese firms to have some sort of ties to the government. It might just be because they have to answer to the government and party leaders back at home. And that infers the state some level of control. NORTHAM: Lysenko says this has become a concern in national security circles because the nature of emerging technology is inherently dual use. In other words, the AI algorithms used to help speed up your smartphones could also be applied to weapons on the battlefield. Lysenko says while he's not aware of any smoking-gun case where a Chinese venture capital has plundered sensitive technology from a startup, it's widely acknowledged that a risk exists. LYSENKO: That venture capital and other minority investments provide Chinese investors to access potentially sensitive technologies, particularly ones that are in, in a sense, an early stage where U. S. governments haven't had a full chance to evaluate the implications of those technologies. NORTHAM: But for many American tech startups, Chinese investment can be critical. CHRIS NICHOLSON: I think we got lucky. We got a beautiful office, right? It used to be an architectural consulting firm. NORTHAM: Chris Nicholson is the CEO of Skymind, which makes cutting-edge artificial intelligence software. Daylight pours through the large windows overlooking San Francisco's Mission Street. NICHOLSON: Mission between 9th and 10th is a little rough, which is not great when you're trying to impress clients. NORTHAM: Nicholson helped create Skymind about four years ago. He says the company, like many startups, was very fragile in the beginning, until it got its first investment of $200,000 from Chinese consumer tech giant Tencent. NICHOLSON: And nobody else followed. But it allowed us to survive. And that's just - like, the whole trip of startups is just surviving. Buy yourself enough time until you can realize your idea and test it on the market. NORTHAM: Skymind now has funding from some major American venture capitalists and a few smaller Chinese ones. Nicholson disputes that venture capital investors automatically gain access to startups' research or knowhow. Later-stage investors putting in much larger amounts typically ask for things like information rights and board seats. Nicholson says no investor has ever asked Skymind for confidential technical information. NICHOLSON: Right now we have a board of two people. And it's me and my co-founder. And we're both Americans. NORTHAM: For decades, funding for firms like Nicholson's was through the Defense Department. It helped create technological breakthroughs that led to things like semiconductors and miniaturized GPS. But that funding over the years has become slow and cumbersome while technology has been moving much faster in the private sector. The Pentagon recognized the problem and, in 2015, set up the DIU, the Defense Innovation Unit. (SOUNDBITE OF DRONE FLYING)NORTHAM: A military engineer maneuvers a tiny drone as it buzzes above a tarmac at the DIU in Silicon Valley. Lt. Col. David Rothzeid says the Pentagon created the unit as a way to make it easier to get funding to startups. DAVID ROTHZEID: We're trying to grow the defense industry base by enticing these companies that traditionally wouldn't work with us and don't honestly need to work with us but have a great capability that we would all benefit from. NORTHAM: This alternate funding is one way Washington is trying to counter Chinese investment. The Trump administration has also been aggressive in blocking several large mergers and acquisitions of U. S. tech companies by Chinese firms. And Congress is beefing up laws on the books aimed at protecting U. S. technology from foreign governments. It will allow closer scrutiny of all sorts of Chinese inroads in the tech sector, says Mario Mancuso, a trade lawyer with Kirkland and Ellis. MARIO MANCUSO: I think some deals where there are Chinese investors will be a lot harder. More deals will get a lot more scrutiny. And some deals just are not going to happen. NORTHAM: Many U. S. tech companies push back against tougher laws on foreign investment, warning it could strangle innovation. Skymind's Nicholson again. NICHOLSON: In Washington, D. C. , people think innovation is an American monopoly. They think that people can't go anywhere else for innovation. That's not true. Tech is moving fast in a lot of different countries. So if we shut the door here, that capital is just going to flow through another door. And it's going to be outside of U. S. jurisdiction. NORTHAM: And that's the conundrum for policymakers. How does the U. S. balance its national security concerns with its drive to be the leader in advanced technology? Jackie Northam, NPR News, Silicon Valley. LULU GARCIA-NAVARRO, HOST:  There is a trade war underway between the U. S. and China, and it is a big concern to the global economy. But there's another deeper rift brewing between Washington and Beijing. And that stems from China's effort to unseat the U. S. as the global leader when it comes to technology and innovation. In the next installment of our series on China's growing role in the world, we turn to the U. S. , where NPR's Jackie Northam reports Washington is pushing back. JACKIE NORTHAM, BYLINE: The U. S. is unquestionably the leader in advanced technology - think artificial intelligence, robotics, sophisticated weaponry. It's what's helped give the country a national security edge. That's something China has recognized and wants in on, says James Lewis, a specialist in China and technology at the Center for Strategic and International Studies. JAMES LEWIS: The Chinese figured out that technology is the key to wealth and power. And the source of technology is still the West for China. So how do they get their hands on that Western technology? NORTHAM: Three years ago, Beijing unveiled its Made in China 2025 strategy, which calls for the country to become a world leader in advanced technologies, including artificial intelligence, AI. As part of that, China is forming global partnerships, pumping massive resources into tech firms at home, in Europe and here in the U. S. MICHAEL BROWN: I'd say they're very systematic, very long term in their approach. And it's very well-funded. NORTHAM: Michael Brown is with the Pentagon's Defense Innovation Unit in Silicon Valley. He says there's serious concern in Washington that China is acquiring too much sensitive U. S. technology and transferring it back home. BROWN: They don't play by the same rules that we do. So cybertheft is on the table. Industrial espionage is on the table - which could include recruiting key talent at networking events that are sponsored by the Chinese government, working with U. S. universities. NORTHAM: Just last week, Bloomberg reported the U. S. was investigating whether China allegedly infiltrated the Pentagon and major companies, such as Apple and Amazon, by building spy chips into server motherboards. The motherboards were manufactured in China. The companies deny the allegations. But there are other ways China is making inroads to capture American innovation. There's a quiet leafy part of Silicon Valley called Sunnyvale, and a lot of the tech giants have a presence here. There's Amazon Labs, LinkedIn, Yahoo. And I'm coming up now to a Google complex. And I can actually see some of the workers sitting at picnic tables and enjoying the fine weather. Just behind the Google complex is a Chinese company called Baidu. And it's Google's rival. Baidu is China's largest Internet search provider. It opened this innovation center here in Silicon Valley with a focus on self-driving vehicles. And other Chinese tech powerhouses - Alibaba, Tencent, Huawei - also have their own research and development centers here. Instead of buying an existing U. S. business, the Chinese tech giants come in and build a new company from the ground up. These moves are called green field investments. And they hire away a lot of U. S. workers, says the CSIS' Lewis. LEWIS: People change jobs frequently in the tech industry. And you say, I'll pay you a little bit more than the market rate. You'll get a lot of talent. This is a way to acquire knowhow. NORTHAM: There's also been a surge of investments in tech startups through Chinese venture capital. Adam Lysenko, a senior analyst at Rhodium Group, an economic research firm, says there were more than 1,300 rounds of funding for U. S. startups with at least one Chinese investor over the past eight years. ADAM LYSENKO: It is very common for Chinese firms to have some sort of ties to the government. It might just be because they have to answer to the government and party leaders back at home. And that infers the state some level of control. NORTHAM: Lysenko says this has become a concern in national security circles because the nature of emerging technology is inherently dual use. In other words, the AI algorithms used to help speed up your smartphones could also be applied to weapons on the battlefield. Lysenko says while he's not aware of any smoking-gun case where a Chinese venture capital has plundered sensitive technology from a startup, it's widely acknowledged that a risk exists. LYSENKO: That venture capital and other minority investments provide Chinese investors to access potentially sensitive technologies, particularly ones that are in, in a sense, an early stage where U. S. governments haven't had a full chance to evaluate the implications of those technologies. NORTHAM: But for many American tech startups, Chinese investment can be critical. CHRIS NICHOLSON: I think we got lucky. We got a beautiful office, right? It used to be an architectural consulting firm. NORTHAM: Chris Nicholson is the CEO of Skymind, which makes cutting-edge artificial intelligence software. Daylight pours through the large windows overlooking San Francisco's Mission Street. NICHOLSON: Mission between 9th and 10th is a little rough, which is not great when you're trying to impress clients. NORTHAM: Nicholson helped create Skymind about four years ago. He says the company, like many startups, was very fragile in the beginning, until it got its first investment of $200,000 from Chinese consumer tech giant Tencent. NICHOLSON: And nobody else followed. But it allowed us to survive. And that's just - like, the whole trip of startups is just surviving. Buy yourself enough time until you can realize your idea and test it on the market. NORTHAM: Skymind now has funding from some major American venture capitalists and a few smaller Chinese ones. Nicholson disputes that venture capital investors automatically gain access to startups' research or knowhow. Later-stage investors putting in much larger amounts typically ask for things like information rights and board seats. Nicholson says no investor has ever asked Skymind for confidential technical information. NICHOLSON: Right now we have a board of two people. And it's me and my co-founder. And we're both Americans. NORTHAM: For decades, funding for firms like Nicholson's was through the Defense Department. It helped create technological breakthroughs that led to things like semiconductors and miniaturized GPS. But that funding over the years has become slow and cumbersome while technology has been moving much faster in the private sector. The Pentagon recognized the problem and, in 2015, set up the DIU, the Defense Innovation Unit. (SOUNDBITE OF DRONE FLYING) NORTHAM: A military engineer maneuvers a tiny drone as it buzzes above a tarmac at the DIU in Silicon Valley. Lt. Col. David Rothzeid says the Pentagon created the unit as a way to make it easier to get funding to startups. DAVID ROTHZEID: We're trying to grow the defense industry base by enticing these companies that traditionally wouldn't work with us and don't honestly need to work with us but have a great capability that we would all benefit from. NORTHAM: This alternate funding is one way Washington is trying to counter Chinese investment. The Trump administration has also been aggressive in blocking several large mergers and acquisitions of U. S. tech companies by Chinese firms. And Congress is beefing up laws on the books aimed at protecting U. S. technology from foreign governments. It will allow closer scrutiny of all sorts of Chinese inroads in the tech sector, says Mario Mancuso, a trade lawyer with Kirkland and Ellis. MARIO MANCUSO: I think some deals where there are Chinese investors will be a lot harder. More deals will get a lot more scrutiny. And some deals just are not going to happen. NORTHAM: Many U. S. tech companies push back against tougher laws on foreign investment, warning it could strangle innovation. Skymind's Nicholson again. NICHOLSON: In Washington, D. C. , people think innovation is an American monopoly. They think that people can't go anywhere else for innovation. That's not true. Tech is moving fast in a lot of different countries. So if we shut the door here, that capital is just going to flow through another door. And it's going to be outside of U. S. jurisdiction. NORTHAM: And that's the conundrum for policymakers. How does the U. S. balance its national security concerns with its drive to be the leader in advanced technology? Jackie Northam, NPR News, Silicon Valley.", "section": "China Unbound", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-08-655635901": {"title": "How Good \u2014 And How Secure \u2014 Is Facial Recognition Technology? : NPR", "url": "https://www.npr.org/2018/10/08/655635901/how-good-and-how-secure-is-facial-recognition-technology", "author": "No author found", "published_date": "2018-10-08", "content": "AILSA CHANG, HOST: It's time now for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CHANG: This month, we're looking at our bodies the way technology sees them. Here in the U. S. , we're gradually getting used to facial recognition technology. It's in our phones, in some airport security systems, on social media. But how good and how secure is this technology? To answer some of those basic questions, we're joined by Alice O'Toole. She's a facial recognition expert at the University of Texas at Dallas. Welcome. ALICE O'TOOLE: Thank you. CHANG: Now, I feel like I'm seeing facial recognition pop up a lot more recently - not just on smartphones but in airport security systems, for example. Law enforcement has also experimented with it. Was there some sort of technological breakthrough that allowed wider use of this software? O'TOOLE: Yes, there has been a breakthrough. In the last five years or so, there's been a new algorithm. And it's modeled after the human visual system whereas older technology would do really quite well with well-controlled facial images - so when the illumination is good, and the viewpoint is frontal. This new kind of algorithm is much better at being able to generalize identity across variable images. So that crazy facial expression or that, you know, change in age and so on, these algorithms really have the potential to be much better at the task. CHANG: One recurring issue though we keep seeing is facial recognition technology that misidentifies people of color much more frequently than it misidentifies white people. There was that one pretty embarrassing incident where Google tagged faces of black people as gorillas. I'm just wondering, from a technological standpoint, why do mistakes like that happen? O'TOOLE: Yeah, this is a really excellent point. The current algorithms learn by example. And examples are basically many, many images of a particular person and an accurate label. And so you get what's out there. And what's out there may not be equally representative of different races. CHANG: So you're saying there are just a lot more photos of white people in the databases these pieces of software read? O'TOOLE: That's exactly right. CHANG: So how do you solve this problem technologically? How do you make the web, I guess, less racially biased? Do you just flood it with more photos of people of color? O'TOOLE: Yes. CHANG: Oh. O'TOOLE: The answer to the question technologically is quite simple. If you had equal representation of faces of different races, we would expect it would be equally accurate. How do you make the web un-race biased? That's a harder question. CHANG: So even when facial recognition software does work - when it is accurate, there are some troubling implications when it's linked to a database of images. For example, civil liberties groups worry that law enforcement could use this technology to target undocumented immigrants or to track protesters. As someone who really understands where this science is today, how do you think about this? O'TOOLE: Is face recognition technology abused? That's certainly a possibility. Any technology has the potential for abuse. Whether or not the potential for abuse of this technology outweighs its utility is something that has to be decided by societies in carefully looking at how it works and making decisions about what the right ways to use the technology are. CHANG: Alice O'Toole is head of the Face Perception Research Lab at the University of Texas at Dallas. Thank you very much. O'TOOLE: Thank you. AILSA CHANG, HOST:  It's time now for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CHANG: This month, we're looking at our bodies the way technology sees them. Here in the U. S. , we're gradually getting used to facial recognition technology. It's in our phones, in some airport security systems, on social media. But how good and how secure is this technology? To answer some of those basic questions, we're joined by Alice O'Toole. She's a facial recognition expert at the University of Texas at Dallas. Welcome. ALICE O'TOOLE: Thank you. CHANG: Now, I feel like I'm seeing facial recognition pop up a lot more recently - not just on smartphones but in airport security systems, for example. Law enforcement has also experimented with it. Was there some sort of technological breakthrough that allowed wider use of this software? O'TOOLE: Yes, there has been a breakthrough. In the last five years or so, there's been a new algorithm. And it's modeled after the human visual system whereas older technology would do really quite well with well-controlled facial images - so when the illumination is good, and the viewpoint is frontal. This new kind of algorithm is much better at being able to generalize identity across variable images. So that crazy facial expression or that, you know, change in age and so on, these algorithms really have the potential to be much better at the task. CHANG: One recurring issue though we keep seeing is facial recognition technology that misidentifies people of color much more frequently than it misidentifies white people. There was that one pretty embarrassing incident where Google tagged faces of black people as gorillas. I'm just wondering, from a technological standpoint, why do mistakes like that happen? O'TOOLE: Yeah, this is a really excellent point. The current algorithms learn by example. And examples are basically many, many images of a particular person and an accurate label. And so you get what's out there. And what's out there may not be equally representative of different races. CHANG: So you're saying there are just a lot more photos of white people in the databases these pieces of software read? O'TOOLE: That's exactly right. CHANG: So how do you solve this problem technologically? How do you make the web, I guess, less racially biased? Do you just flood it with more photos of people of color? O'TOOLE: Yes. CHANG: Oh. O'TOOLE: The answer to the question technologically is quite simple. If you had equal representation of faces of different races, we would expect it would be equally accurate. How do you make the web un-race biased? That's a harder question. CHANG: So even when facial recognition software does work - when it is accurate, there are some troubling implications when it's linked to a database of images. For example, civil liberties groups worry that law enforcement could use this technology to target undocumented immigrants or to track protesters. As someone who really understands where this science is today, how do you think about this? O'TOOLE: Is face recognition technology abused? That's certainly a possibility. Any technology has the potential for abuse. Whether or not the potential for abuse of this technology outweighs its utility is something that has to be decided by societies in carefully looking at how it works and making decisions about what the right ways to use the technology are. CHANG: Alice O'Toole is head of the Face Perception Research Lab at the University of Texas at Dallas. Thank you very much. O'TOOLE: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-08-654893289": {"title": "Why The Tech Industry Wants Federal Control Over Data Privacy Laws : NPR", "url": "https://www.npr.org/2018/10/08/654893289/why-the-tech-industry-wants-federal-control-over-data-privacy-laws", "author": "No author found", "published_date": "2018-10-08", "content": "DAVID GREENE, HOST: 2018 has been a big year for supporters of data privacy. Europe enacted a tough law in May, and then California passed comprehensive legislation in June. And tech companies are feeling the heat, so they are working behind the scenes on a federal privacy law. But they are not just trying to influence it. They're actually starting to write it. NPR's Dina Temple-Raston has the story. DINA TEMPLE-RASTON, BYLINE: Facebook is still assessing the fallout from the latest epic hack in which at least 50 million user accounts were compromised. We heard about it because in order to comply with Europe's new data privacy law, the company has to make hacks public within 72 hours of discovering them. A California privacy law, if it's enacted as written, would go even further and allow consumers to sue and potentially collect enormous damages for exactly this kind of data breach. Ernesto Falcon is with the digital advocacy group the Electronic Frontier Foundation. ERNESTO FALCON: They don't want to entertain the possibility that they would be liable to individuals for doing some sort of harm from all the data that they collect. TEMPLE-RASTON: Early this summer, a who's who in tech attended a high-level private meeting in San Francisco organized by the Information Technology Industry Council. According to two people with knowledge of the meeting, it was there that Facebook's top lobbyist, Joel Kaplan, warned the executives about the threat the California privacy law posed to all of them. If the California law spread to other states, he said, it would present an even bigger problem than Europe's privacy law. So companies have decided to weigh in before new laws start coming in from all fronts. Again, privacy advocate Falcon. FALCON: You have just this year a data broker law from Vermont. And then dating back even further, the state of Illinois has a biometric law that Facebook has opposed and has been trying to amend. TEMPLE-RASTON: The warning at the San Francisco meeting sparked an industry-wide effort to not just get behind federal privacy legislation but to actually write it. And while there's no one document that lays out their proposal yet, according to two people familiar with the process, the working drafts so far include two things, and the first, a pre-emption clause that would essentially override any privacy laws the states might pass, and the second, an agreement that enforcement of the law be left to the Federal Trade Commission. Ariel Fox Johnson of the advocacy group Common Sense Media says that while the FTC's a watchdog, it's not a very aggressive one. ARIEL FOX JOHNSON: So I don't know what the FTC can do besides, like, put out guides or try to go after people for violating statements that they've made in their privacy policies. (SOUNDBITE OF ARCHIVED RECORDING)JOHN THUNE: Good morning. A decade from now, we may look back and view this past year as a watershed with respect to the issue of consumer data privacy. TEMPLE-RASTON: Late last month, officials from Apple, Amazon, AT&T and Twitter testified before the Senate commerce committee about privacy and the need for a new federal law. (SOUNDBITE OF ARCHIVED RECORDING)JERRY MORAN: A yes or no question for each of you - would your company support federal legislation to pre-empt inconsistent state privacy laws? TEMPLE-RASTON: And all the executives said they would. (SOUNDBITE OF MONTAGE)LEN CALI: Yes, Senator. In. . . ANDREW DEVORE: Yes. UNIDENTIFIED PERSON: Yes, Senator. DAMIEN KIERAN: Yes, Senator, we would support. . . TEMPLE-RASTON: Tech companies are working the other end of Pennsylvania Avenue as well. Google CEO Sundar Pichai was at the White House just recently, and Trump economic adviser Larry Kudlow announced last week that tech executives would be back for a big meeting later in the month. (SOUNDBITE OF ARCHIVED RECORDING)LARRY KUDLOW: We're going to have a little conference. The president will preside over it. We will have the big Internet companies, the big social media companies. TEMPLE-RASTON: A reporter at the White House asked Kudlow if the invitation list would include big tech players like Facebook, Google and Twitter. Kudlow nodded and said, that's our hope. Dina Temple-Raston, NPR News. (SOUNDBITE OF WHYSP X PLAYER DAVE'S \"REVIVAL\") DAVID GREENE, HOST:  2018 has been a big year for supporters of data privacy. Europe enacted a tough law in May, and then California passed comprehensive legislation in June. And tech companies are feeling the heat, so they are working behind the scenes on a federal privacy law. But they are not just trying to influence it. They're actually starting to write it. NPR's Dina Temple-Raston has the story. DINA TEMPLE-RASTON, BYLINE: Facebook is still assessing the fallout from the latest epic hack in which at least 50 million user accounts were compromised. We heard about it because in order to comply with Europe's new data privacy law, the company has to make hacks public within 72 hours of discovering them. A California privacy law, if it's enacted as written, would go even further and allow consumers to sue and potentially collect enormous damages for exactly this kind of data breach. Ernesto Falcon is with the digital advocacy group the Electronic Frontier Foundation. ERNESTO FALCON: They don't want to entertain the possibility that they would be liable to individuals for doing some sort of harm from all the data that they collect. TEMPLE-RASTON: Early this summer, a who's who in tech attended a high-level private meeting in San Francisco organized by the Information Technology Industry Council. According to two people with knowledge of the meeting, it was there that Facebook's top lobbyist, Joel Kaplan, warned the executives about the threat the California privacy law posed to all of them. If the California law spread to other states, he said, it would present an even bigger problem than Europe's privacy law. So companies have decided to weigh in before new laws start coming in from all fronts. Again, privacy advocate Falcon. FALCON: You have just this year a data broker law from Vermont. And then dating back even further, the state of Illinois has a biometric law that Facebook has opposed and has been trying to amend. TEMPLE-RASTON: The warning at the San Francisco meeting sparked an industry-wide effort to not just get behind federal privacy legislation but to actually write it. And while there's no one document that lays out their proposal yet, according to two people familiar with the process, the working drafts so far include two things, and the first, a pre-emption clause that would essentially override any privacy laws the states might pass, and the second, an agreement that enforcement of the law be left to the Federal Trade Commission. Ariel Fox Johnson of the advocacy group Common Sense Media says that while the FTC's a watchdog, it's not a very aggressive one. ARIEL FOX JOHNSON: So I don't know what the FTC can do besides, like, put out guides or try to go after people for violating statements that they've made in their privacy policies. (SOUNDBITE OF ARCHIVED RECORDING) JOHN THUNE: Good morning. A decade from now, we may look back and view this past year as a watershed with respect to the issue of consumer data privacy. TEMPLE-RASTON: Late last month, officials from Apple, Amazon, AT&T and Twitter testified before the Senate commerce committee about privacy and the need for a new federal law. (SOUNDBITE OF ARCHIVED RECORDING) JERRY MORAN: A yes or no question for each of you - would your company support federal legislation to pre-empt inconsistent state privacy laws? TEMPLE-RASTON: And all the executives said they would. (SOUNDBITE OF MONTAGE) LEN CALI: Yes, Senator. In. . . ANDREW DEVORE: Yes. UNIDENTIFIED PERSON: Yes, Senator. DAMIEN KIERAN: Yes, Senator, we would support. . . TEMPLE-RASTON: Tech companies are working the other end of Pennsylvania Avenue as well. Google CEO Sundar Pichai was at the White House just recently, and Trump economic adviser Larry Kudlow announced last week that tech executives would be back for a big meeting later in the month. (SOUNDBITE OF ARCHIVED RECORDING) LARRY KUDLOW: We're going to have a little conference. The president will preside over it. We will have the big Internet companies, the big social media companies. TEMPLE-RASTON: A reporter at the White House asked Kudlow if the invitation list would include big tech players like Facebook, Google and Twitter. Kudlow nodded and said, that's our hope. Dina Temple-Raston, NPR News. (SOUNDBITE OF WHYSP X PLAYER DAVE'S \"REVIVAL\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-09-655947299": {"title": "Goodbye, Google+ \u2014 We Forgot You Existed : NPR", "url": "https://www.npr.org/2018/10/09/655947299/goodbye-google-we-forgot-you-existed", "author": "No author found", "published_date": "2018-10-09", "content": "MARY LOUISE KELLY, HOST: This week, Google disclosed a data breach, one that potentially affected hundreds of thousands of users. It was on the company's social media platform Google+. AILSA CHANG, HOST: This sounds familiar. But instead of announcing sweeping changes to the platform, the tech giant announced that it would simply close down Google+ by next year, which left many feeling. . . MIKE ISAAC: Mostly shocked that this was still online and up and running. KELLY: Mike Isaac, tech reporter for The New York Times. Now, if you haven't heard of Google+, you're probably not alone. But Isaac says back in 2011 when it was first launched, Google pitched it as the next Facebook, only more selective. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: I have lots to share, but I don't want to share everything with everyone. What if online sharing worked more like your real-life relationships where you choose who gets to know what? ISAAC: And it was essentially something that had scared Mark Zuckerberg even that Google is finally going to do a social network. CHANG: But the Facebook CEO didn't have to worry about that. ISAAC: Pretty much no one thought it was something (laughter) worth using. And everyone was already on Facebook, so why even switch over to this? And it became this thing that - they kept trying to keep making Google+ happen, and it just would never happen. CHANG: We did find one person who was bummed to see it go. AMANDA BLAIN: So I started on Google+ the first day that it officially launched. CHANG: Amanda Blain is a 38-year-old tech writer in Canada. She has almost 5 million Google+ followers. BLAIN: So I've never had that ghost-town, it-was-dead, nobody-used-it experience. It was never like that for me. So when it actually died, it has been kind of sad for those of us that actually had used it. KELLY: Well, Amanda Blain may be in the minority. Many people took to other social media sites to pay tribute to Google+, mostly to say that they had long forgotten about it. CHANG: Here's our favorite, a 36-year-old singer named Jonathan Mann. (SOUNDBITE OF ARCHIVED RECORDING)JONATHAN MANN: One, two, three, four - (singing) Google+ is dead. Nobody cares. CHANG: He says he wanted to highlight what Google+ could have been, the potential of a life cut short. (SOUNDBITE OF ARCHIVED RECORDING)MANN: (Singing) It would be nice, I'll admit, to have an alternative to Facebook but not from another great big company. Sadly this seems like it's not meant to be. KELLY: (Laughter) Well, here's to you, Google+. We hardly knew you. MARY LOUISE KELLY, HOST:  This week, Google disclosed a data breach, one that potentially affected hundreds of thousands of users. It was on the company's social media platform Google+. AILSA CHANG, HOST:  This sounds familiar. But instead of announcing sweeping changes to the platform, the tech giant announced that it would simply close down Google+ by next year, which left many feeling. . . MIKE ISAAC: Mostly shocked that this was still online and up and running. KELLY: Mike Isaac, tech reporter for The New York Times. Now, if you haven't heard of Google+, you're probably not alone. But Isaac says back in 2011 when it was first launched, Google pitched it as the next Facebook, only more selective. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: I have lots to share, but I don't want to share everything with everyone. What if online sharing worked more like your real-life relationships where you choose who gets to know what? ISAAC: And it was essentially something that had scared Mark Zuckerberg even that Google is finally going to do a social network. CHANG: But the Facebook CEO didn't have to worry about that. ISAAC: Pretty much no one thought it was something (laughter) worth using. And everyone was already on Facebook, so why even switch over to this? And it became this thing that - they kept trying to keep making Google+ happen, and it just would never happen. CHANG: We did find one person who was bummed to see it go. AMANDA BLAIN: So I started on Google+ the first day that it officially launched. CHANG: Amanda Blain is a 38-year-old tech writer in Canada. She has almost 5 million Google+ followers. BLAIN: So I've never had that ghost-town, it-was-dead, nobody-used-it experience. It was never like that for me. So when it actually died, it has been kind of sad for those of us that actually had used it. KELLY: Well, Amanda Blain may be in the minority. Many people took to other social media sites to pay tribute to Google+, mostly to say that they had long forgotten about it. CHANG: Here's our favorite, a 36-year-old singer named Jonathan Mann. (SOUNDBITE OF ARCHIVED RECORDING) JONATHAN MANN: One, two, three, four - (singing) Google+ is dead. Nobody cares. CHANG: He says he wanted to highlight what Google+ could have been, the potential of a life cut short. (SOUNDBITE OF ARCHIVED RECORDING) MANN: (Singing) It would be nice, I'll admit, to have an alternative to Facebook but not from another great big company. Sadly this seems like it's not meant to be. KELLY: (Laughter) Well, here's to you, Google+. We hardly knew you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-09-655880190": {"title": "Cyber Tests Showed 'Nearly All' New Pentagon Weapons Vulnerable To Attack, GAO Says : NPR", "url": "https://www.npr.org/2018/10/09/655880190/cyber-tests-showed-nearly-all-new-pentagon-weapons-vulnerable-to-attack-gao-says", "author": "No author found", "published_date": "2018-10-09", "content": "", "section": "Technology", "disclaimer": ""}, "2018-10-09-655793156": {"title": "Google+ Shutting Down For Consumers After Data Vulnerability Is Revealed : NPR", "url": "https://www.npr.org/2018/10/09/655793156/google-shuts-down-google-for-consumers-after-revealing-data-vulnerability", "author": "No author found", "published_date": "2018-10-09", "content": "", "section": "Technology", "disclaimer": ""}, "2018-10-09-655824435": {"title": "The 'Weaponization' Of Social Media \u2014 And Its Real-World Consequences : NPR", "url": "https://www.npr.org/2018/10/09/655824435/the-weaponization-of-social-media-and-its-real-world-consequences", "author": "No author found", "published_date": "2018-10-09", "content": "TERRY GROSS, HOST: This is FRESH AIR. I'm Terry Gross. The New York Times just reported that in 2016 Rick Gates, then-deputy campaign manager for Donald Trump, requested proposals from an Israeli company to create fake online identities and use social media manipulation to help defeat Trump's rivals for the Republican nomination in the primary and Hillary Clinton in the general election. The proposals were apparently never acted on, but we know that Russian operatives made effective use of Facebook and Twitter in seeking to influence the 2016 presidential campaign. Our country is hardly the only one in which political opinions have been manipulated by millions of fake social media accounts promoting messages that are slanted or even made up. Our guests today, P. W. Singer and Emerson Brooking, say that social media has also been weaponized in ways that most of us remain unaware of, like fueling popular uprisings and affecting the course of military campaigns. For instance, when the Israeli army moved into Gaza in 2012, the operation was supported by a viral marketing campaign. P. W. Singer is a strategist at New America, a consultant for the U. S. military and intelligence community and the author of several books. Emerson T. Brooking writes about conflict and social media and was recently a research fellow at the Council on Foreign Relations. FRESH AIR'S Dave Davies spoke to them about their new book, \"Like War: The Weaponization Of Social Media. \"DAVE DAVIES, BYLINE: Well, P. W. Singer and Emerson Brooking, welcome to FRESH AIR. You know, we all know that social media have become a big tool in politics, but you write in this book that it can affect not just opinions and reputations, but the course of war. And you cite a fascinating example, a new, rapid communications medium transforming a military offensive. This was the German invasion of France. What happened? P W SINGER: So you had this new technology that connected everything from tanks to airplanes to the broader population. It was wireless communication. And radio proved to be the secret sauce, so to speak, for everything from the political changes, the rise of Nazism. Goebbels said that it wouldn't have been possible what they did without the radio. But it also had strategic and tactical military impact. When the Germans invade France, an invasion that most people didn't - at the time, didn't think would work, it not only allows them to connect all their military units in a new manner, allow them to coordinate and move faster than ever before, but it also connected back into the enemy population, and it spread a contagion of fear. And it's one of these things that the contemporaries described as this strange defeat. They couldn't express what happened until they figured out, really, radio was what caused it all, caused the world to change. And we saw many of the same things play out in the rise of ISIS at a broader level, but also its military operation, its surprise invasion of Iraq that allowed it to defeat a force that was multiple times bigger. It's part of these ways that social media has, in effect, changed the world. DAVIES: Let's talk about that, the ISIS invasion of Iraq, and particularly when they were advancing on the city of Mosul. How did they use social media to their benefit? EMERSON BROOKING: There was a column of about 1,500 ISIS fighters who rolled into northern Iraq. And they were equipped with dusty pickup trucks and second hand AK-47s of these militant groups past. But the crucial difference was that they also broadcast their offensive. You know, if you look back into the history of military operations, typically if you're launching an invasion, you don't want the adversary to know about it. They wanted everyone to know about it. You had legions of fans and botnets on Twitter that coalesced around a hashtag, #AllEyesOnISIS. And they used that to propel their propaganda, images of what they were doing to the towns as they went through them. They broadcast this propaganda such that it rose to the top of Arabic-speaking Twitter and soon became - quickly became a topic of global conversation. And that contributed to the contagion of fear that swept through Mosul and led to something like 30,000 defenders retreating and leaving much of their equipment for ISIS to claim, which then fueled another propaganda victory, as suddenly they could broadcast using all this cast off American equipment. SINGER: And you had this wonderful double effect where at the local level, Iraqi soldiers are looking down at their smartphones, and they're seeing what seems like an ISIS victory playing out on the screens in their hands. And then it becomes one. It causes them to decide to run away, this army that's much larger, backed by the most powerful military in the world, the U. S. military. It has tanks. It has helicopters and the like. And yet what's playing out on social media changes the dynamics of the battlefield. But simultaneous to that, it changes the way the rest of us look at the world and fear it. You get this phenomena where more Americans are afraid of terrorism at that point than they were in the weeks after 9/11. And so it shows the power of social media and how it can sort of rewrite the narrative, and then in turn that incident is leveraged into playing out with impact on our own politics as part of the changes in the 2016 election. DAVIES: You also cite an example from another part of the world where social media can lead and it can affect events on the ground. And this is in Chicago, where social media taunts interact with gang violence in ways that can be deadly. You want to give us an example of that? BROOKING: Sure. We tell the story of Shaquon Thomas, who had grown up from an early age interested in music and was a very successful emerging rapper in the South Side of Chicago. But, unfortunately, he'd also been drawn into the gang scene. He was a Gangster Disciple, and he wanted everyone to know it. So he often, across Instagram and YouTube, would broadcast his affiliation with the Disciples. And he was the subject of not one or two but three assassination attempts by rival gang members. He survives the first two. And as he survives, rather than, you know, thinking twice or maybe laying low for a bit, he posts bold new videos on YouTube basically challenging these folks to come at him again. What we've seen in gang violence across this country is that much of it has migrated to social media because so much of being in a gang is about face and it's about fronting. It matters a great deal if someone, say, finds your Facebook account or trolls you or leaves a disrespectful Instagram comment. And something else that's happened as this gang violence moves the social media, is that it also means that violence is no longer about who's feuding for a particular block, but gangs of the same franchise, even if they're many miles apart, might start fighting each other over something that originated online. DAVIES: So if someone calls you out online, you are going to respond, and it may not - it may be online, but it may also be on the street. BROOKING: Right. SINGER: And that's the difference between gang feuds in the past, is if two people yelled words at each other in the street, only the people around them would hear it and the moment would be done. If you post it online, the whole world is watching - at least, in their perception - and it sticks up there until something is done about it. And that's why it's at the origin of so many of these fights. And not just - 80 percent of fights in schools spark from something online but also of course the spate of gang killings. And what's interesting and a little bit scary is that we see the same phenomena of Internet beefs at the gang level moving up to levels of drug cartels, all the way to how it's changed diplomacy, how it's changed politics. We have this question of, what happens when leaders of nations are interacting in the same way, when they're having these beefs in front of the world? Will it make it harder for them to negotiate if the whole world knows that they've been insulted and they've done nothing about it? DAVIES: You have an interesting kind of look at the development of mass communications. You say that the Internet is the most consequential communications development since the invention of the written word, and that it really got supercharged when everything could be done on smartphones. And there are many examples where social media have been used to fuel popular uprisings, the most striking example in the Arab Spring. At the time, a lot of people talked about social media as being this powerful, democratizing force. I mean, now, you know, expression is in the hands of everyone. The trouble is the authoritarian regimes also had access to the tool. How did they strike back? SINGER: The Arab Spring was probably the high point of the techno-optimism about the Internet and social media. You have everything from national media sources describing its liberating power, to people in Egypt are literally naming their kids Facebook, but then authoritarians figure it out. They figure how to fight back. We use this idea of like war to express that there's two sides to it. And so there's always a back and forth. And so what we've seen is that authoritarians have figured out also how to use social media in a couple of key ways. One is that it gives them a new way of monitoring their population and what they're thinking and what they're saying. For example, there's the question online, does retweet equal endorsement? Well, now we finally have the answer. The example we use is a journalist in Turkey who retweets something just for a couple of minutes before he takes it down, and yet it lands him in a Turkish prison. So governments can use the power of the law to monitor. Then you have this new model that China is presenting which is this almost perverse incentive system - it's called the social credit system - where essentially all your different online activities, everything from what you say to what you buy, can all be monitored and is brought together into a single score of your social trustworthiness. So for example, if you buy diapers, your score goes up because you're a good parent. If you play video games too long, your score goes down because you're screwing around. That score then is used to give you rewards in society, everything from free charges of your smartphone at coffee shops, to negative side. You can't take planes. It's used in job evaluations. It's even used in dating profiles, so it affects how attractive the person you're going to be able to go on dates or even marry. And what's notable about this - what goes in a way that Orwell never could have imagined is the network side of it. Your score reflects not only what you do but what everyone else in your network does. So if, for example, your brother is not being positive enough about the regime online, your score will go down. So then you'll go to your brother and say, hey, you know, get it together. And this is this strange way of using technology in essence to steer us to a behavior that at least in China the government wants. DAVIES: So people who have phones and do a lot of social media will have a social credit score. The aim is so that eventually everyone will get one. SINGER: The goal of it is to achieve what's known as mass control, to steer an entire population towards a certain direction. It's not just the idea that you will have your online activity but that the government will force you to have this kind of Internet presence. For example, there are certain places in China where there are police checkpoints, physical checkpoints where they will check your smartphone to ensure that you have the app that allows monitoring of you. DAVIES: Right. And just to get a sense of the reach, does - I mean, do we know how many people now have social credit scores in China? Is it hundreds of thousands, tens of millions? BROOKING: I believe it's tens of millions. This is a gradually escalating process with a couple different companies that are doing this piecemeal. But it's the stated intent of the Chinese Communist Party that this will be a unitary score in place for every Chinese Internet user, of which there are about 800 million, by the year 2020. DAVIES: Wow. The other fascinating story you tell about China is they realize how commentary on social media can affect opinion, and so they have a large army of people whose job it is to comment favorably on the government policies, right? BROOKING: That's right. And it is a very large army. It started out in the tens of thousands, moved to the hundreds of thousands. And now one estimate has it as high as 3 million of these folks who are known as part of the 50 Cent Army. And their job - and it has its own training programs and accreditation systems. Their job is to monitor and infiltrate political conversations on WeChat and other Chinese platforms and basically seed positive things about the Chinese government. DAVIES: Emerson Brooking writes about conflict and social media. He was recently a research fellow at the Council on Foreign Relations. P. W. Singer is a strategist at New America and a consultant to the U. S. military and intelligence community. Their new book is called \"LikeWar: The Weaponization Of Social Media. \" We'll continue our conversation in just a moment. This is FRESH AIR. (SOUNDBITE OF AVISHAI COHEN SONG, \"GBEDE TEMIN\")DAVIES: This is FRESH AIR, and we're speaking with P. W. Singer, a strategist at New America and a consultant to the U. S. military and intelligence community - also with Emerson Brooking. He writes about conflict and social media. Their new book is called \"LikeWar: The Weaponization Of Social Media. \"You look at the Israeli incursions into the Gaza Strip and its battles with Hamas, and you write that the Israeli military efforts were accompanied by a viral online marketing campaign. They were hip to the importance of this. What did they do? BROOKING: So it was actually the 2012 Pillar of Defense operation - eight days of Israeli airstrikes - that drew Peter and myself to this subject originally and got us researching it ever since. This military campaign was accompanied by a remarkable online viral marketing campaign. The campaign in 2012 opens with the assassination of a high-level Hamas militant. That's not particularly unusual. But what is unusual is that the drone strike video was up within a few hours, and the official Twitter account of the Israeli Defense Forces was broadcasting it out again and again. In time, Hamas Twitter proxies respond, threatening war and hellfire. And as the operation commenced, as Israel would strike particular targets, it would put out information about what had been done, about the weapons caches that had been recovered. Then Hamas would respond in a similarly taunting manner. And what's important is that this wasn't just the social media operatives of these two sides but also millions and millions of Twitter messages exchanged back and forth by the proxies of one side or another, 10 million in all in this first 2012 conflict, 90 percent of which were outside the region. And this was an example of a limited military campaign that nonetheless was a global war for public opinion. And this set a model that we're starting to see in all future conflicts. SINGER: And two things stand out about it. The first is that we could all participate. It was people outside the region deciding whose message went viral, whose side of the story was winning out. But importantly, it was not just winning out online, but it was also affecting the real battlefield decisions. When you go back and map out the targeting by Israeli airstrikes and the pace of them, essentially they changed by almost half dependent on which side was winning out online. So what was happening is that the politicians and the generals were making battlefield decisions while watching what was playing out on their smartphones, reflecting what you and I were clicking on. DAVIES: You know, you make the point again that scale matters. If you put up an effective post on Twitter or Facebook or Instagram, that's one thing. If you have an army of people organized to retweet it and magnify its effect, that's all the more important. Did the Israelis anticipate this and have their own cadre of social media users ready to give their message more residents? SINGER: So originally the Israeli military is not ready for this whole space. And they're quite taken aback by how they're losing this battle of narrative but with very real physical and political effects. And like any other military, they learn - they reorganize. And they reorganize in everything from creating new military units. In fact, one of the fun images that you can see online is how they've changed their recruiting to let young Israelis know that you can do your military service by serving in one of these new units that tries to affect the online battle. Instead of, you know, carrying a gun, you can be using Internet memes and the like. But it's also woven into the broader population. For example, they create organizations within Israeli universities where not only Israeli students but students from around the world can participate in this battle of narrative too. They have created apps that allow you and I to download them and serve the cause and, you know, gain badges and the like. And in many ways, what Israel has done has created a model for other militaries around the world that are looking at this and saying, maybe we need to engage in this kind of reorganization too. And you see it everywhere from Europe to some discussion within the U. S. military. DAVIES: So you can enlist in an online army and get, you know, badges, promotions? SINGER: The same way that you can enlist in Taylor Swift's army of Swifties (ph), you can enlist in the Israeli Defense Forces online army. The cause may not be the same, but the tactics are. And by contrast with the Israeli approach, which is fairly centralized - you know, giving you a mission, giving a set of messages to push - the Hamas one pulls from the world. It crowd sources, so to speak. And one of the other tools of the trade - one of the tricks that is used in these, but also in political campaigns to marketing, is basically to find out what's trending elsewhere and try and layer on to it. So for example, you had the ice bucket challenge a couple of years ago where we were all, you know, drawing attention to ourselves as a way to fundraise for disease research. This was woven into this battlefield where people were doing the ice bucket challenge but using rubble after Israeli airstrikes. DAVIES: You would pour rubble on your head? SINGER: You would pour rubble on your head, and then the image would go viral. And it was a way of kind of hijacking one discussion to drive it to your own ends - the same way that ISIS, for example, would hijack discussion about #WorldCup and jump into it - the same way that Trump's online army would try and hijack discussion that was trending for Hillary or the like. GROSS: Were listening to the interview FRESH AIR's Dave Davies recorded with P. W. Singer and Emerson Brooking, authors of the new book \"LikeWar: The Weaponization Of Social Media. \" They'll talk about social media and political campaigns after a break. And John Powers will review the new film \"22 July,\" based on the story of a terrorist attack on a summer camp in Norway in 2011. I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF MUSIC)GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to the interview FRESH AIR's Dave Davies recorded with P. W. Singer and Emerson Brooking about their new book \"LikeWar: The Weaponization Of Social Media. \" It's about how social media has been weaponized for political campaigns, uprisings and wars. DAVIES: Let's talk about social media in political campaigns. There's been a lot of attention paid to the Russian efforts to interfere with the 2016 elections. And you describe the appearance in 2017 of somebody on social media named Angie Dixon (ph). She establishes a Twitter account. What was her agenda? What do we know about her? SINGER: So Angie Dixon presents herself to the world as a Christian American who's mad about the direction that our country has taken, and she wants her country back. Make America great again. And she's pushing out messaging to that effect. And particularly around the period of Charlottesville, she is so angry that right-wingers are being blamed for this. And she says it's this Internet conspiracy and the like. And, of course, Angie is a fake. And how do we know that Angie is a fake? Angie is presenting herself with imagery that is actually Leonardo DiCaprio's German girlfriend. And what's interesting is that when reports then circulate online that Angie is a fake, she then attacks the people going after her - basically to say that they're part of this conspiracy theory. And you get this strange outcome where her fake messages get greater traction than the messages debunking her fakeness. DAVIES: So Angie Dixon was a fake, but she attracted a lot of - quite a following on social media. Was that because she was so compelling? Or was it because there were many, many other fake accounts or robotic accounts that were retweeting her stuff? SINGER: So Angie was a bot. She was a machine voice masquerading as a human online. And each of those is powerful in and of themselves. But where they gain their true power is creating what are known as bot nets. So for example, Angie was only one of at least 60,000 Russian accounts in a single bot net that infects Twitter almost like a cancer. And it's warping and twisting the American political dialogue. But what's fascinating is that it's not just about persuading the targets. It's about hijacking the network's own algorithms. So these bot nets drive viral different points of view that then push that up into the news feed. It makes them trend, so people even outside those networks begin to see them. And the impact of this can be seen in examples that range from Brexit - about one-third of the online conversation related to Brexit were these artificial voices - to even just a couple of months ago, the Mexican election - same percentage. One-third were these artificial voices. And, of course, they played a huge impact in the American 2016 election. Many people think it's going to do the same in the upcoming vote. DAVIES: Right. So just to be sure we understand this. We all know that you can establish a fake account, make up a name and get on a social media platform like Twitter. And there are a lot of those. But you're saying this 60,000 were actually robotically generated accounts generating their messages. Is that right? SINGER: Yes. One of the challenges of this space is that we don't have a good handle on the lingo. So you'll hear a lot of discussion of Russian trolls. Well, trolls are basically, you know, people online who are trying to provoke some kind of emotional response - usually to make you angry. But when we're talking about the Russian operations, they broke into two parts. There were sock puppets. Those are real humans behind the account who are posing as something they're not. So it's some Russian hipster in St. Petersburg who is acting like they are an American veteran, a grandmother from Texas. But then you have also bots. These are machines. These are algorithms that are basically pushing out messages automatically. The combination of the two can be incredibly powerful because the sock puppet can say one thing, and then the bots can drive that message viral by having thousands or tens of thousands of voices echo it out further. BROOKING: You know, repeatedly these social media platforms, like Twitter, get rid of these accounts, but they have ways to automatically regenerate. And what's worrying, as we look ahead, is that these bots don't necessarily need to be active now. Something that these Russian operatives - but anyone who wants to manipulate social media conversation - something they're increasingly doing is creating accounts and having them lie dormant and have a history of messages that seem authentic and benign and not related to a hot political event. So when they activate them months or years down the road, it will be much harder to detect them as something other than a real person. DAVIES: You said an amazing number here. You say that when Donald Trump announced for president in 2015, 58 percent of his Facebook followers were from outside the United States, right? How do we know this? SINGER: So you're able to detect the origin of accounts. One of the new rules of the game when everything is online is that it's out in the open. So you can follow it. You can see the patterns of behavior. Maybe one of the more notable aspects of Donald Trump's follower count is that at that point in time about 4 percent supposedly lived in Mexico. Now, of course, these were not real Mexican citizens behind these accounts. These were in essence fake followers created to give the semblance of a massive online following that can then be turned into a real online following by driving your message viral. DAVIES: Right. But if 58 percent of the followers from outside the U. S. , can we infer that a lot of them are fake? SINGER: Yes, a vast majority of them were. They were basically bought accounts generated out of what are known as click farms. These are almost - you think of like a sweatshop factory. It's the same thing for people that are essentially being paid to click on accounts in places like the Philippines or Bangladesh. And then you combine these physical people with bots, and it can give you a much greater power in politics. DAVIES: You know, a lot's been written about the Russian efforts to interfere in the American 2016 election. We've seen indictments. Give us your sense of the scale and impact of this effort. SINGER: The challenge with this is that the information about it came out in little dribs and drabs and the result is that the scale of it is not appreciated by most people - just how big in number but in extent the campaign was. So, for example, if you're looking at Facebook, it now belatedly says, you know, roughly over 140 million Americans, about half the population, saw some aspect of Russian information - misinformation pushed online. You move over to other platforms, like, for example, Twitter, where it's literally hundreds of millions of messages being retweeted back and forth. A great illustration of this would be one single account - Tennessee GOP that was posing as if it was the hub for Tennessee Republicans. We now know that it was one of these Russians sitting in St. Petersburg. It was the seventh most read account on election day - not the seventh most read of the thousands of Russian accounts but the seventh most read overall. You saw similar campaigns playing out on Instagram too. There's actually one that was just detected a couple of weeks ago on Reddit. And the impact that this was not just what the Russians were pushing but the echo effect and to the broader population, including the media itself. One of the accounts that was posing as a young American woman, she was actually quoted in everything from Washington Post to New York Times to USA Today and the like as if she was a real young American woman. And actually, again, it was one of these Russians behind it. And so, you know, it's up for the historians to, you know, argue about whether it swung the election or not in such a close election. What we can say is the scale of that was immense and that the Russians believe it worked because they're coming back for more. They're doing it in the election right now. DAVIES: Emerson Brooking writes about conflict and social media. P. W. Singer is a strategist at New America. Their new book is called \"LikeWar: The Weaponization Of Social Media. \" We'll continue our conversation after a short break. This is FRESH AIR. (SOUNDBITE OF NOAM WIESENBERG'S \"DAVKA\")DAVIES: This is FRESH AIR, and we're speaking with Emerson Brooking. He writes about conflict and social media, was recently a research fellow at the Council on Foreign Relations. Also with us, P. W. Singer, a strategist at New America and a consultant to the U. S. military and intelligence community. Their new book is called \"LikeWar: The Weaponization Of Social Media. \"I'm interested in your sense of how social media wars affected opinions about the Kavanaugh confirmation controversy. I mean, this was a case where there were facts in dispute, a lot of strongly held opinions. Do you have any sense of - I mean, there must've been social media campaigns - what kind of impact they had on how this was perceived and, you know, the enduring impact it'll have on our discourse? SINGER: If you were watching your social media feed during the Kavanaugh hearings, in many ways it felt like a war going back and forth. And it was in a certain way. You had both sides using the exact same tactics of, you know, what we call LikeWar that militaries were using all around the world. You could see everything from open-source intelligence gathering campaigns where they were enlisting the online crowd to find bits of information to bring it together that wouldn't have previously been possible. So for example, on one side, you had a crowdsource hunt using images from Washington Nationals' baseball games to try and figure out who sat next to Kavanaugh to unpack this strange story of his $200,000 in baseball ticket debt. Then you add the other side, this combination of Google Earth images and Zillow real estate data to basically push this narrative, the doppelganger narrative, that Dr. Ford - the claim was, well, she's not a liar, but she's confused about who it was. Now, that narrative that was pushed online points to a second thing that played out - the spread of disinformation, the spread of alternative theories, the idea of burying the truth underneath a sea of lies. It's something that, for example, Russia is specialized in. And again, you saw both sides engaging in this in the Kavanaugh debate, pushing out all sorts of different false stories, false personas and the like in the battle to have their own narrative win out and the other side's narrative be buried underneath this. You even saw attempts to change not just your point of view but history itself for at least the online place that we go to establish and learn about history, Wikipedia. So for example in the book, we had the story of how after the Russian shoot-down of the airliner over Ukraine, Russian actors go onto Wikipedia very quickly to change the story to make it seem like they weren't involved. The same thing played out during the Kavanaugh hearing where he used a term in his - a high school yearbook that was a sexual term. And during his hearing, he says no, no, no. It's not a sexual term. It's about a drinking game. The problem for him is that there's literally no evidence on the entire Internet of that being a drinking game until someone within the House of Representatives - again, everything is out in the open so we can geolocate it to that. During the middle of the hearing, someone in the House of Representatives goes to create the evidence on Wikipedia to make it seem as if he is telling the truth. So you have this back-and-forth, back-and-forth. And it's our contention that every single political debate moving forward is going to see these kind of tactics utilized again because both sides not only use them but believe that they were the key to their effort, including in winning. DAVIES: You write that the social media platforms are private companies. And they're now coming to terms with the fact that they play, like it or not, a very powerful role in elections, in the national discourse and have to take some responsibility for dealing with this content - which is a huge challenge just because of the sheer volume, billions of posts a day and, you know, the subtle task of figuring out what is acceptable and what is not and that there is a developing tool called neural networks. Explain what they are, what they can do. SINGER: So neural networks is part of this larger area of research that people will talk about as - essentially artificial intelligence, using machines to not just mimic but maybe in some ways surpass human intelligence and thinking. And so neural networks can be used, like any other technology, for both good and bad. So for example, they can create wonderful imagery and scenes, images of a volcano that doesn't exist in the real world. And it's very powerful in use of, for example, the future of marketing. It's going to move into areas like customer service. The companies see this also as a way to save money, for example, if you're interacting with a machine rather than someone behind a help desk. But like everything else, it's also going to be weaponized. And so those very same chat bots, for example, that might be trying to persuade you to buy a product might be trying to persuade you to vote in a certain way or convince someone to take a certain position on a war a certain way. The challenge also moves into visual imagery. There's an area known as deep fakes. These are hyper-realistic images - videos, for example, of speeches that politicians never made, scenes that never happened. And so with humans finding it harder and harder to figure out what is real or not, the companies in turn are looking to artificial intelligence to help find it, to help police their networks. And so you get this strange but yet kind of wonderfully appropriate outcome. The origin of social networks online were a bunch of scientists talking about science fiction. That's where this all came from. And now we have this science fiction-like outcome emerging where you have two AI battling back and forth with us humans caught in the middle. DAVIES: Right. So these neural networks, like, could be good at subtly detecting what a robotic Twitter account might be in a way that would be hard for humans. SINGER: Exactly. DAVIES: But they can also produce a picture of - you know, name somebody - Joe Biden, you know, uttering some, you know, profane comment at something. SINGER: To give real-world examples of this, there have already been creations of speeches that Barack Obama never gave, or there's other ones where you can layer real and fake imagery together where, for example, it can be your facial expressions, but you're controlling an online image of everything from - and the example's Arnold Schwarzenegger to George W. Bush. Basically anyone that you have enough data points on what their face looks like, which, oh, by the way you can get off of Facebook, you can take that and meld it together to allow you to in essence create a fake scene of them saying or doing something. And so just like everything else in this space, it's going to be used for good, for jokes, for laughs. It's going to be monetized for profit. And it's also going to be weaponized. DAVIES: Emerson Brooking, P. W. Singer, thanks so much for speaking with us. SINGER: Appreciate it. Thank you. BROOKING: Thank you. GROSS: P. W. Singer and Emerson Brooking are the authors of \"LikeWar: The Weaponization Of Social Media. \" They spoke with FRESH AIR's Dave Davies, who is also WHYY's senior reporter. After a break, John Powers will review the new film \"22 July\" based on the story of a terrorist attack on a summer camp in Norway in 2011. This is FRESH AIR. (SOUNDBITE OF STEFON HARRIS' \"UNTIL\") TERRY GROSS, HOST:  This is FRESH AIR. I'm Terry Gross. The New York Times just reported that in 2016 Rick Gates, then-deputy campaign manager for Donald Trump, requested proposals from an Israeli company to create fake online identities and use social media manipulation to help defeat Trump's rivals for the Republican nomination in the primary and Hillary Clinton in the general election. The proposals were apparently never acted on, but we know that Russian operatives made effective use of Facebook and Twitter in seeking to influence the 2016 presidential campaign. Our country is hardly the only one in which political opinions have been manipulated by millions of fake social media accounts promoting messages that are slanted or even made up. Our guests today, P. W. Singer and Emerson Brooking, say that social media has also been weaponized in ways that most of us remain unaware of, like fueling popular uprisings and affecting the course of military campaigns. For instance, when the Israeli army moved into Gaza in 2012, the operation was supported by a viral marketing campaign. P. W. Singer is a strategist at New America, a consultant for the U. S. military and intelligence community and the author of several books. Emerson T. Brooking writes about conflict and social media and was recently a research fellow at the Council on Foreign Relations. FRESH AIR'S Dave Davies spoke to them about their new book, \"Like War: The Weaponization Of Social Media. \" DAVE DAVIES, BYLINE: Well, P. W. Singer and Emerson Brooking, welcome to FRESH AIR. You know, we all know that social media have become a big tool in politics, but you write in this book that it can affect not just opinions and reputations, but the course of war. And you cite a fascinating example, a new, rapid communications medium transforming a military offensive. This was the German invasion of France. What happened? P W SINGER: So you had this new technology that connected everything from tanks to airplanes to the broader population. It was wireless communication. And radio proved to be the secret sauce, so to speak, for everything from the political changes, the rise of Nazism. Goebbels said that it wouldn't have been possible what they did without the radio. But it also had strategic and tactical military impact. When the Germans invade France, an invasion that most people didn't - at the time, didn't think would work, it not only allows them to connect all their military units in a new manner, allow them to coordinate and move faster than ever before, but it also connected back into the enemy population, and it spread a contagion of fear. And it's one of these things that the contemporaries described as this strange defeat. They couldn't express what happened until they figured out, really, radio was what caused it all, caused the world to change. And we saw many of the same things play out in the rise of ISIS at a broader level, but also its military operation, its surprise invasion of Iraq that allowed it to defeat a force that was multiple times bigger. It's part of these ways that social media has, in effect, changed the world. DAVIES: Let's talk about that, the ISIS invasion of Iraq, and particularly when they were advancing on the city of Mosul. How did they use social media to their benefit? EMERSON BROOKING: There was a column of about 1,500 ISIS fighters who rolled into northern Iraq. And they were equipped with dusty pickup trucks and second hand AK-47s of these militant groups past. But the crucial difference was that they also broadcast their offensive. You know, if you look back into the history of military operations, typically if you're launching an invasion, you don't want the adversary to know about it. They wanted everyone to know about it. You had legions of fans and botnets on Twitter that coalesced around a hashtag, #AllEyesOnISIS. And they used that to propel their propaganda, images of what they were doing to the towns as they went through them. They broadcast this propaganda such that it rose to the top of Arabic-speaking Twitter and soon became - quickly became a topic of global conversation. And that contributed to the contagion of fear that swept through Mosul and led to something like 30,000 defenders retreating and leaving much of their equipment for ISIS to claim, which then fueled another propaganda victory, as suddenly they could broadcast using all this cast off American equipment. SINGER: And you had this wonderful double effect where at the local level, Iraqi soldiers are looking down at their smartphones, and they're seeing what seems like an ISIS victory playing out on the screens in their hands. And then it becomes one. It causes them to decide to run away, this army that's much larger, backed by the most powerful military in the world, the U. S. military. It has tanks. It has helicopters and the like. And yet what's playing out on social media changes the dynamics of the battlefield. But simultaneous to that, it changes the way the rest of us look at the world and fear it. You get this phenomena where more Americans are afraid of terrorism at that point than they were in the weeks after 9/11. And so it shows the power of social media and how it can sort of rewrite the narrative, and then in turn that incident is leveraged into playing out with impact on our own politics as part of the changes in the 2016 election. DAVIES: You also cite an example from another part of the world where social media can lead and it can affect events on the ground. And this is in Chicago, where social media taunts interact with gang violence in ways that can be deadly. You want to give us an example of that? BROOKING: Sure. We tell the story of Shaquon Thomas, who had grown up from an early age interested in music and was a very successful emerging rapper in the South Side of Chicago. But, unfortunately, he'd also been drawn into the gang scene. He was a Gangster Disciple, and he wanted everyone to know it. So he often, across Instagram and YouTube, would broadcast his affiliation with the Disciples. And he was the subject of not one or two but three assassination attempts by rival gang members. He survives the first two. And as he survives, rather than, you know, thinking twice or maybe laying low for a bit, he posts bold new videos on YouTube basically challenging these folks to come at him again. What we've seen in gang violence across this country is that much of it has migrated to social media because so much of being in a gang is about face and it's about fronting. It matters a great deal if someone, say, finds your Facebook account or trolls you or leaves a disrespectful Instagram comment. And something else that's happened as this gang violence moves the social media, is that it also means that violence is no longer about who's feuding for a particular block, but gangs of the same franchise, even if they're many miles apart, might start fighting each other over something that originated online. DAVIES: So if someone calls you out online, you are going to respond, and it may not - it may be online, but it may also be on the street. BROOKING: Right. SINGER: And that's the difference between gang feuds in the past, is if two people yelled words at each other in the street, only the people around them would hear it and the moment would be done. If you post it online, the whole world is watching - at least, in their perception - and it sticks up there until something is done about it. And that's why it's at the origin of so many of these fights. And not just - 80 percent of fights in schools spark from something online but also of course the spate of gang killings. And what's interesting and a little bit scary is that we see the same phenomena of Internet beefs at the gang level moving up to levels of drug cartels, all the way to how it's changed diplomacy, how it's changed politics. We have this question of, what happens when leaders of nations are interacting in the same way, when they're having these beefs in front of the world? Will it make it harder for them to negotiate if the whole world knows that they've been insulted and they've done nothing about it? DAVIES: You have an interesting kind of look at the development of mass communications. You say that the Internet is the most consequential communications development since the invention of the written word, and that it really got supercharged when everything could be done on smartphones. And there are many examples where social media have been used to fuel popular uprisings, the most striking example in the Arab Spring. At the time, a lot of people talked about social media as being this powerful, democratizing force. I mean, now, you know, expression is in the hands of everyone. The trouble is the authoritarian regimes also had access to the tool. How did they strike back? SINGER: The Arab Spring was probably the high point of the techno-optimism about the Internet and social media. You have everything from national media sources describing its liberating power, to people in Egypt are literally naming their kids Facebook, but then authoritarians figure it out. They figure how to fight back. We use this idea of like war to express that there's two sides to it. And so there's always a back and forth. And so what we've seen is that authoritarians have figured out also how to use social media in a couple of key ways. One is that it gives them a new way of monitoring their population and what they're thinking and what they're saying. For example, there's the question online, does retweet equal endorsement? Well, now we finally have the answer. The example we use is a journalist in Turkey who retweets something just for a couple of minutes before he takes it down, and yet it lands him in a Turkish prison. So governments can use the power of the law to monitor. Then you have this new model that China is presenting which is this almost perverse incentive system - it's called the social credit system - where essentially all your different online activities, everything from what you say to what you buy, can all be monitored and is brought together into a single score of your social trustworthiness. So for example, if you buy diapers, your score goes up because you're a good parent. If you play video games too long, your score goes down because you're screwing around. That score then is used to give you rewards in society, everything from free charges of your smartphone at coffee shops, to negative side. You can't take planes. It's used in job evaluations. It's even used in dating profiles, so it affects how attractive the person you're going to be able to go on dates or even marry. And what's notable about this - what goes in a way that Orwell never could have imagined is the network side of it. Your score reflects not only what you do but what everyone else in your network does. So if, for example, your brother is not being positive enough about the regime online, your score will go down. So then you'll go to your brother and say, hey, you know, get it together. And this is this strange way of using technology in essence to steer us to a behavior that at least in China the government wants. DAVIES: So people who have phones and do a lot of social media will have a social credit score. The aim is so that eventually everyone will get one. SINGER: The goal of it is to achieve what's known as mass control, to steer an entire population towards a certain direction. It's not just the idea that you will have your online activity but that the government will force you to have this kind of Internet presence. For example, there are certain places in China where there are police checkpoints, physical checkpoints where they will check your smartphone to ensure that you have the app that allows monitoring of you. DAVIES: Right. And just to get a sense of the reach, does - I mean, do we know how many people now have social credit scores in China? Is it hundreds of thousands, tens of millions? BROOKING: I believe it's tens of millions. This is a gradually escalating process with a couple different companies that are doing this piecemeal. But it's the stated intent of the Chinese Communist Party that this will be a unitary score in place for every Chinese Internet user, of which there are about 800 million, by the year 2020. DAVIES: Wow. The other fascinating story you tell about China is they realize how commentary on social media can affect opinion, and so they have a large army of people whose job it is to comment favorably on the government policies, right? BROOKING: That's right. And it is a very large army. It started out in the tens of thousands, moved to the hundreds of thousands. And now one estimate has it as high as 3 million of these folks who are known as part of the 50 Cent Army. And their job - and it has its own training programs and accreditation systems. Their job is to monitor and infiltrate political conversations on WeChat and other Chinese platforms and basically seed positive things about the Chinese government. DAVIES: Emerson Brooking writes about conflict and social media. He was recently a research fellow at the Council on Foreign Relations. P. W. Singer is a strategist at New America and a consultant to the U. S. military and intelligence community. Their new book is called \"LikeWar: The Weaponization Of Social Media. \" We'll continue our conversation in just a moment. This is FRESH AIR. (SOUNDBITE OF AVISHAI COHEN SONG, \"GBEDE TEMIN\") DAVIES: This is FRESH AIR, and we're speaking with P. W. Singer, a strategist at New America and a consultant to the U. S. military and intelligence community - also with Emerson Brooking. He writes about conflict and social media. Their new book is called \"LikeWar: The Weaponization Of Social Media. \" You look at the Israeli incursions into the Gaza Strip and its battles with Hamas, and you write that the Israeli military efforts were accompanied by a viral online marketing campaign. They were hip to the importance of this. What did they do? BROOKING: So it was actually the 2012 Pillar of Defense operation - eight days of Israeli airstrikes - that drew Peter and myself to this subject originally and got us researching it ever since. This military campaign was accompanied by a remarkable online viral marketing campaign. The campaign in 2012 opens with the assassination of a high-level Hamas militant. That's not particularly unusual. But what is unusual is that the drone strike video was up within a few hours, and the official Twitter account of the Israeli Defense Forces was broadcasting it out again and again. In time, Hamas Twitter proxies respond, threatening war and hellfire. And as the operation commenced, as Israel would strike particular targets, it would put out information about what had been done, about the weapons caches that had been recovered. Then Hamas would respond in a similarly taunting manner. And what's important is that this wasn't just the social media operatives of these two sides but also millions and millions of Twitter messages exchanged back and forth by the proxies of one side or another, 10 million in all in this first 2012 conflict, 90 percent of which were outside the region. And this was an example of a limited military campaign that nonetheless was a global war for public opinion. And this set a model that we're starting to see in all future conflicts. SINGER: And two things stand out about it. The first is that we could all participate. It was people outside the region deciding whose message went viral, whose side of the story was winning out. But importantly, it was not just winning out online, but it was also affecting the real battlefield decisions. When you go back and map out the targeting by Israeli airstrikes and the pace of them, essentially they changed by almost half dependent on which side was winning out online. So what was happening is that the politicians and the generals were making battlefield decisions while watching what was playing out on their smartphones, reflecting what you and I were clicking on. DAVIES: You know, you make the point again that scale matters. If you put up an effective post on Twitter or Facebook or Instagram, that's one thing. If you have an army of people organized to retweet it and magnify its effect, that's all the more important. Did the Israelis anticipate this and have their own cadre of social media users ready to give their message more residents? SINGER: So originally the Israeli military is not ready for this whole space. And they're quite taken aback by how they're losing this battle of narrative but with very real physical and political effects. And like any other military, they learn - they reorganize. And they reorganize in everything from creating new military units. In fact, one of the fun images that you can see online is how they've changed their recruiting to let young Israelis know that you can do your military service by serving in one of these new units that tries to affect the online battle. Instead of, you know, carrying a gun, you can be using Internet memes and the like. But it's also woven into the broader population. For example, they create organizations within Israeli universities where not only Israeli students but students from around the world can participate in this battle of narrative too. They have created apps that allow you and I to download them and serve the cause and, you know, gain badges and the like. And in many ways, what Israel has done has created a model for other militaries around the world that are looking at this and saying, maybe we need to engage in this kind of reorganization too. And you see it everywhere from Europe to some discussion within the U. S. military. DAVIES: So you can enlist in an online army and get, you know, badges, promotions? SINGER: The same way that you can enlist in Taylor Swift's army of Swifties (ph), you can enlist in the Israeli Defense Forces online army. The cause may not be the same, but the tactics are. And by contrast with the Israeli approach, which is fairly centralized - you know, giving you a mission, giving a set of messages to push - the Hamas one pulls from the world. It crowd sources, so to speak. And one of the other tools of the trade - one of the tricks that is used in these, but also in political campaigns to marketing, is basically to find out what's trending elsewhere and try and layer on to it. So for example, you had the ice bucket challenge a couple of years ago where we were all, you know, drawing attention to ourselves as a way to fundraise for disease research. This was woven into this battlefield where people were doing the ice bucket challenge but using rubble after Israeli airstrikes. DAVIES: You would pour rubble on your head? SINGER: You would pour rubble on your head, and then the image would go viral. And it was a way of kind of hijacking one discussion to drive it to your own ends - the same way that ISIS, for example, would hijack discussion about #WorldCup and jump into it - the same way that Trump's online army would try and hijack discussion that was trending for Hillary or the like. GROSS: Were listening to the interview FRESH AIR's Dave Davies recorded with P. W. Singer and Emerson Brooking, authors of the new book \"LikeWar: The Weaponization Of Social Media. \" They'll talk about social media and political campaigns after a break. And John Powers will review the new film \"22 July,\" based on the story of a terrorist attack on a summer camp in Norway in 2011. I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF MUSIC) GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to the interview FRESH AIR's Dave Davies recorded with P. W. Singer and Emerson Brooking about their new book \"LikeWar: The Weaponization Of Social Media. \" It's about how social media has been weaponized for political campaigns, uprisings and wars. DAVIES: Let's talk about social media in political campaigns. There's been a lot of attention paid to the Russian efforts to interfere with the 2016 elections. And you describe the appearance in 2017 of somebody on social media named Angie Dixon (ph). She establishes a Twitter account. What was her agenda? What do we know about her? SINGER: So Angie Dixon presents herself to the world as a Christian American who's mad about the direction that our country has taken, and she wants her country back. Make America great again. And she's pushing out messaging to that effect. And particularly around the period of Charlottesville, she is so angry that right-wingers are being blamed for this. And she says it's this Internet conspiracy and the like. And, of course, Angie is a fake. And how do we know that Angie is a fake? Angie is presenting herself with imagery that is actually Leonardo DiCaprio's German girlfriend. And what's interesting is that when reports then circulate online that Angie is a fake, she then attacks the people going after her - basically to say that they're part of this conspiracy theory. And you get this strange outcome where her fake messages get greater traction than the messages debunking her fakeness. DAVIES: So Angie Dixon was a fake, but she attracted a lot of - quite a following on social media. Was that because she was so compelling? Or was it because there were many, many other fake accounts or robotic accounts that were retweeting her stuff? SINGER: So Angie was a bot. She was a machine voice masquerading as a human online. And each of those is powerful in and of themselves. But where they gain their true power is creating what are known as bot nets. So for example, Angie was only one of at least 60,000 Russian accounts in a single bot net that infects Twitter almost like a cancer. And it's warping and twisting the American political dialogue. But what's fascinating is that it's not just about persuading the targets. It's about hijacking the network's own algorithms. So these bot nets drive viral different points of view that then push that up into the news feed. It makes them trend, so people even outside those networks begin to see them. And the impact of this can be seen in examples that range from Brexit - about one-third of the online conversation related to Brexit were these artificial voices - to even just a couple of months ago, the Mexican election - same percentage. One-third were these artificial voices. And, of course, they played a huge impact in the American 2016 election. Many people think it's going to do the same in the upcoming vote. DAVIES: Right. So just to be sure we understand this. We all know that you can establish a fake account, make up a name and get on a social media platform like Twitter. And there are a lot of those. But you're saying this 60,000 were actually robotically generated accounts generating their messages. Is that right? SINGER: Yes. One of the challenges of this space is that we don't have a good handle on the lingo. So you'll hear a lot of discussion of Russian trolls. Well, trolls are basically, you know, people online who are trying to provoke some kind of emotional response - usually to make you angry. But when we're talking about the Russian operations, they broke into two parts. There were sock puppets. Those are real humans behind the account who are posing as something they're not. So it's some Russian hipster in St. Petersburg who is acting like they are an American veteran, a grandmother from Texas. But then you have also bots. These are machines. These are algorithms that are basically pushing out messages automatically. The combination of the two can be incredibly powerful because the sock puppet can say one thing, and then the bots can drive that message viral by having thousands or tens of thousands of voices echo it out further. BROOKING: You know, repeatedly these social media platforms, like Twitter, get rid of these accounts, but they have ways to automatically regenerate. And what's worrying, as we look ahead, is that these bots don't necessarily need to be active now. Something that these Russian operatives - but anyone who wants to manipulate social media conversation - something they're increasingly doing is creating accounts and having them lie dormant and have a history of messages that seem authentic and benign and not related to a hot political event. So when they activate them months or years down the road, it will be much harder to detect them as something other than a real person. DAVIES: You said an amazing number here. You say that when Donald Trump announced for president in 2015, 58 percent of his Facebook followers were from outside the United States, right? How do we know this? SINGER: So you're able to detect the origin of accounts. One of the new rules of the game when everything is online is that it's out in the open. So you can follow it. You can see the patterns of behavior. Maybe one of the more notable aspects of Donald Trump's follower count is that at that point in time about 4 percent supposedly lived in Mexico. Now, of course, these were not real Mexican citizens behind these accounts. These were in essence fake followers created to give the semblance of a massive online following that can then be turned into a real online following by driving your message viral. DAVIES: Right. But if 58 percent of the followers from outside the U. S. , can we infer that a lot of them are fake? SINGER: Yes, a vast majority of them were. They were basically bought accounts generated out of what are known as click farms. These are almost - you think of like a sweatshop factory. It's the same thing for people that are essentially being paid to click on accounts in places like the Philippines or Bangladesh. And then you combine these physical people with bots, and it can give you a much greater power in politics. DAVIES: You know, a lot's been written about the Russian efforts to interfere in the American 2016 election. We've seen indictments. Give us your sense of the scale and impact of this effort. SINGER: The challenge with this is that the information about it came out in little dribs and drabs and the result is that the scale of it is not appreciated by most people - just how big in number but in extent the campaign was. So, for example, if you're looking at Facebook, it now belatedly says, you know, roughly over 140 million Americans, about half the population, saw some aspect of Russian information - misinformation pushed online. You move over to other platforms, like, for example, Twitter, where it's literally hundreds of millions of messages being retweeted back and forth. A great illustration of this would be one single account - Tennessee GOP that was posing as if it was the hub for Tennessee Republicans. We now know that it was one of these Russians sitting in St. Petersburg. It was the seventh most read account on election day - not the seventh most read of the thousands of Russian accounts but the seventh most read overall. You saw similar campaigns playing out on Instagram too. There's actually one that was just detected a couple of weeks ago on Reddit. And the impact that this was not just what the Russians were pushing but the echo effect and to the broader population, including the media itself. One of the accounts that was posing as a young American woman, she was actually quoted in everything from Washington Post to New York Times to USA Today and the like as if she was a real young American woman. And actually, again, it was one of these Russians behind it. And so, you know, it's up for the historians to, you know, argue about whether it swung the election or not in such a close election. What we can say is the scale of that was immense and that the Russians believe it worked because they're coming back for more. They're doing it in the election right now. DAVIES: Emerson Brooking writes about conflict and social media. P. W. Singer is a strategist at New America. Their new book is called \"LikeWar: The Weaponization Of Social Media. \" We'll continue our conversation after a short break. This is FRESH AIR. (SOUNDBITE OF NOAM WIESENBERG'S \"DAVKA\") DAVIES: This is FRESH AIR, and we're speaking with Emerson Brooking. He writes about conflict and social media, was recently a research fellow at the Council on Foreign Relations. Also with us, P. W. Singer, a strategist at New America and a consultant to the U. S. military and intelligence community. Their new book is called \"LikeWar: The Weaponization Of Social Media. \" I'm interested in your sense of how social media wars affected opinions about the Kavanaugh confirmation controversy. I mean, this was a case where there were facts in dispute, a lot of strongly held opinions. Do you have any sense of - I mean, there must've been social media campaigns - what kind of impact they had on how this was perceived and, you know, the enduring impact it'll have on our discourse? SINGER: If you were watching your social media feed during the Kavanaugh hearings, in many ways it felt like a war going back and forth. And it was in a certain way. You had both sides using the exact same tactics of, you know, what we call LikeWar that militaries were using all around the world. You could see everything from open-source intelligence gathering campaigns where they were enlisting the online crowd to find bits of information to bring it together that wouldn't have previously been possible. So for example, on one side, you had a crowdsource hunt using images from Washington Nationals' baseball games to try and figure out who sat next to Kavanaugh to unpack this strange story of his $200,000 in baseball ticket debt. Then you add the other side, this combination of Google Earth images and Zillow real estate data to basically push this narrative, the doppelganger narrative, that Dr. Ford - the claim was, well, she's not a liar, but she's confused about who it was. Now, that narrative that was pushed online points to a second thing that played out - the spread of disinformation, the spread of alternative theories, the idea of burying the truth underneath a sea of lies. It's something that, for example, Russia is specialized in. And again, you saw both sides engaging in this in the Kavanaugh debate, pushing out all sorts of different false stories, false personas and the like in the battle to have their own narrative win out and the other side's narrative be buried underneath this. You even saw attempts to change not just your point of view but history itself for at least the online place that we go to establish and learn about history, Wikipedia. So for example in the book, we had the story of how after the Russian shoot-down of the airliner over Ukraine, Russian actors go onto Wikipedia very quickly to change the story to make it seem like they weren't involved. The same thing played out during the Kavanaugh hearing where he used a term in his - a high school yearbook that was a sexual term. And during his hearing, he says no, no, no. It's not a sexual term. It's about a drinking game. The problem for him is that there's literally no evidence on the entire Internet of that being a drinking game until someone within the House of Representatives - again, everything is out in the open so we can geolocate it to that. During the middle of the hearing, someone in the House of Representatives goes to create the evidence on Wikipedia to make it seem as if he is telling the truth. So you have this back-and-forth, back-and-forth. And it's our contention that every single political debate moving forward is going to see these kind of tactics utilized again because both sides not only use them but believe that they were the key to their effort, including in winning. DAVIES: You write that the social media platforms are private companies. And they're now coming to terms with the fact that they play, like it or not, a very powerful role in elections, in the national discourse and have to take some responsibility for dealing with this content - which is a huge challenge just because of the sheer volume, billions of posts a day and, you know, the subtle task of figuring out what is acceptable and what is not and that there is a developing tool called neural networks. Explain what they are, what they can do. SINGER: So neural networks is part of this larger area of research that people will talk about as - essentially artificial intelligence, using machines to not just mimic but maybe in some ways surpass human intelligence and thinking. And so neural networks can be used, like any other technology, for both good and bad. So for example, they can create wonderful imagery and scenes, images of a volcano that doesn't exist in the real world. And it's very powerful in use of, for example, the future of marketing. It's going to move into areas like customer service. The companies see this also as a way to save money, for example, if you're interacting with a machine rather than someone behind a help desk. But like everything else, it's also going to be weaponized. And so those very same chat bots, for example, that might be trying to persuade you to buy a product might be trying to persuade you to vote in a certain way or convince someone to take a certain position on a war a certain way. The challenge also moves into visual imagery. There's an area known as deep fakes. These are hyper-realistic images - videos, for example, of speeches that politicians never made, scenes that never happened. And so with humans finding it harder and harder to figure out what is real or not, the companies in turn are looking to artificial intelligence to help find it, to help police their networks. And so you get this strange but yet kind of wonderfully appropriate outcome. The origin of social networks online were a bunch of scientists talking about science fiction. That's where this all came from. And now we have this science fiction-like outcome emerging where you have two AI battling back and forth with us humans caught in the middle. DAVIES: Right. So these neural networks, like, could be good at subtly detecting what a robotic Twitter account might be in a way that would be hard for humans. SINGER: Exactly. DAVIES: But they can also produce a picture of - you know, name somebody - Joe Biden, you know, uttering some, you know, profane comment at something. SINGER: To give real-world examples of this, there have already been creations of speeches that Barack Obama never gave, or there's other ones where you can layer real and fake imagery together where, for example, it can be your facial expressions, but you're controlling an online image of everything from - and the example's Arnold Schwarzenegger to George W. Bush. Basically anyone that you have enough data points on what their face looks like, which, oh, by the way you can get off of Facebook, you can take that and meld it together to allow you to in essence create a fake scene of them saying or doing something. And so just like everything else in this space, it's going to be used for good, for jokes, for laughs. It's going to be monetized for profit. And it's also going to be weaponized. DAVIES: Emerson Brooking, P. W. Singer, thanks so much for speaking with us. SINGER: Appreciate it. Thank you. BROOKING: Thank you. GROSS: P. W. Singer and Emerson Brooking are the authors of \"LikeWar: The Weaponization Of Social Media. \" They spoke with FRESH AIR's Dave Davies, who is also WHYY's senior reporter. After a break, John Powers will review the new film \"22 July\" based on the story of a terrorist attack on a summer camp in Norway in 2011. This is FRESH AIR. (SOUNDBITE OF STEFON HARRIS' \"UNTIL\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-11-656436335": {"title": "U.S. Stocks Down Sharply Again After Wednesday's Rout : NPR", "url": "https://www.npr.org/2018/10/11/656436335/us-stock-markets-regain-footing-after-plunge", "author": "No author found", "published_date": "2018-10-11", "content": "", "section": "Business", "disclaimer": ""}, "2018-10-12-656517555": {"title": "To Deter Foreign Hackers, Some States May Also Be Deterring Voters : NPR", "url": "https://www.npr.org/2018/10/12/656517555/to-deter-foreign-hackers-some-states-may-also-be-deterring-voters", "author": "No author found", "published_date": "2018-10-12", "content": "", "section": "Politics", "disclaimer": ""}, "2018-10-13-657172112": {"title": "Facebook Says 14 Million Accounts Had Broad Array Of Personal Data Stolen : NPR", "url": "https://www.npr.org/2018/10/13/657172112/facebook-says-14-million-accounts-had-broad-array-of-personal-data-stolen", "author": "No author found", "published_date": "2018-10-13", "content": "", "section": "Technology", "disclaimer": ""}, "2018-10-15-657640313": {"title": "Microsoft Co-Founder Paul Allen Dies At 65 : NPR", "url": "https://www.npr.org/2018/10/15/657640313/microsoft-co-founder-paul-allen-dies-at-65", "author": "No author found", "published_date": "2018-10-15", "content": "", "section": "Business", "disclaimer": ""}, "2018-10-16-657952792": {"title": "FCC Chief Calls For Investigation Of Florida Cellphone Service Outages  : NPR", "url": "https://www.npr.org/2018/10/16/657952792/fcc-chief-calls-for-investigation-of-florida-cellphone-service-outages", "author": "No author found", "published_date": "2018-10-16", "content": "", "section": "Technology", "disclaimer": ""}, "2018-10-17-658138211": {"title": "'Irresponsible' To Conflate Chinese, Russian Influence Schemes, Democrats Charge : NPR", "url": "https://www.npr.org/2018/10/17/658138211/irresponsible-to-conflate-chinese-russian-influence-schemes-democrats-charge", "author": "No author found", "published_date": "2018-10-17", "content": "", "section": "National Security", "disclaimer": ""}, "2018-10-18-658384024": {"title": "Amy Winehouse Hologram Expected To 'Tour' With A Backing Band : NPR", "url": "https://www.npr.org/2018/10/18/658384024/amy-winehouse-hologram-expected-to-tour-with-a-backing-band", "author": "No author found", "published_date": "2018-10-18", "content": "", "section": "Music News", "disclaimer": ""}, "2018-10-19-648720360": {"title": "South Korean Women Fight Back Against Spy Cams In Public Bathrooms : NPR", "url": "https://www.npr.org/2018/10/19/648720360/south-korean-women-fight-back-against-spy-cams-in-public-bathrooms", "author": "No author found", "published_date": "2018-10-19", "content": "", "section": "World", "disclaimer": ""}, "2018-10-19-658931932": {"title": "Spy Bosses Warn Of Foreign Interference As Feds Unseal New Russia Charges : NPR", "url": "https://www.npr.org/2018/10/19/658931932/spy-bosses-warn-of-foreign-interference-as-feds-unseal-new-russia-charges", "author": "No author found", "published_date": "2018-10-19", "content": "", "section": "National Security", "disclaimer": ""}, "2018-10-22-659680894": {"title": "A.I. Produced 'Portrait' Will Go Up For Auction At Christie's : NPR", "url": "https://www.npr.org/2018/10/22/659680894/a-i-produced-portrait-will-go-up-for-auction-at-christie-s", "author": "No author found", "published_date": "2018-10-22", "content": "", "section": "Art & Design", "disclaimer": ""}, "2018-10-22-659611181": {"title": "This Portrait Is Reminiscent Of A Rembrandt But Artificial Intelligence Created It : NPR", "url": "https://www.npr.org/2018/10/22/659611181/this-portrait-is-reminiscent-of-a-rembrandt-but-artificial-intelligence-created-", "author": "No author found", "published_date": "2018-10-22", "content": "AILSA CHANG, HOST: A new portrait going on the Christie's auction block Thursday may be reminiscent of a Rembrandt, but it's actually the work of artificial intelligence. It's called \"Portrait Of Edmond Belamy. \" The work was created using an algorithm, and it might fetch around $10,000. Computers making art is nothing new, but a computer creating a portrait that's being auctioned by such a prestigious art house - now, that is new. Art appraiser Erin-Marie Wallace has been following the buzz around the upcoming sale, and she joins us now to talk about how the art world is taking all this in. Welcome. ERIN-MARIE WALLACE: Thank you. A pleasure to be here, Ailsa. CHANG: How controversial is this in the art world for a painting to be painted by a non-human and then auctioned off like fine art? WALLACE: Well, I think every couple generations, you have something that happens that begins to redefine what we consider art. You can go back to the early 20th century, and you can look at the urinal by Marcel Duchamp where he literally took a men's urinal and hung it on a wall, right? And this completely shattered what people thought of as art. We're redefining what art actually is for the 21st century, so I don't think that it is a once-in-a-lifetime moment. I think these things happen again and again. We are redefining what we consider art within our historical context. CHANG: That is so fascinating. But it does raise the question of how do you price a painting that is not the product of a human - because usually - I'm no art expert, but usually, the way I understand it, things like technique and the uniqueness of a piece of work - those are the factors that drive the price in art. So how do you price art where those things don't really apply? WALLACE: Well, they still apply. But let's add to that. Let's add to that the marketability of a painting. Let's add to the hype that's created around a painting or any work of art really. I tend to think that the whole thing is orchestrated from go. They knew exactly where they wanted to price it in comparison with where they wanted to sell it via Christie's. All are within the same wheelhouse in terms of number. CHANG: And does that orchestration deplete the integrity of the artistic process, the rollout? WALLACE: My opinion is no because I don't think it is up to any person to define what art is or what art is valued at. I feel that art is valued at what people are willing to pay for it. CHANG: So who is the artist in this case? Is it the coders who wrote the algorithm? Is it the algorithm, or is it the machine? WALLACE: Oh, the artist has to be a little bit of all of those. Original artist ownership, I do believe, is split between the programming, the coders, the collective and maybe even a little bit of a nod to Christie's for helping to create the frenzy around it. CHANG: And given that there has been some deliberate frenzy built around this, if this painting were created by a human, do you think it would have the same attractiveness? Would it garner the same price? WALLACE: No, I don't think so. I really don't think so. I think in terms of art, the things that matter the most are the firsts, and this is the first time an AI-generated artwork has gone to a major auction house to be auctioned off. And so this is a first. It is the absolute first time this has ever happened. Because this is a canvas print, it is absolutely possible that the collective could decide to release it in a smaller, limited and numbered edition after the sale. I don't believe that those will be valued as high as this particular one because this is the first. CHANG: Erin-Marie Wallace is the CEO of Rare-Era Appraisals just outside Washington, D. C. Thank you very much. WALLACE: Thank you so much. A pleasure, Ailsa. AILSA CHANG, HOST:  A new portrait going on the Christie's auction block Thursday may be reminiscent of a Rembrandt, but it's actually the work of artificial intelligence. It's called \"Portrait Of Edmond Belamy. \" The work was created using an algorithm, and it might fetch around $10,000. Computers making art is nothing new, but a computer creating a portrait that's being auctioned by such a prestigious art house - now, that is new. Art appraiser Erin-Marie Wallace has been following the buzz around the upcoming sale, and she joins us now to talk about how the art world is taking all this in. Welcome. ERIN-MARIE WALLACE: Thank you. A pleasure to be here, Ailsa. CHANG: How controversial is this in the art world for a painting to be painted by a non-human and then auctioned off like fine art? WALLACE: Well, I think every couple generations, you have something that happens that begins to redefine what we consider art. You can go back to the early 20th century, and you can look at the urinal by Marcel Duchamp where he literally took a men's urinal and hung it on a wall, right? And this completely shattered what people thought of as art. We're redefining what art actually is for the 21st century, so I don't think that it is a once-in-a-lifetime moment. I think these things happen again and again. We are redefining what we consider art within our historical context. CHANG: That is so fascinating. But it does raise the question of how do you price a painting that is not the product of a human - because usually - I'm no art expert, but usually, the way I understand it, things like technique and the uniqueness of a piece of work - those are the factors that drive the price in art. So how do you price art where those things don't really apply? WALLACE: Well, they still apply. But let's add to that. Let's add to that the marketability of a painting. Let's add to the hype that's created around a painting or any work of art really. I tend to think that the whole thing is orchestrated from go. They knew exactly where they wanted to price it in comparison with where they wanted to sell it via Christie's. All are within the same wheelhouse in terms of number. CHANG: And does that orchestration deplete the integrity of the artistic process, the rollout? WALLACE: My opinion is no because I don't think it is up to any person to define what art is or what art is valued at. I feel that art is valued at what people are willing to pay for it. CHANG: So who is the artist in this case? Is it the coders who wrote the algorithm? Is it the algorithm, or is it the machine? WALLACE: Oh, the artist has to be a little bit of all of those. Original artist ownership, I do believe, is split between the programming, the coders, the collective and maybe even a little bit of a nod to Christie's for helping to create the frenzy around it. CHANG: And given that there has been some deliberate frenzy built around this, if this painting were created by a human, do you think it would have the same attractiveness? Would it garner the same price? WALLACE: No, I don't think so. I really don't think so. I think in terms of art, the things that matter the most are the firsts, and this is the first time an AI-generated artwork has gone to a major auction house to be auctioned off. And so this is a first. It is the absolute first time this has ever happened. Because this is a canvas print, it is absolutely possible that the collective could decide to release it in a smaller, limited and numbered edition after the sale. I don't believe that those will be valued as high as this particular one because this is the first. CHANG: Erin-Marie Wallace is the CEO of Rare-Era Appraisals just outside Washington, D. C. Thank you very much. WALLACE: Thank you so much. A pleasure, Ailsa.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-22-658648856": {"title": "Facebook, Exploited By Influence Campaigns, Tries To Clamp Down With 'War Room' : NPR", "url": "https://www.npr.org/2018/10/22/658648856/facebook-exploited-by-influence-campaigns-tries-to-clamp-down-with-war-room", "author": "No author found", "published_date": "2018-10-22", "content": "", "section": "National Security", "disclaimer": ""}, "2018-10-22-658808705": {"title": "Thousands Of Swedes Are Inserting Microchips Under Their Skin : NPR", "url": "https://www.npr.org/2018/10/22/658808705/thousands-of-swedes-are-inserting-microchips-under-their-skin", "author": "No author found", "published_date": "2018-10-22", "content": "AILSA CHANG, HOST: This month in All Tech Considered, we're looking at our bodies the way technology sees them. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CHANG: Lots of technology promises to make routine tasks easier and quicker, but how far would you go to adopt that technology? Would you, say, implant a microchip underneath your skin? Lots of people in Sweden are doing just that. In fact, there's so much demand for the microchips that the country's main supplier says it cannot keep up with the requests. Maddy Savage reports from Stockholm. (SOUNDBITE OF READER SCANNING)MADDY SAVAGE, BYLINE: Erik Frisk is unlocking the door to his 16th-century townhouse with just a swipe of his hand. The 30-year-old web developer and designer lives here with a group of friends and recently organized a chipping party, inviting a biohacking company to inject tiny microchips into the skin just above his housemates' thumbs. ERIK FRISK: They work exactly the same as your key tag, the thing you scan to get into your garage or into your office or - it's just completely passive. It has no energy source or anything. When you tap it against a reader, the chip sends back an ID that tells the reader which chip this is. And then if it's a door, for example, it can decide whether or not it should let you in. SAVAGE: Growing numbers of Swedish offices, coworking hubs and gyms have started adopting the technology, too. The chips can also be used to store emergency contact details, social media profiles or e-tickets for events. SZILVIA VARSZEGI: Why did I get the chip? Mostly because I was curious, and I wanted to make my life a little bit easier. SAVAGE: That's Szilvia Varszegi, one of Erik's housemates. VARSZEGI: This morning, when I leave, the only thing that I need is my bank card. That's the one thing that I basically carry around. But for every other things, the chip basically solves my problems. SAVAGE: What was it like getting it inserted? VARSZEGI: A little bit scary but not painful at all. I was surprised. Maybe it was the adrenaline, but it feels perfectly fine. SAVAGE: More than 4,000 Swedes have had the chips inserted. One company's dominating the market. It was started by a former professional body piercer turned tech addict. JOWAN OSTERLUND: My name is Jowan Osterlund. I'm the CEO and founder of Biohax International. For the last five years, it's escalated quickly. Last two years, I've been doing this full-time. SAVAGE: Privacy is the big issue that a lot of people will be thinking about when they hear this. What happens to people's data? OSTERLUND: It's not much different than today. Everything is hackable, but the reason to hack them will never be bigger because it's a microchip. It's harder for someone to get to since you put it in you. SAVAGE: What happens if a software update is needed? OSTERLUND: Well, that happens in the back end. It's got a lifetime of about 10 years. Not like today's phones - there won't be a need to get a new one every year. SAVAGE: Sweden's largest train company recently started allowing commuters to use chips instead of tickets, and there's talk they could soon be used to make payments in shops and restaurants. But I'm on my way to meet Ben Libberton, a British scientist based in Sweden who's campaigning for lawmakers to keep a closer eye on the trend. (SOUNDBITE OF TRAIN LOUDSPEAKER)BEN LIBBERTON: What is happening now is relatively safe. But if it's used everywhere, if every time you want to do something, you want to - kind of instead of using a card, use your chip, it could be very, very easy to let go of that information. SAVAGE: Swedes are early adopters when it comes to technology, and the economy is already one of the most cashless on the planet. Microchips are a niche trend right now, but it might not be too long before they go mainstream, and that's something that worries Ben Libberton. LIBBERTON: Because it's implanted in your body, when more health-related information starts being used and incorporated into the chip and being transmitted, that could add an extra layer of privacy that we really need to look at and take care of before it's widely used. SAVAGE: Yet his voice is very much in the minority here in Sweden, where there's a high level of trust for government institutions, banks and corporations. For NPR News, I'm Maddy Savage in Stockholm. AILSA CHANG, HOST:  This month in All Tech Considered, we're looking at our bodies the way technology sees them. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CHANG: Lots of technology promises to make routine tasks easier and quicker, but how far would you go to adopt that technology? Would you, say, implant a microchip underneath your skin? Lots of people in Sweden are doing just that. In fact, there's so much demand for the microchips that the country's main supplier says it cannot keep up with the requests. Maddy Savage reports from Stockholm. (SOUNDBITE OF READER SCANNING) MADDY SAVAGE, BYLINE: Erik Frisk is unlocking the door to his 16th-century townhouse with just a swipe of his hand. The 30-year-old web developer and designer lives here with a group of friends and recently organized a chipping party, inviting a biohacking company to inject tiny microchips into the skin just above his housemates' thumbs. ERIK FRISK: They work exactly the same as your key tag, the thing you scan to get into your garage or into your office or - it's just completely passive. It has no energy source or anything. When you tap it against a reader, the chip sends back an ID that tells the reader which chip this is. And then if it's a door, for example, it can decide whether or not it should let you in. SAVAGE: Growing numbers of Swedish offices, coworking hubs and gyms have started adopting the technology, too. The chips can also be used to store emergency contact details, social media profiles or e-tickets for events. SZILVIA VARSZEGI: Why did I get the chip? Mostly because I was curious, and I wanted to make my life a little bit easier. SAVAGE: That's Szilvia Varszegi, one of Erik's housemates. VARSZEGI: This morning, when I leave, the only thing that I need is my bank card. That's the one thing that I basically carry around. But for every other things, the chip basically solves my problems. SAVAGE: What was it like getting it inserted? VARSZEGI: A little bit scary but not painful at all. I was surprised. Maybe it was the adrenaline, but it feels perfectly fine. SAVAGE: More than 4,000 Swedes have had the chips inserted. One company's dominating the market. It was started by a former professional body piercer turned tech addict. JOWAN OSTERLUND: My name is Jowan Osterlund. I'm the CEO and founder of Biohax International. For the last five years, it's escalated quickly. Last two years, I've been doing this full-time. SAVAGE: Privacy is the big issue that a lot of people will be thinking about when they hear this. What happens to people's data? OSTERLUND: It's not much different than today. Everything is hackable, but the reason to hack them will never be bigger because it's a microchip. It's harder for someone to get to since you put it in you. SAVAGE: What happens if a software update is needed? OSTERLUND: Well, that happens in the back end. It's got a lifetime of about 10 years. Not like today's phones - there won't be a need to get a new one every year. SAVAGE: Sweden's largest train company recently started allowing commuters to use chips instead of tickets, and there's talk they could soon be used to make payments in shops and restaurants. But I'm on my way to meet Ben Libberton, a British scientist based in Sweden who's campaigning for lawmakers to keep a closer eye on the trend. (SOUNDBITE OF TRAIN LOUDSPEAKER) BEN LIBBERTON: What is happening now is relatively safe. But if it's used everywhere, if every time you want to do something, you want to - kind of instead of using a card, use your chip, it could be very, very easy to let go of that information. SAVAGE: Swedes are early adopters when it comes to technology, and the economy is already one of the most cashless on the planet. Microchips are a niche trend right now, but it might not be too long before they go mainstream, and that's something that worries Ben Libberton. LIBBERTON: Because it's implanted in your body, when more health-related information starts being used and incorporated into the chip and being transmitted, that could add an extra layer of privacy that we really need to look at and take care of before it's widely used. SAVAGE: Yet his voice is very much in the minority here in Sweden, where there's a high level of trust for government institutions, banks and corporations. For NPR News, I'm Maddy Savage in Stockholm.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-23-659545242": {"title": "Here's How Russia Runs Its Disinformation Effort Against The 2018 Midterms : NPR", "url": "https://www.npr.org/2018/10/23/659545242/heres-how-russia-runs-its-disinformation-effort-against-the-2018-midterms", "author": "No author found", "published_date": "2018-10-23", "content": "", "section": "National Security", "disclaimer": ""}, "2018-10-25-660441213": {"title": "Blockchain And Climate Change : NPR", "url": "https://www.npr.org/2018/10/25/660441213/blockchain-and-climate-change", "author": "No author found", "published_date": "2018-10-25", "content": "DAVID GREENE, HOST: According to a U. N. climate report that was released earlier this month, the world has only about 12 years to roll back carbon emissions and avoid the worst impacts of climate change. Silicon Valley has been at work on a solution for years, often promising quick fixes. Blockchain, the technology behind cryptocurrencies like bitcoin, is the latest solution coming from big tech. Member station KQED's Sam Harnett has more. SAM HARNETT, BYLINE: Until about a year ago, hardly anyone in climate science was talking about blockchain. But at the recent Global Climate Action Summit in San Francisco, it was all over the place. There were panels and papers. People were talking about how it could help mitigate climate change in all sorts of ways. TOM BAUMANN: Land use, agriculture, carbon soil sequestration, renewable power. . . HARNETT: Tom Baumann co-chairs the Climate Chain Coalition. He came to the conference to spread awareness about blockchain. So what is it? BAUMANN: Blockchain, or distributed ledger technology, really references a larger ecosystem of technologies. HARNETT: OK, think of it like this. Blockchain is like an accounting book everyone can see, a distributed ledger. It's a record of transactions that isn't held by a central authority, like a bank, but instead on the computers of everyone on the blockchain. Transparency is its main advantage. Tons of new companies are springing up saying that they will use the transparency of blockchain to solve all sorts of problems - everything from curing cancer to ending human trafficking - and now even climate change. PAUL GAMBILL: It's our intention to build the infrastructure to make it possible to reverse climate change. HARNETT: Paul Gambill used to be a software product manager. He's working on climate change now. But he still makes the grand tech-world pitches. He's the CEO of a new Silicon Valley company called Nori. GAMBILL: Nori's a blockchain-based marketplace that makes it easier for people to pay for removing carbon dioxide from the atmosphere. HARNETT: Right now California, parts of the U. S. and Canada, the European Union and a number of other countries are participating in cap-and-trade carbon credit systems. The total number of carbon possible to emit is capped. And then companies can buy carbon credits that let them emit more. Economists love this market-based solution. But there's a problem when it comes to counting the carbon. GAMBILL: Carbon markets in the past have been plagued with issues of fraud and double counting. HARNETT: Nori wants to use blockchain to verify the accounting and to go one step further, to create a market for people taking carbon out of the environment. Now, the company hasn't launched yet. But that isn't limiting the ambition of the sales pitch. GAMBILL: We want to make this whole climate change problem just go away. HARNETT: So do scientists like Douglas McCauley. DOUGLAS MCCAULEY: Yeah, I wish it was that easy. HARNETT: McCauley's a researcher at UC Santa Barbara. He says the danger of Silicon Valley's big-promise, disrupt-everything ethos is that it suggests there's a quick and easy solution. MCCAULEY: The things that fix climate change are going to be really hard. HARNETT: Like getting countries to cut emissions or people to stop driving cars that pollute. McCauley says before it was blockchain, artificial intelligence and big data were the hot new things. Now, these have proven to be valuable tools for climate research, but in each case, it took scientists a few years to cut through all the hype. MCCAULEY: We need to look past the shiny thing and start talking about exactly how to apply it usefully. HARNETT: McCauley says blockchain could help with things like counting carbon credits or keeping track of, say, how many trees a company is planting to suck carbon out of the air. Katharine Mach, a climate scientist at Stanford, agrees. But she says it's dangerous to believe blockchain will be some kind of miracle cure. KATHARINE MACH: It's essentially can-kicking ethics, where we won't be putting into place the solutions that are actually more important to make the entirety of the problem something that's solvable. HARNETT: With the clock ticking on climate change, Mach says there's no time to waste sorting facts from hype. For NPR News, I'm Sam Harnett. DAVID GREENE, HOST:  According to a U. N. climate report that was released earlier this month, the world has only about 12 years to roll back carbon emissions and avoid the worst impacts of climate change. Silicon Valley has been at work on a solution for years, often promising quick fixes. Blockchain, the technology behind cryptocurrencies like bitcoin, is the latest solution coming from big tech. Member station KQED's Sam Harnett has more. SAM HARNETT, BYLINE: Until about a year ago, hardly anyone in climate science was talking about blockchain. But at the recent Global Climate Action Summit in San Francisco, it was all over the place. There were panels and papers. People were talking about how it could help mitigate climate change in all sorts of ways. TOM BAUMANN: Land use, agriculture, carbon soil sequestration, renewable power. . . HARNETT: Tom Baumann co-chairs the Climate Chain Coalition. He came to the conference to spread awareness about blockchain. So what is it? BAUMANN: Blockchain, or distributed ledger technology, really references a larger ecosystem of technologies. HARNETT: OK, think of it like this. Blockchain is like an accounting book everyone can see, a distributed ledger. It's a record of transactions that isn't held by a central authority, like a bank, but instead on the computers of everyone on the blockchain. Transparency is its main advantage. Tons of new companies are springing up saying that they will use the transparency of blockchain to solve all sorts of problems - everything from curing cancer to ending human trafficking - and now even climate change. PAUL GAMBILL: It's our intention to build the infrastructure to make it possible to reverse climate change. HARNETT: Paul Gambill used to be a software product manager. He's working on climate change now. But he still makes the grand tech-world pitches. He's the CEO of a new Silicon Valley company called Nori. GAMBILL: Nori's a blockchain-based marketplace that makes it easier for people to pay for removing carbon dioxide from the atmosphere. HARNETT: Right now California, parts of the U. S. and Canada, the European Union and a number of other countries are participating in cap-and-trade carbon credit systems. The total number of carbon possible to emit is capped. And then companies can buy carbon credits that let them emit more. Economists love this market-based solution. But there's a problem when it comes to counting the carbon. GAMBILL: Carbon markets in the past have been plagued with issues of fraud and double counting. HARNETT: Nori wants to use blockchain to verify the accounting and to go one step further, to create a market for people taking carbon out of the environment. Now, the company hasn't launched yet. But that isn't limiting the ambition of the sales pitch. GAMBILL: We want to make this whole climate change problem just go away. HARNETT: So do scientists like Douglas McCauley. DOUGLAS MCCAULEY: Yeah, I wish it was that easy. HARNETT: McCauley's a researcher at UC Santa Barbara. He says the danger of Silicon Valley's big-promise, disrupt-everything ethos is that it suggests there's a quick and easy solution. MCCAULEY: The things that fix climate change are going to be really hard. HARNETT: Like getting countries to cut emissions or people to stop driving cars that pollute. McCauley says before it was blockchain, artificial intelligence and big data were the hot new things. Now, these have proven to be valuable tools for climate research, but in each case, it took scientists a few years to cut through all the hype. MCCAULEY: We need to look past the shiny thing and start talking about exactly how to apply it usefully. HARNETT: McCauley says blockchain could help with things like counting carbon credits or keeping track of, say, how many trees a company is planting to suck carbon out of the air. Katharine Mach, a climate scientist at Stanford, agrees. But she says it's dangerous to believe blockchain will be some kind of miracle cure. KATHARINE MACH: It's essentially can-kicking ethics, where we won't be putting into place the solutions that are actually more important to make the entirety of the problem something that's solvable. HARNETT: With the clock ticking on climate change, Mach says there's no time to waste sorting facts from hype. For NPR News, I'm Sam Harnett.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-26-660775910": {"title": "Should Self-Driving Cars Have Ethics? : NPR", "url": "https://www.npr.org/2018/10/26/660775910/should-self-driving-cars-have-ethics", "author": "No author found", "published_date": "2018-10-26", "content": "", "section": "Technology", "disclaimer": ""}, "2018-10-27-661313336": {"title": "Russian Trolls Tried To Influence Debate Over NFL Players Kneeling During Anthem : NPR", "url": "https://www.npr.org/2018/10/27/661313336/russian-trolls-tried-to-influence-debate-over-nfl-players-kneeling-during-anthem", "author": "No author found", "published_date": "2018-10-27", "content": "SCOTT SIMON, HOST: Last week, federal prosecutors unsealed a criminal complaint which described efforts by the Russian-backed Internet Research Agency to use Twitter bots to influence Americans on a wide variety of controversial topics, including issues raised in sports. Andrew Beaton, a sports reporter for The Wall Street Journal, has written about the impact that Russian trolls have had on the controversy over players who kneel during the playing of the national anthem. Mr. Beaton joins us from New York. Thanks so much for being with us. ANDREW BEATON: Thanks so much for having me on. SIMON: Why was this Internet Research Agency interested in whether football players took a knee or not during the playing of the anthem? BEATON: Well, at first glance, it seems a little strange, right? We have them talking about these weighty, weighty issues like health care, like the Second Amendment - issues that are directly related to politics. But then when you think about it, what their efforts were aimed at were sowing discord. This issue with players kneeling during the national anthem, which started in 2016 to draw attention to social issues, racial inequality, this issue has been pretty much as divisive as it gets. But it's not just people that are weighing in on this. It's a topic that reached the president of the United States. Last year, President Donald Trump weighed in on the topic regularly, whether that was in stump speeches, whether that was on Twitter. So it's not just a big social, cultural issue. This was an issue that reached the top of the food chain. SIMON: What did you find when you dug into the Twitter accounts that are connected to the Internet Research Agency? BEATON: Well, what we found was these tweets had begun all the way dating back to 2014. But when Colin Kaepernick began the movement then we saw these issues sparked to life. And then in September of 2017, President Trump gives a speech in Alabama that really attacks the players using crude language. It became a big cultural moment. And then the Twitter trolls spring to life even more. And so that's one of the first things. One of the second things you look at when you start to parse through the data is which angle they were coming from. So of the partisan tweets, 87 percent were from what the researchers at Clemson University, who helped provide us with this data, 87 percent they identified as right-wing conservative. Those are the people who would be assailing the players for the kneeling, calling them unpatriotic. Thirteen percent were supporting Colin Kaepernick, supporting the players. So it's interesting to look at that breakdown and wonder how that influenced the conversation. SIMON: So it wasn't directed to achieving a certain result, just to stir things up, apparently? BEATON: Yeah. And the word we hear over and over when it comes to politics and what they were trying to do on social media is sow discord. And this is, in some ways, just the perfect issue to do that because it is a fraught, contentious issue, and also you want people to pay attention to what you're doing. When you want something to go viral, football is the most popular sport in America. They chose some pretty red meat, some pretty rabid fans to try and attract this debate. SIMON: I gather a lot of these accounts have been closed, right? BEATON: Yeah. So the accounts from this study have been closed. But, you know, what we see with these, it sometimes seems to be a game of whack-a-mole. One's closed down, new ones pop up. And one of the interesting notes that the Clemson professors had was that these are closed, but they are monitoring new ones that they suspect to be connected to the same operations. And they're continuing to weigh in on controversial sporting topics, such as a month ago, we saw the new Nike campaign with Colin Kaepernick, which, again, became a charged moment. And then that's the type of things where they would continue to weigh in. SIMON: Andrew Beaton of The Wall Street Journal, thanks so much for being with us. BEATON: Thanks so much for having me on. SCOTT SIMON, HOST:  Last week, federal prosecutors unsealed a criminal complaint which described efforts by the Russian-backed Internet Research Agency to use Twitter bots to influence Americans on a wide variety of controversial topics, including issues raised in sports. Andrew Beaton, a sports reporter for The Wall Street Journal, has written about the impact that Russian trolls have had on the controversy over players who kneel during the playing of the national anthem. Mr. Beaton joins us from New York. Thanks so much for being with us. ANDREW BEATON: Thanks so much for having me on. SIMON: Why was this Internet Research Agency interested in whether football players took a knee or not during the playing of the anthem? BEATON: Well, at first glance, it seems a little strange, right? We have them talking about these weighty, weighty issues like health care, like the Second Amendment - issues that are directly related to politics. But then when you think about it, what their efforts were aimed at were sowing discord. This issue with players kneeling during the national anthem, which started in 2016 to draw attention to social issues, racial inequality, this issue has been pretty much as divisive as it gets. But it's not just people that are weighing in on this. It's a topic that reached the president of the United States. Last year, President Donald Trump weighed in on the topic regularly, whether that was in stump speeches, whether that was on Twitter. So it's not just a big social, cultural issue. This was an issue that reached the top of the food chain. SIMON: What did you find when you dug into the Twitter accounts that are connected to the Internet Research Agency? BEATON: Well, what we found was these tweets had begun all the way dating back to 2014. But when Colin Kaepernick began the movement then we saw these issues sparked to life. And then in September of 2017, President Trump gives a speech in Alabama that really attacks the players using crude language. It became a big cultural moment. And then the Twitter trolls spring to life even more. And so that's one of the first things. One of the second things you look at when you start to parse through the data is which angle they were coming from. So of the partisan tweets, 87 percent were from what the researchers at Clemson University, who helped provide us with this data, 87 percent they identified as right-wing conservative. Those are the people who would be assailing the players for the kneeling, calling them unpatriotic. Thirteen percent were supporting Colin Kaepernick, supporting the players. So it's interesting to look at that breakdown and wonder how that influenced the conversation. SIMON: So it wasn't directed to achieving a certain result, just to stir things up, apparently? BEATON: Yeah. And the word we hear over and over when it comes to politics and what they were trying to do on social media is sow discord. And this is, in some ways, just the perfect issue to do that because it is a fraught, contentious issue, and also you want people to pay attention to what you're doing. When you want something to go viral, football is the most popular sport in America. They chose some pretty red meat, some pretty rabid fans to try and attract this debate. SIMON: I gather a lot of these accounts have been closed, right? BEATON: Yeah. So the accounts from this study have been closed. But, you know, what we see with these, it sometimes seems to be a game of whack-a-mole. One's closed down, new ones pop up. And one of the interesting notes that the Clemson professors had was that these are closed, but they are monitoring new ones that they suspect to be connected to the same operations. And they're continuing to weigh in on controversial sporting topics, such as a month ago, we saw the new Nike campaign with Colin Kaepernick, which, again, became a charged moment. And then that's the type of things where they would continue to weigh in. SIMON: Andrew Beaton of The Wall Street Journal, thanks so much for being with us. BEATON: Thanks so much for having me on.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-27-661149098": {"title": "The Russia Investigations: The U.S. Launches A Digital Offensive \u2014 Gently : NPR", "url": "https://www.npr.org/2018/10/27/661149098/the-russia-investigations-the-u-s-launches-a-digital-offensive-gently", "author": "No author found", "published_date": "2018-10-27", "content": "", "section": "National Security", "disclaimer": ""}, "2018-10-28-661598557": {"title": "IBM Will Acquire Open-Source Software Company Red Hat In $34 Billion Deal : NPR", "url": "https://www.npr.org/2018/10/28/661598557/in-major-acquisition-ibm-will-acquire-open-source-software-company-red-hat", "author": "No author found", "published_date": "2018-10-28", "content": "", "section": "Business", "disclaimer": ""}, "2018-10-28-661532688": {"title": "Gab, Site Where Synagogue Shooting Suspect Posted, Is Suspended : NPR", "url": "https://www.npr.org/2018/10/28/661532688/a-look-at-gab-the-free-speech-social-site-where-synagogue-shooting-suspect-poste", "author": "No author found", "published_date": "2018-10-28", "content": "", "section": "Technology", "disclaimer": ""}, "2018-10-29-661676103": {"title": "After Synagogue Attack, Web-Hosting Sites Suspend Gab  : NPR", "url": "https://www.npr.org/2018/10/29/661676103/after-synagogue-attack-web-hosting-sites-suspend-gab", "author": "No author found", "published_date": "2018-10-29", "content": "STEVE INSKEEP, HOST: Now we have more about a social media site where the Pittsburgh synagogue shooting suspect reportedly posted anti-Semitic attacks. Before walking into the Tree of Life synagogue with four weapons, Robert Bowers apparently got fired up with words. He reportedly used Gab, which calls itself an alternative to Twitter and is popular among the far right. Here's NPR's Jasmine Garsd. JASMINE GARSD, BYLINE: Gab is a site that proudly promotes free speech. It boasts that it lets anyone say anything. But it's been controversial. Critics have called it a home for anti-Semites and white nationalists. Robert Bowers was a user. Before allegedly going on a killing spree, he posted about the Hebrew Immigrant Aid Society, a group that supports refugees. He said the group, quote, \"likes to bring invaders that kill our people. I can't sit by and watch my people get slaughtered. Screw your optics - I'm going in. \" Andrew Torba is the CEO of Gab. In an interview with NPR, he defended the platform. ANDREW TORBA: I don't know. Do you see a direct threat in there? 'Cause I don't. What would you expect us to do with posts like that? You want us to just censor anybody that says the phrase I'm going in? Is that what you're proposing here? UNIDENTIFIED REPORTER: Well, I think that. . . TORBA: 'Cause I think that's absurd. And here's the thing. The answer to bad speech or hate speech - however you want to define that - is more speech, and it always will be. GARSD: When does online free speech become a threat? This isn't the first time the issue has come up in social media. Just last week, it came to light that the man accused of sending explosive devices to prominent Democrats in the media had a history of threatening tweets. Torba says Gab follows strict rules, including no threats. He says he created Gab because he saw no room for conservative points of view on social media. Take Twitter. TORBA: Where there are thousands upon thousands of people calling for someone to kill Donald Trump, saying they're going to kill Donald Trump, expressing hate towards white people, towards Christians, towards minorities who may now support Donald Trump. They allow hate to be spewed at certain groups and certain people. GARSD: So where is the line between free speech and inciting violence? KELLY MCBRIDE: I think the line is where free speech becomes a threat. GARSD: Kelly McBride is a senior vice president at the Poynter Institute for Media Studies. She says Torba is prioritizing free speech above all other constitutional values. MCBRIDE: And that is not necessarily what our constitutional framers intended, right? They didn't want the government curbing free speech. GARSD: Nor did they want citizens to be irresponsible with their speech, she says. Last year, Google banned Gab's app. Apple rejected it. And Microsoft terminated its agreement with it last month. Just in the last 24 hours, at least two web-hosting platforms have suspended Gab. CEO Andrew Torba is not backing down. TORBA: We're not going anywhere. GARSD: Torba says Gab condemns the shooting. But he thinks it's now being targeted unfairly. Over the weekend, the social media site was filled with anger, some of it directed at the Jewish community. Jasmine Garsd, NPR News, New York. (SOUNDBITE OF AK AND SUBLAB'S \"TRANQUIL\")INSKEEP: So that was over the weekend. And then last night, Gab released a statement saying it would be inaccessible for a period as it transitions to a new hosting provider. (SOUNDBITE OF AK AND SUBLAB'S \"TRANQUIL\") STEVE INSKEEP, HOST:  Now we have more about a social media site where the Pittsburgh synagogue shooting suspect reportedly posted anti-Semitic attacks. Before walking into the Tree of Life synagogue with four weapons, Robert Bowers apparently got fired up with words. He reportedly used Gab, which calls itself an alternative to Twitter and is popular among the far right. Here's NPR's Jasmine Garsd. JASMINE GARSD, BYLINE: Gab is a site that proudly promotes free speech. It boasts that it lets anyone say anything. But it's been controversial. Critics have called it a home for anti-Semites and white nationalists. Robert Bowers was a user. Before allegedly going on a killing spree, he posted about the Hebrew Immigrant Aid Society, a group that supports refugees. He said the group, quote, \"likes to bring invaders that kill our people. I can't sit by and watch my people get slaughtered. Screw your optics - I'm going in. \" Andrew Torba is the CEO of Gab. In an interview with NPR, he defended the platform. ANDREW TORBA: I don't know. Do you see a direct threat in there? 'Cause I don't. What would you expect us to do with posts like that? You want us to just censor anybody that says the phrase I'm going in? Is that what you're proposing here? UNIDENTIFIED REPORTER: Well, I think that. . . TORBA: 'Cause I think that's absurd. And here's the thing. The answer to bad speech or hate speech - however you want to define that - is more speech, and it always will be. GARSD: When does online free speech become a threat? This isn't the first time the issue has come up in social media. Just last week, it came to light that the man accused of sending explosive devices to prominent Democrats in the media had a history of threatening tweets. Torba says Gab follows strict rules, including no threats. He says he created Gab because he saw no room for conservative points of view on social media. Take Twitter. TORBA: Where there are thousands upon thousands of people calling for someone to kill Donald Trump, saying they're going to kill Donald Trump, expressing hate towards white people, towards Christians, towards minorities who may now support Donald Trump. They allow hate to be spewed at certain groups and certain people. GARSD: So where is the line between free speech and inciting violence? KELLY MCBRIDE: I think the line is where free speech becomes a threat. GARSD: Kelly McBride is a senior vice president at the Poynter Institute for Media Studies. She says Torba is prioritizing free speech above all other constitutional values. MCBRIDE: And that is not necessarily what our constitutional framers intended, right? They didn't want the government curbing free speech. GARSD: Nor did they want citizens to be irresponsible with their speech, she says. Last year, Google banned Gab's app. Apple rejected it. And Microsoft terminated its agreement with it last month. Just in the last 24 hours, at least two web-hosting platforms have suspended Gab. CEO Andrew Torba is not backing down. TORBA: We're not going anywhere. GARSD: Torba says Gab condemns the shooting. But he thinks it's now being targeted unfairly. Over the weekend, the social media site was filled with anger, some of it directed at the Jewish community. Jasmine Garsd, NPR News, New York. (SOUNDBITE OF AK AND SUBLAB'S \"TRANQUIL\") INSKEEP: So that was over the weekend. And then last night, Gab released a statement saying it would be inaccessible for a period as it transitions to a new hosting provider. (SOUNDBITE OF AK AND SUBLAB'S \"TRANQUIL\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-30-660006488": {"title": "The Tinder-Bumble Feud: Dating Apps Fight Over Who Owns The Swipe : NPR", "url": "https://www.npr.org/2018/10/30/660006488/the-tinder-bumble-feud-dating-apps-fight-over-who-owns-the-swipe", "author": "No author found", "published_date": "2018-10-30", "content": "AILSA CHANG, HOST: OK. Today on All Tech Considered - an all-out battle in the world of online dating. (SOUNDBITE OF MUSIC)CHANG: If you're looking for love, you are not going to find any - at least not between the apps Bumble and Tinder. You might know how these apps work. You look at someone's photo, and you either swipe left or swipe right. Well, Bumble and Tinder are now fighting in court over whether Bumble swiped ideas from Tinder. NPR's Camila Domonoske explains that this fight raises questions about how patents work in the Internet age. CAMILA DOMONOSKE, BYLINE: Shauna O'Hara has tried a number of dating apps. SHAUNA O'HARA: There are tons of them, and they're all equally horrible. DOMONOSKE: Dating is hard, but using the apps is pretty easy. In both Bumble and Tinder, users see a picture of a possible date. O'HARA: When you swipe left, it's not someone that you're interested in. And if you swipe right, then you are interested, and if they're interested as well, then you connect. DOMONOSKE: These are snap decisions. O'HARA: Oh, no, bad shoes, wrinkled shirt - not my type. It's very rapid fire - like, swipe, swipe, swipe. DOMONOSKE: That swipe was a key feature of Tinder, which launched first. Then an early Tinder employee, who was dating her boss, one of the co-founders, broke up with him, left the company, alleged sexual harassment. She went on to found Bumble - like Tinder, except women move first. The apps are very similar - maybe too similar. SARAH BURSTEIN: Match, the parent company of Tinder, is suing Bumble for almost every type of IP infringement you could think of. DOMONOSKE: Sarah Burstein is a professor at the University of Oklahoma College of Law. And by IP, she means intellectual property. Tinder has patents and trademarks covering the way it works. But Bumble has countersued and called those IP claims bogus. BURSTEIN: You don't own the concept of swiping right or swiping left. You don't own the concept of matchmaking. DOMONOSKE: And there's a lot of money at stake. Forbes values Bumble at over a billion dollars and Tinder's worth even more. So Tinder didn't invent matchmaking or swiping, but can it own the idea of swipe-based dating apps? It turns out that's a complicated question, and it raises much bigger issues. Patents are supposed to cover specific inventions. They aren't supposed to cover abstract ideas. Daniel Nazer is a staff attorney at the Electronic Frontier Foundation. DANIEL NAZER: You don't get a patent for saying cure dementia with a drug. You have to say what the drug is. DOMONOSKE: Then along came the Internet, and people discovered they could patent some pretty abstract ideas as long as they added a computer. Like, you couldn't patent the idea of meal planning, but you could patent meal planning online. You can't patent restaurant menus, but you could patent online menus. NAZER: The patent system had started really handing out patents for solve this problem with software. DOMONOSKE: That changed four years ago. A company called the Alice Corporation had some abstract online banking patents, and the Supreme Court threw them out. The court ruled that an abstract idea plus a computer is still an abstract idea. Nazer says the Alice decision could be bad news for Tinder. If Tinder's patent is just the abstract idea of matchmaking but online, that's no longer allowed. But did I mention that it's complicated? NAZER: What is abstract is itself a pretty abstract and challenging question. DOMONOSKE: You can patent software. Your idea just has to be an innovation. So, of course, Tinder says that swiping to match people was unique and innovative. The swipe fight is still working its way through the courts, but in the meantime, it's clear the Supreme Court's decision tightened the rules for software patents, which has had a big impact - way beyond the dating industry. Nazer argues it's been a positive change promoting healthy competition, but others worry that good patents are being thrown out as well. Michael Risch is a professor at Villanova University's law school. MICHAEL RISCH: If you applied the definitions courts are using for abstractness, many of our most famous patents would wind up being unpatentable today, like the telephone. DOMONOSKE: Sarah Burstein says this push and pull goes right to the heart of patent law. BURSTEIN: It's this sort of eternal tension we have between trying to get the rights not too broad, not too narrow but really trying to get them just right. DOMONOSKE: Trying to find the right balance - almost as hard as trying to find the right match. Camila Domonoske, NPR News. AILSA CHANG, HOST:  OK. Today on All Tech Considered - an all-out battle in the world of online dating. (SOUNDBITE OF MUSIC) CHANG: If you're looking for love, you are not going to find any - at least not between the apps Bumble and Tinder. You might know how these apps work. You look at someone's photo, and you either swipe left or swipe right. Well, Bumble and Tinder are now fighting in court over whether Bumble swiped ideas from Tinder. NPR's Camila Domonoske explains that this fight raises questions about how patents work in the Internet age. CAMILA DOMONOSKE, BYLINE: Shauna O'Hara has tried a number of dating apps. SHAUNA O'HARA: There are tons of them, and they're all equally horrible. DOMONOSKE: Dating is hard, but using the apps is pretty easy. In both Bumble and Tinder, users see a picture of a possible date. O'HARA: When you swipe left, it's not someone that you're interested in. And if you swipe right, then you are interested, and if they're interested as well, then you connect. DOMONOSKE: These are snap decisions. O'HARA: Oh, no, bad shoes, wrinkled shirt - not my type. It's very rapid fire - like, swipe, swipe, swipe. DOMONOSKE: That swipe was a key feature of Tinder, which launched first. Then an early Tinder employee, who was dating her boss, one of the co-founders, broke up with him, left the company, alleged sexual harassment. She went on to found Bumble - like Tinder, except women move first. The apps are very similar - maybe too similar. SARAH BURSTEIN: Match, the parent company of Tinder, is suing Bumble for almost every type of IP infringement you could think of. DOMONOSKE: Sarah Burstein is a professor at the University of Oklahoma College of Law. And by IP, she means intellectual property. Tinder has patents and trademarks covering the way it works. But Bumble has countersued and called those IP claims bogus. BURSTEIN: You don't own the concept of swiping right or swiping left. You don't own the concept of matchmaking. DOMONOSKE: And there's a lot of money at stake. Forbes values Bumble at over a billion dollars and Tinder's worth even more. So Tinder didn't invent matchmaking or swiping, but can it own the idea of swipe-based dating apps? It turns out that's a complicated question, and it raises much bigger issues. Patents are supposed to cover specific inventions. They aren't supposed to cover abstract ideas. Daniel Nazer is a staff attorney at the Electronic Frontier Foundation. DANIEL NAZER: You don't get a patent for saying cure dementia with a drug. You have to say what the drug is. DOMONOSKE: Then along came the Internet, and people discovered they could patent some pretty abstract ideas as long as they added a computer. Like, you couldn't patent the idea of meal planning, but you could patent meal planning online. You can't patent restaurant menus, but you could patent online menus. NAZER: The patent system had started really handing out patents for solve this problem with software. DOMONOSKE: That changed four years ago. A company called the Alice Corporation had some abstract online banking patents, and the Supreme Court threw them out. The court ruled that an abstract idea plus a computer is still an abstract idea. Nazer says the Alice decision could be bad news for Tinder. If Tinder's patent is just the abstract idea of matchmaking but online, that's no longer allowed. But did I mention that it's complicated? NAZER: What is abstract is itself a pretty abstract and challenging question. DOMONOSKE: You can patent software. Your idea just has to be an innovation. So, of course, Tinder says that swiping to match people was unique and innovative. The swipe fight is still working its way through the courts, but in the meantime, it's clear the Supreme Court's decision tightened the rules for software patents, which has had a big impact - way beyond the dating industry. Nazer argues it's been a positive change promoting healthy competition, but others worry that good patents are being thrown out as well. Michael Risch is a professor at Villanova University's law school. MICHAEL RISCH: If you applied the definitions courts are using for abstractness, many of our most famous patents would wind up being unpatentable today, like the telephone. DOMONOSKE: Sarah Burstein says this push and pull goes right to the heart of patent law. BURSTEIN: It's this sort of eternal tension we have between trying to get the rights not too broad, not too narrow but really trying to get them just right. DOMONOSKE: Trying to find the right balance - almost as hard as trying to find the right match. Camila Domonoske, NPR News.", "section": "Business", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-10-30-662095109": {"title": "Old Voting Machines Confuse Some Texans During Midterm Election : NPR", "url": "https://www.npr.org/2018/10/30/662095109/old-voting-machines-confuse-some-texans-during-midterm-election", "author": "No author found", "published_date": "2018-10-30", "content": "", "section": "National", "disclaimer": ""}, "2018-10-31-662630534": {"title": "Nearly 30 Percent Of Anti-Semitic Online Attacks Are Bots : NPR", "url": "https://www.npr.org/2018/10/31/662630534/nearly-30-percent-of-anti-semitic-online-attacks-are-bots", "author": "No author found", "published_date": "2018-10-31", "content": "", "section": "National", "disclaimer": ""}, "2018-11-01-663123125": {"title": "Anti-Defamation League Report Says Online Anti-Semitism Is A 'Daily Occurrence' : NPR", "url": "https://www.npr.org/2018/11/01/663123125/anti-defamation-league-report-says-online-anti-semitism-is-a-daily-occurrence", "author": "No author found", "published_date": "2018-11-01", "content": "ARI SHAPIRO, HOST: A day before the shooting in Pittsburgh, the ADL published a study about online harassment and propaganda against Jews in the U. S. The ADL works to fight anti-Semitism. And its report says online attacks against Jews were rare a few years ago and now, quote, \"anti-Semitism has become normalized, and harassment is a daily occurrence. \" Sam Woolley is an author of the report and joins us now. Welcome. SAM WOOLLEY: Thanks for having me. SHAPIRO: This report is based on an analysis of more than 7 1/2 million tweets over a couple of weeks this past September. And it finds that 30 percent of Twitter accounts using derogatory terms were highly automated. What do those words highly automated mean? WOOLLEY: Highly automated is shorthand for a social media bot. And so a bot is a profile on a site like Twitter that appears to be real and appears to be run by a person but is actually using automation to amplify posts on a particular topic. SHAPIRO: Who's running these anti-Semitic bots? WOOLLEY: That's a really good question. And part of the problem is that we don't actually know who is running these bots. It appears that a lot of the content related to these accounts is coming from white nationalists and the \"alt-right. \" That said, it's very hard for researchers like me to determine the provenance of these bots. And past experience shows that while a lot of attacks can come from domestic actors, they can also come from foreign entities as well, much like we saw in 2016 with the Russian interference in the election. SHAPIRO: People who experience online harassment are often told - report the offensive users; block the offensive content. Is that enough? WOOLLEY: I don't think it's enough. I think that it's a shame that we put the burden of proof and the burden of reporting upon the very people who are experiencing harassment. I think the social media firms need to do more to protect people who experience extreme trolling and extreme harassment online. At the moment, the mechanisms that exist for reporting things like doxxing, or releasing of personal details online, fall really short of what they should. SHAPIRO: Do the people engaging in this kind of harassment have a goal beyond just harassing people? WOOLLEY: You know, the goal in a lot of cases is to prevent people from speaking out, to prevent their voices from being heard in American politics. Bots generally get used to do two things, to amplify particular voices and suppress others. So if you have one account that can tweet about politics, that's one thing. But if you have 10,000 automated accounts that can tweet about something else, then you can imagine what effect they have on someone's ability to actually get things done. These accounts generate a lot of noise, and they generate a lot of fear amongst the people who are the recipients of attacks from them. Some of the content here points people away from voting, suggests they can vote via text and all sorts of things like that. So they're actually undermining democracy in a big way. SHAPIRO: Can technology be used to counteract these bots on a large scale? WOOLLEY: Well, I think that there's a perception that AI is the big solution to this problem because of questions of scale. The social media companies have grown so quickly and expanded so fast that now we come to accept the idea of scale in our day-to-day life. But I think that it's really crucial that we have people on the other end making decisions about what hate speech and propaganda looks like because technology can't suss out details. It can't feel. It can't understand humor or sentiment. And so we have to also have human-based ways of mitigating the problems of automation and computational propaganda. SHAPIRO: Sam Woolley is director of the Digital Intelligence Lab at the Institute for the Future and one of the authors of the ADL report on anti-Semitic propaganda online. Thanks for joining us today. WOOLLEY: Thanks for having me. (SOUNDBITE OF THE XX'S \"INTRO (THE XX SONG)\") ARI SHAPIRO, HOST:  A day before the shooting in Pittsburgh, the ADL published a study about online harassment and propaganda against Jews in the U. S. The ADL works to fight anti-Semitism. And its report says online attacks against Jews were rare a few years ago and now, quote, \"anti-Semitism has become normalized, and harassment is a daily occurrence. \" Sam Woolley is an author of the report and joins us now. Welcome. SAM WOOLLEY: Thanks for having me. SHAPIRO: This report is based on an analysis of more than 7 1/2 million tweets over a couple of weeks this past September. And it finds that 30 percent of Twitter accounts using derogatory terms were highly automated. What do those words highly automated mean? WOOLLEY: Highly automated is shorthand for a social media bot. And so a bot is a profile on a site like Twitter that appears to be real and appears to be run by a person but is actually using automation to amplify posts on a particular topic. SHAPIRO: Who's running these anti-Semitic bots? WOOLLEY: That's a really good question. And part of the problem is that we don't actually know who is running these bots. It appears that a lot of the content related to these accounts is coming from white nationalists and the \"alt-right. \" That said, it's very hard for researchers like me to determine the provenance of these bots. And past experience shows that while a lot of attacks can come from domestic actors, they can also come from foreign entities as well, much like we saw in 2016 with the Russian interference in the election. SHAPIRO: People who experience online harassment are often told - report the offensive users; block the offensive content. Is that enough? WOOLLEY: I don't think it's enough. I think that it's a shame that we put the burden of proof and the burden of reporting upon the very people who are experiencing harassment. I think the social media firms need to do more to protect people who experience extreme trolling and extreme harassment online. At the moment, the mechanisms that exist for reporting things like doxxing, or releasing of personal details online, fall really short of what they should. SHAPIRO: Do the people engaging in this kind of harassment have a goal beyond just harassing people? WOOLLEY: You know, the goal in a lot of cases is to prevent people from speaking out, to prevent their voices from being heard in American politics. Bots generally get used to do two things, to amplify particular voices and suppress others. So if you have one account that can tweet about politics, that's one thing. But if you have 10,000 automated accounts that can tweet about something else, then you can imagine what effect they have on someone's ability to actually get things done. These accounts generate a lot of noise, and they generate a lot of fear amongst the people who are the recipients of attacks from them. Some of the content here points people away from voting, suggests they can vote via text and all sorts of things like that. So they're actually undermining democracy in a big way. SHAPIRO: Can technology be used to counteract these bots on a large scale? WOOLLEY: Well, I think that there's a perception that AI is the big solution to this problem because of questions of scale. The social media companies have grown so quickly and expanded so fast that now we come to accept the idea of scale in our day-to-day life. But I think that it's really crucial that we have people on the other end making decisions about what hate speech and propaganda looks like because technology can't suss out details. It can't feel. It can't understand humor or sentiment. And so we have to also have human-based ways of mitigating the problems of automation and computational propaganda. SHAPIRO: Sam Woolley is director of the Digital Intelligence Lab at the Institute for the Future and one of the authors of the ADL report on anti-Semitic propaganda online. Thanks for joining us today. WOOLLEY: Thanks for having me. (SOUNDBITE OF THE XX'S \"INTRO (THE XX SONG)\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-01-663123109": {"title": "Google Employees Stage Protest Of Company's Handling Of Sexual Harassment Complaints : NPR", "url": "https://www.npr.org/2018/11/01/663123109/google-employees-stage-protest-of-companys-handling-of-sexual-harassment-complai", "author": "No author found", "published_date": "2018-11-01", "content": "ARI SHAPIRO, HOST: Thousands of Google employees around the world hit the streets today to show that they are fed up with the way the company handles sexual harassment complaints and to demand changes in workplace culture. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PEOPLE: (Chanting) Time is up. Time is up. Time is up. SHAPIRO: Chants there of time is up from employees outside Google's offices in New York City. Joining us to discuss the protest and Google's response is NPR's Yuki Noguchi. Hi, Yuki. YUKI NOGUCHI, BYLINE: Hi, Ari. SHAPIRO: How did Google reach this point? NOGUCHI: Well, workplace tensions at Google have just bubbled over worldwide, as it turns out. Employee activists, in a fairly short period of time, were able to mobilize many people across 70 of Google's offices worldwide. And it's because there's this concern that Google is mistreating women. That is, they might be letting harassers off the hook, or protecting executives that are accused of harassment or not paying women equally. And the backdrop to today's movement is that The New York Times last week detailed allegations against top executives, including Andrew Rubin, who was a key figure in creating Google's famous Android platform. Rubin denies sexually harassing anyone, but Google said it had credible evidence against him yet allowed him to resign quietly in 2014 and, notably, gave him a $90 million severance, which was, of course - you know, sparked a lot of outrage. SHAPIRO: Yeah. Golden parachute by any definition. How is Google responding to today's protests? NOGUCHI: Well, it's in a weird position because it's actually supporting its protesters. They don't obviously want to be seen as fighting the changes that their workers are asking for. And this afternoon, CEO Sundar Pichai spoke at a conference sponsored by The New York Times, and here's what he had to say. (SOUNDBITE OF ARCHIVED RECORDING)SUNDAR PICHAI: Moments like this show that we didn't always get it right, and so we are committed to doing better. We are listening to employees. That's partly why today is important. And, you know, and I think there are concrete steps coming out in terms of what we could do better. NOGUCHI: So basically what he's saying is that the company is taking this seriously and we're in the process of changing. Now, remember, this all happened before the #MeToo movement started and before the public was clamoring for accountability on this kind of thing. Google says that since Rubin's departure, the company has fired nearly 50 workers for sexual harassment, and none of them got any severance. SHAPIRO: And yet today's walkout shows that many employees are not satisfied with what the company has done. What are those people saying? NOGUCHI: Their position is that the company has a long way to go. There's still more that needs to change. And not just for Google itself, but for Silicon Valley at large, where you see a lot of these similar problems. One of the protesters in Washington, D. C. , was Aerica Banks. She's a patent analyst for Google who says she's been harassed throughout her career. AERICA BANKS: If Google is able to make these changes to end and appropriately give consequences to harassment and abuse and assault then hopefully that will ripple across the industry. SHAPIRO: Briefly, what changes is she talking about there? NOGUCHI: There's a list of demands that they want. They all kind of boil down to this idea of greater transparency around how the company deals with harassment and other issues, and equal opportunity and pay for women. And the CEO says he's looking at those and he's considering them. SHAPIRO: That's NPR's Yuki Noguchi. Thank you. NOGUCHI: Thank you. ARI SHAPIRO, HOST:  Thousands of Google employees around the world hit the streets today to show that they are fed up with the way the company handles sexual harassment complaints and to demand changes in workplace culture. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PEOPLE: (Chanting) Time is up. Time is up. Time is up. SHAPIRO: Chants there of time is up from employees outside Google's offices in New York City. Joining us to discuss the protest and Google's response is NPR's Yuki Noguchi. Hi, Yuki. YUKI NOGUCHI, BYLINE: Hi, Ari. SHAPIRO: How did Google reach this point? NOGUCHI: Well, workplace tensions at Google have just bubbled over worldwide, as it turns out. Employee activists, in a fairly short period of time, were able to mobilize many people across 70 of Google's offices worldwide. And it's because there's this concern that Google is mistreating women. That is, they might be letting harassers off the hook, or protecting executives that are accused of harassment or not paying women equally. And the backdrop to today's movement is that The New York Times last week detailed allegations against top executives, including Andrew Rubin, who was a key figure in creating Google's famous Android platform. Rubin denies sexually harassing anyone, but Google said it had credible evidence against him yet allowed him to resign quietly in 2014 and, notably, gave him a $90 million severance, which was, of course - you know, sparked a lot of outrage. SHAPIRO: Yeah. Golden parachute by any definition. How is Google responding to today's protests? NOGUCHI: Well, it's in a weird position because it's actually supporting its protesters. They don't obviously want to be seen as fighting the changes that their workers are asking for. And this afternoon, CEO Sundar Pichai spoke at a conference sponsored by The New York Times, and here's what he had to say. (SOUNDBITE OF ARCHIVED RECORDING) SUNDAR PICHAI: Moments like this show that we didn't always get it right, and so we are committed to doing better. We are listening to employees. That's partly why today is important. And, you know, and I think there are concrete steps coming out in terms of what we could do better. NOGUCHI: So basically what he's saying is that the company is taking this seriously and we're in the process of changing. Now, remember, this all happened before the #MeToo movement started and before the public was clamoring for accountability on this kind of thing. Google says that since Rubin's departure, the company has fired nearly 50 workers for sexual harassment, and none of them got any severance. SHAPIRO: And yet today's walkout shows that many employees are not satisfied with what the company has done. What are those people saying? NOGUCHI: Their position is that the company has a long way to go. There's still more that needs to change. And not just for Google itself, but for Silicon Valley at large, where you see a lot of these similar problems. One of the protesters in Washington, D. C. , was Aerica Banks. She's a patent analyst for Google who says she's been harassed throughout her career. AERICA BANKS: If Google is able to make these changes to end and appropriately give consequences to harassment and abuse and assault then hopefully that will ripple across the industry. SHAPIRO: Briefly, what changes is she talking about there? NOGUCHI: There's a list of demands that they want. They all kind of boil down to this idea of greater transparency around how the company deals with harassment and other issues, and equal opportunity and pay for women. And the CEO says he's looking at those and he's considering them. SHAPIRO: That's NPR's Yuki Noguchi. Thank you. NOGUCHI: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-01-662851489": {"title": "Google Employees Walk Out To Protest Company's Treatment Of Women : NPR", "url": "https://www.npr.org/2018/11/01/662851489/google-employees-plan-global-walkout-to-protest-companys-treatment-of-women", "author": "No author found", "published_date": "2018-11-01", "content": "", "section": "Business", "disclaimer": ""}, "2018-11-01-662603605": {"title": "Hawaii's Supreme Court OKs Construction Of Giant Telescope Despite Native Objections : NPR", "url": "https://www.npr.org/2018/11/01/662603605/hawaiis-supreme-court-oks-construction-of-giant-telescope-despite-native-objecti", "author": "No author found", "published_date": "2018-11-01", "content": "", "section": "Science", "disclaimer": ""}, "2018-11-02-662612151": {"title": "James Bridle: What Do Kids' Videos on YouTube Reveal About the Internet's Dark Side? : NPR", "url": "https://www.npr.org/2018/11/02/662612151/james-bridle-what-do-kids-videos-on-youtube-reveal-about-the-internets-dark-side", "author": "No author found", "published_date": "2018-11-02", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. I'm Guy Raz. (SOUNDBITE OF MUSIC)RAZ: So have you ever seen these surprise egg videos on YouTube? (SOUNDBITE OF YOUTUBE VIDEO)UNIDENTIFIED PERSON #1: Look. We've got all kinds of surprise egg for Doc McStuffins open. UNIDENTIFIED PERSON #2: More than 100 surprise eggs. RAZ: They're these videos for kids that show someone opening a plastic or chocolate egg, usually with some kind of cartoon character or superhero theme. (SOUNDBITE OF YOUTUBE VIDEO)UNIDENTIFIED PERSON #3: What Kinder Egg is that one? UNIDENTIFIED PERSON #4: It's a \"Frozen\" one. RAZ: And then finding a small toy inside. (SOUNDBITE OF YOUTUBE VIDEO)UNIDENTIFIED PERSON #5: Let's see what we have here. UNIDENTIFIED PERSON #6: We open egg in a car. RAZ: These videos don't just have a few thousand views. Some of them have hundreds of millions of views. And there are tons of them. (SOUNDBITE OF YOUTUBE VIDEO)UNIDENTIFIED PERSON #7: What's inside? (SOUNDBITE OF MUSIC)JAMES BRIDLE: Yeah, I mean, there's something that's evolved there that's just - that kids love. RAZ: This is James Bridle. He's a writer and artist. BRIDLE: And it's just this - these videos that can go on for hours at a time of just a pair of hands on the screen. . . (SOUNDBITE OF YOUTUBE VIDEO)UNIDENTIFIED PERSON #8: And let's see what this is. BRIDLE: . . . Softly and gently opening up product after product, kind of reveal what's inside. (SOUNDBITE OF YOUTUBE VIDEO)UNIDENTIFIED PERSON #8: A cookie. BRIDLE: And they're these incredibly sort of gentle, quiet, but - and seemingly endless. You know, once you watch one, then there's another one. There's another one. There's another one. And there's just vast amounts of this. (SOUNDBITE OF YOUTUBE VIDEO)UNIDENTIFIED PERSON #9: Why don't we get another egg and open it? BRIDLE: And it does something to kids brains, essentially. RAZ: What do you mean? BRIDLE: Well, I've been trying to understand it a little bit, and I'm really not a child psychologist or a kind of specialist in this area, which, also - I mean, more adults might be more familiar with unboxing videos, which have been around for a while, which is this kind of like fetishistic opening up of consumer goods. But if you look at the history of children's TV, for example, \"Sesame Street\" kind of pioneered this. And then I think there was a program in the U. S. that was called \"Blue's Clues\" or something like this? RAZ: Sure, yeah. (SOUNDBITE OF TV SHOW, \"BLUE'S CLUES\")STEVE BURNS: (As Steve) Hi, out there. It's me, Steve. Have you seen Blue, my puppy? BRIDLE: And the first innovation that \"Blue's Clues\" did was that they showed the same episode over and over again. Like, they showed the same episode for a week. (SOUNDBITE OF TV SHOW, \"BLUE'S CLUES\")BURNS: (As Steve) Hi, out there. It's me, Steve. Have you seen Blue, my puppy? BRIDLE: And they discovered - well, they knew in advance, but - what was shown, that the kids absolutely loved this. They loved the repetition of it. (SOUNDBITE OF TV SHOW, \"BLUE'S CLUES\")BURNS: (As Steve) Hi, out there. It's me, Steve. BRIDLE: And building a kind of a world that is predictable, in this way, seems to be like catnip for kids. And you can throw in these kind of little surprises which get you a little dopamine hit. And so when that's built into the kind of educational programs of something like \"Sesame Street,\" you can see it kind of being used for good. (SOUNDBITE OF MUSIC)BRIDLE: And that's not for me to say that these egg videos are necessarily being used for evil, but they've just picked out just that mechanism. (SOUNDBITE OF YOUTUBE VIDEO)UNIDENTIFIED PERSON #10: Assistant needs to hand me another egg. Which one do you think she's going to pick this time? BRIDLE: They're not trying to do anything else with it. They're not going to try and get it for kids to, like, hook them with that mechanism, and then teach them arithmetic. They're just using the hook. (SOUNDBITE OF YOUTUBE VIDEO)UNIDENTIFIED PERSON #11: Let's open them. Let's go. BRIDLE: And so that makes kids susceptible to it in super-obvious ways. It also makes adults super susceptible to it in less obvious and more complex, but I think also, like, you know, really quite dangerous and damaging ways, as well. (SOUNDBITE OF YOUTUBE VIDEO)UNIDENTIFIED PERSON #12: Yeah, I'm going to open my first one. Here we go. RAZ: James Bridle picks up this idea from the TED stage. (SOUNDBITE OF TED TALK)BRIDLE: So this is this is where we start. It's 2018, and someone, or lots of people, are using the same mechanism that, like, Facebook and Instagram are using to get you to keep checking that app. And they're using it on YouTube to hack the brains of very small children in return for advertising revenue. At least, I hope that's what they're doing, right? I hope that's what they're doing it for. Because there's easier ways of making ad revenue on YouTube, right? You can just make stuff up or steal stuff. So if you search for, like, really popular kids' cartoons, like \"Peppa Pig\" or \"Paw Patrol,\" you'll find that there's millions and millions of these online, as well. Of course, most of them aren't posted by the original content creators. They come from loads and loads of different kind of random accounts, and it's impossible to know who's posting them or what their motives might be. All right. Does that sound kind of familiar? Because really, it's exactly the same mechanism that's happening across most of our digital services, where it's impossible to know where this information is coming from. It's basically fake news for kids, all right, and we're training them from birth to click on the very first link that comes along, regardless of what the source is. That doesn't seem like a terribly good idea. (SOUNDBITE OF MUSIC)RAZ: A lot of the technology we invent can be amusing and educational. It can also be amazing and life changing. Because the human impulse to chase technological advancement is a fact. We can't stop that train. But how often do we pause and just think about the dark side of innovation? Well, today on the show, we're going to explore some of those unintended consequences and whether we have the capacity to manage them. Because, even if we can do something in bigger and faster and flashier ways, does that always mean we should? Well, when it comes to videos on YouTube or anywhere online, James Bridle says there could be big unintended them - and not just from getting kids addicted to them, but something even more unsettling. (SOUNDBITE OF TED TALK)BRIDLE: So the main way people get views on their videos - and remember, views mean money - is that they stuff the titles of these videos with these popular terms. So you take, like, surprise eggs and then you add \"Paw Patrol\" or Easter egg or whatever these things are - all of these words from other popular videos into your title - until you end up with this kind of meaningless mash of language - right? - that doesn't make sense to humans at all. Because, of course, it's only like really tiny kids who are watching your video, and what the hell do they know? Like, your real audience for this stuff is software. It's the algorithms. It's the software that YouTube uses to select which videos are like other videos, to make them popular, to make them recommended. And that's why you end up with this kind of completely meaningless mash, both of title and of content. And also, on the other side of the screen, there still are these little kids watching this stuff - right? - their full attention grabbed by these weird mechanisms. And so there's autoplay, where it just keeps playing these videos over and over and over on a loop, endlessly, for hours and hours at a time. And there's so much weirdness in the system now that autoplay takes you to some pretty strange places. This is how within, like, a dozen steps, you can go from a cute video of a counting train to masturbating Mickey Mouse. Yeah, I'm sorry about that. This does get worse. This is what happens when all of these different keywords, this desperate generation of content, all comes together into a single place. This is where all those deeply weird keywords come home to roost. The stuff that tends to upset parents is the stuff that has kind of violent or sexual content, right? Children's cartoons getting assaulted, getting killed, weird pranks that actually genuinely terrify children. What you have is software pulling in all of these different influences to automatically generate kids' worst nightmares. And this stuff really, really does affect small children, all right? Parents report their children being traumatized, becoming afraid of the dark, becoming afraid of their favorite cartoon characters. If you take one thing away from this, it's that if you have small children, keep them the hell away from YouTube. (SOUNDBITE OF MUSIC)RAZ: You know, I - I was talking to the child of a friend of mine who's in high school. You probably know this, but when a high school student wants to learn how to do something, they will find a video on YouTube. There's a video on YouTube for almost anything. You know, how to fix your car or how to boil an egg in an Instant Pot, which I just watched because I wanted to know how to boil an egg in an Instant Pot. And I, as a 43-year-old man, am just kind of discovering this world, right? There are amazing things about access to YouTube and, obviously, the Internet and technology - right? - which have been transformational. But at the same time, I have to wonder whether we are all, especially people who were born into that digital world, are all part of this giant uncontrolled experiment. And we just don't really know what the results of that experiment will be, how it will change us as a species. Am I sounding like a crazy person or is there something to that? BRIDLE: No, I think you're totally right. I think describing it as a kind of grand experiment is spot on. We have kind of released these - but, I mean, the release of any new technology or any one of these advances is always an experiment. RAZ: Yeah, sure. BRIDLE: Like, it's impossible to test these things at scale. The thing that I find fascinating, really, is the fact that we're not really paying attention to the results of that experiment. Because the point of an experiment is you test something, and then you make decisions or changes based on the results of that experiment. RAZ: Yeah. BRIDLE: And it's fairly clear that the experiment, which we've been participating in for some time now, of completely unregulated, particularly advert-driven content online is not one that's working out very well. And you can see the results of that at kind of multiple levels. You can see it in the kind of the weird kid stuff that we're talking about, but you can also see it at the larger scale of this kind of, like, dilution of knowledge or this kind of fundamentalization of knowledge, by which I mean that, you know, one of the other things that YouTube optimizes for is sensation. And this has become really clear that, particularly as YouTube has become this kind of repository of knowledge that people are going to look for, you know, certain systems of knowledge are designed that when you discover something and you want to discover more, it takes you deeper and deeper into that subject. YouTube is designed to show you the thing about that subject that is the most sensational, right? Which is why, I mean, just the other day I was watching a speech from Walter Cronkite, from the early '80s, about climate change, right? And it's really interesting to watch from back then just, like, how obvious and subtle this debate was. But the YouTube's autoplay's next recommendation was a three-hour speech from a climate change denier - right? - from a year ago. And that's not because YouTube holds some inherent belief about climate change. It's because that content is sensational. And what YouTube wants to do is show you things that will cause you strong reactions because that's what gets you watching. So we've decided to optimize for reactions and sensation over other forms of kind of verifying knowledge. But we've decided to do that, and we could decide to do otherwise if we pay attention to the results of this experiment. (SOUNDBITE OF MUSIC)RAZ: James Bridle. He's a writer, artist and author of the book \"New Dark Age: Technology And The End Of The future. \" You can see his full talk at ted. com. GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. (SOUNDBITE OF MUSIC) RAZ: So have you ever seen these surprise egg videos on YouTube? (SOUNDBITE OF YOUTUBE VIDEO) UNIDENTIFIED PERSON #1: Look. We've got all kinds of surprise egg for Doc McStuffins open. UNIDENTIFIED PERSON #2: More than 100 surprise eggs. RAZ: They're these videos for kids that show someone opening a plastic or chocolate egg, usually with some kind of cartoon character or superhero theme. (SOUNDBITE OF YOUTUBE VIDEO) UNIDENTIFIED PERSON #3: What Kinder Egg is that one? UNIDENTIFIED PERSON #4: It's a \"Frozen\" one. RAZ: And then finding a small toy inside. (SOUNDBITE OF YOUTUBE VIDEO) UNIDENTIFIED PERSON #5: Let's see what we have here. UNIDENTIFIED PERSON #6: We open egg in a car. RAZ: These videos don't just have a few thousand views. Some of them have hundreds of millions of views. And there are tons of them. (SOUNDBITE OF YOUTUBE VIDEO) UNIDENTIFIED PERSON #7: What's inside? (SOUNDBITE OF MUSIC) JAMES BRIDLE: Yeah, I mean, there's something that's evolved there that's just - that kids love. RAZ: This is James Bridle. He's a writer and artist. BRIDLE: And it's just this - these videos that can go on for hours at a time of just a pair of hands on the screen. . . (SOUNDBITE OF YOUTUBE VIDEO) UNIDENTIFIED PERSON #8: And let's see what this is. BRIDLE: . . . Softly and gently opening up product after product, kind of reveal what's inside. (SOUNDBITE OF YOUTUBE VIDEO) UNIDENTIFIED PERSON #8: A cookie. BRIDLE: And they're these incredibly sort of gentle, quiet, but - and seemingly endless. You know, once you watch one, then there's another one. There's another one. There's another one. And there's just vast amounts of this. (SOUNDBITE OF YOUTUBE VIDEO) UNIDENTIFIED PERSON #9: Why don't we get another egg and open it? BRIDLE: And it does something to kids brains, essentially. RAZ: What do you mean? BRIDLE: Well, I've been trying to understand it a little bit, and I'm really not a child psychologist or a kind of specialist in this area, which, also - I mean, more adults might be more familiar with unboxing videos, which have been around for a while, which is this kind of like fetishistic opening up of consumer goods. But if you look at the history of children's TV, for example, \"Sesame Street\" kind of pioneered this. And then I think there was a program in the U. S. that was called \"Blue's Clues\" or something like this? RAZ: Sure, yeah. (SOUNDBITE OF TV SHOW, \"BLUE'S CLUES\") STEVE BURNS: (As Steve) Hi, out there. It's me, Steve. Have you seen Blue, my puppy? BRIDLE: And the first innovation that \"Blue's Clues\" did was that they showed the same episode over and over again. Like, they showed the same episode for a week. (SOUNDBITE OF TV SHOW, \"BLUE'S CLUES\") BURNS: (As Steve) Hi, out there. It's me, Steve. Have you seen Blue, my puppy? BRIDLE: And they discovered - well, they knew in advance, but - what was shown, that the kids absolutely loved this. They loved the repetition of it. (SOUNDBITE OF TV SHOW, \"BLUE'S CLUES\") BURNS: (As Steve) Hi, out there. It's me, Steve. BRIDLE: And building a kind of a world that is predictable, in this way, seems to be like catnip for kids. And you can throw in these kind of little surprises which get you a little dopamine hit. And so when that's built into the kind of educational programs of something like \"Sesame Street,\" you can see it kind of being used for good. (SOUNDBITE OF MUSIC) BRIDLE: And that's not for me to say that these egg videos are necessarily being used for evil, but they've just picked out just that mechanism. (SOUNDBITE OF YOUTUBE VIDEO) UNIDENTIFIED PERSON #10: Assistant needs to hand me another egg. Which one do you think she's going to pick this time? BRIDLE: They're not trying to do anything else with it. They're not going to try and get it for kids to, like, hook them with that mechanism, and then teach them arithmetic. They're just using the hook. (SOUNDBITE OF YOUTUBE VIDEO) UNIDENTIFIED PERSON #11: Let's open them. Let's go. BRIDLE: And so that makes kids susceptible to it in super-obvious ways. It also makes adults super susceptible to it in less obvious and more complex, but I think also, like, you know, really quite dangerous and damaging ways, as well. (SOUNDBITE OF YOUTUBE VIDEO) UNIDENTIFIED PERSON #12: Yeah, I'm going to open my first one. Here we go. RAZ: James Bridle picks up this idea from the TED stage. (SOUNDBITE OF TED TALK) BRIDLE: So this is this is where we start. It's 2018, and someone, or lots of people, are using the same mechanism that, like, Facebook and Instagram are using to get you to keep checking that app. And they're using it on YouTube to hack the brains of very small children in return for advertising revenue. At least, I hope that's what they're doing, right? I hope that's what they're doing it for. Because there's easier ways of making ad revenue on YouTube, right? You can just make stuff up or steal stuff. So if you search for, like, really popular kids' cartoons, like \"Peppa Pig\" or \"Paw Patrol,\" you'll find that there's millions and millions of these online, as well. Of course, most of them aren't posted by the original content creators. They come from loads and loads of different kind of random accounts, and it's impossible to know who's posting them or what their motives might be. All right. Does that sound kind of familiar? Because really, it's exactly the same mechanism that's happening across most of our digital services, where it's impossible to know where this information is coming from. It's basically fake news for kids, all right, and we're training them from birth to click on the very first link that comes along, regardless of what the source is. That doesn't seem like a terribly good idea. (SOUNDBITE OF MUSIC) RAZ: A lot of the technology we invent can be amusing and educational. It can also be amazing and life changing. Because the human impulse to chase technological advancement is a fact. We can't stop that train. But how often do we pause and just think about the dark side of innovation? Well, today on the show, we're going to explore some of those unintended consequences and whether we have the capacity to manage them. Because, even if we can do something in bigger and faster and flashier ways, does that always mean we should? Well, when it comes to videos on YouTube or anywhere online, James Bridle says there could be big unintended them - and not just from getting kids addicted to them, but something even more unsettling. (SOUNDBITE OF TED TALK) BRIDLE: So the main way people get views on their videos - and remember, views mean money - is that they stuff the titles of these videos with these popular terms. So you take, like, surprise eggs and then you add \"Paw Patrol\" or Easter egg or whatever these things are - all of these words from other popular videos into your title - until you end up with this kind of meaningless mash of language - right? - that doesn't make sense to humans at all. Because, of course, it's only like really tiny kids who are watching your video, and what the hell do they know? Like, your real audience for this stuff is software. It's the algorithms. It's the software that YouTube uses to select which videos are like other videos, to make them popular, to make them recommended. And that's why you end up with this kind of completely meaningless mash, both of title and of content. And also, on the other side of the screen, there still are these little kids watching this stuff - right? - their full attention grabbed by these weird mechanisms. And so there's autoplay, where it just keeps playing these videos over and over and over on a loop, endlessly, for hours and hours at a time. And there's so much weirdness in the system now that autoplay takes you to some pretty strange places. This is how within, like, a dozen steps, you can go from a cute video of a counting train to masturbating Mickey Mouse. Yeah, I'm sorry about that. This does get worse. This is what happens when all of these different keywords, this desperate generation of content, all comes together into a single place. This is where all those deeply weird keywords come home to roost. The stuff that tends to upset parents is the stuff that has kind of violent or sexual content, right? Children's cartoons getting assaulted, getting killed, weird pranks that actually genuinely terrify children. What you have is software pulling in all of these different influences to automatically generate kids' worst nightmares. And this stuff really, really does affect small children, all right? Parents report their children being traumatized, becoming afraid of the dark, becoming afraid of their favorite cartoon characters. If you take one thing away from this, it's that if you have small children, keep them the hell away from YouTube. (SOUNDBITE OF MUSIC) RAZ: You know, I - I was talking to the child of a friend of mine who's in high school. You probably know this, but when a high school student wants to learn how to do something, they will find a video on YouTube. There's a video on YouTube for almost anything. You know, how to fix your car or how to boil an egg in an Instant Pot, which I just watched because I wanted to know how to boil an egg in an Instant Pot. And I, as a 43-year-old man, am just kind of discovering this world, right? There are amazing things about access to YouTube and, obviously, the Internet and technology - right? - which have been transformational. But at the same time, I have to wonder whether we are all, especially people who were born into that digital world, are all part of this giant uncontrolled experiment. And we just don't really know what the results of that experiment will be, how it will change us as a species. Am I sounding like a crazy person or is there something to that? BRIDLE: No, I think you're totally right. I think describing it as a kind of grand experiment is spot on. We have kind of released these - but, I mean, the release of any new technology or any one of these advances is always an experiment. RAZ: Yeah, sure. BRIDLE: Like, it's impossible to test these things at scale. The thing that I find fascinating, really, is the fact that we're not really paying attention to the results of that experiment. Because the point of an experiment is you test something, and then you make decisions or changes based on the results of that experiment. RAZ: Yeah. BRIDLE: And it's fairly clear that the experiment, which we've been participating in for some time now, of completely unregulated, particularly advert-driven content online is not one that's working out very well. And you can see the results of that at kind of multiple levels. You can see it in the kind of the weird kid stuff that we're talking about, but you can also see it at the larger scale of this kind of, like, dilution of knowledge or this kind of fundamentalization of knowledge, by which I mean that, you know, one of the other things that YouTube optimizes for is sensation. And this has become really clear that, particularly as YouTube has become this kind of repository of knowledge that people are going to look for, you know, certain systems of knowledge are designed that when you discover something and you want to discover more, it takes you deeper and deeper into that subject. YouTube is designed to show you the thing about that subject that is the most sensational, right? Which is why, I mean, just the other day I was watching a speech from Walter Cronkite, from the early '80s, about climate change, right? And it's really interesting to watch from back then just, like, how obvious and subtle this debate was. But the YouTube's autoplay's next recommendation was a three-hour speech from a climate change denier - right? - from a year ago. And that's not because YouTube holds some inherent belief about climate change. It's because that content is sensational. And what YouTube wants to do is show you things that will cause you strong reactions because that's what gets you watching. So we've decided to optimize for reactions and sensation over other forms of kind of verifying knowledge. But we've decided to do that, and we could decide to do otherwise if we pay attention to the results of this experiment. (SOUNDBITE OF MUSIC) RAZ: James Bridle. He's a writer, artist and author of the book \"New Dark Age: Technology And The End Of The future. \" You can see his full talk at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-02-662631773": {"title": "Kashmir Hill: Do Your Smart Devices Know Too Much? : NPR", "url": "https://www.npr.org/2018/11/02/662631773/kashmir-hill-do-your-smart-devices-know-too-much", "author": "No author found", "published_date": "2018-11-02", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, Unintended Consequences - ideas about the things we invent to make life better, and the outcomes we never see coming. (SOUNDBITE OF MUSIC)RAZ: Do you remember, like, the first smart device you got? KASHMIR HILL: Well, I think the Fitbit was the first thing I got. . . RAZ: Yeah. HILL: . . . That was kind of Internet of things. Like, taking real-world data and quantifying it for me so I could reflect on it, and it could, you know, improve my life, improve my fitness. RAZ: And did it? HILL: Um (ph). . . RAZ: This is Kashmir Hill. HILL: (Laughter) No. RAZ: Kashmir is a tech reporter for Gizmodo Media. HILL: I'm specifically focused on privacy. So I spend a lot of time thinking about the way that technology is changing the way that we live and what happens to our data and our information. RAZ: Do you remember when you first heard the term Internet of things? Like, oh, you're going to have a smart fridge that's going to tell you you need more eggs. And you're going to have a smart coffee maker that's going to, you know, tell you how much coffee you're drinking. And everything in your kitchen is going to be connected, and it's going to be awesome. And I'm thinking, that's awesome. It's like those \"Wallace and Gromit\" movies where he's, like, you know, like springboarded off the bed, and then, like, drops into his breakfast seat, and there's the toast and the coffee, you know. I was really - this was exciting to me. HILL: Yeah, and hopefully, you know, they would do all this work for you, so you would have more free time to do more interesting, creative things or watch more Netflix. But yeah, I was kind of imagining my house anticipating my needs and really taking care of me. (SOUNDBITE OF MUSIC)RAZ: And Kashmir actually got a chance to experience what it would be like to live in an entire home filled with smart devices. HILL: I got a smart toothbrush, a smart coffeemaker, a robot vacuum - the Roomba. (Laughter) I bought a smart sex toy. RAZ: Except that things didn't go exactly as planned. Kashmir Hill picks up the story from the TED stage. (SOUNDBITE OF TED TALK)HILL: Being smart means the device can connect to the Internet, it can gather data and it can talk to its owner. But once your appliances can talk to you, who else are they going to be talking to? I wanted to find out, so I went all-in and turned my one-bedroom apartment in San Francisco into a smart home. Altogether, I installed 18 Internet-connected devices in my home. I also installed a Surya. SURYA MATTU: Hi, I'm Surya. (LAUGHTER)MATTU: And I monitored everything the smart home did. I built a special router that let me look at all the network activity. HILL: Surya and I are both journalists. He's not my husband. We just work together at Gizmodo. MATTU: Thank you for clarifying (laughter). The devices Kashmir bought - we were interested in understanding what they were saying to their manufacturers. But we were also interested in understanding what the home's digital emissions looked like to the Internet service provider. We were seeing what her ISP could see, but more importantly, what they could sell. HILL: We ran the experiment for two months. In that two months, there wasn't a single hour of digital silence in the house - not even when we went away for a week. MATTU: Yeah, it's so true. Based on the data, I knew when you guys woke up, and I when you went to bed. I even knew when Kashmir brushed her teeth. The devices Kashmir bought almost all pinged their servers daily. But do you know which device was especially chatty? The Amazon Echo. It contacted its servers every three minutes, regardless of whether you were using it or not. (SOUNDBITE OF MUSIC)RAZ: Wow. I'm just wondering here, with all these smart devices, what are we actually giving up? HILL: I mean, so there's a couple of things. I always focus on privacy. There are definitely security concerns by connecting our devices to the Internet. Devices are made by companies that have traditionally not been Internet companies, so they are not as savvy about internet security. We are exposing ourselves to the possibility of people intruding in our homes through these devices. So that has happened with baby monitors, for example. And so you had hackers that were accessing cameras in baby's rooms, sometimes even able to talk to the babies. So that's really alarming. What I was more concerned about was this tracking of what we're doing in our most intimate spaces and what is eventually done with that data and just the feeling of being constantly observed in our own homes, how that changes the - really, the sanctity of the home. (SOUNDBITE OF TED TALK)MATTU: The devices Kashmir bought range from useful to annoying, but the thing they all had in common was sharing data with the companies that made them. With email service providers and social media, we've long been told that if it's free, you're the product. But with the Internet of things, it seems, even if you pay, you're still the product. So you really have to ask, who is the true beneficiary of your smart home, you or the company mining you? (SOUNDBITE OF MUSIC)RAZ: So when a smart TV or an Amazon Echo or Google Home or whatever device you have - right? - when those devices are pinging the companies and then sending that data back, I mean, they're just building a profile of who we are. HILL: I mean, absolutely. You know Roomba that makes my - iRobot makes Roomba, the smart vacuum. So they know, for example, like, how often I vacuum my house and which parts of the house are dirty. The CEO of iRobot, at one point, was talking to journalists and said, yeah, we actually have access to great data. We have maps of people's homes, and we could potentially sell that to all these - you know, these companies are getting into the smart home market, like Google and Amazon and Apple. And people who had Roombas flipped out because they hadn't thought about that at all - that their vacuum was sucking up information about their home that could potentially be sold. An iRobot CEO later walked it back and said, oh, you know, we'd never do that without people's consent. But it was one of these wake-ups to the fact that these devices in our homes that don't - you know, they don't look like data collectors. They don't look like cameras. They don't have obvious lenses that they are watching what we're doing and collecting information about what we're doing. And I think every single company now is thinking, you know, how do we monetize data? Data's the new oil. What can we do to make new revenue streams? And they're looking at data. RAZ: Yeah. Yes. I mean, once there are these comprehensive profiles of our behaviors and our habits and our likes and dislikes, I mean, we could be judged before we even walk through the door, like for a bank loan or a job or, you know, or anything. HILL: Right. And I don't think - companies don't think of this as nefarious. They say, like, oh, we just want to know what you want to buy. RAZ: Yeah. HILL: But I think what we've seen with online tracking is that there are nefarious uses of this data - you know, attempts to manipulate the way that we think about the world, try to influence the way that we vote. And it's all happening with a profile of us that we don't know about that's been compiled by a company we've never heard of before. And it's just creating this real paranoia for people because they don't know why they're seeing what they're seeing, but they kind of know that there's this data collection going on. And I think people are getting really worried about how these companies are influencing us and how much access to our data they have. (SOUNDBITE OF MUSIC)RAZ: Kashmir Hill - she's a reporter for Gizmodo Media. You can see Kashmir and Surya's full talk at ted. com. GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, Unintended Consequences - ideas about the things we invent to make life better, and the outcomes we never see coming. (SOUNDBITE OF MUSIC) RAZ: Do you remember, like, the first smart device you got? KASHMIR HILL: Well, I think the Fitbit was the first thing I got. . . RAZ: Yeah. HILL: . . . That was kind of Internet of things. Like, taking real-world data and quantifying it for me so I could reflect on it, and it could, you know, improve my life, improve my fitness. RAZ: And did it? HILL: Um (ph). . . RAZ: This is Kashmir Hill. HILL: (Laughter) No. RAZ: Kashmir is a tech reporter for Gizmodo Media. HILL: I'm specifically focused on privacy. So I spend a lot of time thinking about the way that technology is changing the way that we live and what happens to our data and our information. RAZ: Do you remember when you first heard the term Internet of things? Like, oh, you're going to have a smart fridge that's going to tell you you need more eggs. And you're going to have a smart coffee maker that's going to, you know, tell you how much coffee you're drinking. And everything in your kitchen is going to be connected, and it's going to be awesome. And I'm thinking, that's awesome. It's like those \"Wallace and Gromit\" movies where he's, like, you know, like springboarded off the bed, and then, like, drops into his breakfast seat, and there's the toast and the coffee, you know. I was really - this was exciting to me. HILL: Yeah, and hopefully, you know, they would do all this work for you, so you would have more free time to do more interesting, creative things or watch more Netflix. But yeah, I was kind of imagining my house anticipating my needs and really taking care of me. (SOUNDBITE OF MUSIC) RAZ: And Kashmir actually got a chance to experience what it would be like to live in an entire home filled with smart devices. HILL: I got a smart toothbrush, a smart coffeemaker, a robot vacuum - the Roomba. (Laughter) I bought a smart sex toy. RAZ: Except that things didn't go exactly as planned. Kashmir Hill picks up the story from the TED stage. (SOUNDBITE OF TED TALK) HILL: Being smart means the device can connect to the Internet, it can gather data and it can talk to its owner. But once your appliances can talk to you, who else are they going to be talking to? I wanted to find out, so I went all-in and turned my one-bedroom apartment in San Francisco into a smart home. Altogether, I installed 18 Internet-connected devices in my home. I also installed a Surya. SURYA MATTU: Hi, I'm Surya. (LAUGHTER) MATTU: And I monitored everything the smart home did. I built a special router that let me look at all the network activity. HILL: Surya and I are both journalists. He's not my husband. We just work together at Gizmodo. MATTU: Thank you for clarifying (laughter). The devices Kashmir bought - we were interested in understanding what they were saying to their manufacturers. But we were also interested in understanding what the home's digital emissions looked like to the Internet service provider. We were seeing what her ISP could see, but more importantly, what they could sell. HILL: We ran the experiment for two months. In that two months, there wasn't a single hour of digital silence in the house - not even when we went away for a week. MATTU: Yeah, it's so true. Based on the data, I knew when you guys woke up, and I when you went to bed. I even knew when Kashmir brushed her teeth. The devices Kashmir bought almost all pinged their servers daily. But do you know which device was especially chatty? The Amazon Echo. It contacted its servers every three minutes, regardless of whether you were using it or not. (SOUNDBITE OF MUSIC) RAZ: Wow. I'm just wondering here, with all these smart devices, what are we actually giving up? HILL: I mean, so there's a couple of things. I always focus on privacy. There are definitely security concerns by connecting our devices to the Internet. Devices are made by companies that have traditionally not been Internet companies, so they are not as savvy about internet security. We are exposing ourselves to the possibility of people intruding in our homes through these devices. So that has happened with baby monitors, for example. And so you had hackers that were accessing cameras in baby's rooms, sometimes even able to talk to the babies. So that's really alarming. What I was more concerned about was this tracking of what we're doing in our most intimate spaces and what is eventually done with that data and just the feeling of being constantly observed in our own homes, how that changes the - really, the sanctity of the home. (SOUNDBITE OF TED TALK) MATTU: The devices Kashmir bought range from useful to annoying, but the thing they all had in common was sharing data with the companies that made them. With email service providers and social media, we've long been told that if it's free, you're the product. But with the Internet of things, it seems, even if you pay, you're still the product. So you really have to ask, who is the true beneficiary of your smart home, you or the company mining you? (SOUNDBITE OF MUSIC) RAZ: So when a smart TV or an Amazon Echo or Google Home or whatever device you have - right? - when those devices are pinging the companies and then sending that data back, I mean, they're just building a profile of who we are. HILL: I mean, absolutely. You know Roomba that makes my - iRobot makes Roomba, the smart vacuum. So they know, for example, like, how often I vacuum my house and which parts of the house are dirty. The CEO of iRobot, at one point, was talking to journalists and said, yeah, we actually have access to great data. We have maps of people's homes, and we could potentially sell that to all these - you know, these companies are getting into the smart home market, like Google and Amazon and Apple. And people who had Roombas flipped out because they hadn't thought about that at all - that their vacuum was sucking up information about their home that could potentially be sold. An iRobot CEO later walked it back and said, oh, you know, we'd never do that without people's consent. But it was one of these wake-ups to the fact that these devices in our homes that don't - you know, they don't look like data collectors. They don't look like cameras. They don't have obvious lenses that they are watching what we're doing and collecting information about what we're doing. And I think every single company now is thinking, you know, how do we monetize data? Data's the new oil. What can we do to make new revenue streams? And they're looking at data. RAZ: Yeah. Yes. I mean, once there are these comprehensive profiles of our behaviors and our habits and our likes and dislikes, I mean, we could be judged before we even walk through the door, like for a bank loan or a job or, you know, or anything. HILL: Right. And I don't think - companies don't think of this as nefarious. They say, like, oh, we just want to know what you want to buy. RAZ: Yeah. HILL: But I think what we've seen with online tracking is that there are nefarious uses of this data - you know, attempts to manipulate the way that we think about the world, try to influence the way that we vote. And it's all happening with a profile of us that we don't know about that's been compiled by a company we've never heard of before. And it's just creating this real paranoia for people because they don't know why they're seeing what they're seeing, but they kind of know that there's this data collection going on. And I think people are getting really worried about how these companies are influencing us and how much access to our data they have. (SOUNDBITE OF MUSIC) RAZ: Kashmir Hill - she's a reporter for Gizmodo Media. You can see Kashmir and Surya's full talk at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-02-662619612": {"title": "Yuval Noah Harari: Could Big Data Destroy Liberal Democracy? : NPR", "url": "https://www.npr.org/2018/11/02/662619612/yuval-noah-harari-could-big-data-destroy-liberal-democracy", "author": "No author found", "published_date": "2018-11-02", "content": "GUY RAZ, HOST: Could we just acknowledge, for a moment, that things like big data and AI are going to be revolutionary? I mean, they are going to change everything. YUVAL NOAH HARARI: Yes, in almost every field of action, and the opportunities are amazing. (SOUNDBITE OF MUSIC)RAZ: This is Yuval Noah Harari. He's a historian and an author. HARARI: So in 30 years, artificial intelligence and biometric sensors might provide even the poorest people in society with far better health care than the richest people get today. You can hardly think of a system, whether it's communication or traffic or electricity, which won't benefit from these kinds of developments. RAZ: But Yuval thinks that, in the bigger picture, how AI and big data might affect political power could have really dangerous unintended consequences. HARARI: More and more governments are reaching the conclusion that this is like the industrial revolution in the 19th century. Whoever leads the world in AI will dominate the entire world. RAZ: And when we come back in just a moment, Yuval Harari will explain how AI might threaten to destroy liberal democracy. Stay with us. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC)RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about Unintended Consequences and the dark side of innovation. And just a moment ago, we were hearing from historian Yuval Noah Harari, who warns that AI and big data could present a real threat to liberal democracy. HARARI: If you look at the clash between communism and liberalism, you can say it was a clash between two different systems for processing data and for making decisions. The liberal system is, in essence, a distributed system. It distributes information and the power to make decisions between many individuals and organizations. In contrast, communism and other dictatorial systems - they centralize. They concentrate all the information and power in one place, like in Moscow in the case of the Soviet Union. RAZ: Yeah. HARARI: Now, given the technology of the 20th century, it was simply inefficient because nobody had the ability to process all the information fast enough and make good decisions - I mean, how many cabbages to grow in the Soviet Union. How many cars to manufacture - how much will each car cost? We tried to make all these decisions in one place when what you have is typewriters and filing cabinets and pen and paper and things like that. It just doesn't work. (SOUNDBITE OF MUSIC)HARARI: And this is one of the main reasons, if not the main reason, that the Soviet Union collapsed. So this was really a result of the prevailing technological conditions. RAZ: But those technological conditions have obviously changed. Here's more from Yuval Noah Harari on the TED stage. (SOUNDBITE OF TED TALK)HARARI: In the 20th century, democracy and capitalism defeated fascism and communism because democracy was better at processing data and making decisions. But it is not a law of nature that centralized data processing is always less efficient than distributed data processing. With the rise of artificial intelligence and machine learning, it might become feasible to process enormous amounts of information very efficiently in one place, and then centralized data processing will be more efficient than distributed data processing. The greatest danger that now faces liberal democracy is that the revolution in information technology will make dictatorships more efficient than democracies, and then the main handicap of authoritarian regimes in the 20th century - their attempt to concentrate all the information in one place - it will become their greatest advantage. RAZ: So are you saying that the threat to liberal democracy increases as the ability of machines to process more and more amounts of data improves? HARARI: Yes, in many ways. So in the 20th century, the supporters of liberal democracy had a kind of relatively easy time because you did not have to choose between ethics and efficiency. The most ethical thing to do was also the most efficient thing to do. To give power to the people, to give freedom to individuals - all these things were good, both ethically and economically. And most governments around the world that liberalized their societies in the last few decades - they thought, if we want a thriving economy like the U. S. economy or like the German economy, we need to liberalize our societies. So even if we don't like very much to do it, we have to do it. But what happens if, suddenly, this is no longer the case? It's still the best thing to do from an ethical perspective - to protect the privacy and the rights of individuals. But it's no longer the most efficient thing to do. The most efficient thing to do is perhaps to build these giant databases, which ignore completely the privacy and the rights of individuals. And it's the most efficient thing to do to allow algorithms to make decisions on behalf of human beings. The algorithms will decide who we will accept to these universities. The algorithms will tell you what to study and where to live and even whom to marry. And if this is more efficient, what happens to the ideals of freedom and human rights and individualism? This becomes a much more problematic issue than in the 20th century. (SOUNDBITE OF TED TALK)HARARI: Another technological danger that threatens the future of democracy is the merger of information technology with biotechnology, which might result in the creation of algorithms that know me better than I know myself. And once you have such algorithms, an external system, like the government, cannot just predict my decisions, it can also manipulate my feelings, my emotions. A dictator may not be able to provide me with good health care, but he will be able to make me love him and to make me hate the opposition. The enemies of liberal democracy, they have a method. They hack our feelings, not our emails, not our bank accounts - they hack our feelings of fear and hate and vanity, and then use these feelings to polarize and destroy democracy from within because, in the end, democracy is not based on the human rationality. It's based on human feelings. During elections and referendums, you're not being asked, what do you think? You're actually being asked, how do you feel? And if somebody can manipulate your emotions effectively, democracy will become an emotional puppet show. RAZ: So your conclusion is, he who controls the data, controls the people. HARARI: Yes, and if you start with the understanding that, at least according to science, our feelings do not represent some mystical free will. They represent biochemical processes in our bodies and, of course, influences from the environment. Now, what we also need to remember is that it should be technically possible to decipher, to hack human beings and human feelings. In order to hack a human being, you need a lot of biological knowledge and you need a lot of computing power. And until today, nobody could do it. RAZ: Yeah. HARARI: And therefore, people could believe that humans are unhackable - that human feelings reflect free will, and nobody can ever understand me and manipulate me. And this was true for the whole of history, but this is no longer true. Once you have a system that can decipher the human operating system, it can predict human decisions, and it can manipulate human desires and human feelings. I mean, until today, no politician really had the ability to understand the human emotional system. By trial and error, they see what works, and it changes all the time. But if we reach a point when we can reliably decipher the human biochemical system and basically sell you anything, whether it's a product or a politician, then we have a completely new kind of politics. RAZ: You know, I know that you probably come across, you know, people who have helped to create this technology - people who had this kind of utopian idea of how data and data processing could change the world in positive ways, you know? But those same people, you have to wonder whether they stop to think about the unintended consequences. HARARI: When you develop this kind of the technology, in most cases, you obviously focus on the positive implications. And until today, humankind has managed to avoid the worst consequences. The most obvious example is nuclear technology. All the doomsday prophecies from the 1950s and 1960s about a nuclear war which will destroy human civilization, it didn't happen. Humankind successfully rose to the challenge of nuclear technology. Whether we can do it again with AI and with biotechnology is an open question. RAZ: Yuval, you will know this well as an Israeli - somebody who lives in the biblical lands. Prophets are rarely rewarded. In fact, they're usually disliked, even when they're right. And oftentimes, when they're right, it doesn't matter because their warnings are so dark, and we ignore them at our peril. HARARI: I definitely don't see myself as a prophet. And I don't think that anybody can prophesize the future. Actually, it's pointless. Again, I define myself as a historian. And what I try to do is map different possibilities. There are always more than one way in which we can go from here. And the reason I think it's important to have this discussion is because it's not too late. (SOUNDBITE OF MUSIC)HARARI: I see my job in changing the discussion in the present. We can still influence the direction in which this technology is going. There are always different possibilities. RAZ: Yuval Noah Harari teaches history at the Hebrew University of Jerusalem. His latest book is called \"21 Lessons For The 21st Century. \" You can see his entire talk at ted. com. GUY RAZ, HOST:  Could we just acknowledge, for a moment, that things like big data and AI are going to be revolutionary? I mean, they are going to change everything. YUVAL NOAH HARARI: Yes, in almost every field of action, and the opportunities are amazing. (SOUNDBITE OF MUSIC) RAZ: This is Yuval Noah Harari. He's a historian and an author. HARARI: So in 30 years, artificial intelligence and biometric sensors might provide even the poorest people in society with far better health care than the richest people get today. You can hardly think of a system, whether it's communication or traffic or electricity, which won't benefit from these kinds of developments. RAZ: But Yuval thinks that, in the bigger picture, how AI and big data might affect political power could have really dangerous unintended consequences. HARARI: More and more governments are reaching the conclusion that this is like the industrial revolution in the 19th century. Whoever leads the world in AI will dominate the entire world. RAZ: And when we come back in just a moment, Yuval Harari will explain how AI might threaten to destroy liberal democracy. Stay with us. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC) RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about Unintended Consequences and the dark side of innovation. And just a moment ago, we were hearing from historian Yuval Noah Harari, who warns that AI and big data could present a real threat to liberal democracy. HARARI: If you look at the clash between communism and liberalism, you can say it was a clash between two different systems for processing data and for making decisions. The liberal system is, in essence, a distributed system. It distributes information and the power to make decisions between many individuals and organizations. In contrast, communism and other dictatorial systems - they centralize. They concentrate all the information and power in one place, like in Moscow in the case of the Soviet Union. RAZ: Yeah. HARARI: Now, given the technology of the 20th century, it was simply inefficient because nobody had the ability to process all the information fast enough and make good decisions - I mean, how many cabbages to grow in the Soviet Union. How many cars to manufacture - how much will each car cost? We tried to make all these decisions in one place when what you have is typewriters and filing cabinets and pen and paper and things like that. It just doesn't work. (SOUNDBITE OF MUSIC) HARARI: And this is one of the main reasons, if not the main reason, that the Soviet Union collapsed. So this was really a result of the prevailing technological conditions. RAZ: But those technological conditions have obviously changed. Here's more from Yuval Noah Harari on the TED stage. (SOUNDBITE OF TED TALK) HARARI: In the 20th century, democracy and capitalism defeated fascism and communism because democracy was better at processing data and making decisions. But it is not a law of nature that centralized data processing is always less efficient than distributed data processing. With the rise of artificial intelligence and machine learning, it might become feasible to process enormous amounts of information very efficiently in one place, and then centralized data processing will be more efficient than distributed data processing. The greatest danger that now faces liberal democracy is that the revolution in information technology will make dictatorships more efficient than democracies, and then the main handicap of authoritarian regimes in the 20th century - their attempt to concentrate all the information in one place - it will become their greatest advantage. RAZ: So are you saying that the threat to liberal democracy increases as the ability of machines to process more and more amounts of data improves? HARARI: Yes, in many ways. So in the 20th century, the supporters of liberal democracy had a kind of relatively easy time because you did not have to choose between ethics and efficiency. The most ethical thing to do was also the most efficient thing to do. To give power to the people, to give freedom to individuals - all these things were good, both ethically and economically. And most governments around the world that liberalized their societies in the last few decades - they thought, if we want a thriving economy like the U. S. economy or like the German economy, we need to liberalize our societies. So even if we don't like very much to do it, we have to do it. But what happens if, suddenly, this is no longer the case? It's still the best thing to do from an ethical perspective - to protect the privacy and the rights of individuals. But it's no longer the most efficient thing to do. The most efficient thing to do is perhaps to build these giant databases, which ignore completely the privacy and the rights of individuals. And it's the most efficient thing to do to allow algorithms to make decisions on behalf of human beings. The algorithms will decide who we will accept to these universities. The algorithms will tell you what to study and where to live and even whom to marry. And if this is more efficient, what happens to the ideals of freedom and human rights and individualism? This becomes a much more problematic issue than in the 20th century. (SOUNDBITE OF TED TALK) HARARI: Another technological danger that threatens the future of democracy is the merger of information technology with biotechnology, which might result in the creation of algorithms that know me better than I know myself. And once you have such algorithms, an external system, like the government, cannot just predict my decisions, it can also manipulate my feelings, my emotions. A dictator may not be able to provide me with good health care, but he will be able to make me love him and to make me hate the opposition. The enemies of liberal democracy, they have a method. They hack our feelings, not our emails, not our bank accounts - they hack our feelings of fear and hate and vanity, and then use these feelings to polarize and destroy democracy from within because, in the end, democracy is not based on the human rationality. It's based on human feelings. During elections and referendums, you're not being asked, what do you think? You're actually being asked, how do you feel? And if somebody can manipulate your emotions effectively, democracy will become an emotional puppet show. RAZ: So your conclusion is, he who controls the data, controls the people. HARARI: Yes, and if you start with the understanding that, at least according to science, our feelings do not represent some mystical free will. They represent biochemical processes in our bodies and, of course, influences from the environment. Now, what we also need to remember is that it should be technically possible to decipher, to hack human beings and human feelings. In order to hack a human being, you need a lot of biological knowledge and you need a lot of computing power. And until today, nobody could do it. RAZ: Yeah. HARARI: And therefore, people could believe that humans are unhackable - that human feelings reflect free will, and nobody can ever understand me and manipulate me. And this was true for the whole of history, but this is no longer true. Once you have a system that can decipher the human operating system, it can predict human decisions, and it can manipulate human desires and human feelings. I mean, until today, no politician really had the ability to understand the human emotional system. By trial and error, they see what works, and it changes all the time. But if we reach a point when we can reliably decipher the human biochemical system and basically sell you anything, whether it's a product or a politician, then we have a completely new kind of politics. RAZ: You know, I know that you probably come across, you know, people who have helped to create this technology - people who had this kind of utopian idea of how data and data processing could change the world in positive ways, you know? But those same people, you have to wonder whether they stop to think about the unintended consequences. HARARI: When you develop this kind of the technology, in most cases, you obviously focus on the positive implications. And until today, humankind has managed to avoid the worst consequences. The most obvious example is nuclear technology. All the doomsday prophecies from the 1950s and 1960s about a nuclear war which will destroy human civilization, it didn't happen. Humankind successfully rose to the challenge of nuclear technology. Whether we can do it again with AI and with biotechnology is an open question. RAZ: Yuval, you will know this well as an Israeli - somebody who lives in the biblical lands. Prophets are rarely rewarded. In fact, they're usually disliked, even when they're right. And oftentimes, when they're right, it doesn't matter because their warnings are so dark, and we ignore them at our peril. HARARI: I definitely don't see myself as a prophet. And I don't think that anybody can prophesize the future. Actually, it's pointless. Again, I define myself as a historian. And what I try to do is map different possibilities. There are always more than one way in which we can go from here. And the reason I think it's important to have this discussion is because it's not too late. (SOUNDBITE OF MUSIC) HARARI: I see my job in changing the discussion in the present. We can still influence the direction in which this technology is going. There are always different possibilities. RAZ: Yuval Noah Harari teaches history at the Hebrew University of Jerusalem. His latest book is called \"21 Lessons For The 21st Century. \" You can see his entire talk at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-02-662627058": {"title": "Yasmin Green: How Did The Internet Become A Platform For Hate Groups? : NPR", "url": "https://www.npr.org/2018/11/02/662627058/yasmin-green-how-did-the-internet-become-a-platform-for-hate-groups", "author": "No author found", "published_date": "2018-11-02", "content": "GUY RAZ, HOST: On the show today, ideas about the scope and scale of human innovation and some of its Unintended Consequences. What do you remember about the talk around the Internet in 2006? YASMIN GREEN: I look back then, and I recall a time when we all used to float around, just thinking that we are doing work that's essentially altruistic. We were just like, we're connecting people to information, to each other. This is going to transform democracies, and it's going to empower populations. And we really didn't think about all of the platforms and apps being developed as tools for everyone who's malicious to just be even more effective. RAZ: This is Yasmin Green. GREEN: I'm the director of research and development at Jigsaw. RAZ: And Jigsaw is a technology company created by Google. GREEN: So we look at problems like repressive censorship, cyberattacks, online radicalization. And we try to build technology that can help protect people from these threats. RAZ: Yasmin started out at Jigsaw in 2006. This was a year after YouTube was born and the same year that Twitter was created. But that utopian vision of the Internet that she was just describing - well, it didn't anticipate a platform for trolls and hate groups and extremists to find each other and build a like-minded community. And one prime example - the extremist group ISIS. GREEN: ISIS has been kind of given the accolade of being, you know, the first terrorist group to really understand the Internet. There was no technological genius in their use of the Internet. There's nothing that ISIS did that was impressive from an innovation perspective. They used the tools that are open to all of us to use for connecting, for sharing, for activism. They used those tools almost in the way that the rest of us use them, but they used them with destructive ends in mind. RAZ: And so back in 2015, around the time ISIS was recruiting heavily and gaining momentum, Yasmin and her team at Jigsaw wanted to find out how ISIS was so effective. Here's Yasmin Green on the TED stage. (SOUNDBITE OF TED TALK)GREEN: So in order to understand the radicalization process, we met with dozens of former members of violent extremist groups. One was a British schoolgirl who'd been taken off of a plane at London Heathrow as she was trying to make her way to Syria to join ISIS. And she was 13 years old. So I sat down with her and her father, and I said, why? And she said, I was looking at pictures of what life is like in Syria, and I thought I was going to go and live in the Islamic Disney World. That's what she saw in ISIS. She thought she'd meet and marry a jihadi Brad Pitt and go shopping in the mall all day and live happily ever after. ISIS understands what drives people, and they carefully crafted a message for each audience. Just look at how many languages they translate their marketing material into. They make pamphlets, radio shows and videos, in not just English and Arabic, but German, Russian, French, Turkish, Kurdish, Hebrew, Mandarin Chinese. I've even seen an ISIS-produced video in sign language. It's actually not tech-savviness that is the reason why ISIS wins hearts and minds. It's their insight into the prejudices, the vulnerabilities, the desires, of the people who are trying to reach that does that. That's why it's not enough for the online platforms to focus on removing recruiting material. If we want to have a shot at building meaningful technology that's going to counter radicalization, we have to start with the human journey at its core. (SOUNDBITE OF MUSIC)RAZ: Yeah, I mean, given the Internet's virtues, you know, to connect people, would it ever have been possible to prevent bad actors from also taking advantage of those tools? GREEN: There's a lot of bad stuff that does get stopped. And it's easy and dangerous to say, well, there are good people and bad people because it ends up - the prescription is really punitive technologies or policies, which is, let's suspend people or let's censor people or let's punish people. And in most of the cases, in my conversations with former, you know, either ISIS recruits or supporters or extremists, is that they were people with almost legitimate questions. And they went down a bad path, but more information, better information, earlier in the process could have steered them into a different direction. (SOUNDBITE OF TED TALK)GREEN: Radicalization isn't this yes-or-no choice. It's a process, during which people have questions - about ideology, religion, living conditions - and they're coming online for answers, which is an opportunity to reach them. So in 2016, we partnered with Moonshot CVE to pilot a new approach to countering radicalization called the Redirect Method. It uses the power of online advertising to bridge the gap between those susceptible to ISIS's messaging and those credible voices that are debunking that messaging. And it works like this - someone looking for extremist materials - say they search for, how do I join ISIS? - will see an ad appear that invites them to watch a YouTube video of a cleric, of a defector - someone who has an authentic answer. And because violent extremism isn't confined to any one language, religion or ideology, the Redirect Method is now being deployed globally to protect people being courted online by violent ideologues, whether they're Islamists, white supremacists or other violent extremists, with the goal of giving them the chance to hear from someone on the other side of that journey. (SOUNDBITE OF MUSIC)RAZ: I mean, it sounds like you're trying to kind of break this down with the hope, I guess, of at some point figuring out how to solve this. But this is a long-term project. This is not going to happen overnight, right? GREEN: When our group was started seven years ago, I remember feeling like, wow, this is a real gamble. And now, you know, I think we have to have more people within technology companies that think about the world through this lens. Like, it's not enough just to focus on your platform and the, you know, the micro-instances that you see. Like, you have to think about terrorist groups and their goal and their strategies and what they're doing across the whole Internet. And you have to have a big-picture view. We can't be so tunnel-visioned anymore. The more that we do that, the better we'll be at spotting problems early. RAZ: Yeah. GREEN: When you go around the world and you see how groups who have power are actually using technology to reinforce their power, you realize that the kind of utopian vision of what the Internet was going to be was not inevitable. We'd have to be proactive and step in if we wanted to have a chance of realizing that. (SOUNDBITE OF MUSIC)RAZ: That's Yasmin Green. She's the director of research and development at Jigsaw. You can see her full talk at ted. com. On the show today, ideas about Unintended Consequences. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC) GUY RAZ, HOST:  On the show today, ideas about the scope and scale of human innovation and some of its Unintended Consequences. What do you remember about the talk around the Internet in 2006? YASMIN GREEN: I look back then, and I recall a time when we all used to float around, just thinking that we are doing work that's essentially altruistic. We were just like, we're connecting people to information, to each other. This is going to transform democracies, and it's going to empower populations. And we really didn't think about all of the platforms and apps being developed as tools for everyone who's malicious to just be even more effective. RAZ: This is Yasmin Green. GREEN: I'm the director of research and development at Jigsaw. RAZ: And Jigsaw is a technology company created by Google. GREEN: So we look at problems like repressive censorship, cyberattacks, online radicalization. And we try to build technology that can help protect people from these threats. RAZ: Yasmin started out at Jigsaw in 2006. This was a year after YouTube was born and the same year that Twitter was created. But that utopian vision of the Internet that she was just describing - well, it didn't anticipate a platform for trolls and hate groups and extremists to find each other and build a like-minded community. And one prime example - the extremist group ISIS. GREEN: ISIS has been kind of given the accolade of being, you know, the first terrorist group to really understand the Internet. There was no technological genius in their use of the Internet. There's nothing that ISIS did that was impressive from an innovation perspective. They used the tools that are open to all of us to use for connecting, for sharing, for activism. They used those tools almost in the way that the rest of us use them, but they used them with destructive ends in mind. RAZ: And so back in 2015, around the time ISIS was recruiting heavily and gaining momentum, Yasmin and her team at Jigsaw wanted to find out how ISIS was so effective. Here's Yasmin Green on the TED stage. (SOUNDBITE OF TED TALK) GREEN: So in order to understand the radicalization process, we met with dozens of former members of violent extremist groups. One was a British schoolgirl who'd been taken off of a plane at London Heathrow as she was trying to make her way to Syria to join ISIS. And she was 13 years old. So I sat down with her and her father, and I said, why? And she said, I was looking at pictures of what life is like in Syria, and I thought I was going to go and live in the Islamic Disney World. That's what she saw in ISIS. She thought she'd meet and marry a jihadi Brad Pitt and go shopping in the mall all day and live happily ever after. ISIS understands what drives people, and they carefully crafted a message for each audience. Just look at how many languages they translate their marketing material into. They make pamphlets, radio shows and videos, in not just English and Arabic, but German, Russian, French, Turkish, Kurdish, Hebrew, Mandarin Chinese. I've even seen an ISIS-produced video in sign language. It's actually not tech-savviness that is the reason why ISIS wins hearts and minds. It's their insight into the prejudices, the vulnerabilities, the desires, of the people who are trying to reach that does that. That's why it's not enough for the online platforms to focus on removing recruiting material. If we want to have a shot at building meaningful technology that's going to counter radicalization, we have to start with the human journey at its core. (SOUNDBITE OF MUSIC) RAZ: Yeah, I mean, given the Internet's virtues, you know, to connect people, would it ever have been possible to prevent bad actors from also taking advantage of those tools? GREEN: There's a lot of bad stuff that does get stopped. And it's easy and dangerous to say, well, there are good people and bad people because it ends up - the prescription is really punitive technologies or policies, which is, let's suspend people or let's censor people or let's punish people. And in most of the cases, in my conversations with former, you know, either ISIS recruits or supporters or extremists, is that they were people with almost legitimate questions. And they went down a bad path, but more information, better information, earlier in the process could have steered them into a different direction. (SOUNDBITE OF TED TALK) GREEN: Radicalization isn't this yes-or-no choice. It's a process, during which people have questions - about ideology, religion, living conditions - and they're coming online for answers, which is an opportunity to reach them. So in 2016, we partnered with Moonshot CVE to pilot a new approach to countering radicalization called the Redirect Method. It uses the power of online advertising to bridge the gap between those susceptible to ISIS's messaging and those credible voices that are debunking that messaging. And it works like this - someone looking for extremist materials - say they search for, how do I join ISIS? - will see an ad appear that invites them to watch a YouTube video of a cleric, of a defector - someone who has an authentic answer. And because violent extremism isn't confined to any one language, religion or ideology, the Redirect Method is now being deployed globally to protect people being courted online by violent ideologues, whether they're Islamists, white supremacists or other violent extremists, with the goal of giving them the chance to hear from someone on the other side of that journey. (SOUNDBITE OF MUSIC) RAZ: I mean, it sounds like you're trying to kind of break this down with the hope, I guess, of at some point figuring out how to solve this. But this is a long-term project. This is not going to happen overnight, right? GREEN: When our group was started seven years ago, I remember feeling like, wow, this is a real gamble. And now, you know, I think we have to have more people within technology companies that think about the world through this lens. Like, it's not enough just to focus on your platform and the, you know, the micro-instances that you see. Like, you have to think about terrorist groups and their goal and their strategies and what they're doing across the whole Internet. And you have to have a big-picture view. We can't be so tunnel-visioned anymore. The more that we do that, the better we'll be at spotting problems early. RAZ: Yeah. GREEN: When you go around the world and you see how groups who have power are actually using technology to reinforce their power, you realize that the kind of utopian vision of what the Internet was going to be was not inevitable. We'd have to be proactive and step in if we wanted to have a chance of realizing that. (SOUNDBITE OF MUSIC) RAZ: That's Yasmin Green. She's the director of research and development at Jigsaw. You can see her full talk at ted. com. On the show today, ideas about Unintended Consequences. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC)", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-02-662634919": {"title": "Edward Tenner: Can We View Technology's Unintended Consequences In A Positive Light? : NPR", "url": "https://www.npr.org/2018/11/02/662634919/edward-tenner-can-we-view-technologys-unintended-consequences-in-a-positive-ligh", "author": "No author found", "published_date": "2018-11-02", "content": "GUY RAZ, HOST: On the show today, ideas about the dark side of innovation - the fallout, the downstream effects and the Unintended Consequences of all of this technology we're creating today. EDWARD TENNER: I'm not sure that's exactly the way I would put it. RAZ: And then there are some of us who aren't freaking out, who actually love unintended consequences. TENNER: What I love about them is the way that life is so unpredictable. And you really wouldn't have positive surprises unless there were also negative ones. RAZ: This is Edward Tenner. He's a historian of technology. TENNER: To me, the philosophy of unintended consequences really means keeping open. It means constantly observing. The people who see endless despair and suffering on one side from technology and the people who see a wonderful new world are really both ideological, and I don't think that either is wrong. I think it's important to have perspective on decisions and on history that will let us look at change with more equanimity. RAZ: And as Edward points out, if you look at innovation throughout history, it's always better to take the long view. Here's more from Edward Tenner on the TED stage. (SOUNDBITE OF TED TALK)TENNER: Let's go to 10,000 years before the present to the time of the domestication of grains. What would our ancestors 10,000 years ago have said if they really had technology assessment? And I can just imagine the committees reporting back to them on where agriculture was going to take humanity, at least in the next few hundred years. It was really bad news. First of all, worse nutrition, shorter life spans. It was simply awful for women. The skeletal remains from that period have shown that they were grinding grain morning, noon and night. And politically, it was the beginning of a much higher degree of inequality. If there had been rational technology assessment then, I think they very well might have said, let's call the whole thing off. RAZ: Of course, this was going to be better in the long term for humans. But there were going to be some bad things that were going to happen as well. TENNER: That's right. And my point there was we have to recognize the limits of technological assessment. We have to take a longer view, which means that sometimes, as I like to say, things can go right only after they've gone wrong. And if we try to prevent any new development with potentially bad consequences, we may be freezing in the bad consequences that we already have. RAZ: Which is interesting because it reminds me of something you bring up in your TED talk, which is - which I didn't know, which is the example of the Titanic, which is after it sank, there were all these laws that were passed that required ships to carry more lifeboats. TENNER: Yes. Well, I think when we think about the Titanic, we have to disregard, for the moment, the films we've seen about the Titanic and put ourselves in the position of captains of ships on the North Atlantic at that time. Sea ice was known as a problem, but it was not known as a problem that caused massive loss of life. It could damage a ship, but there was always time for the rescue of the passengers and crew. So the Titanic was really a case where everything worked out wrong, and the rescue didn't come. (SOUNDBITE OF TED TALK)TENNER: The lesson of the Titanic, for a lot of the contemporaries, was that you must have enough lifeboats for everyone on the ship. And this was the result of the tragic loss of lives of people who could not get into them. However, there was another case, the Eastland, a ship that capsized in Chicago Harbor in 1915, and it killed 841 people. That was 14 more than the passenger toll of the Titanic. The reason for it, in part, was the extra lifeboats that were added that made this already unstable ship even more unstable. And that, again, proves that when you're talking about unintended consequences, it's not that easy to know the right lessons to draw. It's really a question of the system, how the ship was loaded, the ballast and many other things. (SOUNDBITE OF MUSIC)RAZ: I mean, it is an amazing example of how we are kind of wired to react, right? Like, something bad happens, and then to solve it or to prevent it, the reaction or the solution that we put forward has worse consequences than doing nothing. TENNER: Yes. Well, I think we can say that, very often, the means that you put in place after some kind of disaster will, in the long run, lead to the next disaster. For example, new bridge designs have a life span of about 30 years. There is some disaster that leads to a new type of bridge. And then engineers get more and more confident in the design. They get bolder and bolder. And then there is some kind of new catastrophe that leads to reconsidering that technology, and the cycle starts all over again. RAZ: I mean, it sounds like what you're saying is that, look; there's no point in worrying about this stuff or bothering with this stuff because, you know, the course of history is the course of history - that we can't necessarily shape it. And I wonder whether that's true. TENNER: I'm not saying that we should do nothing, that we shouldn't take any action. But we should also realize that, really, two things happen. That, first of all, the positive outcomes that we expect are usually not nearly as positive as we imagine them. But also, the negative things don't turn out in the same way. For example, we tend to think that what is going on is just going to go on and on and get worse and worse, or it's going to go on and on and get better and better. And reality usually has surprises for us. RAZ: But, Edward, take something, you know, as scary as climate change, right? I mean, isn't there some value in anticipating the worst-case scenarios and then trying to prevent them? TENNER: I think it's very important that the fear of worst-case scenarios is leading to all kinds of proposals for geoengineering, for 100 percent renewable power. I'm all for this, and I think it's very good that our fear of apocalypse is motivating that. So I don't dispute that at all. But I don't think it's really terribly helpful, unless you're actually working on something concretely to deal with a problem, to worry too much about the problem if there isn't something that you can do about it. RAZ: I don't know. I mean, I think you're right, and I feel very reassured by this. But, you know, in the middle of the night when I wake up in a cold sweat, I'm thinking we're, like, at the very edge of destroying ourselves. Like, this can be the end of our species. TENNER: Yes, it could. Or probably, more likely, it would mean a worldwide degradation of the living conditions of humanity. But remember, there was always a positive side of these epidemics. So for example, after the Black Death in the 14th century, if you survived, it was a very good time to be a peasant. You had lower rents. There were more opportunities for people to become artisans and masters of their own workshops. So there was really a lot of opportunity if you didn't get killed by the epidemic. RAZ: Great if you made it through, right? TENNER: That's it. So maybe that's the one bright spot. So you know, hope that you - hope that you'll be one of the survivors. (SOUNDBITE OF MUSIC)RAZ: That's Edward Tenner. His latest book is called \"Our Own Devices: The Past And Future Of Body Technology. \" You can watch his entire talk at ted. com. (SOUNDBITE OF SONG, \"APOCALYPSE DREAMS\")TAME IMPALA: (Singing) This could be the day that we push through. It could be the day that all our dreams come true for me and you. RAZ: Hey, thanks for listening to our episode on Unintended Consequences this week. If you want to find out more about who was on it, go to ted. npr. org. To see hundreds more TED Talks, check out ted. com or the TED app. Our production staff at NPR includes Jeff Rogers, Sanaz Meshkinpour, Jinae West, Neva Grant, Casey Herman, Rachel Faulkner and Diba Mohtasham, with help from Daniel Shukin (ph) and Megan Schellong. Our intern is Daryth Gayles. Our partners at TED are Chris Anderson, Colin Helms, Anna Phelan and Janet Lee. I'm Guy Raz, and you've been listening to ideas worth spreading, right here on the TED Radio Hour from NPR. GUY RAZ, HOST:  On the show today, ideas about the dark side of innovation - the fallout, the downstream effects and the Unintended Consequences of all of this technology we're creating today. EDWARD TENNER: I'm not sure that's exactly the way I would put it. RAZ: And then there are some of us who aren't freaking out, who actually love unintended consequences. TENNER: What I love about them is the way that life is so unpredictable. And you really wouldn't have positive surprises unless there were also negative ones. RAZ: This is Edward Tenner. He's a historian of technology. TENNER: To me, the philosophy of unintended consequences really means keeping open. It means constantly observing. The people who see endless despair and suffering on one side from technology and the people who see a wonderful new world are really both ideological, and I don't think that either is wrong. I think it's important to have perspective on decisions and on history that will let us look at change with more equanimity. RAZ: And as Edward points out, if you look at innovation throughout history, it's always better to take the long view. Here's more from Edward Tenner on the TED stage. (SOUNDBITE OF TED TALK) TENNER: Let's go to 10,000 years before the present to the time of the domestication of grains. What would our ancestors 10,000 years ago have said if they really had technology assessment? And I can just imagine the committees reporting back to them on where agriculture was going to take humanity, at least in the next few hundred years. It was really bad news. First of all, worse nutrition, shorter life spans. It was simply awful for women. The skeletal remains from that period have shown that they were grinding grain morning, noon and night. And politically, it was the beginning of a much higher degree of inequality. If there had been rational technology assessment then, I think they very well might have said, let's call the whole thing off. RAZ: Of course, this was going to be better in the long term for humans. But there were going to be some bad things that were going to happen as well. TENNER: That's right. And my point there was we have to recognize the limits of technological assessment. We have to take a longer view, which means that sometimes, as I like to say, things can go right only after they've gone wrong. And if we try to prevent any new development with potentially bad consequences, we may be freezing in the bad consequences that we already have. RAZ: Which is interesting because it reminds me of something you bring up in your TED talk, which is - which I didn't know, which is the example of the Titanic, which is after it sank, there were all these laws that were passed that required ships to carry more lifeboats. TENNER: Yes. Well, I think when we think about the Titanic, we have to disregard, for the moment, the films we've seen about the Titanic and put ourselves in the position of captains of ships on the North Atlantic at that time. Sea ice was known as a problem, but it was not known as a problem that caused massive loss of life. It could damage a ship, but there was always time for the rescue of the passengers and crew. So the Titanic was really a case where everything worked out wrong, and the rescue didn't come. (SOUNDBITE OF TED TALK) TENNER: The lesson of the Titanic, for a lot of the contemporaries, was that you must have enough lifeboats for everyone on the ship. And this was the result of the tragic loss of lives of people who could not get into them. However, there was another case, the Eastland, a ship that capsized in Chicago Harbor in 1915, and it killed 841 people. That was 14 more than the passenger toll of the Titanic. The reason for it, in part, was the extra lifeboats that were added that made this already unstable ship even more unstable. And that, again, proves that when you're talking about unintended consequences, it's not that easy to know the right lessons to draw. It's really a question of the system, how the ship was loaded, the ballast and many other things. (SOUNDBITE OF MUSIC) RAZ: I mean, it is an amazing example of how we are kind of wired to react, right? Like, something bad happens, and then to solve it or to prevent it, the reaction or the solution that we put forward has worse consequences than doing nothing. TENNER: Yes. Well, I think we can say that, very often, the means that you put in place after some kind of disaster will, in the long run, lead to the next disaster. For example, new bridge designs have a life span of about 30 years. There is some disaster that leads to a new type of bridge. And then engineers get more and more confident in the design. They get bolder and bolder. And then there is some kind of new catastrophe that leads to reconsidering that technology, and the cycle starts all over again. RAZ: I mean, it sounds like what you're saying is that, look; there's no point in worrying about this stuff or bothering with this stuff because, you know, the course of history is the course of history - that we can't necessarily shape it. And I wonder whether that's true. TENNER: I'm not saying that we should do nothing, that we shouldn't take any action. But we should also realize that, really, two things happen. That, first of all, the positive outcomes that we expect are usually not nearly as positive as we imagine them. But also, the negative things don't turn out in the same way. For example, we tend to think that what is going on is just going to go on and on and get worse and worse, or it's going to go on and on and get better and better. And reality usually has surprises for us. RAZ: But, Edward, take something, you know, as scary as climate change, right? I mean, isn't there some value in anticipating the worst-case scenarios and then trying to prevent them? TENNER: I think it's very important that the fear of worst-case scenarios is leading to all kinds of proposals for geoengineering, for 100 percent renewable power. I'm all for this, and I think it's very good that our fear of apocalypse is motivating that. So I don't dispute that at all. But I don't think it's really terribly helpful, unless you're actually working on something concretely to deal with a problem, to worry too much about the problem if there isn't something that you can do about it. RAZ: I don't know. I mean, I think you're right, and I feel very reassured by this. But, you know, in the middle of the night when I wake up in a cold sweat, I'm thinking we're, like, at the very edge of destroying ourselves. Like, this can be the end of our species. TENNER: Yes, it could. Or probably, more likely, it would mean a worldwide degradation of the living conditions of humanity. But remember, there was always a positive side of these epidemics. So for example, after the Black Death in the 14th century, if you survived, it was a very good time to be a peasant. You had lower rents. There were more opportunities for people to become artisans and masters of their own workshops. So there was really a lot of opportunity if you didn't get killed by the epidemic. RAZ: Great if you made it through, right? TENNER: That's it. So maybe that's the one bright spot. So you know, hope that you - hope that you'll be one of the survivors. (SOUNDBITE OF MUSIC) RAZ: That's Edward Tenner. His latest book is called \"Our Own Devices: The Past And Future Of Body Technology. \" You can watch his entire talk at ted. com. (SOUNDBITE OF SONG, \"APOCALYPSE DREAMS\") TAME IMPALA: (Singing) This could be the day that we push through. It could be the day that all our dreams come true for me and you. RAZ: Hey, thanks for listening to our episode on Unintended Consequences this week. If you want to find out more about who was on it, go to ted. npr. org. To see hundreds more TED Talks, check out ted. com or the TED app. Our production staff at NPR includes Jeff Rogers, Sanaz Meshkinpour, Jinae West, Neva Grant, Casey Herman, Rachel Faulkner and Diba Mohtasham, with help from Daniel Shukin (ph) and Megan Schellong. Our intern is Daryth Gayles. Our partners at TED are Chris Anderson, Colin Helms, Anna Phelan and Janet Lee. I'm Guy Raz, and you've been listening to ideas worth spreading, right here on the TED Radio Hour from NPR.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-02-663372770": {"title": "Mario Segale, Inspiration For Nintendo's Hero Plumber, Has Died : NPR", "url": "https://www.npr.org/2018/11/02/663372770/mario-segale-inspiration-for-nintendos-hero-plumber-has-died", "author": "No author found", "published_date": "2018-11-02", "content": "", "section": "Technology", "disclaimer": ""}, "2018-11-03-661765301": {"title": "Voters May Tax Tech Companies To Fight Homelessness : NPR", "url": "https://www.npr.org/2018/11/03/661765301/voters-may-tax-tech-companies-to-fight-homelessness", "author": "No author found", "published_date": "2018-11-03", "content": "SCOTT SIMON, HOST: Voters in San Francisco are considering raising taxes on the city's biggest companies, like Twitter and Uber, to pay for services for homeless people. From member station KQED in San Francisco, Guy Marzorati has more. GUY MARZORATI, BYLINE: As it tried to climb out of the Great Recession, San Francisco changed its tax policies to help tech companies grow in the city. But like in LA and Seattle, the boom led to higher housing costs, which contributed to the worsening homeless crisis. So now San Francisco is deciding whether to change its tax policy again - this time, to raise taxes on certain corporations. MARC BENIOFF: This is an extraordinary moment where we're in a crisis. MARZORATI: That's Salesforce CEO Marc Benioff. His cloud-computing company is the city's largest employer. And he is, surprisingly, the biggest advocate for the tax hike. Proposition C would use the new tax money to pay for things like rental assistance and shelters. And not only is Benioff donating millions to the Yes on Prop C campaign, he's calling out other local CEOs who have opposed Prop C. BENIOFF: What I've found is there's two kinds of people in San Francisco. There's people who are willing to give. And there's those who won't give at all, no matter what. MARZORATI: Benioff specifically called out Twitter CEO Jack Dorsey, who says the tax would unfairly punish some smaller tech companies. In years past, the city specifically focused on helping these smaller companies grow and eventually go public while staying in the city. (SOUNDBITE OF CABLE CAR WHIRRING)MARZORATI: One of San Francisco's biggest tax breaks got companies like Twitter and Uber to move here to the city's mid-market neighborhood. On a weekday morning, you can see workers head to the office to start the day and homeless residents sitting on the ground nearby. Jennifer Friedenbach, head of the Coalition On Homelessness in San Francisco, says the neighborhood hasn't changed for the better. JENNIFER FRIEDENBACH: I think the most dramatic change has been displacement of people who were living here. And so we've had mass evictions of folks in this area. We've had real increases in rent. MARZORATI: Friedenbach says the massive federal tax cut for corporations has made it easier to ask big companies to pay more to fix local problems. FRIEDENBACH: Now is the moment. And we can't predict that this same moment will happen in the future. MARZORATI: But not everyone sees the proposed tax hike, or even this neighborhood, the same. JIM LAZARUS: We're standing here with hundreds of people walking to and from places of work. The street looks good today. MARZORATI: Jim Lazarus is with San Francisco's Chamber of Commerce. He says businesses already pay an outsized amount of taxes and extra costs to stay in the region. LAZARUS: At some point, the straw breaks the camel's back, and some chief financial officer says, when the lease comes up, we're leaving town. MARZORATI: That kind of threat actually happened in Seattle earlier this year. The city approved a new business tax. Amazon threatened to mount a repeal campaign, and the city council backed down and undid the tax. Molly Turner, who teaches business at UC Berkeley, says the economic boom in the Bay Area has motivated advocates and local governments to push ahead with tax increases. MOLLY TURNER: I think there's certainly an inspiration to take advantage of the enormous prosperity we have in the region while we have it. MARZORATI: If there's any risk, Turner says, it's that a future economic bust could make the money raised by the tax evaporate. For NPR News, I'm Guy Marzorati in San Francisco. (SOUNDBITE OF RYAN DUGRE'S \"MUTE SWANS\") SCOTT SIMON, HOST:  Voters in San Francisco are considering raising taxes on the city's biggest companies, like Twitter and Uber, to pay for services for homeless people. From member station KQED in San Francisco, Guy Marzorati has more. GUY MARZORATI, BYLINE: As it tried to climb out of the Great Recession, San Francisco changed its tax policies to help tech companies grow in the city. But like in LA and Seattle, the boom led to higher housing costs, which contributed to the worsening homeless crisis. So now San Francisco is deciding whether to change its tax policy again - this time, to raise taxes on certain corporations. MARC BENIOFF: This is an extraordinary moment where we're in a crisis. MARZORATI: That's Salesforce CEO Marc Benioff. His cloud-computing company is the city's largest employer. And he is, surprisingly, the biggest advocate for the tax hike. Proposition C would use the new tax money to pay for things like rental assistance and shelters. And not only is Benioff donating millions to the Yes on Prop C campaign, he's calling out other local CEOs who have opposed Prop C. BENIOFF: What I've found is there's two kinds of people in San Francisco. There's people who are willing to give. And there's those who won't give at all, no matter what. MARZORATI: Benioff specifically called out Twitter CEO Jack Dorsey, who says the tax would unfairly punish some smaller tech companies. In years past, the city specifically focused on helping these smaller companies grow and eventually go public while staying in the city. (SOUNDBITE OF CABLE CAR WHIRRING) MARZORATI: One of San Francisco's biggest tax breaks got companies like Twitter and Uber to move here to the city's mid-market neighborhood. On a weekday morning, you can see workers head to the office to start the day and homeless residents sitting on the ground nearby. Jennifer Friedenbach, head of the Coalition On Homelessness in San Francisco, says the neighborhood hasn't changed for the better. JENNIFER FRIEDENBACH: I think the most dramatic change has been displacement of people who were living here. And so we've had mass evictions of folks in this area. We've had real increases in rent. MARZORATI: Friedenbach says the massive federal tax cut for corporations has made it easier to ask big companies to pay more to fix local problems. FRIEDENBACH: Now is the moment. And we can't predict that this same moment will happen in the future. MARZORATI: But not everyone sees the proposed tax hike, or even this neighborhood, the same. JIM LAZARUS: We're standing here with hundreds of people walking to and from places of work. The street looks good today. MARZORATI: Jim Lazarus is with San Francisco's Chamber of Commerce. He says businesses already pay an outsized amount of taxes and extra costs to stay in the region. LAZARUS: At some point, the straw breaks the camel's back, and some chief financial officer says, when the lease comes up, we're leaving town. MARZORATI: That kind of threat actually happened in Seattle earlier this year. The city approved a new business tax. Amazon threatened to mount a repeal campaign, and the city council backed down and undid the tax. Molly Turner, who teaches business at UC Berkeley, says the economic boom in the Bay Area has motivated advocates and local governments to push ahead with tax increases. MOLLY TURNER: I think there's certainly an inspiration to take advantage of the enormous prosperity we have in the region while we have it. MARZORATI: If there's any risk, Turner says, it's that a future economic bust could make the money raised by the tax evaporate. For NPR News, I'm Guy Marzorati in San Francisco. (SOUNDBITE OF RYAN DUGRE'S \"MUTE SWANS\")", "section": "National", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-05-660642531": {"title": "Right-Wing Hate Groups Are Recruiting Video Gamers : NPR", "url": "https://www.npr.org/2018/11/05/660642531/right-wing-hate-groups-are-recruiting-video-gamers", "author": "No author found", "published_date": "2018-11-05", "content": "AILSA CHANG, HOST:  All right, recent acts of violence - the shooting at a Pittsburgh synagogue, the explosives sent to critics of President Trump - happened in the real world, but the warnings took place online on social media. On this month's All Tech Considered, we look at toxic content - how it spreads and what should be done about it. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CHANG: The Pew Research Center says 97 percent of boys and the vast majority of girls play video games. Increasingly, they're playing online with strangers. Although it's rare, experts say these games can become an avenue for recruitment by right-wing extremist groups. NPR's Anya Kamenetz has our story. ANYA KAMENETZ, BYLINE: John's a father of two teenagers, and he said he never worried about his son's video game habit. JOHN: I knew nothing about any of this until one night I'd walked into my home office. KAMENETZ: We're not using John's last name to protect his family's privacy. JOHN: I was shaking when I saw it. I'm like, what in the world is this? And why is this in our house? KAMENETZ: He saw a pile of papers face-down next to his printer. He turned them over and found a copy of a notorious neo-Nazi propaganda book. JOHN: It's white culture is in trouble, and we are under attack by Jews and blacks and every other minority. It was scary. It was absolutely frightening to even see that in my house. KAMENETZ: John confronted his son angrily. JOHN: And then I went back to my room. And I honestly was crying. I felt like a failure that a child that I had raised would even be remotely interested in this stuff. KAMENETZ: Like millions of other kids, John's son loved first-person shooter games. It's increasingly popular to play these games online and form teams with friends or strangers. JOHN: There wasn't anything obvious to me at first because it's common. Online gaming and chatting with friends online - that's the norm now instead of going out and hanging out at the drive-in or whatever. KAMENETZ: And it's this way, John says, that his son started hanging out with avowed white supremacists. They keyed into his interests in military history and Nordic mythology. JOHN: He was now in the in-crowd with these guys. KAMENETZ: John learned his son had been drawn into at least one group that the Southern Poverty Law Center calls a Nazi terrorist organization. He searched online for help and found it. CHRISTIAN PICCIOLINI: Christian Picciolini, founder of the Free Radicals Project, author of \"White American Youth. \"KAMENETZ: Picciolini is a heavily tattooed former skinhead. He's now dedicated his life to helping people move away from hate. PICCIOLINI: We are assembling a team of people around the world to help people disengage from violent extremism. KAMENETZ: Thirty years ago, he says. . . PICCIOLINI: You had to meet somebody to be recruited. Or you had a pamphlet or a flyer put on your car, and you reached out. KAMENETZ: Today, he says, it's much more common for extremists to reach out online, and that includes over kids' headsets during video games. PICCIOLINI: Typically, they'll start out with dropping slurs about different races or religions and test the waters. Once they sense that they've got their hooks in them, they ramp it up. And then they start sending propaganda. They start sending links to other sites, or they start talking about these old racist, anti-Semitic tropes. JOAN DONOVAN: Gaming culture is one of the spaces of recruitment that must be addressed. KAMENETZ: Joan Donovan is with Data & Society, a research institute. For years, she's been tracking white supremacists from platform to platform online. DONOVAN: They were really trying to figure out what young men were angry about and how they could leverage that to bring about broad-based social movement. KAMENETZ: And first-person shooter games, chat rooms and video platforms, she says, are good places to find angry young men. Video games are a hundred-billion-dollar industry. So what are companies' responsibilities to ensure that teens won't encounter hate groups? Greg Boyd is a lawyer who represents the game industry for the firm Frankfurt Kurnit. He says companies take the problem seriously and remove or ban people when they're flagged by other players, but the scale of the issue is daunting. GREG BOYD: You're talking about Microsoft, PlayStation and Steam. You're talking about 48 million, 70 million and 130 million monthly active players or players that are playing, you know, probably on a weekly basis. I mean, that's the populations of Spain, France and Russia. KAMENETZ: Imagine, he says, moderating all that chat, text and voice moment by moment. BOYD: In literally every language dialect and subdialect spoken in the world. KAMENETZ: While the industry struggles to contain the threat, experts say it's up to parents to keep an ear out and to step in if they notice something that concerns them. John tried. And lately, he says, his son, now 16, seems to have left these ideas behind. He's playing fewer online shooter games. And on his own, he started attending church. Anya Kamenetz, NPR News. AILSA CHANG, HOST:   All right, recent acts of violence - the shooting at a Pittsburgh synagogue, the explosives sent to critics of President Trump - happened in the real world, but the warnings took place online on social media. On this month's All Tech Considered, we look at toxic content - how it spreads and what should be done about it. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CHANG: The Pew Research Center says 97 percent of boys and the vast majority of girls play video games. Increasingly, they're playing online with strangers. Although it's rare, experts say these games can become an avenue for recruitment by right-wing extremist groups. NPR's Anya Kamenetz has our story. ANYA KAMENETZ, BYLINE: John's a father of two teenagers, and he said he never worried about his son's video game habit. JOHN: I knew nothing about any of this until one night I'd walked into my home office. KAMENETZ: We're not using John's last name to protect his family's privacy. JOHN: I was shaking when I saw it. I'm like, what in the world is this? And why is this in our house? KAMENETZ: He saw a pile of papers face-down next to his printer. He turned them over and found a copy of a notorious neo-Nazi propaganda book. JOHN: It's white culture is in trouble, and we are under attack by Jews and blacks and every other minority. It was scary. It was absolutely frightening to even see that in my house. KAMENETZ: John confronted his son angrily. JOHN: And then I went back to my room. And I honestly was crying. I felt like a failure that a child that I had raised would even be remotely interested in this stuff. KAMENETZ: Like millions of other kids, John's son loved first-person shooter games. It's increasingly popular to play these games online and form teams with friends or strangers. JOHN: There wasn't anything obvious to me at first because it's common. Online gaming and chatting with friends online - that's the norm now instead of going out and hanging out at the drive-in or whatever. KAMENETZ: And it's this way, John says, that his son started hanging out with avowed white supremacists. They keyed into his interests in military history and Nordic mythology. JOHN: He was now in the in-crowd with these guys. KAMENETZ: John learned his son had been drawn into at least one group that the Southern Poverty Law Center calls a Nazi terrorist organization. He searched online for help and found it. CHRISTIAN PICCIOLINI: Christian Picciolini, founder of the Free Radicals Project, author of \"White American Youth. \" KAMENETZ: Picciolini is a heavily tattooed former skinhead. He's now dedicated his life to helping people move away from hate. PICCIOLINI: We are assembling a team of people around the world to help people disengage from violent extremism. KAMENETZ: Thirty years ago, he says. . . PICCIOLINI: You had to meet somebody to be recruited. Or you had a pamphlet or a flyer put on your car, and you reached out. KAMENETZ: Today, he says, it's much more common for extremists to reach out online, and that includes over kids' headsets during video games. PICCIOLINI: Typically, they'll start out with dropping slurs about different races or religions and test the waters. Once they sense that they've got their hooks in them, they ramp it up. And then they start sending propaganda. They start sending links to other sites, or they start talking about these old racist, anti-Semitic tropes. JOAN DONOVAN: Gaming culture is one of the spaces of recruitment that must be addressed. KAMENETZ: Joan Donovan is with Data & Society, a research institute. For years, she's been tracking white supremacists from platform to platform online. DONOVAN: They were really trying to figure out what young men were angry about and how they could leverage that to bring about broad-based social movement. KAMENETZ: And first-person shooter games, chat rooms and video platforms, she says, are good places to find angry young men. Video games are a hundred-billion-dollar industry. So what are companies' responsibilities to ensure that teens won't encounter hate groups? Greg Boyd is a lawyer who represents the game industry for the firm Frankfurt Kurnit. He says companies take the problem seriously and remove or ban people when they're flagged by other players, but the scale of the issue is daunting. GREG BOYD: You're talking about Microsoft, PlayStation and Steam. You're talking about 48 million, 70 million and 130 million monthly active players or players that are playing, you know, probably on a weekly basis. I mean, that's the populations of Spain, France and Russia. KAMENETZ: Imagine, he says, moderating all that chat, text and voice moment by moment. BOYD: In literally every language dialect and subdialect spoken in the world. KAMENETZ: While the industry struggles to contain the threat, experts say it's up to parents to keep an ear out and to step in if they notice something that concerns them. John tried. And lately, he says, his son, now 16, seems to have left these ideas behind. He's playing fewer online shooter games. And on his own, he started attending church. Anya Kamenetz, NPR News.", "section": "Social Entrepreneurs: Taking On World Problems", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-05-663704712": {"title": "Reading The Game: Donut County : NPR", "url": "https://www.npr.org/2018/11/05/663704712/reading-the-game-donut-county", "author": "No author found", "published_date": "2018-11-05", "content": "", "section": "Reading The Game", "disclaimer": ""}, "2018-11-06-646301285": {"title": "Amazon HQ2 Plot Twist: Two Cities Will Split The 2nd Headquarters : NPR", "url": "https://www.npr.org/2018/11/06/646301285/amazon-plot-twist-two-cities-will-split-the-2nd-headquarters", "author": "No author found", "published_date": "2018-11-06", "content": "", "section": "Business", "disclaimer": ""}, "2018-11-06-664610468": {"title": "Facebook Blocks More Than 100 Accounts, Citing Possible Foreign Influence  : NPR", "url": "https://www.npr.org/2018/11/06/664610468/facebook-blocks-more-than-100-accounts-citing-possible-foreign-influence", "author": "No author found", "published_date": "2018-11-06", "content": "", "section": "Technology", "disclaimer": ""}, "2018-11-08-665648215": {"title": "Gab Server Subpoenaed By Pennsylvania Attorney General  : NPR", "url": "https://www.npr.org/2018/11/08/665648215/gab-server-subpoenaed-by-pennsylvania-attorney-general", "author": "No author found", "published_date": "2018-11-08", "content": "AILSA CHANG, HOST:  Several new social media sites have popped up as alternatives to Facebook and Twitter. They advertise themselves as censorship-free, but some are also accused of harboring hate groups. One of those sites is called Gab. And it looks a lot like Twitter. It's under scrutiny after one of its users posted an anti-Semitic rant and then allegedly went on a deadly shooting spree at a synagogue in Pittsburgh. NPR's Jasmine Garsd has this report on what keeps these sites in business. JASMINE GARSD, BYLINE: In the hours after the Pittsburgh shooting, one after another, tech companies like GoDaddy and PayPal dropped Gab, forcing it offline. Brian Hughes at American University, who studies the alt-right's presence on the Internet - he says this is why social media sites like Vote (ph) and Gab aren't hugely successful. Most third-party vendors just don't want to be linked to the hate groups that are often on them. BRIAN HUGHES: When there are these big controversies - like, say, a domestic terrorist using your platform - DNS hosts like GoDaddy decide that they don't want to do business with you anymore. And you're forced to find an alternative who's maybe less reliable. GARSD: And there's something else - a successful social media platform requires that a critical mass of people be on it. What's the point of shouting your opinion to a couple of hundred thousand followers who agree with you when you could be trolling the whole world on Twitter? HUGHES: So this really creates a winner take all scenario where there can be only one Twitter. There can be only one Facebook. GARSD: Nevertheless, about a week after the synagogue shooting, Gab was back online thanks to hosting site epik. com. Epik was subpoenaed by the attorney general of Pennsylvania. The CEO of Epik hit back, warning about the dangers of silencing opinions on the Internet. That's the same view held by Matthew Prince, the CEO of a company called CloudFlare. It protects Gab from cyberattacks. Prince works with thousands of mainstream sites. When it comes to the controversial ones, he says there's not much money to be made. MATTHEW PRINCE: Oftentimes, they use just the free version a service. If they do pass, they pass not much at all. GARSD: So why even bother? Prince talks about something that happened last year. After a deadly white supremacist rally in Charlottesville, Va. , he cut ties to a controversial client - the neo-Nazi publication The Daily Stormer. And Prince has said he immediately regretted that. PRINCE: So while I find something like The Daily Stormer absolutely vile and reprehensible and disgusting, it becomes very dangerous for deep infrastructure companies like us to be effectively silencing one side or another. GARSD: Brian Hughes from American University says Prince's dilemma might be short-lived. He thinks very few of these sites which provide a platform for hate groups will be around for too long. HUGHES: Most of them will be gone in a matter of years. GARSD: But that's not necessarily good news. Hughes and other experts worry that in another generation or so hate groups will migrate completely to the dark web, where they can't be found using traditional search engines or browsers. HUGHES: And it wouldn't be subject to the same kind of oversight from journalists and the people that we typically expect to keep an eye on these things for the health of our democracy. GARSD: He says ultimately it boils down to this - would you rather hate speech and groups be out in the open or hidden from view but still among us? Jasmine Garsd, NPR News, New York. (SOUNDBITE OF MUSIC) AILSA CHANG, HOST:   Several new social media sites have popped up as alternatives to Facebook and Twitter. They advertise themselves as censorship-free, but some are also accused of harboring hate groups. One of those sites is called Gab. And it looks a lot like Twitter. It's under scrutiny after one of its users posted an anti-Semitic rant and then allegedly went on a deadly shooting spree at a synagogue in Pittsburgh. NPR's Jasmine Garsd has this report on what keeps these sites in business. JASMINE GARSD, BYLINE: In the hours after the Pittsburgh shooting, one after another, tech companies like GoDaddy and PayPal dropped Gab, forcing it offline. Brian Hughes at American University, who studies the alt-right's presence on the Internet - he says this is why social media sites like Vote (ph) and Gab aren't hugely successful. Most third-party vendors just don't want to be linked to the hate groups that are often on them. BRIAN HUGHES: When there are these big controversies - like, say, a domestic terrorist using your platform - DNS hosts like GoDaddy decide that they don't want to do business with you anymore. And you're forced to find an alternative who's maybe less reliable. GARSD: And there's something else - a successful social media platform requires that a critical mass of people be on it. What's the point of shouting your opinion to a couple of hundred thousand followers who agree with you when you could be trolling the whole world on Twitter? HUGHES: So this really creates a winner take all scenario where there can be only one Twitter. There can be only one Facebook. GARSD: Nevertheless, about a week after the synagogue shooting, Gab was back online thanks to hosting site epik. com. Epik was subpoenaed by the attorney general of Pennsylvania. The CEO of Epik hit back, warning about the dangers of silencing opinions on the Internet. That's the same view held by Matthew Prince, the CEO of a company called CloudFlare. It protects Gab from cyberattacks. Prince works with thousands of mainstream sites. When it comes to the controversial ones, he says there's not much money to be made. MATTHEW PRINCE: Oftentimes, they use just the free version a service. If they do pass, they pass not much at all. GARSD: So why even bother? Prince talks about something that happened last year. After a deadly white supremacist rally in Charlottesville, Va. , he cut ties to a controversial client - the neo-Nazi publication The Daily Stormer. And Prince has said he immediately regretted that. PRINCE: So while I find something like The Daily Stormer absolutely vile and reprehensible and disgusting, it becomes very dangerous for deep infrastructure companies like us to be effectively silencing one side or another. GARSD: Brian Hughes from American University says Prince's dilemma might be short-lived. He thinks very few of these sites which provide a platform for hate groups will be around for too long. HUGHES: Most of them will be gone in a matter of years. GARSD: But that's not necessarily good news. Hughes and other experts worry that in another generation or so hate groups will migrate completely to the dark web, where they can't be found using traditional search engines or browsers. HUGHES: And it wouldn't be subject to the same kind of oversight from journalists and the people that we typically expect to keep an eye on these things for the health of our democracy. GARSD: He says ultimately it boils down to this - would you rather hate speech and groups be out in the open or hidden from view but still among us? Jasmine Garsd, NPR News, New York. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-09-666239216": {"title": "AI News Anchor Makes Debut In China  : NPR", "url": "https://www.npr.org/2018/11/09/666239216/ai-news-anchor-makes-debut-in-china", "author": "No author found", "published_date": "2018-11-09", "content": "", "section": "Media", "disclaimer": ""}, "2018-11-12-667118322": {"title": "'The Cleaners' Looks At Who Cleans Up The Internet's Toxic Content : NPR", "url": "https://www.npr.org/2018/11/12/667118322/the-cleaners-looks-at-who-cleans-up-the-internets-toxic-content", "author": "No author found", "published_date": "2018-11-12", "content": "ARI SHAPIRO, HOST: From violent threats to trolling, we are looking at toxic content on this month's All Tech Considered. (SOUNDBITE OF MUSIC)SHAPIRO: Companies like Google and Facebook often get asked how they decide what can stay up on their sites and what gets removed. Often they give answers like this. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: By the end of this year, we're going have more than 20,000 people working on security and content review. (SOUNDBITE OF ARCHIVED RECORDING)SUSAN WOJCICKI: There are content that we will remove if it violates our policy. And so we're in the process of having 10,000 people looking at controversial content. SHAPIRO: That was YouTube CEO Susan Wojcicki speaking with Recode in February and, before that, Facebook CEO Mark Zuckerberg speaking before a congressional committee in April. Well, a documentary airing today on PBS looks at who those thousands of content reviewers are deciding what we see and what we don't. The film is called \"The Cleaners. \"(SOUNDBITE OF FILM, \"THE CLEANERS\")UNIDENTIFIED PERSON #1: I have seen hundreds of beheadings. Sometimes they are lucky that it's just a very sharp blade that's being used to them. SHAPIRO: Directors Hans Block and Moritz Riesewieck join us now. Welcome. HANS BLOCK: Hello. MORITZ RIESEWIECK: Hello. SHAPIRO: Just give us a typical job description. What's a day in the life of somebody who works as a moderator for one of these companies? BLOCK: They see all these things which we don't want to see online on social media. That could be terror. That could be beheading videos like the ones the voice was talking about before. It could be pornography. It can be sexual abuse. It could be necrophilia on one hand. And on the other hand, it could be content which could be useful for political debates or to make awareness about war crimes and so on. So they have to moderate thousands of pictures every day. And they need to be quick in order to reach the score for the day. SHAPIRO: Something like 25,000 images a day. BLOCK: Exactly. It's sometimes up to so many pictures a day. And then they need to decide whether to delete it or to let it stay up. And that is called ignore. (SOUNDBITE OF FILM, \"THE CLEANERS\")UNIDENTIFIED PERSON #2: Ignore. UNIDENTIFIED PERSON #3: Di-di (ph), di-di (ph). . . UNIDENTIFIED PERSON #4: Ignore. UNIDENTIFIED PERSON #5: Ignore. SHAPIRO: All of these tech companies use algorithms to weed out toxic content. And so why do they still need humans to do this job? RISESWIECK: Yeah. This is very interesting because when they talk about solutions, they sometimes offer that artificial intelligence will do the job in the future. And this is in a way not true because an algorithm can analyze what is in the picture, for example, a chair or a horse or a glass of water. But what is important to review content all over the world is to see the context of an image. For example, if you see a image with violence, it can be for several reasons - to document violence on social media. It can be for propaganda. So you need humans to see the context of an image. SHAPIRO: But having a human doesn't solve the problem of controlling for context. You use an example of an image that many listeners will have seen from the Vietnam War that shows people fleeing a napalm strike, including a screaming naked girl. This image won a Pulitzer Prize. What do the content moderators do with it? BLOCK: This content moderator, he decides that he would rather delete it because it depicts a young naked child. So he applies this rule against child nudity, which is strictly prohibited. So it is always necessary to distinguish between so many different cases, cases in which you should rather apply this rule or this rule. And there are so many gray areas which remain and which the content moderators sometimes told us they had to decide by their gut feelings. SHAPIRO: Is it unfair to ask any human to try to distinguish between news images and terrorist propaganda, between fine art and pornography? I mean, these are debates that people have been having for as long as these things have existed. RISESWIECK: Absolutely. It's an overwhelming job. It's so complex to distinguish between all these different kinds of rules. But what we - facing when we researched in the Philippines, that these young Filippino workers there have a training from three to five days, which is not enough to do a job like this. SHAPIRO: You explore the impact that looking at these appalling videos and images has on the people who spend all day doing that. And at one part of the film, a woman describes what happened after she viewed sexual content involving a child. (SOUNDBITE OF FILM, \"THE CLEANERS\")UNIDENTIFIED PERSON #6: I went straight to my team leader and told him that I can't do this. I really can't do this. I can't look at the child. But then he told me that I should do it because this is my job and I signed a contract for it. SHAPIRO: What kind of impact does this work have on the people doing it? RISESWIECK: Many of the young people are highly traumatized because of the work. The symptoms are very different. Sometimes people told us they are afraid to go into public places because they're reviewing terror attacks every day or they're afraid to have a intimate relationship with his boy or girlfriend because they're seeing sexual abuse videos every day. So this is kind of the effect this work has. SHAPIRO: The film raises a lot of provocative questions about free speech, about the impact of viewing these images on the people doing the work. What kinds of answers did the tech companies give you? RISESWIECK: Every interview request we sent to the companies, there was no answer at all. So that was really frustrating for us because there is something like a code of silence in the Silicon Valley. No one liked to talk about the insides of these companies. SHAPIRO: Watching the movie, it felt almost dystopian to me. This idea of an underclass of people far, far away who spend all day handling the stuff that we literally cannot bring ourselves to look at, it just seemed so bleak. BLOCK: Yeah, it is. And it's interesting because Manila was a place where the analog toxic waste was sent from the Western world, has been shipped there for years on container ships. And today, the digital garbage is brought there. Now thousands of young content moderators in air-conditioned office towers are clicking through the infinity toxic sea of images and tons of intellectual junk. SHAPIRO: Hans Block and Moritz Riesewieck, thank you so much. BLOCK: Thank you. RISESWIECK: Thank you. SHAPIRO: They directed the documentary \"The Cleaners,\" which airs on PBS tonight. (SOUNDBITE OF MUSIC) ARI SHAPIRO, HOST:  From violent threats to trolling, we are looking at toxic content on this month's All Tech Considered. (SOUNDBITE OF MUSIC) SHAPIRO: Companies like Google and Facebook often get asked how they decide what can stay up on their sites and what gets removed. Often they give answers like this. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: By the end of this year, we're going have more than 20,000 people working on security and content review. (SOUNDBITE OF ARCHIVED RECORDING) SUSAN WOJCICKI: There are content that we will remove if it violates our policy. And so we're in the process of having 10,000 people looking at controversial content. SHAPIRO: That was YouTube CEO Susan Wojcicki speaking with Recode in February and, before that, Facebook CEO Mark Zuckerberg speaking before a congressional committee in April. Well, a documentary airing today on PBS looks at who those thousands of content reviewers are deciding what we see and what we don't. The film is called \"The Cleaners. \" (SOUNDBITE OF FILM, \"THE CLEANERS\") UNIDENTIFIED PERSON #1: I have seen hundreds of beheadings. Sometimes they are lucky that it's just a very sharp blade that's being used to them. SHAPIRO: Directors Hans Block and Moritz Riesewieck join us now. Welcome. HANS BLOCK: Hello. MORITZ RIESEWIECK: Hello. SHAPIRO: Just give us a typical job description. What's a day in the life of somebody who works as a moderator for one of these companies? BLOCK: They see all these things which we don't want to see online on social media. That could be terror. That could be beheading videos like the ones the voice was talking about before. It could be pornography. It can be sexual abuse. It could be necrophilia on one hand. And on the other hand, it could be content which could be useful for political debates or to make awareness about war crimes and so on. So they have to moderate thousands of pictures every day. And they need to be quick in order to reach the score for the day. SHAPIRO: Something like 25,000 images a day. BLOCK: Exactly. It's sometimes up to so many pictures a day. And then they need to decide whether to delete it or to let it stay up. And that is called ignore. (SOUNDBITE OF FILM, \"THE CLEANERS\") UNIDENTIFIED PERSON #2: Ignore. UNIDENTIFIED PERSON #3: Di-di (ph), di-di (ph). . . UNIDENTIFIED PERSON #4: Ignore. UNIDENTIFIED PERSON #5: Ignore. SHAPIRO: All of these tech companies use algorithms to weed out toxic content. And so why do they still need humans to do this job? RISESWIECK: Yeah. This is very interesting because when they talk about solutions, they sometimes offer that artificial intelligence will do the job in the future. And this is in a way not true because an algorithm can analyze what is in the picture, for example, a chair or a horse or a glass of water. But what is important to review content all over the world is to see the context of an image. For example, if you see a image with violence, it can be for several reasons - to document violence on social media. It can be for propaganda. So you need humans to see the context of an image. SHAPIRO: But having a human doesn't solve the problem of controlling for context. You use an example of an image that many listeners will have seen from the Vietnam War that shows people fleeing a napalm strike, including a screaming naked girl. This image won a Pulitzer Prize. What do the content moderators do with it? BLOCK: This content moderator, he decides that he would rather delete it because it depicts a young naked child. So he applies this rule against child nudity, which is strictly prohibited. So it is always necessary to distinguish between so many different cases, cases in which you should rather apply this rule or this rule. And there are so many gray areas which remain and which the content moderators sometimes told us they had to decide by their gut feelings. SHAPIRO: Is it unfair to ask any human to try to distinguish between news images and terrorist propaganda, between fine art and pornography? I mean, these are debates that people have been having for as long as these things have existed. RISESWIECK: Absolutely. It's an overwhelming job. It's so complex to distinguish between all these different kinds of rules. But what we - facing when we researched in the Philippines, that these young Filippino workers there have a training from three to five days, which is not enough to do a job like this. SHAPIRO: You explore the impact that looking at these appalling videos and images has on the people who spend all day doing that. And at one part of the film, a woman describes what happened after she viewed sexual content involving a child. (SOUNDBITE OF FILM, \"THE CLEANERS\") UNIDENTIFIED PERSON #6: I went straight to my team leader and told him that I can't do this. I really can't do this. I can't look at the child. But then he told me that I should do it because this is my job and I signed a contract for it. SHAPIRO: What kind of impact does this work have on the people doing it? RISESWIECK: Many of the young people are highly traumatized because of the work. The symptoms are very different. Sometimes people told us they are afraid to go into public places because they're reviewing terror attacks every day or they're afraid to have a intimate relationship with his boy or girlfriend because they're seeing sexual abuse videos every day. So this is kind of the effect this work has. SHAPIRO: The film raises a lot of provocative questions about free speech, about the impact of viewing these images on the people doing the work. What kinds of answers did the tech companies give you? RISESWIECK: Every interview request we sent to the companies, there was no answer at all. So that was really frustrating for us because there is something like a code of silence in the Silicon Valley. No one liked to talk about the insides of these companies. SHAPIRO: Watching the movie, it felt almost dystopian to me. This idea of an underclass of people far, far away who spend all day handling the stuff that we literally cannot bring ourselves to look at, it just seemed so bleak. BLOCK: Yeah, it is. And it's interesting because Manila was a place where the analog toxic waste was sent from the Western world, has been shipped there for years on container ships. And today, the digital garbage is brought there. Now thousands of young content moderators in air-conditioned office towers are clicking through the infinity toxic sea of images and tons of intellectual junk. SHAPIRO: Hans Block and Moritz Riesewieck, thank you so much. BLOCK: Thank you. RISESWIECK: Thank you. SHAPIRO: They directed the documentary \"The Cleaners,\" which airs on PBS tonight. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-12-666883376": {"title": "R.I.P. HAL: Douglas Rain, Voice Of Computer In '2001,' Dies At 90 : NPR", "url": "https://www.npr.org/2018/11/12/666883376/r-i-p-hal-douglas-rain-voice-of-computer-in-2001-dies-at-90", "author": "No author found", "published_date": "2018-11-12", "content": "", "section": "Obituaries", "disclaimer": ""}, "2018-11-13-666274605": {"title": "How Big Is Amazon? Its Many Businesses In One Chart : NPR", "url": "https://www.npr.org/2018/11/13/666274605/how-big-is-amazon", "author": "No author found", "published_date": "2018-11-13", "content": "", "section": "Business", "disclaimer": ""}, "2018-11-13-665646050": {"title": "Amazon HQ2: NYC and Arlington, Va., Near D.C. Will Split 2nd Headquarters : NPR", "url": "https://www.npr.org/2018/11/13/665646050/amazons-grand-search-for-2nd-headquarters-ends-with-split-nyc-and-d-c-suburb", "author": "No author found", "published_date": "2018-11-13", "content": "", "section": "Business", "disclaimer": ""}, "2018-11-15-668380514": {"title": "Facebook Is On The Defensive After 'NYT' Report On Response To Russian Interference : NPR", "url": "https://www.npr.org/2018/11/15/668380514/facebook-is-on-the-defensive-after-nyt-report-on-response-to-russian-interferenc", "author": "No author found", "published_date": "2018-11-15", "content": "ARI SHAPIRO, HOST: Facebook is under pressure again, this time because of a New York Times report suggesting the company didn't do enough to address Russian interference during the 2016 presidential elections despite alarms raised by its own employees. The company is also accused of hiring a political opposition research firm in Washington to help turn the conversation elsewhere when Facebook was under fire. Facebook CEO Mark Zuckerberg addressed the report on a media call today. And NPR's Alina Selyukh was listening in. Hi, Alina. ALINA SELYUKH, BYLINE: Hello. SHAPIRO: What did Zuckerberg say about all these accusations in the New York Times report? SELYUKH: Well, you know, he held this press conference on a completely - not completely, somewhat unrelated topic. And in the end, the press call did turn into a very long conversation about the New York Times report. The article lays out the case that Facebook essentially spent a long time around the 2016 elections and after them downplaying the spread of Russian misinformation campaigns on the platform, which of course had huge implications for the election. Zuckerberg and Facebook have since basically argued that the company did react and that they did not discourage further investigations, as the article suggests. Here's how Zuckerberg addressed it head-on on the media call. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: To suggest that we weren't interested in knowing the truth or that we wanted to hide what we knew or that we tried to prevent investigations is simply untrue. SELYUKH: The Times article says the internal security team knew as early as 2016 that there were some hackers with ties to Russia doing a little bit of prodding and sending journalists information about leaked emails. Zuckerberg was not combative on the call today. He took questions for over an hour. He repeated himself a lot. And mostly what he just kept saying is reiterating his support, specifically for Chief Operating Officer Sheryl Sandberg, who actually comes out looking the worst in the article that sort of paints her as making politically motivated decisions rather than decisions for truth and democracy. SHAPIRO: The article is such a deep dive, with so many reporter bylines. They apparently spent months working on it. What else in the article did Zuckerberg respond to today? SELYUKH: There's another sort of thread of information about this Republican PR firm that Facebook had hired called Definers Public Affairs. It's a PR firm that in this particular case is criticized for paying for articles and making articles that were going after Facebook's rivals, Google and Apple, and also encouraging reporters to look into this anti-Facebook activist group and allegations of connections to the liberal billionaire who's of course hugely controversial, George Soros. And in this particular case, with this group, Zuckerberg said, you know, they fired this group. And his biggest defense was that he simply had no idea that this firm was being used by the New York Times. (SOUNDBITE OF ARCHIVED RECORDING)ZUCKERBERG: You know, I've mentioned a couple of times that I was not in the loop on a bunch of these decisions. And I should have been clearer that the team has made a bunch of decisions. And I think Sheryl was also not involved. She learned about this at the same time that I did. And we talked about this and came to the conclusion about what we should do here. SELYUKH: So what I was intending to say is that this group was disclosed in - by The New York Times article. And Zuckerberg says this was the first time he heard of them and that the purpose of hiring this PR firm was not - to show just that this anti-Facebook group was not a grassroots campaign spontaneously organized, but one funded by billionaire. And again, he kept saying they've been now fired. SHAPIRO: Facebook has had years of criticism. Democrats about to take over the House have said they're going to look into this. Just in the last 30 seconds or so, are they going to be in a lot of hot water in the months and years ahead? SELYUKH: I think so. There were a lot of questions today about why people should keep trusting Facebook, why people should keep trusting Zuckerberg to be able to keep running this company. To this point, he says, you know, he doesn't expect to talk about layoffs or anybody losing their jobs about this. But I am certain that we will keep hearing about Sheryl Sandberg and Mark Zuckerberg on Facebook and what they've done. SHAPIRO: NPR's Alina Selyukh. Thank you. (SOUNDBITE OF MUSIC) ARI SHAPIRO, HOST:  Facebook is under pressure again, this time because of a New York Times report suggesting the company didn't do enough to address Russian interference during the 2016 presidential elections despite alarms raised by its own employees. The company is also accused of hiring a political opposition research firm in Washington to help turn the conversation elsewhere when Facebook was under fire. Facebook CEO Mark Zuckerberg addressed the report on a media call today. And NPR's Alina Selyukh was listening in. Hi, Alina. ALINA SELYUKH, BYLINE: Hello. SHAPIRO: What did Zuckerberg say about all these accusations in the New York Times report? SELYUKH: Well, you know, he held this press conference on a completely - not completely, somewhat unrelated topic. And in the end, the press call did turn into a very long conversation about the New York Times report. The article lays out the case that Facebook essentially spent a long time around the 2016 elections and after them downplaying the spread of Russian misinformation campaigns on the platform, which of course had huge implications for the election. Zuckerberg and Facebook have since basically argued that the company did react and that they did not discourage further investigations, as the article suggests. Here's how Zuckerberg addressed it head-on on the media call. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: To suggest that we weren't interested in knowing the truth or that we wanted to hide what we knew or that we tried to prevent investigations is simply untrue. SELYUKH: The Times article says the internal security team knew as early as 2016 that there were some hackers with ties to Russia doing a little bit of prodding and sending journalists information about leaked emails. Zuckerberg was not combative on the call today. He took questions for over an hour. He repeated himself a lot. And mostly what he just kept saying is reiterating his support, specifically for Chief Operating Officer Sheryl Sandberg, who actually comes out looking the worst in the article that sort of paints her as making politically motivated decisions rather than decisions for truth and democracy. SHAPIRO: The article is such a deep dive, with so many reporter bylines. They apparently spent months working on it. What else in the article did Zuckerberg respond to today? SELYUKH: There's another sort of thread of information about this Republican PR firm that Facebook had hired called Definers Public Affairs. It's a PR firm that in this particular case is criticized for paying for articles and making articles that were going after Facebook's rivals, Google and Apple, and also encouraging reporters to look into this anti-Facebook activist group and allegations of connections to the liberal billionaire who's of course hugely controversial, George Soros. And in this particular case, with this group, Zuckerberg said, you know, they fired this group. And his biggest defense was that he simply had no idea that this firm was being used by the New York Times. (SOUNDBITE OF ARCHIVED RECORDING) ZUCKERBERG: You know, I've mentioned a couple of times that I was not in the loop on a bunch of these decisions. And I should have been clearer that the team has made a bunch of decisions. And I think Sheryl was also not involved. She learned about this at the same time that I did. And we talked about this and came to the conclusion about what we should do here. SELYUKH: So what I was intending to say is that this group was disclosed in - by The New York Times article. And Zuckerberg says this was the first time he heard of them and that the purpose of hiring this PR firm was not - to show just that this anti-Facebook group was not a grassroots campaign spontaneously organized, but one funded by billionaire. And again, he kept saying they've been now fired. SHAPIRO: Facebook has had years of criticism. Democrats about to take over the House have said they're going to look into this. Just in the last 30 seconds or so, are they going to be in a lot of hot water in the months and years ahead? SELYUKH: I think so. There were a lot of questions today about why people should keep trusting Facebook, why people should keep trusting Zuckerberg to be able to keep running this company. To this point, he says, you know, he doesn't expect to talk about layoffs or anybody losing their jobs about this. But I am certain that we will keep hearing about Sheryl Sandberg and Mark Zuckerberg on Facebook and what they've done. SHAPIRO: NPR's Alina Selyukh. Thank you. (SOUNDBITE OF MUSIC)", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-16-668189361": {"title": "Electric Scooters: Why Ford Is Getting Into The Scooter Business : NPR", "url": "https://www.npr.org/2018/11/16/668189361/why-ford-is-getting-into-the-scooter-business", "author": "No author found", "published_date": "2018-11-16", "content": "", "section": "Business", "disclaimer": ""}, "2018-11-16-668359548": {"title": "Simone Giertz: What Can Making Useless Robots Teach Us About Joy? : NPR", "url": "https://www.npr.org/2018/11/16/668359548/simone-giertz-what-can-making-useless-robots-teach-us-about-joy", "author": "No author found", "published_date": "2018-11-16", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. Hi, Simone. Can you hear me? SIMONE GIERTZ: Yeah. Who is this? RAZ: This is Guy Raz. I'm the host of the program. GIERTZ: Oh, that's Guy. Oh, OK. I thought it was the guy in the control room. . . RAZ: That's all right. GIERTZ: . . . 'Cause can I hear him, too. How are you? RAZ: I'm good. How are you? GIERTZ: I'm good. I just watched my TED Talk 'cause I was like, wait - what did I say now again? (SOUNDBITE OF MUSIC)RAZ: This is Simone Giertz. GIERTZ: And my last name is spelled G-I-E-R-T-Z, so the pronunciation makes no sense whatsoever. . . RAZ: (Laughter). GIERTZ: . . . I think, in any language. I've just concluded that my ancestors were trolls. RAZ: And what do you do? GIERTZ: I run a YouTube channel. I'm an inventor of useless machines. (SOUNDBITE OF MUSIC)RAZ: All right. So a little explanation. Simone builds these useless machines in order to teach herself about engineering and robotics. GIERTZ: I mean, generally, a useless machine comes from a problem. So I find a problem that I want to solve, and then I just go with the most kind of ridiculous way of doing it. RAZ: Now, the problems Simone works on are actually pretty simple tasks, like pouring a bowl of cereal or chopping vegetables. And Simone's useless machines - they usually don't work all that well. GIERTZ: Yeah. They're useless at solving the problem that they're attempting to solve, but they're useful in the way that they can make people laugh, which sounds really cheesy, but (laughter). . . RAZ: Here's Simone Giertz on the TED stage. (SOUNDBITE OF TED TALK)GIERTZ: As an inventor, I'm interested in things that people struggle with. It can be small things or big things or medium-sized things. And identifying a problem is the first step in my process of building a useless machine. For example, brushing your teeth - like, it's the thing we all have to do. It's kind of boring, and nobody really likes it. So what about if you had a machine that could do it for you? RAZ: Why don't you describe this thing that you showed to the audience? GIERTZ: This was the first kind of useless machine that I made. It was a helmet that had a robot arm on it just right on the top of it like a unicorn horn. And at the end of it, it had a toothbrush. (SOUNDBITE OF TED TALK)GIERTZ: I call it the Toothbrush Helmet. And the robot arm kind of lowers the toothbrush in front of my mouth and brushes my teeth. (SOUNDBITE OF TED TALK)GIERTZ: So my Toothbrush Helmet is recommended by 0 out of 10 dentists, and it definitely did not revolutionize the world of dentistry. But it did completely change my life because I finished making this Toothbrush Helmet three years ago. And after I finished making it, I went into my living room, and I put up a camera and I filmed a seven-second clip of it working. Since then, I've carved out this little niche for myself on the Internet as an inventor of useless machines, because, as we all know, the easiest way to be at the top of your field is to choose a very small field. (SOUNDBITE OF MUSIC)RAZ: You spend many hours - days of your life making these useless robots. GIERTZ: Do I hear judgment in your. . . RAZ: No, no. No judgment. (LAUGHTER)RAZ: It's just amazing. I mean, right? GIERTZ: Yeah. I mean, I do have to, like, have that conversation with myself. Like, can I really spend 14 hours on doing this? Is this really - does this make sense? RAZ: Yeah. GIERTZ: I'm almost 30. What am I doing? But, no. It is - it is joyful because I'm like, well, I'm creating something. My motto has kind of become, if I find it interesting, then there are probably other people who will find it interesting, too. (SOUNDBITE OF MUSIC)RAZ: There's so much focus today on achievement and success that it's easy to lose sight of a simple emotion - joy. And joy isn't just this nice, pleasant feeling. It can actually lay the groundwork for a richer life and a deep connection with the world around us. So on our show today, we're going to talk about finding joy in some places you might expect, and in some places you might not. And if you watch Simone's videos on YouTube, it's not very hard to see why more than a million people subscribe to her channel, because Simone just shares her joy creating these machines - machines that have no real purpose. Can you just kind of describe a couple of other things that you've invented? GIERTZ: I think the machine that most people have seen that I've made is the Wake-Up Machine. And it's this alarm clock that has a big plastic hand on it and that slaps me to wake me up. (SOUNDBITE OF YOUTUBE VIDEO, \"THE WAKE-UP MACHINE VLOG\")GIERTZ: (Screaming) Ow. I've made a plethora of other useless machines. More recently, I made a machine that serves me soup, and it kind of just throws the bowl of soup at me. (SOUNDBITE OF YOUTUBE VIDEO, \"I MADE A ROBOT THAT SERVES ME SOUP\")GIERTZ: Hey, Google. Turn on the Soup Robot. COMPUTER-GENERATED VOICE: You got it. Turning the Soup Robot on. GIERTZ: I've also just - I mean, I'm doing a lot of different things. (SOUNDBITE OF YOUTUBE VIDEO, \"WHAT IF YOU COULD HAVE THINGS ORBIT AROUND YOUR HEAD? \")GIERTZ: What if you could have things orbit around your head like a personal delivery system? (SOUNDBITE OF YOUTUBE VIDEO, \"I BUILT A HAMMERING MACHINE THAT DESTROYS EVERYTHING\")GIERTZ: The Hammering Robot is definitely more destructive than constructive. I think we need a little bit of iteration, but we're getting there. I think it'll work. This would be so much easier if I just knew what I was doing. Looking back, it's pretty obvious that I didn't have a game plan. I was just, like, skipping from one stone to another and being like, I'm going to try this, and I'm going to try this. I mean, engineering is so much about what the relationship you have to the tools you use. It's almost like a love relationship where you have to make sure that you use them for fun things as well to kind of keep yourself excited about it and to keep yourself engaged. At least that's how it's been for me. (SOUNDBITE OF TED TALK)GIERTZ: I'm not an engineer. I did not study engineering in school, but I was a super ambitious student growing up. In middle school and high school, I had straight A's, and I graduated at the top of my year. On the flip side of that, I struggled with very severe performance anxiety. Here's an email I sent to my brother around that time. You won't understand how difficult it is for me to confess this. I'm so freaking embarrassed. I don't want people to think that I'm stupid. And, no, I did not accidentally burn our parents' house down. The thing is I'm so upset about is that I got a B on a math test. So something obviously happened between here and here. I got interested in building robots, and I wanted to teach myself about hardware. But building things with hardware, especially if you're teaching yourself, is something that's really difficult to do. It has a high likelihood of failure, and moreover, it has a high likelihood of making you feel stupid. So I came up with a setup that would guarantee success a hundred percent of the time. And that was that instead of trying to succeed, I was going to try to build things that would fail. And even though I didn't realize it at the time, building stupid things was actually quite smart because as I kept on learning about hardware, for the first time in my life, I did not have to deal with my performance anxiety. And as soon as I removed all pressure and expectations for myself, that pressure quickly got replaced by enthusiasm, and it allowed me to just play. (SOUNDBITE OF MUSIC)RAZ: I mean, it strikes me that on the one hand, you could describe this as, there's Simone. She's making useless robots. Right? But on the other hand, like, this is something that brings joy to a lot of people. People love watching this because it's really funny. Is there part of this idea that you just want people to laugh? GIERTZ: Yeah. Most definitely. I mean, at least for me, that's always been how I justify taking up any space in the world. It's with a punch line. But to me, it became this, like - engineering is really intimidating. And trying to set out to learn about engineering, especially if you're teaching yourself, like I did, it's just such a daunting process and made me not want to go in and learn about it 'cause I was really scared to be questioned. So building useless machines, for me, started as, like, kind of a cathartic, just, like, I want to learn about engineering but I don't dare to actually try to make it in a serious way so I'm just going to build things that fail. RAZ: You know, I mean, like, most of us were sort of trained to believe that a life of purpose and meaning requires goals. Like, building a career or creating community is, like, something very concrete, and that's what gives us meaning. But it strikes me that that's actually incomplete, that there are things, there are pursuits without a goal, like, just purely because they're fun or joyful that are just as meaningful. You know what I mean? GIERTZ: I also feel like creating things doesn't have to be much more than that. And there is definitely a lot of joy to be found. It's just that there's so much else getting in the way. And we live in a time of distraction. I've just started just thinking about my phone. If my phone would be a person then it would be the biggest jerk of all time 'cause whenever you're, like, with family, or you're trying to relax or something, it would just be that friend who comes and, like, taps you on the shoulder. And it's like, hey, I got something. Yeah. You really want to see this. Come. Just check this message. So we're constantly just being so distracted by things. And I think that is the biggest joy killer, at least for me. And just making things is such a good counterbalance to that. (SOUNDBITE OF TED TALK)GIERTZ: So as much as my machines can seem like simple engineering slapstick, I've realized that I stumbled on something bigger than that. It's this expression of joy and humility that often gets lost in engineering. And for me, it was a way to learn about hardware without having my performance anxiety get in the way. And I often get asked if I think I'm ever going to build something useful. But the way I see it, I already have because I've built myself this job. And it's something that I could never have planned for. Instead, it happened just because I was so enthusiastic about what I was doing and I was sharing that enthusiasm with other people. To me, there's the true beauty of making useless things. Because it's this acknowledgement that you don't always know what the best answer is. It turns off that voice in your head that tells you that you know exactly how the world works. And maybe a toothbrush helmet isn't the answer, but at least you're asking the question. Thank you. (APPLAUSE)RAZ: That's Simone Giertz. You can find all of her videos on YouTube, and you can watch her entire talk at ted. com. GUY RAZ, HOST:  It's the TED Radio Hour from NPR. Hi, Simone. Can you hear me? SIMONE GIERTZ: Yeah. Who is this? RAZ: This is Guy Raz. I'm the host of the program. GIERTZ: Oh, that's Guy. Oh, OK. I thought it was the guy in the control room. . . RAZ: That's all right. GIERTZ: . . . 'Cause can I hear him, too. How are you? RAZ: I'm good. How are you? GIERTZ: I'm good. I just watched my TED Talk 'cause I was like, wait - what did I say now again? (SOUNDBITE OF MUSIC) RAZ: This is Simone Giertz. GIERTZ: And my last name is spelled G-I-E-R-T-Z, so the pronunciation makes no sense whatsoever. . . RAZ: (Laughter). GIERTZ: . . . I think, in any language. I've just concluded that my ancestors were trolls. RAZ: And what do you do? GIERTZ: I run a YouTube channel. I'm an inventor of useless machines. (SOUNDBITE OF MUSIC) RAZ: All right. So a little explanation. Simone builds these useless machines in order to teach herself about engineering and robotics. GIERTZ: I mean, generally, a useless machine comes from a problem. So I find a problem that I want to solve, and then I just go with the most kind of ridiculous way of doing it. RAZ: Now, the problems Simone works on are actually pretty simple tasks, like pouring a bowl of cereal or chopping vegetables. And Simone's useless machines - they usually don't work all that well. GIERTZ: Yeah. They're useless at solving the problem that they're attempting to solve, but they're useful in the way that they can make people laugh, which sounds really cheesy, but (laughter). . . RAZ: Here's Simone Giertz on the TED stage. (SOUNDBITE OF TED TALK) GIERTZ: As an inventor, I'm interested in things that people struggle with. It can be small things or big things or medium-sized things. And identifying a problem is the first step in my process of building a useless machine. For example, brushing your teeth - like, it's the thing we all have to do. It's kind of boring, and nobody really likes it. So what about if you had a machine that could do it for you? RAZ: Why don't you describe this thing that you showed to the audience? GIERTZ: This was the first kind of useless machine that I made. It was a helmet that had a robot arm on it just right on the top of it like a unicorn horn. And at the end of it, it had a toothbrush. (SOUNDBITE OF TED TALK) GIERTZ: I call it the Toothbrush Helmet. And the robot arm kind of lowers the toothbrush in front of my mouth and brushes my teeth. (SOUNDBITE OF TED TALK) GIERTZ: So my Toothbrush Helmet is recommended by 0 out of 10 dentists, and it definitely did not revolutionize the world of dentistry. But it did completely change my life because I finished making this Toothbrush Helmet three years ago. And after I finished making it, I went into my living room, and I put up a camera and I filmed a seven-second clip of it working. Since then, I've carved out this little niche for myself on the Internet as an inventor of useless machines, because, as we all know, the easiest way to be at the top of your field is to choose a very small field. (SOUNDBITE OF MUSIC) RAZ: You spend many hours - days of your life making these useless robots. GIERTZ: Do I hear judgment in your. . . RAZ: No, no. No judgment. (LAUGHTER) RAZ: It's just amazing. I mean, right? GIERTZ: Yeah. I mean, I do have to, like, have that conversation with myself. Like, can I really spend 14 hours on doing this? Is this really - does this make sense? RAZ: Yeah. GIERTZ: I'm almost 30. What am I doing? But, no. It is - it is joyful because I'm like, well, I'm creating something. My motto has kind of become, if I find it interesting, then there are probably other people who will find it interesting, too. (SOUNDBITE OF MUSIC) RAZ: There's so much focus today on achievement and success that it's easy to lose sight of a simple emotion - joy. And joy isn't just this nice, pleasant feeling. It can actually lay the groundwork for a richer life and a deep connection with the world around us. So on our show today, we're going to talk about finding joy in some places you might expect, and in some places you might not. And if you watch Simone's videos on YouTube, it's not very hard to see why more than a million people subscribe to her channel, because Simone just shares her joy creating these machines - machines that have no real purpose. Can you just kind of describe a couple of other things that you've invented? GIERTZ: I think the machine that most people have seen that I've made is the Wake-Up Machine. And it's this alarm clock that has a big plastic hand on it and that slaps me to wake me up. (SOUNDBITE OF YOUTUBE VIDEO, \"THE WAKE-UP MACHINE VLOG\") GIERTZ: (Screaming) Ow. I've made a plethora of other useless machines. More recently, I made a machine that serves me soup, and it kind of just throws the bowl of soup at me. (SOUNDBITE OF YOUTUBE VIDEO, \"I MADE A ROBOT THAT SERVES ME SOUP\") GIERTZ: Hey, Google. Turn on the Soup Robot. COMPUTER-GENERATED VOICE: You got it. Turning the Soup Robot on. GIERTZ: I've also just - I mean, I'm doing a lot of different things. (SOUNDBITE OF YOUTUBE VIDEO, \"WHAT IF YOU COULD HAVE THINGS ORBIT AROUND YOUR HEAD? \") GIERTZ: What if you could have things orbit around your head like a personal delivery system? (SOUNDBITE OF YOUTUBE VIDEO, \"I BUILT A HAMMERING MACHINE THAT DESTROYS EVERYTHING\") GIERTZ: The Hammering Robot is definitely more destructive than constructive. I think we need a little bit of iteration, but we're getting there. I think it'll work. This would be so much easier if I just knew what I was doing. Looking back, it's pretty obvious that I didn't have a game plan. I was just, like, skipping from one stone to another and being like, I'm going to try this, and I'm going to try this. I mean, engineering is so much about what the relationship you have to the tools you use. It's almost like a love relationship where you have to make sure that you use them for fun things as well to kind of keep yourself excited about it and to keep yourself engaged. At least that's how it's been for me. (SOUNDBITE OF TED TALK) GIERTZ: I'm not an engineer. I did not study engineering in school, but I was a super ambitious student growing up. In middle school and high school, I had straight A's, and I graduated at the top of my year. On the flip side of that, I struggled with very severe performance anxiety. Here's an email I sent to my brother around that time. You won't understand how difficult it is for me to confess this. I'm so freaking embarrassed. I don't want people to think that I'm stupid. And, no, I did not accidentally burn our parents' house down. The thing is I'm so upset about is that I got a B on a math test. So something obviously happened between here and here. I got interested in building robots, and I wanted to teach myself about hardware. But building things with hardware, especially if you're teaching yourself, is something that's really difficult to do. It has a high likelihood of failure, and moreover, it has a high likelihood of making you feel stupid. So I came up with a setup that would guarantee success a hundred percent of the time. And that was that instead of trying to succeed, I was going to try to build things that would fail. And even though I didn't realize it at the time, building stupid things was actually quite smart because as I kept on learning about hardware, for the first time in my life, I did not have to deal with my performance anxiety. And as soon as I removed all pressure and expectations for myself, that pressure quickly got replaced by enthusiasm, and it allowed me to just play. (SOUNDBITE OF MUSIC) RAZ: I mean, it strikes me that on the one hand, you could describe this as, there's Simone. She's making useless robots. Right? But on the other hand, like, this is something that brings joy to a lot of people. People love watching this because it's really funny. Is there part of this idea that you just want people to laugh? GIERTZ: Yeah. Most definitely. I mean, at least for me, that's always been how I justify taking up any space in the world. It's with a punch line. But to me, it became this, like - engineering is really intimidating. And trying to set out to learn about engineering, especially if you're teaching yourself, like I did, it's just such a daunting process and made me not want to go in and learn about it 'cause I was really scared to be questioned. So building useless machines, for me, started as, like, kind of a cathartic, just, like, I want to learn about engineering but I don't dare to actually try to make it in a serious way so I'm just going to build things that fail. RAZ: You know, I mean, like, most of us were sort of trained to believe that a life of purpose and meaning requires goals. Like, building a career or creating community is, like, something very concrete, and that's what gives us meaning. But it strikes me that that's actually incomplete, that there are things, there are pursuits without a goal, like, just purely because they're fun or joyful that are just as meaningful. You know what I mean? GIERTZ: I also feel like creating things doesn't have to be much more than that. And there is definitely a lot of joy to be found. It's just that there's so much else getting in the way. And we live in a time of distraction. I've just started just thinking about my phone. If my phone would be a person then it would be the biggest jerk of all time 'cause whenever you're, like, with family, or you're trying to relax or something, it would just be that friend who comes and, like, taps you on the shoulder. And it's like, hey, I got something. Yeah. You really want to see this. Come. Just check this message. So we're constantly just being so distracted by things. And I think that is the biggest joy killer, at least for me. And just making things is such a good counterbalance to that. (SOUNDBITE OF TED TALK) GIERTZ: So as much as my machines can seem like simple engineering slapstick, I've realized that I stumbled on something bigger than that. It's this expression of joy and humility that often gets lost in engineering. And for me, it was a way to learn about hardware without having my performance anxiety get in the way. And I often get asked if I think I'm ever going to build something useful. But the way I see it, I already have because I've built myself this job. And it's something that I could never have planned for. Instead, it happened just because I was so enthusiastic about what I was doing and I was sharing that enthusiasm with other people. To me, there's the true beauty of making useless things. Because it's this acknowledgement that you don't always know what the best answer is. It turns off that voice in your head that tells you that you know exactly how the world works. And maybe a toothbrush helmet isn't the answer, but at least you're asking the question. Thank you. (APPLAUSE) RAZ: That's Simone Giertz. You can find all of her videos on YouTube, and you can watch her entire talk at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-17-668408122": {"title": "Facebook Increasingly Reliant on A.I. To Predict Suicide Risk : NPR", "url": "https://www.npr.org/2018/11/17/668408122/facebook-increasingly-reliant-on-a-i-to-predict-suicide-risk", "author": "No author found", "published_date": "2018-11-17", "content": "LAKSHMI SINGH, HOST: For the last year, Facebook has been running a new system that automatically scans people's accounts for signs of suicide risk and alerts the police. As NPR's Martin Kaste reports, it raises new questions about social media companies intervening in the real-world lives of their customers. MARTIN KASTE, BYLINE: Facebook's using artificial intelligence to find cases of people who seem about to harm themselves. The AI is learning which kinds of online chatter it should take seriously. For instance, if a person is streaming a live video, and the replies to that video start to sound ominous. . . ANTIGONE DAVIS: Maybe, like, please don't do this. We really care about you. There are different types of signals like that that will give us a strong sense that someone may be posting self-harm content. KASTE: That's Antigone Davis, Facebook's global head of safety. When the software flags someone, she says Facebook staffers decide whether to call the local police. And AI comes into play there, too. DAVIS: We also are able to use AI to coordinate a bunch of information on location to try to identify the location of that individual so that we can reach out to the right emergency response team. KASTE: In this first year of the system's operation, that's happened now about 3,500 times, Facebook says. In other words, about 10 times a day, Facebook is calling police or first responders somewhere in the world to check on someone based on an initial alert produced by the monitoring software. This is a Facebook promotional video with testimonials from police in upstate New York talking about getting one of those alerts. (SOUNDBITE OF ARCHIVED RECORDING)JAMES GRICE: We did find her. She admitted to taking medication, and we were able to get her to a local hospital. JOSEPH A. GERACE: There's no doubt in my mind that this saved her life. KASTE: The new system has been welcomed by suicide prevention advocates, especially given the rising suicide numbers of recent years. But Mason Marks is more cautious. MASON MARKS: I don't know if Facebook should be doing this. KASTE: Marks studies the intersection between medicine, privacy and artificial intelligence. He says he gets why Facebook is doing this. The company has been under pressure, especially after some people used the livestream video to broadcast suicides and self-harm. But he wonders whether using an AI to flag cases for police attention is the right solution. MARKS: It needs to be done very methodically, very cautiously, transparently and really looking at the evidence. KASTE: Marks doesn't like the fact that Facebook is holding back some key details. For instance, how accurate is this? How many of those 3,500 calls actually turned out to be real emergencies? He says outsiders have to be able to evaluate this system and its potential side effects. MARKS: People may also learn that if they do talk about suicide openly that they might fear a visit from police, so they might pull back and not engage in an open, honest dialogue. And I'm not sure that's a good thing. KASTE: And this kind of AI-based monitoring may soon go beyond suicide prevention. Again, Facebook's Antigone Davis. DAVIS: I think more and more we will see AI used in the context of safety and in the context of potentially preventing harm. KASTE: For instance, using AI to detect inappropriate interactions online between adults and minors. She says that's also something Facebook is experimenting with. Law professor Ryan Calo says this would be the typical pattern for how a new monitoring technology would expand into law enforcement. RYAN CALO: The way it would happen would be we would take something that everybody agrees is terrible. It would be something like suicide, which is epidemic, something like child pornography, something like terrorism - so these early things. And then, if they showed promise in those sectors, we broaden them to more and more things. And that - you know, that's a concern. KASTE: Calo was co-director of the tech policy lab at the University of Washington, and he specializes in technology and privacy. He says we need to think about the possibility that this kind of AI will be used more broadly - say, to monitor social media chatter for signs of impending violence between people. Would that be desirable? CALO: If you can truly get an up or down, yes or no, and that's reliable, if intervention is not likely to cause additional harm. And then this is something that we think is important enough to prevent that this is justified. And so that's a difficult calculus, and it's one that I think we're going to be making more and more. KASTE: Especially if tech companies continue to show a willingness to call the police because of something an AI spotted online. Martin Kaste, NPR News. LAKSHMI SINGH, HOST:  For the last year, Facebook has been running a new system that automatically scans people's accounts for signs of suicide risk and alerts the police. As NPR's Martin Kaste reports, it raises new questions about social media companies intervening in the real-world lives of their customers. MARTIN KASTE, BYLINE: Facebook's using artificial intelligence to find cases of people who seem about to harm themselves. The AI is learning which kinds of online chatter it should take seriously. For instance, if a person is streaming a live video, and the replies to that video start to sound ominous. . . ANTIGONE DAVIS: Maybe, like, please don't do this. We really care about you. There are different types of signals like that that will give us a strong sense that someone may be posting self-harm content. KASTE: That's Antigone Davis, Facebook's global head of safety. When the software flags someone, she says Facebook staffers decide whether to call the local police. And AI comes into play there, too. DAVIS: We also are able to use AI to coordinate a bunch of information on location to try to identify the location of that individual so that we can reach out to the right emergency response team. KASTE: In this first year of the system's operation, that's happened now about 3,500 times, Facebook says. In other words, about 10 times a day, Facebook is calling police or first responders somewhere in the world to check on someone based on an initial alert produced by the monitoring software. This is a Facebook promotional video with testimonials from police in upstate New York talking about getting one of those alerts. (SOUNDBITE OF ARCHIVED RECORDING) JAMES GRICE: We did find her. She admitted to taking medication, and we were able to get her to a local hospital. JOSEPH A. GERACE: There's no doubt in my mind that this saved her life. KASTE: The new system has been welcomed by suicide prevention advocates, especially given the rising suicide numbers of recent years. But Mason Marks is more cautious. MASON MARKS: I don't know if Facebook should be doing this. KASTE: Marks studies the intersection between medicine, privacy and artificial intelligence. He says he gets why Facebook is doing this. The company has been under pressure, especially after some people used the livestream video to broadcast suicides and self-harm. But he wonders whether using an AI to flag cases for police attention is the right solution. MARKS: It needs to be done very methodically, very cautiously, transparently and really looking at the evidence. KASTE: Marks doesn't like the fact that Facebook is holding back some key details. For instance, how accurate is this? How many of those 3,500 calls actually turned out to be real emergencies? He says outsiders have to be able to evaluate this system and its potential side effects. MARKS: People may also learn that if they do talk about suicide openly that they might fear a visit from police, so they might pull back and not engage in an open, honest dialogue. And I'm not sure that's a good thing. KASTE: And this kind of AI-based monitoring may soon go beyond suicide prevention. Again, Facebook's Antigone Davis. DAVIS: I think more and more we will see AI used in the context of safety and in the context of potentially preventing harm. KASTE: For instance, using AI to detect inappropriate interactions online between adults and minors. She says that's also something Facebook is experimenting with. Law professor Ryan Calo says this would be the typical pattern for how a new monitoring technology would expand into law enforcement. RYAN CALO: The way it would happen would be we would take something that everybody agrees is terrible. It would be something like suicide, which is epidemic, something like child pornography, something like terrorism - so these early things. And then, if they showed promise in those sectors, we broaden them to more and more things. And that - you know, that's a concern. KASTE: Calo was co-director of the tech policy lab at the University of Washington, and he specializes in technology and privacy. He says we need to think about the possibility that this kind of AI will be used more broadly - say, to monitor social media chatter for signs of impending violence between people. Would that be desirable? CALO: If you can truly get an up or down, yes or no, and that's reliable, if intervention is not likely to cause additional harm. And then this is something that we think is important enough to prevent that this is justified. And so that's a difficult calculus, and it's one that I think we're going to be making more and more. KASTE: Especially if tech companies continue to show a willingness to call the police because of something an AI spotted online. Martin Kaste, NPR News.", "section": "National", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-18-668847241": {"title": "Supercomputers Assist Firefighters As Wildfires Spread In California  : NPR", "url": "https://www.npr.org/2018/11/18/668847241/supercomputers-assist-firefighters-in-evacuations-as-wildfires-spread-in-califor", "author": "No author found", "published_date": "2018-11-18", "content": "LULU GARCIA-NAVARRO, HOST: Back to California now. The devastating fires burning across California have put a spotlight on how to evacuate people. Knowing where a wildfire is going to spread can save lives. Now Californian firefighters are getting a better picture of that, thanks to supercomputers, as Lauren Sommer reports from member station KQED. LAUREN SOMMER, BYLINE: The thing that shocked everyone about the Camp Fire was its Speed. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #1: The winds are too strong. There's fire on the roadways around us. SOMMER: At one point, the fire was burning 80 acres per minute. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON #2: There's a report of a possible entrapment. RP advising they cannot evacuate due to fire. JONATHAN COX: The abnormal is the new normal. SOMMER: Jonathan Cox is a division chief with Cal Fire. COX: It's something that, you know, 30-year firefighters have never seen. SOMMER: Cox says in these kinds of fires, the first few hours are key for predicting how it will spread. COX: This is an inexact science that is having to be done during the middle of a disaster. So it can be extremely difficult to get a really precise idea of where a fire is going. SOMMER: They look at wind speed, moisture, terrain and rely on years of experience to make those predictions. But lately, California firefighters have been getting some help from a powerful, new tool. So does your supercomputer have a name? ILKAY ALTINTAS: Yes. Our current supercomputer is called Comet. SOMMER: Ilkay Altintas works at the San Diego Supercomputer Center at UC San Diego. The supercomputer Comet equals the power of 2 million smartphones. It's been using real-time data from reconnaissance flights and NASA satellites to quickly model how a wildfire will behave. ALTINTAS: So we can understand where the fire will be, its rate of spread, its direction for the next couple of hours. Having that information in your hand as fast as possible is very important. SOMMER: It's a big step up from the basic software firefighters have had on laptops for about a decade. Cal Fire has been getting these supercomputer forecasts for the Woolsey Fire burning in Southern California. Jonathan Cox of Cal Fire told me from the scene that it's still experimental, but more information is always better. COX: The more we can do and the more information we can get and decisions we can make based on technology is obviously the future. SOMMER: But there are some kinds of fires these computer models cannot predict very well, the ones that make their own weather. JANICE COEN: The winds within a fire can be incredibly intense - 50, 70 mph. SOMMER: Janice Coen is a scientist at the National Center for Atmospheric Research in Colorado. She says a lot of fires propel themselves forward, even creating fire tornadoes. So she's developing a computer model to predict that. COEN: I have a lot of hope that we'll be able to understand fires and anticipate their behavior so that we can learn from it and avoid more catastrophes in the future. SOMMER: Something that's crucial as fires become more extreme in a warming climate. For NPR News, I'm Lauren Sommer. LULU GARCIA-NAVARRO, HOST:  Back to California now. The devastating fires burning across California have put a spotlight on how to evacuate people. Knowing where a wildfire is going to spread can save lives. Now Californian firefighters are getting a better picture of that, thanks to supercomputers, as Lauren Sommer reports from member station KQED. LAUREN SOMMER, BYLINE: The thing that shocked everyone about the Camp Fire was its Speed. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #1: The winds are too strong. There's fire on the roadways around us. SOMMER: At one point, the fire was burning 80 acres per minute. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON #2: There's a report of a possible entrapment. RP advising they cannot evacuate due to fire. JONATHAN COX: The abnormal is the new normal. SOMMER: Jonathan Cox is a division chief with Cal Fire. COX: It's something that, you know, 30-year firefighters have never seen. SOMMER: Cox says in these kinds of fires, the first few hours are key for predicting how it will spread. COX: This is an inexact science that is having to be done during the middle of a disaster. So it can be extremely difficult to get a really precise idea of where a fire is going. SOMMER: They look at wind speed, moisture, terrain and rely on years of experience to make those predictions. But lately, California firefighters have been getting some help from a powerful, new tool. So does your supercomputer have a name? ILKAY ALTINTAS: Yes. Our current supercomputer is called Comet. SOMMER: Ilkay Altintas works at the San Diego Supercomputer Center at UC San Diego. The supercomputer Comet equals the power of 2 million smartphones. It's been using real-time data from reconnaissance flights and NASA satellites to quickly model how a wildfire will behave. ALTINTAS: So we can understand where the fire will be, its rate of spread, its direction for the next couple of hours. Having that information in your hand as fast as possible is very important. SOMMER: It's a big step up from the basic software firefighters have had on laptops for about a decade. Cal Fire has been getting these supercomputer forecasts for the Woolsey Fire burning in Southern California. Jonathan Cox of Cal Fire told me from the scene that it's still experimental, but more information is always better. COX: The more we can do and the more information we can get and decisions we can make based on technology is obviously the future. SOMMER: But there are some kinds of fires these computer models cannot predict very well, the ones that make their own weather. JANICE COEN: The winds within a fire can be incredibly intense - 50, 70 mph. SOMMER: Janice Coen is a scientist at the National Center for Atmospheric Research in Colorado. She says a lot of fires propel themselves forward, even creating fire tornadoes. So she's developing a computer model to predict that. COEN: I have a lot of hope that we'll be able to understand fires and anticipate their behavior so that we can learn from it and avoid more catastrophes in the future. SOMMER: Something that's crucial as fires become more extreme in a warming climate. For NPR News, I'm Lauren Sommer.", "section": "Environment And Energy Collaborative", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-18-667944005": {"title": "2018 Elections: 9 New Lawmakers With STEM Backgrounds Set To Join Congress : NPR", "url": "https://www.npr.org/2018/11/18/667944005/science-technology-math-engineering-and-now-congress", "author": "No author found", "published_date": "2018-11-18", "content": "", "section": "Elections", "disclaimer": ""}, "2018-11-21-669995003": {"title": "No Plans Of Leaving: Facebook CEO Mark Zuckerberg Fires Back At Critics : NPR", "url": "https://www.npr.org/2018/11/21/669995003/no-plans-of-leaving-facebook-ceo-mark-zuckerberg-fires-back-at-critics", "author": "No author found", "published_date": "2018-11-21", "content": "", "section": "Technology", "disclaimer": ""}, "2018-11-21-660168325": {"title": "Amazon Prime: How AI And Forecasting Power One-Hour Deliveries : NPR", "url": "https://www.npr.org/2018/11/21/660168325/optimized-prime-how-ai-and-anticipation-power-amazons-1-hour-deliveries", "author": "No author found", "published_date": "2018-11-21", "content": "RACHEL MARTIN, HOST: Americans take for granted that most things they buy online will arrive on their doorstep two days after they clicked the order button. When Amazon introduced two-day shipping, it was a huge shift in retail thinking. To stay competitive, most major stores now offer similar speedy delivery. But getting you that package in just two days - or in some cases in just one hour - is extremely expensive. NPR's Alina Selyukh got a rare look at how Amazon makes it work. ALINA SELYUKH, BYLINE: By the time someone clicks buy on Amazon, usually Jenny Freshwater had long predicted it. JENNY FRESHWATER: In advance of them even knowing they want the product, in many cases. SELYUKH: Her team's job is to anticipate demand for every single thing sold by Amazon in the world - not just how many blouses should Amazon sell but what color, size, sleeve length and most importantly. . . FRESHWATER: Where do we actually put the product so that our customers can get it when they click buy? SELYUKH: This is key to how Amazon cuts down delivery time, by stocking items as close as possible to the people who will buy them. That's why Amazon has been building smaller warehouses closer to city centers, where delivery might be within hours. And we should note, Amazon is one of NPR's sponsors. FRESHWATER: We've started to build these AI algorithms, and the more we build, the better they get. SELYUKH: AI algorithms, as in artificial intelligence, as in computers analyzing massive amounts of data. For example, AI knows that new doesn't always mean more sales. Like, with tax software, sure it does. Everyone wants the latest version. But a new DSLR camera actually creates more demand for the older version, which is cheaper. AI also knows that people often abandon their online grocery cart if bananas are sold out. And think about sunscreen. AI found many surges in the winter and spring. FRESHWATER: Right around the holidays and then again when schools typically have spring breaks. SELYUKH: Both AI and forecasting are not unique to Amazon. All retail stores work hard to plan for the future, and all major ones have their own algorithms and automated warehouses and delivery tricks. But it was Amazon Prime that got Americans hooked on two-day shipping. Now it's a race for a one-hour delivery with Amazon Prime Now. Few companies can afford that, and few rely quite so much on AI to control costs while growing. CEM SIBAY: Really, AI is an underpinner technology for almost the entire Prime experience. SELYUKH: Cem Sibay is an executive at Prime whose promise of free two-day shipping is the main reason millions of Americans shell out $119 a year for membership. Sibay and other Amazon executives push this illusion that fast delivery is magic. Like, the code name for Prime Now was Houdini. But the reality is forecasting on steroids and a meticulous shaving off of minutes and seconds on the journey of the package. SIBAY: A lot of it is sort of end-to-end control of the experience as well. SELYUKH: As in, Amazon has control of the entire process from the website to the warehouses to the actual delivery to your doorstep. In corporate lingo, that's first mile, middle mile and last mile. AI is woven through it all. In the first mile, when you order, AI analyzes your search and tells you upfront how fast your item could ship. In the warehouse - that's the middle mile - AI powers the Kiva robots. They look like large Roombas carrying yellow shelving units. In traditional warehouses, it's the people who walk to the shelves. Here, it's the robots that bring the shelves to people. The machines know what to bring and when to get each order packed in time for delivery. It's AI keeping track of all items in almost a million square feet of this warehouse. AI constantly arranges those shelves so that things you're about to buy are ready to go. BRAD PORTER: There's a mix of industrial automation, manual processes and more sophisticated robotics. SELYUKH: Brad Porter is the head of robotics for Amazon operations. This is controversial work in retail, where layoffs are rampant just as automation reshapes the workforce. Economists are divided on how much exactly AI will eliminate or create jobs, especially for lower-income Americans. In its defense, Amazon often points to how much it's actually been hiring. To Porter, we are in the latest chapter of industrialization. PORTER: Industrial automation and robotics are here. They've been here for a long time. SELYUKH: One area where AI has created a new type of job is in deliveries in the last mile. In busy cities, Amazon has to pull out all the stops. The company took a page from Uber and now hires drivers for side gigs, making superfast deliveries that pay as much as $25 an hour. AI fuels this. It matches package size to car size and even recommends what package to put in last. Sibay says when you're in a one-hour race, every minute counts. So AI's timing estimates consider traffic and building entry codes, and it learns from its mistakes. SIBAY: The driver forgets his key at reception and has to walk a little bit longer. The driver is delivering a package, and it's an elderly lady. And they, you know, talk a little bit. SELYUKH: It's hard for AI to predict all these scenarios, he says. But next time, maybe the address with a chatty Cathy will get a few more minutes baked into the algorithm. Alina Selyukh, NPR News, Seattle. (SOUNDBITE OF DECEPTIKON'S \"INACCESSIBILITY\") RACHEL MARTIN, HOST:  Americans take for granted that most things they buy online will arrive on their doorstep two days after they clicked the order button. When Amazon introduced two-day shipping, it was a huge shift in retail thinking. To stay competitive, most major stores now offer similar speedy delivery. But getting you that package in just two days - or in some cases in just one hour - is extremely expensive. NPR's Alina Selyukh got a rare look at how Amazon makes it work. ALINA SELYUKH, BYLINE: By the time someone clicks buy on Amazon, usually Jenny Freshwater had long predicted it. JENNY FRESHWATER: In advance of them even knowing they want the product, in many cases. SELYUKH: Her team's job is to anticipate demand for every single thing sold by Amazon in the world - not just how many blouses should Amazon sell but what color, size, sleeve length and most importantly. . . FRESHWATER: Where do we actually put the product so that our customers can get it when they click buy? SELYUKH: This is key to how Amazon cuts down delivery time, by stocking items as close as possible to the people who will buy them. That's why Amazon has been building smaller warehouses closer to city centers, where delivery might be within hours. And we should note, Amazon is one of NPR's sponsors. FRESHWATER: We've started to build these AI algorithms, and the more we build, the better they get. SELYUKH: AI algorithms, as in artificial intelligence, as in computers analyzing massive amounts of data. For example, AI knows that new doesn't always mean more sales. Like, with tax software, sure it does. Everyone wants the latest version. But a new DSLR camera actually creates more demand for the older version, which is cheaper. AI also knows that people often abandon their online grocery cart if bananas are sold out. And think about sunscreen. AI found many surges in the winter and spring. FRESHWATER: Right around the holidays and then again when schools typically have spring breaks. SELYUKH: Both AI and forecasting are not unique to Amazon. All retail stores work hard to plan for the future, and all major ones have their own algorithms and automated warehouses and delivery tricks. But it was Amazon Prime that got Americans hooked on two-day shipping. Now it's a race for a one-hour delivery with Amazon Prime Now. Few companies can afford that, and few rely quite so much on AI to control costs while growing. CEM SIBAY: Really, AI is an underpinner technology for almost the entire Prime experience. SELYUKH: Cem Sibay is an executive at Prime whose promise of free two-day shipping is the main reason millions of Americans shell out $119 a year for membership. Sibay and other Amazon executives push this illusion that fast delivery is magic. Like, the code name for Prime Now was Houdini. But the reality is forecasting on steroids and a meticulous shaving off of minutes and seconds on the journey of the package. SIBAY: A lot of it is sort of end-to-end control of the experience as well. SELYUKH: As in, Amazon has control of the entire process from the website to the warehouses to the actual delivery to your doorstep. In corporate lingo, that's first mile, middle mile and last mile. AI is woven through it all. In the first mile, when you order, AI analyzes your search and tells you upfront how fast your item could ship. In the warehouse - that's the middle mile - AI powers the Kiva robots. They look like large Roombas carrying yellow shelving units. In traditional warehouses, it's the people who walk to the shelves. Here, it's the robots that bring the shelves to people. The machines know what to bring and when to get each order packed in time for delivery. It's AI keeping track of all items in almost a million square feet of this warehouse. AI constantly arranges those shelves so that things you're about to buy are ready to go. BRAD PORTER: There's a mix of industrial automation, manual processes and more sophisticated robotics. SELYUKH: Brad Porter is the head of robotics for Amazon operations. This is controversial work in retail, where layoffs are rampant just as automation reshapes the workforce. Economists are divided on how much exactly AI will eliminate or create jobs, especially for lower-income Americans. In its defense, Amazon often points to how much it's actually been hiring. To Porter, we are in the latest chapter of industrialization. PORTER: Industrial automation and robotics are here. They've been here for a long time. SELYUKH: One area where AI has created a new type of job is in deliveries in the last mile. In busy cities, Amazon has to pull out all the stops. The company took a page from Uber and now hires drivers for side gigs, making superfast deliveries that pay as much as $25 an hour. AI fuels this. It matches package size to car size and even recommends what package to put in last. Sibay says when you're in a one-hour race, every minute counts. So AI's timing estimates consider traffic and building entry codes, and it learns from its mistakes. SIBAY: The driver forgets his key at reception and has to walk a little bit longer. The driver is delivering a package, and it's an elderly lady. And they, you know, talk a little bit. SELYUKH: It's hard for AI to predict all these scenarios, he says. But next time, maybe the address with a chatty Cathy will get a few more minutes baked into the algorithm. Alina Selyukh, NPR News, Seattle. (SOUNDBITE OF DECEPTIKON'S \"INACCESSIBILITY\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-22-669591667": {"title": "From Get-Out-To-Vote To Text-Out-To-Vote: The Rise Of Peer-To-Peer Texting : NPR", "url": "https://www.npr.org/2018/11/22/669591667/from-get-out-to-vote-to-text-out-to-vote-the-rise-of-peer-to-peer-texting", "author": "No author found", "published_date": "2018-11-22", "content": "", "section": "Elections", "disclaimer": ""}, "2018-11-23-664364583": {"title": "From Believeland To Blockland \u2014 Cleveland Aims To Be A Tech Hub : NPR", "url": "https://www.npr.org/2018/11/23/664364583/from-believeland-to-blockland-cleveland-aims-to-be-a-tech-hub", "author": "No author found", "published_date": "2018-11-23", "content": "STEVE INSKEEP, HOST:  Cleveland, Ohio, is moving to update its image and its economy. The city known for old-line industries, Lake Erie, LeBron James and the Rock & Roll Hall of Fame wants to make itself a center of blockchain technology, the software behind cryptocurrencies like bitcoin. Here's Jeff St. Clair of member station WKSU. JEFF ST CLAIR, BYLINE: Believeland was Cleveland's catchphrase when, in 2016, LeBron James and the Cavaliers delivered the city's first sports championship in half a century. Now LeBron is gone, and Clevelanders may be looking for something new to believe in. Bernie Moreno thinks that should be embracing Cleveland as a tech town. Moreno is a blockchain evangelist and luxury car dealer who is quick to resist a sports comparison. BERNIE MORENO: I know LeBron James, and I'm no LeBron James (laughter), OK? So let's get that out of the way. ST CLAIR: Moreno is trying to marshal his salesmanship skills to promote Cleveland as a center for blockchain innovation. MORENO: If you're going to have a blockchain startup, this is the place you'd do it. If you're going to invest in blockchain startups, this is the place I would invest in. And if you're a developer, you're a student who wants to do blockchain development, Cleveland's the place where you would do it. ST CLAIR: Blockchain is basically a digital lockbox that stores any kind of transaction in a secure block that's shared across independent computers. It's the technology behind the cryptocurrency Bitcoin but is quickly transitioning to other uses, like digital home deeds, municipal bonds. . . MORENO: Digital car titles. There's digital driver's licenses, birth certificates, college degrees, medical records. ST CLAIR: One Cleveland startup is even testing a blockchain-based voting app. Hilary Carter is managing director of the Blockchain Research Institute in Toronto, a think tank laying out road maps for business uses of the technology. HILARY CARTER: Cleveland stepped up to be the very first city outside of Canada to want access to the thought leadership that we're creating at the BRI. ST CLAIR: Here's what Moreno says it would take to build Blockland. The city would need to attract around 1,000 software developers to the region who will help launch a couple dozen blockchain-based startups. He's planning to open a 100,000-square-foot downtown campus to incubate those companies, complete with a K-12 school. MORENO: We call the school Genesis because in the blockchain world, genesis is the first block in the blockchain. ST CLAIR: Moreno has also put together a dream team of Cleveland's civic leaders to help organize the Blockland Cleveland conference in December. Suzanne Rivera is vice president of research at Case Western Reserve University and likes the ambitious idea. SUZANNE RIVERA: I really think of it as a movement. This is going to be the way of the future. We can either let it pass us by, or we can seize this opportunity. ST CLAIR: But many here struggle with understanding blockchain itself and its potential benefits. WAVERLY WILLIS: It's regular, blue-collar guys that's saying, why do I need to know about this stuff? And so you need a person like me to be able to explain it. ST CLAIR: Waverly Willis is the first barber in Cleveland to accept bitcoin as payment and an early backer of the Blockland project. He says Cleveland needs a plan that goes beyond the city's CEOs and academic leaders to lift people in his neighborhood. Bernie Moreno agrees. MORENO: This whole thing works when the average citizen of Cleveland is dramatically smarter than the average citizen anywhere else relative to blockchain technology. ST CLAIR: From Believeland to Blockland, it's a technology Hail Mary of sorts from a city competing to stake its claim as a blockchain capital. For NPR News, I'm Jeff St. Clair. STEVE INSKEEP, HOST:   Cleveland, Ohio, is moving to update its image and its economy. The city known for old-line industries, Lake Erie, LeBron James and the Rock & Roll Hall of Fame wants to make itself a center of blockchain technology, the software behind cryptocurrencies like bitcoin. Here's Jeff St. Clair of member station WKSU. JEFF ST CLAIR, BYLINE: Believeland was Cleveland's catchphrase when, in 2016, LeBron James and the Cavaliers delivered the city's first sports championship in half a century. Now LeBron is gone, and Clevelanders may be looking for something new to believe in. Bernie Moreno thinks that should be embracing Cleveland as a tech town. Moreno is a blockchain evangelist and luxury car dealer who is quick to resist a sports comparison. BERNIE MORENO: I know LeBron James, and I'm no LeBron James (laughter), OK? So let's get that out of the way. ST CLAIR: Moreno is trying to marshal his salesmanship skills to promote Cleveland as a center for blockchain innovation. MORENO: If you're going to have a blockchain startup, this is the place you'd do it. If you're going to invest in blockchain startups, this is the place I would invest in. And if you're a developer, you're a student who wants to do blockchain development, Cleveland's the place where you would do it. ST CLAIR: Blockchain is basically a digital lockbox that stores any kind of transaction in a secure block that's shared across independent computers. It's the technology behind the cryptocurrency Bitcoin but is quickly transitioning to other uses, like digital home deeds, municipal bonds. . . MORENO: Digital car titles. There's digital driver's licenses, birth certificates, college degrees, medical records. ST CLAIR: One Cleveland startup is even testing a blockchain-based voting app. Hilary Carter is managing director of the Blockchain Research Institute in Toronto, a think tank laying out road maps for business uses of the technology. HILARY CARTER: Cleveland stepped up to be the very first city outside of Canada to want access to the thought leadership that we're creating at the BRI. ST CLAIR: Here's what Moreno says it would take to build Blockland. The city would need to attract around 1,000 software developers to the region who will help launch a couple dozen blockchain-based startups. He's planning to open a 100,000-square-foot downtown campus to incubate those companies, complete with a K-12 school. MORENO: We call the school Genesis because in the blockchain world, genesis is the first block in the blockchain. ST CLAIR: Moreno has also put together a dream team of Cleveland's civic leaders to help organize the Blockland Cleveland conference in December. Suzanne Rivera is vice president of research at Case Western Reserve University and likes the ambitious idea. SUZANNE RIVERA: I really think of it as a movement. This is going to be the way of the future. We can either let it pass us by, or we can seize this opportunity. ST CLAIR: But many here struggle with understanding blockchain itself and its potential benefits. WAVERLY WILLIS: It's regular, blue-collar guys that's saying, why do I need to know about this stuff? And so you need a person like me to be able to explain it. ST CLAIR: Waverly Willis is the first barber in Cleveland to accept bitcoin as payment and an early backer of the Blockland project. He says Cleveland needs a plan that goes beyond the city's CEOs and academic leaders to lift people in his neighborhood. Bernie Moreno agrees. MORENO: This whole thing works when the average citizen of Cleveland is dramatically smarter than the average citizen anywhere else relative to blockchain technology. ST CLAIR: From Believeland to Blockland, it's a technology Hail Mary of sorts from a city competing to stake its claim as a blockchain capital. For NPR News, I'm Jeff St. Clair.", "section": "National", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-27-671223173": {"title": "Google Tweaks Email Program That Assumed An Investor Was Male  : NPR", "url": "https://www.npr.org/2018/11/27/671223173/google-tweaks-email-program-that-assumed-an-investor-was-male", "author": "No author found", "published_date": "2018-11-27", "content": "", "section": "Technology", "disclaimer": ""}, "2018-11-27-671285261": {"title": "Critics Say YouTube Hasn't Done Enough To Crack Down On Extremist Content : NPR", "url": "https://www.npr.org/2018/11/27/671285261/critics-say-youtube-hasnt-done-enough-to-crack-down-on-extremist-content", "author": "No author found", "published_date": "2018-11-27", "content": "MARY LOUISE KELLY, HOST: And time now for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")KELLY: All this month, we've been reporting on toxic content - what it is, what's to be done about it, both questions that YouTube has thought long and hard about. AUDIE CORNISH, HOST: For years, the YouTube videos of radical Muslim cleric Anwar al-Awlaki inspired terrorists like the Fort Hood gunman and the Boston Marathon bombers. Last year, YouTube pulled down its propaganda videos. It's been trying to reassure people that it's addressing the problem of extremism on its website. KELLY: But critics say YouTube, which is owned by Google, has not done nearly enough to prevent extremist videos, such as jihadist or white nationalist propaganda, from being hosted on the platform. NPR's Tim Mak has more. TIM MAK, BYLINE: Viewers worldwide watch more than a billion hours of YouTube a day. In the midst of all these videos, the platform has struggled to keep extremism out. What's more, it has also struggled against its own systems that suggest to people what they might like to watch. REBECCA LEWIS: YouTube's algorithms themselves can sometimes lead people down these rabbit holes. MAK: That's Rebecca Lewis, a researcher for the Data & Society institute. LEWIS: YouTube as a platform has an incentive to keep viewers on its website. So if you watch a video that is criticizing feminism, then the next video that YouTube may suggest will be one step a little bit more intense than that. So maybe it'll be a little bit more overtly sexist or a little bit more overtly racist. MAK: YouTube has resisted public scrutiny of these problems. The company declined a request from NPR to be interviewed on tape. Google also declined to send a high-level executive to testify before a Senate Intelligence Committee hearing earlier this year, drawing the ire of the panel's members. Facebook and Twitter sent its COO and CEO, respectively. MARK WARNER: My frustration level with Google grows virtually every week. MAK: That's Senator Mark Warner, the top Democrat on the Senate Intelligence Committee. WARNER: Increasingly, researchers are saying that some of the most disruptive behavior in terms of radicalizing and incenting violence and incenting hate behavior on both the left and the right is actually used with the YouTube platform. MAK: Marc Ginsberg, an adviser to the Counter Extremism Project, a group dedicated to confronting extremist messaging online, was highly critical of YouTube for not doing enough to remove extremist videos. MARC GINSBERG: YouTube's management is insensitive, full of hubris, unwilling to be held responsible to the public for its failures to adopt necessary policies and procedures to remove extremist content. MAK: In a statement, YouTube provided a list of steps that it has taken to address the issue and said it takes, quote, \"swift action against terrorism content and content that incites violence. \"It said that YouTube had expanded the use of automated machine learning techniques to remove violent extremist content. It also hired 10,000 people to address content that violates its terms. GINSBERG: We've watched people who've been hired or flagged part-time as consultants, and their ability to stay online and to watch this content burns them out. MAK: That's Ginsberg again. Why are people being so burnt out? GINSBERG: Because they've seen content that is horrific. MAK: YouTube also says that it has implemented what it calls the redirect method on its platform so that when people search for particularly sensitive keywords, they will be redirected to content that debunks violent extremist messages. RENEE DIRESTA: They've begun to use Wikipedia, like linking out to anti-conspiracy theory content. That's a very naive approach. MAK: That's Renee DiResta, a researcher for New Knowledge who investigates the spread of narratives across social networks. DIRESTA: The best defense is actually preventing people from going down these radicalization pathways in the first place. But that requires the platforms to take a paternalistic approach to what they show. That makes a lot of people uncomfortable. MAK: The question she asks is, are we comfortable with a more aggressive approach where Google becomes the content police? Tim Mak, NPR News, Washington. (SOUNDBITE OF MALA RODRIGUEZ'S \"LA NINA\") MARY LOUISE KELLY, HOST:  And time now for All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") KELLY: All this month, we've been reporting on toxic content - what it is, what's to be done about it, both questions that YouTube has thought long and hard about. AUDIE CORNISH, HOST:  For years, the YouTube videos of radical Muslim cleric Anwar al-Awlaki inspired terrorists like the Fort Hood gunman and the Boston Marathon bombers. Last year, YouTube pulled down its propaganda videos. It's been trying to reassure people that it's addressing the problem of extremism on its website. KELLY: But critics say YouTube, which is owned by Google, has not done nearly enough to prevent extremist videos, such as jihadist or white nationalist propaganda, from being hosted on the platform. NPR's Tim Mak has more. TIM MAK, BYLINE: Viewers worldwide watch more than a billion hours of YouTube a day. In the midst of all these videos, the platform has struggled to keep extremism out. What's more, it has also struggled against its own systems that suggest to people what they might like to watch. REBECCA LEWIS: YouTube's algorithms themselves can sometimes lead people down these rabbit holes. MAK: That's Rebecca Lewis, a researcher for the Data & Society institute. LEWIS: YouTube as a platform has an incentive to keep viewers on its website. So if you watch a video that is criticizing feminism, then the next video that YouTube may suggest will be one step a little bit more intense than that. So maybe it'll be a little bit more overtly sexist or a little bit more overtly racist. MAK: YouTube has resisted public scrutiny of these problems. The company declined a request from NPR to be interviewed on tape. Google also declined to send a high-level executive to testify before a Senate Intelligence Committee hearing earlier this year, drawing the ire of the panel's members. Facebook and Twitter sent its COO and CEO, respectively. MARK WARNER: My frustration level with Google grows virtually every week. MAK: That's Senator Mark Warner, the top Democrat on the Senate Intelligence Committee. WARNER: Increasingly, researchers are saying that some of the most disruptive behavior in terms of radicalizing and incenting violence and incenting hate behavior on both the left and the right is actually used with the YouTube platform. MAK: Marc Ginsberg, an adviser to the Counter Extremism Project, a group dedicated to confronting extremist messaging online, was highly critical of YouTube for not doing enough to remove extremist videos. MARC GINSBERG: YouTube's management is insensitive, full of hubris, unwilling to be held responsible to the public for its failures to adopt necessary policies and procedures to remove extremist content. MAK: In a statement, YouTube provided a list of steps that it has taken to address the issue and said it takes, quote, \"swift action against terrorism content and content that incites violence. \" It said that YouTube had expanded the use of automated machine learning techniques to remove violent extremist content. It also hired 10,000 people to address content that violates its terms. GINSBERG: We've watched people who've been hired or flagged part-time as consultants, and their ability to stay online and to watch this content burns them out. MAK: That's Ginsberg again. Why are people being so burnt out? GINSBERG: Because they've seen content that is horrific. MAK: YouTube also says that it has implemented what it calls the redirect method on its platform so that when people search for particularly sensitive keywords, they will be redirected to content that debunks violent extremist messages. RENEE DIRESTA: They've begun to use Wikipedia, like linking out to anti-conspiracy theory content. That's a very naive approach. MAK: That's Renee DiResta, a researcher for New Knowledge who investigates the spread of narratives across social networks. DIRESTA: The best defense is actually preventing people from going down these radicalization pathways in the first place. But that requires the platforms to take a paternalistic approach to what they show. That makes a lot of people uncomfortable. MAK: The question she asks is, are we comfortable with a more aggressive approach where Google becomes the content police? Tim Mak, NPR News, Washington. (SOUNDBITE OF MALA RODRIGUEZ'S \"LA NINA\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-27-671285216": {"title": "Google Employees Join Others In Asking The Search Engine To Stay Out Of China : NPR", "url": "https://www.npr.org/2018/11/27/671285216/google-employees-join-others-in-asking-the-search-engine-to-stay-out-of-china", "author": "No author found", "published_date": "2018-11-27", "content": "AUDIE CORNISH, HOST: Dozens of Google employees are speaking out against the company's plan to build a special search engine for China. The employees have joined with Amnesty International, urging Google to cancel the project. The company's plan calls for a search engine that would comply with China's policy of online censorship, often known as the Great Firewall. NPR's Jasmine Garsd reports on the controversial project. JASMINE GARSD, BYLINE: The Chinese blogger known as Super Vulgar Butcher used to post about the government's abuses of power. His blog became quite popular. These days, he's simply known as the inmate Wu Gan, serving eight years for expressing dissent. Sarah Cook, a researcher with Freedom House, a nonprofit for the expansion of democracy, says this is common. SARAH COOK: So the Chinese government has actually managed to develop the most sophisticated and multi-layered apparatus of Internet censorship and surveillance anywhere in the world. GARSD: It's the kind of censorship that ultimately led Google to pull out of China in 2010. But the company has been working on a way to get back. After all, there are currently over 700 million Internet users in China. Because of its large population, China is an extremely attractive market, one that's relatively closed off to American tech companies. COOK: Pretty much any major website or media, social media service that you think of - YouTube, Twitter, Facebook - all of those are blocked. The Chinese government uses the Great Firewall to block Chinese users' access to those. GARSD: Project Dragonfly would be Google but censored for China. That means if users in China search for words like Tiananmen Square, repression or human rights, they'll only find government-approved information. Also, users' search records would be accessible to the government. Joe Westby researches technology and human rights at Amnesty International, which has asked Google to kill Project Dragonfly. JOE WESTBY: If Google breaks that trust by sharing that it's willing to compromise its principles in order to gain access to the Chinese market and effectively for profit, how can we be sure that it won't do the same in other countries and with other governments? GARSD: Google told NPR that their work has been, quote, \"exploratory, and we are not close to launching a search product in China. \"But in a letter published today, a group of Google workers said they stand with Amnesty International. The letters read, quote, \"we refuse to build technologies that aid the powerful in oppressing the vulnerable. \" It's part of an increasing activist tide across big tech. Earlier this year, several Google workers quit in protest of Project Maven, which provided artificial intelligence to the Pentagon for drone facial recognition. The outcry was so loud, Google backed down. Jasmine Garsd, NPR News, New York. (SOUNDBITE OF EMEFE SONG, \"STUTTER\") AUDIE CORNISH, HOST:  Dozens of Google employees are speaking out against the company's plan to build a special search engine for China. The employees have joined with Amnesty International, urging Google to cancel the project. The company's plan calls for a search engine that would comply with China's policy of online censorship, often known as the Great Firewall. NPR's Jasmine Garsd reports on the controversial project. JASMINE GARSD, BYLINE: The Chinese blogger known as Super Vulgar Butcher used to post about the government's abuses of power. His blog became quite popular. These days, he's simply known as the inmate Wu Gan, serving eight years for expressing dissent. Sarah Cook, a researcher with Freedom House, a nonprofit for the expansion of democracy, says this is common. SARAH COOK: So the Chinese government has actually managed to develop the most sophisticated and multi-layered apparatus of Internet censorship and surveillance anywhere in the world. GARSD: It's the kind of censorship that ultimately led Google to pull out of China in 2010. But the company has been working on a way to get back. After all, there are currently over 700 million Internet users in China. Because of its large population, China is an extremely attractive market, one that's relatively closed off to American tech companies. COOK: Pretty much any major website or media, social media service that you think of - YouTube, Twitter, Facebook - all of those are blocked. The Chinese government uses the Great Firewall to block Chinese users' access to those. GARSD: Project Dragonfly would be Google but censored for China. That means if users in China search for words like Tiananmen Square, repression or human rights, they'll only find government-approved information. Also, users' search records would be accessible to the government. Joe Westby researches technology and human rights at Amnesty International, which has asked Google to kill Project Dragonfly. JOE WESTBY: If Google breaks that trust by sharing that it's willing to compromise its principles in order to gain access to the Chinese market and effectively for profit, how can we be sure that it won't do the same in other countries and with other governments? GARSD: Google told NPR that their work has been, quote, \"exploratory, and we are not close to launching a search product in China. \" But in a letter published today, a group of Google workers said they stand with Amnesty International. The letters read, quote, \"we refuse to build technologies that aid the powerful in oppressing the vulnerable. \" It's part of an increasing activist tide across big tech. Earlier this year, several Google workers quit in protest of Project Maven, which provided artificial intelligence to the Pentagon for drone facial recognition. The outcry was so loud, Google backed down. Jasmine Garsd, NPR News, New York. (SOUNDBITE OF EMEFE SONG, \"STUTTER\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-27-671123903": {"title": "'We're Taking A Stand': Google Workers Protest Plans For Censored Search In China : NPR", "url": "https://www.npr.org/2018/11/27/671123903/we-re-taking-a-stand-google-workers-protest-plans-for-censored-search-in-china", "author": "No author found", "published_date": "2018-11-27", "content": "", "section": "Technology", "disclaimer": ""}, "2018-11-28-671133977": {"title": "Bitcoin Is Bouncing Around Again. Here Are Some Possible Causes : NPR", "url": "https://www.npr.org/2018/11/28/671133977/bitcoin-is-bouncing-around-again-here-are-some-possible-causes", "author": "No author found", "published_date": "2018-11-28", "content": "", "section": "Economy", "disclaimer": ""}, "2018-11-30-672342419": {"title": "Data Of Some 500 Million Marriott Customers Stolen In Breach : NPR", "url": "https://www.npr.org/2018/11/30/672342419/data-of-some-500-million-marriott-customers-stolen-in-breach", "author": "No author found", "published_date": "2018-11-30", "content": "AUDIE CORNISH, HOST: Marriott said today that as many as 500 million hotel guests have had their data stolen. We're talking about people who made reservations at a Starwood property since 2014. What makes this data breach stand out is the sensitive personal data that was scooped up from passport numbers to lengths of stays. Now several state attorneys general, including Illinois and New York, are investigating. NPR's Alina Selyukh has more. ALINA SELYUKH, BYLINE: Outside of the upscale St. Regis Hotel in downtown Washington, the day seemed to be business as usual. (SOUNDBITE OF WHISTLE BLOWING)SELYUKH: But for the parent company of St. Regis, this was far from a regular day. Marriott International says its Starwood hotel reservation system was compromised by hackers for at least four years. The breach affects hotels such as Sheraton, Westin, W, Aloft and St. Regis. MATT TAIT: This particular breach is enormous. SELYUKH: Matt Tait is a senior cybersecurity fellow at the University of Texas at Austin. He points out the sheer breadth of data compromised in this breach. Marriott says for 327 million customers the stolen data might include names, email and mailing addresses, dates of birth, but also passport information and in some cases credit card numbers along with expiration dates. Those details were encrypted, but Marriott says it cannot rule out the possibility that the hackers also stole the keys to decrypt them. TAIT: A lot of people - they're going to be very confused why Marriott stored this sort of volume of data. SELYUKH: Tait says he is actually a rewards member with Marriott himself. And he was surprised to find out that the hotel chain was storing sensitive details such as passport numbers. And this is why to him this hack is a big deal, not just because of the number of people affected but the quality of the stolen data. TAIT: This is going to make it very easy for these hackers who have taken this information to potentially commit identity fraud. SELYUKH: And yet at both the St. Regis and Westin in downtown D. C. , most hotel guests were unsurprised by the news and even resigned. ANDRIA MCCLELLAN: It's our new reality. And it's unfortunate. And it's - you know, we know what to do. JIM MULLIGAN: If you're accessing the Internet, your expectations for security are pretty low at this point. LANCE HOLMAN: I think that's just the new normal. SELYUKH: That doesn't change your attitude toward the company? HOLMAN: No, it doesn't really. SELYUKH: That's Andria McClellan from Virginia, Jim Mulligan from Chicago and Lance Holman from California. All the guests I interviewed said they will now pay extra attention to their credit card charges and change passwords. But they said they would keep staying at Marriott hotels. HOLMAN: They own everything. You can't get away from them. MULLIGAN: I'm married to them at this point. Yeah (laughter). SELYUKH: But to cybersecurity and intelligence experts, there's far more at stake with this scale of breach. MICHAEL DALY: We need people to understand that there is a bigger picture. SELYUKH: Michael Daly is chief technology officer for cybersecurity at the defense company Raytheon. DALY: Sort of like the boiling-the-frog problem, every time we have a breach and the numbers get larger and larger, then the world doesn't collapse, we all move the bar a little higher and say, well, I guess that wasn't as bad. SELYUKH: But he says this isn't a matter of one person's travel schedule, money or even identity. He's worried about the value of all this information for spies, foreign adversaries. Think travel patterns of politicians, potential major business meetings. All that in one massive data breach on top of all the other breaches - he says that is a matter of national security. Alina Selyukh, NPR News. AUDIE CORNISH, HOST:  Marriott said today that as many as 500 million hotel guests have had their data stolen. We're talking about people who made reservations at a Starwood property since 2014. What makes this data breach stand out is the sensitive personal data that was scooped up from passport numbers to lengths of stays. Now several state attorneys general, including Illinois and New York, are investigating. NPR's Alina Selyukh has more. ALINA SELYUKH, BYLINE: Outside of the upscale St. Regis Hotel in downtown Washington, the day seemed to be business as usual. (SOUNDBITE OF WHISTLE BLOWING) SELYUKH: But for the parent company of St. Regis, this was far from a regular day. Marriott International says its Starwood hotel reservation system was compromised by hackers for at least four years. The breach affects hotels such as Sheraton, Westin, W, Aloft and St. Regis. MATT TAIT: This particular breach is enormous. SELYUKH: Matt Tait is a senior cybersecurity fellow at the University of Texas at Austin. He points out the sheer breadth of data compromised in this breach. Marriott says for 327 million customers the stolen data might include names, email and mailing addresses, dates of birth, but also passport information and in some cases credit card numbers along with expiration dates. Those details were encrypted, but Marriott says it cannot rule out the possibility that the hackers also stole the keys to decrypt them. TAIT: A lot of people - they're going to be very confused why Marriott stored this sort of volume of data. SELYUKH: Tait says he is actually a rewards member with Marriott himself. And he was surprised to find out that the hotel chain was storing sensitive details such as passport numbers. And this is why to him this hack is a big deal, not just because of the number of people affected but the quality of the stolen data. TAIT: This is going to make it very easy for these hackers who have taken this information to potentially commit identity fraud. SELYUKH: And yet at both the St. Regis and Westin in downtown D. C. , most hotel guests were unsurprised by the news and even resigned. ANDRIA MCCLELLAN: It's our new reality. And it's unfortunate. And it's - you know, we know what to do. JIM MULLIGAN: If you're accessing the Internet, your expectations for security are pretty low at this point. LANCE HOLMAN: I think that's just the new normal. SELYUKH: That doesn't change your attitude toward the company? HOLMAN: No, it doesn't really. SELYUKH: That's Andria McClellan from Virginia, Jim Mulligan from Chicago and Lance Holman from California. All the guests I interviewed said they will now pay extra attention to their credit card charges and change passwords. But they said they would keep staying at Marriott hotels. HOLMAN: They own everything. You can't get away from them. MULLIGAN: I'm married to them at this point. Yeah (laughter). SELYUKH: But to cybersecurity and intelligence experts, there's far more at stake with this scale of breach. MICHAEL DALY: We need people to understand that there is a bigger picture. SELYUKH: Michael Daly is chief technology officer for cybersecurity at the defense company Raytheon. DALY: Sort of like the boiling-the-frog problem, every time we have a breach and the numbers get larger and larger, then the world doesn't collapse, we all move the bar a little higher and say, well, I guess that wasn't as bad. SELYUKH: But he says this isn't a matter of one person's travel schedule, money or even identity. He's worried about the value of all this information for spies, foreign adversaries. Think travel patterns of politicians, potential major business meetings. All that in one massive data breach on top of all the other breaches - he says that is a matter of national security. Alina Selyukh, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-11-30-672167870": {"title": "Marriott Says Up To 500 Million Customers' Data Stolen In Breach : NPR", "url": "https://www.npr.org/2018/11/30/672167870/marriott-says-up-to-500-million-customers-data-stolen-in-breach", "author": "No author found", "published_date": "2018-11-30", "content": "", "section": "Technology", "disclaimer": ""}, "2018-12-01-672511667": {"title": "A Tech Foundation Discusses Security And Privacy When It Comes To Smart Devices : NPR", "url": "https://www.npr.org/2018/12/01/672511667/a-tech-foundation-discusses-security-and-privacy-when-it-comes-to-smart-devices", "author": "No author found", "published_date": "2018-12-01", "content": "SCOTT SIMON, HOST:  Is your smart device keeping tabs on you? Are Siri or Alexa sharing information you'd rather they not? One tech foundation wants people to think about security and privacy as much as performance or price when it comes to smart devices, from the Amazon Dot to water bottles. The Mozilla Foundation has what amounts to a naughty list of gifts - the privacy not included guide. Ashley Boyd is vice president for advocacy for the Mozilla Foundation. She joins us from Berkeley, Calif. Thanks so much for being with us. ASHLEY BOYD: Thanks so much for having me. SIMON: Why the need for this guide? BOYD: We know that people are going to be asking themselves is the gift that I'm giving or asking for going to be secure and private? And as you said, most holiday shopping guides are focused on price and performance, and this guide is focused on privacy and security. SIMON: It's not just smart speakers, is it? It's a lot of devices that we've quite willingly and comfortably made part of our everyday lives. BOYD: That's absolutely right. And we're particularly looking at the area of connected-to gifts because more and more consumer products are connected to the Internet. So one interesting space that we've looked at is this new emerging toy category of drones. I have an 11-year-old who's pretty excited about drones, and it turns out that many of them connect to insecure wireless and therefore can be taken over, both the information they collect but also the device itself. So we think that consumers would want to know information like that when they're considering a drone gift. SIMON: I mean, forgive me, but that's a movie plot, I mean, the 11-year-old who has a drone that's taken over by some nefarious power who - you can fill in that blank. BOYD: Indeed. It may be - provide an experience that's more exciting than the child or the parent is counting on. SIMON: Water bottles? BOYD: Water bottles - so the design here is a good intention, which is to actually help people drink more water by connecting with their friends and almost to have a bit of a competition about how much water they're drinking. However, it does actually connect location, and you might not want to provide all the details to your network about where you're drinking water and when. So it's not just an issue of being creepy. There's a real safety concern that we're wanting to address and make folks aware of. SIMON: Don't we all, at some point, hit a little check box agreeing to have this privacy disclosure that - and we agree to have this information made available? BOYD: Yes. So the privacy policy is where most of this information is really laid out for consumers. But most anyone who's interacted with a privacy policy knows that they're lengthy, they're filled with legalese, and many of them require even a graduate degree reading level to go through them. So this is a bit of where this relationship between the companies and consumers really gets off on the wrong foot. We don't think you should have to have a college degree in order to read a privacy policy. And wouldn't it be great if we had something like a nutrition label? You pick up a box for a product, you look at about it online and you see almost in a quick scan what information it collects and what security features it has. We think that that's possible, and we think companies can do better. SIMON: People care about this? BOYD: I think increasingly people are caring about it. This year, of course, we saw Cambridge Analytica - what a good example of people doing one thing online and it turning into something entirely different. So we're seeing people change their behavior online and wanting more information. And that's really why we provided and created this guide. We think that the information that consumers have isn't enough to match their interest on this issue. SIMON: Ashley Boyd with the Mozilla Foundation, thanks so much for being with us. BOYD: Thanks for having me. SCOTT SIMON, HOST:   Is your smart device keeping tabs on you? Are Siri or Alexa sharing information you'd rather they not? One tech foundation wants people to think about security and privacy as much as performance or price when it comes to smart devices, from the Amazon Dot to water bottles. The Mozilla Foundation has what amounts to a naughty list of gifts - the privacy not included guide. Ashley Boyd is vice president for advocacy for the Mozilla Foundation. She joins us from Berkeley, Calif. Thanks so much for being with us. ASHLEY BOYD: Thanks so much for having me. SIMON: Why the need for this guide? BOYD: We know that people are going to be asking themselves is the gift that I'm giving or asking for going to be secure and private? And as you said, most holiday shopping guides are focused on price and performance, and this guide is focused on privacy and security. SIMON: It's not just smart speakers, is it? It's a lot of devices that we've quite willingly and comfortably made part of our everyday lives. BOYD: That's absolutely right. And we're particularly looking at the area of connected-to gifts because more and more consumer products are connected to the Internet. So one interesting space that we've looked at is this new emerging toy category of drones. I have an 11-year-old who's pretty excited about drones, and it turns out that many of them connect to insecure wireless and therefore can be taken over, both the information they collect but also the device itself. So we think that consumers would want to know information like that when they're considering a drone gift. SIMON: I mean, forgive me, but that's a movie plot, I mean, the 11-year-old who has a drone that's taken over by some nefarious power who - you can fill in that blank. BOYD: Indeed. It may be - provide an experience that's more exciting than the child or the parent is counting on. SIMON: Water bottles? BOYD: Water bottles - so the design here is a good intention, which is to actually help people drink more water by connecting with their friends and almost to have a bit of a competition about how much water they're drinking. However, it does actually connect location, and you might not want to provide all the details to your network about where you're drinking water and when. So it's not just an issue of being creepy. There's a real safety concern that we're wanting to address and make folks aware of. SIMON: Don't we all, at some point, hit a little check box agreeing to have this privacy disclosure that - and we agree to have this information made available? BOYD: Yes. So the privacy policy is where most of this information is really laid out for consumers. But most anyone who's interacted with a privacy policy knows that they're lengthy, they're filled with legalese, and many of them require even a graduate degree reading level to go through them. So this is a bit of where this relationship between the companies and consumers really gets off on the wrong foot. We don't think you should have to have a college degree in order to read a privacy policy. And wouldn't it be great if we had something like a nutrition label? You pick up a box for a product, you look at about it online and you see almost in a quick scan what information it collects and what security features it has. We think that that's possible, and we think companies can do better. SIMON: People care about this? BOYD: I think increasingly people are caring about it. This year, of course, we saw Cambridge Analytica - what a good example of people doing one thing online and it turning into something entirely different. So we're seeing people change their behavior online and wanting more information. And that's really why we provided and created this guide. We think that the information that consumers have isn't enough to match their interest on this issue. SIMON: Ashley Boyd with the Mozilla Foundation, thanks so much for being with us. BOYD: Thanks for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-02-672339610": {"title": "Is Your Holiday Gift Spying On You? A Guide Rates The Security Of Smart Devices : NPR", "url": "https://www.npr.org/2018/12/02/672339610/is-your-holiday-gift-spying-on-you-a-guide-rates-the-security-of-smart-devices", "author": "No author found", "published_date": "2018-12-02", "content": "", "section": "Technology", "disclaimer": ""}, "2018-12-03-673105423": {"title": "Khashoggi Friend Accuses Cyber Security Firm Of Helping Saudis Spy On Their Messages : NPR", "url": "https://www.npr.org/2018/12/03/673105423/khashoggi-friend-accuses-cyber-security-firm-of-helping-saudis-spy-on-their-mess", "author": "No author found", "published_date": "2018-12-03", "content": "", "section": "Middle East", "disclaimer": ""}, "2018-12-03-673022546": {"title": "A Look At Facebook COO Sheryl Sandberg's Trajectory In 2018 : NPR", "url": "https://www.npr.org/2018/12/03/673022546/a-look-a-facebook-coo-sheryl-sandbergs-trajectory-in-2018", "author": "No author found", "published_date": "2018-12-03", "content": "AILSA CHANG, HOST:  This month as we get ready to say goodbye to 2018, we're taking stock of the year in tech on All Tech Considered. (SOUNDBITE OF MUSIC)CHANG: Today, how the year went for one person in particular. MARY LOUISE KELLY, HOST:  Sheryl Sandberg, chief operating officer at Facebook, feminist icon in a male-dominated industry - and Sandberg cultivated that image through her bestseller \"Lean In. \"(SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #1: Well, they say you can't have it all. But Sheryl Sandberg comes awfully close. KELLY: Lately though, she has been at the center of some of Facebook's worst moments - the Cambridge-Analytica scandal, the effort to dig up dirt on those who criticized how Facebook handled fake news in the 2016 presidential election. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #2: A front-page investigation this morning by The New York Times dives into the compounding crises that plague Facebook over the last year, involving Russian interference and privacy breaches. KELLY: Joining us to take stock of Sheryl Sandberg's 2018 is Kara Swisher of Recode. Kara Swisher, welcome. KARA SWISHER, BYLINE: Thanks a lot. KELLY: So you broke the story ten years ago when Mark Zuckerberg lured Sandberg away from Google and over to Facebook. SWISHER: Yeah. KELLY: Yeah, I mean, remind us what her reputation was then - what she was seen as bringing. SWISHER: She wasn't very well-known then. I mean, she was one of the many really strong Google executives. She ran one of its advertising businesses. And it was a big coup for Facebook, which had struggled with executives. The thought was that she was the adult supervision. That's what they called her at the time. KELLY: The grown-up in the room for the dude in the sweatshirt and all the people he'd brought with him. SWISHER: Yeah, you know, because they had been running it - they had mistake after mistake. And so here was someone who was going to bring some level of discipline into the operations of the company. And she's since then done a great job - until just recently. KELLY: Right. I mean, she did help build Facebook into a multibillion-dollar company. SWISHER: Yeah, especially around advertising which was her area of expertise. KELLY: So fast-forward to this year when she has taken much blame for being blind to how Facebook was being used in the 2016 presidential campaign - and then more recently for questionable measures to try to salvage the company's image. I mean, how much of the blame does Sheryl Sandberg deserve - do you think? SWISHER: Well, let's be clear. She deserves a lot of the blame. She's the COO, and she's in charge of vast parts of that company. But to me, a lot of focus should be also be on the CEO Mark Zuckerberg. And it's not because, one, they see him as sort of this feckless geek that just can't, like, speak or something, which is not true. And he has most of the power in the company. He controls the company through his stock. And so he's unfireable essentially, and so the focus is on her. But you also need to look at the whole package of executives there, including the CTO. There's a head of platform. There's all kinds of people - all of whom are men, by the way. It just goes on and on because it's such an interconnected company - that a lot of this is a disaster larger than one person I think. KELLY: You said Mark Zuckerberg is unfireable. Is she? Is her job safe? SWISHER: She is not unfireable at all. KELLY: I don't mean like legally in her contract. But I just mean can you imagine that happening. SWISHER: I'd be surprised given - it's not something Mark tends to do. He tends to stick by his executives. And it would not be a good look at Facebook to do that. At the same time, you know, you have today Michelle - or yesterday, Michelle Obama sort of insulted \"Lean In. \" So it's - and then another article in Vanity Fair compared her to Jeff Skilling of Enron, which is kind of an astonishing comparison. I think she's got two problems. It's hard for her to leave now under this cloud, and it's hard for her to stay. KELLY: You know, I have to ask. You're referring to Mark Zuckerberg as Mark. You're on a first name basis with him. I know you talk to these people all the time at Facebook. Do you have a sense of how Sheryl Sandberg thinks this year has gone? SWISHER: I think she thinks it's gone badly. I think she knows that they made some missteps. They're very aware that they're very much under the gun and that every day something more is going to come out based on past behavior that doesn't make them look very good. And so recovering from this is going to be hard. But, you know, Microsoft did, and now Microsoft's one of the most valuable companies in the world. And if you recall, they had a pretty hard time more than a decade ago - like 20 years ago. KELLY: Do you have a sense that Sandberg has a plan for how to make 2019 a better year than 2018 has been? SWISHER: Well, it couldn't be worse, right? Maybe it could. I don't know. I think what they're doing - the things they've done recently to try to improve their systems, which were incredibly sloppy, I think are better. And I think they've met - they really do - they're really leaning into - I hate to use that term - fixing this problem. At the same time, people have had it. And they've become an iconic company for all that's wrong with tech. And it's going to be difficult to shake that without enormous effort by Sandberg and by Zuckerberg and by the entire team of managers there. KELLY: Thank you, Kara. SWISHER: You're welcome. KELLY: That's Kara Swisher. She is editor at large of Recode and a contributor to The New York Times opinion page. AILSA CHANG, HOST:   This month as we get ready to say goodbye to 2018, we're taking stock of the year in tech on All Tech Considered. (SOUNDBITE OF MUSIC) CHANG: Today, how the year went for one person in particular. MARY LOUISE KELLY, HOST:   Sheryl Sandberg, chief operating officer at Facebook, feminist icon in a male-dominated industry - and Sandberg cultivated that image through her bestseller \"Lean In. \" (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #1: Well, they say you can't have it all. But Sheryl Sandberg comes awfully close. KELLY: Lately though, she has been at the center of some of Facebook's worst moments - the Cambridge-Analytica scandal, the effort to dig up dirt on those who criticized how Facebook handled fake news in the 2016 presidential election. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #2: A front-page investigation this morning by The New York Times dives into the compounding crises that plague Facebook over the last year, involving Russian interference and privacy breaches. KELLY: Joining us to take stock of Sheryl Sandberg's 2018 is Kara Swisher of Recode. Kara Swisher, welcome. KARA SWISHER, BYLINE: Thanks a lot. KELLY: So you broke the story ten years ago when Mark Zuckerberg lured Sandberg away from Google and over to Facebook. SWISHER: Yeah. KELLY: Yeah, I mean, remind us what her reputation was then - what she was seen as bringing. SWISHER: She wasn't very well-known then. I mean, she was one of the many really strong Google executives. She ran one of its advertising businesses. And it was a big coup for Facebook, which had struggled with executives. The thought was that she was the adult supervision. That's what they called her at the time. KELLY: The grown-up in the room for the dude in the sweatshirt and all the people he'd brought with him. SWISHER: Yeah, you know, because they had been running it - they had mistake after mistake. And so here was someone who was going to bring some level of discipline into the operations of the company. And she's since then done a great job - until just recently. KELLY: Right. I mean, she did help build Facebook into a multibillion-dollar company. SWISHER: Yeah, especially around advertising which was her area of expertise. KELLY: So fast-forward to this year when she has taken much blame for being blind to how Facebook was being used in the 2016 presidential campaign - and then more recently for questionable measures to try to salvage the company's image. I mean, how much of the blame does Sheryl Sandberg deserve - do you think? SWISHER: Well, let's be clear. She deserves a lot of the blame. She's the COO, and she's in charge of vast parts of that company. But to me, a lot of focus should be also be on the CEO Mark Zuckerberg. And it's not because, one, they see him as sort of this feckless geek that just can't, like, speak or something, which is not true. And he has most of the power in the company. He controls the company through his stock. And so he's unfireable essentially, and so the focus is on her. But you also need to look at the whole package of executives there, including the CTO. There's a head of platform. There's all kinds of people - all of whom are men, by the way. It just goes on and on because it's such an interconnected company - that a lot of this is a disaster larger than one person I think. KELLY: You said Mark Zuckerberg is unfireable. Is she? Is her job safe? SWISHER: She is not unfireable at all. KELLY: I don't mean like legally in her contract. But I just mean can you imagine that happening. SWISHER: I'd be surprised given - it's not something Mark tends to do. He tends to stick by his executives. And it would not be a good look at Facebook to do that. At the same time, you know, you have today Michelle - or yesterday, Michelle Obama sort of insulted \"Lean In. \" So it's - and then another article in Vanity Fair compared her to Jeff Skilling of Enron, which is kind of an astonishing comparison. I think she's got two problems. It's hard for her to leave now under this cloud, and it's hard for her to stay. KELLY: You know, I have to ask. You're referring to Mark Zuckerberg as Mark. You're on a first name basis with him. I know you talk to these people all the time at Facebook. Do you have a sense of how Sheryl Sandberg thinks this year has gone? SWISHER: I think she thinks it's gone badly. I think she knows that they made some missteps. They're very aware that they're very much under the gun and that every day something more is going to come out based on past behavior that doesn't make them look very good. And so recovering from this is going to be hard. But, you know, Microsoft did, and now Microsoft's one of the most valuable companies in the world. And if you recall, they had a pretty hard time more than a decade ago - like 20 years ago. KELLY: Do you have a sense that Sandberg has a plan for how to make 2019 a better year than 2018 has been? SWISHER: Well, it couldn't be worse, right? Maybe it could. I don't know. I think what they're doing - the things they've done recently to try to improve their systems, which were incredibly sloppy, I think are better. And I think they've met - they really do - they're really leaning into - I hate to use that term - fixing this problem. At the same time, people have had it. And they've become an iconic company for all that's wrong with tech. And it's going to be difficult to shake that without enormous effort by Sandberg and by Zuckerberg and by the entire team of managers there. KELLY: Thank you, Kara. SWISHER: You're welcome. KELLY: That's Kara Swisher. She is editor at large of Recode and a contributor to The New York Times opinion page.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-03-671595075": {"title": "Store, Warehouse And Delivery Workers: NPR Wants To Hear From You : NPR", "url": "https://www.npr.org/2018/12/03/671595075/store-warehouse-and-delivery-workers-npr-wants-to-hear-from-you", "author": "No author found", "published_date": "2018-12-03", "content": "", "section": "Business", "disclaimer": ""}, "2018-12-03-672817706": {"title": "Study Shows Americans Are Meaner On Twitter Than Canadians : NPR", "url": "https://www.npr.org/2018/12/03/672817706/examining-differences-between-canadian-and-u-s-tweets", "author": "No author found", "published_date": "2018-12-03", "content": "RACHEL MARTIN, HOST: And I'm Rachel Martin with a question that vexes us all. Are Canadians really nicer than Americans? There's news from the department of stereotypes that it could be true. Here's David Greene. DAVID GREENE, BYLINE: Researchers in Canada have determined that Twitter users in their country tend to be more positive than Twitter users here in the United States of America. In short, does that mean that Canadians are nicer, at least, online, than Americans or something like that? Let's bring in Bryor Snefjella. He is one of the researchers at McMaster University and a lead author of this study. Bryor, welcome to our program. BRYOR SNEFJELLA: Hi, hi. GREENE: So what'd you actually find here? SNEFJELLA: So what we found is that if you go into Twitter and you quantify the most sort of characteristic language of Canadians and Americans, you find that that characteristic language really, really strongly matches the sort of established stereotypes of Canadians and Americans. GREENE: What were you actually doing in terms of this research? Were you collecting, like, a huge portfolio of tweets? SNEFJELLA: Yeah. Yeah. So the data set for the study is 40 million tweets. GREENE: Forty million? SNEFJELLA: Yeah. GREENE: I'm assuming you weren't reading all of those. SNEFJELLA: No. So we're a group of computational linguists. So we're parsing out the words or other symbols and then counting them on computers. GREENE: OK. Can you give me an example of some of the words that really stand out in Canadian tweets versus American tweets? SNEFJELLA: Yeah. Sure. So for Canada, you're talking about words like great, amazing, awesome, thanks being very characteristic. GREENE: OK. SNEFJELLA: For Americans, it's words like - well, it's swear words of all kinds. GREENE: Yeah. Some you probably can't even say on the radio. But, yeah. OK. SNEFJELLA: Exactly. And then a lot of words about negative emotional states, like hate, miss, bored, tired. GREENE: OK. So why do you think Americans are using more negative words like that? Are we just kind of more morose and unhappy? SNEFJELLA: Well, we're really, really curious about why this is, but it's a very, very difficult question to answer. GREENE: All right. SNEFJELLA: There is a whole body of research in social psychology that basically says that those stereotypes of us aren't true. If you just survey lots of Canadians and Americans. . . GREENE: Are not true? SNEFJELLA: Yeah. No, that our personality traits, on average, just don't seem to be any different. But people really, really believe those stereotypes are true. GREENE: Are you on Twitter? Do you use Twitter? SNEFJELLA: Very minimally. GREENE: OK. (Laughter). I'm just wondering if there's, like, any advice you could give to Americans who sit there stewing on social media for hours, like, just angrily tweeting. SNEFJELLA: Well, I think, you know, if there's a takeaway from the study, you wind up with the picture that, you know, it might be appealing to think that there's, like, a Canadian or American essence that makes us think we're different, or something. But our data actually isn't compatible with that. It says something more like we construct our stereotypes through our language choices. We're actually pointing that maybe the true thing isn't, you know, our characters. Maybe the true thing is how we choose to talk. So this is a case where maybe it does say something about our cultures, our interactions, our expectations of each other or how we compare ourselves to others, but it also says that, you know, that should be changeable. It's not a fact about our personalities. So if what we're finding in this study is the sum of the language choices of Canadians and Americans, we can make different choices and wind up with a different picture of who we are. GREENE: Well, listen. This has been really cool. And it's great research, and I'm going to, you know, have some things to think about next time I'm on Twitter. Thanks a lot. SNEFJELLA: Hey. No problem. GREENE: Bryor Snefjella is a researcher at McMaster University in Ontario, Canada. (SOUNDBITE OF DO MAKE SAY THINK'S \"SAY\")MARTIN: Seems like an appropriate moment to give a shout-out to our former editor, David McGuffin, a Canadian who is super, super nice. RACHEL MARTIN, HOST:  And I'm Rachel Martin with a question that vexes us all. Are Canadians really nicer than Americans? There's news from the department of stereotypes that it could be true. Here's David Greene. DAVID GREENE, BYLINE: Researchers in Canada have determined that Twitter users in their country tend to be more positive than Twitter users here in the United States of America. In short, does that mean that Canadians are nicer, at least, online, than Americans or something like that? Let's bring in Bryor Snefjella. He is one of the researchers at McMaster University and a lead author of this study. Bryor, welcome to our program. BRYOR SNEFJELLA: Hi, hi. GREENE: So what'd you actually find here? SNEFJELLA: So what we found is that if you go into Twitter and you quantify the most sort of characteristic language of Canadians and Americans, you find that that characteristic language really, really strongly matches the sort of established stereotypes of Canadians and Americans. GREENE: What were you actually doing in terms of this research? Were you collecting, like, a huge portfolio of tweets? SNEFJELLA: Yeah. Yeah. So the data set for the study is 40 million tweets. GREENE: Forty million? SNEFJELLA: Yeah. GREENE: I'm assuming you weren't reading all of those. SNEFJELLA: No. So we're a group of computational linguists. So we're parsing out the words or other symbols and then counting them on computers. GREENE: OK. Can you give me an example of some of the words that really stand out in Canadian tweets versus American tweets? SNEFJELLA: Yeah. Sure. So for Canada, you're talking about words like great, amazing, awesome, thanks being very characteristic. GREENE: OK. SNEFJELLA: For Americans, it's words like - well, it's swear words of all kinds. GREENE: Yeah. Some you probably can't even say on the radio. But, yeah. OK. SNEFJELLA: Exactly. And then a lot of words about negative emotional states, like hate, miss, bored, tired. GREENE: OK. So why do you think Americans are using more negative words like that? Are we just kind of more morose and unhappy? SNEFJELLA: Well, we're really, really curious about why this is, but it's a very, very difficult question to answer. GREENE: All right. SNEFJELLA: There is a whole body of research in social psychology that basically says that those stereotypes of us aren't true. If you just survey lots of Canadians and Americans. . . GREENE: Are not true? SNEFJELLA: Yeah. No, that our personality traits, on average, just don't seem to be any different. But people really, really believe those stereotypes are true. GREENE: Are you on Twitter? Do you use Twitter? SNEFJELLA: Very minimally. GREENE: OK. (Laughter). I'm just wondering if there's, like, any advice you could give to Americans who sit there stewing on social media for hours, like, just angrily tweeting. SNEFJELLA: Well, I think, you know, if there's a takeaway from the study, you wind up with the picture that, you know, it might be appealing to think that there's, like, a Canadian or American essence that makes us think we're different, or something. But our data actually isn't compatible with that. It says something more like we construct our stereotypes through our language choices. We're actually pointing that maybe the true thing isn't, you know, our characters. Maybe the true thing is how we choose to talk. So this is a case where maybe it does say something about our cultures, our interactions, our expectations of each other or how we compare ourselves to others, but it also says that, you know, that should be changeable. It's not a fact about our personalities. So if what we're finding in this study is the sum of the language choices of Canadians and Americans, we can make different choices and wind up with a different picture of who we are. GREENE: Well, listen. This has been really cool. And it's great research, and I'm going to, you know, have some things to think about next time I'm on Twitter. Thanks a lot. SNEFJELLA: Hey. No problem. GREENE: Bryor Snefjella is a researcher at McMaster University in Ontario, Canada. (SOUNDBITE OF DO MAKE SAY THINK'S \"SAY\") MARTIN: Seems like an appropriate moment to give a shout-out to our former editor, David McGuffin, a Canadian who is super, super nice.", "section": "Research News", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-04-673287352": {"title": "House GOP Campaign Arm Says It Was Hacked In 2018 : NPR", "url": "https://www.npr.org/2018/12/04/673287352/house-gop-campaign-arm-says-it-was-hacked-during-the-2018-election-cycle", "author": "No author found", "published_date": "2018-12-04", "content": "", "section": "Politics", "disclaimer": ""}, "2018-12-04-670829785": {"title": "Teachers Turn Lessons Into Instagram-Worthy Photos  : NPR", "url": "https://www.npr.org/2018/12/04/670829785/teachers-turn-lessons-into-instagram-worthy-photos", "author": "No author found", "published_date": "2018-12-04", "content": "", "section": "Education", "disclaimer": ""}, "2018-12-04-673144745": {"title": "100 Million Quora Users Affected By 'Malicious' Data Breach : NPR", "url": "https://www.npr.org/2018/12/04/673144745/100-million-quora-users-affected-by-malicious-data-breach", "author": "No author found", "published_date": "2018-12-04", "content": "", "section": "Technology", "disclaimer": ""}, "2018-12-05-673958138": {"title": "After New Jersey Indictment, Georgia Adds Charges Against Iranian 'SamSam' Hacke : NPR", "url": "https://www.npr.org/2018/12/05/673958138/georgia-charges-iranians-in-ransomware-attack-on-atlanta", "author": "No author found", "published_date": "2018-12-05", "content": "", "section": "Law", "disclaimer": ""}, "2018-12-05-673748642": {"title": "Facebook Execs Seen Discussing Data Privacy, Competitors In Leaked Documents  : NPR", "url": "https://www.npr.org/2018/12/05/673748642/facebook-execs-seen-discussing-data-privacy-competitors-in-leaked-documents", "author": "No author found", "published_date": "2018-12-05", "content": "", "section": "Business", "disclaimer": ""}, "2018-12-05-673686983": {"title": "Cuba Extends Internet To Mobile Phones, Promising New Access : NPR", "url": "https://www.npr.org/2018/12/05/673686983/cuba-will-allow-full-internet-on-mobile-phones-starting-thursday", "author": "No author found", "published_date": "2018-12-05", "content": "", "section": "Latin America", "disclaimer": ""}, "2018-12-06-673364305": {"title": "Native Americans On Tribal Land Are 'The Least Connected' To High-Speed Internet : NPR", "url": "https://www.npr.org/2018/12/06/673364305/native-americans-on-tribal-land-are-the-least-connected-to-high-speed-internet", "author": "No author found", "published_date": "2018-12-06", "content": "", "section": "National", "disclaimer": ""}, "2018-12-06-674310978": {"title": "Microsoft Urges Congress To Regulate Facial Recognition Technology : NPR", "url": "https://www.npr.org/2018/12/06/674310978/microsoft-urges-congress-to-regulate-facial-recognition-technology", "author": "No author found", "published_date": "2018-12-06", "content": "MARY LOUISE KELLY, HOST: You don't usually find companies asking for regulation on the technology that they're developing, but Microsoft is doing just that. The company wants Congress to write laws for its facial recognition technology in 2019. Microsoft is positioning itself as an outspoken elder statesman while still trying to beat its competitors. NPR's Alina Selyukh reports. ALINA SELYUKH, BYLINE: For Silicon Valley, this has been a troubled year. (SOUNDBITE OF MONTAGE)UNIDENTIFIED REPORTER #1: The market's down, and Apple's a big reason why. UNIDENTIFIED REPORTER #2: It's a wave of walkouts at Google offices around the globe. UNIDENTIFIED REPORTER #3: Facebook just had what might be the biggest wipeout in stock market history. SELYUKH: And in the midst of it all, this happened. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED REPORTER #4: Microsoft surpassed Apple. SELYUKH: Microsoft has now several times unseated Apple as the world's most valuable company - a designation Microsoft last occupied in the early 2000s. Part of Microsoft's steady rise in the recent years has to do with avoiding crises. The company has been cultivating an image as Silicon Valley's moral compass. It does business with the government while also suing the government. It profits from controversial new technologies while also urging regulation. Here is Microsoft president Brad Smith speaking today at the Brookings Institution. (SOUNDBITE OF ARCHIVED RECORDING)BRAD SMITH: We have turned down deals because we worry that the technology would be used in ways that would actually put people's rights at risk. You don't want to see the race run by some people who are taking the high road while others who may just not be thinking enough about these issues. SELYUKH: We should note, Microsoft is one of NPR's sponsors. For a long time, Microsoft was viewed as a stodgy company. After years of struggling with its identity, it's coming into its own, finding its place among the newer tech stars like Google and Amazon. Technologically, Microsoft decided to bet on data stored on the cloud, and cloud computing is paying off big. ANDREW HUNTER: They have made a tremendous amount of progress in the last two to three years of really bringing their product up on par with what Amazon has been offering. And Amazon has been the market leader a lot in the cloud computing space. SELYUKH: Andrew Hunter is with the Center for Strategic and International Studies where he follows government contracts. Now, much of the government already runs on Windows and Office, but now it's a race to put government data on the cloud and to sell the government modern technologies, including to the Defense Department. HUNTER: Their profile within the market has really risen in the last couple years. SELYUKH: In Silicon Valley, this kind of work took on new controversy this year. Amazon faced an outcry for giving its facial recognition technology to law enforcement, raising concerns about surveillance. Google canceled a contract with the Pentagon after employees protested that their artificial intelligence could be used for drone strikes. At Microsoft, CEO Satya Nadella also had to quell dissent over the company's products being used by Immigration and Customs Enforcement. At the same time, Microsoft signed a deal worth almost half a billion dollars with the U. S. Army. It will equip soldiers with augmented reality headsets. Again, Microsoft is trying to walk the middle line. Here's Brad Smith speaking last week at a defense conference. (SOUNDBITE OF ARCHIVED RECORDING)SMITH: For us, we've been clear. We are going to provide the U. S. military with access to the best technology, to all the technology we create - full stop. SELYUKH: But then he adds Microsoft will also be actively involved in setting boundaries for this technology, especially artificial intelligence and autonomous weapons. In today's speech, Smith did just that, laying out a plan to regulate facial recognition. He says, if we wait while technology spreads, we could be on our way to Big Brother dystopia. Alina Selyukh, NPR News. MARY LOUISE KELLY, HOST:  You don't usually find companies asking for regulation on the technology that they're developing, but Microsoft is doing just that. The company wants Congress to write laws for its facial recognition technology in 2019. Microsoft is positioning itself as an outspoken elder statesman while still trying to beat its competitors. NPR's Alina Selyukh reports. ALINA SELYUKH, BYLINE: For Silicon Valley, this has been a troubled year. (SOUNDBITE OF MONTAGE) UNIDENTIFIED REPORTER #1: The market's down, and Apple's a big reason why. UNIDENTIFIED REPORTER #2: It's a wave of walkouts at Google offices around the globe. UNIDENTIFIED REPORTER #3: Facebook just had what might be the biggest wipeout in stock market history. SELYUKH: And in the midst of it all, this happened. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED REPORTER #4: Microsoft surpassed Apple. SELYUKH: Microsoft has now several times unseated Apple as the world's most valuable company - a designation Microsoft last occupied in the early 2000s. Part of Microsoft's steady rise in the recent years has to do with avoiding crises. The company has been cultivating an image as Silicon Valley's moral compass. It does business with the government while also suing the government. It profits from controversial new technologies while also urging regulation. Here is Microsoft president Brad Smith speaking today at the Brookings Institution. (SOUNDBITE OF ARCHIVED RECORDING) BRAD SMITH: We have turned down deals because we worry that the technology would be used in ways that would actually put people's rights at risk. You don't want to see the race run by some people who are taking the high road while others who may just not be thinking enough about these issues. SELYUKH: We should note, Microsoft is one of NPR's sponsors. For a long time, Microsoft was viewed as a stodgy company. After years of struggling with its identity, it's coming into its own, finding its place among the newer tech stars like Google and Amazon. Technologically, Microsoft decided to bet on data stored on the cloud, and cloud computing is paying off big. ANDREW HUNTER: They have made a tremendous amount of progress in the last two to three years of really bringing their product up on par with what Amazon has been offering. And Amazon has been the market leader a lot in the cloud computing space. SELYUKH: Andrew Hunter is with the Center for Strategic and International Studies where he follows government contracts. Now, much of the government already runs on Windows and Office, but now it's a race to put government data on the cloud and to sell the government modern technologies, including to the Defense Department. HUNTER: Their profile within the market has really risen in the last couple years. SELYUKH: In Silicon Valley, this kind of work took on new controversy this year. Amazon faced an outcry for giving its facial recognition technology to law enforcement, raising concerns about surveillance. Google canceled a contract with the Pentagon after employees protested that their artificial intelligence could be used for drone strikes. At Microsoft, CEO Satya Nadella also had to quell dissent over the company's products being used by Immigration and Customs Enforcement. At the same time, Microsoft signed a deal worth almost half a billion dollars with the U. S. Army. It will equip soldiers with augmented reality headsets. Again, Microsoft is trying to walk the middle line. Here's Brad Smith speaking last week at a defense conference. (SOUNDBITE OF ARCHIVED RECORDING) SMITH: For us, we've been clear. We are going to provide the U. S. military with access to the best technology, to all the technology we create - full stop. SELYUKH: But then he adds Microsoft will also be actively involved in setting boundaries for this technology, especially artificial intelligence and autonomous weapons. In today's speech, Smith did just that, laying out a plan to regulate facial recognition. He says, if we wait while technology spreads, we could be on our way to Big Brother dystopia. Alina Selyukh, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-06-674201649": {"title": "Robot Punctures Can Of Bear Repellent At Amazon Warehouse, Sickening Workers  : NPR", "url": "https://www.npr.org/2018/12/06/674201649/robot-punctures-can-of-bear-repellent-at-amazon-warehouse-sickening-workers", "author": "No author found", "published_date": "2018-12-06", "content": "", "section": "National", "disclaimer": ""}, "2018-12-06-674073296": {"title": "More Potholes, Traffic Jams On The Horizon Unless Interstates Are Fixed, Report Finds : NPR", "url": "https://www.npr.org/2018/12/06/674073296/more-potholes-traffic-jams-on-the-horizon-unless-interstates-are-fixed-report-fi", "author": "No author found", "published_date": "2018-12-06", "content": "", "section": "National", "disclaimer": ""}, "2018-12-06-674075032": {"title": "California Gives Final OK To Require Solar Panels On New Houses : NPR", "url": "https://www.npr.org/2018/12/06/674075032/california-gives-final-ok-to-requiring-solar-panels-on-new-houses", "author": "No author found", "published_date": "2018-12-06", "content": "", "section": "Environment", "disclaimer": ""}, "2018-12-06-674025148": {"title": "Huawei Finance Chief's Arrest Threatens To Inflame U.S.-China Tensions : NPR", "url": "https://www.npr.org/2018/12/06/674025148/huawei-finance-chiefs-arrest-threatens-to-inflame-u-s-china-tensions", "author": "No author found", "published_date": "2018-12-06", "content": "", "section": "World", "disclaimer": ""}, "2018-12-07-673661569": {"title": "Netflix Wants Oscars, So It Sent 'Roma' To Theaters First : NPR", "url": "https://www.npr.org/2018/12/07/673661569/with-an-eye-on-oscars-netflix-sent-roma-to-theaters-first", "author": "No author found", "published_date": "2018-12-07", "content": "AUDIE CORNISH, HOST: The movie \"Roma\" is seen by critics as an Oscar-worthy masterpiece for director Alfonso Cuaron. But as NPR's Jasmine Garsd reports, behind the scenes, the film is part of a battle between streaming services, such as Netflix, and movie theaters. They're fighting over who gets to show the movies first. JASMINE GARSD, BYLINE: To premiere on the big screen or on a digital streaming service - that is the question. It's not a new one. Just last year at the Toronto Film Festival, the makers of \"I, Tonya\" were looking for distribution, and they reportedly turned down Netflix. Why - because they wanted \"I, Tonya\" in movie theaters, and they probably wanted to qualify for an Oscar, which actress Allison Janney won. (SOUNDBITE OF FILM, \"I, TONYA\")ALLISON JANNEY: (As LaVona) I didn't stay home making apple brown betties. No, I made you a champion knowing you'd hate me for it. That's the sacrifice a mother makes. GARSD: Scott Feinberg, an awards columnist for The Hollywood Reporter, explains. SCOTT FEINBERG: The Oscars are about rewarding the best movies, not TV shows. So that means you have to have had at least what they call a qualifying run in theaters in New York and LA. GARSD: That little golden statuette carries a lot of weight. You get more audience interest, more money, more work opportunities, and your film gets written into the history books. And all this presents a conundrum for Netflix as it tries to stand out among a growing number of streaming services. Michael Pachter is a media analyst at Wedbush Securities. MICHAEL PACHTER: Netflix's goal is to be a major production company that makes compelling content that is available exclusively on Netflix to give consumers a reason to sign up and stay as subscribers. GARSD: In order to do this, Netflix needs to attract A-list talent, and they want Oscars. Netflix has yet to get a best picture Oscar. To do so, it has to premiere its films in theaters. In other words, Netflix needs the movie theater to kill the movie theater. The new movie \"Roma\" might be a chance to change all this. (SOUNDBITE OF FILM, \"ROMA\")GARSD: On the one hand, it's directed by Oscar award winner Alfonso Cuaron. But it's also in black and white, in Spanish with subtitles and with unknown actors, unlikely to be a blockbuster. So Cuaron needed Netflix. Joe Pichirallo, a former executive at Fox Searchlight Pictures, says Netflix can give this movie a wide streaming audience. JOE PICHIRALLO: Suddenly his film can be seen right away in 190 countries around the world at a potential audience of 130 million people. GARSD: In a rare move, Netflix is showing the film first in cinemas nationwide and just for a few weeks. PICHIRALLO: And \"Roma\" is now being taken seriously at least right at this stage. It's still early. But right now \"Roma\" is being talked about as a serious Oscar contender. GARSD: Will this bring Netflix a best picture Oscar? Scott Feinberg from The Hollywood Reporter says times are changing. FEINBERG: The reality is we're sort of swimming upstream at this point because people are increasingly consuming movies in other ways thanks not only to Netflix but primarily to Netflix. You know, people love the convenience of being able to click a few buttons and watch from their couch. GARSD: But for the moment, the way to win that best picture is still to show it on the big screen. Jasmine Garsd, NPR News, New York. (SOUNDBITE OF STEPHANE WREMBEL TRIO'S \"BIG BROTHER\")CORNISH: And we should note that Netflix is an underwriter of NPR. (SOUNDBITE OF STEPHANE WREMBEL TRIO'S \"BIG BROTHER\") AUDIE CORNISH, HOST:  The movie \"Roma\" is seen by critics as an Oscar-worthy masterpiece for director Alfonso Cuaron. But as NPR's Jasmine Garsd reports, behind the scenes, the film is part of a battle between streaming services, such as Netflix, and movie theaters. They're fighting over who gets to show the movies first. JASMINE GARSD, BYLINE: To premiere on the big screen or on a digital streaming service - that is the question. It's not a new one. Just last year at the Toronto Film Festival, the makers of \"I, Tonya\" were looking for distribution, and they reportedly turned down Netflix. Why - because they wanted \"I, Tonya\" in movie theaters, and they probably wanted to qualify for an Oscar, which actress Allison Janney won. (SOUNDBITE OF FILM, \"I, TONYA\") ALLISON JANNEY: (As LaVona) I didn't stay home making apple brown betties. No, I made you a champion knowing you'd hate me for it. That's the sacrifice a mother makes. GARSD: Scott Feinberg, an awards columnist for The Hollywood Reporter, explains. SCOTT FEINBERG: The Oscars are about rewarding the best movies, not TV shows. So that means you have to have had at least what they call a qualifying run in theaters in New York and LA. GARSD: That little golden statuette carries a lot of weight. You get more audience interest, more money, more work opportunities, and your film gets written into the history books. And all this presents a conundrum for Netflix as it tries to stand out among a growing number of streaming services. Michael Pachter is a media analyst at Wedbush Securities. MICHAEL PACHTER: Netflix's goal is to be a major production company that makes compelling content that is available exclusively on Netflix to give consumers a reason to sign up and stay as subscribers. GARSD: In order to do this, Netflix needs to attract A-list talent, and they want Oscars. Netflix has yet to get a best picture Oscar. To do so, it has to premiere its films in theaters. In other words, Netflix needs the movie theater to kill the movie theater. The new movie \"Roma\" might be a chance to change all this. (SOUNDBITE OF FILM, \"ROMA\") GARSD: On the one hand, it's directed by Oscar award winner Alfonso Cuaron. But it's also in black and white, in Spanish with subtitles and with unknown actors, unlikely to be a blockbuster. So Cuaron needed Netflix. Joe Pichirallo, a former executive at Fox Searchlight Pictures, says Netflix can give this movie a wide streaming audience. JOE PICHIRALLO: Suddenly his film can be seen right away in 190 countries around the world at a potential audience of 130 million people. GARSD: In a rare move, Netflix is showing the film first in cinemas nationwide and just for a few weeks. PICHIRALLO: And \"Roma\" is now being taken seriously at least right at this stage. It's still early. But right now \"Roma\" is being talked about as a serious Oscar contender. GARSD: Will this bring Netflix a best picture Oscar? Scott Feinberg from The Hollywood Reporter says times are changing. FEINBERG: The reality is we're sort of swimming upstream at this point because people are increasingly consuming movies in other ways thanks not only to Netflix but primarily to Netflix. You know, people love the convenience of being able to click a few buttons and watch from their couch. GARSD: But for the moment, the way to win that best picture is still to show it on the big screen. Jasmine Garsd, NPR News, New York. (SOUNDBITE OF STEPHANE WREMBEL TRIO'S \"BIG BROTHER\") CORNISH: And we should note that Netflix is an underwriter of NPR. (SOUNDBITE OF STEPHANE WREMBEL TRIO'S \"BIG BROTHER\")", "section": "Business", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-10-675382748": {"title": "2018 In Review: What Happened In The World Of Big Tech : NPR", "url": "https://www.npr.org/2018/12/10/675382748/2018-in-review-what-happened-in-the-world-of-big-tech", "author": "No author found", "published_date": "2018-12-10", "content": "AUDIE CORNISH, HOST:  And now a look at the year in big tech in this week's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\")CORNISH: In 2018, we've gotten a clear look at some of the uglier results of Silicon Valley's move-fast-and-break-things philosophy. Here to talk about the year that was for big tech are two members of NPR's tech team, Alina Selyukh - hey there, Alina. . . ALINA SELYUKH, BYLINE: Hello. CORNISH: . . . And Jasmine Garsd. Welcome back, Jasmine. JASMINE GARSD, BYLINE: Hi. CORNISH: So Alina, I want to start with you because you've been covering Facebook, which has had a lot of troubles this year. Give us a recap of what went down. SELYUKH: Super short - you know, Cambridge Analytica started this year on a low note for Facebook, and we went into many months of disclosures of massive Russia-backed disinformation campaigns, then a huge discovery of an Iranian-backed disinformation campaign, several data breaches and news of just how hectic it has been inside Facebook through all of this. CORNISH: What's been the fallout? For a long time, this company and especially its leadership has seemed untouchable. SELYUKH: A lot of the fallout actually had to do with the fact that Facebook stayed silent for a really long time as these crises started emerging. And then of course we did get Mark Zuckerberg on Capitol Hill, where he was grilled for many, many hours. And there, he essentially started apologizing. (SOUNDBITE OF ARCHIVED RECORDING)MARK ZUCKERBERG: It's clear now that we didn't do enough to prevent these tools from being used for harm as well. And that goes for fake news, foreign interference in elections and hate speech as well as developers and data privacy. We didn't take a broad enough view of our responsibility, and that was a big mistake. SELYUKH: This whole year started with Mark Zuckerberg sort of offering this lofty goal of fixing Facebook and really hunkering down on what it was meant to be to begin with, which is a fun place to hang out with friends and family. It's safe to say we're now at the end of the year, and Facebook only continues to get deeper and deeper into the conversation of what really is the purpose and the effect of the social network that has now encompassed most of the country. CORNISH: And of course this isn't just about data breaches or data sharing. It's felt like this is the year that social media companies have been asked real questions about their moral obligations to society. Jasmine, can you talk a little bit about that? GARSD: Yeah. Facebook got into serious hot water this year when it stated that it would allow Holocaust deniers because that's just a matter of free speech. Also there was the case of Myanmar where military officials intentionally waged a misinformation campaign against the Rohingya Muslim population. And xenophobic fears were stoked. There was ultimately an ethnic cleansing and a mass displacement. And the United Nations condemned Facebook's role in being slow and ineffective in addressing this. So definitely these social media platforms - and it's not just Facebook - have been grappling with the question of, what is hate speech? What is a threat? Should free speech be allowed for everyone. SELYUKH: And as the companies have been trying to make moves to essentially clean up their platforms, they have also walked into hot water politically, and they've been accused of anti-conservative biases. And that has made it even more challenging for them to sort of appease the political party and appear neutral while also trying to decide just how much they should be involved in moderating speech on their platforms. CORNISH: How has this resonated among the community of workers for these companies? Have we been seeing people speak up, speak out? GARSD: I think that the best example of that is Google. This year was the year in which Google employees around the world protested, whether it was about how they are treated within the company. But also there were protests about various projects Google has embarked on with governments, with the military. And, yeah, we saw this this moment in which tech workers themselves are reckoning with these companies. SELYUKH: And there's a bit to add, too, about the interesting contrast between how the companies have been dealing with this really rough year. So Google - as Jasmine's saying, they're staging protests and walkouts. At Facebook, it has been the reverse of that, which is there's been closing of the ranks, really this unified internal comment about how, you know, we need to come together as a company but also this broader conversation about, you know, this tech work that used to be so glamorous. What does it really stand for? And did people, you know, go into this work for the purposes that they were still fulfilling? I've heard from folks inside Amazon talking about how frustrating they find it to be in this situation where in any conversation about big tech that spawns from the Facebook crisis, they end up being pulled in and being part of the criticism because they are part of this big tech, Silicon Valley community. CORNISH: Alina, catch us up on Amazon. That's a company we haven't been talking about a lot here even though they sort of had a splashy year in talking about where to put their second headquarters. SELYUKH: Yes. This was one of the biggest business stories of the year, and it became extremely anticlimactic in the end. Amazon spent months luring cities and towns from all across the country to bid for these new second headquarters. And then in the end, the company decided to split it among two communities which are really large already. It's the suburb of Washington, D. C. , and then Queens in New York City. And the reaction has been disillusioned by a lot of people in the business community, including among the people in the communities that are going to play host to these new headquarters. GARSD: Yeah. I spoke to councilmember Jimmy Van Bramer recently. He's from Queens, and here's what he had to say. (SOUNDBITE OF ARCHIVED RECORDING)JIMMY VAN BRAMER: I would feel much better about us throwing $3 billion at solving the crisis in public housing in the city of New York than for damn sure throwing it at Jeff Bezos. SELYUKH: And so the next question is going to be what exactly these communities get for investing a large amount of taxpayer money into helping Amazon build these really large new offices. CORNISH: Last question - it's a big-picture one, which is, how do you sum up the lasting changes to the tech industry from all of this? What lessons will they have learned? GARSD: I think this was the year in which the public fell out of love a little bit with big tech. And just for Facebook, for example, in the U. S. , user growth has plateaued. In Europe, it's decreased. So I think, yeah, it's the year in which the public became disillusioned and critical of these big tech companies. SELYUKH: And it's also a year when a lot of people have come to understand at a much deeper level what exactly these companies sell. And even around dinner tables around the country, I know people are having conversations about what happens when you sign up for a free service - finally starting to talk about what the product is. And in many cases, it is the data that gets collected about you, and this data is increasingly extremely detailed and very wide-ranging. CORNISH: That's NPR's Alina Selyukh. Alina, thank you. SELYUKH: Thank you. CORNISH: And NPR's Jasmine Garsd, thanks for sharing with us, Jasmine. GARSD: Thank you. AUDIE CORNISH, HOST:   And now a look at the year in big tech in this week's All Tech Considered. (SOUNDBITE OF ULRICH SCHNAUSS' \"NOTHING HAPPENS IN JUNE\") CORNISH: In 2018, we've gotten a clear look at some of the uglier results of Silicon Valley's move-fast-and-break-things philosophy. Here to talk about the year that was for big tech are two members of NPR's tech team, Alina Selyukh - hey there, Alina. . . ALINA SELYUKH, BYLINE: Hello. CORNISH: . . . And Jasmine Garsd. Welcome back, Jasmine. JASMINE GARSD, BYLINE: Hi. CORNISH: So Alina, I want to start with you because you've been covering Facebook, which has had a lot of troubles this year. Give us a recap of what went down. SELYUKH: Super short - you know, Cambridge Analytica started this year on a low note for Facebook, and we went into many months of disclosures of massive Russia-backed disinformation campaigns, then a huge discovery of an Iranian-backed disinformation campaign, several data breaches and news of just how hectic it has been inside Facebook through all of this. CORNISH: What's been the fallout? For a long time, this company and especially its leadership has seemed untouchable. SELYUKH: A lot of the fallout actually had to do with the fact that Facebook stayed silent for a really long time as these crises started emerging. And then of course we did get Mark Zuckerberg on Capitol Hill, where he was grilled for many, many hours. And there, he essentially started apologizing. (SOUNDBITE OF ARCHIVED RECORDING) MARK ZUCKERBERG: It's clear now that we didn't do enough to prevent these tools from being used for harm as well. And that goes for fake news, foreign interference in elections and hate speech as well as developers and data privacy. We didn't take a broad enough view of our responsibility, and that was a big mistake. SELYUKH: This whole year started with Mark Zuckerberg sort of offering this lofty goal of fixing Facebook and really hunkering down on what it was meant to be to begin with, which is a fun place to hang out with friends and family. It's safe to say we're now at the end of the year, and Facebook only continues to get deeper and deeper into the conversation of what really is the purpose and the effect of the social network that has now encompassed most of the country. CORNISH: And of course this isn't just about data breaches or data sharing. It's felt like this is the year that social media companies have been asked real questions about their moral obligations to society. Jasmine, can you talk a little bit about that? GARSD: Yeah. Facebook got into serious hot water this year when it stated that it would allow Holocaust deniers because that's just a matter of free speech. Also there was the case of Myanmar where military officials intentionally waged a misinformation campaign against the Rohingya Muslim population. And xenophobic fears were stoked. There was ultimately an ethnic cleansing and a mass displacement. And the United Nations condemned Facebook's role in being slow and ineffective in addressing this. So definitely these social media platforms - and it's not just Facebook - have been grappling with the question of, what is hate speech? What is a threat? Should free speech be allowed for everyone. SELYUKH: And as the companies have been trying to make moves to essentially clean up their platforms, they have also walked into hot water politically, and they've been accused of anti-conservative biases. And that has made it even more challenging for them to sort of appease the political party and appear neutral while also trying to decide just how much they should be involved in moderating speech on their platforms. CORNISH: How has this resonated among the community of workers for these companies? Have we been seeing people speak up, speak out? GARSD: I think that the best example of that is Google. This year was the year in which Google employees around the world protested, whether it was about how they are treated within the company. But also there were protests about various projects Google has embarked on with governments, with the military. And, yeah, we saw this this moment in which tech workers themselves are reckoning with these companies. SELYUKH: And there's a bit to add, too, about the interesting contrast between how the companies have been dealing with this really rough year. So Google - as Jasmine's saying, they're staging protests and walkouts. At Facebook, it has been the reverse of that, which is there's been closing of the ranks, really this unified internal comment about how, you know, we need to come together as a company but also this broader conversation about, you know, this tech work that used to be so glamorous. What does it really stand for? And did people, you know, go into this work for the purposes that they were still fulfilling? I've heard from folks inside Amazon talking about how frustrating they find it to be in this situation where in any conversation about big tech that spawns from the Facebook crisis, they end up being pulled in and being part of the criticism because they are part of this big tech, Silicon Valley community. CORNISH: Alina, catch us up on Amazon. That's a company we haven't been talking about a lot here even though they sort of had a splashy year in talking about where to put their second headquarters. SELYUKH: Yes. This was one of the biggest business stories of the year, and it became extremely anticlimactic in the end. Amazon spent months luring cities and towns from all across the country to bid for these new second headquarters. And then in the end, the company decided to split it among two communities which are really large already. It's the suburb of Washington, D. C. , and then Queens in New York City. And the reaction has been disillusioned by a lot of people in the business community, including among the people in the communities that are going to play host to these new headquarters. GARSD: Yeah. I spoke to councilmember Jimmy Van Bramer recently. He's from Queens, and here's what he had to say. (SOUNDBITE OF ARCHIVED RECORDING) JIMMY VAN BRAMER: I would feel much better about us throwing $3 billion at solving the crisis in public housing in the city of New York than for damn sure throwing it at Jeff Bezos. SELYUKH: And so the next question is going to be what exactly these communities get for investing a large amount of taxpayer money into helping Amazon build these really large new offices. CORNISH: Last question - it's a big-picture one, which is, how do you sum up the lasting changes to the tech industry from all of this? What lessons will they have learned? GARSD: I think this was the year in which the public fell out of love a little bit with big tech. And just for Facebook, for example, in the U. S. , user growth has plateaued. In Europe, it's decreased. So I think, yeah, it's the year in which the public became disillusioned and critical of these big tech companies. SELYUKH: And it's also a year when a lot of people have come to understand at a much deeper level what exactly these companies sell. And even around dinner tables around the country, I know people are having conversations about what happens when you sign up for a free service - finally starting to talk about what the product is. And in many cases, it is the data that gets collected about you, and this data is increasingly extremely detailed and very wide-ranging. CORNISH: That's NPR's Alina Selyukh. Alina, thank you. SELYUKH: Thank you. CORNISH: And NPR's Jasmine Garsd, thanks for sharing with us, Jasmine. GARSD: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-10-675254096": {"title": "The Revolution Will Be Driverless: Autonomous Cars Usher In Big Changes : NPR", "url": "https://www.npr.org/2018/12/10/675254096/the-revolution-will-be-driverless-autonomous-cars-usher-in-big-changes", "author": "No author found", "published_date": "2018-12-10", "content": "TERRY GROSS, HOST: This is FRESH AIR. I'm Terry Gross. The future of the driverless car is going to affect the future of how we travel and what we do in cars. But driverless cars are also likely to transform roads, cities, suburbs, jobs, the economy and daily life. My guest Samuel Schwartz expects it to be a very disruptive technology. Schwartz is the author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future,\" which he says is about the good, the bad and the ugly of how driverless cars will change our world. He knows a lot about transportation systems. He served as the traffic commissioner of New York City and chief engineer of the city's Department of Transportation. He now has his own consulting firm and has worked with cities around the world on transportation-related issues. Later in our conversation, after we talk about the future, we're going to talk about traffic problems that plague us today. We're going to use the words driverless car interchangeably with the words autonomous vehicle, or AV. Sam Schwartz, welcome to FRESH AIR. In your book, you write that AVs, autonomous vehicles, will be the most disruptive technology to hit society worldwide since the advent of the motorcar. Give us a couple of examples of industries or jobs or roadways that we might not realize will be profoundly affected by AVs once they start to really dominate. SAMUEL SCHWARTZ: I think everybody is expecting fewer drivers, and, you know, that's no surprise. But it also means that there're probably going to be fewer repair shops because AVs lend themselves to fleet operations, especially if they're going to be offering rides, as opposed to selling maximum vehicles. So car dealerships may disappear. So this is going to have wide impacts. Truckers, of course, are going to be impacted - how we move about in so many different ways. But lots of industries will be affected. The insurance industry, certainly, will be affected since we will have fewer crashes, and about a third of the insurance industry is based on crashes. And if we have fewer crashes, there are going to be fewer cases in court. There'll be less of a burden on the court system. GROSS: So kind of nobody's going to have a job, is what (laughter) - is what you're saying. SCHWARTZ: No, not quite, but. . . GROSS: You know, I've always - until reading your book, I thought of AVs - autonomous vehicles, driverless cars - as looking a lot like my car, except that I wouldn't have to sit in the driver's seat, or if I was in the driver's seat, I wouldn't have to do anything. But as you point out in your book, once you have an autonomous vehicle, you get to rethink the whole design of the car and probably also what the car is made of. I mean, let's face it. Our cars are designed for, like, the gas engine under the hood, and the steering wheel and the brake pedal. So give us a sense of, visually - like, of how a car might be designed differently. SCHWARTZ: A lot of people have the image that you have, Terry. And a lot of them look - they kind of look - the Google car looks like a smaller car, like a smart car, and it's got some intelligence on top of it. But I liken this a bit to the cellphone. In 1982, I was traffic commissioner. I had one of the first cellphones that I could use during emergencies. And the cellphone came - with a man carrying it - on a 12-volt battery and this huge phone. And today - the cellphone of today doesn't look anything like the cellphone that I had in 1982. The same thing can happen with cars. It could - the autonomous vehicle of the future, most people think that by the second half of the century, at least, and maybe sooner, will have no steering wheel, will have no brake. It will be a room. It could be a room any size. It could be a conference room. It could be, you know, the width of a house. It - there's no reason to think it's going to look like a car, other than that's what we're used to. So others are beginning to think of these vehicles as being something totally different - different uses. And I see that as a real potential - as a meeting place, a sleeping place, an eating place. Medical facilities will come to you. It'll look like a hospital room or a doctor's office - a place to be examined by a doctor remotely in which a remote stethoscope listens to your heart, and your blood pressure is taken, and all those types of things are possible. Some of this is good. But we should be - go forward with our eyes wide open. GROSS: What's your assessment so far of the safety of autonomous vehicles and what that would be like in the future? I mean, on the one hand, you don't have a driver's misjudgment or a driver falling asleep at the wheel, so that could eliminate certain crashes. On the other hand, technology is always subject to failure, and computers often mess up, and they - you know, everybody's been on the computer when it just kind of freezes, and you can't work anymore. So imagine that happening to your car. SCHWARTZ: And it could happen, and, of course, hacking could happen. But many of the features that autonomous vehicles are - operators are claiming are needed, you don't need in the autonomous mode. So there is automatic braking features. There are lane-control features. There are blind-spot monitoring. There's a whole host of things that are being offered today to people who could afford it. So the low-income people are not getting all these safety features that could cut down on crashes tremendously. And then you take the driver out of it. Yes, you'll - you can reduce crashes further because you don't have to worry about the judgment of a driver, but you may create other types of crashes. Right now, the industry isn't sharing anything. This is really an industry that has tons and tons of data, but they're not sharing it with the public. The only data that's accessible is the state of California, which is reporting anytime there is a crash when a vehicle is in autonomous mode. And so far, the results are not good. So far, we know of three deaths, and not because of any reporting. That's the media reporting of three deaths in autonomous mode. For conventional cars, that would take 260 million miles before that would occur. And here we have three deaths, and AVs have driven maybe 10 or 15 million miles. In California, which requires the reporting of crashes, the cars in autonomous mode are crashing nine or 10 times more often than the conventional cars. Will they solve a lot of that? Yes, they're going to. They could solve a lot of the safety problems in the next few years. Everybody could have automatic braking. Everybody could have lane-control protections and could have blind-spot monitoring and other safety features. But there is this meme that we have to wait until we have autonomous vehicles for safety, instead of the meme that maybe we could do a lot now. GROSS: So what kind of features are you talking about? SCHWARTZ: Right now, in - for example, I have a car, a 2017 Volvo, that will stop on its own if I'm not paying attention and about to rear-end another car. There are features that will let me know if I am leaving the lane. The car itself will tug and tell me that I'm crossing a lane. And so if I'm drowsy, if I'm not paying attention or if I had too much to drink, which I would never do, I would know that. I know that there's a warning if there's a car in my blind spot and I'm trying to change lanes. So there are lots of features that exist today that if the industry that is claiming, safety, safety, safety, and that's why we need AVs, would really focus on safety, they'd say, we could have - by 2022, every single car sold in America could be the safest car possible, and we'll make it even safer eventually with autonomous vehicles. But so far, the track record is not very good with autonomous vehicles. They can't figure out what a pedestrian is or the pedestrian is going to do. They can't separate a child from a dog. There's sometimes - a tree branch overhanging the road will be taken as something in the way. So, you know, we're far from perfecting. GROSS: How good are AVs at recognizing a human being? SCHWARTZ: They're in the high 90s - they will - percent-wise. But there are times that they'll have a false reading, or they will still say, I think this is a person. And this is what happened with the Uber crash in Tempe, Ariz. , that killed a woman. The woman was walking with a bicycle. So you would think, A, it would recognize the woman and, two, it would also recognize a bicycle. But the system said, hmm, I'm not sure what this is; it doesn't look right to me, so I don't want to stop suddenly because that also contributes to crashes; I'm going to plow right ahead. And it plowed right ahead and killed her. GROSS: Wow. Yeah. I think I didn't understand how that happened. My guest is Samuel Schwartz. He's the author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future. \" He is the former New York City traffic commissioner and the former chief engineer of New York City's Department of Transportation. We'll be right back. This is FRESH AIR. (SOUNDBITE OF MUSIC)GROSS: This is FRESH AIR. And if you're just joining us, my guest is Sam Schwartz. He's the author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future. \" He knows a lot about roads and cars and traffic. He's New York City's former traffic commissioner and former chief engineer of the Department of Transportation. He has his own consulting firm now. So let's talk about how AVs could affect transportation in terms of, like, traffic. And, you know, driverless cars are great if they're not stuck in traffic (laughter). And they're going to affect the amount of traffic on the road. Then they'll also affect what the road looks like. You lay out a scenario. There's a family with a couple of kids, two parents. The AV takes one parent to work, drops them off work, comes back home, picks up the kids, takes them to school, drops them off at school. The autonomous vehicle comes back home, picks up the second parent, takes them to work. And then the car does - I'm not sure what. I'm not sure where it stays (laughter) in between all of these trips. But you point out, in between trips, you have an empty car. You have what you describe as, like, a zombie car. So what does that mean in terms of, like, the number of cars that's going to be on the road? SCHWARTZ: Well, one way to look at the future is the intent of many people in the AV industry is to replicate Uber, Lyft, Via and other car services but without a driver. So in lots of cases, you - it would be an app or whatever succeeds an app. You will call for a car. The difference between that and an Uber today is there will be no driver. So the - we've already been able to determine a lot of things about these new app-based services. The average app-based service, when it carries you 1 mile, it needs to travel 1. 6 miles. So a 10-mile trip means that vehicle would have traveled 16 miles over the course of the day. And that's because it's empty during portions of the day. The car has to get into position to be near where there's density of people. So we already know it increases vehicle miles traveled. We also know that there's a concentration - unless we're smart, there's a concentration of where the well-to-do are. It's a concentration in most cities like New York, like Philadelphia, like San Francisco and Chicago that a lot of the well-to-do are now near or in city centers, where we already have the best transportation and the best transit systems. So what we've been finding is the congregation of the Ubers and Lyfts, A, where we already have good transportation service, B, where many of our wealthiest citizens live and, C, where we already had the worst traffic. So the future, unless we change that model, is quite bleak because there'll be many, many more autonomous vehicles out there because it'll be so inexpensive to run them, especially if they're run as a fleet. There'll be no downtime. The average car today is in - 95 percent of the time sits idle. These cars will be moving around doing their job 80, 90 percent of the time. So far more vehicles will be on the road. The other thing is there is this hype from the industry that autonomous vehicles are so efficient, they could follow each other so closely that we could replicate on a highway the capacity of a train. That's nonsense. A train or a bus in an exclusive lane could move 100 people in 60 feet, or certainly a subway car, whereas an autonomous vehicle, unless we change our behavior, is going to have one person per car - even if it has one person per car - it may have zero, less than that - will be moving, at most, two or three people in that same distance. So it - the math doesn't work. GROSS: So are you, like, making an argument here that, in spite of AVs, we need good public transportation, like trains that can move a lot of people at one time with high density within each car? SCHWARTZ: Autonomous vehicles are coming. There's no doubt about it. But we should maintain good public transportation systems. We should - and I use the adjective good because there are bad public transportation systems out there. Ninety percent of the country has lousy public transportation. It's called a bus that comes around every half hour or hour. It largely serves poor people. When a system only serves poor people, it's a poor system. Often it's largely people of color. We have an opportunity to transform public transportation in those areas, not to offer a half-hour service, but we can triple the service by using micro-transit AV vehicles, small buses that are smart buses that are on-demand that know where the people are, know where they want to get off. We already have that technology. We have a company here called Via that is doing it in New York, Chicago and a few others. We have Ford Chariot that's beginning to do it in San Francisco and a few other places. The technology is here. So AVs could be terrific for lower-density public transportation. But we're going to still need, in the big cities or in places where we want to move lots of people - the Northeast Corridor - you're going to still need trains. You're still going to need subways, streetcars. We can't substitute cars for that kind of service. GROSS: So, you know, we're talking about cars changing. And we're talking about the road infrastructure. What kinds of new highways will driverless cars require? Like, how might highways change? SCHWARTZ: If we're really smart, we can have less infrastructure if autonomous vehicles really look something like cars. If they have a width of 6 feet as cars have today, if you have a three-lane highway - the average three-lane highway is 36 feet - you're probably going - three 12-foot lanes - you're only going to need, say, 21 feet. So you can either add more lanes and add more capacities to highways or do something else with all that land that will be unleashed and not have to maintain as much infrastructure. GROSS: But let me interrupt you for a second because you are saying that we're going to be able to, like, eat and sleep and have romantic evenings in our car. So that requires room. But if you're making the car smaller because there isn't going to be a driver, then there isn't going to be a - you know, space to, like, live in your car the way you described it before. SCHWARTZ: Yeah. So I go through the good, the bad, the ugly. GROSS: OK (laughter). SCHWARTZ: What I'm painting right now is the good. GROSS: OK. SCHWARTZ: And the good is they look like cars. They don't look like conference rooms. They don't look like houses. They don't look like coffee shops. They look like cars. What a great opportunity if they stay in - looking like cars because of the narrowness of cars. They're 6 feet wide, but we have 12-foot lanes because people weave back and forth. They're imprecise in their driving. They're not paying attention. An autonomous vehicle could be like a tracked vehicle - stay in lane. So for - a 6-foot vehicle could have a 7-foot lane. So in that three-lane highway that's 36 feet - you can get a three-lane highway down to 21 feet, and it'll be a more efficient highway in terms of moving vehicles 'cause the autonomous vehicles could be more closely together. GROSS: GM recently announced plans to shut down five factories in North America and cut about 14,000 jobs. Earlier this year, Ford announced it will stop making sedans. In 2016, Fiat Chrysler stopped making small and mid-sized cars. I think GM is also going to be cutting back its, you know, mid-sized cars. And the emphasis seems to be in two directions - one on AVs, autonomous vehicles, and the other on, like, trucks and SUVs. So we seem to be heading in two opposite directions at the same time in a way. Like, the most fuel-efficient and the least (laughter) fuel-efficient cars and, you know, more lightweight autonomous vehicles and incredibly, like, big and heavyweight larger vehicles. Do you see us heading in two opposite directions at the same time? SCHWARTZ: Yeah. I'm very disappointed in that the auto industry - and that, you know, they're reflecting the desires of the American people to be in bigger cars, to be in heavier vehicles and to have these front ends that are so high. That has an impact. That's why pedestrian deaths - that's one of the main reasons pedestrian deaths have soared - is that people are getting hit by SUVs, getting hit not - no longer with knee injuries - and this comes from ER doctors who've seen me, who talk to me. There's a real connection between the health industry right now and transportation. They say in the past, someone got hit by a car, it was a knee injury. It was leg injury. Now it's a chest injury, and those are more likely to be fatal. So I'm really not happy about that. But yes, it is going to be a problem - moving heavier vehicles and trying to consume less energy. GROSS: My guest is Sam Schwartz, author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future. \" He's the former traffic commissioner of New York City. And after a break, we'll talk about the kinds of traffic nightmares we face today. I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF CHARLES MINGUS'S \"A FOGGY DAY\")GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to my interview with Sam Schwartz, author of the new book \"No One At The Wheel\" about driverless cars and how they're likely to change the future of transportation, roads, jobs, cities and daily life. Now we're going to talk about the traffic nightmares we face today. Schwartz is an expert on that. He served as traffic commissioner of New York City and the chief engineer of the city's Department of Transportation. He now has a consulting firm which works with cities in the U. S. and other countries on transportation and traffic-related issues. There are so many changes going on right now in who drives and what they drive and what the roads are like, and I'd like to spend some time talking about that 'cause I'm as interested in what's happening right now as I am as what's going to happen in the future as more and more AVs come onto the road. And you know a lot about what's happening now because your consulting firm advises, you know, different cities around the world how to deal with current changes. So let's look at what's happening in some places in America right now. As you point out, millennials are generationally less likely to own a car or even drive a car than previous generations. So let's start with, why do you think that is? SCHWARTZ: For a variety of reasons. And I did write a previous book, \"Street Smart,\" in which I interviewed a lot of millennials. They came up with lots of reasons - you know, the cost, the - certainly the availability of other forms of transportation with the apps that you have today. The loans that they still have from college make it difficult. The - there was also an interesting thing that I learned talking to millennials - is they saw it as old-fashioned. They saw it from the back of the car. Many of them grew up in suburbs - saw very unhappy parents fighting over who's going to drive the kids to soccer practice and school and to the dentist and all of those things. And they didn't - that's not a lifestyle that they want. So there's been a big 20 percent drop in millennials. It doesn't mean the majority don't drive. The majority do drive. But it's gone from 90 percent driving to 70 percent driving. A 20 percent drop is like an earthquake in transportation. GROSS: And of course bicycles have become much more popular. But also millennials are taking advantage of app-based car services like Uber and Lyft. And I'm wondering how you think app-based car services like Uber and Lyft are affecting larger traffic patterns in urban areas. SCHWARTZ: Yeah, you know, I've been in the transportation business for a half century. I was a New York City cab driver back in 1968. And I watched transportation evolve over time. I have never seen anything as rapid as what has happened this decade. So Uber and Lyft - we were caught flatfooted, all those of us in city planning. We never saw it coming. We never thought it would be as popular as it is. But it is so popular that in five years, a brand new service eclipsed the number of people taking taxi rides in just about every city from taxi industries that have been around for a hundred years. It has rapidly increased the amount of miles driven in the densest part of cities, so it's made congestion much worse in those places. And it's not provided the kind of service improvements in transit since it's taking a lot of people out of transit. In New York City, perhaps half the people in Uber and Lyft come out of transit. In California, which is really a car capital of the world, it's 33 percent of Uber and Lyft drivers - Lyft passengers say that they came out of public transportation. In Denver, in Boston - any city, it's the same thing. So we're putting more cars on the road and we're putting them in the worst traffic spots. So it's been a shock to the system. If we can go to - when these services came in, they talked about taking people to transit. That would be the best use of these services. GROSS: Another thing that I think is really changing the roads in cities are the delivery services, like the trucks that deliver groceries to your door through various apps and the bicycles that deliver food from restaurants through various apps to your door. And the delivery trucks - you know, when they double-park, that takes up a lot of space. And I know they're only running in and out, but it still kind of backs up traffic for that amount of time. So what's your impression about how the convenience that so many of us are taking advantage of through these delivery services, including deliveries on things that - you know, on goods that you order through the Internet - how these delivery trucks are affecting traffic? SCHWARTZ: They're having a big impact, these micro-deliveries where you can have a - you know, toothpaste delivered, and that could be the only thing in the package. It adds to many more trucks. So we're seeing an increase in the number of trucks that are traveling in cities. It may work well in - again, in low-density areas, but in cities, it's been a problem. What we're seeing is some of the fulfillment centers are now using Uber and Lyft to make these micro-deliveries so, again, adding far more traffic. If we go back, you know, again, to the last century, people walked to stores. People drove to stores. People then collected and bought far more goods than they would at one time than they are now getting, every single day, a little package delivered. GROSS: I think bicycles are having a huge impact on traffic, especially millennials who don't have a car and don't always need or want to take, you know, a vehicle service like Uber or Lyft. You know, you bicycle, but many urban roads are not really designed for bicycles. Where I live, for instance, in Philadelphia, it's an old city. It's a Ben Franklin-era city in some ways. And the streets are very narrow except for a couple of the main streets in Philly. And to squeeze in parked cars and drivers and bikes, you know, on one road is often really challenging. And there's wonderful things to be said environmentally and in terms of convenience about the positive nature of bicycles, but it's also, I think, having some adverse effects for life on the roads. SCHWARTZ: Well, you know, I - again, I'd like to change the mindset a bit and remember that for hundreds of thousands of years, since the first cities started, it was the pedestrians that were walking, and the carriages had to go around them or move at the same speed. And somehow we've accepted the fact that we need lots of cars, lots of vehicles, and the intruders are the bike riders and the pedestrians, and they shouldn't be second-class citizens. What happened in cities like New York City, Seattle and others that really went - I wouldn't say all out but at least in their central business districts provide far more pedestrian zones, far more bicycle lanes, the total number of cars coming in has gone down. And so we can get by with fewer cars. If we get by with fewer cars, then suddenly maybe Chestnut and Walnut in Philadelphia could have different uses. Maybe the cars go very slowly, which they do anyhow. But if you go to many European cities, you'll see the mix of pedestrians and cars moving very slowly. We may have to use examples like that, but we have to change our mindset. GROSS: So what is your greatest frustration now as a driver when you're driving as opposed to using public transportation? SCHWARTZ: You know, having been traffic commissioner, having been in charge of traffic enforcement, my blood boils when I see violations. And when I see violations where people are illegally parked or people are turning against a red light when they're not permitted, those things really drive me crazy. In New York City, we have government workers who drive in at twice the rate of anyone else because they get placards to park their cars. That drives me nuts. If I were back as traffic commissioner, I'd be telling all of them - in fact, I did that, and I told so many diplomats that I was invited to what I am told is the best-attended session of the U. N. General Assembly. GROSS: (Laughter) They wanted you to fix their tickets (laughter). SCHWARTZ: I had the - the Israelis and Arabs were on the same side as. . . GROSS: (Laughter). SCHWARTZ: . . . Iraq and Iran in the 1980s. And it was just me being interpreted into a dozen languages. And one who would've gone by and just seen that scene not hearing what was going on would assume this lone guy at the table that the whole world was focusing on had the answer to world peace. It was all about parking. GROSS: That is really, really hilarious. Let me reintroduce you here. If you're just joining us, my guest is Sam Schwartz. He's the author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future. \" He's also New York City's former traffic commissioner and the former chief engineer for New York City's Department of Transportation. We'll be right back. This is FRESH AIR. (SOUNDBITE OF JERRY GRANELLI, ROBBEN FORD, BILL FRISELL AND J. ANTHONY GRANELLI'S \"AIN'T THAT A SHAME\")GROSS: This is FRESH AIR. And if you're just joining us, my guest is Sam Schwartz, author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future. \" He's also New York City's former traffic commissioner and former chief engineer for New York City's Department of Transportation. He now has his own consulting firm. So let's talk briefly about your time as New York City traffic commissioner and chief transportation engineer in New York. One of the things I've always wondered is, how do you get traffic lights synced up? 'Cause I could see the possibility of syncing them up, say, going north-south, but what about the intersecting streets going east-west? Like, and you were doing that before, like, personal computers. So just tell us a little bit about the difficulties of making that happen. SCHWARTZ: Actually, I learned how to do signal timing right near you at the University of Pennsylvania 50 years ago, and we did it by using graphs. And we used the speed and distance graphs to figure out - if a vehicle is moving at 30 miles an hour in six seconds, it'll be at the next street so we have to change the signal at the next street. Now, that's easy to do when you have just a linear problem. But when you have a grid problem, how do you do it crosstown? Well, we soon learned that we couldn't do it on every street, but we could do it on every fourth street. So it's a mathematical problem. But ultimately, the conclusion that I came to in the 1980s, and that's after a dozen years at - working at the traffic department, was that we're not going to solve the traffic problem by moving more cars and that the best signal program for an area like Midtown Manhattan or center city is one that recognizes when the number of vehicles in the center city or the Midtown area is getting to a critical volume. And at that point, begin to slowly entry of vehicles into it using the signal system kind of as a belt tightening the number of cars coming in. I first learned that in the city of Nottingham, England, which did that in the 1970s. I call it metering of high-density sectors, but we can't solve the problem of north, south, east, west. We really have to solve the problem as a grid problem. GROSS: Shortly after you popularized the word gridlock - and you are the guy who popularized it - I was witness to an amazing gridlock in Manhattan. And I was fortunately a pedestrian at that moment (laughter) - not in a car. But it was a symphony of horns honking. And like, all around, the intersections were just totally bumper-to-bumper with cars. There was no place for any car in the area to move. And I always wondered, like, how does a gridlock like that - when it's, like, a super gridlock, how does it end? SCHWARTZ: OK. Well, it starts because people are piggish. It starts because people block intersections which then block the perpendicular movement. And then. . . GROSS: Oh, you're trying to make it through the (laughter) - you're trying to make it through the right. SCHWARTZ: Right. GROSS: And so is the car. . . SCHWARTZ: Yes. GROSS: . . . In front of you. And yeah. And that's really a recipe for gridlock. SCHWARTZ: Right, right. So. . . GROSS: So yeah. So a gridlock's created. SCHWARTZ: Right. So a bird's-eye view would show you're blocking yourself. So the key to that - and I've witnessed many gridlocks - is you have to slow the traffic coming into the area. On April 9, 1980, there was a thunderstorm - a terrible storm during the transit strike. And gridlock was happening. I don't know if it's the day you're talking about. And what I did was I took many of the bridges and tunnels and I reversed the lanes so all lanes went outbound. Imagine Manhattan now as a bathtub. GROSS: (Laughter). SCHWARTZ: And I turned all the faucets off that were bringing traffic in. I didn't let traffic into Manhattan for over an hour, and let the traffic go out the drain. I drained the traffic from Manhattan. So you need to do that. What happens on a daily basis is literally the traffic cop manually wrestling with the traffic to get moving. And sometimes it could last for many hours. And I actually keep a record of the 10 worst gridlocks to ever hit New York City. GROSS: Was one of them around Christmas one year? SCHWARTZ: Yes, it was around Christmas one year. It was on a matinee day. And what happened was everybody came into the city for the matinee. And many of them were not regular drivers. And they ended up blocking each other, and we had so many pedestrians at the same time. Nothing moved. And ultimately, the cops had to come and wrestle the traffic. GROSS: I think that's the gridlock that I witnessed. (LAUGHTER)GROSS: Do a lot - did a lot of people hate you in New York 'cause you were regulating traffic in ways that helped some people but hurt others? SCHWARTZ: Well, yeah. I mean, a lot of people have complained to me. And even today - I mean, I'm long gone. But during my tenure under my supervision, ultimately, we wrote about 50 million summonses and towed about 8 million cars. So I was not terribly popular. And also when I inherited the city's infrastructure, I closed so many bridges that were in danger of falling - and a few of them did fall - that people saw me and got scared and perhaps went the other way. Or I created bridge phobia which I think created a business for a lot of psychiatrists. GROSS: (Laughter) All right. You know, I - I'm leaving this interview so conflicted because I see all the points you're making, but I drive. I use parking ramps when I need to. I take taxis or, you know, app car services when I need to. I use all of that, and I use it at my convenience for my needs. And you're making me feel, like, a little guilty about everything that I do. But on the other hand, we all need to get to where we're going in the best way that we can. So I don't really have a question here. Maybe this is more of a confession. I don't know what it is (laughter). SCHWARTZ: Well, I end the book with a quote by John F. Kennedy when he was president - the car is not the villain. What he called for was balanced transportation, and that's what I'm calling for. There are absolute times I take an Uber. I'm multi-modal man. I take a bus. I take a subway. I drive. I do all those things. What we need is a better balance, in that the pendulum just swung in the direction of too many people driving. If the pendulum goes back, taxis will flow better. Those people that are driving will get to their destinations faster. Public transportation will have more revenue. There is a better way. GROSS: Sam Schwartz, a pleasure to talk with you. Thank you so much. This is really interesting. I appreciate it. SCHWARTZ: Thank you. And please don't feel guilty the - when next time you drive. GROSS: (Laughter) OK. SCHWARTZ: (Laughter). GROSS: Sam Schwartz is the author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future. \" He's a former traffic commissioner of New York City. This is FRESH AIR. TERRY GROSS, HOST:  This is FRESH AIR. I'm Terry Gross. The future of the driverless car is going to affect the future of how we travel and what we do in cars. But driverless cars are also likely to transform roads, cities, suburbs, jobs, the economy and daily life. My guest Samuel Schwartz expects it to be a very disruptive technology. Schwartz is the author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future,\" which he says is about the good, the bad and the ugly of how driverless cars will change our world. He knows a lot about transportation systems. He served as the traffic commissioner of New York City and chief engineer of the city's Department of Transportation. He now has his own consulting firm and has worked with cities around the world on transportation-related issues. Later in our conversation, after we talk about the future, we're going to talk about traffic problems that plague us today. We're going to use the words driverless car interchangeably with the words autonomous vehicle, or AV. Sam Schwartz, welcome to FRESH AIR. In your book, you write that AVs, autonomous vehicles, will be the most disruptive technology to hit society worldwide since the advent of the motorcar. Give us a couple of examples of industries or jobs or roadways that we might not realize will be profoundly affected by AVs once they start to really dominate. SAMUEL SCHWARTZ: I think everybody is expecting fewer drivers, and, you know, that's no surprise. But it also means that there're probably going to be fewer repair shops because AVs lend themselves to fleet operations, especially if they're going to be offering rides, as opposed to selling maximum vehicles. So car dealerships may disappear. So this is going to have wide impacts. Truckers, of course, are going to be impacted - how we move about in so many different ways. But lots of industries will be affected. The insurance industry, certainly, will be affected since we will have fewer crashes, and about a third of the insurance industry is based on crashes. And if we have fewer crashes, there are going to be fewer cases in court. There'll be less of a burden on the court system. GROSS: So kind of nobody's going to have a job, is what (laughter) - is what you're saying. SCHWARTZ: No, not quite, but. . . GROSS: You know, I've always - until reading your book, I thought of AVs - autonomous vehicles, driverless cars - as looking a lot like my car, except that I wouldn't have to sit in the driver's seat, or if I was in the driver's seat, I wouldn't have to do anything. But as you point out in your book, once you have an autonomous vehicle, you get to rethink the whole design of the car and probably also what the car is made of. I mean, let's face it. Our cars are designed for, like, the gas engine under the hood, and the steering wheel and the brake pedal. So give us a sense of, visually - like, of how a car might be designed differently. SCHWARTZ: A lot of people have the image that you have, Terry. And a lot of them look - they kind of look - the Google car looks like a smaller car, like a smart car, and it's got some intelligence on top of it. But I liken this a bit to the cellphone. In 1982, I was traffic commissioner. I had one of the first cellphones that I could use during emergencies. And the cellphone came - with a man carrying it - on a 12-volt battery and this huge phone. And today - the cellphone of today doesn't look anything like the cellphone that I had in 1982. The same thing can happen with cars. It could - the autonomous vehicle of the future, most people think that by the second half of the century, at least, and maybe sooner, will have no steering wheel, will have no brake. It will be a room. It could be a room any size. It could be a conference room. It could be, you know, the width of a house. It - there's no reason to think it's going to look like a car, other than that's what we're used to. So others are beginning to think of these vehicles as being something totally different - different uses. And I see that as a real potential - as a meeting place, a sleeping place, an eating place. Medical facilities will come to you. It'll look like a hospital room or a doctor's office - a place to be examined by a doctor remotely in which a remote stethoscope listens to your heart, and your blood pressure is taken, and all those types of things are possible. Some of this is good. But we should be - go forward with our eyes wide open. GROSS: What's your assessment so far of the safety of autonomous vehicles and what that would be like in the future? I mean, on the one hand, you don't have a driver's misjudgment or a driver falling asleep at the wheel, so that could eliminate certain crashes. On the other hand, technology is always subject to failure, and computers often mess up, and they - you know, everybody's been on the computer when it just kind of freezes, and you can't work anymore. So imagine that happening to your car. SCHWARTZ: And it could happen, and, of course, hacking could happen. But many of the features that autonomous vehicles are - operators are claiming are needed, you don't need in the autonomous mode. So there is automatic braking features. There are lane-control features. There are blind-spot monitoring. There's a whole host of things that are being offered today to people who could afford it. So the low-income people are not getting all these safety features that could cut down on crashes tremendously. And then you take the driver out of it. Yes, you'll - you can reduce crashes further because you don't have to worry about the judgment of a driver, but you may create other types of crashes. Right now, the industry isn't sharing anything. This is really an industry that has tons and tons of data, but they're not sharing it with the public. The only data that's accessible is the state of California, which is reporting anytime there is a crash when a vehicle is in autonomous mode. And so far, the results are not good. So far, we know of three deaths, and not because of any reporting. That's the media reporting of three deaths in autonomous mode. For conventional cars, that would take 260 million miles before that would occur. And here we have three deaths, and AVs have driven maybe 10 or 15 million miles. In California, which requires the reporting of crashes, the cars in autonomous mode are crashing nine or 10 times more often than the conventional cars. Will they solve a lot of that? Yes, they're going to. They could solve a lot of the safety problems in the next few years. Everybody could have automatic braking. Everybody could have lane-control protections and could have blind-spot monitoring and other safety features. But there is this meme that we have to wait until we have autonomous vehicles for safety, instead of the meme that maybe we could do a lot now. GROSS: So what kind of features are you talking about? SCHWARTZ: Right now, in - for example, I have a car, a 2017 Volvo, that will stop on its own if I'm not paying attention and about to rear-end another car. There are features that will let me know if I am leaving the lane. The car itself will tug and tell me that I'm crossing a lane. And so if I'm drowsy, if I'm not paying attention or if I had too much to drink, which I would never do, I would know that. I know that there's a warning if there's a car in my blind spot and I'm trying to change lanes. So there are lots of features that exist today that if the industry that is claiming, safety, safety, safety, and that's why we need AVs, would really focus on safety, they'd say, we could have - by 2022, every single car sold in America could be the safest car possible, and we'll make it even safer eventually with autonomous vehicles. But so far, the track record is not very good with autonomous vehicles. They can't figure out what a pedestrian is or the pedestrian is going to do. They can't separate a child from a dog. There's sometimes - a tree branch overhanging the road will be taken as something in the way. So, you know, we're far from perfecting. GROSS: How good are AVs at recognizing a human being? SCHWARTZ: They're in the high 90s - they will - percent-wise. But there are times that they'll have a false reading, or they will still say, I think this is a person. And this is what happened with the Uber crash in Tempe, Ariz. , that killed a woman. The woman was walking with a bicycle. So you would think, A, it would recognize the woman and, two, it would also recognize a bicycle. But the system said, hmm, I'm not sure what this is; it doesn't look right to me, so I don't want to stop suddenly because that also contributes to crashes; I'm going to plow right ahead. And it plowed right ahead and killed her. GROSS: Wow. Yeah. I think I didn't understand how that happened. My guest is Samuel Schwartz. He's the author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future. \" He is the former New York City traffic commissioner and the former chief engineer of New York City's Department of Transportation. We'll be right back. This is FRESH AIR. (SOUNDBITE OF MUSIC) GROSS: This is FRESH AIR. And if you're just joining us, my guest is Sam Schwartz. He's the author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future. \" He knows a lot about roads and cars and traffic. He's New York City's former traffic commissioner and former chief engineer of the Department of Transportation. He has his own consulting firm now. So let's talk about how AVs could affect transportation in terms of, like, traffic. And, you know, driverless cars are great if they're not stuck in traffic (laughter). And they're going to affect the amount of traffic on the road. Then they'll also affect what the road looks like. You lay out a scenario. There's a family with a couple of kids, two parents. The AV takes one parent to work, drops them off work, comes back home, picks up the kids, takes them to school, drops them off at school. The autonomous vehicle comes back home, picks up the second parent, takes them to work. And then the car does - I'm not sure what. I'm not sure where it stays (laughter) in between all of these trips. But you point out, in between trips, you have an empty car. You have what you describe as, like, a zombie car. So what does that mean in terms of, like, the number of cars that's going to be on the road? SCHWARTZ: Well, one way to look at the future is the intent of many people in the AV industry is to replicate Uber, Lyft, Via and other car services but without a driver. So in lots of cases, you - it would be an app or whatever succeeds an app. You will call for a car. The difference between that and an Uber today is there will be no driver. So the - we've already been able to determine a lot of things about these new app-based services. The average app-based service, when it carries you 1 mile, it needs to travel 1. 6 miles. So a 10-mile trip means that vehicle would have traveled 16 miles over the course of the day. And that's because it's empty during portions of the day. The car has to get into position to be near where there's density of people. So we already know it increases vehicle miles traveled. We also know that there's a concentration - unless we're smart, there's a concentration of where the well-to-do are. It's a concentration in most cities like New York, like Philadelphia, like San Francisco and Chicago that a lot of the well-to-do are now near or in city centers, where we already have the best transportation and the best transit systems. So what we've been finding is the congregation of the Ubers and Lyfts, A, where we already have good transportation service, B, where many of our wealthiest citizens live and, C, where we already had the worst traffic. So the future, unless we change that model, is quite bleak because there'll be many, many more autonomous vehicles out there because it'll be so inexpensive to run them, especially if they're run as a fleet. There'll be no downtime. The average car today is in - 95 percent of the time sits idle. These cars will be moving around doing their job 80, 90 percent of the time. So far more vehicles will be on the road. The other thing is there is this hype from the industry that autonomous vehicles are so efficient, they could follow each other so closely that we could replicate on a highway the capacity of a train. That's nonsense. A train or a bus in an exclusive lane could move 100 people in 60 feet, or certainly a subway car, whereas an autonomous vehicle, unless we change our behavior, is going to have one person per car - even if it has one person per car - it may have zero, less than that - will be moving, at most, two or three people in that same distance. So it - the math doesn't work. GROSS: So are you, like, making an argument here that, in spite of AVs, we need good public transportation, like trains that can move a lot of people at one time with high density within each car? SCHWARTZ: Autonomous vehicles are coming. There's no doubt about it. But we should maintain good public transportation systems. We should - and I use the adjective good because there are bad public transportation systems out there. Ninety percent of the country has lousy public transportation. It's called a bus that comes around every half hour or hour. It largely serves poor people. When a system only serves poor people, it's a poor system. Often it's largely people of color. We have an opportunity to transform public transportation in those areas, not to offer a half-hour service, but we can triple the service by using micro-transit AV vehicles, small buses that are smart buses that are on-demand that know where the people are, know where they want to get off. We already have that technology. We have a company here called Via that is doing it in New York, Chicago and a few others. We have Ford Chariot that's beginning to do it in San Francisco and a few other places. The technology is here. So AVs could be terrific for lower-density public transportation. But we're going to still need, in the big cities or in places where we want to move lots of people - the Northeast Corridor - you're going to still need trains. You're still going to need subways, streetcars. We can't substitute cars for that kind of service. GROSS: So, you know, we're talking about cars changing. And we're talking about the road infrastructure. What kinds of new highways will driverless cars require? Like, how might highways change? SCHWARTZ: If we're really smart, we can have less infrastructure if autonomous vehicles really look something like cars. If they have a width of 6 feet as cars have today, if you have a three-lane highway - the average three-lane highway is 36 feet - you're probably going - three 12-foot lanes - you're only going to need, say, 21 feet. So you can either add more lanes and add more capacities to highways or do something else with all that land that will be unleashed and not have to maintain as much infrastructure. GROSS: But let me interrupt you for a second because you are saying that we're going to be able to, like, eat and sleep and have romantic evenings in our car. So that requires room. But if you're making the car smaller because there isn't going to be a driver, then there isn't going to be a - you know, space to, like, live in your car the way you described it before. SCHWARTZ: Yeah. So I go through the good, the bad, the ugly. GROSS: OK (laughter). SCHWARTZ: What I'm painting right now is the good. GROSS: OK. SCHWARTZ: And the good is they look like cars. They don't look like conference rooms. They don't look like houses. They don't look like coffee shops. They look like cars. What a great opportunity if they stay in - looking like cars because of the narrowness of cars. They're 6 feet wide, but we have 12-foot lanes because people weave back and forth. They're imprecise in their driving. They're not paying attention. An autonomous vehicle could be like a tracked vehicle - stay in lane. So for - a 6-foot vehicle could have a 7-foot lane. So in that three-lane highway that's 36 feet - you can get a three-lane highway down to 21 feet, and it'll be a more efficient highway in terms of moving vehicles 'cause the autonomous vehicles could be more closely together. GROSS: GM recently announced plans to shut down five factories in North America and cut about 14,000 jobs. Earlier this year, Ford announced it will stop making sedans. In 2016, Fiat Chrysler stopped making small and mid-sized cars. I think GM is also going to be cutting back its, you know, mid-sized cars. And the emphasis seems to be in two directions - one on AVs, autonomous vehicles, and the other on, like, trucks and SUVs. So we seem to be heading in two opposite directions at the same time in a way. Like, the most fuel-efficient and the least (laughter) fuel-efficient cars and, you know, more lightweight autonomous vehicles and incredibly, like, big and heavyweight larger vehicles. Do you see us heading in two opposite directions at the same time? SCHWARTZ: Yeah. I'm very disappointed in that the auto industry - and that, you know, they're reflecting the desires of the American people to be in bigger cars, to be in heavier vehicles and to have these front ends that are so high. That has an impact. That's why pedestrian deaths - that's one of the main reasons pedestrian deaths have soared - is that people are getting hit by SUVs, getting hit not - no longer with knee injuries - and this comes from ER doctors who've seen me, who talk to me. There's a real connection between the health industry right now and transportation. They say in the past, someone got hit by a car, it was a knee injury. It was leg injury. Now it's a chest injury, and those are more likely to be fatal. So I'm really not happy about that. But yes, it is going to be a problem - moving heavier vehicles and trying to consume less energy. GROSS: My guest is Sam Schwartz, author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future. \" He's the former traffic commissioner of New York City. And after a break, we'll talk about the kinds of traffic nightmares we face today. I'm Terry Gross, and this is FRESH AIR. (SOUNDBITE OF CHARLES MINGUS'S \"A FOGGY DAY\") GROSS: This is FRESH AIR. I'm Terry Gross. Let's get back to my interview with Sam Schwartz, author of the new book \"No One At The Wheel\" about driverless cars and how they're likely to change the future of transportation, roads, jobs, cities and daily life. Now we're going to talk about the traffic nightmares we face today. Schwartz is an expert on that. He served as traffic commissioner of New York City and the chief engineer of the city's Department of Transportation. He now has a consulting firm which works with cities in the U. S. and other countries on transportation and traffic-related issues. There are so many changes going on right now in who drives and what they drive and what the roads are like, and I'd like to spend some time talking about that 'cause I'm as interested in what's happening right now as I am as what's going to happen in the future as more and more AVs come onto the road. And you know a lot about what's happening now because your consulting firm advises, you know, different cities around the world how to deal with current changes. So let's look at what's happening in some places in America right now. As you point out, millennials are generationally less likely to own a car or even drive a car than previous generations. So let's start with, why do you think that is? SCHWARTZ: For a variety of reasons. And I did write a previous book, \"Street Smart,\" in which I interviewed a lot of millennials. They came up with lots of reasons - you know, the cost, the - certainly the availability of other forms of transportation with the apps that you have today. The loans that they still have from college make it difficult. The - there was also an interesting thing that I learned talking to millennials - is they saw it as old-fashioned. They saw it from the back of the car. Many of them grew up in suburbs - saw very unhappy parents fighting over who's going to drive the kids to soccer practice and school and to the dentist and all of those things. And they didn't - that's not a lifestyle that they want. So there's been a big 20 percent drop in millennials. It doesn't mean the majority don't drive. The majority do drive. But it's gone from 90 percent driving to 70 percent driving. A 20 percent drop is like an earthquake in transportation. GROSS: And of course bicycles have become much more popular. But also millennials are taking advantage of app-based car services like Uber and Lyft. And I'm wondering how you think app-based car services like Uber and Lyft are affecting larger traffic patterns in urban areas. SCHWARTZ: Yeah, you know, I've been in the transportation business for a half century. I was a New York City cab driver back in 1968. And I watched transportation evolve over time. I have never seen anything as rapid as what has happened this decade. So Uber and Lyft - we were caught flatfooted, all those of us in city planning. We never saw it coming. We never thought it would be as popular as it is. But it is so popular that in five years, a brand new service eclipsed the number of people taking taxi rides in just about every city from taxi industries that have been around for a hundred years. It has rapidly increased the amount of miles driven in the densest part of cities, so it's made congestion much worse in those places. And it's not provided the kind of service improvements in transit since it's taking a lot of people out of transit. In New York City, perhaps half the people in Uber and Lyft come out of transit. In California, which is really a car capital of the world, it's 33 percent of Uber and Lyft drivers - Lyft passengers say that they came out of public transportation. In Denver, in Boston - any city, it's the same thing. So we're putting more cars on the road and we're putting them in the worst traffic spots. So it's been a shock to the system. If we can go to - when these services came in, they talked about taking people to transit. That would be the best use of these services. GROSS: Another thing that I think is really changing the roads in cities are the delivery services, like the trucks that deliver groceries to your door through various apps and the bicycles that deliver food from restaurants through various apps to your door. And the delivery trucks - you know, when they double-park, that takes up a lot of space. And I know they're only running in and out, but it still kind of backs up traffic for that amount of time. So what's your impression about how the convenience that so many of us are taking advantage of through these delivery services, including deliveries on things that - you know, on goods that you order through the Internet - how these delivery trucks are affecting traffic? SCHWARTZ: They're having a big impact, these micro-deliveries where you can have a - you know, toothpaste delivered, and that could be the only thing in the package. It adds to many more trucks. So we're seeing an increase in the number of trucks that are traveling in cities. It may work well in - again, in low-density areas, but in cities, it's been a problem. What we're seeing is some of the fulfillment centers are now using Uber and Lyft to make these micro-deliveries so, again, adding far more traffic. If we go back, you know, again, to the last century, people walked to stores. People drove to stores. People then collected and bought far more goods than they would at one time than they are now getting, every single day, a little package delivered. GROSS: I think bicycles are having a huge impact on traffic, especially millennials who don't have a car and don't always need or want to take, you know, a vehicle service like Uber or Lyft. You know, you bicycle, but many urban roads are not really designed for bicycles. Where I live, for instance, in Philadelphia, it's an old city. It's a Ben Franklin-era city in some ways. And the streets are very narrow except for a couple of the main streets in Philly. And to squeeze in parked cars and drivers and bikes, you know, on one road is often really challenging. And there's wonderful things to be said environmentally and in terms of convenience about the positive nature of bicycles, but it's also, I think, having some adverse effects for life on the roads. SCHWARTZ: Well, you know, I - again, I'd like to change the mindset a bit and remember that for hundreds of thousands of years, since the first cities started, it was the pedestrians that were walking, and the carriages had to go around them or move at the same speed. And somehow we've accepted the fact that we need lots of cars, lots of vehicles, and the intruders are the bike riders and the pedestrians, and they shouldn't be second-class citizens. What happened in cities like New York City, Seattle and others that really went - I wouldn't say all out but at least in their central business districts provide far more pedestrian zones, far more bicycle lanes, the total number of cars coming in has gone down. And so we can get by with fewer cars. If we get by with fewer cars, then suddenly maybe Chestnut and Walnut in Philadelphia could have different uses. Maybe the cars go very slowly, which they do anyhow. But if you go to many European cities, you'll see the mix of pedestrians and cars moving very slowly. We may have to use examples like that, but we have to change our mindset. GROSS: So what is your greatest frustration now as a driver when you're driving as opposed to using public transportation? SCHWARTZ: You know, having been traffic commissioner, having been in charge of traffic enforcement, my blood boils when I see violations. And when I see violations where people are illegally parked or people are turning against a red light when they're not permitted, those things really drive me crazy. In New York City, we have government workers who drive in at twice the rate of anyone else because they get placards to park their cars. That drives me nuts. If I were back as traffic commissioner, I'd be telling all of them - in fact, I did that, and I told so many diplomats that I was invited to what I am told is the best-attended session of the U. N. General Assembly. GROSS: (Laughter) They wanted you to fix their tickets (laughter). SCHWARTZ: I had the - the Israelis and Arabs were on the same side as. . . GROSS: (Laughter). SCHWARTZ: . . . Iraq and Iran in the 1980s. And it was just me being interpreted into a dozen languages. And one who would've gone by and just seen that scene not hearing what was going on would assume this lone guy at the table that the whole world was focusing on had the answer to world peace. It was all about parking. GROSS: That is really, really hilarious. Let me reintroduce you here. If you're just joining us, my guest is Sam Schwartz. He's the author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future. \" He's also New York City's former traffic commissioner and the former chief engineer for New York City's Department of Transportation. We'll be right back. This is FRESH AIR. (SOUNDBITE OF JERRY GRANELLI, ROBBEN FORD, BILL FRISELL AND J. ANTHONY GRANELLI'S \"AIN'T THAT A SHAME\") GROSS: This is FRESH AIR. And if you're just joining us, my guest is Sam Schwartz, author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future. \" He's also New York City's former traffic commissioner and former chief engineer for New York City's Department of Transportation. He now has his own consulting firm. So let's talk briefly about your time as New York City traffic commissioner and chief transportation engineer in New York. One of the things I've always wondered is, how do you get traffic lights synced up? 'Cause I could see the possibility of syncing them up, say, going north-south, but what about the intersecting streets going east-west? Like, and you were doing that before, like, personal computers. So just tell us a little bit about the difficulties of making that happen. SCHWARTZ: Actually, I learned how to do signal timing right near you at the University of Pennsylvania 50 years ago, and we did it by using graphs. And we used the speed and distance graphs to figure out - if a vehicle is moving at 30 miles an hour in six seconds, it'll be at the next street so we have to change the signal at the next street. Now, that's easy to do when you have just a linear problem. But when you have a grid problem, how do you do it crosstown? Well, we soon learned that we couldn't do it on every street, but we could do it on every fourth street. So it's a mathematical problem. But ultimately, the conclusion that I came to in the 1980s, and that's after a dozen years at - working at the traffic department, was that we're not going to solve the traffic problem by moving more cars and that the best signal program for an area like Midtown Manhattan or center city is one that recognizes when the number of vehicles in the center city or the Midtown area is getting to a critical volume. And at that point, begin to slowly entry of vehicles into it using the signal system kind of as a belt tightening the number of cars coming in. I first learned that in the city of Nottingham, England, which did that in the 1970s. I call it metering of high-density sectors, but we can't solve the problem of north, south, east, west. We really have to solve the problem as a grid problem. GROSS: Shortly after you popularized the word gridlock - and you are the guy who popularized it - I was witness to an amazing gridlock in Manhattan. And I was fortunately a pedestrian at that moment (laughter) - not in a car. But it was a symphony of horns honking. And like, all around, the intersections were just totally bumper-to-bumper with cars. There was no place for any car in the area to move. And I always wondered, like, how does a gridlock like that - when it's, like, a super gridlock, how does it end? SCHWARTZ: OK. Well, it starts because people are piggish. It starts because people block intersections which then block the perpendicular movement. And then. . . GROSS: Oh, you're trying to make it through the (laughter) - you're trying to make it through the right. SCHWARTZ: Right. GROSS: And so is the car. . . SCHWARTZ: Yes. GROSS: . . . In front of you. And yeah. And that's really a recipe for gridlock. SCHWARTZ: Right, right. So. . . GROSS: So yeah. So a gridlock's created. SCHWARTZ: Right. So a bird's-eye view would show you're blocking yourself. So the key to that - and I've witnessed many gridlocks - is you have to slow the traffic coming into the area. On April 9, 1980, there was a thunderstorm - a terrible storm during the transit strike. And gridlock was happening. I don't know if it's the day you're talking about. And what I did was I took many of the bridges and tunnels and I reversed the lanes so all lanes went outbound. Imagine Manhattan now as a bathtub. GROSS: (Laughter). SCHWARTZ: And I turned all the faucets off that were bringing traffic in. I didn't let traffic into Manhattan for over an hour, and let the traffic go out the drain. I drained the traffic from Manhattan. So you need to do that. What happens on a daily basis is literally the traffic cop manually wrestling with the traffic to get moving. And sometimes it could last for many hours. And I actually keep a record of the 10 worst gridlocks to ever hit New York City. GROSS: Was one of them around Christmas one year? SCHWARTZ: Yes, it was around Christmas one year. It was on a matinee day. And what happened was everybody came into the city for the matinee. And many of them were not regular drivers. And they ended up blocking each other, and we had so many pedestrians at the same time. Nothing moved. And ultimately, the cops had to come and wrestle the traffic. GROSS: I think that's the gridlock that I witnessed. (LAUGHTER) GROSS: Do a lot - did a lot of people hate you in New York 'cause you were regulating traffic in ways that helped some people but hurt others? SCHWARTZ: Well, yeah. I mean, a lot of people have complained to me. And even today - I mean, I'm long gone. But during my tenure under my supervision, ultimately, we wrote about 50 million summonses and towed about 8 million cars. So I was not terribly popular. And also when I inherited the city's infrastructure, I closed so many bridges that were in danger of falling - and a few of them did fall - that people saw me and got scared and perhaps went the other way. Or I created bridge phobia which I think created a business for a lot of psychiatrists. GROSS: (Laughter) All right. You know, I - I'm leaving this interview so conflicted because I see all the points you're making, but I drive. I use parking ramps when I need to. I take taxis or, you know, app car services when I need to. I use all of that, and I use it at my convenience for my needs. And you're making me feel, like, a little guilty about everything that I do. But on the other hand, we all need to get to where we're going in the best way that we can. So I don't really have a question here. Maybe this is more of a confession. I don't know what it is (laughter). SCHWARTZ: Well, I end the book with a quote by John F. Kennedy when he was president - the car is not the villain. What he called for was balanced transportation, and that's what I'm calling for. There are absolute times I take an Uber. I'm multi-modal man. I take a bus. I take a subway. I drive. I do all those things. What we need is a better balance, in that the pendulum just swung in the direction of too many people driving. If the pendulum goes back, taxis will flow better. Those people that are driving will get to their destinations faster. Public transportation will have more revenue. There is a better way. GROSS: Sam Schwartz, a pleasure to talk with you. Thank you so much. This is really interesting. I appreciate it. SCHWARTZ: Thank you. And please don't feel guilty the - when next time you drive. GROSS: (Laughter) OK. SCHWARTZ: (Laughter). GROSS: Sam Schwartz is the author of the new book \"No One At The Wheel: Driverless Cars And The Road Of The Future. \" He's a former traffic commissioner of New York City. This is FRESH AIR.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-11-675648155": {"title": "Huawei Executive Granted Bail In Canada, Former Canadian Diplomat Is Detained In China : NPR", "url": "https://www.npr.org/2018/12/11/675648155/huawei-executive-seeks-bail-in-canada-former-canadian-diplomat-is-detained-in-ch", "author": "No author found", "published_date": "2018-12-11", "content": "", "section": "World", "disclaimer": ""}, "2018-12-11-675543073": {"title": "Google CEO Says He Leads 'Without Political Bias' In Congressional Testimony : NPR", "url": "https://www.npr.org/2018/12/11/675543073/google-ceo-says-he-leads-without-political-bias-in-congressional-testimony", "author": "No author found", "published_date": "2018-12-11", "content": "", "section": "Business", "disclaimer": ""}, "2018-12-11-675529798": {"title": "Google+ Shutdown Accelerated After 52.5 Million Users' Data Exposed : NPR", "url": "https://www.npr.org/2018/12/11/675529798/with-52-5-million-users-data-exposed-on-google-google-quickens-shutdown", "author": "No author found", "published_date": "2018-12-11", "content": "", "section": "Technology", "disclaimer": ""}, "2018-12-12-675983642": {"title": "Chinese Hackers Are Likely Responsible For Marriott Data Breach, Reports Say : NPR", "url": "https://www.npr.org/2018/12/12/675983642/chinese-hackers-are-responsible-for-marriott-data-breach-reports-say", "author": "No author found", "published_date": "2018-12-12", "content": "", "section": "Business", "disclaimer": ""}, "2018-12-13-676366640": {"title": "Apple Will Build $1 Billion Campus in Austin, Adding 5,000 Jobs : NPR", "url": "https://www.npr.org/2018/12/13/676366640/apple-will-build-1-billion-campus-in-austin-adding-5-000-jobs", "author": "No author found", "published_date": "2018-12-13", "content": "", "section": "National", "disclaimer": ""}, "2018-12-14-676773863": {"title": "Facebook Says Some Users' Private Photos Were Accidentally Shared With Developers : NPR", "url": "https://www.npr.org/2018/12/14/676773863/facebook-says-some-users-private-photos-were-accidentally-shared-with-developers", "author": "No author found", "published_date": "2018-12-14", "content": "", "section": "Technology", "disclaimer": ""}, "2018-12-17-672976298": {"title": "Teen Girls And Their Moms Get Candid About Phones And Social Media : NPR", "url": "https://www.npr.org/2018/12/17/672976298/teen-girls-and-their-moms-get-candid-about-phones-and-social-media", "author": "No author found", "published_date": "2018-12-17", "content": "AILSA CHANG, HOST:  And now it's time for All Tech Considered. (SOUNDBITE OF MUSIC)CHANG: This month as we get ready to say goodbye to 2018, we're taking stock of the year in tech. MARY LOUISE KELLY, HOST:  Today, we chart the fear that grew over 2018 - the fear of using tech too much. Consider a few dates. January 6, Apple shareholders write a letter demanding that the company address addiction and compulsive use. June 18, the World Health Organization classifies gaming disorder as a mental health condition - not unlike gambling. CHANG: And November 28, an annual survey by Brigham Young University and the Deseret News showed that parents of teenagers fear tech overuse more than they fear sex or drug use by their teenagers. NPR's Anya Kamenetz has been following all of these developments on her smartphone screen. And she joins us now. Welcome. ANYA KAMENETZ, BYLINE: Hi, Ailsa. CHANG: Should we be worried? Is there a consensus among health experts on how screen time is actually harmful for our health? KAMENETZ: A consensus, not exactly - so the World Health Organization now does say, yes, video games can be a behavioral addiction not unlike gambling. Here in the United States, the psychiatric profession still lists Internet over use as a, quote, \"condition for further study. \" So they haven't fully jumped on board with that. And the doctors I talked to you, they use the term problematic use, which could be social media or of games or general use of the Internet. And that's defined pragmatically as any use that causes problems in other areas of your life. It could be work or sleep or relationships. Maybe you're lying, or it's causing fights at home. And, you know, these kinds of problems often come hand in hand with other conditions - like depression or anxiety or conditions like ADHD or even being on the autism spectrum. CHANG: Well, we mentioned that the year began with this shareholder letter to Apple. Can you just update us on what is Apple - and what are other tech companies doing to address this growing concern about tech addiction? KAMENETZ: Yeah. So Android and Apple both came out with new tools this year to track your use of the phone. And the iPhone now automatically shows you things like how often you pick up your phone. And in June, Apple CEO Tim Cook told NPR. . . (SOUNDBITE OF ARCHIVED BROADCAST)TIM COOK: I think there are cases in life where anything good used to the extreme becomes not good. KAMENETZ: Apple of course makes its money primarily from selling devices. So for companies like Facebook or Snapchat that make most of their money essentially directly from the time you spend inside the app, it might be a lot more difficult for them to promote people cutting back. CHANG: Then with all the growing attention on this, are people actually changing their behavior? I mean, I feel like for me my reliance on devices is only getting worse. KAMENETZ: I know what you mean. And, you know, I checked in with some families about this. This is Bella Butler who is 16 and lives in Brooklyn. She's talking about her phone. BELLA BUTLER: I enjoy being on it. At the same time, I also know what it's doing to me. Like, I know that it actually causes me a lot of anxiety. CHANG: Totally relate to that. KAMENETZ: Me too. Me too. And so it's anxiety from FOMO - the fear of missing out. Anxiety from feeling like she's scrutinized for every little photo that she posts. And then there's losing sleep because she's Snapchatting with her friends. And so growing up, it does feel different in some ways. And this was a big issue - also the lack of sleep for Yassiry Gonzalez who lives in the Bronx. She's also 16. And she says her mom can't control her phone use. YASSIRY GONZALEZ: She won't really know that I'm on my phone because I go to sleep early, and then I wake up in the middle of the night. KAMENETZ: What time would you wake up? YASSIRY: Like 1, 2 in the morning. CHANG: She'll wake up at 1 or 2 to use her phone? KAMENETZ: Yeah. And sometimes she says she stays up the whole rest of the night - especially on the weekends. CHANG: And her mom has no clue it's to use her phone. KAMENETZ: It's not so much that she has no clue but just that she doesn't know what she can do about it. I hear these mixed feelings from a lot of the teens I meet actually. The latest Pew Research Center poll shows more than half of all teens have taken a break from their phones at some point. CHANG: An extended break? KAMENETZ: Well, it depends. I mean, it can be very, very hard to put it down especially if you need it for school or for keeping up with friends. But at least they're recognizing that there's something they'd like to change - maybe it's a New Year's resolution at some point. CHANG: That's NPR's Anya Kamenetz. Thanks so much. KAMENETZ: Thanks, Ailsa. AILSA CHANG, HOST:   And now it's time for All Tech Considered. (SOUNDBITE OF MUSIC) CHANG: This month as we get ready to say goodbye to 2018, we're taking stock of the year in tech. MARY LOUISE KELLY, HOST:   Today, we chart the fear that grew over 2018 - the fear of using tech too much. Consider a few dates. January 6, Apple shareholders write a letter demanding that the company address addiction and compulsive use. June 18, the World Health Organization classifies gaming disorder as a mental health condition - not unlike gambling. CHANG: And November 28, an annual survey by Brigham Young University and the Deseret News showed that parents of teenagers fear tech overuse more than they fear sex or drug use by their teenagers. NPR's Anya Kamenetz has been following all of these developments on her smartphone screen. And she joins us now. Welcome. ANYA KAMENETZ, BYLINE: Hi, Ailsa. CHANG: Should we be worried? Is there a consensus among health experts on how screen time is actually harmful for our health? KAMENETZ: A consensus, not exactly - so the World Health Organization now does say, yes, video games can be a behavioral addiction not unlike gambling. Here in the United States, the psychiatric profession still lists Internet over use as a, quote, \"condition for further study. \" So they haven't fully jumped on board with that. And the doctors I talked to you, they use the term problematic use, which could be social media or of games or general use of the Internet. And that's defined pragmatically as any use that causes problems in other areas of your life. It could be work or sleep or relationships. Maybe you're lying, or it's causing fights at home. And, you know, these kinds of problems often come hand in hand with other conditions - like depression or anxiety or conditions like ADHD or even being on the autism spectrum. CHANG: Well, we mentioned that the year began with this shareholder letter to Apple. Can you just update us on what is Apple - and what are other tech companies doing to address this growing concern about tech addiction? KAMENETZ: Yeah. So Android and Apple both came out with new tools this year to track your use of the phone. And the iPhone now automatically shows you things like how often you pick up your phone. And in June, Apple CEO Tim Cook told NPR. . . (SOUNDBITE OF ARCHIVED BROADCAST) TIM COOK: I think there are cases in life where anything good used to the extreme becomes not good. KAMENETZ: Apple of course makes its money primarily from selling devices. So for companies like Facebook or Snapchat that make most of their money essentially directly from the time you spend inside the app, it might be a lot more difficult for them to promote people cutting back. CHANG: Then with all the growing attention on this, are people actually changing their behavior? I mean, I feel like for me my reliance on devices is only getting worse. KAMENETZ: I know what you mean. And, you know, I checked in with some families about this. This is Bella Butler who is 16 and lives in Brooklyn. She's talking about her phone. BELLA BUTLER: I enjoy being on it. At the same time, I also know what it's doing to me. Like, I know that it actually causes me a lot of anxiety. CHANG: Totally relate to that. KAMENETZ: Me too. Me too. And so it's anxiety from FOMO - the fear of missing out. Anxiety from feeling like she's scrutinized for every little photo that she posts. And then there's losing sleep because she's Snapchatting with her friends. And so growing up, it does feel different in some ways. And this was a big issue - also the lack of sleep for Yassiry Gonzalez who lives in the Bronx. She's also 16. And she says her mom can't control her phone use. YASSIRY GONZALEZ: She won't really know that I'm on my phone because I go to sleep early, and then I wake up in the middle of the night. KAMENETZ: What time would you wake up? YASSIRY: Like 1, 2 in the morning. CHANG: She'll wake up at 1 or 2 to use her phone? KAMENETZ: Yeah. And sometimes she says she stays up the whole rest of the night - especially on the weekends. CHANG: And her mom has no clue it's to use her phone. KAMENETZ: It's not so much that she has no clue but just that she doesn't know what she can do about it. I hear these mixed feelings from a lot of the teens I meet actually. The latest Pew Research Center poll shows more than half of all teens have taken a break from their phones at some point. CHANG: An extended break? KAMENETZ: Well, it depends. I mean, it can be very, very hard to put it down especially if you need it for school or for keeping up with friends. But at least they're recognizing that there's something they'd like to change - maybe it's a New Year's resolution at some point. CHANG: That's NPR's Anya Kamenetz. Thanks so much. KAMENETZ: Thanks, Ailsa.", "section": "All Tech Considered", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-18-677663325": {"title": "Pacific Garbage Patch: Boyan Slat's Ocean Cleanup Device Isn't Catching Trash : NPR", "url": "https://www.npr.org/2018/12/18/677663325/creator-of-floating-garbage-collector-struggling-to-capture-plastic-in-pacific", "author": "No author found", "published_date": "2018-12-18", "content": "", "section": "Technology", "disclaimer": ""}, "2018-12-19-678163901": {"title": "Report: Facebook Underreported Amount Of User Data Third Parties Had Access To : NPR", "url": "https://www.npr.org/2018/12/19/678163901/facebook-privacy-breach", "author": "No author found", "published_date": "2018-12-19", "content": "NOEL KING, HOST:  Facebook has drawn criticism for failing to protect users' privacy. Now, The New York Times is reporting that the social media giant allowed third parties access to users' personal data, largely without their consent. Joining us to talk more about this is Gabe Dance. he's one of the authors of today's report. Good morning, Gabe. GABE DANCE: Good morning. Thanks for having me. KING: So really blockbuster reporting in the Times this morning. What are the new allegations exactly? DANCE: So we have acquired a set of documents that date to 2010 and run through 2017 that show a large list of companies that Facebook has given special access to users' information. KING: And what kind of access are they giving? What kind of data is being shared? DANCE: Right. So, for example, Facebook allowed Microsoft's Bing search engine to see the names of virtually all Facebook users' friends without consent. They gave Netflix and Spotify the ability to read Facebook users' private messages. Amazon was able to obtain names and contact information through friends, which it was supposed to not be able to do. Yahoo still had access to users' posts despite them no longer needing that for over five years. So it's a long list. KING: Gabe, can you explain, is it unusual for Facebook to have shared the data of its users with other large companies? DANCE: It's not necessarily unusual, but there are a few key differences that's important. One is that Facebook has for - since it began, talked about how they protect your data. They've built their company on this idea that you're sharing data with your friends, and it's only with your friends. And then they've started to expand the definition of your friends basically without telling you to the point where, now, your friends include companies like Amazon and Yahoo and Netflix and some of the other ones. KING: And I'm not aware of that is the deal. DANCE: That's the second rub, which is that, generally, people are not aware of any of this. And, in fact, it looks like Facebook has clearly taken steps to make sure that that's the case. KING: Facebook's founder, Mark Zuckerberg, testified before Congress in April. He said, quote, \"we don't sell data to anyone. \" Obviously, this is not the sale of data. How are Facebook and Zuckerberg responding to your reporting so far? DANCE: Well, Facebook has interest in making sure - Facebook had to enter into a 2011 consent agreement with the federal government, and that was for deceptive practices. So Facebook has a lot on the line here. And what they're interested in doing is explaining these relationships as being OK because they say these companies acted as service providers, which is to say they acted as Facebook itself. Now, that is a confusing argument on its face. And it's an argument that is important to them, again, because of this federal agreement. But basically, they're trying to say, look, all of these people are essentially acting as Facebook itself, and therefore we're not exempting them from any privacy things because how could we exempt ourselves from privacy things? The challenge with that is convincing their users that BlackBerry, Yahoo, Apple and et cetera are Facebook as well. KING: Sounds like it's going to be a fairly hard sell. Gabe Dance of The New York Times, thanks so much. DANCE: Thank you for having me. NOEL KING, HOST:   Facebook has drawn criticism for failing to protect users' privacy. Now, The New York Times is reporting that the social media giant allowed third parties access to users' personal data, largely without their consent. Joining us to talk more about this is Gabe Dance. he's one of the authors of today's report. Good morning, Gabe. GABE DANCE: Good morning. Thanks for having me. KING: So really blockbuster reporting in the Times this morning. What are the new allegations exactly? DANCE: So we have acquired a set of documents that date to 2010 and run through 2017 that show a large list of companies that Facebook has given special access to users' information. KING: And what kind of access are they giving? What kind of data is being shared? DANCE: Right. So, for example, Facebook allowed Microsoft's Bing search engine to see the names of virtually all Facebook users' friends without consent. They gave Netflix and Spotify the ability to read Facebook users' private messages. Amazon was able to obtain names and contact information through friends, which it was supposed to not be able to do. Yahoo still had access to users' posts despite them no longer needing that for over five years. So it's a long list. KING: Gabe, can you explain, is it unusual for Facebook to have shared the data of its users with other large companies? DANCE: It's not necessarily unusual, but there are a few key differences that's important. One is that Facebook has for - since it began, talked about how they protect your data. They've built their company on this idea that you're sharing data with your friends, and it's only with your friends. And then they've started to expand the definition of your friends basically without telling you to the point where, now, your friends include companies like Amazon and Yahoo and Netflix and some of the other ones. KING: And I'm not aware of that is the deal. DANCE: That's the second rub, which is that, generally, people are not aware of any of this. And, in fact, it looks like Facebook has clearly taken steps to make sure that that's the case. KING: Facebook's founder, Mark Zuckerberg, testified before Congress in April. He said, quote, \"we don't sell data to anyone. \" Obviously, this is not the sale of data. How are Facebook and Zuckerberg responding to your reporting so far? DANCE: Well, Facebook has interest in making sure - Facebook had to enter into a 2011 consent agreement with the federal government, and that was for deceptive practices. So Facebook has a lot on the line here. And what they're interested in doing is explaining these relationships as being OK because they say these companies acted as service providers, which is to say they acted as Facebook itself. Now, that is a confusing argument on its face. And it's an argument that is important to them, again, because of this federal agreement. But basically, they're trying to say, look, all of these people are essentially acting as Facebook itself, and therefore we're not exempting them from any privacy things because how could we exempt ourselves from privacy things? The challenge with that is convincing their users that BlackBerry, Yahoo, Apple and et cetera are Facebook as well. KING: Sounds like it's going to be a fairly hard sell. Gabe Dance of The New York Times, thanks so much. DANCE: Thank you for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-20-678631013": {"title": "Amazon Customer Receives 1,700 Audio Files Of A Stranger Who Used Alexa : NPR", "url": "https://www.npr.org/2018/12/20/678631013/amazon-customer-receives-1-700-audio-files-of-a-stranger-who-used-alexa", "author": "No author found", "published_date": "2018-12-20", "content": "", "section": "Technology", "disclaimer": ""}, "2018-12-20-678557728": {"title": "Facebook Grapples With Another Privacy Scandal : NPR", "url": "https://www.npr.org/2018/12/20/678557728/facebook-grapples-with-another-privacy-scandal", "author": "No author found", "published_date": "2018-12-20", "content": "NOEL KING, HOST: Facebook has drawn a lot of criticism for failing to protect the privacy of its users. And then yesterday, The New York Times reported in great detail about how Facebook gave other giant companies access to its users' data. Those companies include Microsoft, Amazon and Yahoo. Some of them reportedly even had access to users' private messages. So what does all of this mean for Facebook? And possibly as important, what should it mean for Facebook? Anand Giridharadas is with me now. He wrote the book \"Winners Take All: The Elite Charade Of Changing The World. \" Good morning, Anand. ANAND GIRIDHARADAS: Good morning. KING: So is it fair to say right off the bat that you have been critical of Facebook and other big corporations? GIRIDHARADAS: That is fair. KING: OK (laughter). We'll go from there. Facebook has misused user data before. I mean, we've seen these stories break several times over the past couple of years, and it often feels like nothing happens. Why does this company seem immune from repercussions? GIRIDHARADAS: I think it's because we haven't actually figured out how to claim our power as a public to do something about this. You know, this week, as yet another revelation spilled out - and this one, in many ways, I think, was a Rubicon for a lot of people. I mean, this is private messages. . . KING: Yeah. GIRIDHARADAS: . . . Being read by other companies. And yet the dominant response that I witnessed was a kind of delete Facebook campaign, which you've seen in the past. And at one level, this is emotionally satisfying. And it's sort of maybe fun to delete the app. KING: And. . . GIRIDHARADAS: But I think part of why we are unable to solve it, as you ask, is because when you delete Facebook as an individual, you are personalizing what is, in fact, an issue of systems and the abuse of power. And this cannot be solved, in my view, by individual action and kind of personal fortitude. KING: That's interesting. You're saying this is not about personal responsibility, as many people would argue. It's about something bigger than that. So when we get to the bigger than that, let's talk about what direction you'd like to see this go in. What specific changes would you want to see either to protect user data or simply to regulate a company like Facebook? GIRIDHARADAS: There are two major areas of change when you look at Facebook. One - we need a privacy law that actually deals with the reality of what these companies have now been doing time and again. You know, Mark Zuckerberg and Sheryl Sandberg and their squad have revealed themselves to be, you know, stewards of our privacy that we don't need. And so there needs to be just privacy protection. The second is the revival of antitrust in American life. This is a predatory monopoly. You don't - if you want to go online and, you know, look for, you know, high school sweethearts to feel nostalgic about, there is no other option besides Facebook and all the other things we do on Facebook. KING: We should say. We should be clear. The question of whether or not Facebook is a monopoly - I mean, you could put two economists into the room - into a room and get a fistfight, right? That isn't. . . GIRIDHARADAS: Yes. KING: That isn't yet really agreed upon. GIRIDHARADAS: It might be a weak fistfight, but it would be a fistfight. KING: (Laughter). GIRIDHARADAS: But I think. . . KING: Go ahead. GIRIDHARADAS: . . . When it comes to these - the reason - whether it's privacy or antitrust or other things, what I'm talking about are legal fixes, you know? And I think it's worth thinking about analogies here. When you think about the predatory power of big food, and big sugar specifically, I think we've now learned that, you know, all of us trying to diet harder is no match for their political power. When we think about, you know, the opioid crisis and the people who promoted OxyContin, people individually fighting the demons of addiction is no match for that power. When you think about, you know, the Koch brothers and the deregulation agenda around pollution, you buying better paper towels and doing homework on which rivers are safe for your kids to swim in is no match for that political power. And I think, by the way, when you think about Sheryl Sandberg of Facebook and her advice to women - as Michelle Obama recently reminded us, you know, women simply leaning in and raising their hand higher is actually no match for thousands of years of oppressive patriarchy. So this company is actually part of this larger narrative that plutocrats and big corporations have been spreading in American life, which is that abusive behavior by the powerful is, in fact, your problem to solve as an individual. KING: You're arguing that it should be the problem of lawmakers who regulate things. Just briefly, Anand, why hasn't - why haven't lawmakers done it? I mean, Facebook's gone before Congress. What's going on? Where's the disconnect? GIRIDHARADAS: I mean, let's be real. We have a grandpa/grandma Congress that is unable to understand the apps that now shape the modern world as is evident every time we have these hearings. And one of the most important things we need to do is bring the age of Congress into line with the age of this country. Also, these companies present new legal issues because we are - they are conveniences for us that we love and yet are abusing us. And so we need a new legal framework. And I - there are members of Congress out there who are smart and thinking about this. And we need to support their efforts to solve this through the law. KING: Anand Giridharadas wrote the book \"Winners Take All. \"Thanks, Anand. GIRIDHARADAS: Thank you so much. NOEL KING, HOST:  Facebook has drawn a lot of criticism for failing to protect the privacy of its users. And then yesterday, The New York Times reported in great detail about how Facebook gave other giant companies access to its users' data. Those companies include Microsoft, Amazon and Yahoo. Some of them reportedly even had access to users' private messages. So what does all of this mean for Facebook? And possibly as important, what should it mean for Facebook? Anand Giridharadas is with me now. He wrote the book \"Winners Take All: The Elite Charade Of Changing The World. \" Good morning, Anand. ANAND GIRIDHARADAS: Good morning. KING: So is it fair to say right off the bat that you have been critical of Facebook and other big corporations? GIRIDHARADAS: That is fair. KING: OK (laughter). We'll go from there. Facebook has misused user data before. I mean, we've seen these stories break several times over the past couple of years, and it often feels like nothing happens. Why does this company seem immune from repercussions? GIRIDHARADAS: I think it's because we haven't actually figured out how to claim our power as a public to do something about this. You know, this week, as yet another revelation spilled out - and this one, in many ways, I think, was a Rubicon for a lot of people. I mean, this is private messages. . . KING: Yeah. GIRIDHARADAS: . . . Being read by other companies. And yet the dominant response that I witnessed was a kind of delete Facebook campaign, which you've seen in the past. And at one level, this is emotionally satisfying. And it's sort of maybe fun to delete the app. KING: And. . . GIRIDHARADAS: But I think part of why we are unable to solve it, as you ask, is because when you delete Facebook as an individual, you are personalizing what is, in fact, an issue of systems and the abuse of power. And this cannot be solved, in my view, by individual action and kind of personal fortitude. KING: That's interesting. You're saying this is not about personal responsibility, as many people would argue. It's about something bigger than that. So when we get to the bigger than that, let's talk about what direction you'd like to see this go in. What specific changes would you want to see either to protect user data or simply to regulate a company like Facebook? GIRIDHARADAS: There are two major areas of change when you look at Facebook. One - we need a privacy law that actually deals with the reality of what these companies have now been doing time and again. You know, Mark Zuckerberg and Sheryl Sandberg and their squad have revealed themselves to be, you know, stewards of our privacy that we don't need. And so there needs to be just privacy protection. The second is the revival of antitrust in American life. This is a predatory monopoly. You don't - if you want to go online and, you know, look for, you know, high school sweethearts to feel nostalgic about, there is no other option besides Facebook and all the other things we do on Facebook. KING: We should say. We should be clear. The question of whether or not Facebook is a monopoly - I mean, you could put two economists into the room - into a room and get a fistfight, right? That isn't. . . GIRIDHARADAS: Yes. KING: That isn't yet really agreed upon. GIRIDHARADAS: It might be a weak fistfight, but it would be a fistfight. KING: (Laughter). GIRIDHARADAS: But I think. . . KING: Go ahead. GIRIDHARADAS: . . . When it comes to these - the reason - whether it's privacy or antitrust or other things, what I'm talking about are legal fixes, you know? And I think it's worth thinking about analogies here. When you think about the predatory power of big food, and big sugar specifically, I think we've now learned that, you know, all of us trying to diet harder is no match for their political power. When we think about, you know, the opioid crisis and the people who promoted OxyContin, people individually fighting the demons of addiction is no match for that power. When you think about, you know, the Koch brothers and the deregulation agenda around pollution, you buying better paper towels and doing homework on which rivers are safe for your kids to swim in is no match for that political power. And I think, by the way, when you think about Sheryl Sandberg of Facebook and her advice to women - as Michelle Obama recently reminded us, you know, women simply leaning in and raising their hand higher is actually no match for thousands of years of oppressive patriarchy. So this company is actually part of this larger narrative that plutocrats and big corporations have been spreading in American life, which is that abusive behavior by the powerful is, in fact, your problem to solve as an individual. KING: You're arguing that it should be the problem of lawmakers who regulate things. Just briefly, Anand, why hasn't - why haven't lawmakers done it? I mean, Facebook's gone before Congress. What's going on? Where's the disconnect? GIRIDHARADAS: I mean, let's be real. We have a grandpa/grandma Congress that is unable to understand the apps that now shape the modern world as is evident every time we have these hearings. And one of the most important things we need to do is bring the age of Congress into line with the age of this country. Also, these companies present new legal issues because we are - they are conveniences for us that we love and yet are abusing us. And so we need a new legal framework. And I - there are members of Congress out there who are smart and thinking about this. And we need to support their efforts to solve this through the law. KING: Anand Giridharadas wrote the book \"Winners Take All. \" Thanks, Anand. GIRIDHARADAS: Thank you so much.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-21-679155506": {"title": "Huge Martian Crater 'Korolev' Appears Topped With Miles Of Pristine Snow  : NPR", "url": "https://www.npr.org/2018/12/21/679155506/huge-martian-crater-korolev-appears-topped-with-miles-of-pristine-snow", "author": "No author found", "published_date": "2018-12-21", "content": "", "section": "Science", "disclaimer": ""}, "2018-12-21-679293917": {"title": "How To Stop A Drone? There's No Good Answer : NPR", "url": "https://www.npr.org/2018/12/21/679293917/how-to-stop-a-drone-theres-no-good-answer", "author": "No author found", "published_date": "2018-12-21", "content": "SCOTT SIMON, HOST: A small drone sparked enormous disruptions at London's Gatwick Airport this week. Thousands of passengers were grounded as police struggled to stop the drone or drones from flying into restricted airspace. Police have arrested two suspects in connection with the incident. NPR's Geoff Brumfiel reports on why it's so hard to catch a drone. GEOFF BRUMFIEL, BYLINE: If you haven't been keeping up with drone technology, well, you're not alone. I had no idea how small they'd gotten. (SOUNDBITE OF VIDEO)IJUSTINE EZARIK: I have a drone in my purse. There's a drone in here. BRUMFIEL: That's one of about a bazillion drone unboxing videos you can find on YouTube. In addition to shrinking, drones are also getting more powerful. And they're flying for longer and further than ever. (SOUNDBITE OF VIDEO)EZARIK: These batteries will also give you about 27 minutes of flight time, which is a pretty long time, actually. BRUMFIEL: The authorities at Gatwick Airport near London discovered all of this for themselves on Wednesday when a small drone shut down the airport. Again and again, it flew into restricted airspace around the runway. Police arrived. A helicopter was called in. And yet, the drone, or possibly several drones - police aren't sure - it couldn't be stopped. Now, the solution might seem obvious. Why not shoot it down? Well, there's an answer for that also on YouTube. (SOUNDBITE OF GUNSHOTS)BRUMFIEL: This is video of a bunch of guys at a gun range in Arizona trying to hit a drone. (SOUNDBITE OF GUNSHOTS)BRUMFIEL: They get it, eventually. But obviously, unleashing that kind of firepower at an airport in the London suburbs would be extremely dangerous. Developing technology to safely stop drones is becoming big business. Arthur Holland Michel is co-director of the Center for the Study of the Drone at Bard College in New York. He says everyone from small start-ups to established military contractors are getting in the game with technologies to bring down drones. ARTHUR HOLLAND MICHEL: We've seen things like, you know, water cannons, lights and lasers to dazzle the drone sensors. BRUMFIEL: Some companies are even developing other drones to crash into the trespassing drones. But Michel says given all the different shapes and sizes that today's drones come in. . . MICHEL: There is no single technique for detecting drones or for bringing them out of the sky that is going to be 100 percent effective against 100 percent of drones in 100 percent of cases. BRUMFIEL: At Gatwick Airport, the military eventually had to be called in. According to press reports, they deployed an Israeli-made system known as Drone Dome to jam the signal to the drone from its controller. So far, it seems to be working. Geoff Brumfiel, NPR News. SCOTT SIMON, HOST:  A small drone sparked enormous disruptions at London's Gatwick Airport this week. Thousands of passengers were grounded as police struggled to stop the drone or drones from flying into restricted airspace. Police have arrested two suspects in connection with the incident. NPR's Geoff Brumfiel reports on why it's so hard to catch a drone. GEOFF BRUMFIEL, BYLINE: If you haven't been keeping up with drone technology, well, you're not alone. I had no idea how small they'd gotten. (SOUNDBITE OF VIDEO) IJUSTINE EZARIK: I have a drone in my purse. There's a drone in here. BRUMFIEL: That's one of about a bazillion drone unboxing videos you can find on YouTube. In addition to shrinking, drones are also getting more powerful. And they're flying for longer and further than ever. (SOUNDBITE OF VIDEO) EZARIK: These batteries will also give you about 27 minutes of flight time, which is a pretty long time, actually. BRUMFIEL: The authorities at Gatwick Airport near London discovered all of this for themselves on Wednesday when a small drone shut down the airport. Again and again, it flew into restricted airspace around the runway. Police arrived. A helicopter was called in. And yet, the drone, or possibly several drones - police aren't sure - it couldn't be stopped. Now, the solution might seem obvious. Why not shoot it down? Well, there's an answer for that also on YouTube. (SOUNDBITE OF GUNSHOTS) BRUMFIEL: This is video of a bunch of guys at a gun range in Arizona trying to hit a drone. (SOUNDBITE OF GUNSHOTS) BRUMFIEL: They get it, eventually. But obviously, unleashing that kind of firepower at an airport in the London suburbs would be extremely dangerous. Developing technology to safely stop drones is becoming big business. Arthur Holland Michel is co-director of the Center for the Study of the Drone at Bard College in New York. He says everyone from small start-ups to established military contractors are getting in the game with technologies to bring down drones. ARTHUR HOLLAND MICHEL: We've seen things like, you know, water cannons, lights and lasers to dazzle the drone sensors. BRUMFIEL: Some companies are even developing other drones to crash into the trespassing drones. But Michel says given all the different shapes and sizes that today's drones come in. . . MICHEL: There is no single technique for detecting drones or for bringing them out of the sky that is going to be 100 percent effective against 100 percent of drones in 100 percent of cases. BRUMFIEL: At Gatwick Airport, the military eventually had to be called in. According to press reports, they deployed an Israeli-made system known as Drone Dome to jam the signal to the drone from its controller. So far, it seems to be working. Geoff Brumfiel, NPR News.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-21-678641696": {"title": "Stephen Petranek: How Will Humans Live On Mars? : NPR", "url": "https://www.npr.org/2018/12/21/678641696/stephen-petranek-how-will-humans-live-on-mars", "author": "No author found", "published_date": "2018-12-21", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about The Next Frontier. And you may think that, right now, sending a handful of astronauts to Mars would be a major milestone. But what about sending a hundred people to Mars to, you know, start a colony? Could we actually do that one day? STEPHEN PETRANEK: Yep. RAZ: But we're talking, like - like, hundreds of years in the future, right? PETRANEK: We're talking a decade to 15 years from now, at the most. (SOUNDBITE OF MUSIC)RAZ: This is science writer Stephen Petranek. And even though it might sound like science fiction, Stephen says it will happen, whether through NASA or SpaceX or some other group, humans will colonize Mars. PETRANEK: We've had all the technology we need - the machines that can make oxygen for us to breathe on Mars, the habitats that we need on Mars. We've developed all this stuff 30, 40 years ago. We've had this technology for decades. RAZ: Stephen Petranek has all of this mapped out. In fact, he's written a book about it called, \"How We'll Live On Mars. \" And he picks up the idea from the TED stage. (SOUNDBITE OF TED TALK)PETRANEK: Strap yourselves in. We're going to Mars - not just a few astronauts. Thousands of people are going to colonize Mars, and I am telling you that they're going to do this soon. Some of you will end up working on projects on Mars, and I guarantee that some of your children will end up living there. That probably sounds preposterous. So let me tell you about the extraordinary adventure we're about to undertake. Mars is not our sister planet. RAZ: And that extraordinary adventure Stephen describes? It starts with traveling 250 million miles to Mars, which means living in a spacecraft for eight months. PETRANEK: One of the most important things about sending people to Mars is making the trip getting there pleasant and fun. The accommodations will be extremely pleasant, far more pleasant than the most luxurious first-class accommodations in any aircraft in the world. There will be much more space than you think. There will be common meeting areas. People will have meals together. RAZ: Wow. So you've got this great first-class accommodation and great food. And, by the way, like, where's the water going to - like, water's super heavy. How do you carry all that delicious food up? PETRANEK: Water is super heavy, and you want to get the water down to a minimum. So, for example, all the food will be freeze-dried and reconstructed with water aboard. But all the water in that spacecraft will be recycled. The water that humans use doesn't disappear. Part of it goes into their body, and the rest of it is expelled. And the part that is expelled will be reconditioned and reused. RAZ: So you're talking about, like, you know, bottled water of pee? PETRANEK: Correct. (SOUNDBITE OF MUSIC)PETRANEK: That happens now on the space station, for example, on the International Space Station. That happens in submarines. That happens in lots of different environments. RAZ: So what happens when we - when we land? Like, where would we go? Where would we live? What would we eat? PETRANEK: We will send everything we need for the first few years of life on Mars to Mars, before the first humans get there. Then we will use robots to assemble habitats, to unpack the cargo shipments that we've sent, to create some kind of living quarters that we can move out of the spaceship into. RAZ: OK. So I get it. So you're going to sort of pre-launch the - it's like doing the Appalachian Trail, right? You, like, send packages to yourself ahead of time? PETRANEK: Correct. RAZ: All right. So we're there with, like, a hundred other people. And we're like, OK, here we are. We're on Mars. And food eventually is going to run out. Like, we've got a - I mean, a clock starts ticking once we land there because, yeah, we've sent some shipments of food and stuff, but I mean - right? We've got to start growing stuff. What happens? PETRANEK: To live on Earth, you need food, water, shelter and clothing. To live on Mars, you need food, water, shelter, clothing and something to breathe. (SOUNDBITE OF TED TALK)PETRANEK: So let's look at the most important thing on this list first. Water is the basis of all life as we know it. And if you look at Mars, it looks really dry. It looks like the entire planet is a desert. But it turns out that it's not. A number of orbiters that we still have flying around Mars have shown us that lots of craters on Mars have a sheet of water ice in them. So there's plenty of water there, but most of it's ice, and it turns out the Mars atmosphere is often 100 percent humid. This is a device hooked up at the University of Washington back in 1998. This device can extract all the water that humans will need, simply from the atmosphere on Mars. Next, we have to worry about what we will breathe. This is a scientist at MIT named Michael Hecht, and he's developed this machine, MOXIE. I love this thing. It's a reverse fuel cell, essentially, that sucks in the Martian atmosphere and pumps out oxygen. And it will be able to produce enough oxygen to keep one person alive indefinitely. But the secret to this is that this thing was designed from the get-go to be scalable by a factor of 100. Next, what will we eat? Well, we'll use hydroponics to grow food, but we're not going to be able to grow more than 15 to 20 percent of our food there, at least not until we actually have the probability and the capability of planting crops. In the meantime, most of our food will arrive from Earth. And it will be dried. And then we need some shelter. There is too much solar radiation and too much radiation from cosmic rays, so we really have to go underground. Now, it turns out that the soil on Mars, by and large, is perfect for making bricks. And NASA's figured this one out, too. They're going to throw some polymer plastic into the bricks, shove them in a microwave oven and then you will be able to build buildings with really thick walls. And finally, there's clothing. On Earth, we have miles of atmosphere piled up on us, which creates 15 pounds of pressure on our bodies at all times. And we're constantly pushing out against that. On Mars, there's hardly any atmospheric pressure. So Dava Newman, a scientist at MIT, has created this sleek spacesuit that would keep us together, block radiation and keep us warm. So let's think about this for a minute - food, shelter, clothing, water, oxygen. We can do this. (SOUNDBITE OF MUSIC)RAZ: I don't know about this - radiation, living underground, freeze-dried food. Like, we might have some hydroponics, but it's a really harsh environment. So would you go? Like, would you want to live in those conditions? PETRANEK: I would. I wouldn't go on the first trip. I think the early trips are going to be extremely dangerous. But I would like to die on Mars, just, preferably, not on impact. I think it would be the greatest adventure of a lifetime. RAZ: Yeah. PETRANEK: I think what happens with Mars is we build a civilization of maybe a million people within the first hundred years, and we start to terraform the planet. RAZ: And when you say terraform, you mean, like, basically, to reengineer Mars to make it - like, the climate and the atmosphere more like Earth. PETRANEK: Correct. (SOUNDBITE OF TED TALK)PETRANEK: That sounds like a lot of hubris. But the truth is that the technology to do everything I'm about to tell you already exists. First, we've got to warm it up. Mars is incredibly cold because it has a very thin atmosphere. The answer lies here at the South Pole and at the North Pole of Mars, both of which are covered with an incredible amount of frozen carbon dioxide - dry ice. If we heat it up, it sublimes directly into the atmosphere and thickens the atmosphere the same way it does on Earth. And as we know, CO2 is an incredibly potent greenhouse gas. It actually won't take long for the temperature on Mars to start rising, probably less than 20 years. Right now on a perfect day, temperatures can actually reach 70 degrees. But then they go down to -100 at night. What we're shooting for is a runaway greenhouse effect - enough temperature rise to see a lot of that ice on Mars, especially the ice in the ground, melt. As the atmosphere gets thicker, everything gets better. We get more protection from radiation. More atmosphere makes the planet warmer. So we get running water, and that makes crops possible. It will rain and it will snow on Mars. Eventually, Mars will be made to feel a lot like British Columbia. RAZ: (Laughter) I mean, I get - even though the technology exists, to me, and to, I think, a lot of people listening, this sounds outlandish. PETRANEK: Yes. RAZ: But you are convinced this is going to happen, that humans will do this. PETRANEK: One of the reasons that I wrote the TED book \"How We'll Live On Mars\" is that I wanted to tell people that this is a completely unregulated environment. People are going to Mars. Whether we like it or not, SpaceX is devoted to getting to Mars. There's going to be a space race to Mars. People are going to Mars. There's going to be a civilization on Mars. It's going to happen a lot sooner than people think. The only reason that I'm trying to point all this stuff out is that now is the time to think about this. RAZ: Let me just push back on something for a second here because a lot of people, like astronomers and scientists, they're saying, we should not be focused on looking to Mars as a backup planet, right? - that instead, we should be focused on fixing Earth. Like, this is our planet. We - and we're destroying it. PETRANEK: I'm sympathetic with the idea of leaving Mars alone for scientific exploration and not for massive human habitation because Mars is an extraordinary scientific park, just as we've treated Antarctica as a scientific park. But we need to get realistic here. (SOUNDBITE OF MUSIC)PETRANEK: Homosapiens arose in Africa 2 million years ago. It wasn't till about 60,000 years ago that they began to leave Africa en masse. And they move beyond the next horizon. And they move beyond the next horizon. And they eventually populated the entire Earth. They learned along the way that that is a survival technique. Humans have been a nomadic species for 99 percent of their existence. It's only in the last 20,000 years that we've actually built towns, had sustainable agriculture and created a food supply that would allow us to stay in one place. That's a very recent development. I think that, actually, exploration is built into our DNA. And I think it's important that it's built into our DNA because if the human species is to survive, it must become an interplanetary species. And I'm not just talking about Mars. You have to go well beyond Mars. I think by then, we've found other habitable, Earth-like planets, where we don't have to terraform them, that we can actually reach in a human lifespan. And then we begin to move off of Mars and beyond Mars in this nomadic way to other planets. And that - it, like, hurts your Earth-first kind of sensibilities to say, oh, we're just a species that keeps moving on and using up resources. But the truth is that those resources that we're using up are expendable. You know, eventually, the sun begins to expand and completely destroys the Earth. RAZ: Yeah, we get a couple hundred million years before that happens. I mean, you know, call me an Earth-firster (ph) - guilty - but, I mean, you know, like, this is - kind of makes sense that we're here to stay for a while until - well, until, you know, until we go. But I think we still have some time. PETRANEK: Well, first of, all humans are not going to move to Mars as the preferable place to be. There's going to be just a few million people. The vast majority of people are going to continue to live on Earth, but you do need a backup planet. (SOUNDBITE OF TED TALK)PETRANEK: Ask any 10-year-old girl if she wants to go to Mars. Children who are now in elementary school are going to choose to live there. Think for a moment what we had when John F. Kennedy told us we would put a human on the moon. (SOUNDBITE OF ARCHIVED RECORDING)JOHN F KENNEDY: Like, why, some say, the moon? Why choose this as our goal? And they may well ask, why climb the highest mountain? (SOUNDBITE OF TED TALK)PETRANEK: He excited an entire generation to dream. (SOUNDBITE OF ARCHIVED RECORDING)KENNEDY: We choose to go to the moon. We choose to go to the moon in this decade and do the other things, not because they are easy, but because they are hard. . . (SOUNDBITE OF TED TALK)PETRANEK: Remember when we landed humans on the moon? When that happened, people looked at each other and said, if we can do this, we can do anything. Think how inspired we will be to see a landing on Mars. What are they going to think when we actually form a colony on Mars? Most importantly, it will make us a spacefaring species. And that means humans will survive no matter what happens on Earth. We will never be the last of our kind. Thank you. RAZ: That's Stephen Petranek. He's the author of the book \"How We'll Live On Mars. \" You can see his full talk at ted. com. (SOUNDBITE OF MUSIC) GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about The Next Frontier. And you may think that, right now, sending a handful of astronauts to Mars would be a major milestone. But what about sending a hundred people to Mars to, you know, start a colony? Could we actually do that one day? STEPHEN PETRANEK: Yep. RAZ: But we're talking, like - like, hundreds of years in the future, right? PETRANEK: We're talking a decade to 15 years from now, at the most. (SOUNDBITE OF MUSIC) RAZ: This is science writer Stephen Petranek. And even though it might sound like science fiction, Stephen says it will happen, whether through NASA or SpaceX or some other group, humans will colonize Mars. PETRANEK: We've had all the technology we need - the machines that can make oxygen for us to breathe on Mars, the habitats that we need on Mars. We've developed all this stuff 30, 40 years ago. We've had this technology for decades. RAZ: Stephen Petranek has all of this mapped out. In fact, he's written a book about it called, \"How We'll Live On Mars. \" And he picks up the idea from the TED stage. (SOUNDBITE OF TED TALK) PETRANEK: Strap yourselves in. We're going to Mars - not just a few astronauts. Thousands of people are going to colonize Mars, and I am telling you that they're going to do this soon. Some of you will end up working on projects on Mars, and I guarantee that some of your children will end up living there. That probably sounds preposterous. So let me tell you about the extraordinary adventure we're about to undertake. Mars is not our sister planet. RAZ: And that extraordinary adventure Stephen describes? It starts with traveling 250 million miles to Mars, which means living in a spacecraft for eight months. PETRANEK: One of the most important things about sending people to Mars is making the trip getting there pleasant and fun. The accommodations will be extremely pleasant, far more pleasant than the most luxurious first-class accommodations in any aircraft in the world. There will be much more space than you think. There will be common meeting areas. People will have meals together. RAZ: Wow. So you've got this great first-class accommodation and great food. And, by the way, like, where's the water going to - like, water's super heavy. How do you carry all that delicious food up? PETRANEK: Water is super heavy, and you want to get the water down to a minimum. So, for example, all the food will be freeze-dried and reconstructed with water aboard. But all the water in that spacecraft will be recycled. The water that humans use doesn't disappear. Part of it goes into their body, and the rest of it is expelled. And the part that is expelled will be reconditioned and reused. RAZ: So you're talking about, like, you know, bottled water of pee? PETRANEK: Correct. (SOUNDBITE OF MUSIC) PETRANEK: That happens now on the space station, for example, on the International Space Station. That happens in submarines. That happens in lots of different environments. RAZ: So what happens when we - when we land? Like, where would we go? Where would we live? What would we eat? PETRANEK: We will send everything we need for the first few years of life on Mars to Mars, before the first humans get there. Then we will use robots to assemble habitats, to unpack the cargo shipments that we've sent, to create some kind of living quarters that we can move out of the spaceship into. RAZ: OK. So I get it. So you're going to sort of pre-launch the - it's like doing the Appalachian Trail, right? You, like, send packages to yourself ahead of time? PETRANEK: Correct. RAZ: All right. So we're there with, like, a hundred other people. And we're like, OK, here we are. We're on Mars. And food eventually is going to run out. Like, we've got a - I mean, a clock starts ticking once we land there because, yeah, we've sent some shipments of food and stuff, but I mean - right? We've got to start growing stuff. What happens? PETRANEK: To live on Earth, you need food, water, shelter and clothing. To live on Mars, you need food, water, shelter, clothing and something to breathe. (SOUNDBITE OF TED TALK) PETRANEK: So let's look at the most important thing on this list first. Water is the basis of all life as we know it. And if you look at Mars, it looks really dry. It looks like the entire planet is a desert. But it turns out that it's not. A number of orbiters that we still have flying around Mars have shown us that lots of craters on Mars have a sheet of water ice in them. So there's plenty of water there, but most of it's ice, and it turns out the Mars atmosphere is often 100 percent humid. This is a device hooked up at the University of Washington back in 1998. This device can extract all the water that humans will need, simply from the atmosphere on Mars. Next, we have to worry about what we will breathe. This is a scientist at MIT named Michael Hecht, and he's developed this machine, MOXIE. I love this thing. It's a reverse fuel cell, essentially, that sucks in the Martian atmosphere and pumps out oxygen. And it will be able to produce enough oxygen to keep one person alive indefinitely. But the secret to this is that this thing was designed from the get-go to be scalable by a factor of 100. Next, what will we eat? Well, we'll use hydroponics to grow food, but we're not going to be able to grow more than 15 to 20 percent of our food there, at least not until we actually have the probability and the capability of planting crops. In the meantime, most of our food will arrive from Earth. And it will be dried. And then we need some shelter. There is too much solar radiation and too much radiation from cosmic rays, so we really have to go underground. Now, it turns out that the soil on Mars, by and large, is perfect for making bricks. And NASA's figured this one out, too. They're going to throw some polymer plastic into the bricks, shove them in a microwave oven and then you will be able to build buildings with really thick walls. And finally, there's clothing. On Earth, we have miles of atmosphere piled up on us, which creates 15 pounds of pressure on our bodies at all times. And we're constantly pushing out against that. On Mars, there's hardly any atmospheric pressure. So Dava Newman, a scientist at MIT, has created this sleek spacesuit that would keep us together, block radiation and keep us warm. So let's think about this for a minute - food, shelter, clothing, water, oxygen. We can do this. (SOUNDBITE OF MUSIC) RAZ: I don't know about this - radiation, living underground, freeze-dried food. Like, we might have some hydroponics, but it's a really harsh environment. So would you go? Like, would you want to live in those conditions? PETRANEK: I would. I wouldn't go on the first trip. I think the early trips are going to be extremely dangerous. But I would like to die on Mars, just, preferably, not on impact. I think it would be the greatest adventure of a lifetime. RAZ: Yeah. PETRANEK: I think what happens with Mars is we build a civilization of maybe a million people within the first hundred years, and we start to terraform the planet. RAZ: And when you say terraform, you mean, like, basically, to reengineer Mars to make it - like, the climate and the atmosphere more like Earth. PETRANEK: Correct. (SOUNDBITE OF TED TALK) PETRANEK: That sounds like a lot of hubris. But the truth is that the technology to do everything I'm about to tell you already exists. First, we've got to warm it up. Mars is incredibly cold because it has a very thin atmosphere. The answer lies here at the South Pole and at the North Pole of Mars, both of which are covered with an incredible amount of frozen carbon dioxide - dry ice. If we heat it up, it sublimes directly into the atmosphere and thickens the atmosphere the same way it does on Earth. And as we know, CO2 is an incredibly potent greenhouse gas. It actually won't take long for the temperature on Mars to start rising, probably less than 20 years. Right now on a perfect day, temperatures can actually reach 70 degrees. But then they go down to -100 at night. What we're shooting for is a runaway greenhouse effect - enough temperature rise to see a lot of that ice on Mars, especially the ice in the ground, melt. As the atmosphere gets thicker, everything gets better. We get more protection from radiation. More atmosphere makes the planet warmer. So we get running water, and that makes crops possible. It will rain and it will snow on Mars. Eventually, Mars will be made to feel a lot like British Columbia. RAZ: (Laughter) I mean, I get - even though the technology exists, to me, and to, I think, a lot of people listening, this sounds outlandish. PETRANEK: Yes. RAZ: But you are convinced this is going to happen, that humans will do this. PETRANEK: One of the reasons that I wrote the TED book \"How We'll Live On Mars\" is that I wanted to tell people that this is a completely unregulated environment. People are going to Mars. Whether we like it or not, SpaceX is devoted to getting to Mars. There's going to be a space race to Mars. People are going to Mars. There's going to be a civilization on Mars. It's going to happen a lot sooner than people think. The only reason that I'm trying to point all this stuff out is that now is the time to think about this. RAZ: Let me just push back on something for a second here because a lot of people, like astronomers and scientists, they're saying, we should not be focused on looking to Mars as a backup planet, right? - that instead, we should be focused on fixing Earth. Like, this is our planet. We - and we're destroying it. PETRANEK: I'm sympathetic with the idea of leaving Mars alone for scientific exploration and not for massive human habitation because Mars is an extraordinary scientific park, just as we've treated Antarctica as a scientific park. But we need to get realistic here. (SOUNDBITE OF MUSIC) PETRANEK: Homosapiens arose in Africa 2 million years ago. It wasn't till about 60,000 years ago that they began to leave Africa en masse. And they move beyond the next horizon. And they move beyond the next horizon. And they eventually populated the entire Earth. They learned along the way that that is a survival technique. Humans have been a nomadic species for 99 percent of their existence. It's only in the last 20,000 years that we've actually built towns, had sustainable agriculture and created a food supply that would allow us to stay in one place. That's a very recent development. I think that, actually, exploration is built into our DNA. And I think it's important that it's built into our DNA because if the human species is to survive, it must become an interplanetary species. And I'm not just talking about Mars. You have to go well beyond Mars. I think by then, we've found other habitable, Earth-like planets, where we don't have to terraform them, that we can actually reach in a human lifespan. And then we begin to move off of Mars and beyond Mars in this nomadic way to other planets. And that - it, like, hurts your Earth-first kind of sensibilities to say, oh, we're just a species that keeps moving on and using up resources. But the truth is that those resources that we're using up are expendable. You know, eventually, the sun begins to expand and completely destroys the Earth. RAZ: Yeah, we get a couple hundred million years before that happens. I mean, you know, call me an Earth-firster (ph) - guilty - but, I mean, you know, like, this is - kind of makes sense that we're here to stay for a while until - well, until, you know, until we go. But I think we still have some time. PETRANEK: Well, first of, all humans are not going to move to Mars as the preferable place to be. There's going to be just a few million people. The vast majority of people are going to continue to live on Earth, but you do need a backup planet. (SOUNDBITE OF TED TALK) PETRANEK: Ask any 10-year-old girl if she wants to go to Mars. Children who are now in elementary school are going to choose to live there. Think for a moment what we had when John F. Kennedy told us we would put a human on the moon. (SOUNDBITE OF ARCHIVED RECORDING) JOHN F KENNEDY: Like, why, some say, the moon? Why choose this as our goal? And they may well ask, why climb the highest mountain? (SOUNDBITE OF TED TALK) PETRANEK: He excited an entire generation to dream. (SOUNDBITE OF ARCHIVED RECORDING) KENNEDY: We choose to go to the moon. We choose to go to the moon in this decade and do the other things, not because they are easy, but because they are hard. . . (SOUNDBITE OF TED TALK) PETRANEK: Remember when we landed humans on the moon? When that happened, people looked at each other and said, if we can do this, we can do anything. Think how inspired we will be to see a landing on Mars. What are they going to think when we actually form a colony on Mars? Most importantly, it will make us a spacefaring species. And that means humans will survive no matter what happens on Earth. We will never be the last of our kind. Thank you. RAZ: That's Stephen Petranek. He's the author of the book \"How We'll Live On Mars. \" You can see his full talk at ted. com. (SOUNDBITE OF MUSIC)", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-21-678641893": {"title": "Lisa Nip: How Can We Engineer The Human Body To Survive On Mars ... And Beyond? : NPR", "url": "https://www.npr.org/2018/12/21/678641893/lisa-nip-how-can-we-engineer-the-human-body-to-survive-on-mars-and-beyond", "author": "No author found", "published_date": "2018-12-21", "content": "GUY RAZ, HOST: OK. So Stephen Petranek says we are going to Mars, maybe in the next decade. And let's just say for a moment this is going to happen. And then you have the chance to help colonize it. This would mean spending eight months in a spacecraft before finally touching down on the red planet. You'd put on your space suit. You'd open up the hatch. And then you'd see the vast Martian desert stretched out right in front of you. So what are you experiencing at that moment? Like, what's going on? LISA NIP: Well, a lot of things you probably can't see yet. RAZ: This is Lisa Nip. She's a researcher at MIT. NIP: Probably a lot of very strong cosmic rays have passed through your body. RAZ: Oh, that doesn't sound good. NIP: Unfortunately, that will be the case for every planet that we visit that doesn't have an atmosphere like down on Earth. And so you're going to be bombarded every day with a lot of radiation. And over time, all that damage that you don't see will start to manifest itself as cancer. And you will just experience a very slow and painful death. RAZ: OK (laughter). NIP: Sorry. I don't mean to sound morbid. RAZ: Lisa Nip has spent a lot of time thinking about, what will happen when humans start traveling across our solar system and beyond? What will happen to our bodies? And, unlike Stephen Petranek, Lisa has come to believe that future colonizers will not be able to change their environments enough to allow them to survive. She says that people will have to change themselves using synthetic biology. Lisa Nip picks up her idea from the TED stage. (SOUNDBITE OF TED TALK)NIP: Our cosmic voyages will be fraught with dangers, both known and unknown. So far, we've been looking to this new piece of mechanical technology or that great next-generation robot as part of a lineup to ensure our species' safe passage in space. Wonderful as they are, I believe the time has come for us to complement these bulky electronic giants with what is known as synthetic biology. It comes from molecular biology, which has given us antibiotics, vaccines and better ways to observe the physiological nuances of the human body. Using the tools of synthetic biology, we can now edit the genes of nearly any organism, microscopic or not, with incredible speed and fidelity. Given the limitations around man-made machines, synthetic biology will be a means for us to engineer not only our food, our fuel and our environment but also ourselves to compensate for our physical inadequacies and to ensure our survival in space. To give you an example of how we can use synthetic biology for a space exploration, let us return to the Mars environment. What if Martian soil could actually support plant growth without using Earth-derived nutrients? How would we make our plants cold-tolerant? How do we make our plants drought-tolerant? Well, it turns out we've already done things like this. By borrowing genes for anti-freeze protein from fish and genes for drought tolerance from other plants, like rice, and then stitching them into the plants that need them, we now have plants that can tolerate most droughts and freezes. So we can use synthetic biology to bring highly engineered plants with us, but what else can we do? Unless we plan to stay holed up underground for the duration of our stay on every new planet, we must find better ways of protecting ourselves without needing to hide behind a wall of lead. RAZ: When we come back in just a moment, Lisa Nip explains that the key to humans becoming a spacefaring species might be fungus. On our show today, ideas about The Next Frontier. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC)RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about The Next Frontier. And we were just hearing from MIT researcher Lisa Nip, who says that even with the latest technology, we humans just aren't built for living out in space. And so her solution? Synthetic biology. Genetically modifying ourselves to adapt. (SOUNDBITE OF MUSIC)RAZ: So how would we do that? NIP: OK. So this is - this is very many leap years away. But have you heard of how, in Chernobyl - you know, the radioactive site now? RAZ: Yeah. Yeah. NIP: So scientists have discovered fungal species are able to actually survive on radiation - not only just live, but they survive. RAZ: There's a fungus that survives on radiation in Chernobyl? NIP: Yes. RAZ: Wow. NIP: What's interesting is that, when they're exposed to radiation, they have melanin that actually generates electricity. And obviously, to translate that into an entire human being is going to be many, many steps away. But if we still have funding for science, and are OK with it as a species, that's an avenue that will and can be explored. RAZ: Essentially, a human that would need and require radiation to live and thrive? NIP: Correct. RAZ: Which is what Mars is filled with. NIP: (Laughter) Yes, exactly. And so you can imagine that a human being that just feeds off radiation probably won't need to eat anymore. RAZ: But that just sounds so - so out there. Like, how would that work? How would scientists go about figuring out how to inject people with the powers of fungus? NIP: Right. So it's not easy. But the radio-tolerant fungi, we know that they survive with melanin. And conveniently, humans also have melanin, and that's what's in our skin that makes us tan, brown. And the way to go about actually testing it would obviously start in a dish, in a lab, and to see whether the fungal version would actually survive conversion to a mammalian version. And after finding the similarities, we start coming in with our molecular scalpels and tweaking until we find something that is able to survive on its own and also be able to withstand any kind of immuno rejection from the human body. RAZ: I mean, it's not - I guess it's not that crazy to have this conversation. Like, if you and me were like an ocean or sea-faring creature several million years ago, and we all had a conference, and one of us piped up and said, hey, we have to figure out how to live on the land. You know, we're going to have to figure out how to breathe differently and, you know, grow different parts of our bodies. Like, you know, all the other tadpoles or whatever would say, you're nuts. You're crazy. But look at us now. NIP: (Laughter). RAZ: Right? Here we are. Here we are walking on two legs on planet Earth. NIP: Yeah. I think what you're pointing out to is that natural evolution has allowed organisms to morph in ways that make changes so drastic that you wouldn't have expected a connection. RAZ: What is the timeline we're talking about here? Are we talking about, like, hundreds of years, thousands of years? NIP: It's possibly thousands of years away. But I think we're starting to get our feet wet and being able to handle the biological tools to know when and where to make changes. (SOUNDBITE OF TED TALK)NIP: Every day, the human body evolves by accidental mutations that equally, accidentally allow certain humans to persevere in dismal situations. But such evolution requires two things that we may not always have, and they are death and time. We may not always have the time necessary for the natural evolution of extra functions for survival on non-Earth planets. We're living in what E. O. Wilson has termed the age of gene circumvention, during which we remedy our genetic defects like cystic fibrosis or muscular dystrophy with temporary external supplements. But with every passing day, we approach the age of volitional evolution, a time during which we, as a species, will have the capacity to decide, for ourselves, our own genetic destiny. Augmenting the human body with new abilities is no longer a question of how but of when. (SOUNDBITE OF MUSIC)RAZ: So there was a recent report in China, where a scientist claimed to have genetically engineered a human baby to be resistant to AIDS. NIP: Twins, in fact. We, as a scientific community, kind of balk at this because it was done without considering the moral and ethical consequences. RAZ: Right. NIP: For us to actually reach any kind of consensus on the editing of humans requires a greater amount of scientific education worldwide, so that everybody is informed before we have a discussion. I think the way that was done, it just doesn't sit well with people. RAZ: I mean, so what you're saying is that if humans want to go to Mars, we have to come to some kind of consensus about gene editing around human synthetic biology because that's the only way we're going to survive there long term? NIP: Yes. That's exactly right. If you - if we, as a species, decide that we intend to stay there for hundreds of years, and you want to do that without any changes to the human body, then you'll have to accept that there is a certain number of deaths that will happen. People will die, and you will have to say that they died because we wanted to keep them as human as possible. But if you want them to survive, I think the mind needs to accept that the human body is malleable, and that it can change, and that's OK. (SOUNDBITE OF TED TALK)NIP: Mars is a destination, but it will not be our last. Our path to the stars will be rife with trials that will bring us to question not only who we are but where we will be going. The answers will lie in our choice to use or abandon the technology that we have gleaned from life itself. And it will define us for the remainder of our term in this universe. Thank you. (APPLAUSE)RAZ: Lisa Nip is a researcher at MIT's Media Lab. You can see her full talk at ted. com. GUY RAZ, HOST:  OK. So Stephen Petranek says we are going to Mars, maybe in the next decade. And let's just say for a moment this is going to happen. And then you have the chance to help colonize it. This would mean spending eight months in a spacecraft before finally touching down on the red planet. You'd put on your space suit. You'd open up the hatch. And then you'd see the vast Martian desert stretched out right in front of you. So what are you experiencing at that moment? Like, what's going on? LISA NIP: Well, a lot of things you probably can't see yet. RAZ: This is Lisa Nip. She's a researcher at MIT. NIP: Probably a lot of very strong cosmic rays have passed through your body. RAZ: Oh, that doesn't sound good. NIP: Unfortunately, that will be the case for every planet that we visit that doesn't have an atmosphere like down on Earth. And so you're going to be bombarded every day with a lot of radiation. And over time, all that damage that you don't see will start to manifest itself as cancer. And you will just experience a very slow and painful death. RAZ: OK (laughter). NIP: Sorry. I don't mean to sound morbid. RAZ: Lisa Nip has spent a lot of time thinking about, what will happen when humans start traveling across our solar system and beyond? What will happen to our bodies? And, unlike Stephen Petranek, Lisa has come to believe that future colonizers will not be able to change their environments enough to allow them to survive. She says that people will have to change themselves using synthetic biology. Lisa Nip picks up her idea from the TED stage. (SOUNDBITE OF TED TALK) NIP: Our cosmic voyages will be fraught with dangers, both known and unknown. So far, we've been looking to this new piece of mechanical technology or that great next-generation robot as part of a lineup to ensure our species' safe passage in space. Wonderful as they are, I believe the time has come for us to complement these bulky electronic giants with what is known as synthetic biology. It comes from molecular biology, which has given us antibiotics, vaccines and better ways to observe the physiological nuances of the human body. Using the tools of synthetic biology, we can now edit the genes of nearly any organism, microscopic or not, with incredible speed and fidelity. Given the limitations around man-made machines, synthetic biology will be a means for us to engineer not only our food, our fuel and our environment but also ourselves to compensate for our physical inadequacies and to ensure our survival in space. To give you an example of how we can use synthetic biology for a space exploration, let us return to the Mars environment. What if Martian soil could actually support plant growth without using Earth-derived nutrients? How would we make our plants cold-tolerant? How do we make our plants drought-tolerant? Well, it turns out we've already done things like this. By borrowing genes for anti-freeze protein from fish and genes for drought tolerance from other plants, like rice, and then stitching them into the plants that need them, we now have plants that can tolerate most droughts and freezes. So we can use synthetic biology to bring highly engineered plants with us, but what else can we do? Unless we plan to stay holed up underground for the duration of our stay on every new planet, we must find better ways of protecting ourselves without needing to hide behind a wall of lead. RAZ: When we come back in just a moment, Lisa Nip explains that the key to humans becoming a spacefaring species might be fungus. On our show today, ideas about The Next Frontier. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC) RAZ: It's the TED Radio Hour from NPR. I'm Guy Raz. And on the show today, ideas about The Next Frontier. And we were just hearing from MIT researcher Lisa Nip, who says that even with the latest technology, we humans just aren't built for living out in space. And so her solution? Synthetic biology. Genetically modifying ourselves to adapt. (SOUNDBITE OF MUSIC) RAZ: So how would we do that? NIP: OK. So this is - this is very many leap years away. But have you heard of how, in Chernobyl - you know, the radioactive site now? RAZ: Yeah. Yeah. NIP: So scientists have discovered fungal species are able to actually survive on radiation - not only just live, but they survive. RAZ: There's a fungus that survives on radiation in Chernobyl? NIP: Yes. RAZ: Wow. NIP: What's interesting is that, when they're exposed to radiation, they have melanin that actually generates electricity. And obviously, to translate that into an entire human being is going to be many, many steps away. But if we still have funding for science, and are OK with it as a species, that's an avenue that will and can be explored. RAZ: Essentially, a human that would need and require radiation to live and thrive? NIP: Correct. RAZ: Which is what Mars is filled with. NIP: (Laughter) Yes, exactly. And so you can imagine that a human being that just feeds off radiation probably won't need to eat anymore. RAZ: But that just sounds so - so out there. Like, how would that work? How would scientists go about figuring out how to inject people with the powers of fungus? NIP: Right. So it's not easy. But the radio-tolerant fungi, we know that they survive with melanin. And conveniently, humans also have melanin, and that's what's in our skin that makes us tan, brown. And the way to go about actually testing it would obviously start in a dish, in a lab, and to see whether the fungal version would actually survive conversion to a mammalian version. And after finding the similarities, we start coming in with our molecular scalpels and tweaking until we find something that is able to survive on its own and also be able to withstand any kind of immuno rejection from the human body. RAZ: I mean, it's not - I guess it's not that crazy to have this conversation. Like, if you and me were like an ocean or sea-faring creature several million years ago, and we all had a conference, and one of us piped up and said, hey, we have to figure out how to live on the land. You know, we're going to have to figure out how to breathe differently and, you know, grow different parts of our bodies. Like, you know, all the other tadpoles or whatever would say, you're nuts. You're crazy. But look at us now. NIP: (Laughter). RAZ: Right? Here we are. Here we are walking on two legs on planet Earth. NIP: Yeah. I think what you're pointing out to is that natural evolution has allowed organisms to morph in ways that make changes so drastic that you wouldn't have expected a connection. RAZ: What is the timeline we're talking about here? Are we talking about, like, hundreds of years, thousands of years? NIP: It's possibly thousands of years away. But I think we're starting to get our feet wet and being able to handle the biological tools to know when and where to make changes. (SOUNDBITE OF TED TALK) NIP: Every day, the human body evolves by accidental mutations that equally, accidentally allow certain humans to persevere in dismal situations. But such evolution requires two things that we may not always have, and they are death and time. We may not always have the time necessary for the natural evolution of extra functions for survival on non-Earth planets. We're living in what E. O. Wilson has termed the age of gene circumvention, during which we remedy our genetic defects like cystic fibrosis or muscular dystrophy with temporary external supplements. But with every passing day, we approach the age of volitional evolution, a time during which we, as a species, will have the capacity to decide, for ourselves, our own genetic destiny. Augmenting the human body with new abilities is no longer a question of how but of when. (SOUNDBITE OF MUSIC) RAZ: So there was a recent report in China, where a scientist claimed to have genetically engineered a human baby to be resistant to AIDS. NIP: Twins, in fact. We, as a scientific community, kind of balk at this because it was done without considering the moral and ethical consequences. RAZ: Right. NIP: For us to actually reach any kind of consensus on the editing of humans requires a greater amount of scientific education worldwide, so that everybody is informed before we have a discussion. I think the way that was done, it just doesn't sit well with people. RAZ: I mean, so what you're saying is that if humans want to go to Mars, we have to come to some kind of consensus about gene editing around human synthetic biology because that's the only way we're going to survive there long term? NIP: Yes. That's exactly right. If you - if we, as a species, decide that we intend to stay there for hundreds of years, and you want to do that without any changes to the human body, then you'll have to accept that there is a certain number of deaths that will happen. People will die, and you will have to say that they died because we wanted to keep them as human as possible. But if you want them to survive, I think the mind needs to accept that the human body is malleable, and that it can change, and that's OK. (SOUNDBITE OF TED TALK) NIP: Mars is a destination, but it will not be our last. Our path to the stars will be rife with trials that will bring us to question not only who we are but where we will be going. The answers will lie in our choice to use or abandon the technology that we have gleaned from life itself. And it will define us for the remainder of our term in this universe. Thank you. (APPLAUSE) RAZ: Lisa Nip is a researcher at MIT's Media Lab. You can see her full talk at ted. com.", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-21-678641588": {"title": "James Green: Could The Building Blocks Of Life Exist Elsewhere In Our Solar System? : NPR", "url": "https://www.npr.org/2018/12/21/678641588/james-green-could-the-building-blocks-of-life-exist-elsewhere-in-our-solar-syste", "author": "No author found", "published_date": "2018-12-21", "content": "GUY RAZ, HOST: It's the TED Radio Hour from NPR. I'm Guy Raz. So what do we know about life out there beyond our planet? (SOUNDBITE OF TV SHOW, \"STAR TREK\")UNIDENTIFIED PERSON: (As character) Space - a final frontier. RAZ: Does it even exist? And if it does, how close are we to finding it? (SOUNDBITE OF TV SHOW, \"STAR TREK\")UNIDENTIFIED PERSON: (As character) To seek out new life and new civilizations, to boldly go where no man has gone before. JIM GREEN: So from a - the science perspective, the probability, I think, for life, even complex life, is getting better. RAZ: This is Jim Green. He's chief scientist at NASA. GREEN: And it's based on scientific knowledge of finding locations like exoplanets, like we now know in our own solar system - places where even the giant planets with tidal forces sitting out in the outer edges of those solar systems could indeed produce environments where there's a significant amount of water. And if you have the right organic materials and enough time, that perhaps life evolved in that - in those particular areas. So I think we all are going to say, we haven't found it yet. We haven't found it in the solar system. But I think the probability is getting better. You know, I think we have the abilities and the technologies. It's a matter of will. (SOUNDBITE OF MUSIC)RAZ: Here's more from Jim Green on the TED stage. (SOUNDBITE OF TED TALK)GREEN: So how do we make this journey? What we decided to do is first look for those ingredients for life. The ingredients of life are liquid water. We have to have a solvent. We also have to have energy. We also have to have organic material, things that make us up but also things that we need to consume. So we have to have these elements in environments for long periods of time for us to be able to be confident that life can spark and then grow and evolve. Well, I have to tell you that early in my career when we looked at those three elements, I didn't believe that they were beyond Earth in any length of time and for any real quantity. Why? We look at the inner planets. Venus is way too hot. It's got no water. Mars - dry and arid. It's got no water. And beyond Mars, the water in the solar system is all frozen. But recent observations have changed all that. It's now turning our attention to the right places for us to take a deeper look and really start to answer our life question. RAZ: So what would it take for us to find or even sustain life outside of our own planet? Could we discover another place to call home? - another Earth? Well, today on the show, we're going to explore The Next Frontier - ideas about living beyond our own planet. Will we actually colonize Mars or another planet? Can humans become a spacefaring species? - and should we? Or does the real challenge of our lifetime actually start a bit closer to home? Well, for Jim Green, the possibility of life elsewhere is very real, especially when it comes to life in one of its simplest and smallest forms - as microorganisms. And they could exist in our very own solar system. I have to assume that this is a question. Is there life beyond our Earth? I have to assume this is a question that is taken very seriously by scientists. GREEN: Yeah. You have to take a broader view, too - I do, anyway - of the solar system. And that is when we look for life beyond Earth in the solar system, when we have a good idea as to what these planets look like and what their characteristics are today, they haven't always been like that. RAZ: Right. GREEN: We now know, you know, that our climate has done nothing but change. It's going to continue to change. That's true on our other two major terrestrial planets - Venus and Mars. In fact, we have great indications that both Venus and Mars were much more Earthlike early on in their life with a significant amount of water. You know, Venus had a significant ocean and maybe for a significant period of time before it went through a runaway greenhouse effect. And of course, what happens is you break the normal water cycle, and you start losing water. And water is also a great greenhouse gas. And then you have Mars, which we also have great indication that it has a significant amount of water. And I mean, that's physical because we're sitting there right now making those kind of measurements with our rovers. In fact, two-thirds of the Northern Hemisphere, probably, was water, even up to about a mile deep. So those are wonderful environments, so they could have harbored life in their past. (SOUNDBITE OF TED TALK)GREEN: So what about Mars? Let's go through the evidence. Well, Mars we thought was initially moonlike - full of craters and a dead world. And so about 15 years ago, we started a series of missions to go to Mars and see if water existed on Mars in its past that changed its geology. We ought to be able to notice that. And indeed, we started to be surprised right away. Our higher-resolution images showed deltas and river valleys and gullies that were there in the past. And in fact, Curiosity, which has been roving on the surface now for about three years, has really shown us that it's sitting in an ancient riverbed where water flowed rapidly. And not for a little while, perhaps hundreds of millions of years. And if everything was there, including organics, perhaps life had started. Curiosity has also drilled in that red soil and brought up other material. And we were really excited when we saw that because it wasn't red Mars. It was gray material. It was gray Mars. We brought it into the rover. We tasted it. And guess what? We tasted organics - carbon, hydrogen, oxygen, nitrogen, phosphorus, sulphur. They were all there. So Mars, in its past, with a lot of water, perhaps plenty of time, could have had life, could have had that spark, could have grown. And is that life still there? Well, it tells us that Mars has all the ingredients necessary for life. It has all the right conditions. (SOUNDBITE OF MUSIC)RAZ: I mean, if you know, as you say, we tasted organics on Mars - right? - carbon, hydrogen, oxygen, nitrogen, et cetera, I mean, would that be enough to allow humans to live there? GREEN: Absolutely. And the more we know about Mars, the more feasible it's becoming. For instance, Mars doesn't have a moon like we do, and so its axis has really swung around significantly. We have found, buried, the ancient pole of Mars. And in some cases, it's 700 meters or more of ice. This would probably be pure water. The place is large. It's bigger than the state of Maryland. RAZ: This is - you're talking about this frozen lake? GREEN: No, it's a frozen polar cap. It's the ancient polar cap. RAZ: Size of Maryland. OK. That's a lot of water. GREEN: Yeah. Yeah. And it's mid-latitudes right now. So the ability to be able to have a base of operation where you can leverage and use that resource is completely viable. In fact, a Japanese research group at University of Kyoto is looking at how to grow poplars, OK? Trees. . . RAZ: The trees. Yeah, sure. GREEN: . . . In Mars soil. They have a simulant, in addition to the carbon dioxide pressure and temperature of Mars. And you know, we'll check in with them in another year and see what happens. So conceptually, a lot of research can be move - move in the right direction that will really be quite exciting. RAZ: I know that we humans, like, we are a self-absorbed species, right? That we think of ourselves at the center of everything. But if we do discover some kind of life on Mars, or even beyond, you know, like, single-celled organisms, or whatever it might be, and that of course will be super exciting, but just out of curiosity, like, what does that do for us? Like, how does it - does it actually make a difference for humans? GREEN: Well, sure, it does. Oh, absolutely. So let's take a couple examples. All right. Once you find life, let's say single-celled organisms on Mars, a whole new series of questions come about, OK? Those questions are - how are we related to it? Is it DNA-based? Is it a concept of panspermia, the idea of impacts carrying life away from a planet until it falls on another planet and then seeding that planet and starting life again, is that the right relationship? Or is it really a second genesis? So now, if we jumped to a place like Europa. . . RAZ: And Europa's one of Jupiter's moons. GREEN: Correct. Europa has got this magnificent ocean with what we believe are hydrothermal vents. It's an active geological body where ice tectonics are going on, where one plate is slipping under others. As cracks are forming, water is circulating to the surface. There's an entire circulation of material in the ocean. The radiation environment that it sends seems to indicate that, as the high-energy particles hit the surface, that the oxygenation occurs in the ocean and has a fabulous environment. And it's been like that for 4 1/2 billion years. RAZ: Wow. GREEN: OK. And so that would definitely be a second genesis. We couldn't get rocks from Mars and Earth from impacts with life on them all the way up that far into the solar system to be able to pelt Europa so much that it could start life. So we would want to probe that. But the bottom line is, we might have now answered the question, yes, life is elsewhere in the solar system - maybe related to us, verifying panspermia, maybe not. But we have opportunities, clearly, to potentially find life that's a second genesis. (SOUNDBITE OF MUSIC)GREEN: I wouldn't discount the outer part of the solar system, particularly once Mars is firmly in human grasp, and we're able to explore it freely and really understand it. Then there is the possibility of launching from Mars on outward. So that would be entertained in the very distant future. (SOUNDBITE OF TED TALK)GREEN: Well, is there life beyond Earth in the solar system? We don't know yet, but we're hot on the pursuit. The data that we're receiving is really exciting and telling us, forcing us, to think about this in new and exciting ways. I believe we're on the right track, that in the next 10 years, we will answer that question. And if we answer it, and it's positive, then life is everywhere in the solar system. Just think about that. We may not be alone. Thank you. (APPLAUSE)RAZ: Jim Green. He's chief scientist at NASA. You can see his full talk at ted. com. On the show today, exploring The Next Frontier. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC) GUY RAZ, HOST:  It's the TED Radio Hour from NPR. I'm Guy Raz. So what do we know about life out there beyond our planet? (SOUNDBITE OF TV SHOW, \"STAR TREK\") UNIDENTIFIED PERSON: (As character) Space - a final frontier. RAZ: Does it even exist? And if it does, how close are we to finding it? (SOUNDBITE OF TV SHOW, \"STAR TREK\") UNIDENTIFIED PERSON: (As character) To seek out new life and new civilizations, to boldly go where no man has gone before. JIM GREEN: So from a - the science perspective, the probability, I think, for life, even complex life, is getting better. RAZ: This is Jim Green. He's chief scientist at NASA. GREEN: And it's based on scientific knowledge of finding locations like exoplanets, like we now know in our own solar system - places where even the giant planets with tidal forces sitting out in the outer edges of those solar systems could indeed produce environments where there's a significant amount of water. And if you have the right organic materials and enough time, that perhaps life evolved in that - in those particular areas. So I think we all are going to say, we haven't found it yet. We haven't found it in the solar system. But I think the probability is getting better. You know, I think we have the abilities and the technologies. It's a matter of will. (SOUNDBITE OF MUSIC) RAZ: Here's more from Jim Green on the TED stage. (SOUNDBITE OF TED TALK) GREEN: So how do we make this journey? What we decided to do is first look for those ingredients for life. The ingredients of life are liquid water. We have to have a solvent. We also have to have energy. We also have to have organic material, things that make us up but also things that we need to consume. So we have to have these elements in environments for long periods of time for us to be able to be confident that life can spark and then grow and evolve. Well, I have to tell you that early in my career when we looked at those three elements, I didn't believe that they were beyond Earth in any length of time and for any real quantity. Why? We look at the inner planets. Venus is way too hot. It's got no water. Mars - dry and arid. It's got no water. And beyond Mars, the water in the solar system is all frozen. But recent observations have changed all that. It's now turning our attention to the right places for us to take a deeper look and really start to answer our life question. RAZ: So what would it take for us to find or even sustain life outside of our own planet? Could we discover another place to call home? - another Earth? Well, today on the show, we're going to explore The Next Frontier - ideas about living beyond our own planet. Will we actually colonize Mars or another planet? Can humans become a spacefaring species? - and should we? Or does the real challenge of our lifetime actually start a bit closer to home? Well, for Jim Green, the possibility of life elsewhere is very real, especially when it comes to life in one of its simplest and smallest forms - as microorganisms. And they could exist in our very own solar system. I have to assume that this is a question. Is there life beyond our Earth? I have to assume this is a question that is taken very seriously by scientists. GREEN: Yeah. You have to take a broader view, too - I do, anyway - of the solar system. And that is when we look for life beyond Earth in the solar system, when we have a good idea as to what these planets look like and what their characteristics are today, they haven't always been like that. RAZ: Right. GREEN: We now know, you know, that our climate has done nothing but change. It's going to continue to change. That's true on our other two major terrestrial planets - Venus and Mars. In fact, we have great indications that both Venus and Mars were much more Earthlike early on in their life with a significant amount of water. You know, Venus had a significant ocean and maybe for a significant period of time before it went through a runaway greenhouse effect. And of course, what happens is you break the normal water cycle, and you start losing water. And water is also a great greenhouse gas. And then you have Mars, which we also have great indication that it has a significant amount of water. And I mean, that's physical because we're sitting there right now making those kind of measurements with our rovers. In fact, two-thirds of the Northern Hemisphere, probably, was water, even up to about a mile deep. So those are wonderful environments, so they could have harbored life in their past. (SOUNDBITE OF TED TALK) GREEN: So what about Mars? Let's go through the evidence. Well, Mars we thought was initially moonlike - full of craters and a dead world. And so about 15 years ago, we started a series of missions to go to Mars and see if water existed on Mars in its past that changed its geology. We ought to be able to notice that. And indeed, we started to be surprised right away. Our higher-resolution images showed deltas and river valleys and gullies that were there in the past. And in fact, Curiosity, which has been roving on the surface now for about three years, has really shown us that it's sitting in an ancient riverbed where water flowed rapidly. And not for a little while, perhaps hundreds of millions of years. And if everything was there, including organics, perhaps life had started. Curiosity has also drilled in that red soil and brought up other material. And we were really excited when we saw that because it wasn't red Mars. It was gray material. It was gray Mars. We brought it into the rover. We tasted it. And guess what? We tasted organics - carbon, hydrogen, oxygen, nitrogen, phosphorus, sulphur. They were all there. So Mars, in its past, with a lot of water, perhaps plenty of time, could have had life, could have had that spark, could have grown. And is that life still there? Well, it tells us that Mars has all the ingredients necessary for life. It has all the right conditions. (SOUNDBITE OF MUSIC) RAZ: I mean, if you know, as you say, we tasted organics on Mars - right? - carbon, hydrogen, oxygen, nitrogen, et cetera, I mean, would that be enough to allow humans to live there? GREEN: Absolutely. And the more we know about Mars, the more feasible it's becoming. For instance, Mars doesn't have a moon like we do, and so its axis has really swung around significantly. We have found, buried, the ancient pole of Mars. And in some cases, it's 700 meters or more of ice. This would probably be pure water. The place is large. It's bigger than the state of Maryland. RAZ: This is - you're talking about this frozen lake? GREEN: No, it's a frozen polar cap. It's the ancient polar cap. RAZ: Size of Maryland. OK. That's a lot of water. GREEN: Yeah. Yeah. And it's mid-latitudes right now. So the ability to be able to have a base of operation where you can leverage and use that resource is completely viable. In fact, a Japanese research group at University of Kyoto is looking at how to grow poplars, OK? Trees. . . RAZ: The trees. Yeah, sure. GREEN: . . . In Mars soil. They have a simulant, in addition to the carbon dioxide pressure and temperature of Mars. And you know, we'll check in with them in another year and see what happens. So conceptually, a lot of research can be move - move in the right direction that will really be quite exciting. RAZ: I know that we humans, like, we are a self-absorbed species, right? That we think of ourselves at the center of everything. But if we do discover some kind of life on Mars, or even beyond, you know, like, single-celled organisms, or whatever it might be, and that of course will be super exciting, but just out of curiosity, like, what does that do for us? Like, how does it - does it actually make a difference for humans? GREEN: Well, sure, it does. Oh, absolutely. So let's take a couple examples. All right. Once you find life, let's say single-celled organisms on Mars, a whole new series of questions come about, OK? Those questions are - how are we related to it? Is it DNA-based? Is it a concept of panspermia, the idea of impacts carrying life away from a planet until it falls on another planet and then seeding that planet and starting life again, is that the right relationship? Or is it really a second genesis? So now, if we jumped to a place like Europa. . . RAZ: And Europa's one of Jupiter's moons. GREEN: Correct. Europa has got this magnificent ocean with what we believe are hydrothermal vents. It's an active geological body where ice tectonics are going on, where one plate is slipping under others. As cracks are forming, water is circulating to the surface. There's an entire circulation of material in the ocean. The radiation environment that it sends seems to indicate that, as the high-energy particles hit the surface, that the oxygenation occurs in the ocean and has a fabulous environment. And it's been like that for 4 1/2 billion years. RAZ: Wow. GREEN: OK. And so that would definitely be a second genesis. We couldn't get rocks from Mars and Earth from impacts with life on them all the way up that far into the solar system to be able to pelt Europa so much that it could start life. So we would want to probe that. But the bottom line is, we might have now answered the question, yes, life is elsewhere in the solar system - maybe related to us, verifying panspermia, maybe not. But we have opportunities, clearly, to potentially find life that's a second genesis. (SOUNDBITE OF MUSIC) GREEN: I wouldn't discount the outer part of the solar system, particularly once Mars is firmly in human grasp, and we're able to explore it freely and really understand it. Then there is the possibility of launching from Mars on outward. So that would be entertained in the very distant future. (SOUNDBITE OF TED TALK) GREEN: Well, is there life beyond Earth in the solar system? We don't know yet, but we're hot on the pursuit. The data that we're receiving is really exciting and telling us, forcing us, to think about this in new and exciting ways. I believe we're on the right track, that in the next 10 years, we will answer that question. And if we answer it, and it's positive, then life is everywhere in the solar system. Just think about that. We may not be alone. Thank you. (APPLAUSE) RAZ: Jim Green. He's chief scientist at NASA. You can see his full talk at ted. com. On the show today, exploring The Next Frontier. I'm Guy Raz, and you're listening to the TED Radio Hour from NPR. (SOUNDBITE OF MUSIC)", "section": "TED Radio Hour", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-23-679705342": {"title": "Zoox Inc. Snags First California Permit To Transport Passengers In Self-Driving Cars : NPR", "url": "https://www.npr.org/2018/12/23/679705342/zoox-inc-snags-first-california-permit-to-transport-passengers-in-self-driving-c", "author": "No author found", "published_date": "2018-12-23", "content": "", "section": "Technology", "disclaimer": ""}, "2018-12-26-676731720": {"title": "Mars And Beyond: Minisatellites Open New Vistas : NPR", "url": "https://www.npr.org/2018/12/26/676731720/whats-next-for-tiny-satellites", "author": "No author found", "published_date": "2018-12-26", "content": "MARY LOUISE KELLY, HOST: To another story now, two miniature spacecraft pulled off a remarkable feat last month. They provided live play-by-play as NASA's InSight probe made its successful touchdown on Mars. NPR's Joe Palca has the story of the two experimental mini-satellites known collectively as MarCO. JOE PALCA, BYLINE: When InSight left Earth last May, NASA provided a familiar narration. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED NASA EMPLOYEE: Five, four, three, two. . . PALCA: Blastoff. Now, you expect to hear that. What's more unusual is this. (SOUNDBITE OF ARCHIVED RECORDING)CHRISTINE SZALAI: Altitude 400 meters, 300 meters, 200 meters. PALCA: That's Christine Szalai, a member of the InSight landing team at the Jet Propulsion Laboratory in Pasadena. She narrated InSight's descent from the control room at JPL. (SOUNDBITE OF ARCHIVED RECORDING)SZALAI: Thirty meters, 20 meters, 17 meters. PALCA: And then blastoff - I mean, touchdown. (SOUNDBITE OF ARCHIVED RECORDING)SZALAI: Touchdown confirmed. (APPLAUSE)PALCA: Now, most of the time there is very little information coming back to Earth from a space probe as it comes in for a landing, maybe just a simple tone at key moments. And if there are any numbers, they're based on where mission controllers think the spacecraft should be based on a projected timeline. But the numbers Szalai was reading off were coming from InSight's onboard radar, actual data as it was coming in for landing. Anne Marinan helped provide that data. She's an engineer at JPL who worked on MarCO. And she heard Szalai's narration. ANNE MARINAN: She was doing it based on data that she was seeing that was transmitted from MarCO. PALCA: How did that make you feel? MARINAN: I didn't quite appreciate that that's what she was doing until I saw a rebroadcast. And it kind of hit me that she was able to make those calls based on data that my spacecraft had sent. PALCA: MarCO is actually two briefcase-sized spacecraft that left Earth with InSight. The pair trailed close behind InSight on the way to Mars. InSight's radio couldn't reach Earth directly as it plummeted to the surface, but it could reach MarCO flying nearby. And MarCO's job was to relay back to mission control what InSight was sending. Marinan says the miniature satellite was an experiment. MARINAN: MarCO was primarily a technology demonstration. So there were brand-new components and technologies that we flew for the very first time in deep space. PALCA: A new kind of antenna, a new kind of propulsion. And Marinan says the new technology didn't always work perfectly. The propulsion system gave them headaches. But. . . MARINAN: Well, it worked perfectly when it had to. PALCA: This was Marinan's first assignment at JPL. She only recently earned her Ph. D. So what do you do for an encore when your first mission works so well? MARINAN: I am actually currently putting together another spacecraft. PALCA: Another miniature spacecraft, this one more science-y (ph), going to take pictures of a nearby asteroid. Joe Palca, NPR News. (SOUNDBITE OF JOHN WILLIAMS' \"CANTO BIGHT\") MARY LOUISE KELLY, HOST:  To another story now, two miniature spacecraft pulled off a remarkable feat last month. They provided live play-by-play as NASA's InSight probe made its successful touchdown on Mars. NPR's Joe Palca has the story of the two experimental mini-satellites known collectively as MarCO. JOE PALCA, BYLINE: When InSight left Earth last May, NASA provided a familiar narration. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED NASA EMPLOYEE: Five, four, three, two. . . PALCA: Blastoff. Now, you expect to hear that. What's more unusual is this. (SOUNDBITE OF ARCHIVED RECORDING) CHRISTINE SZALAI: Altitude 400 meters, 300 meters, 200 meters. PALCA: That's Christine Szalai, a member of the InSight landing team at the Jet Propulsion Laboratory in Pasadena. She narrated InSight's descent from the control room at JPL. (SOUNDBITE OF ARCHIVED RECORDING) SZALAI: Thirty meters, 20 meters, 17 meters. PALCA: And then blastoff - I mean, touchdown. (SOUNDBITE OF ARCHIVED RECORDING) SZALAI: Touchdown confirmed. (APPLAUSE) PALCA: Now, most of the time there is very little information coming back to Earth from a space probe as it comes in for a landing, maybe just a simple tone at key moments. And if there are any numbers, they're based on where mission controllers think the spacecraft should be based on a projected timeline. But the numbers Szalai was reading off were coming from InSight's onboard radar, actual data as it was coming in for landing. Anne Marinan helped provide that data. She's an engineer at JPL who worked on MarCO. And she heard Szalai's narration. ANNE MARINAN: She was doing it based on data that she was seeing that was transmitted from MarCO. PALCA: How did that make you feel? MARINAN: I didn't quite appreciate that that's what she was doing until I saw a rebroadcast. And it kind of hit me that she was able to make those calls based on data that my spacecraft had sent. PALCA: MarCO is actually two briefcase-sized spacecraft that left Earth with InSight. The pair trailed close behind InSight on the way to Mars. InSight's radio couldn't reach Earth directly as it plummeted to the surface, but it could reach MarCO flying nearby. And MarCO's job was to relay back to mission control what InSight was sending. Marinan says the miniature satellite was an experiment. MARINAN: MarCO was primarily a technology demonstration. So there were brand-new components and technologies that we flew for the very first time in deep space. PALCA: A new kind of antenna, a new kind of propulsion. And Marinan says the new technology didn't always work perfectly. The propulsion system gave them headaches. But. . . MARINAN: Well, it worked perfectly when it had to. PALCA: This was Marinan's first assignment at JPL. She only recently earned her Ph. D. So what do you do for an encore when your first mission works so well? MARINAN: I am actually currently putting together another spacecraft. PALCA: Another miniature spacecraft, this one more science-y (ph), going to take pictures of a nearby asteroid. Joe Palca, NPR News. (SOUNDBITE OF JOHN WILLIAMS' \"CANTO BIGHT\")", "section": "Changing The World One Invention At A Time", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-28-680747866": {"title": "FCC Investigates Widespread CenturyLink Outage That Disrupted 911 Service : NPR", "url": "https://www.npr.org/2018/12/28/680747866/fcc-investigates-widespread-centurylink-outage-that-disrupted-911-service", "author": "No author found", "published_date": "2018-12-28", "content": "ARI SHAPIRO, HOST:  A phone and Internet outage has been disrupting 911 services across the country since yesterday, and today the FCC says it's launched an investigation. Montana Public Radio's Eric Whitney reports. ERIC WHITNEY, BYLINE: Louisiana-based CenturyLink announced the outage a little after 8 a. m. yesterday, and people calling 911 and other emergency service numbers started getting messages like this. (SOUNDBITE OF ARCHIVED RECORDING)UNIDENTIFIED PERSON: We're sorry. Your call did not go through. Will you please try your call again? WHITNEY: Twenty-four hours later, FCC Chairman Ajit Pai tweeted the CenturyLink outage is completely unacceptable and that the agency was launching an investigation. It's unclear exactly how many people were affected. The website DownDetector said the outage was mostly in Western states, but emergency service providers on both coasts reported disruptions. CenturyLink has offered no details about the nature of the problem beyond saying a network element was impacting customer services. Brian Fontes, CEO of the National Emergency Number Association, says the outage is a reminder that people should have backup plans for emergency communications. BRIAN FONTES: Do I have another provider for wireless service? Do I know the tentative phone numbers for the police or fire department? WHITNEY: CenturyLink tweeted that in case of an emergency, customers should use their wireless phones to call 911 or drive to the nearest fire station or emergency facility. That comment drew ridicule on social media. This afternoon, CenturyLink said that in areas where they're the 911 service provider, 911 calls are now completing but that their network is still experiencing service disruptions. CenturyLink's disruptions also affected Verizon customers in at least two states and knocked ATMs offline in Montana and Idaho. A hospital in Greeley, Colo. , reported losing access to electronic medical records. The last time the FCC investigated a 911 outage was last year. It fined AT&T $5. 25 million for two nationwide disruptions that lasted for a total of about six hours. For NPR News, I'm Eric Whitney in Missoula. ARI SHAPIRO, HOST:   A phone and Internet outage has been disrupting 911 services across the country since yesterday, and today the FCC says it's launched an investigation. Montana Public Radio's Eric Whitney reports. ERIC WHITNEY, BYLINE: Louisiana-based CenturyLink announced the outage a little after 8 a. m. yesterday, and people calling 911 and other emergency service numbers started getting messages like this. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: We're sorry. Your call did not go through. Will you please try your call again? WHITNEY: Twenty-four hours later, FCC Chairman Ajit Pai tweeted the CenturyLink outage is completely unacceptable and that the agency was launching an investigation. It's unclear exactly how many people were affected. The website DownDetector said the outage was mostly in Western states, but emergency service providers on both coasts reported disruptions. CenturyLink has offered no details about the nature of the problem beyond saying a network element was impacting customer services. Brian Fontes, CEO of the National Emergency Number Association, says the outage is a reminder that people should have backup plans for emergency communications. BRIAN FONTES: Do I have another provider for wireless service? Do I know the tentative phone numbers for the police or fire department? WHITNEY: CenturyLink tweeted that in case of an emergency, customers should use their wireless phones to call 911 or drive to the nearest fire station or emergency facility. That comment drew ridicule on social media. This afternoon, CenturyLink said that in areas where they're the 911 service provider, 911 calls are now completing but that their network is still experiencing service disruptions. CenturyLink's disruptions also affected Verizon customers in at least two states and knocked ATMs offline in Montana and Idaho. A hospital in Greeley, Colo. , reported losing access to electronic medical records. The last time the FCC investigated a 911 outage was last year. It fined AT&T $5. 25 million for two nationwide disruptions that lasted for a total of about six hours. For NPR News, I'm Eric Whitney in Missoula.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-28-680759871": {"title": "How Much Of The Internet Is Fake? A Lot. : NPR", "url": "https://www.npr.org/2018/12/28/680759871/how-much-of-the-internet-is-fake-a-lot", "author": "No author found", "published_date": "2018-12-28", "content": "ARI SHAPIRO, HOST:  In the last year, we've heard a lot about fake stuff online. Troll farms in Russia spread viral hoaxes to sway public opinion. Click farms in China download apps thousands of times to boost their ratings. It's reasonable to ask how much of the internet is fake. Max Read tried to answer that question in a piece for New York Magazine, and he joins us now. Hi there. MAX READ: Hi. SHAPIRO: So let's start with a term that I learned from your article, the inversion. What is the inversion? READ: The inversion is the point at which there's so much fakery going on that our natural ability to tell the difference between what's real and what's fake becomes inverted. And real things all of a sudden seem totally fake to us, and fake things have this sort of power and the presence of the real. SHAPIRO: And as you write in this article, the Internet might have actually passed the inversion in some ways years ago. But you say that 2018 felt to you like the year that we passed the inversion. Why did 2018 feel to you like the year we crossed this threshold? READ: It was just a year in which there was such a barrage of incidences of news stories both big and small that seemed to involve fakery at scales from the minute to the enormous. In November, for example, the Justice Department prosecuted a bunch of ad scammers who had more or less created a sort of simulacrum of the Internet where they had fake users making fake mouse movements, clicking on fake websites with fake social media profiles. And the only real thing on it was the ads. This was also a year when deepfakes was debuted, this technology that allows you to fake videos in a way that really was previously unforeseen and for basically anybody with a desktop computer to do it. It was the kind of thing, once you start thinking about it in this way, everywhere you turn you were sort of like, is this real; is this not real? And at the end, I was sort of realizing every day, when I try to log into The New York Times, I am prodded by a little dialog box that asks me to prove that I'm human and not a bot. SHAPIRO: Right, identify the photos with a motorcycle, or say what the numbers on the house are. READ: Exactly. And all of a sudden, I'm thinking, well, gosh, am I real? Am I actually a bot? SHAPIRO: Why should this matter to most people? I mean, if I like an Instagram account and that account has 500 followers and I think the photographs are beautiful, what do I care whether 400 of those followers are fake? READ: My problem with it is that it has this corrosive effect on how we talk to each other, that if you start to understand how much of what you see online isn't what it represents itself to be, you start to see everything in that same way. If I can put a really minor example of it forward, I woke up this morning, and on Twitter somebody was going viral for accusing Netflix of having faked a bunch of viral memes about a new movie that they have. And I looked into it a little bit, and I realized, no, these were all people who, shocking as it might be, really liked this movie \"Bird Box. \" But we were so used to the idea that this is something that a company might do, that it might seed the world with these AstroTurfed, you know, crowd reactions or whatever that we weren't able to see that, no, these were just regular people who wanted to make jokes about something. SHAPIRO: OK, so reading your article, at the end I felt like you had a lot of insights into everything that's fake online. And it seemed like some editor said, you have to answer the question, so what are we going to do about it? READ: (Laughter). SHAPIRO: And it doesn't seem like you have a great answer. So what are we going to do about it (laughter)? READ: Well, I'm still trying to figure out if I'm a human or a bot. SHAPIRO: (Laughter). READ: So solving that is my first priority. You know, I think that the first thing is figuring out what's wrong, that we need to really focus on the idea that this is a problem not of lost truth, which is I know a very popular kind of idea, but that it's a problem of trust - that we no longer are able to trust in institutions, that we are no longer able to trust in each other. And trust requires networks. It requires a bunch of different people acting in different arenas to lean on one another and to trust one another. So it can't just be a thing where we say, oh, Facebook and Google have to develop better technology because the truth is they have fantastic technology for detecting frauds and fakes. This has to be a thing where people in tech, people in media, people in politics and sort of people everywhere have to be willing to make changes in how they make money, make changes in how they approach people, users, consumers, voters and to recreate that sense of trust in society. SHAPIRO: Max Read of New York Magazine talking with us about how much of the Internet is fake, thanks a lot. READ: Thank you. ARI SHAPIRO, HOST:   In the last year, we've heard a lot about fake stuff online. Troll farms in Russia spread viral hoaxes to sway public opinion. Click farms in China download apps thousands of times to boost their ratings. It's reasonable to ask how much of the internet is fake. Max Read tried to answer that question in a piece for New York Magazine, and he joins us now. Hi there. MAX READ: Hi. SHAPIRO: So let's start with a term that I learned from your article, the inversion. What is the inversion? READ: The inversion is the point at which there's so much fakery going on that our natural ability to tell the difference between what's real and what's fake becomes inverted. And real things all of a sudden seem totally fake to us, and fake things have this sort of power and the presence of the real. SHAPIRO: And as you write in this article, the Internet might have actually passed the inversion in some ways years ago. But you say that 2018 felt to you like the year that we passed the inversion. Why did 2018 feel to you like the year we crossed this threshold? READ: It was just a year in which there was such a barrage of incidences of news stories both big and small that seemed to involve fakery at scales from the minute to the enormous. In November, for example, the Justice Department prosecuted a bunch of ad scammers who had more or less created a sort of simulacrum of the Internet where they had fake users making fake mouse movements, clicking on fake websites with fake social media profiles. And the only real thing on it was the ads. This was also a year when deepfakes was debuted, this technology that allows you to fake videos in a way that really was previously unforeseen and for basically anybody with a desktop computer to do it. It was the kind of thing, once you start thinking about it in this way, everywhere you turn you were sort of like, is this real; is this not real? And at the end, I was sort of realizing every day, when I try to log into The New York Times, I am prodded by a little dialog box that asks me to prove that I'm human and not a bot. SHAPIRO: Right, identify the photos with a motorcycle, or say what the numbers on the house are. READ: Exactly. And all of a sudden, I'm thinking, well, gosh, am I real? Am I actually a bot? SHAPIRO: Why should this matter to most people? I mean, if I like an Instagram account and that account has 500 followers and I think the photographs are beautiful, what do I care whether 400 of those followers are fake? READ: My problem with it is that it has this corrosive effect on how we talk to each other, that if you start to understand how much of what you see online isn't what it represents itself to be, you start to see everything in that same way. If I can put a really minor example of it forward, I woke up this morning, and on Twitter somebody was going viral for accusing Netflix of having faked a bunch of viral memes about a new movie that they have. And I looked into it a little bit, and I realized, no, these were all people who, shocking as it might be, really liked this movie \"Bird Box. \" But we were so used to the idea that this is something that a company might do, that it might seed the world with these AstroTurfed, you know, crowd reactions or whatever that we weren't able to see that, no, these were just regular people who wanted to make jokes about something. SHAPIRO: OK, so reading your article, at the end I felt like you had a lot of insights into everything that's fake online. And it seemed like some editor said, you have to answer the question, so what are we going to do about it? READ: (Laughter). SHAPIRO: And it doesn't seem like you have a great answer. So what are we going to do about it (laughter)? READ: Well, I'm still trying to figure out if I'm a human or a bot. SHAPIRO: (Laughter). READ: So solving that is my first priority. You know, I think that the first thing is figuring out what's wrong, that we need to really focus on the idea that this is a problem not of lost truth, which is I know a very popular kind of idea, but that it's a problem of trust - that we no longer are able to trust in institutions, that we are no longer able to trust in each other. And trust requires networks. It requires a bunch of different people acting in different arenas to lean on one another and to trust one another. So it can't just be a thing where we say, oh, Facebook and Google have to develop better technology because the truth is they have fantastic technology for detecting frauds and fakes. This has to be a thing where people in tech, people in media, people in politics and sort of people everywhere have to be willing to make changes in how they make money, make changes in how they approach people, users, consumers, voters and to recreate that sense of trust in society. SHAPIRO: Max Read of New York Magazine talking with us about how much of the Internet is fake, thanks a lot. READ: Thank you.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-29-680882610": {"title": "Author Of 'The New Childhood' Advises Parents: Don't Panic About Screen Time : NPR", "url": "https://www.npr.org/2018/12/29/680882610/author-of-the-new-childhood-advises-parents-dont-panic-about-screen-time", "author": "No author found", "published_date": "2018-12-29", "content": "DEBBIE ELLIOTT, HOST:  The week between Christmas and New Year's is a celebration of family, togetherness and, for parents, observing your children losing themselves in screens big and small. And really at any time of the year, it's hard not to get a chill of anxiety when you see your kids so immersed in the pixellated world of Minecraft, Mario and Fortnite. But Jordan Shapiro says calm down. He's the author of the book \"The New Childhood: Raising Kids To Thrive In A Connected World\" and joins us now from member station WHYY in Philadelphia. Welcome to the show. JORDAN SHAPIRO: Thanks for having me, great to be here. ELLIOTT: So your message is stop panicking. Really? SHAPIRO: Yeah. I think you should stop panicking. You know, I was listening to your intro. You said they lose themselves in their devices, but what if they're finding themselves in their devices? What if it's the - that's the way they're going to learn how to articulate themselves, how to live a fulfilled life, how to live an authentic life in a world that's connected? ELLIOTT: I think, though, when we see our kids and they're hunched over with their screen and there are other people around, it just sort of panics you. You want them to engage with you. SHAPIRO: I certainly understand that, but parents to some extent need to remember that this sort of obsessive behavior around toys of interest, games of interest, this is normal for kids. This is not unique to the devices. While I certainly get worried about my own children when I tell them come to the dinner table and they say, hey, let me just finish this one life, I'll be right there, that's certainly frustrating. But I want parents to sort of back up and realize that it's through play that kids prepare themselves for those sort of social interactions that you're so worried about. That's how all the research tells us kids learn to play - through play, they learn to interact with each other, to follow rules, the executive function skills - all of those good things come through play. ELLIOTT: You know, you do a good bit of history in this book that I found interesting - the way you look back at the role of emerging technology in childhood and the way that people looked at it over time, that this is nothing new, parents sort of freaking out about something new that their kids got their hands on. SHAPIRO: I mean, parents have always been freaking out. Doctors have always been freaking out. In my research, I found old newspaper articles in which physicians warned parents not to let their kids sit near the window on trains because their brains could not handle things going by so quickly - right? - that this would cause neurological damage. We're not built to be able to deal with things so fast. You know, I think when you really look at it, what you discover is that so many toys, so many games, so many ways of playing arrived along with particular economic models and particular technological models of our society. And so I don't think that anyone intentionally said, hey, let's create this in order to prepare kids to be factory workers. But nevertheless, those toys arose, and those toys taught kids how to play in ways and how to habituate themselves to the behaviors that make it easy for them to adapt into the adult world around them. ELLIOTT: You even wrote that people panicked about the printing press. SHAPIRO: (Laughter) A lot of people panicked about the printing press. That's absolutely true. But I think there was also a lot of panic about the idea that it forced people to be in a cocoon, right? Storytelling prior to the printing press meant you sat in a circle. You sat around a campfire. You sat at church. You sat in these group activities together. It was more like going to a movie or the theater or something where you would be with a storyteller. And as soon as you had books, people could go into a nook in the corner of their house and read by themselves and sort of isolate themselves. ELLIOTT: And isolation is still an issue, right? That's one of the things I think I think about. If kids are playing video games or looking at screens in their rooms and they're not out playing and interacting, are they isolating themselves? SHAPIRO: Well, I mean, that's a valid concern. I think most of the way they're playing now is connected. Most of the way they're playing is online. It's talking to people. You know, the Fortnite, which is unbelievably popular right now, involves lots of kids arriving together to play in a game all at once, right? They all have their headphones on. They're talking back and forth. If there's a problem in my house, it's too much talking back and forth, right? Sometimes I wish my kids would quiet down a little bit so I could play in peace - so I could work in peace while they were playing. ELLIOTT: Hey, you had a little slip there. You play in peace you say. SHAPIRO: (Laughter) I want to play in peace. Well, I think that's actually not a terrible slip because I think we have to remember that their play is their work and our work is our play, right? We often think of work as the thing that adults do, but we get very obsessed. We get focused on the work that we're doing and don't want to be interrupted either. ELLIOTT: Now, you know, we've heard so much about, like, two-hour screen time, three-hour screen time. Do you put a limit on the amount of time per day? SHAPIRO: I don't put a limit on time. I never have put a limit on the time. And the reason I don't is because I prefer to be positive. So I certainly require lots of other things besides screen time that my kids must do. They must read books. They must go outside. They must brush their teeth. But these digital devices, our phones, our video game machines, our iPads, our laptops, these are interactive tools. And they're involved in so much of what we do. And the idea that we would say, hey, two hours and then no more is sort of disingenuous to me, right? I think we need to be much more thinking about what they're doing on those devices. My younger son is really into 3D printing. And so he'll spend hours using 3D modeling software to try to build these sculptures that he's going to print. Well, should I limit that to two hours? He's sort of deeply engaged in something creative and active that involves math and art and science. And I think that's a great thing that doesn't need to be cut off. However, if all he's doing is watching YouTube videos of people opening toys, I might have a different attitude. ELLIOTT: Now, it does worry me a little bit to think that kids aren't outside as much as they should be. You know, you talk about somehow that these games can be the new skateboards. But I think you need sunshine. You need to get out and ride your bike, especially in this country where we have an issue with obesity among our young people. I mean, shouldn't we make sure that they put the screens down and go outside? SHAPIRO: Actually, when you talk to most kids - right? - when they've done large surveys of large groups of kids, what they find out is the kids actually want to go outside and do that, but often it's the parents' restrictions, right? We're very scared of people being outside unsupervised right now. But more, what I would say is let's not frame those as opposites - right? - screen time or outdoor time. I would like to see them integrated even, right? There's so much that you could use technology for to interact with nature, right? So much of science is about using technology to understand the natural world. You know, they can go photograph nature. They can go measure nature. There's so much that they could be doing outside that's not one or the other. This idea that engineered technology is in opposition to the natural world is sort of against everything that science has always been about. ELLIOTT: Jordan Shapiro, author of \"The New Childhood: Raising Kids To Thrive In A Connected World,\" thank you for speaking with us. SHAPIRO: Thanks for having me. DEBBIE ELLIOTT, HOST:   The week between Christmas and New Year's is a celebration of family, togetherness and, for parents, observing your children losing themselves in screens big and small. And really at any time of the year, it's hard not to get a chill of anxiety when you see your kids so immersed in the pixellated world of Minecraft, Mario and Fortnite. But Jordan Shapiro says calm down. He's the author of the book \"The New Childhood: Raising Kids To Thrive In A Connected World\" and joins us now from member station WHYY in Philadelphia. Welcome to the show. JORDAN SHAPIRO: Thanks for having me, great to be here. ELLIOTT: So your message is stop panicking. Really? SHAPIRO: Yeah. I think you should stop panicking. You know, I was listening to your intro. You said they lose themselves in their devices, but what if they're finding themselves in their devices? What if it's the - that's the way they're going to learn how to articulate themselves, how to live a fulfilled life, how to live an authentic life in a world that's connected? ELLIOTT: I think, though, when we see our kids and they're hunched over with their screen and there are other people around, it just sort of panics you. You want them to engage with you. SHAPIRO: I certainly understand that, but parents to some extent need to remember that this sort of obsessive behavior around toys of interest, games of interest, this is normal for kids. This is not unique to the devices. While I certainly get worried about my own children when I tell them come to the dinner table and they say, hey, let me just finish this one life, I'll be right there, that's certainly frustrating. But I want parents to sort of back up and realize that it's through play that kids prepare themselves for those sort of social interactions that you're so worried about. That's how all the research tells us kids learn to play - through play, they learn to interact with each other, to follow rules, the executive function skills - all of those good things come through play. ELLIOTT: You know, you do a good bit of history in this book that I found interesting - the way you look back at the role of emerging technology in childhood and the way that people looked at it over time, that this is nothing new, parents sort of freaking out about something new that their kids got their hands on. SHAPIRO: I mean, parents have always been freaking out. Doctors have always been freaking out. In my research, I found old newspaper articles in which physicians warned parents not to let their kids sit near the window on trains because their brains could not handle things going by so quickly - right? - that this would cause neurological damage. We're not built to be able to deal with things so fast. You know, I think when you really look at it, what you discover is that so many toys, so many games, so many ways of playing arrived along with particular economic models and particular technological models of our society. And so I don't think that anyone intentionally said, hey, let's create this in order to prepare kids to be factory workers. But nevertheless, those toys arose, and those toys taught kids how to play in ways and how to habituate themselves to the behaviors that make it easy for them to adapt into the adult world around them. ELLIOTT: You even wrote that people panicked about the printing press. SHAPIRO: (Laughter) A lot of people panicked about the printing press. That's absolutely true. But I think there was also a lot of panic about the idea that it forced people to be in a cocoon, right? Storytelling prior to the printing press meant you sat in a circle. You sat around a campfire. You sat at church. You sat in these group activities together. It was more like going to a movie or the theater or something where you would be with a storyteller. And as soon as you had books, people could go into a nook in the corner of their house and read by themselves and sort of isolate themselves. ELLIOTT: And isolation is still an issue, right? That's one of the things I think I think about. If kids are playing video games or looking at screens in their rooms and they're not out playing and interacting, are they isolating themselves? SHAPIRO: Well, I mean, that's a valid concern. I think most of the way they're playing now is connected. Most of the way they're playing is online. It's talking to people. You know, the Fortnite, which is unbelievably popular right now, involves lots of kids arriving together to play in a game all at once, right? They all have their headphones on. They're talking back and forth. If there's a problem in my house, it's too much talking back and forth, right? Sometimes I wish my kids would quiet down a little bit so I could play in peace - so I could work in peace while they were playing. ELLIOTT: Hey, you had a little slip there. You play in peace you say. SHAPIRO: (Laughter) I want to play in peace. Well, I think that's actually not a terrible slip because I think we have to remember that their play is their work and our work is our play, right? We often think of work as the thing that adults do, but we get very obsessed. We get focused on the work that we're doing and don't want to be interrupted either. ELLIOTT: Now, you know, we've heard so much about, like, two-hour screen time, three-hour screen time. Do you put a limit on the amount of time per day? SHAPIRO: I don't put a limit on time. I never have put a limit on the time. And the reason I don't is because I prefer to be positive. So I certainly require lots of other things besides screen time that my kids must do. They must read books. They must go outside. They must brush their teeth. But these digital devices, our phones, our video game machines, our iPads, our laptops, these are interactive tools. And they're involved in so much of what we do. And the idea that we would say, hey, two hours and then no more is sort of disingenuous to me, right? I think we need to be much more thinking about what they're doing on those devices. My younger son is really into 3D printing. And so he'll spend hours using 3D modeling software to try to build these sculptures that he's going to print. Well, should I limit that to two hours? He's sort of deeply engaged in something creative and active that involves math and art and science. And I think that's a great thing that doesn't need to be cut off. However, if all he's doing is watching YouTube videos of people opening toys, I might have a different attitude. ELLIOTT: Now, it does worry me a little bit to think that kids aren't outside as much as they should be. You know, you talk about somehow that these games can be the new skateboards. But I think you need sunshine. You need to get out and ride your bike, especially in this country where we have an issue with obesity among our young people. I mean, shouldn't we make sure that they put the screens down and go outside? SHAPIRO: Actually, when you talk to most kids - right? - when they've done large surveys of large groups of kids, what they find out is the kids actually want to go outside and do that, but often it's the parents' restrictions, right? We're very scared of people being outside unsupervised right now. But more, what I would say is let's not frame those as opposites - right? - screen time or outdoor time. I would like to see them integrated even, right? There's so much that you could use technology for to interact with nature, right? So much of science is about using technology to understand the natural world. You know, they can go photograph nature. They can go measure nature. There's so much that they could be doing outside that's not one or the other. This idea that engineered technology is in opposition to the natural world is sort of against everything that science has always been about. ELLIOTT: Jordan Shapiro, author of \"The New Childhood: Raising Kids To Thrive In A Connected World,\" thank you for speaking with us. SHAPIRO: Thanks for having me.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-29-680882527": {"title": "What Asimov Predicted For 2019: Computerization And Lunar Life : NPR", "url": "https://www.npr.org/2018/12/29/680882527/what-asimov-predicted-for-2019-computerization-and-lunar-life", "author": "No author found", "published_date": "2018-12-29", "content": "DEBBIE ELLIOTT, HOST:  Thirty-five years ago, the Toronto Star asked science fiction writer Isaac Asimov to predict what the year 2019 would be like. Many of Asimov's predictions were off base, like the forecast of an up and running mining operation on the moon or the prediction that countries would work together under what he called the faint semblance of a world government by cooperation. The sci-fi writer was closer to reality when he predicted robots taking over repetitive assembly line jobs and the massive retraining needed for those left out of work. EUGENE FIUME: He had a very utopian view of how computers would change work, but he had also a kind of warning. ELLIOTT: That's computer scientist Eugene Fiume of Simon Fraser University. Fiume says that despite his concern about the need for rapid re-education, Asimov was an optimist in his predictions. FIUME: What he predicted was that computers themselves would allow people to become researchers and scientists and artists - this idea of replacing that low-level intellectual work and allowing people to improve themselves by looking at higher level intellectual work. ELLIOTT: It's hard to say how many laid-off factory workers have moved on to pick up paintbrushes, but Fiume says that Asimov's 1983 essay envisions how, left to their own devices. . . FIUME: People will seek to improve themselves and avail themselves of tools to gain an increased understanding of the world and the cosmos. I love how that is a central aspect of his predictions. (SOUNDBITE OF ZOE KEATING'S \"FORTE\")ELLIOTT: Eugene Fiume, a computer scientist at Simon Fraser University. (SOUNDBITE OF ZOE KEATING'S \"FORTE\") DEBBIE ELLIOTT, HOST:   Thirty-five years ago, the Toronto Star asked science fiction writer Isaac Asimov to predict what the year 2019 would be like. Many of Asimov's predictions were off base, like the forecast of an up and running mining operation on the moon or the prediction that countries would work together under what he called the faint semblance of a world government by cooperation. The sci-fi writer was closer to reality when he predicted robots taking over repetitive assembly line jobs and the massive retraining needed for those left out of work. EUGENE FIUME: He had a very utopian view of how computers would change work, but he had also a kind of warning. ELLIOTT: That's computer scientist Eugene Fiume of Simon Fraser University. Fiume says that despite his concern about the need for rapid re-education, Asimov was an optimist in his predictions. FIUME: What he predicted was that computers themselves would allow people to become researchers and scientists and artists - this idea of replacing that low-level intellectual work and allowing people to improve themselves by looking at higher level intellectual work. ELLIOTT: It's hard to say how many laid-off factory workers have moved on to pick up paintbrushes, but Fiume says that Asimov's 1983 essay envisions how, left to their own devices. . . FIUME: People will seek to improve themselves and avail themselves of tools to gain an increased understanding of the world and the cosmos. I love how that is a central aspect of his predictions. (SOUNDBITE OF ZOE KEATING'S \"FORTE\") ELLIOTT: Eugene Fiume, a computer scientist at Simon Fraser University. (SOUNDBITE OF ZOE KEATING'S \"FORTE\")", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-29-680882603": {"title": "Cuba's Mobile Internet Access Is A Big Step, But It's Not Without Flaws : NPR", "url": "https://www.npr.org/2018/12/29/680882603/cubas-mobile-internet-access-is-a-big-step-but-its-not-without-flaws", "author": "No author found", "published_date": "2018-12-29", "content": "DEBBIE ELLIOTT, HOST:  Cuba now has 3G mobile wireless. What that means is that Cubans can get Internet on their phones, but it comes from the state-run monopoly. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: Iliana Hernandez (ph) is an independent journalist based in Cuba. She says a few days ago while riding the bus, she got emotional. ILIANA HERNANDEZ: (Speaking Spanish). GARSD: She was able to video chat on her phone with friends who live in Miami. When I reached her over the phone in Havana, she says, years ago, this would be unthinkable. This is the latest of several moves in recent years to bring down the digital divide between the island and the rest of the world. Cubans have had limited access to Internet for years now in hotels, Internet cafes and public hotspots. But as of early December, Cubans can get 3G Wi-Fi on their mobile phones. Hernandez says she's excited, and she's also scared. HERNANDEZ: (Speaking Spanish). GARSD: \"You know,\" she says, \"our phones are tapped. They hear our conversations. They know what we are doing. \"HERNANDEZ: (Speaking Spanish). GARSD: She says because of her work as a journalist, every so often, the government detains her. Sometimes, she sees them watching her house. Professor Ted Henken teaches Latin America studies at City University of New York. He says that government surveillance is part of everyday life in Cuba. When it comes to the Internet. . . TED HENKEN: I wouldn't compare it to China quite yet because Cuba - the surveillance of the Internet, it will have to catch up with the technology of the Internet. But most Cubans will tell you that everything that they send through the media, whether it's a telephone call or an Internet message or where they surf, they assume that it's being monitored. GARSD: Henken says Cubans being able to access Internet on their phones - that's a big step, but it's not without flaws. The service is pretty bad. Several dissident sites are completely blocked. And also, Henken says that most Cubans can't afford the Internet packages the government is offering. HENKEN: You can get a plan. And you pay a certain amount per month - $7, $10, $20 or $30. The irony, however, is that that would be equivalent to a whole month's salary for a Cuban professional - $30 a month. GARSD: The day after Internet came to Cuban phones, Elian Gonzalez, once a child at the center of a custody battle between the U. S. and Cuban governments, joined Twitter with a post praising the regime. But this could also be a big deal for Cuba's growing wave of independent journalists, like Iliana Hernandez. HERNANDEZ: (Speaking Spanish). GARSD: \"Sometimes, it would take eight hours to upload a video post,\" she says. And even though Cubans on the island would watch and read her blog, the audience was limited. Hernandez says she pays for mobile Internet with savings from living abroad a couple of years ago. She's just not sure how many Cubans can afford it. But Hernandez also believes slowly but surely things are changing in Cuba. And the Internet is going to accelerate that. People will be able to express their frustrations at the government. HERNANDEZ: (Speaking Spanish). GARSD: She says, \"this is a little piece of the freedom we are all looking for. Now what we need to do is lose our fear. \" Jasmine Garsd, NPR News, New York. DEBBIE ELLIOTT, HOST:   Cuba now has 3G mobile wireless. What that means is that Cubans can get Internet on their phones, but it comes from the state-run monopoly. NPR's Jasmine Garsd reports. JASMINE GARSD, BYLINE: Iliana Hernandez (ph) is an independent journalist based in Cuba. She says a few days ago while riding the bus, she got emotional. ILIANA HERNANDEZ: (Speaking Spanish). GARSD: She was able to video chat on her phone with friends who live in Miami. When I reached her over the phone in Havana, she says, years ago, this would be unthinkable. This is the latest of several moves in recent years to bring down the digital divide between the island and the rest of the world. Cubans have had limited access to Internet for years now in hotels, Internet cafes and public hotspots. But as of early December, Cubans can get 3G Wi-Fi on their mobile phones. Hernandez says she's excited, and she's also scared. HERNANDEZ: (Speaking Spanish). GARSD: \"You know,\" she says, \"our phones are tapped. They hear our conversations. They know what we are doing. \" HERNANDEZ: (Speaking Spanish). GARSD: She says because of her work as a journalist, every so often, the government detains her. Sometimes, she sees them watching her house. Professor Ted Henken teaches Latin America studies at City University of New York. He says that government surveillance is part of everyday life in Cuba. When it comes to the Internet. . . TED HENKEN: I wouldn't compare it to China quite yet because Cuba - the surveillance of the Internet, it will have to catch up with the technology of the Internet. But most Cubans will tell you that everything that they send through the media, whether it's a telephone call or an Internet message or where they surf, they assume that it's being monitored. GARSD: Henken says Cubans being able to access Internet on their phones - that's a big step, but it's not without flaws. The service is pretty bad. Several dissident sites are completely blocked. And also, Henken says that most Cubans can't afford the Internet packages the government is offering. HENKEN: You can get a plan. And you pay a certain amount per month - $7, $10, $20 or $30. The irony, however, is that that would be equivalent to a whole month's salary for a Cuban professional - $30 a month. GARSD: The day after Internet came to Cuban phones, Elian Gonzalez, once a child at the center of a custody battle between the U. S. and Cuban governments, joined Twitter with a post praising the regime. But this could also be a big deal for Cuba's growing wave of independent journalists, like Iliana Hernandez. HERNANDEZ: (Speaking Spanish). GARSD: \"Sometimes, it would take eight hours to upload a video post,\" she says. And even though Cubans on the island would watch and read her blog, the audience was limited. Hernandez says she pays for mobile Internet with savings from living abroad a couple of years ago. She's just not sure how many Cubans can afford it. But Hernandez also believes slowly but surely things are changing in Cuba. And the Internet is going to accelerate that. People will be able to express their frustrations at the government. HERNANDEZ: (Speaking Spanish). GARSD: She says, \"this is a little piece of the freedom we are all looking for. Now what we need to do is lose our fear. \" Jasmine Garsd, NPR News, New York.", "section": "Technology", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}, "2018-12-30-680726060": {"title": "In 2018, Streaming Services Helped Fuel A New Latin Explosion : NPR", "url": "https://www.npr.org/2018/12/30/680726060/in-2018-streaming-services-helped-fuel-a-new-latin-explosion", "author": "No author found", "published_date": "2018-12-30", "content": "LEILA FADEL, HOST: NPR Music's Alt. Latino is out with their year end list of 2018's best songs and albums. And they've also been doing some reading and deep thinking about a trend that started in 2017 and shows no signs of slowing down. Alt. Latino hosts Felix Contreras and NPR Music contributor Stefanie Fernandez, now a producer with The Atlantic, are here to talk about that. Welcome. FELIX CONTRERAS, BYLINE: Good morning. STEFANIE FERNANDEZ: Morning, Leila. FADEL: So let's start with the trend, this ongoing thing you've been following. Tell us what it is. CONTRERAS: It's all about streaming, Leila - things like YouTube, Spotify, Apple Music and not just here but in Spanish-speaking countries around the world. FERNANDEZ: According to data from YouTube, the three most streamed artists with the most views in 2018 were all Spanish-language artists - eye-popping numbers from Ozuna at 10 billion views and J Balvin at 11 billion views. FADEL: And that's billion with a B. What explains how these numbers got to be so big? CONTRERAS: A variety of factors, OK? Last year, there was a perfect storm of a catchy tune, an unforgettable video and a title that could be easily sung by those who don't speak Spanish. (SOUNDBITE OF SONG, \"DESPACITO\")LUIS FONSI: (Singing) Despacito, quiero respirar tu cuello despacito. CONTRERAS: \"Despacito\" by Luis Fonsi and Daddy Yankee was the song of the spring and summer of 2017. And it was really like catching lightning in a bottle. It was unexpected, organic and driven by the fans, not conceived in a marketing meeting. And that sparked interest in Latin pop music. It was fueled by a very popular reggaeton beat. And this thing took hold all over the world. It was international, not just here in the United States. FADEL: Right. I mean, you couldn't really go anywhere without hearing that song. FERNANDEZ: And this year, the most viewed video was the remix of \"Te Bote. \"(SOUNDBITE OF SONG, \"TE BOTE REMIX\")NIO GARCIA: (Singing) Te bote, de mi vida te bote - yo te bote - y te di banda y te solte. CONTRERAS: This is a track that included a lot of heavy hitters. FERNANDEZ: Yeah. It's not new, but the remix with Ozuna, Bad Bunny, Nicky Jam, Casper, et. al. was incredibly popular just because it gathered the most popular players in the genre together in one video. CONTRERAS: And, you know, eight of the 10 most watched music videos released this year on YouTube are Latin songs or urbano songs - Latin urban. And that's not an accident. FADEL: OK, so we've talked about the numbers. So put on your sociologist hats for a moment. What's going on that's driving this? CONTRERAS: The Felix theory - there are a couple of things, OK? Demographics, first of all - I mean, the Latino population skews very young - OK? - which means they're more tech savvy, more mobile devices. And they're mostly bilingual. FERNANDEZ: And they've been turning to streaming services in big numbers as a younger group on YouTube especially, which is a more egalitarian platform, I think, than Apple Music and Spotify, and, of course, has held the monopoly on video streaming for over a decade now. It's easily shareable. And around the world, it's been accessible for a long time. CONTRERAS: And then after \"Despacito\" and later \"Mi Gente,\" it led to explosions in revenue last year. Like, they discovered that you could make a lot of money at this stuff. And it proved that streaming, for the first time, was the most profitable way to sell music, especially to this demographic. FADEL: Stef, you talk about this group. Do you consider yourself part of it? FERNANDEZ: I guess I am, Leila. I honestly grew up listening to reggaeton at parties and out in the world, but I never considered it music I liked, strangely enough. And I think this is common across younger Latin American groups. And as I got older, I realized I do love this music. It was music that was previously looked down upon and seen as low class, in many ways, across our community. But nowadays, it's become impossible to ignore the popularity and the infectiousness of it. FADEL: And, Felix - how do I put this delicately? You're not exactly part of that youth-driven trend. (LAUGHTER)CONTRERAS: OK. My experience is really the - thank you for putting it that way, too, by the way. FADEL: (Laughter). CONTRERAS: My experience reflects the declining influence of Latino baby boomers. Now, not unlike the entire baby boomer class, when it comes to music, we were really slow to catch on to streaming. I make Spotify lists, but I still ask the bands to send me CDs to preview for Alt. Latino because I listen mostly in my car. FADEL: CDs, man, that's real old school. CONTRERAS: (Laughter). FADEL: Time for a little best of, Stefanie. What was on your list this year? FERNANDEZ: Oh, man, a lot of the artists that we've talked about already - J Balvin and Bad Bunny definitely. But this year, especially in streaming, was kind to women in reggaeton, which is something new that we've seen. In particular, Natti Natasha and Becky G released a song called \"Sin Pijama\" that while is not necessarily a feminist anthem is definitely a reflection of the growing power of women in reggaeton. (SOUNDBITE OF SONG, \"SIN PIJAMA\")BECKY G AND NATTI NATASHA: (Singing) Si tu me llama', nos vamo' pa' tu casa, nos quedamo' en la cama sin pijama, sin pijama. FADEL: We've heard the word reggaeton a few times now. Remind us what that is. FERNANDEZ: Here's the thing. Reggaeton is not a new genre. It's been around for 40 years as it was pioneered by Afro-Panamanians in the 1980s and '90s. It's a genre that takes many genres from the Caribbean - Dembow, reggae, dancehall - and transforms it into urban music that is danceable and infuses rap and trap nowadays. FADEL: What are you expecting for 2019? FERNANDEZ: Well, we're going to be watching the Grammys in February. And we remember \"Despacito\" losing Record of the Year to Bruno Mars despite being the most ubiquitous song of that year. Time will tell if Cardi B, Bad Bunny and J Balvin's \"I Like It\" can take Record of the Year this year. But for now, \"Estamos Bien. \"(SOUNDBITE OF SONG, \"ESTAMOS BIEN\")BAD BUNNY: (Rapping) No te preocupes, estamos bien. CONTRERAS: \"Estamos Bien\" is one of the songs that caught our attention this year. It's also a Bad Bunny tune, and we thought it would be a nice tune to go out on. FADEL: Alt. Latino host Felix Contreras and Alt. Latino alum Stefanie Fernandez, now a producer with The Atlantic. You can find great coverage of Latino arts and culture on the Alt. Latino podcast and blog. Thank you both. CONTRERAS: Thank you. FERNANDEZ: Thank you. (SOUNDBITE OF SONG, \"ESTAMOS BIEN\")BAD BUNNY: (Rapping) No te preocupes, estamos bien, yeah. LEILA FADEL, HOST:  NPR Music's Alt. Latino is out with their year end list of 2018's best songs and albums. And they've also been doing some reading and deep thinking about a trend that started in 2017 and shows no signs of slowing down. Alt. Latino hosts Felix Contreras and NPR Music contributor Stefanie Fernandez, now a producer with The Atlantic, are here to talk about that. Welcome. FELIX CONTRERAS, BYLINE: Good morning. STEFANIE FERNANDEZ: Morning, Leila. FADEL: So let's start with the trend, this ongoing thing you've been following. Tell us what it is. CONTRERAS: It's all about streaming, Leila - things like YouTube, Spotify, Apple Music and not just here but in Spanish-speaking countries around the world. FERNANDEZ: According to data from YouTube, the three most streamed artists with the most views in 2018 were all Spanish-language artists - eye-popping numbers from Ozuna at 10 billion views and J Balvin at 11 billion views. FADEL: And that's billion with a B. What explains how these numbers got to be so big? CONTRERAS: A variety of factors, OK? Last year, there was a perfect storm of a catchy tune, an unforgettable video and a title that could be easily sung by those who don't speak Spanish. (SOUNDBITE OF SONG, \"DESPACITO\") LUIS FONSI: (Singing) Despacito, quiero respirar tu cuello despacito. CONTRERAS: \"Despacito\" by Luis Fonsi and Daddy Yankee was the song of the spring and summer of 2017. And it was really like catching lightning in a bottle. It was unexpected, organic and driven by the fans, not conceived in a marketing meeting. And that sparked interest in Latin pop music. It was fueled by a very popular reggaeton beat. And this thing took hold all over the world. It was international, not just here in the United States. FADEL: Right. I mean, you couldn't really go anywhere without hearing that song. FERNANDEZ: And this year, the most viewed video was the remix of \"Te Bote. \" (SOUNDBITE OF SONG, \"TE BOTE REMIX\") NIO GARCIA: (Singing) Te bote, de mi vida te bote - yo te bote - y te di banda y te solte. CONTRERAS: This is a track that included a lot of heavy hitters. FERNANDEZ: Yeah. It's not new, but the remix with Ozuna, Bad Bunny, Nicky Jam, Casper, et. al. was incredibly popular just because it gathered the most popular players in the genre together in one video. CONTRERAS: And, you know, eight of the 10 most watched music videos released this year on YouTube are Latin songs or urbano songs - Latin urban. And that's not an accident. FADEL: OK, so we've talked about the numbers. So put on your sociologist hats for a moment. What's going on that's driving this? CONTRERAS: The Felix theory - there are a couple of things, OK? Demographics, first of all - I mean, the Latino population skews very young - OK? - which means they're more tech savvy, more mobile devices. And they're mostly bilingual. FERNANDEZ: And they've been turning to streaming services in big numbers as a younger group on YouTube especially, which is a more egalitarian platform, I think, than Apple Music and Spotify, and, of course, has held the monopoly on video streaming for over a decade now. It's easily shareable. And around the world, it's been accessible for a long time. CONTRERAS: And then after \"Despacito\" and later \"Mi Gente,\" it led to explosions in revenue last year. Like, they discovered that you could make a lot of money at this stuff. And it proved that streaming, for the first time, was the most profitable way to sell music, especially to this demographic. FADEL: Stef, you talk about this group. Do you consider yourself part of it? FERNANDEZ: I guess I am, Leila. I honestly grew up listening to reggaeton at parties and out in the world, but I never considered it music I liked, strangely enough. And I think this is common across younger Latin American groups. And as I got older, I realized I do love this music. It was music that was previously looked down upon and seen as low class, in many ways, across our community. But nowadays, it's become impossible to ignore the popularity and the infectiousness of it. FADEL: And, Felix - how do I put this delicately? You're not exactly part of that youth-driven trend. (LAUGHTER) CONTRERAS: OK. My experience is really the - thank you for putting it that way, too, by the way. FADEL: (Laughter). CONTRERAS: My experience reflects the declining influence of Latino baby boomers. Now, not unlike the entire baby boomer class, when it comes to music, we were really slow to catch on to streaming. I make Spotify lists, but I still ask the bands to send me CDs to preview for Alt. Latino because I listen mostly in my car. FADEL: CDs, man, that's real old school. CONTRERAS: (Laughter). FADEL: Time for a little best of, Stefanie. What was on your list this year? FERNANDEZ: Oh, man, a lot of the artists that we've talked about already - J Balvin and Bad Bunny definitely. But this year, especially in streaming, was kind to women in reggaeton, which is something new that we've seen. In particular, Natti Natasha and Becky G released a song called \"Sin Pijama\" that while is not necessarily a feminist anthem is definitely a reflection of the growing power of women in reggaeton. (SOUNDBITE OF SONG, \"SIN PIJAMA\") BECKY G AND NATTI NATASHA: (Singing) Si tu me llama', nos vamo' pa' tu casa, nos quedamo' en la cama sin pijama, sin pijama. FADEL: We've heard the word reggaeton a few times now. Remind us what that is. FERNANDEZ: Here's the thing. Reggaeton is not a new genre. It's been around for 40 years as it was pioneered by Afro-Panamanians in the 1980s and '90s. It's a genre that takes many genres from the Caribbean - Dembow, reggae, dancehall - and transforms it into urban music that is danceable and infuses rap and trap nowadays. FADEL: What are you expecting for 2019? FERNANDEZ: Well, we're going to be watching the Grammys in February. And we remember \"Despacito\" losing Record of the Year to Bruno Mars despite being the most ubiquitous song of that year. Time will tell if Cardi B, Bad Bunny and J Balvin's \"I Like It\" can take Record of the Year this year. But for now, \"Estamos Bien. \" (SOUNDBITE OF SONG, \"ESTAMOS BIEN\") BAD BUNNY: (Rapping) No te preocupes, estamos bien. CONTRERAS: \"Estamos Bien\" is one of the songs that caught our attention this year. It's also a Bad Bunny tune, and we thought it would be a nice tune to go out on. FADEL: Alt. Latino host Felix Contreras and Alt. Latino alum Stefanie Fernandez, now a producer with The Atlantic. You can find great coverage of Latino arts and culture on the Alt. Latino podcast and blog. Thank you both. CONTRERAS: Thank you. FERNANDEZ: Thank you. (SOUNDBITE OF SONG, \"ESTAMOS BIEN\") BAD BUNNY: (Rapping) No te preocupes, estamos bien, yeah.", "section": "Music News", "disclaimer": " Copyright \u00a9 2018 NPR.  All rights reserved.  Visit our website terms of use and permissions pages at www.npr.org for further information. NPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record."}}